{
  "org.apache.hadoop.io.SequenceFile$Writer$FileOption:<init>(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.Server$MetricsUpdateRunner:run()" : null,
  "org.apache.hadoop.net.NetUtils:canonicalizeHost(java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider:options(org.apache.hadoop.conf.Configuration)" : "* A helper function to create an options object.\n   * @param conf the configuration to use\n   * @return a new options object",
  "org.apache.hadoop.io.compress.CompressionInputStream:getPos()" : "* This method returns the current position in the stream.\n   *\n   * @return Current position in stream as a long",
  "org.apache.hadoop.ha.HAServiceTarget:isAutoFailoverEnabled()" : "* @return true if auto failover should be considered enabled",
  "org.apache.hadoop.crypto.CryptoCodec:getCodecClasses(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:setDictionary(byte[],int,int)" : "* Does nothing.",
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:deleteCacheFiles()" : "* Delete cache files as part of the close call.",
  "org.apache.hadoop.io.ByteWritable$Comparator:<init>()" : null,
  "org.apache.hadoop.fs.shell.PathData:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Creates an object to wrap the given parameters as fields.  The string\n   * used to create the path will be recorded since the Path object does not\n   * return exactly the same string used to initialize it\n   * @param localPath a local URI\n   * @param conf the configuration file\n   * @throws IOException if anything goes wrong...",
  "org.apache.hadoop.util.bloom.BloomFilter:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.FilterFs:getAclStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.shell.Delete$Expunge:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:maximums()" : null,
  "org.apache.hadoop.fs.Options$HandleOpt$Data:allowChange()" : "* Tracks whether any changes to file content are permitted.\n       * @return True if content changes are allowed, false otherwise.",
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : null,
  "org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,java.lang.Object,org.apache.hadoop.io.retry.RetryPolicy)" : "* <p>\n   * Create a proxy for an interface of an implementation class\n   * using the same retry policy for each method in the interface. \n   * </p>\n   * @param iface the interface that the retry will implement\n   * @param implementation the instance whose methods should be retried\n   * @param retryPolicy the policy for retrying method call failures\n   * @param <T> T.\n   * @return the retry proxy",
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:setInputFromSavedData()" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion:isPmem()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockData:getStateString()" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree:getRootDir()" : null,
  "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.net.URI[])" : "* Add nfly link to configuration for the given mount table.\n   *\n   * @param conf configuration.\n   * @param mountTableName mount table.\n   * @param src src.\n   * @param settings settings.\n   * @param targets targets.",
  "org.apache.hadoop.util.ReflectionUtils:setConf(java.lang.Object,org.apache.hadoop.conf.Configuration)" : "* Check and set 'configuration' if necessary.\n   * \n   * @param theObject object for which to set configuration\n   * @param conf Configuration",
  "org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:create(java.lang.String,java.lang.String[])" : "* Create a new SharedFileDescriptorFactory.\n   *\n   * @param prefix       The prefix to prepend to all the file names created\n   *                       by this factory.\n   * @param paths        An array of paths to use.  We will try each path in \n   *                       succession, and return a factory using the first \n   *                       usable path.\n   * @return             The factory.\n   * @throws IOException If a factory could not be created for any reason.",
  "org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.ipc.CallQueueManager:offer(org.apache.hadoop.ipc.Schedulable,long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMaximum(java.lang.String,long)" : null,
  "org.apache.hadoop.service.AbstractService:getConfig()" : null,
  "org.apache.hadoop.conf.Configuration:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)" : "* Get the socket address for <code>hostProperty</code> as a\n   * <code>InetSocketAddress</code>. If <code>hostProperty</code> is\n   * <code>null</code>, <code>addressProperty</code> will be used. This\n   * is useful for cases where we want to differentiate between host\n   * bind address and address clients should use to establish connection.\n   *\n   * @param hostProperty bind host property name.\n   * @param addressProperty address property name.\n   * @param defaultAddressValue the default value\n   * @param defaultPort the default port\n   * @return InetSocketAddress",
  "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:initFromFS()" : "* Initialize from a filesystem.",
  "org.apache.hadoop.tools.TableListing$Builder:build()" : "* Create a new TableListing.\n     *\n     * @return TableListing.",
  "org.apache.hadoop.ha.ZKFailoverController:fenceOldActive(byte[])" : null,
  "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:close()" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Compressor:setInputFromSavedData()" : "* If a write would exceed the capacity of the direct buffers, it is set\n   * aside to be loaded by this function while the compressed data are\n   * consumed.",
  "org.apache.hadoop.fs.DF:parseExecResult(java.io.BufferedReader)" : null,
  "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStop(org.apache.hadoop.util.Daemon)" : null,
  "org.apache.hadoop.conf.ReconfigurationException:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.Throwable)" : "* Create a new instance of {@link ReconfigurationException}.\n   * @param property property name.\n   * @param newVal new value.\n   * @param oldVal old value.\n   * @param cause original exception.",
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])" : "* Decode with inputs and erasedIndexes, generates outputs.\n   * How to prepare for inputs:\n   * 1. Create an array containing data units + parity units. Please note the\n   *    data units should be first or before the parity units.\n   * 2. Set null in the array locations specified via erasedIndexes to indicate\n   *    they're erased and no data are to read from;\n   * 3. Set null in the array locations for extra redundant items, as they're\n   *    not necessary to read when decoding. For example in RS-6-3, if only 1\n   *    unit is really erased, then we have 2 extra items as redundant. They can\n   *    be set as null to indicate no data will be used from them.\n   *\n   * For an example using RS (6, 3), assuming sources (d0, d1, d2, d3, d4, d5)\n   * and parities (p0, p1, p2), d2 being erased. We can and may want to use only\n   * 6 units like (d1, d3, d4, d5, p0, p2) to recover d2. We will have:\n   *     inputs = [null(d0), d1, null(d2), d3, d4, d5, p0, null(p1), p2]\n   *     erasedIndexes = [2] // index of d2 into inputs array\n   *     outputs = [a-writable-buffer]\n   *\n   * Note, for both inputs and outputs, no mixing of on-heap buffers and direct\n   * buffers are allowed.\n   *\n   * If the coder option ALLOW_CHANGE_INPUTS is set true (false by default), the\n   * content of input buffers may change after the call, subject to concrete\n   * implementation.\n   *\n   * @param inputs input buffers to read data from. The buffers' remaining will\n   *               be 0 after decoding\n   * @param erasedIndexes indexes of erased units in the inputs array\n   * @param outputs output buffers to put decoded data into according to\n   *                erasedIndexes, ready for read after the call\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:setReplication(org.apache.hadoop.fs.Path,short)" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:getStringData(java.lang.String)" : "* Get the data in a ZNode.\n   * @param path Path of the ZNode.\n   * @return The data in the ZNode.\n   * @throws Exception If it cannot contact Zookeeper.",
  "org.apache.hadoop.metrics2.lib.MutableGaugeLong:decr(long)" : "* decrement by delta\n   * @param delta of the decrement",
  "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:createBlockGrouper()" : null,
  "org.apache.hadoop.fs.shell.find.FilterExpression:prepare()" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:toString()" : null,
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:extractId(java.lang.String)" : "* Extract the ID from the suffix of the given file name.\n   *\n   * @param file the file name\n   * @return the ID or -1 if no ID could be extracted",
  "org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object,java.lang.String,java.lang.Object[])" : "* <p>Preconditions that the specified argument is not {@code null},\n   * throwing a NPE exception otherwise.\n   *\n   * <p>The message of the exception is {@code String.format(f, m)}.</p>\n   *\n   * @param <T> the object type\n   * @param obj  the object to check\n   * @param message  the {@link String#format(String, Object...)}\n   *                 exception message if valid. Otherwise,\n   *                 the message is {@link #VALIDATE_IS_NOT_NULL_EX_MESSAGE}\n   * @param values  the optional values for the formatted exception message\n   * @return the validated object\n   * @throws NullPointerException if the object is {@code null}\n   * @see #checkNotNull(Object, Supplier)",
  "org.apache.hadoop.fs.impl.WeakReferenceThreadMap:currentThreadId()" : "* Get the current thread ID.\n   * @return thread ID.",
  "org.apache.hadoop.fs.shell.find.Find:buildDescription(org.apache.hadoop.fs.shell.find.ExpressionFactory)" : "Build the description used by the help command.",
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:reset()" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)" : "* The method sends metrics to Ganglia servers. The method has been taken from\n   * org.apache.hadoop.metrics.ganglia.GangliaContext31 with minimal changes in\n   * order to keep it in sync.\n   * @param groupName The group name of the metric\n   * @param name The metric name\n   * @param type The type of the metric\n   * @param value The value of the metric\n   * @param gConf The GangliaConf for this metric\n   * @param gSlope The slope for this metric\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:isFile(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)" : "* Convenience method, so that we don't open a new connection when using this\n   * method from within another method. Otherwise every API invocation incurs\n   * the overhead of opening/closing a TCP connection.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[],int,int)" : "* Move the cursor to the first entry whose key is strictly greater than\n       * the input key. The entry returned by the previous entry() call will be\n       * invalid.\n       * \n       * @param key\n       *          The input key\n       * @param keyOffset\n       *          offset in the key buffer.\n       * @param keyLen\n       *          key buffer length.\n       * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.ha.SshFenceByTcpPort:createSession(java.lang.String,org.apache.hadoop.ha.SshFenceByTcpPort$Args)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(byte[])" : "* Copy the key into user supplied buffer.\n         * \n         * @param buf\n         *          The buffer supplied by user. The length of the buffer must\n         *          not be shorter than the key length.\n         * @return The length of the key.\n         * \n         * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.ChunkedArrayList:get(int)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem$FsOperation:run(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)" : null,
  "org.apache.hadoop.security.SecurityUtil:setTokenService(org.apache.hadoop.security.token.Token,java.net.InetSocketAddress)" : "* Set the given token's service to the format expected by the RPC client \n   * @param token a delegation token\n   * @param addr the socket for the rpc connection",
  "org.apache.hadoop.ipc.CallQueueManager:iterator()" : null,
  "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:newAttrInfo(java.lang.String,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:append(java.lang.StringBuilder,java.lang.String,java.lang.Object[])" : null,
  "org.apache.hadoop.ha.StreamPumper:pump()" : null,
  "org.apache.hadoop.ha.ZKFailoverController$ActiveAttemptRecord:<init>(boolean,java.lang.String)" : null,
  "org.apache.hadoop.fs.Globber$GlobBuilder:build()" : "* Build the Globber.\n     * @return a new instance.",
  "org.apache.hadoop.ipc.Server$Call:isCallCoordinated()" : null,
  "org.apache.hadoop.io.IOUtils:writeFully(java.nio.channels.FileChannel,java.nio.ByteBuffer,long)" : "* Write a ByteBuffer to a FileChannel at a given offset, \n   * handling short writes.\n   * \n   * @param fc               The FileChannel to write to\n   * @param buf              The input buffer\n   * @param offset           The offset in the file to start writing at\n   * @throws IOException     On I/O error",
  "org.apache.hadoop.security.http.RestCsrfPreventionFilter:init(javax.servlet.FilterConfig)" : null,
  "org.apache.hadoop.util.WeakReferenceMap:create(java.lang.Object)" : "* Create a new instance under a key.\n   * <p>\n   * The instance is created, added to the map and then the\n   * map value retrieved.\n   * This ensures that the reference returned is that in the map,\n   * even if there is more than one entry being created at the same time.\n   * If that race does occur, it will be logged the first time it happens\n   * for this specific map instance.\n   * <p>\n   * HADOOP-18456 highlighted the risk of a concurrent GC resulting a null\n   * value being retrieved and so returned.\n   * To prevent this:\n   * <ol>\n   *   <li>A strong reference is retained to the newly created instance\n   *       in a local variable.</li>\n   *   <li>That variable is used after the resolution process, to ensure\n   *       the JVM doesn't consider it \"unreachable\" and so eligible for GC.</li>\n   *   <li>A check is made for the resolved reference being null, and if so,\n   *       the put() is repeated</li>\n   * </ol>\n   * @param key key\n   * @return the created value",
  "org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization:accept(java.lang.Class)" : null,
  "org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem)" : "* Create a FileContext for specified file system using the default config.\n   * \n   * @param defaultFS default fs.\n   * @return a FileContext with the specified AbstractFileSystem\n   *                 as the default FS.",
  "org.apache.hadoop.util.DataChecksum:newCrc32()" : "* Create a Crc32 Checksum object. The implementation of the Crc32 algorithm\n   * is chosen depending on the platform.\n   *\n   * @return Checksum.",
  "org.apache.hadoop.io.file.tfile.ByteArray:<init>(byte[])" : "* Wrap a whole byte array as a RawComparable.\n   * \n   * @param buffer\n   *          the byte array buffer.",
  "org.apache.hadoop.fs.DelegateToFileSystem:truncate(org.apache.hadoop.fs.Path,long)" : null,
  "org.apache.hadoop.fs.shell.CommandFormat:parse(java.lang.String[],int)" : "Parse parameters starting from the given position\n   * Consider using the variant that directly takes a List\n   * \n   * @param args an array of input arguments\n   * @param pos the position at which starts to parse\n   * @return a list of parameters",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:reset()" : "* Reset all statistics.",
  "org.apache.hadoop.conf.Configuration:getPropsWithPrefix(java.lang.String)" : "* Constructs a mapping of configuration and includes all properties that\n   * start with the specified configuration prefix.  Property names in the\n   * mapping are trimmed to remove the configuration prefix.\n   *\n   * @param confPrefix configuration prefix\n   * @return mapping of configuration properties with prefix stripped",
  "org.apache.hadoop.io.SequenceFile$Reader:getKeyClass()" : "@return Returns the class of keys in this file.",
  "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getOverflowedCalls()" : null,
  "org.apache.hadoop.fs.impl.CombinedFileRange:<init>(long,long,org.apache.hadoop.fs.FileRange)" : null,
  "org.apache.hadoop.fs.http.HttpsFileSystem:getScheme()" : null,
  "org.apache.hadoop.fs.FileUtil:canRead(java.io.File)" : "* Platform independent implementation for {@link File#canRead()}\n   * @param f input file\n   * @return On Unix, same as {@link File#canRead()}\n   *         On Windows, true if process has read access on the path",
  "org.apache.hadoop.conf.Configuration:getInstances(java.lang.String,java.lang.Class)" : "* Get the value of the <code>name</code> property as a <code>List</code>\n   * of objects implementing the interface specified by <code>xface</code>.\n   * \n   * An exception is thrown if any of the classes does not exist, or if it does\n   * not implement the named interface.\n   * \n   * @param name the property name.\n   * @param xface the interface implemented by the classes named by\n   *        <code>name</code>.\n   * @param <U> Interface class type.\n   * @return a <code>List</code> of objects implementing <code>xface</code>.",
  "org.apache.hadoop.fs.BBUploadHandle:<init>(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.util.ComparableVersion$IntegerItem:<init>()" : null,
  "org.apache.hadoop.fs.FileContext:getDelegationTokens(org.apache.hadoop.fs.Path,java.lang.String)" : "* Get delegation tokens for the file systems accessed for a given\n   * path.\n   * @param p Path for which delegations tokens are requested.\n   * @param renewer the account name that is allowed to renew the token.\n   * @return List of delegation tokens.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.util.functional.TaskPool:<init>()" : null,
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_fromJsonString(java.lang.String)" : "* Load IOStatisticsSnapshot from a JSON string.\n   * @param json JSON string value.\n   * @return deserialized snapshot.\n   * @throws UncheckedIOException Any IO/jackson exception.",
  "org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:getIter()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenRenewInterval()" : "* Interval for tokens to be renewed.\n   * @return Renew interval in milliseconds.",
  "org.apache.hadoop.fs.shell.TouchCommands$Touch:touch(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.util.SysInfoLinux:getNumVCoresUsed()" : "{@inheritDoc}",
  "org.apache.hadoop.util.KMSUtil:parseJSONEncKeyVersion(java.lang.String,java.util.Map)" : null,
  "org.apache.hadoop.crypto.key.KeyShell$RollCommand:getUsage()" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:createLockNodeAsync()" : null,
  "org.apache.hadoop.metrics2.util.MetricsCache:get(java.lang.String,java.util.Collection)" : "* Get the cached record\n   * @param name of the record\n   * @param tags of the record\n   * @return the cached record or null",
  "org.apache.hadoop.fs.AbstractFileSystem:getXAttrs(org.apache.hadoop.fs.Path)" : "* Get all of the xattrs for a file or directory.\n   * Only those xattrs for which the logged-in user has permissions to view\n   * are returned.\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to get extended attributes\n   *\n   * @return {@literal Map<String, byte[]>} describing the XAttrs of the file\n   * or directory\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$WaitableMetricsBuffer:notifyAnyWaiters()" : null,
  "org.apache.hadoop.metrics2.impl.AbstractMetricsRecord:toString()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.ipc.Server:getPriorityLevel()" : "* @return Return the priority level assigned by call queue to an RPC\n   * Returns 0 in case no priority is assigned.",
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:executeTrailerState()" : "* Parse the gzip trailer (assuming we're in the appropriate state).\n   * In order to deal with degenerate cases (e.g., user buffer is one byte\n   * long), we copy trailer bytes (all 8 of 'em) to a local buffer.</p>\n   *\n   * See http://www.ietf.org/rfc/rfc1952.txt for the gzip spec.",
  "org.apache.hadoop.http.HttpServer2$Builder:setXFrameOption(java.lang.String)" : "* Sets a valid X-Frame-option that can be used by HttpServer2.\n     * @param option - String DENY, SAMEORIGIN or ALLOW-FROM are the only valid\n     *               options. Any other value will throw IllegalArgument\n     *               Exception.\n     * @return  Builder.",
  "org.apache.hadoop.fs.FileSystem:getStatistics()" : "* Get the Map of Statistics object indexed by URI Scheme.\n   * @return a Map having a key as URI scheme and value as Statistics object\n   * @deprecated use {@link #getGlobalStorageStatistics()}",
  "org.apache.hadoop.ipc.DecayRpcScheduler:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)" : null,
  "org.apache.hadoop.fs.shell.find.FilterExpression:<init>(org.apache.hadoop.fs.shell.find.Expression)" : null,
  "org.apache.hadoop.service.AbstractService:stop()" : "* {@inheritDoc}",
  "org.apache.hadoop.fs.permission.AclEntry:aclSpecToString(java.util.List)" : "* Convert a List of AclEntries into a string - the reverse of parseAclSpec.\n   * @param aclSpec List of AclEntries to convert\n   * @return String representation of aclSpec",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getParameters()" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:getRawSize()" : null,
  "org.apache.hadoop.ipc.CallQueueManager:shouldBackOff(org.apache.hadoop.ipc.Schedulable)" : null,
  "org.apache.hadoop.service.AbstractService:getFailureCause()" : null,
  "org.apache.hadoop.util.DurationInfo:getFormattedText()" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$CallInfo:getMethodName()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:lowerBound(org.apache.hadoop.io.file.tfile.RawComparable)" : "* @param key\n     *          input key.\n     * @return the ID of the first block that contains key >= input key. Or -1\n     *         if no such block exists.",
  "org.apache.hadoop.fs.HarFileSystem$HarStatus:getModificationTime()" : null,
  "org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket)" : "* Same as <code>getInputStream(socket, socket.getSoTimeout()).</code>\n   *\n   * @param socket socket.\n   * @throws IOException raised on errors performing I/O.\n   * @return SocketInputWrapper for reading from the socket.\n   * @see #getInputStream(Socket, long)",
  "org.apache.hadoop.ha.HAAdmin:getServiceState(org.apache.commons.cli.CommandLine)" : null,
  "org.apache.hadoop.io.ObjectWritable$NullInstance:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.ha.HAAdmin:confirmForceManual()" : null,
  "org.apache.hadoop.util.Sets:<init>()" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getCanonicalServiceName()" : null,
  "org.apache.hadoop.util.functional.TaskPool$Builder:<init>(org.apache.hadoop.fs.RemoteIterator)" : "* Create the builder.\n     * @param items items to process",
  "org.apache.hadoop.fs.viewfs.ViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.UTF8:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:fullPath(org.apache.hadoop.fs.Path)" : "* @param path\n   * @return  full path including the chroot",
  "org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.security.UserGroupInformation:getAuthenticationMethod()" : "* Get the authentication method from the subject\n   * \n   * @return AuthenticationMethod in the subject, null if not present.",
  "org.apache.hadoop.fs.FileSystem:getServerDefaults(org.apache.hadoop.fs.Path)" : "* Return a set of server default configuration values.\n   * @param p path is used to identify an FS since an FS could have\n   *          another FS that it could be delegating the call to\n   * @return server default configuration values\n   * @throws IOException IO failure",
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:values()" : "* Evaluate all the entries and provide a list of the results.\n   *\n   * This is not a snapshot, so if the evaluators actually return\n   * references to mutable objects (e.g. a MeanStatistic instance)\n   * then that value may still change.\n   * @return the current list of evaluated results.",
  "org.apache.hadoop.fs.shell.Ls:isDirRecurse()" : "* Should the contents of the directory be shown or just the directory?\n   * @return true if directory contents, false if just directory",
  "org.apache.hadoop.metrics2.lib.MutableRollingAverages:<init>(java.lang.String)" : "* Constructor for {@link MutableRollingAverages}.\n   * @param metricValueName input metricValueName.",
  "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)" : "We search through all the configured dirs for the file's existence\n     *  and return true when we find one",
  "org.apache.hadoop.security.ProviderUtils:unnestUri(java.net.URI)" : "* Convert a nested URI to decode the underlying path. The translation takes\n   * the authority and parses it into the underlying scheme and authority.\n   * For example, \"myscheme://hdfs@nn/my/path\" is converted to\n   * \"hdfs://nn/my/path\".\n   * @param nestedUri the URI from the nested URI\n   * @return the unnested path",
  "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$3:suffix()" : null,
  "org.apache.hadoop.fs.Options$HandleOpt:reference()" : "* Handle is valid iff the referent exists in the namespace.\n     * Equivalent to changed(true), moved(true).\n     * @return Options requiring that the implementation resolve a reference\n     * to this entity regardless of changes to content or location.",
  "org.apache.hadoop.fs.FSOutputSummer:convertToByteStream(java.util.zip.Checksum,int)" : "* Converts a checksum integer value to a byte stream\n   *\n   * @param sum check sum.\n   * @param checksumSize check sum size.\n   * @return byte stream.",
  "org.apache.hadoop.fs.shell.CommandFactory:<init>()" : "Factory constructor for commands",
  "org.apache.hadoop.fs.FilterFs:getLinkTarget(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:innerClose()" : "* Closing the block will release the buffer.",
  "org.apache.hadoop.util.IntrusiveCollection:retainAll(java.util.Collection)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:hasCapability(java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:<init>()" : null,
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:maximums()" : null,
  "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:createEncryptor()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsR(long)" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:getFsAction(int,org.apache.commons.net.ftp.FTPFile)" : null,
  "org.apache.hadoop.conf.Configuration:getHexDigits(java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:reset()" : null,
  "org.apache.hadoop.ipc.RPC$Builder:setInstance(java.lang.Object)" : "* @return Mandatory field.\n     * @param instance input instance.",
  "org.apache.hadoop.fs.PathIOException:setTargetPath(java.lang.String)" : "* Optional path if the exception involved two paths, ex. a copy operation\n   * @param targetPath the of the operation",
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_aggregate(java.lang.Object)" : "* Aggregate into the IOStatistics context the statistics passed in via\n   * IOStatistics/source parameter.\n   * <p>\n   * Returns false if the source is null or does not contain any statistics.\n   * @param source implementation of {@link IOStatisticsSource} or {@link IOStatistics}\n   * @return true if the the source object was aggregated.",
  "org.apache.hadoop.ipc.Client:getRpcTimeout(org.apache.hadoop.conf.Configuration)" : "* The time after which a RPC will timeout.\n   *\n   * @param conf Configuration\n   * @return the timeout period in milliseconds.",
  "org.apache.hadoop.util.SysInfoLinux:getCumulativeCpuTime()" : "{@inheritDoc}",
  "org.apache.hadoop.conf.StorageUnit$1:toMBs(double)" : null,
  "org.apache.hadoop.ha.HealthCheckFailedException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.security.NetgroupCache:clear()" : "* Clear the cache",
  "org.apache.hadoop.ipc.CallQueueManager:createCallQueueInstance(java.lang.Class,int,int,java.lang.String,int[],org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File,boolean)" : "* Delete a directory and all its contents.  If\n   * we return false, the directory may be partially-deleted.\n   * (1) If dir is symlink to a file, the symlink is deleted. The file pointed\n   *     to by the symlink is not deleted.\n   * (2) If dir is symlink to a directory, symlink is deleted. The directory\n   *     pointed to by symlink is not deleted.\n   * (3) If dir is a normal file, it is deleted.\n   * (4) If dir is a normal directory, then dir and all its contents recursively\n   *     are deleted.\n   * @param dir the file or directory to be deleted\n   * @param tryGrantPermissions true if permissions should be modified to delete a file.\n   * @return true on success false on failure.",
  "org.apache.hadoop.fs.impl.FileRangeImpl:getData()" : null,
  "org.apache.hadoop.fs.QuotaUsage:setQuota(long)" : null,
  "org.apache.hadoop.io.VIntWritable:<init>(int)" : null,
  "org.apache.hadoop.security.SecurityUtil$StandardHostResolver:getByName(java.lang.String)" : null,
  "org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)" : "* Construct a client-side proxy object that implements the named protocol,\n   * talking to a server at the named address. \n   *\n   * @param <T> Generics Type T.\n   * @param protocol input protocol.\n   * @param clientVersion input clientVersion.\n   * @param addr input addr.\n   * @param ticket input tocket.\n   * @param conf input conf.\n   * @param factory input factory.\n   * @return the protocol proxy.\n   * @throws IOException raised on errors performing I/O.\n   *",
  "org.apache.hadoop.io.SortedMapWritable:firstKey()" : null,
  "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)" : "* Write lines of text to a file. Each line is a char sequence and is written\n   * to the file in sequence with each line terminated by the platform's line\n   * separator, as defined by the system property {@code\n   * line.separator}. Characters are encoded into bytes using the specified\n   * charset. This utility method opens the file for writing, creating the file\n   * if it does not exist, or overwrites an existing file.\n   *\n   * @param fs the file system with which to create the file\n   * @param path the path to the file\n   * @param lines a Collection to iterate over the char sequences\n   * @param cs the charset to use for encoding\n   *\n   * @return the file system\n   *\n   * @throws NullPointerException if any of the arguments are {@code null}\n   * @throws IOException if an I/O error occurs creating or writing to the file",
  "org.apache.hadoop.service.launcher.ServiceLauncher:<init>(java.lang.String)" : "* Create an instance of the launcher.\n   * @param serviceClassName classname of the service",
  "org.apache.hadoop.util.dynamic.BindingUtils:loadClassSafely(java.lang.String)" : "* Load a class by name.\n   * @param className classname\n   * @return the class.\n   * @throws RuntimeException if the class was not found.",
  "org.apache.hadoop.crypto.key.KeyShell:getCommandUsage()" : null,
  "org.apache.hadoop.service.ServiceStateModel:isValidStateTransition(org.apache.hadoop.service.Service$STATE,org.apache.hadoop.service.Service$STATE)" : "* Is a state transition valid?\n   * There are no checks for current==proposed\n   * as that is considered a non-transition.\n   *\n   * using an array kills off all branch misprediction costs, at the expense\n   * of cache line misses.\n   *\n   * @param current current state\n   * @param proposed proposed new state\n   * @return true if the transition to a new state is valid",
  "org.apache.hadoop.metrics2.lib.MutableMetric:setChanged()" : "* Set the changed flag in mutable operations",
  "org.apache.hadoop.util.PriorityQueue:adjustTop()" : "Should be called when the Object at top changes values.  Still log(n)\n   * worst case, but it's at least twice as fast to <pre>\n   *  { pq.top().change(); pq.adjustTop(); }\n   * </pre> instead of <pre>\n   *  { o = pq.pop(); o.change(); pq.push(o); }\n   * </pre>",
  "org.apache.hadoop.crypto.JceCtrCryptoCodec:generateSecureRandom(byte[])" : null,
  "org.apache.hadoop.fs.GlobFilter:init(java.lang.String,org.apache.hadoop.fs.PathFilter)" : null,
  "org.apache.hadoop.io.compress.ZStandardCodec:getDefaultExtension()" : "* Get the default filename extension for this kind of compression.\n   *\n   * @return <code>.zst</code>.",
  "org.apache.hadoop.fs.FileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)" : "* This create has been added to support the FileContext that processes\n   * the permission with umask before calling this method.\n   * This a temporary method added to support the transition from FileSystem\n   * to FileContext for user applications.\n   *\n   * @param f path.\n   * @param absolutePermission permission.\n   * @param flag create flag.\n   * @param bufferSize buffer size.\n   * @param replication replication.\n   * @param blockSize block size.\n   * @param progress progress.\n   * @param checksumOpt check sum opt.\n   * @return output stream.\n   * @throws IOException IO failure",
  "org.apache.hadoop.security.NetgroupCache:add(java.lang.String,java.util.List)" : "* Add group to cache\n   *\n   * @param group name of the group to add to cache\n   * @param users list of users for a given group",
  "org.apache.hadoop.crypto.key.KeyProvider$Options:getAttributes()" : null,
  "org.apache.hadoop.io.GenericWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getResponseTimeCountInLastWindow()" : null,
  "org.apache.hadoop.conf.Configuration:getProps()" : null,
  "org.apache.hadoop.metrics2.source.JvmMetrics:<init>(java.lang.String,java.lang.String,boolean)" : null,
  "org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRead(java.io.File,java.lang.String,java.lang.String)" : "* @return Same as openForRead() except that it will run even if security is off.\n   * This is used by unit tests.\n   * @param f input f.\n   * @param expectedOwner input expectedOwner.\n   * @param expectedGroup input expectedGroup.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invoke(java.lang.Object,java.lang.Object[])" : null,
  "org.apache.hadoop.util.GenericOptionsParser:expandWildcard(java.util.List,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)" : null,
  "org.apache.hadoop.io.wrappedio.WrappedIO:streamCapabilities_hasCapability(java.lang.Object,java.lang.String)" : "* Does an object implement {@link StreamCapabilities} and, if so,\n   * what is the result of the probe for the capability?\n   * Calls {@link StreamCapabilities#hasCapability(String)},\n   * @param object object to probe\n   * @param capability capability string\n   * @return true iff the object implements StreamCapabilities and the capability is\n   * declared available.",
  "org.apache.hadoop.fs.FileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : "* Create a snapshot.\n   * @param path The directory where snapshots will be taken.\n   * @param snapshotName The name of the snapshot\n   * @return the snapshot path.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported",
  "org.apache.hadoop.util.DiskChecker:replaceFileOutputStreamProvider(org.apache.hadoop.util.DiskChecker$FileIoProvider)" : "* Replace the {@link FileIoProvider} for tests.\n   * This method MUST NOT be used outside of unit tests.\n   *\n   * @param newFosProvider\n   * @return the old FileIoProvider.",
  "org.apache.hadoop.conf.Configuration$ParsedItem:<init>(java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String[])" : null,
  "org.apache.hadoop.util.ExitUtil$ExitException:toString()" : "* String value does not include exception type, just exit code and message.\n     * @return the exit code and any message",
  "org.apache.hadoop.fs.http.HttpsFileSystem:getUri()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:logExpireToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : null,
  "org.apache.hadoop.security.ssl.SSLFactory:configure(java.net.HttpURLConnection)" : "* If the given {@link HttpURLConnection} is an {@link HttpsURLConnection}\n   * configures the connection with the {@link SSLSocketFactory} and\n   * {@link HostnameVerifier} of this SSLFactory, otherwise does nothing.\n   *\n   * @param conn the {@link HttpURLConnection} instance to configure.\n   * @return the configured {@link HttpURLConnection} instance.\n   *\n   * @throws IOException if an IO error occurred.",
  "org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Convenience method, so that we don't open a new connection when using this\n   * method from within another method. Otherwise every API invocation incurs\n   * the overhead of opening/closing a TCP connection.\n   * \n   * @param client\n   * @param src\n   * @param dst\n   * @return\n   * @throws IOException",
  "org.apache.hadoop.io.compress.BZip2Codec:<init>()" : "* Creates a new instance of BZip2Codec.",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream)" : null,
  "org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:getCanonicalUser(java.lang.Class)" : null,
  "org.apache.hadoop.fs.StreamCapabilitiesPolicy:unbuffer(java.io.InputStream)" : "* Implement the policy for {@link CanUnbuffer#unbuffer()}.\n   *\n   * @param in the input stream",
  "org.apache.hadoop.net.TableMapping:getRawMapping()" : null,
  "org.apache.hadoop.service.ServiceStateModel:<init>(java.lang.String,org.apache.hadoop.service.Service$STATE)" : "* Create a service state model instance in the chosen state\n   * @param state the starting state\n   * @param name input name.",
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:finish()" : null,
  "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.Server$Listener:getAddress()" : null,
  "org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.commons.cli.Options,java.lang.String[])" : "* Create an options parser with the given options to parse the args.\n   * @param opts the options\n   * @param args the command line arguments\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:<init>()" : "* Private constructor for a utility class to be used in IOStatisticsContext.",
  "org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder(java.lang.Object)" : null,
  "org.apache.hadoop.util.ZKUtil:parseACLs(java.lang.String)" : "* Parse comma separated list of ACL entries to secure generated nodes, e.g.\n   * <code>sasl:hdfs/host1@MY.DOMAIN:cdrwa,sasl:hdfs/host2@MY.DOMAIN:cdrwa</code>\n   *\n   * @param aclString aclString.\n   * @return ACL list\n   * @throws BadAclFormatException if an ACL is invalid",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE,boolean)" : null,
  "org.apache.hadoop.fs.FileSystem:getAllStatistics()" : "* Return the FileSystem classes that have Statistics.\n   * @deprecated use {@link #getGlobalStorageStatistics()}\n   * @return statistics lists.",
  "org.apache.hadoop.io.ByteWritable:<init>()" : null,
  "org.apache.hadoop.crypto.UnsupportedCodecException:<init>(java.lang.String)" : "* Constructs an UnsupportedCodecException with the specified\n   * detail message.\n   * \n   * @param message the detail message",
  "org.apache.hadoop.ipc.Server:bind(java.net.ServerSocket,java.net.InetSocketAddress,int)" : "* A convenience method to bind to a given address and report \n   * better exceptions if the address is not a valid host.\n   * @param socket the socket to bind\n   * @param address the address to bind to\n   * @param backlog the number of connections allowed in the queue\n   * @throws BindException if the address can't be bound\n   * @throws UnknownHostException if the address isn't a valid host name\n   * @throws IOException other random errors from bind",
  "org.apache.hadoop.fs.FsShell:<init>(org.apache.hadoop.conf.Configuration)" : "* Construct a FsShell with the given configuration.  Commands can be\n   * executed via {@link #run(String[])}\n   * @param conf the hadoop configuration",
  "org.apache.hadoop.util.DirectBufferPool:countBuffersOfSize(int)" : "* Return the number of available buffers of a given size.\n   * This is used only for tests.",
  "org.apache.hadoop.fs.viewfs.NflyFSystem:getNflyTmpPath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForMethod(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.annotation.Metric)" : "* Override to handle custom mutable metrics for methods\n   * @param source the metrics source object\n   * @param method to return the metric\n   * @param annotation of the method\n   * @return a new metric object or null",
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getReplication()" : null,
  "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:addMetricIfNotExists(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.lib.MethodMetric$3:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)" : null,
  "org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.conf.Configuration)" : "* This function is kept to provide backward compatibility.\n   * @param user user.\n   * @param remoteAddress remote address.\n   * @param conf configuration.\n   * @throws AuthorizationException Authorization Exception.\n   * @deprecated use {@link #authorize(UserGroupInformation, String)} instead.",
  "org.apache.hadoop.ha.ActiveStandbyElector:getDataWithRetries(java.lang.String,boolean,org.apache.zookeeper.data.Stat)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:getLargeReadOps()" : "* Get the number of large file system read operations such as list files\n     * under a large directory.\n     * @return number of large read operations",
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,int)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:toString()" : null,
  "org.apache.hadoop.net.ScriptBasedMapping:getConf()" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:dispatchedActiveMax()" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:checkBytes(java.nio.ByteBuffer,long,java.nio.ByteBuffer,long,int,org.apache.hadoop.fs.Path)" : "* Check the data against the checksums.\n     * @param sumsBytes the checksum data\n     * @param sumsOffset where from the checksum file this buffer started\n     * @param data the file data\n     * @param dataOffset where the file data started (must be a multiple of\n     *                  bytesPerSum)\n     * @param bytesPerSum how many bytes per a checksum\n     * @param file the path of the filename\n     * @return the data buffer\n     * @throws CompletionException if the checksums don't match",
  "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:loadTrustManager(java.nio.file.Path)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)" : null,
  "org.apache.hadoop.log.LogLevel$CLI:parseProtocolArgs(java.lang.String[],int)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:getCoderName()" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:decompressDirectBuf()" : null,
  "org.apache.hadoop.security.LdapGroupsMapping:lookupGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext,int)" : "* Perform the second query to get the groups of the user.\n   *\n   * If posixGroups is enabled, use use posix gid/uid to find.\n   * Otherwise, use the general group member attribute to find it.\n   *\n   * @param result the result object returned from the prior user lookup.\n   * @param c the context object of the LDAP connection.\n   * @return a list of strings representing group names of the user.\n   * @throws NamingException if unable to find group names",
  "org.apache.hadoop.security.alias.KeyStoreProvider:stashOriginalFilePermissions()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,long)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:<init>()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:getCoderName()" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:addAvgResponseTimePerPriority(org.apache.hadoop.metrics2.MetricsRecordBuilder)" : null,
  "org.apache.hadoop.io.FastByteComparisons:compareTo(byte[],int,int,byte[],int,int)" : "* Lexicographically compare two byte arrays.",
  "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:setColumnHide(int,boolean)" : "* Hide the given column index",
  "org.apache.hadoop.conf.Configuration:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$MountPoint:<init>(org.apache.hadoop.fs.Path,java.lang.String[])" : null,
  "org.apache.hadoop.http.ProfileServlet$Event:getInternalName()" : null,
  "org.apache.hadoop.http.HttpServer2:getWebAppContext()" : null,
  "org.apache.hadoop.fs.FileSystem:createDataInputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)" : "* Create instance of the standard {@link FSDataInputStreamBuilder} for the\n   * given filesystem and path handle.\n   * @param fileSystem owner\n   * @param pathHandle path handle of file to open.\n   * @return a builder.",
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:remove(java.lang.Object)" : null,
  "org.apache.hadoop.http.HttpServer2:userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)" : "* Get the admin ACLs from the given ServletContext and check if the given\n   * user is in the ACL.\n   *\n   * @param servletContext the context containing the admin ACL.\n   * @param remoteUser the remote user to check for.\n   * @return true if the user is present in the ACL, false if no ACL is set or\n   *         the user is not present",
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesRead()" : null,
  "org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.slf4j.Logger,java.lang.String,long)" : "* Log the current thread stacks at INFO level.\n   * @param log the logger that logs the stack trace\n   * @param title a descriptive title for the call stacks\n   * @param minInterval the minimum time from the last",
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:getTargetFileSystem()" : "* Get the instance of FileSystem to use, creating one if needed.\n     * @return An Initialized instance of T\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.DiskChecker:doDiskIo(java.io.File)" : "* Performs some disk IO by writing to a new file in the given directory\n   * and sync'ing file contents to disk.\n   *\n   * This increases the likelihood of catching catastrophic disk/controller\n   * failures sooner.\n   *\n   * @param dir directory to be checked.\n   * @throws DiskErrorException if we hit an error while trying to perform\n   *         disk IO against the file.",
  "org.apache.hadoop.security.KDiag:usage()" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getCurator()" : null,
  "org.apache.hadoop.io.FloatWritable:toString()" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:processBasicHeader()" : null,
  "org.apache.hadoop.fs.store.EtagChecksum:<init>(java.lang.String)" : "* Create with a string etag.\n   * @param eTag etag",
  "org.apache.hadoop.util.Lists:cast(java.lang.Iterable)" : null,
  "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:createDecryptor()" : null,
  "org.apache.hadoop.io.WritableUtils:readStringArray(java.io.DataInput)" : null,
  "org.apache.hadoop.crypto.OpensslCipher$Padding:get(java.lang.String)" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:reset()" : "* reset is not implemented",
  "org.apache.hadoop.conf.Configuration:loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)" : null,
  "org.apache.hadoop.fs.DF:getMount()" : "* @return the filesystem mount point for the indicated volume.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getAuthHandler()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:equals(java.lang.Object)" : null,
  "org.apache.hadoop.ipc.RpcWritable$WritableWrapper:readFrom(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.security.SaslInputStream:readMoreData()" : "* Read more data and get them processed <br>\n   * Entry condition: ostart = ofinish <br>\n   * Exit condition: ostart <= ofinish <br>\n   * \n   * return (ofinish-ostart) (we have this many bytes for you), 0 (no data now,\n   * but could have more later), or -1 (absolutely no more data)",
  "org.apache.hadoop.fs.shell.find.Find:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.util.FindClass:loadClass(java.lang.String)" : "* Loads the class of the given name\n   * @param name classname\n   * @return outcome code",
  "org.apache.hadoop.security.authorize.AccessControlList:isUserInList(org.apache.hadoop.security.UserGroupInformation)" : "* Checks if a user represented by the provided {@link UserGroupInformation}\n   * is a member of the Access Control List. If user was proxied and\n   * USE_REAL_ACLS + the real user name is in the control list, then treat this\n   * case as if user were in the ACL list.\n   * @param ugi UserGroupInformation to check if contained in the ACL\n   * @return true if ugi is member of the list or if USE_REAL_ACLS + real user\n   * is in the list",
  "org.apache.hadoop.ipc.Client$Connection:updateAddress()" : "* Update the server address if the address corresponding to the host\n     * name has changed.\n     *\n     * @return true if an addr change was detected.\n     * @throws IOException when the hostname cannot be resolved.",
  "org.apache.hadoop.fs.FileSystemStorageStatistics:getLong(java.lang.String)" : null,
  "org.apache.hadoop.security.UserGroupInformation:getTGT()" : "* Get the Kerberos TGT\n   * @return the user's TGT or null if none was found",
  "org.apache.hadoop.security.token.Token:privateClone(org.apache.hadoop.io.Text)" : "* Create a private clone of a public token.\n   * @param newService the new service name\n   * @return a private token",
  "org.apache.hadoop.ipc.Client:setCallIdAndRetryCountUnprotected(java.lang.Integer,int,java.lang.Object)" : null,
  "org.apache.hadoop.ipc.Client:close()" : null,
  "org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.security.LdapGroupsMapping:getPasswordForBindUser(java.lang.String)" : null,
  "org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(org.apache.hadoop.ipc.VersionedProtocol,java.lang.String,long,int)" : "* Get a server protocol's signature\n   *\n   * @param server server implementation\n   * @param protocol server protocol\n   * @param clientVersion client's version\n   * @param clientMethodsHash client's protocol's hash code\n   * @return the server protocol's signature\n   * @throws IOException if any error occurs",
  "org.apache.hadoop.net.StandardSocketFactory:hashCode()" : null,
  "org.apache.hadoop.metrics2.AbstractMetric:hashCode()" : null,
  "org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcResponseTime(long)" : null,
  "org.apache.hadoop.security.token.Token:copyToken()" : null,
  "org.apache.hadoop.util.Shell:getHadoopHome()" : "* Get the Hadoop home directory. Raises an exception if not found\n   * @return the home dir\n   * @throws IOException if the home directory cannot be located.",
  "org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission:<init>(short)" : null,
  "org.apache.hadoop.fs.FsShell:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)" : "* Launch a service catching all exceptions and downgrading them to exit codes\n   * after logging.\n   *\n   * Sets {@link #serviceException} to this value.\n   * @param conf configuration to use\n   * @param processedArgs command line after the launcher-specific arguments\n   * have been stripped out.\n   * @param addShutdownHook should a shutdown hook be added to terminate\n   * this service on shutdown. Tests should set this to false.\n   * @param execute execute/wait for the service to stop.\n   * @return an exit exception, which will have a status code of 0 if it worked",
  "org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.lang.String)" : "* This function gets the instance based on the config.\n   *\n   * @param conf Configuration\n   * @param configKey config key name.\n   * @return Domain name resolver.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getEnclosingRoot(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ha.HAServiceProtocolHelper:monitorHealth(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)" : null,
  "org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,org.apache.hadoop.io.retry.FailoverProxyProvider,java.util.Map,org.apache.hadoop.io.retry.RetryPolicy)" : "* Create a proxy for an interface of implementations of that interface using\n   * the given {@link FailoverProxyProvider} and the a set of retry policies\n   * specified by method name. If no retry policy is defined for a method then a\n   * default of {@link RetryPolicies#TRY_ONCE_THEN_FAIL} is used.\n   * \n   * @param iface the interface that the retry will implement\n   * @param proxyProvider provides implementation instances whose methods should be retried\n   * @param methodNameToPolicyMap map of method names to retry policies\n   * @param defaultPolicy defaultPolicy.\n   * @param <T> T.\n   * @return the retry proxy",
  "org.apache.hadoop.service.ServiceStateModel:enterState(org.apache.hadoop.service.Service$STATE)" : "* Enter a state -thread safe.\n   *\n   * @param proposed proposed new state\n   * @return the original state\n   * @throws ServiceStateException if the transition is not permitted",
  "org.apache.hadoop.fs.FileSystem:setDefaultUri(org.apache.hadoop.conf.Configuration,java.lang.String)" : "Set the default FileSystem URI in a configuration.\n   * @param conf the configuration to alter\n   * @param uri the new default filesystem uri",
  "org.apache.hadoop.ipc.Server$Listener:closeCurrentConnection(java.nio.channels.SelectionKey,java.lang.Throwable)" : null,
  "org.apache.hadoop.fs.LocalDirAllocator:getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Get all of the paths that currently exist in the working directories.\n   * @param pathStr the path underneath the roots\n   * @param conf the configuration to look up the roots in\n   * @return all of the paths that exist under any of the roots\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.bloom.Filter:add(java.util.List)" : "* Adds a list of keys to <i>this</i> filter.\n   * @param keys The list of keys.",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory:createProviders(org.apache.hadoop.conf.Configuration,java.net.URL,int,java.lang.String)" : null,
  "org.apache.hadoop.security.SaslRpcClient:getServerToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)" : "* Try to locate the required token for the server.\n   * \n   * @param authType of the SASL client\n   * @return Token for server, or null if no token available\n   * @throws IOException - token selector cannot be instantiated",
  "org.apache.hadoop.ipc.RPC$Server:registerProtocolAndImpl(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:init()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.FsUsage$Df:processArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:remove()" : null,
  "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:init(org.apache.hadoop.security.ssl.SSLFactory$Mode)" : "* Initializes the keystores of the factory.\n   *\n   * @param mode if the keystores are to be used in client or server mode.\n   * @throws IOException thrown if the keystores could not be initialized due\n   * to an IO error.\n   * @throws GeneralSecurityException thrown if the keystores could not be\n   * initialized due to a security error.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.fs.statistics.impl.StubDurationTracker:failed()" : null,
  "org.apache.hadoop.fs.statistics.MeanStatistic:toString()" : null,
  "org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.util.Map,java.lang.Object)" : "* Register the MBean using our standard MBeanName format\n   * \"hadoop:service={@literal <serviceName>,name=<nameName>}\"\n   * Where the {@literal <serviceName> and <nameName>} are the supplied\n   * parameters.\n   *\n   * @param serviceName serviceName.\n   * @param nameName nameName.\n   * @param properties - Key value pairs to define additional JMX ObjectName\n   *                     properties.\n   * @param theMbean    - the MBean to register\n   * @return the named used to register the MBean",
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:randomiseBlock()" : null,
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.Decryptor,org.apache.hadoop.crypto.key.KeyProvider$KeyVersion,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getStartPos()" : "* Get the starting position of the block in the file.\n       * \n       * @return the starting position of the block in the file.",
  "org.apache.hadoop.ipc.CallerContext$Builder:append(java.lang.String,java.lang.String)" : "* Append new field which contains key and value to the context.\n     * @param key the key of field.\n     * @param value the value of field.\n     * @return the builder.",
  "org.apache.hadoop.fs.HarFileSystem:decodeHarURI(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* decode the raw URI to get the underlying URI\n   * @param rawURI raw Har URI\n   * @return filtered URI of the underlying fileSystem",
  "org.apache.hadoop.fs.impl.CombinedFileRange:getUnderlying()" : "* Get the list of ranges that were merged together to form this one.\n   * @return the list of input ranges",
  "org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:getUsage()" : null,
  "org.apache.hadoop.security.KDiag:exec(org.apache.hadoop.conf.Configuration,java.lang.String[])" : "* Inner entry point, with no logging or system exits.\n   *\n   * @param conf configuration\n   * @param argv argument list\n   * @return an exception\n   * @throws Exception Exception.",
  "org.apache.hadoop.util.AutoCloseableLock:close()" : "* Attempts to release the lock by making a call to {@code release()}.\n   *\n   * This is to implement {@code close()} method from {@code AutoCloseable}\n   * interface. This allows users to user a try-with-resource syntax, where\n   * the lock can be automatically released.",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.io.MapFile$Merger:close()" : null,
  "org.apache.hadoop.fs.VectoredReadUtils:sliceTo(java.nio.ByteBuffer,long,org.apache.hadoop.fs.FileRange)" : "* Slice the data that was read to the user's request.\n   * This function assumes that the user's request is completely subsumed by the\n   * read data. This always creates a new buffer pointing to the same underlying\n   * data but with its own mark and position fields such that reading one buffer\n   * can't effect other's mark and position.\n   * @param readData the buffer with the readData\n   * @param readOffset the offset in the file for the readData\n   * @param request the user's request\n   * @return the readData buffer that is sliced to the user's request",
  "org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.shell.FsUsage$Du:processArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:hflush()" : null,
  "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:toString()" : null,
  "org.apache.hadoop.fs.HarFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Initialize a Har filesystem per har archive. The \n   * archive home directory is the top level directory\n   * in the filesystem that contains the HAR archive.\n   * Be careful with this method, you do not want to go \n   * on creating new Filesystem instances per call to \n   * path.getFileSystem().\n   * the uri of Har is \n   * har://underlyingfsscheme-host:port/archivepath.\n   * or \n   * har:///archivepath. This assumes the underlying filesystem\n   * to be used in case not specified.",
  "org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockCount()" : "* Get the number of data blocks.\n     * \n     * @return the number of data blocks.",
  "org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter:log(int,java.lang.String)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry:<init>(java.lang.String,java.lang.Iterable)" : null,
  "org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>(int,int,int,int)" : "* Constructor.\n   * <p>\n   * Builds an empty Dynamic Bloom filter.\n   * @param vectorSize The number of bits in the vector.\n   * @param nbHash The number of hash function to consider.\n   * @param hashType type of the hashing function (see\n   * {@link org.apache.hadoop.util.hash.Hash}).\n   * @param nr The threshold for the maximum number of keys to record in a\n   * dynamic Bloom filter row.",
  "org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:anyRecoverable(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : "* Given a BlockGroup, tell if any of the missing blocks can be recovered,\n   * to be called by ECManager\n   * @param blockGroup a blockGroup that may contain erased blocks but not sure\n   *                   recoverable or not\n   * @return true if any erased block recoverable, false otherwise",
  "org.apache.hadoop.fs.viewfs.NotInMountpointException:<init>(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.security.http.RestCsrfPreventionFilter:handleHttpInteraction(org.apache.hadoop.security.http.RestCsrfPreventionFilter$HttpInteraction)" : "* Handles an {@link HttpInteraction} by applying the filtering logic.\n   *\n   * @param httpInteraction caller's HTTP interaction\n   * @throws IOException if there is an I/O error\n   * @throws ServletException if the implementation relies on the servlet API\n   *     and a servlet API call has failed",
  "org.apache.hadoop.crypto.CryptoInputStream:getBuffer()" : "Get direct buffer from pool",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withCounters(java.lang.String[])" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:reJoinElectionAfterFailureToBecomeActive()" : "* We failed to become active. Re-join the election, but\n   * sleep for a few seconds after terminating our existing\n   * session, so that other nodes have a chance to become active.\n   * The failure to become active is already logged inside\n   * becomeActive().",
  "org.apache.hadoop.fs.BlockLocation:toString()" : null,
  "org.apache.hadoop.metrics2.lib.MethodMetric:isLong(java.lang.Class)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:close()" : null,
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getWriteLock()" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:close()" : "Close the file.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:initializeMountedFileSystems(java.util.List)" : "* Initialize the target filesystem for all mount points.\n   * @param mountPoints The mount points\n   * @return Mapping of mount point and the initialized target filesystems\n   * @throws RuntimeException when the target file system cannot be initialized",
  "org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler:failed(java.lang.Throwable,java.lang.Integer)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:<init>(org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto,com.google.protobuf.Message)" : null,
  "org.apache.hadoop.io.BytesWritable:<init>(byte[],int)" : "* Create a BytesWritable using the byte array as the initial value\n   * and length as the length. Use this constructor if the array is larger\n   * than the value it represents.\n   * @param bytes This array becomes the backing storage for the object.\n   * @param length The number of bytes to use from array.",
  "org.apache.hadoop.io.retry.MultiException:<init>(java.util.Map)" : null,
  "org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:getBuffer(boolean,int)" : "* {@inheritDoc}\n   *\n   * @param direct whether we want a direct byte buffer or a heap one.\n   * @param length length of requested buffer.\n   * @return returns equal or next greater than capacity buffer from\n   * pool if already available and not garbage collected else creates\n   * a new buffer and return it.",
  "org.apache.hadoop.util.OperationDuration:asDuration()" : "* Get the duration of an operation as a java Duration\n   * instance.\n   * @return a duration.",
  "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.Path[])" : null,
  "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:incr()" : null,
  "org.apache.hadoop.io.compress.SplitCompressionInputStream:setEnd(long)" : null,
  "org.apache.hadoop.fs.shell.SnapshotCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.fs.permission.AclEntry:equals(java.lang.Object)" : null,
  "org.apache.hadoop.crypto.key.kms.ValueQueue:drain(java.lang.String)" : "* Drains the Queue for the provided key.\n   *\n   * @param keyName the key to drain the Queue for",
  "org.apache.hadoop.io.ObjectWritable:<init>()" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkState(boolean,java.lang.String,java.lang.Object[])" : "* Check state.\n   * @param expression expression which must hold.\n   * @param format format string\n   * @param args arguments for the error string\n   * @throws IllegalStateException if the state is not valid.",
  "org.apache.hadoop.io.SetFile$Writer:append(org.apache.hadoop.io.WritableComparable)" : "* Append a key to a set.  The key must be strictly greater than the\n     * previous key added to the set.\n     * @param key input key.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.token.DelegationTokenIssuer:getAdditionalTokenIssuers()" : "* Issuers may need tokens from additional services.\n   *\n   * @return delegation token issuer.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.Text:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.util.GenericOptionsParser:getConfiguration()" : "* Get the modified configuration\n   * @return the configuration that has the modified parameters.",
  "org.apache.hadoop.io.ElasticByteBufferPool:putBuffer(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.util.CrcUtil:intToBytes(int)" : "* @return 4-byte array holding the big-endian representation of\n   *     {@code value}.\n   *\n   * @param value value.",
  "org.apache.hadoop.fs.impl.prefetch.BufferPool:find(int)" : null,
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:convert(org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto)" : null,
  "org.apache.hadoop.fs.RawPathHandle:hashCode()" : null,
  "org.apache.hadoop.io.ArrayWritable:<init>(java.lang.String[])" : null,
  "org.apache.hadoop.fs.http.HttpFileSystem:getUri()" : null,
  "org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean)" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Compressor:compressDirectBuf()" : null,
  "org.apache.hadoop.io.Text:set(byte[],int,int)" : "* Set the Text to range of bytes.\n   *\n   * @param utf8 the data to copy from\n   * @param start the first position of the new string\n   * @param len the number of bytes of the new string",
  "org.apache.hadoop.fs.FilterFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)" : null,
  "org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)" : "* Return time duration in the given time unit. Valid units are encoded in\n   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds\n   * (ms), seconds (s), minutes (m), hours (h), and days (d).\n   *\n   * @param name Property name\n   * @param vStr The string value with time unit suffix to be converted.\n   * @param defaultUnit Unit to convert the stored property, if it exists.\n   * @param returnUnit Unit for the returned value.\n   * @return time duration in given time unit.",
  "org.apache.hadoop.io.retry.RetryPolicies:calculateExponentialTime(long,int,long)" : "* Return a value which is <code>time</code> increasing exponentially as a\n   * function of <code>retries</code>, +/- 0%-50% of that value, chosen\n   * randomly.\n   * \n   * @param time the base amount of time to work with\n   * @param retries the number of retries that have so occurred so far\n   * @param cap value at which to cap the base sleep time\n   * @return an amount of time to sleep",
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:checkInputBuffers(java.nio.ByteBuffer[])" : "* Check and ensure the buffers are of the desired length and type, direct\n   * buffers or not.\n   * @param buffers the buffers to check",
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:toString()" : null,
  "org.apache.hadoop.fs.DU:main(java.lang.String[])" : null,
  "org.apache.hadoop.ipc.DefaultRpcScheduler:stop()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:close()" : null,
  "org.apache.hadoop.security.Credentials:readProto(java.io.DataInput)" : "* Populates keys/values from proto buffer storage.\n   * @param in - stream ready to read a serialized proto buffer message",
  "org.apache.hadoop.fs.FSInputStream:read(long,byte[],int,int)" : null,
  "org.apache.hadoop.fs.FSInputChecker:reset()" : null,
  "org.apache.hadoop.util.Progress:addPhase(float)" : "* Adds a node with a specified progress weightage to the tree.\n   *\n   * @param weightage weightage.\n   * @return Progress.",
  "org.apache.hadoop.ha.ZKFailoverController:recheckElectability()" : "* Check the current state of the service, and join the election\n   * if it should be in the election.",
  "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:setEngineId(java.lang.String)" : null,
  "org.apache.hadoop.io.LongWritable:toString()" : null,
  "org.apache.hadoop.io.serializer.avro.AvroSerialization:getSerializer(java.lang.Class)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:readVectored(java.util.List,java.util.function.IntFunction)" : null,
  "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$2:suffix()" : null,
  "org.apache.hadoop.crypto.key.KeyProvider:invalidateCache(java.lang.String)" : "* Can be used by implementing classes to invalidate the caches. This could be\n   * used after rollNewVersion to provide a strong guarantee to return the new\n   * version of the given key.\n   *\n   * @param name the basename of the key\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.DelegationTokenRenewer:getInstance()" : null,
  "org.apache.hadoop.conf.StorageUnit$4:toTBs(double)" : null,
  "org.apache.hadoop.net.InnerNodeImpl:hashCode()" : null,
  "org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)" : null,
  "org.apache.hadoop.fs.shell.Ls:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.security.Groups:getUserToGroupsMappingServiceWithLoadedConfiguration(org.apache.hadoop.conf.Configuration)" : "* Create new groups used to map user-to-groups with loaded configuration.\n   * @param conf configuration.\n   * @return the groups being used to map user-to-groups.",
  "org.apache.hadoop.fs.shell.find.FindOptions:isFollowLink()" : "* Should symbolic links be follows?\n   *\n   * @return true indicates links should be followed",
  "org.apache.hadoop.util.OperationDuration:finished()" : "* Update the finished time with the current system time.",
  "org.apache.hadoop.io.erasurecode.ECBlockGroup:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[])" : "* A constructor specifying data blocks and parity blocks.\n   * @param dataBlocks data blocks in the group\n   * @param parityBlocks parity blocks in the group",
  "org.apache.hadoop.io.compress.CompressorStream:finish()" : null,
  "org.apache.hadoop.log.LogLevel$CLI:doSetLevel()" : "* Send HTTP/HTTPS request to set log level.\n     *\n     * @throws HadoopIllegalArgumentException if arguments are invalid.\n     * @throws Exception if unable to connect",
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMinimumReference(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.find.And:<init>()" : null,
  "org.apache.hadoop.ipc.CallQueueManager:add(org.apache.hadoop.ipc.Schedulable)" : null,
  "org.apache.hadoop.util.QuickSort:getMaxDepth(int)" : "* Deepest recursion before giving up and doing a heapsort.\n   * Returns 2 * ceil(log(n)).\n   *\n   * @param x x.\n   * @return MaxDepth.",
  "org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean)" : "Return the string representation of the object in the output format.\n   * For description of the options,\n   * @see #toString(boolean, boolean, boolean, boolean, List)\n   *\n   * @param qOption a flag indicating if quota needs to be printed or not\n   * @param hOption a flag indicating if human readable output is to be used\n   * @param xOption a flag indicating if calculation from snapshots is to be\n   *                included in the output\n   * @return the string representation of the object",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:mlock(java.lang.String,java.nio.ByteBuffer,long)" : null,
  "org.apache.hadoop.fs.ChecksumFs:isDirectory(org.apache.hadoop.fs.Path)" : "True iff the named path is a directory.\n   * Note: Avoid using this method. Instead reuse the FileStatus \n   * returned by getFileStatus() or listStatus() methods.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenTrackingId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$7:<init>(org.apache.hadoop.fs.FileSystem$Statistics)" : "* Copy constructor.\n     *\n     * @param other    The input Statistics object which is cloned.",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)" : null,
  "org.apache.hadoop.io.IOUtils:wrappedReadForCompressedData(java.io.InputStream,byte[],int,int)" : "* Utility wrapper for reading from {@link InputStream}. It catches any errors\n   * thrown by the underlying stream (either IO or decompression-related), and\n   * re-throws as an IOException.\n   * \n   * @param is - InputStream to be read from\n   * @param buf - buffer the data is read into\n   * @param off - offset within buf\n   * @param len - amount of data to be read\n   * @return number of bytes read\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.shell.PathData:setStat(org.apache.hadoop.fs.FileStatus)" : null,
  "org.apache.hadoop.fs.http.HttpFileSystem:open(org.apache.hadoop.fs.Path,int)" : null,
  "org.apache.hadoop.ipc.Client$Connection:setupSaslConnection(org.apache.hadoop.ipc.Client$IpcStreams)" : null,
  "org.apache.hadoop.conf.StorageUnit$5:toKBs(double)" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getIntList(java.lang.Iterable)" : "* Produces a human readable list of blocks for the purpose of logging.\n   * This method minimizes the length of returned list by converting\n   * a contiguous list of blocks into a range.\n   * for example,\n   * 1, 3, 4, 5, 6, 8 becomes 1, 3~6, 8",
  "org.apache.hadoop.security.alias.UserProvider:createCredentialEntry(java.lang.String,char[])" : null,
  "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:setExcludeTagPattern(java.lang.String,com.google.re2j.Pattern)" : null,
  "org.apache.hadoop.fs.ClusterStorageCapacityExceededException:<init>(java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.util.ExitUtil:halt(int,java.lang.String)" : "* Forcibly terminates the currently running Java virtual machine.\n   * @param status status code\n   * @param message message\n   * @throws HaltException if {@link Runtime#halt(int)} is disabled.",
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:checksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt)" : "* Set checksum opt.\n   *\n   * @param chksumOpt check sum opt.\n   * @return Generics Type B.",
  "org.apache.hadoop.fs.ChecksumFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)" : "* Open the file as a blocking call to {@link #open(Path, int)}.\n   *\n   * {@inheritDoc}",
  "org.apache.hadoop.util.ConfTest:main(java.lang.String[])" : null,
  "org.apache.hadoop.crypto.key.KeyProviderFactory:get(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Create a KeyProvider based on a provided URI.\n   *\n   * @param uri key provider URI\n   * @param conf configuration to initialize the key provider\n   * @return the key provider for the specified URI, or <code>NULL</code> if\n   *         a provider for the specified URI scheme could not be found.\n   * @throws IOException thrown if the provider failed to initialize.",
  "org.apache.hadoop.ipc.Server$Call:markCallCoordinated(boolean)" : null,
  "org.apache.hadoop.io.compress.CompressionOutputStream:close()" : null,
  "org.apache.hadoop.io.erasurecode.ECChunk:<init>(byte[],int,int)" : null,
  "org.apache.hadoop.security.token.Token$TrivialRenewer:isManaged(org.apache.hadoop.security.token.Token)" : null,
  "org.apache.hadoop.util.GSetByHashMap:contains(java.lang.Object)" : null,
  "org.apache.hadoop.util.ProtoUtil:makeIpcConnectionContext(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.security.SaslRpcServer$AuthMethod)" : "* This method creates the connection context  using exactly the same logic\n   * as the old connection context as was done for writable where\n   * the effective and real users are set based on the auth method.\n   *\n   * @param protocol protocol.\n   * @param ugi ugi.\n   * @param authMethod authMethod.\n   * @return IpcConnectionContextProto.",
  "org.apache.hadoop.fs.FilterFileSystem:getTrashRoots(boolean)" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:clear()" : "* Clear all the maps.",
  "org.apache.hadoop.io.file.tfile.CompareUtils$MemcmpRawComparator:compare(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:getEntryCount()" : "* Get the number of key-value pair entries in TFile.\n     * \n     * @return the number of key-value pairs in TFile",
  "org.apache.hadoop.io.compress.CompressionInputStream:setTrackedDecompressor(org.apache.hadoop.io.compress.Decompressor)" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:startThreads()" : null,
  "org.apache.hadoop.security.LdapGroupsMapping:getGroupNames(javax.naming.directory.SearchResult,java.util.Collection,java.util.Collection,boolean)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:isServiceUser(java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:deleteByName(java.lang.String)" : null,
  "org.apache.hadoop.conf.StorageUnit$4:toGBs(double)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:getTotalServiceUserRawCallVolume()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path)" : "* Return path of the enclosing root for a given path\n   * The enclosing root path is a common ancestor that should be used for temp and staging dirs\n   * as well as within encryption zones and other restricted directories.\n   *\n   * Call makeQualified on the param path to ensure its part of the correct filesystem\n   *\n   * @param path file path to find the enclosing root path for\n   * @return a path to the enclosing root\n   * @throws IOException early checks like failure to resolve path cause IO failures",
  "org.apache.hadoop.ipc.Server:refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)" : "* Refresh the service authorization ACL for the service handled by this server\n   * using the specified Configuration.\n   *\n   * @param conf input Configuration.\n   * @param provider input provider.",
  "org.apache.hadoop.util.Shell:isJavaVersionAtLeast(int)" : "* Query to see if major version of Java specification of the system\n   * is equal or greater than the parameter.\n   *\n   * @param version 8, 9, 10 etc.\n   * @return comparison with system property, always true for 8",
  "org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path[])" : "* Filter files/directories in the given list of paths using default\n   * path filter.\n   * <p>\n   * Does not guarantee to return the List of files/directories status in a\n   * sorted order.\n   *\n   * @param files\n   *          a list of paths\n   * @return a list of statuses for the files under the given paths after\n   *         applying the filter default Path filter\n   * @throws FileNotFoundException when the path does not exist\n   * @throws IOException see specific implementation",
  "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:generateDecodeMatrix(int[])" : null,
  "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:getConfiguration()" : null,
  "org.apache.hadoop.io.compress.DefaultCodec:getConf()" : null,
  "org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String)" : "* Get the comma delimited values of the <code>name</code> property as \n   * an array of <code>String</code>s, trimmed of the leading and trailing whitespace.\n   * If no such property is specified then an empty array is returned.\n   * \n   * @param name property name.\n   * @return property value as an array of trimmed <code>String</code>s, \n   *         or empty array.",
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToLinkedListHead(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)" : "* Helper method to add the given entry to the head of the linked list.\n   *\n   * @param entry Block entry to add.",
  "org.apache.hadoop.fs.FileContext:getLocalFSFileContext(org.apache.hadoop.conf.Configuration)" : "* @param aConf - from which the FileContext is configured\n   * @return a FileContext for the local file system using the specified config.\n   * \n   * @throws UnsupportedFileSystemException If default file system in the config\n   *           is not supported\n   *",
  "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:<init>(java.lang.String,javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration)" : null,
  "org.apache.hadoop.fs.FileContext:printStatistics()" : "* Prints the statistics to standard output. File System is identified by the\n   * scheme and authority.",
  "org.apache.hadoop.ipc.CallQueueManager:parseNumLevels(java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Read the number of levels from the configuration.\n   * This will affect the FairCallQueue's overall capacity.\n   * @throws IllegalArgumentException on invalid queue count",
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNull(java.lang.Object,java.lang.String)" : "* Validates that the given reference argument is not null.\n   * @param obj the argument reference to validate.\n   * @param argName the name of the argument being validated.",
  "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getTrackerFactory()" : "* @return The duration tracker with statistics to update.",
  "org.apache.hadoop.io.ObjectWritable:<init>(java.lang.Object)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)" : "* Get a protocol proxy that contains a proxy connection to a remote server\n   * and a set of methods that are supported by the server.\n   *\n   * @param <T> Generics Type T.\n   * @param protocol protocol\n   * @param clientVersion client's version\n   * @param addr server address\n   * @param ticket security ticket\n   * @param conf configuration\n   * @param factory socket factory\n   * @param rpcTimeout max time for each rpc; 0 means no timeout\n   * @param connectionRetryPolicy retry policy\n   * @return the proxy\n   * @throws IOException if any error occurs",
  "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getRpcVersion()" : "* Returns the rpc version used by the client.\n     * @return rpcVersion",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.ha.HAServiceTarget:getProxyForAddress(org.apache.hadoop.conf.Configuration,int,java.net.InetSocketAddress)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMetrics(org.apache.hadoop.metrics2.impl.MetricsCollectorImpl,boolean)" : null,
  "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:<init>()" : null,
  "org.apache.hadoop.security.token.Token:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.QuotaUsage:setSpaceQuota(long)" : null,
  "org.apache.hadoop.security.UserGroupInformation:loginUserFromSubject(javax.security.auth.Subject)" : "* Log in a user using the given subject\n   * @param subject the subject to use when logging in a user, or null to\n   * create a new subject.\n   *\n   * If subject is not null, the creator of subject is responsible for renewing\n   * credentials.\n   *\n   * @throws IOException if login fails",
  "org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getScheme()" : "* Return the protocol scheme for the FileSystem.\n   *\n   * @return <code>viewfs</code>",
  "org.apache.hadoop.fs.FilterFileSystem:<init>()" : null,
  "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$Context:getAndIncrDirNumLastAccessed(int)" : null,
  "org.apache.hadoop.util.VersionInfo:_getCompilePlatform()" : null,
  "org.apache.hadoop.util.SysInfoLinux:getAvailablePhysicalMemorySize()" : "{@inheritDoc}",
  "org.apache.hadoop.fs.FilterFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.WritableComparator:getConf()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByRecordNum(long,long)" : "* Create a scanner that covers a range of records.\n     * \n     * @param beginRecNum\n     *          The RecordNum for the first record (inclusive).\n     * @param endRecNum\n     *          The RecordNum for the last record (exclusive). To scan the whole\n     *          file, either specify endRecNum==-1 or endRecNum==getEntryCount().\n     * @return The TFile scanner that covers the specified range of records.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.DataInputByteBuffer:<init>()" : null,
  "org.apache.hadoop.fs.FSOutputSummer:setChecksumBufSize(int)" : "* Resets existing buffer with a new one of the specified size.\n   *\n   * @param size size.",
  "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:getMaterial()" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getMetadata(java.lang.String)" : null,
  "org.apache.hadoop.http.HttpServer2:defineFilter(org.eclipse.jetty.servlet.ServletContextHandler,org.eclipse.jetty.servlet.FilterHolder,org.eclipse.jetty.servlet.FilterMapping)" : "* Define a filter for a context and set up default url mappings.",
  "org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:deserialize(org.apache.hadoop.io.Writable)" : null,
  "org.apache.hadoop.util.InstrumentedReadWriteLock:<init>(boolean,java.lang.String,org.slf4j.Logger,long,long)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredToken()" : "Remove expired delegation tokens from cache",
  "org.apache.hadoop.fs.FileSystem:setQuotaByStorageType(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.StorageType,long)" : "* Set per storage type quota for the given {@link Path}.\n   *\n   * @param src the target path to set storage type quota for\n   * @param type the storage type to set\n   * @param quota the quota to set for the given storage type\n   * @throws IOException IO failure",
  "org.apache.hadoop.service.ServiceOperations:stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)" : "* Stop a service; if it is null do nothing. Exceptions are caught and\n   * logged at warn level. (but not Throwables). This operation is intended to\n   * be used in cleanup operations\n   *\n   * @param log the log to warn at\n   * @param service a service; may be null\n   * @return any exception that was caught; null if none was.\n   * @see ServiceOperations#stopQuietly(Service)",
  "org.apache.hadoop.security.token.Token$PrivateToken:<init>(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.io.WritableName:getName(java.lang.Class)" : "* Return the name for a class.\n   * Default is {@link Class#getName()}.\n   * @param writableClass input writableClass.\n   * @return name for a class.",
  "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:write(byte[])" : null,
  "org.apache.hadoop.fs.shell.FsCommand:<init>()" : null,
  "org.apache.hadoop.service.launcher.InterruptEscalator:toString()" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:getValueClass()" : "@return Returns the class of values in this file.",
  "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:info()" : null,
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getFlags()" : null,
  "org.apache.hadoop.fs.shell.Test:testAccess(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.permission.FsAction)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackJavaFunctionDuration(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,java.util.function.Function)" : "* Given a java function/lambda expression,\n   * return a new one which wraps the inner and tracks\n   * the duration of the operation, including whether\n   * it passes/fails.\n   * @param factory factory of duration trackers\n   * @param statistic statistic key\n   * @param inputFn input function\n   * @param <A> type of argument to the input function.\n   * @param <B> return type.\n   * @return a new function which tracks duration and failure.",
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:finished()" : null,
  "org.apache.hadoop.fs.HarFileSystem:getHomeDirectory()" : "* return the top level archive path.",
  "org.apache.hadoop.ipc.ClientId:getClientId()" : "* @return Return clientId as byte[].",
  "org.apache.hadoop.security.HttpCrossOriginFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:readLongArray(java.io.DataInput)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:joinElection(byte[])" : "* To participate in election, the app will call joinElection. The result will\n   * be notified by a callback on either the becomeActive or becomeStandby app\n   * interfaces. <br>\n   * After this the elector will automatically monitor the leader status and\n   * perform re-election if necessary<br>\n   * The app could potentially start off in standby mode and ignore the\n   * becomeStandby call.\n   * \n   * @param data\n   *          to be set by the app. non-null data must be set.\n   * @throws HadoopIllegalArgumentException\n   *           if valid data is not supplied",
  "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:selectDelegationToken(org.apache.hadoop.security.Credentials)" : null,
  "org.apache.hadoop.fs.permission.FsPermission:getFileDefault()" : "* Get the default permission for file.\n   *\n   * @return FileDefault FsPermission.",
  "org.apache.hadoop.fs.permission.AclStatus$Builder:addEntry(org.apache.hadoop.fs.permission.AclEntry)" : "* Adds an ACL entry.\n     *\n     * @param e AclEntry entry to add\n     * @return Builder this builder, for call chaining",
  "org.apache.hadoop.ipc.Server$ConnectionManager:closeAll()" : null,
  "org.apache.hadoop.util.XMLUtils:newSecureDocumentBuilderFactory()" : "* This method should be used if you need a {@link DocumentBuilderFactory}. Use this method\n   * instead of {@link DocumentBuilderFactory#newInstance()}. The factory that is returned has\n   * secure configuration enabled.\n   *\n   * @return a {@link DocumentBuilderFactory} with secure configuration enabled\n   * @throws ParserConfigurationException if the {@code JAXP} parser does not support the\n   * secure configuration",
  "org.apache.hadoop.fs.FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Copy it a file from a remote filesystem to the local one.\n   * delSrc indicates if the src will be removed or not.\n   * @param delSrc whether to delete the src\n   * @param src path src file in the remote filesystem\n   * @param dst path local destination\n   * @throws IOException IO failure",
  "org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.String,java.lang.Object[])" : "* Create a formatted exception.\n   * <p>\n   * This uses {@link String#format(String, Object...)}\n   * to build the formatted exception in the ENGLISH locale.\n   * <p>\n   * If the last argument is a throwable, it becomes the cause of the exception.\n   * It will also be used as a parameter for the format.\n   * @param exitCode exit code\n   * @param format format for message to use in exception\n   * @param args list of arguments",
  "org.apache.hadoop.conf.Configuration:handleDeprecation()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider:<init>(org.apache.hadoop.conf.Configuration)" : "* Constructor.\n   * \n   * @param conf configuration for the provider",
  "org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])" : null,
  "org.apache.hadoop.util.SysInfoWindows:getAvailablePhysicalMemorySize()" : "{@inheritDoc}",
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getServerDefaults(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.HostsFileReader:getHostDetails(java.util.Set,java.util.Map)" : "* Retrieve an atomic view of the included and excluded hosts.\n   *\n   * @param includeHosts set to populate with included hosts\n   * @param excludeHosts map to populate with excluded hosts\n   * @deprecated use {@link #getHostDetails() instead}",
  "org.apache.hadoop.io.BinaryComparable:compareTo(org.apache.hadoop.io.BinaryComparable)" : "* Compare bytes from {#getBytes()}.\n   * @see org.apache.hadoop.io.WritableComparator#compareBytes(byte[],int,int,byte[],int,int)",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)" : "* Get a protocol proxy that contains a proxy connection to a remote server\n   * and a set of methods that are supported by the server.\n   *\n   * @param <T> Generics Type T.\n   * @param protocol protocol class\n   * @param clientVersion client version\n   * @param addr remote address\n   * @param conf configuration to use\n   * @param factory socket factory\n   * @return the protocol proxy\n   * @throws IOException if the far end through a RemoteException",
  "org.apache.hadoop.util.LineReader:close()" : "* Close the underlying stream.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheUpdated()" : "* One cache updated",
  "org.apache.hadoop.util.bloom.BloomFilter:<init>()" : "Default constructor - use with readFields",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.Server$RpcCall:run()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:chooseBlockSize(long)" : "* Chooses a blocksize based on the given length of the data to compress.\n  *\n  * @return The blocksize, between {@link #MIN_BLOCKSIZE} and\n  *         {@link #MAX_BLOCKSIZE} both inclusive. For a negative\n  *         <tt>inputLength</tt> this method returns <tt>MAX_BLOCKSIZE</tt>\n  *         always.\n  *\n  * @param inputLength\n  *            The length of the data which will be compressed by\n  *            <tt>CBZip2OutputStream</tt>.",
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:<init>(org.apache.hadoop.metrics2.MetricsInfo)" : "* Construct the registry with a metadata object\n   * @param info  the info object for the metrics record/group",
  "org.apache.hadoop.util.ExitUtil:terminateCalled()" : "* @return true if terminate has been called.",
  "org.apache.hadoop.ipc.RefreshResponse:<init>(int,java.lang.String)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:getDefaultBackOffResponseTimeThresholds(int)" : null,
  "org.apache.hadoop.ipc.Client$IpcStreams:sendRequest(byte[])" : null,
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:overwrite(boolean)" : "* Set to true to overwrite the existing file.\n   * Set it to false, an exception will be thrown when calling {@link #build()}\n   * if the file exists.",
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getFS()" : null,
  "org.apache.hadoop.io.erasurecode.ECBlock:<init>()" : "* A default constructor. isParity and isErased are false by default.",
  "org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoder(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : "* Create RS raw encoder according to configuration.\n   * @param conf configuration\n   * @param coderOptions coder options that's used to create the coder\n   * @param codec the codec to use. If null, will use the default codec\n   * @return raw encoder",
  "org.apache.hadoop.ipc.Server:setupResponse(org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)" : "* Setup response for the IPC Call.\n   * \n   * @param call {@link Call} to which we are setting up the response\n   * @param status of the IPC call\n   * @param rv return value for the IPC Call, if the call was successful\n   * @param errorClass error class, if the the call failed\n   * @param error error message, if the call failed\n   * @throws IOException",
  "org.apache.hadoop.metrics2.lib.MutableCounterInt:value()" : null,
  "org.apache.hadoop.tracing.Tracer$Builder:<init>(java.lang.String)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:destroy()" : null,
  "org.apache.hadoop.util.HeapSort:<init>()" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)" : null,
  "org.apache.hadoop.net.ScriptBasedMappingWithDependency:getRawMapping()" : "* Get the cached mapping and convert it to its real type\n   * @return the inner raw script mapping.",
  "org.apache.hadoop.fs.ftp.FTPInputStream:seekToNewSource(long)" : null,
  "org.apache.hadoop.util.Shell$ShellCommandExecutor:toString()" : "* Returns the commands of this instance.\n     * Arguments with spaces in are presented with quotes round; other\n     * arguments are presented raw\n     *\n     * @return a string representation of the object.",
  "org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String)" : "@param path for the exception",
  "org.apache.hadoop.security.Credentials:write(java.io.DataOutput)" : "* Stores all the keys to DataOutput.\n   * @param out DataOutput.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.http.HttpServer2:initSpnego(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Properties,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.Throwable,java.lang.String,java.lang.Object[])" : "* Create a formatted exception.\n   * <p>\n   * This uses {@link String#format(String, Object...)}\n   * to build the formatted exception in the ENGLISH locale.\n   * @param exitCode exit code\n   * @param cause inner cause\n   * @param format format for message to use in exception\n   * @param args list of arguments",
  "org.apache.hadoop.fs.FileContext$Util:getContentSummary(org.apache.hadoop.fs.Path)" : "* Return the {@link ContentSummary} of path f.\n     * @param f path\n     *\n     * @return the {@link ContentSummary} of path f.\n     *\n     * @throws AccessControlException If access is denied\n     * @throws FileNotFoundException If <code>f</code> does not exist\n     * @throws UnsupportedFileSystemException If file system for \n     *         <code>f</code> is not supported\n     * @throws IOException If an I/O error occurred\n     * \n     * Exceptions applicable to file systems accessed over RPC:\n     * @throws RpcClientException If an exception occurred in the RPC client\n     * @throws RpcServerException If an exception occurred in the RPC server\n     * @throws UnexpectedServerException If server implementation throws \n     *           undeclared exception to RPC server",
  "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:hasCapacity(long)" : null,
  "org.apache.hadoop.service.ServiceOperations$ServiceListeners:notifyListeners(org.apache.hadoop.service.Service)" : "* Change to a new state and notify all listeners.\n     * This method will block until all notifications have been issued.\n     * It caches the list of listeners before the notification begins,\n     * so additions or removal of listeners will not be visible.\n     * @param service the service that has changed state",
  "org.apache.hadoop.io.Text:validateUTF8(byte[],int,int)" : "* Check to see if a byte array is valid UTF-8.\n   *\n   * @param utf8 the array of bytes\n   * @param start the offset of the first byte in the array\n   * @param len the length of the byte sequence\n   * @throws MalformedInputException if the byte array contains invalid bytes",
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getFlags()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:updateAverageResponseTime(boolean)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:divide(int,int)" : "* Compute the division of two fields\n   *\n   * @param x input field\n   * @param y input field\n   * @return x/y",
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_toJsonString(java.io.Serializable)" : "* Save IOStatisticsSnapshot to a JSON string.\n   * @param snapshot statistics; may be null or of an incompatible type\n   * @return JSON string value\n   * @throws UncheckedIOException Any IO/jackson exception.\n   * @throws IllegalArgumentException if the supplied class is not a snapshot",
  "org.apache.hadoop.metrics2.MetricsJsonBuilder:toString()" : null,
  "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:toString()" : "* Return the statistics dump of the wrapped statistics.\n   * @return the statistics for logging.",
  "org.apache.hadoop.http.HttpServer2$XFrameOption:getEnum(java.lang.String)" : "* We cannot use valueOf since the AllowFrom enum differs from its value\n     * Allow-From. This is a helper method that does exactly what valueof does,\n     * but allows us to handle the AllowFrom issue gracefully.\n     *\n     * @param value - String must be DENY, SAMEORIGIN or ALLOW-FROM.\n     * @return XFrameOption or throws IllegalException.",
  "org.apache.hadoop.conf.Configuration:loadProperty(java.util.Properties,java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String[])" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:<init>(long,long,long,long)" : "* Create a secret manager\n   * @param delegationKeyUpdateInterval the number of milliseconds for rolling\n   *        new secret keys.\n   * @param delegationTokenMaxLifetime the maximum lifetime of the delegation\n   *        tokens in milliseconds\n   * @param delegationTokenRenewInterval how often the tokens must be renewed\n   *        in milliseconds\n   * @param delegationTokenRemoverScanInterval how often the tokens are scanned\n   *        for expired tokens in milliseconds",
  "org.apache.hadoop.conf.Configuration:getPropertySources(java.lang.String)" : "* Gets information about why a property was set.  Typically this is the \n   * path to the resource objects (file, URL, etc.) the property came from, but\n   * it can also indicate that it was set programmatically, or because of the\n   * command line.\n   *\n   * @param name - The property name to get the source of.\n   * @return null - If the property or its source wasn't found. Otherwise, \n   * returns a list of the sources of the resource.  The older sources are\n   * the first ones in the list.  So for example if a configuration is set from\n   * the command line, and then written out to a file that is read back in the\n   * first entry would indicate that it was set from the command line, while\n   * the second one would indicate the file that the new configuration was read\n   * in from.",
  "org.apache.hadoop.util.bloom.CountingBloomFilter:not()" : null,
  "org.apache.hadoop.ipc.Server:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)" : null,
  "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:escapeToPathElement(java.lang.CharSequence)" : "* Perform any escaping to valid path elements in advance of\n   * new URI() doing this itself. Only path separators need to\n   * be escaped/converted at this point.\n   * @param source source string\n   * @return an escaped path element.",
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumDataUnits()" : null,
  "org.apache.hadoop.security.KDiag:validateNTPConf()" : null,
  "org.apache.hadoop.security.token.Token:toString()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(byte[],byte[])" : "* Get a scanner that covers a portion of TFile based on keys.\n     * \n     * @param beginKey\n     *          Begin key of the scan (inclusive). If null, scan from the first\n     *          key-value entry of the TFile.\n     * @param endKey\n     *          End key of the scan (exclusive). If null, scan up to the last\n     *          key-value entry of the TFile.\n     * @return The actual coverage of the returned scanner will cover all keys\n     *         greater than or equal to the beginKey and less than the endKey.\n     * @throws IOException raised on errors performing I/O.\n     * \n     * @deprecated Use {@link #createScannerByKey(byte[], byte[])} instead.",
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:unbuffer()" : null,
  "org.apache.hadoop.fs.ContentSummary:toString(boolean)" : "Return the string representation of the object in the output format.\n   * if qOption is false, output directory count, file count, and content size;\n   * if qOption is true, output quota and remaining quota as well.\n   *\n   * @param qOption a flag indicating if quota needs to be printed or not\n   * @return the string representation of the object",
  "org.apache.hadoop.io.DataInputByteBuffer:getLength()" : null,
  "org.apache.hadoop.fs.VectoredReadUtils:validateVectoredReadRanges(java.util.List)" : "* Validate a list of vectored read ranges.\n   * @param ranges list of ranges.\n   * @throws EOFException any EOF exception.",
  "org.apache.hadoop.io.OutputBuffer:reset()" : "@return Resets the buffer to empty.",
  "org.apache.hadoop.security.SecurityUtil:doAsUser(org.apache.hadoop.security.UserGroupInformation,java.security.PrivilegedExceptionAction)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getDelegationTokens(java.lang.String)" : null,
  "org.apache.hadoop.fs.permission.PermissionParser:<init>(java.lang.String,java.util.regex.Pattern,java.util.regex.Pattern)" : "* Begin parsing permission stored in modeStr\n   * \n   * @param modeStr Permission mode, either octal or symbolic\n   * @param symbolic Use-case specific symbolic pattern to match against\n   * @throws IllegalArgumentException if unable to parse modeStr",
  "org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)" : "* Get a protocol proxy that contains a proxy connection to a remote server\n   * and a set of methods that are supported by the server\n   *\n   * @param <T> Generics Type T.\n   * @param protocol protocol class\n   * @param clientVersion client version\n   * @param addr remote address\n   * @param conf configuration to use\n   * @param connTimeout time in milliseconds before giving up\n   * @return the protocol proxy\n   * @throws IOException if the far end through a RemoteException",
  "org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.lang.String)" : "* Convert Kerberos principal name pattern to valid Kerberos principal\n   * names. It replaces hostname pattern with hostname, which should be\n   * fully-qualified domain name. If hostname is null or \"0.0.0.0\", it uses\n   * dynamically looked-up fqdn of the current host instead.\n   * \n   * @param principalConfig\n   *          the Kerberos principal name conf value to convert\n   * @param hostname\n   *          the fully-qualified domain name used for substitution\n   * @return converted Kerberos principal name\n   * @throws IOException if the client address cannot be determined",
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:invalidateCache(java.lang.String)" : null,
  "org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir()" : null,
  "org.apache.hadoop.fs.FileSystem:getScheme()" : "* Return the protocol scheme for this FileSystem.\n   * <p>\n   * This implementation throws an <code>UnsupportedOperationException</code>.\n   *\n   * @return the protocol scheme for this FileSystem.\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).",
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMeanStatisticFunction(java.lang.String,java.util.function.Function)" : "* add a mapping of a key to a meanStatistic function.\n   * @param key the key\n   * @param eval the evaluator",
  "org.apache.hadoop.util.LightWeightResizableGSet:expandIfNecessary()" : "* Checks if we need to expand, and expands if necessary.",
  "org.apache.hadoop.io.compress.ZStandardCodec:createDecompressor()" : "* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.\n   *\n   * @return a new decompressor for use by this codec",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_gauges(java.io.Serializable)" : "* Get the gauges of an IOStatisticsSnapshot.\n   * @param source source of statistics.\n   * @return the map of gauges.",
  "org.apache.hadoop.util.functional.TaskPool$Builder:runParallel(org.apache.hadoop.util.functional.TaskPool$Task)" : "* Parallel execution.\n     * All tasks run within the same IOStatisticsContext as the\n     * thread calling this method.\n     * @param task task to execute\n     * @param <E> exception which may be raised in execution.\n     * @return true if the operation executed successfully\n     * @throws E any exception raised.\n     * @throws IOException IOExceptions raised by remote iterator or in execution.",
  "org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.Lock,long,long)" : null,
  "org.apache.hadoop.security.Credentials:readFields(java.io.DataInput)" : "* Loads all the keys.\n   * @param in DataInput.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.FileStatus:isEncrypted()" : "* Tell whether the underlying file or directory is encrypted or not.\n   *\n   * @return true if the underlying file is encrypted.",
  "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:isOnSameRack(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)" : null,
  "org.apache.hadoop.net.TableMapping:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:getOutputBlocks()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.fs.shell.Display$Checksum:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:initBlock(int)" : "* Load a compressed block for reading. Expecting blockIndex is valid.\n       * \n       * @throws IOException",
  "org.apache.hadoop.util.curator.ZKCuratorManager:start()" : "* Start the connection to the ZooKeeper ensemble.\n   * @throws IOException If the connection cannot be started.",
  "org.apache.hadoop.fs.Options$CreateOpts$ReplicationFactor:<init>(short)" : null,
  "org.apache.hadoop.io.compress.ZStandardCodec:getCompressionLevel(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.UserGroupInformation:getRealUser()" : "* get RealUser (vs. EffectiveUser)\n   * @return realUser running over proxy user",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.metrics2.MetricsSystem:register(java.lang.Object)" : "* Register a metrics source (deriving name and description from the object)\n   * @param <T>   the actual type of the source object\n   * @param source  object to register\n   * @return  the source object\n   * @exception MetricsException Metrics Exception.",
  "org.apache.hadoop.net.SocketOutputStream:getChannel()" : "* @return Returns underlying channel used by this stream.\n   * This is useful in certain cases like channel for \n   * {@link FileChannel#transferTo(long, long, WritableByteChannel)}",
  "org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:toString()" : null,
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:writeChunk(byte[],int,int,boolean)" : "* Write out a chunk.\n     * \n     * @param chunk\n     *          The chunk buffer.\n     * @param offset\n     *          Offset to chunk buffer for the beginning of chunk.\n     * @param len\n     * @param last\n     *          Is this the last call to flushBuffer?",
  "org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:minimums()" : null,
  "org.apache.hadoop.ipc.RpcClientUtil:putVersionSignatureMap(java.net.InetSocketAddress,java.lang.String,java.lang.String,java.util.Map)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMaximumSample(java.lang.String,long)" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:next(java.lang.Object)" : "* Read the next key in the file, skipping its\n     * value.\n     *\n     * @param key input Object key.\n     * @throws IOException raised on errors performing I/O.\n     * @return Return null at end of file.",
  "org.apache.hadoop.ipc.Server:getCallId()" : "* Returns the currently active RPC call's sequential ID number.  A negative\n   * call ID indicates an invalid value, such as if there is no currently active\n   * RPC call.\n   * \n   * @return int sequential ID number of currently active RPC call",
  "org.apache.hadoop.io.file.tfile.CompareUtils$MemcmpRawComparator:compare(java.lang.Object,java.lang.Object)" : null,
  "org.apache.hadoop.fs.Options$HandleOpt$Location:<init>(boolean)" : null,
  "org.apache.hadoop.fs.FileUtil:makeShellPath(java.io.File)" : "* Convert a os-native filename to a path that works for the shell.\n   * @param file The filename to convert\n   * @return The unix pathname\n   * @throws IOException on windows, there can be problems with the subprocess",
  "org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)" : "* Get a scanner that covers a specific key range.\n     * \n     * @param beginKey\n     *          Begin key of the scan (inclusive). If null, scan from the first\n     *          key-value entry of the TFile.\n     * @param endKey\n     *          End key of the scan (exclusive). If null, scan up to the last\n     *          key-value entry of the TFile.\n     * @return The actual coverage of the returned scanner will cover all keys\n     *         greater than or equal to the beginKey and less than the endKey.\n     * @throws IOException raised on errors performing I/O.\n     * \n     * @deprecated Use {@link #createScannerByKey(RawComparable, RawComparable)}\n     *             instead.",
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getEntry(int)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getType()" : null,
  "org.apache.hadoop.util.functional.RemoteIterators:rangeExcludingIterator(long,long)" : "* A remote iterator which simply counts up, stopping once the\n   * value is greater than the value of {@code excludedFinish}.\n   * This is primarily for tests or when submitting work into a TaskPool.\n   * equivalent to\n   * <pre>\n   *   for(long l = start, l &lt; excludedFinish; l++) yield l;\n   * </pre>\n   * @param start start value\n   * @param excludedFinish excluded finish\n   * @return an iterator which returns longs from [start, finish)",
  "org.apache.hadoop.fs.FileSystem:getChildFileSystems()" : "* Get all the immediate child FileSystems embedded in this FileSystem.\n   * It does not recurse and get grand children.  If a FileSystem\n   * has multiple child FileSystems, then it must return a unique list\n   * of those FileSystems.  Default is to return null to signify no children.\n   *\n   * @return FileSystems that are direct children of this FileSystem,\n   *         or null for \"no children\"",
  "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$4:unit()" : null,
  "org.apache.hadoop.io.MD5Hash:digest(byte[][],int,int)" : "* Construct a hash value for an array of byte array.\n   * @param dataArr dataArr.\n   * @param start start.\n   * @param len len.\n   * @return MD5Hash.",
  "org.apache.hadoop.util.DataChecksum$Type:valueOf(int)" : "* the type corresponding to the id.\n     *\n     * @return the type corresponding to the id.\n     * @param id id.",
  "org.apache.hadoop.ipc.Client:checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto)" : "Check the rpc response header.",
  "org.apache.hadoop.io.VIntWritable:toString()" : null,
  "org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)" : null,
  "org.apache.hadoop.io.Text$Comparator:compare(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.fs.shell.FsUsage:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder)" : null,
  "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:hflush()" : "* If the inner stream is Syncable, flush the buffer and then\n   * invoke the inner stream's hflush() operation.\n   *\n   * Otherwise: throw an exception, unless the stream was constructed with\n   * {@link #downgradeSyncable} set to true, in which case the stream\n   * is just flushed.\n   * @throws IOException IO Problem\n   * @throws UnsupportedOperationException if the inner class is not syncable",
  "org.apache.hadoop.security.Groups:refresh()" : "* Refresh all user-to-groups mappings.",
  "org.apache.hadoop.fs.Trash:moveToTrash(org.apache.hadoop.fs.Path)" : "Move a file or directory to the current trash directory.\n   *\n   * @param path the path.\n   * @return false if the item is already in the trash or trash is disabled\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.BinaryComparable:hashCode()" : "* Return a hash of the bytes returned from {#getBytes()}.\n   * @see org.apache.hadoop.io.WritableComparator#hashBytes(byte[],int)",
  "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:<init>()" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(byte[][],int[],byte[][])" : "* Decode with inputs and erasedIndexes, generates outputs. More see above.\n   *\n   * @param inputs input buffers to read data from\n   * @param erasedIndexes indexes of erased units in the inputs array\n   * @param outputs output buffers to put decoded data into according to\n   *                erasedIndexes, ready for read after the call\n   * @throws IOException if the decoder is closed.",
  "org.apache.hadoop.fs.FileSystem:supportsSymlinks()" : "* See {@link AbstractFileSystem#supportsSymlinks()}.\n   * @return if support symlinkls true, not false.",
  "org.apache.hadoop.io.compress.lz4.Lz4Compressor:finish()" : "* When called, indicates that compression should end\n   * with the current contents of the input buffer.",
  "org.apache.hadoop.service.AbstractService:noteFailure(java.lang.Exception)" : "* Failure handling: record the exception\n   * that triggered it -if there was not one already.\n   * Services are free to call this themselves.\n   * @param exception the exception",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getSequenceNumber()" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:setReplication(org.apache.hadoop.fs.Path,short)" : "* Set replication for an existing file.\n   * Implement the abstract <tt>setReplication</tt> of <tt>FileSystem</tt>\n   * @param src file name\n   * @param replication new replication\n   * @throws IOException if an I/O error occurs.\n   * @return true if successful;\n   *         false if file does not exist or is a directory",
  "org.apache.hadoop.fs.audit.CommonAuditContext:currentAuditContext()" : "* Get the current common audit context. Thread local.\n   * @return the audit context of this thread.",
  "org.apache.hadoop.fs.FilterFileSystem:getDefaultReplication()" : null,
  "org.apache.hadoop.fs.TrashPolicyDefault$Emptier:floor(long,long)" : null,
  "org.apache.hadoop.security.alias.KeyStoreProvider:initFileSystem(java.net.URI)" : null,
  "org.apache.hadoop.fs.FileStatus:getOwner()" : "* Get the owner of the file.\n   * @return owner of the file. The string could be empty if there is no\n   *         notion of owner of a file in a filesystem or if it could not \n   *         be determined (rare).",
  "org.apache.hadoop.crypto.key.KeyShell$ListCommand:getUsage()" : null,
  "org.apache.hadoop.util.LightWeightGSet:values()" : null,
  "org.apache.hadoop.security.token.DtFileOperations:importTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,java.lang.String,org.apache.hadoop.conf.Configuration)" : "Import a token from a base64 encoding into the local filesystem.\n   * @param tokenFile A local File object.\n   * @param fileFormat A string equal to FORMAT_PB or FORMAT_JAVA, for output.\n   * @param alias overwrite Service field of fetched token with this text.\n   * @param base64 urlString Encoding of the token to import.\n   * @param conf Configuration object passed along.\n   * @throws IOException Error to import the token into the file.",
  "org.apache.hadoop.util.KMSUtil:parseJSONEncKeyVersions(java.lang.String,java.util.List)" : null,
  "org.apache.hadoop.fs.shell.Ls:getListingGroupSize()" : null,
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getProtocolSignature(java.lang.String,long,int)" : null,
  "org.apache.hadoop.security.SecurityUtil:buildTokenService(java.net.URI)" : "* Construct the service key for a token\n   * @param uri of remote connection with a token\n   * @return \"ip:port\" or \"host:port\" depending on the value of\n   *          hadoop.security.token.service.use_ip",
  "org.apache.hadoop.metrics2.impl.AbstractMetricsRecord:equals(java.lang.Object)" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:toString()" : null,
  "org.apache.hadoop.io.SequenceFile$RecordCompressWriter:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)" : "Append a key/value pair.",
  "org.apache.hadoop.util.VersionUtil:compareVersions(java.lang.String,java.lang.String)" : "* Compares two version name strings using maven's ComparableVersion class.\n   *\n   * @param version1\n   *          the first version to compare\n   * @param version2\n   *          the second version to compare\n   * @return a negative integer if version1 precedes version2, a positive\n   *         integer if version2 precedes version1, and 0 if and only if the two\n   *         versions are equal.",
  "org.apache.hadoop.util.StringUtils:arrayToString(java.lang.String[])" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getCandidateTokensForCleanup()" : null,
  "org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(boolean,boolean)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getLibraryName(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:flush()" : null,
  "org.apache.hadoop.metrics2.lib.MutableMetric:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder)" : "* Get a snapshot of metric if changed\n   * @param builder the metrics record builder",
  "org.apache.hadoop.fs.FilterFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)" : null,
  "org.apache.hadoop.security.authorize.AccessControlList:<init>()" : "* This constructor exists primarily for AccessControlList to be Writable.",
  "org.apache.hadoop.fs.permission.FsPermission:equals(java.lang.Object)" : null,
  "org.apache.hadoop.net.unix.DomainSocketWatcher$Entry:getHandler()" : null,
  "org.apache.hadoop.io.MapFile$Reader$ComparatorOption:<init>(org.apache.hadoop.io.WritableComparator)" : null,
  "org.apache.hadoop.fs.ContentSummary:getSnapshotHeader()" : null,
  "org.apache.hadoop.util.LightWeightGSet$SetIterator:hasNext()" : null,
  "org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer:serialize(org.apache.hadoop.io.Writable)" : null,
  "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxySuperuserIpConfKey(java.lang.String)" : "* Return configuration key for superuser ip addresses\n   * \n   * @param userName name of the superuser\n   * @return configuration key for superuser ip-addresses",
  "org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],byte[])" : "* Adding a new key-value pair to the TFile. This is synonymous to\n     * append(key, 0, key.length, value, 0, value.length)\n     * \n     * @param key\n     *          Buffer for key.\n     * @param value\n     *          Buffer for value.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:needsInput()" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:decompress(byte[],int,int)" : null,
  "org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getFilterParameters(org.apache.hadoop.conf.Configuration,java.lang.String)" : null,
  "org.apache.hadoop.io.BooleanWritable:<init>()" : "",
  "org.apache.hadoop.ha.ActiveStandbyElector:zkDoWithRetries(org.apache.hadoop.ha.ActiveStandbyElector$ZKAction,org.apache.zookeeper.KeeperException$Code)" : null,
  "org.apache.hadoop.fs.permission.AclStatus$Builder:build()" : "* Builds a new AclStatus populated with the set properties.\n     *\n     * @return AclStatus new AclStatus",
  "org.apache.hadoop.fs.FSBuilder:must(java.lang.String,float)" : "* This parameter is converted to a long and passed\n   * to {@link #mustLong(String, long)} -all\n   * decimal precision is lost.\n   *\n   * @param key key.\n   * @param value value.\n   * @return generic type B.\n   * @deprecated use {@link #mustDouble(String, double)} to set floating point.",
  "org.apache.hadoop.util.ExitUtil:terminate(org.apache.hadoop.util.ExitUtil$ExitException)" : "* Exits the JVM if exit is enabled, rethrow provided exception or any raised error otherwise.\n   * Inner termination: either exit with the exception's exit code,\n   * or, if system exits are disabled, rethrow the exception.\n   * @param ee exit exception\n   * @throws ExitException if {@link System#exit(int)} is disabled and not suppressed by an Error\n   * @throws Error if {@link System#exit(int)} is disabled and one Error arise, suppressing\n   * anything else, even <code>ee</code>",
  "org.apache.hadoop.util.GcTimeMonitor$TsAndData:setValues(long,long)" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(int[],java.lang.String)" : "* Validates that the given array is not null and has at least one element.\n   * @param array the argument reference to validate.\n   * @param argName the name of the argument being validated.",
  "org.apache.hadoop.security.UserGroupInformation:getOsPrincipalClass()" : null,
  "org.apache.hadoop.util.PriorityQueue:pop()" : "* Removes and returns the least element of the PriorityQueue in log(size)\n      time.\n   * @return T Generics Type T.",
  "org.apache.hadoop.util.CombinedIPList:isIn(java.lang.String)" : null,
  "org.apache.hadoop.fs.audit.CommonAuditContext:reset()" : "* Rest the context; will set the standard options again.\n   * Primarily for testing.",
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:verify(java.lang.String,javax.net.ssl.SSLSession)" : "* The javax.net.ssl.HostnameVerifier contract.\n         *\n         * @param host    'hostname' we used to create our socket\n         * @param session SSLSession with the remote server\n         * @return true if the host matched the one in the certificate.",
  "org.apache.hadoop.fs.statistics.MeanStatistic:hashCode()" : "* The hash code is derived from the mean\n   * and sample count: if either is changed\n   * the statistic cannot be used as a key\n   * for hash tables/maps.\n   * @return a hash value",
  "org.apache.hadoop.fs.shell.PathData:getDirectoryContents()" : "* Returns a list of PathData objects of the items contained in the given\n   * directory.\n   * @return list of PathData objects for its children\n   * @throws IOException if anything else goes wrong...",
  "org.apache.hadoop.fs.shell.FsUsage$Df:addToUsagesTable(java.net.URI,org.apache.hadoop.fs.FsStatus,java.lang.String)" : "* Add a new row to the usages table for the given FileSystem URI.\n     *\n     * @param uri - FileSystem URI\n     * @param fsStatus - FileSystem status\n     * @param mountedOnPath - FileSystem mounted on path",
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:init(java.lang.String)" : null,
  "org.apache.hadoop.io.WritableUtils:writeEnum(java.io.DataOutput,java.lang.Enum)" : "* writes String value of enum to DataOutput. \n   * @param out Dataoutput stream\n   * @param enumVal enum value\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.Server:getConnections()" : null,
  "org.apache.hadoop.metrics2.util.SampleQuantiles:getCount()" : "* Returns the number of items that the estimator has processed\n   * \n   * @return count total number of items processed",
  "org.apache.hadoop.security.alias.CredentialShell$CreateCommand:getUsage()" : null,
  "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:closeBlock()" : "* Close the block.\n     * This will delete the block's buffer file if the block has\n     * not previously been closed.",
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:stop()" : null,
  "org.apache.hadoop.fs.UnresolvedLinkException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.security.Groups:getUserToGroupsMappingService()" : "* Get the groups being used to map user-to-groups.\n   * @return the groups being used to map user-to-groups.",
  "org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : "* Create a directory with the provided permission.\n   * The permission of the directory is set to be the provided permission as in\n   * setPermission, not permission{@literal &~}umask\n   *\n   * @see #create(FileSystem, Path, FsPermission)\n   *\n   * @param fs FileSystem handle\n   * @param dir the name of the directory to be created\n   * @param permission the permission of the directory\n   * @return true if the directory creation succeeds; false otherwise\n   * @throws IOException A problem creating the directories.",
  "org.apache.hadoop.util.dynamic.BindingUtils:implemented(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod[])" : "* Given a sequence of methods, verify that they are all available.\n   *\n   * @param methods methods\n   *\n   * @return true if they are all implemented",
  "org.apache.hadoop.service.ServiceOperations$ServiceListeners:add(org.apache.hadoop.service.ServiceStateChangeListener)" : "* Thread-safe addition of a new listener to the end of a list.\n     * Attempts to re-register a listener that is already registered\n     * will be ignored.\n     * @param l listener",
  "org.apache.hadoop.util.HttpExceptionUtils:throwEx(java.lang.Throwable)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:checkOutputBuffers(java.nio.ByteBuffer[])" : "* Check and ensure the buffers are of the desired length and type, direct\n   * buffers or not.\n   * @param buffers the buffers to check",
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:constructNewPath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getPrefix()" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:checkNotClosed()" : "* Verify that the input stream is open. Non blocking; this gives\n   * the last state of the volatile {@link #closed} field.\n   * @throws IOException if the connection is closed.",
  "org.apache.hadoop.metrics2.lib.MutableGaugeInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:dispatchedTimeTotal()" : null,
  "org.apache.hadoop.io.VLongWritable:toString()" : null,
  "org.apache.hadoop.fs.permission.AclEntry$Builder:setPermission(org.apache.hadoop.fs.permission.FsAction)" : "* Sets the set of permissions in the ACL entry.\n     *\n     * @param permission FsAction set of permissions in the ACL entry\n     * @return Builder this builder, for call chaining",
  "org.apache.hadoop.security.KDiag:verify(java.io.File,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)" : "* Verify that tokenFile contains valid Credentials.\n   *\n   * If not, an exception is raised, or, if {@link #nofail} is set,\n   * an error will be logged and the method return false.\n   *",
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:isFile(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)" : "* Convenience method, so that we don't open a new connection when using this\n   * method from within another method. Otherwise every API invocation incurs\n   * the overhead of opening/closing a TCP connection.\n   * @throws IOException",
  "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:isContextReset()" : null,
  "org.apache.hadoop.ipc.ProtobufHelper:tokenFromProto(org.apache.hadoop.security.proto.SecurityProtos$TokenProto)" : "* Get a token from a TokenProto payload.\n   * @param tokenProto marshalled token\n   * @return the token.",
  "org.apache.hadoop.http.lib.StaticUserWebFilter$User:<init>(java.lang.String)" : null,
  "org.apache.hadoop.security.token.Token:setService(org.apache.hadoop.io.Text)" : "* Set the service on which the token is supposed to be used.\n   * @param newService the service name",
  "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.net.URI)" : "* Add a LinkFallback to the config for the default mount table.\n   *\n   * @param conf configuration.\n   * @param target targets.",
  "org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>()" : "* Creates a new compressor with the default buffer size.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)" : null,
  "org.apache.hadoop.tools.TableListing$Column:getRow(int)" : "* Return the ith row of the column as a set of wrapped strings, each at\n     * most wrapWidth in length.",
  "org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:readFully(long,byte[],int,int)" : "Positioned read fully. It is thread-safe",
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeExpiredStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : null,
  "org.apache.hadoop.util.InstrumentedLock$SuppressedStats:incrementSuppressed(long)" : "* Increments the suppressed counter and increases the max wait time if the\n     * passed wait is greater than the current maxSuppressedWait.\n     * @param wait The wait time for this suppressed message",
  "org.apache.hadoop.util.Lists:addAll(java.util.Collection,java.lang.Iterable)" : "* Adds all elements in {@code iterable} to {@code collection}.\n   *\n   * @return {@code true} if {@code collection} was modified as a result of\n   *     this operation.",
  "org.apache.hadoop.fs.viewfs.NflyFSystem:mayThrowFileNotFound(java.util.List,int)" : null,
  "org.apache.hadoop.fs.permission.AclStatus:equals(java.lang.Object)" : null,
  "org.apache.hadoop.util.bloom.DynamicBloomFilter:not()" : null,
  "org.apache.hadoop.io.AbstractMapWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.FilterFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:createIdentifier()" : null,
  "org.apache.hadoop.net.CachedDNSToSwitchMapping:reloadCachedMappings(java.util.List)" : null,
  "org.apache.hadoop.fs.HardLink$HardLinkCGWin:linkCount(java.io.File)" : null,
  "org.apache.hadoop.fs.FSOutputSummer:write1(byte[],int,int)" : "* Write a portion of an array, flushing to the underlying\n   * stream at most once if necessary.",
  "org.apache.hadoop.ha.SshFenceByTcpPort:checkArgs(java.lang.String)" : "* Verify that the argument, if given, in the conf is parseable.",
  "org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:<init>(java.io.InputStream,int)" : "* Buffer an input stream with the chosen buffer size.\n   * @param in input stream\n   * @param size buffer size",
  "org.apache.hadoop.util.LightWeightResizableGSet:size()" : null,
  "org.apache.hadoop.fs.FilterFs:setTimes(org.apache.hadoop.fs.Path,long,long)" : null,
  "org.apache.hadoop.util.ReflectionUtils:copy(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.Object)" : "* Make a copy of the writable object using serialization to a buffer.\n   * @param src the object to copy from\n   * @param dst the object to copy into, which is destroyed\n   * @param <T> Generics Type.\n   * @param conf configuration.\n   * @return dst param (the copy)\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:setWrapped(org.apache.hadoop.fs.statistics.IOStatistics)" : "* Set the wrapped statistics.\n   * Will fail if the field is already set.\n   * @param wrapped new value",
  "org.apache.hadoop.fs.store.ByteBufferInputStream:position()" : "* Get the current buffer position.\n   * @return the buffer position",
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:fillTrailer()" : null,
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMeanStatisticSample(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:<init>()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$WrappingRemoteIterator:next()" : null,
  "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:setExcludePattern(com.google.re2j.Pattern)" : null,
  "org.apache.hadoop.util.SysInfoLinux:getAvailableVirtualMemorySize()" : "{@inheritDoc}",
  "org.apache.hadoop.security.token.Token:cancel(org.apache.hadoop.conf.Configuration)" : "* Cancel this delegation token.\n   *\n   * @param conf configuration.\n   * @throws IOException raised on errors performing I/O.\n   * @throws InterruptedException if the thread is interrupted.",
  "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:asStatic()" : "* Returns this method as a StaticMethod.\n     * @return a {@link StaticMethod} for this method\n     * @throws IllegalStateException if the method is not static",
  "org.apache.hadoop.security.User:getShortName()" : "* Get the user name up to the first '/' or '@'\n   * @return the leading part of the user name",
  "org.apache.hadoop.fs.FileSystem:processDeleteOnExit()" : "* Delete all paths that were marked as delete-on-exit. This recursively\n   * deletes all files and directories in the specified paths.\n   *\n   * The time to process this operation is {@code O(paths)}, with the actual\n   * time dependent on the time for existence and deletion operations to\n   * complete, successfully or not.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation:requiresKerberosCredentials()" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Rename files/dirs",
  "org.apache.hadoop.util.DiskChecker:getFileOutputStreamProvider()" : "* Retrieve the current {@link FileIoProvider}.\n   * This method MUST NOT be used outside of unit tests.\n   *\n   * @return the current FileIoProvider.",
  "org.apache.hadoop.io.serializer.SerializationFactory:getSerializer(java.lang.Class)" : null,
  "org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.ipc.Client$Connection$PingInputStream:read()" : "Read a byte from the stream.\n       * Send a ping if timeout on read. Retries if no failure is detected\n       * until a byte is read.\n       * @throws IOException for any IO problem other than socket timeout",
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String,java.lang.String)" : "* Validates that the expression (that checks a field is valid) is true.\n   * @param isValid indicates whether the given argument is valid.\n   * @param argName the name of the argument being validated.\n   * @param validValues the list of values that are allowed.",
  "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getKeyManagers()" : "* Returns the keymanagers for owned certificates.\n   *\n   * @return the keymanagers for owned certificates.",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDoAsUser()" : "* Get the doAs user name.\n   *\n   * 'actualUGI' is the UGI of the user creating the client\n   * It is possible that the creator of the KMSClientProvier\n   * calls this method on behalf of a proxyUser (the doAsUser).\n   * In which case this call has to be made as the proxy user.\n   *\n   * @return the doAs user name.\n   * @throws IOException",
  "org.apache.hadoop.security.token.Token$PrivateToken:isPrivateCloneOf(org.apache.hadoop.io.Text)" : "* Whether this is a private clone of a public token.\n     * @param thePublicService the public service name\n     * @return true when the public service is the same as specified",
  "org.apache.hadoop.fs.shell.FsUsage$Df:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:hasCapability(java.lang.String)" : "* If the inner stream supports {@link StreamCapabilities},\n   * forward the probe to it.\n   * Otherwise: return false.\n   *\n   * @param capability string to query the stream support for.\n   * @return true if a capability is known to be supported.",
  "org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(org.apache.hadoop.io.Writable)" : "* Get the 'value' corresponding to the last read 'key'.\n     * @param val : The 'value' to be read.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:setData(org.apache.hadoop.fs.impl.prefetch.BufferData,long,long)" : "* Associates a buffer with this file.\n   *\n   * @param bufferData the buffer associated with this file.\n   * @param startOffset Start offset of the buffer relative to the start of a file.\n   * @param readOffset Offset where reading starts relative to the start of a file.\n   *\n   * @throws IllegalArgumentException if bufferData is null.\n   * @throws IllegalArgumentException if startOffset is negative.\n   * @throws IllegalArgumentException if readOffset is negative.\n   * @throws IllegalArgumentException if readOffset is outside the range [startOffset, buffer end].",
  "org.apache.hadoop.fs.HarFileSystem:createFile(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.token.DtFileOperations:aliasTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)" : "Alias a token from a file and save back to file in the local filesystem.\n   *  @param tokenFile a local File object to hold the input and output.\n   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output\n   *  @param alias overwrite service field of fetched token with this text.\n   *  @param service only apply alias to tokens matching this service text.\n   *  @param conf Configuration object passed along.\n   *  @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.net.InnerNodeImpl$Factory:<init>()" : null,
  "org.apache.hadoop.fs.QuotaUsage$Builder:spaceConsumed(long)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:supportAutoAddingFallbackOnNoMounts()" : "* Returns false as it does not support to add fallback link automatically on\n   * no mounts.",
  "org.apache.hadoop.util.StringUtils:toStartupShutdownString(java.lang.String,java.lang.String[])" : "* Return a message for logging.\n   * @param prefix prefix keyword for the message\n   * @param msg content of the message\n   * @return a message for logging",
  "org.apache.hadoop.fs.RawPathHandle:writeObject(java.io.ObjectOutputStream)" : null,
  "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:<init>(long,long,java.util.function.Function)" : null,
  "org.apache.hadoop.net.CachedDNSToSwitchMapping:cacheResolvedHosts(java.util.List,java.util.List)" : "* Caches the resolved host:rack mappings. The two list\n   * parameters must be of equal size.\n   *\n   * @param uncachedHosts a list of hosts that were uncached\n   * @param resolvedHosts a list of resolved host entries where the element\n   * at index(i) is the resolved value for the entry in uncachedHosts[i]",
  "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:preferDirectBuffer()" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX:getFstat(java.io.FileDescriptor)" : "* Returns the file stat for a file descriptor.\n     *\n     * @param fd file descriptor.\n     * @return the file descriptor file stat.\n     * @throws IOException thrown if there was an IO error while obtaining the file stat.",
  "org.apache.hadoop.net.NetworkTopology:getWeight(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)" : "* Returns an integer weight which specifies how far away {node} is away from\n   * {reader}. A lower value signifies that a node is closer.\n   * \n   * @param reader Node where data will be read\n   * @param node Replica of data\n   * @return weight",
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.util.function.Function,java.lang.String)" : "* Construct a simple link (i.e. not a mergeLink).",
  "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getConf()" : "* @return The configuration object.",
  "org.apache.hadoop.fs.FileContext:getFsStatus(org.apache.hadoop.fs.Path)" : "* Returns a status object describing the use and capacity of the\n   * file system denoted by the Parh argument p.\n   * If the file system has multiple partitions, the\n   * use and capacity of the partition pointed to by the specified\n   * path is reflected.\n   * \n   * @param f Path for which status should be obtained. null means the\n   * root partition of the default file system. \n   *\n   * @return a FsStatus object\n   *\n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If <code>f</code> does not exist\n   * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n   *           not supported\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server",
  "org.apache.hadoop.fs.FileContext:getAllStoragePolicies()" : "* Retrieve all the storage policies supported by this file system.\n   *\n   * @return all storage policies supported by this filesystem.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.conf.Configuration:getAllPropertiesByTags(java.util.List)" : "* Get all properties belonging to list of input tags. Calls\n   * getAllPropertiesByTag internally.\n   * @param tagList list of input tags\n   * @return Properties with matching tags",
  "org.apache.hadoop.http.HttpServer2:setHeaders(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.FSDataInputStream:seek(long)" : "* Seek to the given offset.\n   *\n   * @param desired offset to seek to",
  "org.apache.hadoop.security.token.DtUtilShell$Import:execute()" : null,
  "org.apache.hadoop.log.LogLevel$CLI:process(java.lang.String)" : "* Configures the client to send HTTP/HTTPS request to the URL.\n     * Supports SPENGO for authentication.\n     * @param urlString URL and query string to the daemon's web UI\n     * @throws Exception if unable to connect",
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:checkStream()" : null,
  "org.apache.hadoop.fs.HarFileSystem:getInitialWorkingDirectory()" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider$TimeoutConnConfigurator:configure(java.net.HttpURLConnection)" : "* Calls the wrapped configure() method, then sets timeouts\n     * @param conn the {@link HttpURLConnection} instance to configure.\n     * @return the connection\n     * @throws IOException",
  "org.apache.hadoop.security.authorize.AuthorizationException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.fs.BBUploadHandle:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:fixFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:toString()" : null,
  "org.apache.hadoop.fs.permission.ChmodParser:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Metadata)" : "* Sort and merge using an arbitrary {@link RawComparator}.\n     * @param fs input FileSystem.\n     * @param comparator input RawComparator.\n     * @param keyClass input keyClass.\n     * @param valClass input valClass.\n     * @param conf input Configuration.\n     * @param metadata input metadata.",
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:locateKeystore()" : "* Open up and initialize the keyStore.\n   * @throws IOException If there is a problem reading the password file\n   * or a problem reading the keystore.",
  "org.apache.hadoop.fs.FsTracer:<init>()" : null,
  "org.apache.hadoop.util.LineReader:unsetNeedAdditionalRecordAfterSplit()" : null,
  "org.apache.hadoop.conf.Configuration:getLongBytes(java.lang.String,long)" : "* Get the value of the <code>name</code> property as a <code>long</code> or\n   * human readable format. If no such property exists, the provided default\n   * value is returned, or if the specified value is not a valid\n   * <code>long</code> or human readable format, then an error is thrown. You\n   * can use the following suffix (case insensitive): k(kilo), m(mega), g(giga),\n   * t(tera), p(peta), e(exa)\n   *\n   * @param name property name.\n   * @param defaultValue default value.\n   * @throws NumberFormatException when the value is invalid\n   * @return property value as a <code>long</code>,\n   *         or <code>defaultValue</code>.",
  "org.apache.hadoop.io.BoundedByteArrayOutputStream:resetBuffer(byte[],int,int)" : null,
  "org.apache.hadoop.metrics2.lib.MutableStat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)" : null,
  "org.apache.hadoop.security.LdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.RpcWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.FilterFs:unsetStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:isFailover()" : null,
  "org.apache.hadoop.metrics2.impl.AbstractMetricsRecord:hashCode()" : null,
  "org.apache.hadoop.ipc.CallerContext:getSignature()" : null,
  "org.apache.hadoop.fs.FileSystem:debugLogFileSystemClose(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:prepare()" : null,
  "org.apache.hadoop.ipc.RetryCache$CacheEntry:completed(boolean)" : null,
  "org.apache.hadoop.security.LdapGroupsMapping:getGroups(java.lang.String)" : "* Returns list of groups for a user.\n   * \n   * The LdapCtx which underlies the DirContext object is not thread-safe, so\n   * we need to block around this whole method. The caching infrastructure will\n   * ensure that performance stays in an acceptable range.\n   *\n   * @param user get groups for this user\n   * @return list of groups for a given user",
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:close()" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation)" : null,
  "org.apache.hadoop.fs.shell.Truncate:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getMandatoryKeys()" : "* Get all the keys that are set as mandatory keys.\n   * @return mandatory keys.",
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getPath()" : "* Get the path: only valid if constructed with a path.\n   * @return the path\n   * @throws NoSuchElementException if the field is empty.",
  "org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroups(java.lang.String)" : null,
  "org.apache.hadoop.fs.store.ByteBufferInputStream:reset()" : null,
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:createGroupExecutor(java.lang.String)" : "* Create a ShellCommandExecutor object using the user's name.\n   *\n   * @param userName user's name\n   * @return a ShellCommandExecutor object",
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.tracing.TraceUtils:spanContextToByteString(org.apache.hadoop.tracing.SpanContext)" : null,
  "org.apache.hadoop.io.EnumSetWritable:set(java.util.EnumSet,java.lang.Class)" : "* reset the EnumSetWritable with specified\n   * <tt>value</tt> and <tt>elementType</tt>. If the <tt>value</tt> argument\n   * is null or its size is zero, the <tt>elementType</tt> argument must not be\n   * null. If the argument <tt>value</tt>'s size is bigger than zero, the\n   * argument <tt>elementType</tt> is not be used.\n   * \n   * @param value enumSet Value.\n   * @param elementType elementType.",
  "org.apache.hadoop.io.Text:ensureCapacity(int)" : "* Sets the capacity of this Text object to <em>at least</em>\n   * <code>capacity</code> bytes. If the current buffer is longer, then the\n   * capacity and existing content of the buffer are unchanged. If\n   * <code>capacity</code> is larger than the current capacity, the Text\n   * object's capacity is increased to match and any existing data is lost.\n   *\n   * @param capacity the number of bytes we need\n   * @return true if the internal array was resized or false otherwise",
  "org.apache.hadoop.security.ShellBasedIdMapping:updateMapIncr(java.lang.String,boolean)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:<init>(java.util.List,java.util.List,java.util.List,java.util.List,java.util.List)" : "* Constructor invoked via the builder.\n   * @param counters keys to use for the counter statistics.\n   * @param gauges names of gauges\n   * @param minimums names of minimums\n   * @param maximums names of maximums\n   * @param meanStatistics names of mean statistics.",
  "org.apache.hadoop.io.file.tfile.MetaBlockDoesNotExist:<init>(java.lang.String)" : "* Constructor\n   * \n   * @param s\n   *          message.",
  "org.apache.hadoop.conf.Configuration:getClassLoader()" : "* Get the {@link ClassLoader} for this job.\n   *\n   * @return the correct class loader.",
  "org.apache.hadoop.fs.Globber$GlobBuilder:withResolveSymlinks(boolean)" : "* Set the symlink resolution policy.\n     * @param resolve resolution flag.\n     * @return the builder",
  "org.apache.hadoop.ipc.RpcClientUtil$ProtoSigCacheKey:hashCode()" : null,
  "org.apache.hadoop.fs.shell.Command:getCommandFactory()" : "* retrieves the command factory.\n   *\n   * @return command factory.",
  "org.apache.hadoop.fs.http.HttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)" : "* Declare that this filesystem connector is always read only.\n   * {@inheritDoc}",
  "org.apache.hadoop.fs.FilterFileSystem:supportsSymlinks()" : null,
  "org.apache.hadoop.security.UserGroupInformation:executeAutoRenewalTask(java.lang.String,org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable)" : "* Spawn a thread to do periodic renewals of kerberos credentials from a\n   * keytab file. NEVER directly call this method.\n   *\n   * @param userName Name of the user for which login needs to be renewed.\n   * @param task  The reference of the login renewal task.",
  "org.apache.hadoop.fs.FsUrlConnection:<init>(org.apache.hadoop.conf.Configuration,java.net.URL)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:open(org.apache.hadoop.fs.Path,int)" : "* Opens an FSDataInputStream at the indicated Path.\n   * @param f the file name to open\n   * @param bufferSize the size of the buffer to be used.\n   * @throws IOException if an I/O error occurs.",
  "org.apache.hadoop.crypto.key.kms.ValueQueue:indexFor(java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem$DirectoryEntries:hasMore()" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,int)" : "* Set optional int parameter for the Builder.\n   *\n   * @see #opt(String, String)",
  "org.apache.hadoop.io.SequenceFile$CompressedBytes:<init>(org.apache.hadoop.io.compress.CompressionCodec)" : null,
  "org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:<init>(int)" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:convertToExitException(java.lang.Throwable)" : "* Convert an exception to an {@code ExitException}.\n   *\n   * This process may just be a simple pass through, otherwise a new\n   * exception is created with an exit code, the text of the supplied\n   * exception, and the supplied exception as an inner cause.\n   * \n   * <ol>\n   *   <li>If is already the right type, pass it through.</li>\n   *   <li>If it implements {@link ExitCodeProvider#getExitCode()},\n   *   the exit code is extracted and used in the new exception.</li>\n   *   <li>Otherwise, the exit code\n   *   {@link LauncherExitCodes#EXIT_EXCEPTION_THROWN} is used.</li>\n   * </ol>\n   *  \n   * @param thrown the exception thrown\n   * @return an {@code ExitException} with a status code",
  "org.apache.hadoop.conf.Configuration:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getReplication()" : null,
  "org.apache.hadoop.util.Options$PathOption:getValue()" : null,
  "org.apache.hadoop.util.dynamic.DynMethods:<init>()" : null,
  "org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.apache.hadoop.security.UserGroupInformation)" : "* Log all (current, real, login) UGI and token info into UGI debug log.\n   * @param ugi - UGI\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.net.CachedDNSToSwitchMapping:<init>(org.apache.hadoop.net.DNSToSwitchMapping)" : "* cache a raw DNS mapping\n   * @param rawMapping the raw mapping to cache",
  "org.apache.hadoop.fs.FileStatus:<init>(org.apache.hadoop.fs.FileStatus)" : "* Copy constructor.\n   *\n   * @param other FileStatus to copy\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.BlockLocation:setCachedHosts(java.lang.String[])" : "* Set the hosts hosting a cached replica of this block.\n   * @param cachedHosts cached hosts.",
  "org.apache.hadoop.security.token.DtUtilShell$Append:getUsage()" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater:<init>()" : null,
  "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:blockRemovedFromFileCache()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String)" : "* Util method to build socket addr from either.\n   *   {@literal <host>:<port>}\n   *   {@literal <fs>://<host>:<port>/<path>}\n   *\n   * @param target target.\n   * @return socket addr.",
  "org.apache.hadoop.fs.FileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : "* Set owner of a path (i.e. a file or a directory).\n   * The parameters username and groupname cannot both be null.\n   * @param p The path\n   * @param username If it is null, the original username remains unchanged.\n   * @param groupname If it is null, the original groupname remains unchanged.\n   * @throws IOException IO failure",
  "org.apache.hadoop.conf.Configuration:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)" : "* Gets storage size from a config file.\n   *\n   * @param name - Key to read.\n   * @param defaultValue - The default value to return in case the key is\n   * not present.\n   * @param targetUnit - The Storage unit that should be used\n   * for the return value.\n   * @return - double value in the Storage Unit specified.",
  "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:setDateHeader(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getWorkingDirectory()" : "* Get the current working directory for the given file system\n   * \n   * @return the directory pathname",
  "org.apache.hadoop.ipc.Server$Listener:run()" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Writer:close()" : "* Close the BCFile Writer. Attempting to use the Writer after calling\n     * <code>close</code> is not allowed and may lead to undetermined results.",
  "org.apache.hadoop.fs.ChecksumFs:getBytesPerSum()" : "* Return the bytes Per Checksum.\n   *\n   * @return bytes per sum.",
  "org.apache.hadoop.http.HttpServer2:getFilterProperties(org.apache.hadoop.conf.Configuration,java.util.List)" : null,
  "org.apache.hadoop.fs.http.HttpsFileSystem:open(org.apache.hadoop.fs.Path,int)" : null,
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:isLastChunk()" : "* Have we reached the last chunk.\n     * \n     * @return true if we have reached the last chunk.\n     * @throws java.io.IOException",
  "org.apache.hadoop.io.DataOutputBuffer:writeInt(int,int)" : "* Overwrite an integer into the internal buffer. Note that this call can only\n   * be used to overwrite existing data in the buffer, i.e., buffer#count cannot\n   * be increased, and DataOutputStream#written cannot be increased.\n   *\n   * @param v v.\n   * @param offset offset.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:setInput(byte[],int,int)" : null,
  "org.apache.hadoop.util.Timer:monotonicNow()" : "* Current time from some arbitrary time base in the past, counting in\n   * milliseconds, and not affected by settimeofday or similar system clock\n   * changes.  This is appropriate to use when computing how much longer to\n   * wait for an interval to expire.\n   * @return a monotonic clock that counts in milliseconds.",
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfInvertMatrix(byte[],byte[],int)" : "* Invert a matrix assuming it's invertible.\n   *\n   * Ported from Intel ISA-L library.\n   *\n   * @param inMatrix inMatrix.\n   * @param outMatrix outMatrix.\n   * @param n n",
  "org.apache.hadoop.fs.FileContext:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : "* Removes ACL entries from files and directories.  Other ACL entries are\n   * retained.\n   *\n   * @param path Path to modify\n   * @param aclSpec List{@literal <}AclEntry{@literal >} describing entries\n   * to remove\n   * @throws IOException if an ACL could not be modified",
  "org.apache.hadoop.util.functional.TaskPool:foreach(java.lang.Iterable)" : "* Create a task builder for the iterable.\n   * @param items item source.\n   * @param <I> type of result.\n   * @return builder.",
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:generateEncryptedKey(org.apache.hadoop.crypto.Encryptor,org.apache.hadoop.crypto.key.KeyProvider$KeyVersion,byte[],byte[])" : null,
  "org.apache.hadoop.fs.FileSystem:openFile(org.apache.hadoop.fs.Path)" : "* Open a file for reading through a builder API.\n   * Ultimately calls {@link #open(Path, int)} unless a subclass\n   * executes the open command differently.\n   *\n   * The semantics of this call are therefore the same as that of\n   * {@link #open(Path, int)} with one special point: it is in\n   * {@code FSDataInputStreamBuilder.build()} in which the open operation\n   * takes place -it is there where all preconditions to the operation\n   * are checked.\n   * @param path file path\n   * @return a FSDataInputStreamBuilder object to build the input stream\n   * @throws IOException if some early checks cause IO failures.\n   * @throws UnsupportedOperationException if support is checked early.",
  "org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String)" : "* Get the value of the <code>name</code> property as a trimmed <code>String</code>, \n   * <code>null</code> if no such property exists. \n   * If the key is deprecated, it returns the value of\n   * the first key which replaces the deprecated key and is not null\n   * \n   * Values are processed for <a href=\"#VariableExpansion\">variable expansion</a> \n   * before being returned. \n   * \n   * @param name the property name.\n   * @return the value of the <code>name</code> or its replacing property, \n   *         or null if no such property exists.",
  "org.apache.hadoop.crypto.CryptoInputStream:read(byte[],int,int)" : "* Decryption is buffer based.\n   * If there is data in {@link #outBuffer}, then read it out of this buffer.\n   * If there is no data in {@link #outBuffer}, then read more from the \n   * underlying stream and do the decryption.\n   * @param b the buffer into which the decrypted data is read.\n   * @param off the buffer offset.\n   * @param len the maximum number of decrypted data bytes to read.\n   * @return int the total number of decrypted data bytes read into the buffer.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(org.apache.hadoop.metrics2.MetricsSystem$Callback)" : null,
  "org.apache.hadoop.util.Shell:addOsText(java.lang.String)" : "* Optionally extend an error message with some OS-specific text.\n   * @param message core error message\n   * @return error message, possibly with some extra text",
  "org.apache.hadoop.fs.Options$ChecksumOpt:createDisabled()" : "* Create a ChecksumOpts that disables checksum.\n     *\n     * @return ChecksumOpt.",
  "org.apache.hadoop.crypto.key.KeyShell$ListCommand:validate()" : null,
  "org.apache.hadoop.io.ShortWritable:<init>()" : null,
  "org.apache.hadoop.fs.Trash:<init>(org.apache.hadoop.conf.Configuration)" : "* Construct a trash can accessor.\n   * @param conf a Configuration\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.LdapGroupsMapping:getConf()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getFuturePool()" : "* @return The Executor future pool to perform async prefetch tasks.",
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:setSymlink(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isSymlink()" : null,
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:parsePartialGroupNames(java.lang.String,java.lang.String)" : "* Attempt to parse group names given that some names are not resolvable.\n   * Use the group id list to identify those that are not resolved.\n   *\n   * @param groupNames a string representing a list of group names\n   * @param groupIDs a string representing a list of group ids\n   * @return a linked list of group names\n   * @throws PartialGroupNameException",
  "org.apache.hadoop.util.bloom.Key:set(byte[],double)" : "* @param value value.\n   * @param weight weight.",
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:gauges()" : null,
  "org.apache.hadoop.metrics2.util.SampleStat:max()" : "* @return  the maximum value of the samples",
  "org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.ipc.ProtocolSignature:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping$PassThroughMap:<init>(java.util.Map)" : null,
  "org.apache.hadoop.util.bloom.BloomFilter:toString()" : null,
  "org.apache.hadoop.ipc.UnexpectedServerException:<init>(java.lang.String)" : "* Constructs exception with the specified detail message.\n   * \n   * @param messages detailed message.",
  "org.apache.hadoop.fs.shell.Delete$Rmdir:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.HarFileSystem:setReplication(org.apache.hadoop.fs.Path,short)" : "* Not implemented.",
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$CallInfo:getServer()" : null,
  "org.apache.hadoop.fs.sftp.SFTPConnectionPool:<init>(int)" : null,
  "org.apache.hadoop.service.AbstractService:<init>(java.lang.String)" : "* Construct the service.\n   * @param name service name",
  "org.apache.hadoop.fs.DU$DUShell:parseExecResult(java.io.BufferedReader)" : null,
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMinimum(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.store.ByteBufferInputStream:skip(long)" : null,
  "org.apache.hadoop.io.WritableUtils:writeString(java.io.DataOutput,java.lang.String)" : null,
  "org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)" : "* Construct a client-side proxy that implements the named protocol,\n   * talking to a server at the named address.\n   *\n   * @param <T> Generics Type T.\n   * @param protocol protocol\n   * @param clientVersion client's version\n   * @param addr server address\n   * @param ticket security ticket\n   * @param conf configuration\n   * @param factory socket factory\n   * @param rpcTimeout max time for each rpc; 0 means no timeout\n   * @return the proxy\n   * @throws IOException if any error occurs",
  "org.apache.hadoop.fs.shell.PathData:normalizeWindowsPath(java.lang.String)" : "Normalize the given Windows path string. This does the following:\n   *    1. Adds \"file:\" scheme for absolute paths.\n   *    2. Ensures the scheme-specific part starts with '/' per RFC2396.\n   *    3. Replaces backslash path separators with forward slashes.\n   *    @param pathString Path string supplied by the user.\n   *    @return normalized absolute path string. Returns the input string\n   *            if it is not a Windows absolute path.",
  "org.apache.hadoop.util.ExitUtil:haltOnOutOfMemory(java.lang.OutOfMemoryError)" : "* Handler for out of memory events -no attempt is made here\n   * to cleanly shutdown or support halt blocking; a robust\n   * printing of the event to stderr is all that can be done.\n   * @param oome out of memory event",
  "org.apache.hadoop.io.ObjectWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.security.SaslInputStream:unsignedBytesToInt(byte[])" : null,
  "org.apache.hadoop.ipc.ProtocolSignature$1:<init>()" : "* default constructor",
  "org.apache.hadoop.ha.ActiveStandbyElector:tryDeleteOwnBreadCrumbNode()" : "* Try to delete the \"ActiveBreadCrumb\" node when gracefully giving up\n   * active status.\n   * If this fails, it will simply warn, since the graceful release behavior\n   * is only an optimization.",
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:totalPartsLen(java.util.List)" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:setData(java.lang.String,byte[],int)" : "* Set data into a ZNode.\n   * @param path Path of the ZNode.\n   * @param data Data to set.\n   * @param version Version of the data to store.\n   * @throws Exception If it cannot contact Zookeeper.",
  "org.apache.hadoop.util.LimitInputStream:reset()" : null,
  "org.apache.hadoop.ipc.ExternalCall:run()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.service.AbstractService:getFailureState()" : null,
  "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:getAcceptedIssuers()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:getEmptyChunk(int)" : "* Make sure to return an empty chunk buffer for the desired length.\n   * @param leastLength\n   * @return empty chunk of zero bytes",
  "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:resolve(java.util.List)" : null,
  "org.apache.hadoop.fs.FSBuilder:must(java.lang.String,boolean)" : "* Set mandatory boolean option.\n   *\n   * @param key key.\n   * @param value value.\n   * @return generic type B.\n   * @see #must(String, String)",
  "org.apache.hadoop.security.http.XFrameOptionsFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)" : null,
  "org.apache.hadoop.fs.FileStatus:getLen()" : "* Get the length of this file, in bytes.\n   * @return the length of this file, in bytes.",
  "org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:writeTo(org.apache.hadoop.ipc.ResponseBuffer)" : null,
  "org.apache.hadoop.tracing.TraceScope:addKVAnnotation(java.lang.String,java.lang.Number)" : null,
  "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:create(java.lang.String)" : null,
  "org.apache.hadoop.security.KDiag:dumpUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation)" : "* Dump a UGI.\n   *\n   * @param title title of this section\n   * @param ugi UGI to dump\n   * @throws IOException",
  "org.apache.hadoop.security.UserGroupInformation:setConfiguration(org.apache.hadoop.conf.Configuration)" : "* Set the static configuration for UGI.\n   * In particular, set the security authentication mechanism and the\n   * group look up service.\n   * @param conf the configuration to use",
  "org.apache.hadoop.service.ServiceStateException:<init>(int,java.lang.String,java.lang.Throwable)" : "* Instantiate, using the specified exit code as the exit code\n   * of the exception, irrespetive of any exit code supplied by any inner\n   * cause.\n   *\n   * @param exitCode exit code to declare\n   * @param message exception message\n   * @param cause inner cause",
  "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:addDateHeader(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.FsStatus:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:compareTo(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode)" : null,
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:seekToNewSource(long)" : null,
  "org.apache.hadoop.util.HeapSort:downHeap(org.apache.hadoop.util.IndexedSortable,int,int,int)" : null,
  "org.apache.hadoop.ipc.Server$Connection:setupHttpRequestOnIpcPortResponse()" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:storeNewMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])" : null,
  "org.apache.hadoop.io.file.tfile.CompareUtils$ScalarComparator:compare(org.apache.hadoop.io.file.tfile.CompareUtils$Scalar,org.apache.hadoop.io.file.tfile.CompareUtils$Scalar)" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:<init>()" : null,
  "org.apache.hadoop.fs.PartHandle:toByteArray()" : "* @return Serialized from in bytes.",
  "org.apache.hadoop.fs.shell.FsUsage:getUsagesTable()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:setConnectionConfigurator(org.apache.hadoop.security.authentication.client.ConnectionConfigurator)" : null,
  "org.apache.hadoop.io.SequenceFile:<init>()" : null,
  "org.apache.hadoop.ipc.FairCallQueue:signalNotEmpty()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])" : "* The specification of this method matches that of\n   * {@link FileContext#create(Path, EnumSet, Options.CreateOpts...)} except\n   * that the Path f must be fully qualified and the permission is absolute\n   * (i.e. umask has been applied).\n   *\n   * @param f the path.\n   * @param createFlag create_flag.\n   * @param opts create ops.\n   * @throws AccessControlException access controll exception.\n   * @throws FileAlreadyExistsException file already exception.\n   * @throws FileNotFoundException file not found exception.\n   * @throws ParentNotDirectoryException parent not dir exception.\n   * @throws UnsupportedFileSystemException unsupported file system exception.\n   * @throws UnresolvedLinkException unresolved link exception.\n   * @throws IOException raised on errors performing I/O.\n   * @return output stream.",
  "org.apache.hadoop.fs.PathIOException:getMessage()" : "Format:\n   * cmd: {operation} `path' {to `target'}: error string",
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:metricName(java.lang.String,int)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesReadDistanceOfOneOrTwo()" : null,
  "org.apache.hadoop.net.DNS:getHosts(java.lang.String,java.lang.String,boolean)" : "* Returns all the host names associated by the provided nameserver with the\n   * address bound to the specified network interface\n   *\n   * @param strInterface\n   *            The name of the network interface or subinterface to query\n   *            (e.g. eth0 or eth0:0)\n   * @param nameserver\n   *            The DNS host name\n   * @param tryfallbackResolution\n   *            if true and if reverse DNS resolution fails then attempt to\n   *            resolve the hostname with\n   *            {@link InetAddress#getCanonicalHostName()} which includes\n   *            hosts file resolution.\n   * @return A string vector of all host names associated with the IPs tied to\n   *         the specified interface\n   * @throws UnknownHostException if the given interface is invalid",
  "org.apache.hadoop.util.GcTimeMonitor$GcData:getGcTimePercentage()" : "* Returns the percentage (0..100) of time that the JVM spent in GC pauses\n     * within the observation window of the associated GcTimeMonitor.\n     *\n     * @return GcTimePercentage.",
  "org.apache.hadoop.crypto.key.UserProvider:flush()" : null,
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.util.concurrent.Callable)" : null,
  "org.apache.hadoop.http.HttpServer2:constructSecretProvider(org.apache.hadoop.http.HttpServer2$Builder,javax.servlet.ServletContext)" : null,
  "org.apache.hadoop.io.DataOutputOutputStream:write(int)" : null,
  "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeDefaultFactory(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode)" : "* Initialize a singleton SSL socket factory.\n   *\n   * @param preferredMode applicable only if the instance is not initialized.\n   * @throws IOException if an error occurs.",
  "org.apache.hadoop.io.retry.RetryPolicies:retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)" : "* <p>\n   * A retry policy for RemoteException\n   * Set a default policy with some explicit handlers for specific exceptions.\n   * </p>\n   *\n   * @param defaultPolicy defaultPolicy.\n   * @param exceptionToPolicyMap exceptionToPolicyMap.\n   * @return RetryPolicy.",
  "org.apache.hadoop.security.SaslRpcServer$AuthMethod:getMechanismName()" : "* Return the SASL mechanism name.\n     * @return mechanismName.",
  "org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)" : "* Create a {@link CompressionInputStream} that will read from the given\n   * {@link InputStream} with the given {@link Decompressor}, and return a \n   * stream for uncompressed data.\n   *\n   * @param in           the stream to read compressed bytes from\n   * @param decompressor decompressor to use\n   * @return a stream to read uncompressed bytes from\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.service.launcher.IrqHandler:handle(sun.misc.Signal)" : "* Handler for the JVM API for signal handling.\n   * @param s signal raised",
  "org.apache.hadoop.net.SocksSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)" : null,
  "org.apache.hadoop.security.token.DtFileOperations:renewTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)" : "Renew a token from a file in the local filesystem, matching alias.\n   *  @param tokenFile a local File object.\n   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output\n   *  @param alias renew only tokens matching alias; null matches all.\n   *  @param conf Configuration object passed along.\n   *  @throws IOException raised on errors performing I/O.\n   *  @throws InterruptedException if the thread is interrupted.",
  "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:close()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:<init>()" : null,
  "org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.FileSystem:deleteOnExit(org.apache.hadoop.fs.Path)" : "* Mark a path to be deleted when its FileSystem is closed.\n   * When the JVM shuts down cleanly, all cached FileSystem objects will be\n   * closed automatically. These the marked paths will be deleted as a result.\n   *\n   * If a FileSystem instance is not cached, i.e. has been created with\n   * {@link #createFileSystem(URI, Configuration)}, then the paths will\n   * be deleted in when {@link #close()} is called on that instance.\n   *\n   * The path must exist in the filesystem at the time of the method call;\n   * it does not have to exist at the time of JVM shutdown.\n   *\n   * Notes\n   * <ol>\n   *   <li>Clean shutdown of the JVM cannot be guaranteed.</li>\n   *   <li>The time to shut down a FileSystem will depends on the number of\n   *   files to delete. For filesystems where the cost of checking\n   *   for the existence of a file/directory and the actual delete operation\n   *   (for example: object stores) is high, the time to shutdown the JVM can be\n   *   significantly extended by over-use of this feature.</li>\n   *   <li>Connectivity problems with a remote filesystem may delay shutdown\n   *   further, and may cause the files to not be deleted.</li>\n   * </ol>\n   * @param f the path to delete.\n   * @return  true if deleteOnExit is successful, otherwise false.\n   * @throws IOException IO failure",
  "org.apache.hadoop.util.ExitUtil:halt(int,java.lang.Throwable)" : "* Forcibly terminates the currently running Java virtual machine.\n   *\n   * @param status exit code to use if the exception is not a HaltException.\n   * @param t throwable which triggered the termination. If this exception\n   * is a {@link HaltException} its status overrides that passed in.\n   * @throws HaltException if {@link System#exit(int)}  is disabled.",
  "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:hasCapacity(long)" : null,
  "org.apache.hadoop.io.erasurecode.ErasureCodeNative:checkNativeCodeLoaded()" : "* Is the native ISA-L library loaded and initialized? Throw exception if not.",
  "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(org.apache.hadoop.metrics2.MetricsTag)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setRenewer(org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:<init>(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.conf.Configuration)" : "* @param compressionAlgo\n       *          The compression algorithm to be used to for compression.\n       * @throws IOException",
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:scheduleFlush(java.util.Date)" : "* Schedule the current interval's directory to be flushed. If this ends up\n   * running after the top of the next interval, it will execute immediately.\n   *\n   * @param when the time the thread should run",
  "org.apache.hadoop.crypto.key.KeyProviderExtension:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)" : null,
  "org.apache.hadoop.util.bloom.CountingBloomFilter:add(org.apache.hadoop.util.bloom.Key)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileChecksum(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.LdapGroupsMapping:cacheGroupsRefresh()" : "* Caches groups, no need to do that for this provider",
  "org.apache.hadoop.tools.TableListing$Column:<init>(java.lang.String,org.apache.hadoop.tools.TableListing$Justification,boolean)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,byte[][],int[],byte[][])" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getOwner()" : null,
  "org.apache.hadoop.fs.shell.Ls$Lsr:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getWorkingDirectory()" : null,
  "org.apache.hadoop.fs.LocalFileSystemPathHandle:toString()" : null,
  "org.apache.hadoop.fs.Options$CreateOpts$CreateParent:getValue()" : null,
  "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,int)" : "* Create a line reader that reads from the given stream using the \n   * given buffer-size.\n   * @param in The input stream\n   * @param bufferSize Size of the read buffer",
  "org.apache.hadoop.io.WritableUtils:writeVLong(java.io.DataOutput,long)" : "* Serializes a long to a binary stream with zero-compressed encoding.\n   * For -112 {@literal <=} i {@literal <=} 127, only one byte is used with the\n   * actual value.\n   * For other values of i, the first byte value indicates whether the\n   * long is positive or negative, and the number of bytes that follow.\n   * If the first byte value v is between -113 and -120, the following long\n   * is positive, with number of bytes that follow are -(v+112).\n   * If the first byte value v is between -121 and -128, the following long\n   * is negative, with number of bytes that follow are -(v+120). Bytes are\n   * stored in the high-non-zero-byte-first order.\n   * \n   * @param stream Binary output stream\n   * @param i Long to be serialized\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)" : null,
  "org.apache.hadoop.util.bloom.Filter:add(org.apache.hadoop.util.bloom.Key[])" : "* Adds an array of keys to <i>this</i> filter.\n   * @param keys The array of keys.",
  "org.apache.hadoop.fs.http.HttpFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping:parseStaticMap(java.io.File)" : null,
  "org.apache.hadoop.util.ExitUtil:<init>()" : null,
  "org.apache.hadoop.fs.QuotaUsage:getQuotaUsage(boolean)" : null,
  "org.apache.hadoop.metrics2.sink.StatsDSink:init(org.apache.commons.configuration2.SubsetConfiguration)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(byte[][],byte[][])" : "* Encode with inputs and generates outputs. More see above.\n   *\n   * @param inputs input buffers to read data from\n   * @param outputs output buffers to put the encoded data into, read to read\n   *                after the call\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.BytesWritable:set(byte[],int,int)" : "* Set the value to a copy of the given byte range.\n   *\n   * @param newData the new values to copy in\n   * @param offset the offset in newData to start at\n   * @param length the number of bytes to copy",
  "org.apache.hadoop.security.alias.CredentialProviderFactory:getProviders(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheCleared()" : null,
  "org.apache.hadoop.crypto.random.OsSecureRandom:fillReservoir(int)" : null,
  "org.apache.hadoop.fs.shell.TouchCommands$Touch:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)" : null,
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping$PartialGroupNameException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.conf.Configuration:setSocketAddr(java.lang.String,java.net.InetSocketAddress)" : "* Set the socket address for the <code>name</code> property as\n   * a <code>host:port</code>.\n   * @param name property name.\n   * @param addr inetSocketAddress addr.",
  "org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)" : null,
  "org.apache.hadoop.ipc.ProtobufHelper:getFixedByteString(java.lang.String)" : "* Get the ByteString for frequently used fixed and small set strings.\n   * @param key string\n   * @return ByteString for frequently used fixed and small set strings.",
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,int)" : "* Set mandatory int option.\n   *\n   * @see #must(String, String)",
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionGauge(java.lang.String,java.util.function.ToLongFunction)" : "* Add a new evaluator to the gauge statistics.\n   * @param key key of this statistic\n   * @param eval evaluator for the statistic\n   * @return the builder.",
  "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:<init>(int,int,long,java.util.concurrent.TimeUnit,java.util.concurrent.BlockingQueue,java.util.concurrent.ThreadFactory,java.util.concurrent.RejectedExecutionHandler)" : null,
  "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:resetDefaultFactory()" : "* For testing only: reset the socket factory.",
  "org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)" : "* Sort nodes array by network distance to <i>reader</i>.\n   * <p>\n   * In a three-level topology, a node can be either local, on the same rack,\n   * or on a different rack from the reader. Sorting the nodes based on network\n   * distance from the reader reduces network traffic and improves\n   * performance.\n   * <p>\n   * As an additional twist, we also randomize the nodes at each network\n   * distance. This helps with load balancing when there is data skew.\n   *\n   * @param reader    Node where data will be read\n   * @param nodes     Available replicas with the requested data\n   * @param activeLen Number of active nodes at the front of the array",
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:putPart(org.apache.hadoop.fs.UploadHandle,int,org.apache.hadoop.fs.Path,java.io.InputStream,long)" : null,
  "org.apache.hadoop.fs.shell.FsUsage$Df:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:needsInput()" : null,
  "org.apache.hadoop.security.User:hashCode()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:endCompression()" : null,
  "org.apache.hadoop.conf.ReconfigurationServlet:applyChanges(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable,javax.servlet.http.HttpServletRequest)" : "* Apply configuratio changes after admin has approved them.",
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:readFully(long,byte[],int,int)" : "* position readable again.",
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:getBytesRead()" : null,
  "org.apache.hadoop.security.LdapGroupsMapping:loadSslConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.net.SocketOutputStream:waitForWritable()" : "* waits for the underlying channel to be ready for writing.\n   * The timeout specified for this stream applies to this wait.\n   *\n   * @throws SocketTimeoutException \n   *         if select on the channel times out.\n   * @throws IOException\n   *         if any other I/O error occurs.",
  "org.apache.hadoop.util.LightWeightGSet$Values:iterator()" : null,
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>(int,int,int)" : "* Constructor\n   * @param vectorSize The vector size of <i>this</i> filter.\n   * @param nbHash The number of hash function to consider.\n   * @param hashType type of the hashing function (see\n   * {@link org.apache.hadoop.util.hash.Hash}).",
  "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:setDatagramSocket(java.net.DatagramSocket)" : "* Used only by unit test\n   * @param datagramSocket the datagramSocket to set.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)" : "* Renews a delegation token from the server end-point using the\n   * configured <code>Authenticator</code> for authentication.\n   *\n   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are\n   * supported.\n   * @param token the authentication token with the Delegation Token to renew.\n   * @param doAsUser the user to do as, which will be the token owner.\n   * @param dToken abstract delegation token identifier.\n   * @throws IOException if an IO error occurred.\n   * @throws AuthenticationException if an authentication exception occurred.\n   * @return delegation token long value.",
  "org.apache.hadoop.conf.Configuration:addResource(java.lang.String,boolean)" : null,
  "org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:<init>(java.lang.Object,java.lang.String)" : null,
  "org.apache.hadoop.security.alias.LocalKeyStoreProvider:flush()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:dynamicIOStatistics()" : "* Create a builder for dynamic IO Statistics.\n   * @return a builder to be completed.",
  "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:getCipherSuite()" : null,
  "org.apache.hadoop.security.UserGroupInformation$TestingGroups:getGroupsSet(java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:<init>(int,long)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>(org.apache.hadoop.fs.FileSystem,java.net.URI)" : "* Constructor\n   * @param fs base file system\n   * @param uri base uri\n   * @throws IOException",
  "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getCodecOptions()" : "* Get a {@link ErasureCodecOptions}.\n   * @return erasure codec options",
  "org.apache.hadoop.ipc.Server:setTracer(org.apache.hadoop.tracing.Tracer)" : null,
  "org.apache.hadoop.fs.impl.AbstractMultipartUploader:close()" : "* Perform any cleanup.\n   * The upload is not required to support any operations after this.\n   * @throws IOException problems on close.",
  "org.apache.hadoop.metrics2.util.SampleStat:min()" : "* @return  the minimum value of the samples",
  "org.apache.hadoop.net.unix.DomainSocket:getInputStream()" : "* @return                 The socket InputStream",
  "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getConf()" : null,
  "org.apache.hadoop.conf.Configuration:updatePropertiesWithDeprecatedKeys(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String[])" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:parseCostProvider(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)" : "* Construct a client-side proxy object with the default SocketFactory.\n    *\n    * @param <T> Generics Type T.\n    * @param protocol input protocol.\n    * @param clientVersion input clientVersion.\n    * @param addr input addr.\n    * @param conf input Configuration.\n    * @return a proxy instance\n    * @throws IOException  if the thread is interrupted.",
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setCounter(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.store.ByteBufferInputStream:<init>(int,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.ipc.Server$Connection:setShouldClose()" : null,
  "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:getStatus()" : "* Get any status set in {@link #withFileStatus(FileStatus)}.\n   * @return a status value or null.",
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:reset()" : "* Resets everything, including the input buffer, regardless of whether the\n   * current gzip substream is finished.",
  "org.apache.hadoop.metrics2.util.Metrics2Util$TopN:updateTotal(long)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:mkOneDir(java.io.File)" : null,
  "org.apache.hadoop.fs.Globber:createGlobber(org.apache.hadoop.fs.FileSystem)" : "* Create a builder for a Globber, bonded to the specific filesystem.\n   * @param filesystem filesystem\n   * @return the builder to finish configuring.",
  "org.apache.hadoop.io.MapFile$Reader:readIndex()" : null,
  "org.apache.hadoop.ipc.Server$Responder:decPending()" : null,
  "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:reset(java.lang.Iterable)" : null,
  "org.apache.hadoop.fs.FileUtil:addPermissions(java.util.Set,int,java.nio.file.attribute.PosixFilePermission,java.nio.file.attribute.PosixFilePermission,java.nio.file.attribute.PosixFilePermission)" : "* Assign the original permissions to the file\n   * @param permissions The original permissions for files are stored in collections\n   * @param mode Use a value of type int to indicate permissions\n   * @param r Read permission\n   * @param w Write permission\n   * @param x Execute permission",
  "org.apache.hadoop.ipc.RpcWritable$Buffer:<init>()" : null,
  "org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)" : "* Create a {@link BlockDecompressorStream}.\n   * \n   * @param in input stream\n   * @param decompressor decompressor to use\n   * @param bufferSize size of buffer\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.permission.FsAction:getFsAction(java.lang.String)" : "* Get the FsAction enum for String representation of permissions\n   * \n   * @param permission\n   *          3-character string representation of permission. ex: rwx\n   * @return Returns FsAction enum if the corresponding FsAction exists for permission.\n   *         Otherwise returns null",
  "org.apache.hadoop.fs.shell.find.Find:applyItem(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.io.WritableComparator:get(java.lang.Class)" : "* For backwards compatibility.\n   *\n   * @param c WritableComparable Type.\n   * @return WritableComparator.",
  "org.apache.hadoop.conf.Configuration:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)" : "* Sets Storage Size for the specified key.\n   *\n   * @param name - Key to set.\n   * @param value - The numeric value to set.\n   * @param unit - Storage Unit to be used.",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:setInputFromSavedData()" : null,
  "org.apache.hadoop.io.SortedMapWritable:hashCode()" : null,
  "org.apache.hadoop.ipc.FairCallQueue:size()" : "* Size returns the sum of all sub-queue sizes, so it may be greater than\n   * capacity.\n   * Note: size provides no strict consistency, and should not be used to\n   * control queue IO.",
  "org.apache.hadoop.io.AbstractMapWritable:getId(java.lang.Class)" : "* get id.\n   * @return the id for the specified Class.\n   * @param clazz clazz.",
  "org.apache.hadoop.crypto.key.KeyShell:printException(java.lang.Exception)" : null,
  "org.apache.hadoop.ipc.RetryCache:newEntry(java.lang.Object,long,byte[],int)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:stop()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:<init>(int,int,long,java.util.concurrent.TimeUnit,java.util.concurrent.BlockingQueue,java.util.concurrent.ThreadFactory)" : null,
  "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:put(java.lang.Object,java.lang.Object)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getAllKeys()" : null,
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.Server$Call:isResponseDeferred()" : null,
  "org.apache.hadoop.security.token.DtUtilShell$Renew:execute()" : null,
  "org.apache.hadoop.fs.Path:depth()" : "* Returns the number of elements in this path.\n   * @return the number of elements in this path",
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:constructRpcRequestHeader(java.lang.reflect.Method)" : null,
  "org.apache.hadoop.util.bloom.DynamicBloomFilter:and(org.apache.hadoop.util.bloom.Filter)" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:createValueBytes()" : null,
  "org.apache.hadoop.io.ObjectWritable:tryInstantiateProtobuf(java.lang.Class,java.io.DataInput)" : "* Try to instantiate a protocol buffer of the given message class\n   * from the given input stream.\n   * \n   * @param protoClass the class of the generated protocol buffer\n   * @param dataIn the input stream to read from\n   * @return the instantiated Message instance\n   * @throws IOException if an IO problem occurs",
  "org.apache.hadoop.fs.permission.PermissionStatus:getGroupName()" : "* Return group name.\n   * @return group name.",
  "org.apache.hadoop.security.ProviderUtils:noPasswordWarning(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.util.ExitUtil:resetFirstHaltException()" : "* Reset the tracking of process termination. This is for use in unit tests\n   * where one test in the suite expects a halt but others do not.",
  "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.net.URI)" : "* Add a LinkMergeSlash to the config for the default mount table.\n   *\n   * @param conf configuration.\n   * @param target targets.",
  "org.apache.hadoop.io.Text:skip(java.io.DataInput)" : "* Skips over one Text in the input.\n   * @param in input in.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.security.token.Token:decodeFromUrlString(java.lang.String)" : "* Decode the given url safe string into this token.\n   * @param newValue the encoded string\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.permission.FsPermission:set(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer$MetadataOption:<init>(org.apache.hadoop.io.SequenceFile$Metadata)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getRemaining()" : "* Returns the number of bytes remaining in the input buffers; normally\n   * called when finished() is true to determine amount of post-gzip-stream\n   * data.\n   *\n   * @return the total (non-negative) number of unprocessed bytes in input",
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData)" : "* Releases resources allocated to the given block.\n   *\n   * @throws IllegalArgumentException if data is null.",
  "org.apache.hadoop.io.compress.Lz4Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)" : "* Create a {@link CompressionInputStream} that will read from the given\n   * {@link InputStream} with the given {@link Decompressor}.\n   *\n   * @param in           the stream to read compressed bytes from\n   * @param decompressor decompressor to use\n   * @return a stream to read uncompressed bytes from\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)" : "* Create a {@link BlockDecompressorStream}.\n   * \n   * @param in input stream\n   * @param decompressor decompressor to use\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:generateEncryptedKey(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:finished()" : null,
  "org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:close()" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:resetBuffer()" : "* Reset the buffer for the next metric to be built",
  "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:<init>()" : null,
  "org.apache.hadoop.util.SysInfoLinux:<init>()" : null,
  "org.apache.hadoop.io.SortedMapWritable:lastKey()" : null,
  "org.apache.hadoop.service.launcher.IrqHandler:toString()" : null,
  "org.apache.hadoop.security.Groups:noGroupsForUser(java.lang.String)" : null,
  "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)" : "* Construct a <code>Reloading509KeystoreManager</code>\n   *\n   * @param type type of keystore file, typically 'jks'.\n   * @param location local path to the keystore file.\n   * @param storePassword password of the keystore file.\n   * @param keyPassword The password of the key.\n   * @throws IOException raised on errors performing I/O.\n   * @throws GeneralSecurityException thrown if create encryptor error.",
  "org.apache.hadoop.util.SysInfoLinux:getStorageBytesWritten()" : null,
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:throwMetricsException(java.lang.String,java.lang.Throwable)" : "* If the sink isn't set to ignore errors, wrap the Throwable in a\n   * {@link MetricsException} and throw it.  The message parameter will be used\n   * as the new exception's message with the current file name\n   * ({@link #currentFilePath}) and the Throwable's string representation\n   * appended to it.\n   *\n   * @param message the exception message. The message will have a colon, the\n   * current file name ({@link #currentFilePath}), and the Throwable's string\n   * representation (wrapped in square brackets) appended to it.\n   * @param t the Throwable to wrap",
  "org.apache.hadoop.conf.Configuration:setStrings(java.lang.String,java.lang.String[])" : "* Set the array of string values for the <code>name</code> property as \n   * as comma delimited values.  \n   * \n   * @param name property name.\n   * @param values The values",
  "org.apache.hadoop.fs.impl.FlagSet:flags()" : "* Get a copy of the flags.\n   * <p>\n   * This is immutable.\n   * @return the flags.",
  "org.apache.hadoop.fs.DelegateToFileSystem:getFsStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.CallQueueManager:convertQueueClass(java.lang.Class,java.lang.Class)" : null,
  "org.apache.hadoop.io.compress.Lz4Codec:setConf(org.apache.hadoop.conf.Configuration)" : "* Set the configuration to be used by this object.\n   *\n   * @param conf the configuration object.",
  "org.apache.hadoop.service.AbstractService:close()" : "* Relay to {@link #stop()}\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.util.MBeans:getMbeanNameService(javax.management.ObjectName)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:hasDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:checkPathIsSlash(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.FindClass:run(java.lang.String[])" : "* Run the class/resource find or load operation\n   * @param args command specific arguments.\n   * @return the outcome\n   * @throws Exception if something went very wrong",
  "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:getRecord()" : null,
  "org.apache.hadoop.security.Groups:getBackgroundRefreshQueued()" : null,
  "org.apache.hadoop.util.ChunkedArrayList:add(java.lang.Object)" : null,
  "org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String)" : null,
  "org.apache.hadoop.util.Shell$ShellCommandExecutor:getExecString()" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$WrapperException:<init>(java.lang.Throwable)" : null,
  "org.apache.hadoop.fs.ChecksumFs:exists(org.apache.hadoop.fs.Path)" : "Check if exists.\n   * @param f source file",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getMinimumReference(java.lang.String)" : null,
  "org.apache.hadoop.io.ReadaheadPool:submitReadahead(java.lang.String,java.io.FileDescriptor,long,long)" : "* Submit a request to readahead on the given file descriptor.\n   * @param identifier a textual identifier used in error messages, etc.\n   * @param fd the file descriptor to readahead\n   * @param off the offset at which to start the readahead\n   * @param len the number of bytes to read\n   * @return an object representing this pending request",
  "org.apache.hadoop.fs.FsShellPermissions:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : "* Register the permission related commands with the factory\n   * @param factory the command factory",
  "org.apache.hadoop.fs.shell.CommandWithDestination:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)" : "* Copies the source file to the target.\n   * @param src item to copy\n   * @param target where to copy the item\n   * @throws IOException if copy fails",
  "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:incr(float)" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:<init>()" : "* Construct.",
  "org.apache.hadoop.fs.FileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : "* Removes ACL entries from files and directories.  Other ACL entries are\n   * retained.\n   *\n   * @param path Path to modify\n   * @param aclSpec List describing entries to remove\n   * @throws IOException if an ACL could not be modified\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.util.LightWeightCache:<init>(int,int,long,long,org.apache.hadoop.util.Timer)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : "* Remove an xattr of a file or directory.\n   * The name must be prefixed with the namespace followed by \".\". For example,\n   * \"user.attr\".\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to remove extended attribute\n   * @param name xattr name\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.store.LogExactlyOnce:info(java.lang.String,java.lang.Object[])" : null,
  "org.apache.hadoop.security.LdapGroupsMapping$BindUserInfo:<init>(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:<init>()" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_snapshot()" : "* Take a snapshot of the context IOStatistics.\n   * {@code IOStatisticsContext#snapshot()}\n   * @return an instance of {@code IOStatisticsSnapshot}.\n   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",
  "org.apache.hadoop.ipc.FairCallQueue:offer(org.apache.hadoop.ipc.Schedulable)" : null,
  "org.apache.hadoop.io.MapFile:main(java.lang.String[])" : null,
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:put(java.lang.String,java.io.Serializable)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX:syncFileRangeIfPossible(java.io.FileDescriptor,long,long,int)" : "* Call sync_file_range on the given file descriptor. See the manpage\n     * for this syscall for more information. On systems where this\n     * call is not available, does nothing.\n     *\n     * @param fd input fd.\n     * @param offset input offset.\n     * @param nbytes input nbytes.\n     * @param flags input flag.\n     * @throws NativeIOException if there is an error with the syscall",
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:close()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:getFileChecksum(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.WeakReferenceMap:prune()" : "* Prune all null weak references, calling the referenceLost\n   * callback for each one.\n   *\n   * non-atomic and non-blocking.\n   * @return the number of entries pruned.",
  "org.apache.hadoop.conf.StorageUnit$3:getSuffixChar()" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:start(java.util.List,boolean)" : "* Start the connection to the ZooKeeper ensemble.\n   *\n   * @param authInfos  List of authentication keys.\n   * @param sslEnabled If the connection should be SSL/TLS encrypted.\n   * @throws IOException            If the connection cannot be started.",
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])" : "* Decode with inputs and erasedIndexes, generates outputs. More see above.\n   *\n   * Note, for both input and output ECChunks, no mixing of on-heap buffers and\n   * direct buffers are allowed.\n   *\n   * @param inputs input buffers to read data from\n   * @param erasedIndexes indexes of erased units in the inputs array\n   * @param outputs output buffers to put decoded data into according to\n   *                erasedIndexes, ready for read after the call\n   * @throws IOException if the decoder is closed",
  "org.apache.hadoop.security.SecurityUtil:getTokenServiceAddr(org.apache.hadoop.security.token.Token)" : "* Decode the given token's service field into an InetAddress\n   * @param token from which to obtain the service\n   * @return InetAddress for the service",
  "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:finish()" : "* Finishing up the current block.",
  "org.apache.hadoop.service.CompositeService$CompositeServiceShutdownHook:run()" : null,
  "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsWithAcls()" : null,
  "org.apache.hadoop.metrics2.impl.MetricGaugeInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:syncInterval(int)" : null,
  "org.apache.hadoop.ipc.FairCallQueue:offerQueue(int,org.apache.hadoop.ipc.Schedulable)" : "* Offer the element to queue of a specific priority.\n   * @param priority - queue priority\n   * @param e - element to add\n   * @return boolean if added to the given queue",
  "org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.security.UserGroupInformation:getTokenIdentifiers()" : "* Get the set of TokenIdentifiers belonging to this UGI\n   * \n   * @return the set of TokenIdentifiers belonging to this UGI",
  "org.apache.hadoop.ipc.RPC$Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:hasNext()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$WaitableMetricsBuffer:<init>(org.apache.hadoop.metrics2.impl.MetricsBuffer)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:info()" : "* @return the info object of the metrics registry",
  "org.apache.hadoop.crypto.key.kms.ValueQueue:writeLock(java.lang.String)" : null,
  "org.apache.hadoop.ipc.Server:getNumOpenConnectionsPerUser()" : "* @return Get the NumOpenConnections/User.",
  "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:getUsersForNetgroup(java.lang.String)" : "* Calls JNI function to get users for a netgroup, since C functions\n   * are not reentrant we need to make this synchronized (see\n   * documentation for setnetgrent, getnetgrent and endnetgrent)\n   *\n   * @param netgroup return users for this netgroup\n   * @return list of users for a given netgroup",
  "org.apache.hadoop.fs.LocalDirAllocator:getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)" : "Get a path from the local FS for reading. We search through all the\n   *  configured dirs for the file's existence and return the complete\n   *  path to the file when we find one \n   *  @param pathStr the requested file (this will be searched)\n   *  @param conf the Configuration object\n   *  @return the complete path to the file on a local disk\n   *  @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)" : "* Updates an existing DelegationKey in the SQL database.\n   * @param key Updated DelegationKey.",
  "org.apache.hadoop.crypto.key.UserProvider:getKeyVersion(java.lang.String)" : null,
  "org.apache.hadoop.security.SaslOutputStream:flush()" : "* Flushes this output stream\n   * \n   * @exception IOException\n   *              if an I/O error occurs.",
  "org.apache.hadoop.ipc.Server$Connection:getHostInetAddress()" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:counters()" : null,
  "org.apache.hadoop.security.SaslRpcClient:createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)" : "* Try to create a SaslClient for an authentication type.  May return\n   * null if the type isn't supported or the client lacks the required\n   * credentials.\n   * \n   * @param authType - the requested authentication method\n   * @return SaslClient for the authType or null\n   * @throws SaslException - error instantiating client\n   * @throws IOException - misc errors",
  "org.apache.hadoop.fs.FSInputStream:toString()" : "* toString method returns the superclass toString, but if the subclass\n   * implements {@link IOStatisticsSource} then those statistics are\n   * extracted and included in the output.\n   * That is: statistics of subclasses are automatically reported.\n   * @return a string value.",
  "org.apache.hadoop.conf.Configuration:setClassLoader(java.lang.ClassLoader)" : "* Set the class loader that will be used to load the various objects.\n   * \n   * @param classLoader the new class loader.",
  "org.apache.hadoop.metrics2.impl.SinkQueue:dequeue()" : "* Dequeue one element from head of the queue, will block if queue is empty\n   * @return  the first element\n   * @throws InterruptedException",
  "org.apache.hadoop.io.SequenceFile$Reader:getValueClass()" : "@return Returns the class of values in this file.",
  "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:toString()" : null,
  "org.apache.hadoop.metrics2.impl.MetricGaugeLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)" : null,
  "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getTrustManagers()" : "* Returns the trustmanagers for trusted certificates.\n   *\n   * @return the trustmanagers for trusted certificates.",
  "org.apache.hadoop.io.MapWritable:isEmpty()" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:renameOrFail(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:reset()" : null,
  "org.apache.hadoop.io.AbstractMapWritable:getNewClasses()" : "@return the number of known classes",
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:snapshot()" : "* Take a snapshot.\n   * @return a map snapshot.",
  "org.apache.hadoop.io.ArrayPrimitiveWritable:get()" : "* Get the original array.  \n   * Client must cast it back to type componentType[]\n   * (or may use type-specific wrapper classes).\n   * @return - original array as Object",
  "org.apache.hadoop.fs.HarFileSystem$HarStatus:getLength()" : null,
  "org.apache.hadoop.fs.StorageStatistics$LongStatistic:toString()" : null,
  "org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkUploadId(byte[])" : "* Utility method to validate uploadIDs.\n   * @param uploadId Upload ID\n   * @throws IllegalArgumentException invalid ID",
  "org.apache.hadoop.security.SaslOutputStream:<init>(java.io.OutputStream,javax.security.sasl.SaslServer)" : "* Constructs a SASLOutputStream from an OutputStream and a SaslServer <br>\n   * Note: if the specified OutputStream or SaslServer is null, a\n   * NullPointerException may be thrown later when they are used.\n   * \n   * @param outStream\n   *          the OutputStream to be processed\n   * @param saslServer\n   *          an initialized SaslServer object",
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:close()" : null,
  "org.apache.hadoop.ipc.ProtocolSignature:getFingerprints(java.lang.reflect.Method[])" : "* Convert an array of Method into an array of hash codes\n   * \n   * @param methods\n   * @return array of hash codes",
  "org.apache.hadoop.fs.DelegateToFileSystem:getDelegationTokens(java.lang.String)" : null,
  "org.apache.hadoop.fs.FSDataOutputStream:getWrappedStream()" : "* Get a reference to the wrapped output stream.\n   *\n   * @return the underlying output stream",
  "org.apache.hadoop.metrics2.lib.MutableQuantiles:addQuantileInfo(int,org.apache.hadoop.metrics2.MetricsInfo)" : "* Add entry to quantileInfos array.\n   *\n   * @param i array index.\n   * @param info info to be added to  quantileInfos array.",
  "org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpChunks(java.lang.String,org.apache.hadoop.io.erasurecode.ECChunk[])" : "* Print data in hex format in an array of chunks.\n   * @param header header.\n   * @param chunks chunks.",
  "org.apache.hadoop.metrics2.impl.MetricsConfig:getFilter(java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)" : null,
  "org.apache.hadoop.fs.Options$CreateOpts$BufferSize:getValue()" : null,
  "org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:validate()" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler$DecayTask:run()" : null,
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:shutdownInstance()" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:checkStream()" : null,
  "org.apache.hadoop.util.LightWeightGSet:size()" : null,
  "org.apache.hadoop.security.http.RestCsrfPreventionFilter:parseBrowserUserAgents(java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeyVersion(java.lang.String)" : null,
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(org.apache.hadoop.util.bloom.Key)" : "* Adds a false positive information to <i>this</i> retouched Bloom filter.\n   * <p>\n   * <b>Invariant</b>: if the false positive is <code>null</code>, nothing happens.\n   * @param key The false positive key to add.",
  "org.apache.hadoop.fs.shell.CommandWithDestination:checkPathsForReservedRaw(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Check the source and target paths to ensure that they are either both in\n   * /.reserved/raw or neither in /.reserved/raw. If neither src nor target are\n   * in /.reserved/raw, then return false, indicating not to preserve raw.*\n   * xattrs. If both src/target are in /.reserved/raw, then return true,\n   * indicating raw.* xattrs should be preserved. If only one of src/target is\n   * in /.reserved/raw then throw an exception.\n   *\n   * @param src The source path to check. This should be a fully-qualified\n   *            path, not relative.\n   * @param target The target path to check. This should be a fully-qualified\n   *               path, not relative.\n   * @return true if raw.* xattrs should be preserved.\n   * @throws PathOperationException is only one of src/target are in\n   * /.reserved/raw.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfCallable(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,java.util.concurrent.Callable)" : "* Given a callable/lambda expression,\n   * return a new one which wraps the inner and tracks\n   * the duration of the operation, including whether\n   * it passes/fails.\n   * @param factory factory of duration trackers\n   * @param statistic statistic key\n   * @param input input callable.\n   * @param <B> return type.\n   * @return a new callable which tracks duration and failure.",
  "org.apache.hadoop.conf.StorageUnit$5:fromBytes(double)" : null,
  "org.apache.hadoop.ipc.RefreshRegistry:dispatch(java.lang.String,java.lang.String[])" : "* Lookup the responsible handler and return its result.\n   * This should be called by the RPC server when it gets a refresh request.\n   * @param identifier the resource to refresh\n   * @param args the arguments to pass on, not including the program name\n   * @throws IllegalArgumentException on invalid identifier\n   * @return the response from the appropriate handler",
  "org.apache.hadoop.fs.GetSpaceUsed$Builder:build()" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:getPos()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:iostatisticsStore()" : "* Create a builder for an {@link IOStatisticsStore}.\n   *\n   * @return a builder instance.",
  "org.apache.hadoop.fs.FileSystem:createSnapshot(org.apache.hadoop.fs.Path)" : "* Create a snapshot with a default name.\n   * @param path The directory where snapshots will be taken.\n   * @return the snapshot path.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported",
  "org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:next()" : null,
  "org.apache.hadoop.fs.impl.FutureIOSupport:awaitFuture(java.util.concurrent.Future)" : "* Given a future, evaluate it. Raised exceptions are\n   * extracted and handled.\n   * See {@link FutureIO#awaitFuture(Future, long, TimeUnit)}.\n   * @param future future to evaluate\n   * @param <T> type of the result.\n   * @return the result, if all went well.\n   * @throws InterruptedIOException future was interrupted\n   * @throws IOException if something went wrong\n   * @throws RuntimeException any nested RTE thrown",
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMaximumSample(java.lang.String,long)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(byte[],int)" : "* Copy the key into user supplied buffer.\n         * \n         * @param buf\n         *          The buffer supplied by user.\n         * @param offset\n         *          The starting offset of the user buffer where we should copy\n         *          the key into. Requiring the key-length + offset no greater\n         *          than the buffer length.\n         * @return The length of the key.\n         * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.RunJar:getHadoopClasspath()" : null,
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getTokenInfoFromSQL(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : "* Obtains the DelegationTokenInformation associated with the given\n   * TokenIdentifier in the SQL database.\n   * @param ident Existing TokenIdentifier in the SQL database.\n   * @return DelegationTokenInformation that matches the given TokenIdentifier or\n   *         null if it doesn't exist in the database.",
  "org.apache.hadoop.fs.permission.FsPermission:getEncryptedBit()" : "* Returns true if the file is encrypted or directory is in an encryption zone.\n   *\n   * @return if the file is encrypted or directory\n   * is in an encryption zone true, not false.\n   *\n   * @deprecated Get encryption bit from the\n   * {@link org.apache.hadoop.fs.FileStatus} object.",
  "org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String)" : "* Set the <code>value</code> of the <code>name</code> property. If \n   * <code>name</code> is deprecated or there is a deprecated name associated to it,\n   * it sets the value to both names. Name will be trimmed before put into\n   * configuration.\n   * \n   * @param name property name.\n   * @param value property value.",
  "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)" : "* Construct the preferred type of SequenceFile Writer.\n   * @param fs The configured filesystem. \n   * @param conf The configuration.\n   * @param name The name of the file. \n   * @param keyClass The 'key' type.\n   * @param valClass The 'value' type.\n   * @param compressionType The compression type.\n   * @return Returns the handle to the constructed SequenceFile Writer.\n   * @throws IOException raised on errors performing I/O.\n   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}\n   *     instead.",
  "org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressorType(org.apache.hadoop.conf.Configuration)" : "* Return the appropriate type of the zlib compressor. \n   * \n   * @param conf configuration\n   * @return the appropriate type of the zlib compressor.",
  "org.apache.hadoop.net.SocketInputStream:<init>(java.nio.channels.ReadableByteChannel,long)" : "* Create a new input stream with the given timeout. If the timeout\n   * is zero, it will be treated as infinite timeout. The socket's\n   * channel will be configured to be non-blocking.\n   * \n   * @param channel \n   *        Channel for reading, should also be a {@link SelectableChannel}.\n   *        The channel will be configured to be non-blocking.\n   * @param timeout timeout in milliseconds. must not be negative.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:flush()" : null,
  "org.apache.hadoop.util.Shell:getSymlinkCommand(java.lang.String,java.lang.String)" : "* Return a command to create symbolic links.\n   *\n   * @param target target.\n   * @param link link.\n   * @return symlink command.",
  "org.apache.hadoop.security.ProviderUtils:noPasswordInstruction(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.io.LongWritable:compareTo(org.apache.hadoop.io.LongWritable)" : "Compares two LongWritables.",
  "org.apache.hadoop.metrics2.lib.MutableGaugeInt:set(int)" : "* Set the value of the metric\n   * @param value to set",
  "org.apache.hadoop.fs.DF:getDirPath()" : "@return the canonical path to the volume we're checking.",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSKeyVersion:<init>(java.lang.String,java.lang.String,byte[])" : null,
  "org.apache.hadoop.metrics2.lib.MutableQuantiles:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)" : null,
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:gauges()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String)" : null,
  "org.apache.hadoop.fs.QuotaUsage$Builder:build()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)" : "* Decode the token identifier. The subclass can customize the way to decode\n   * the token identifier.\n   * \n   * @param token the token where to extract the identifier\n   * @return the delegation token identifier\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestCaching(org.apache.hadoop.fs.impl.prefetch.BufferData)" : "* Requests that the given block should be copied to the local cache.\n   * The block must not be accessed by the caller after calling this method\n   * because it will released asynchronously relative to the caller.\n   *\n   * @throws IllegalArgumentException if data is null.",
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:osException(int,java.lang.String,java.lang.Throwable,java.util.List)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:toString()" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:checkAndSkipBytesUntilNull()" : null,
  "org.apache.hadoop.ipc.Client$Connection:receiveRpcResponse()" : null,
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:progress(org.apache.hadoop.util.Progressable)" : "* Set the facility of reporting progress.\n   *\n   * @param prog progress.\n   * @return B Generics Type.",
  "org.apache.hadoop.fs.impl.StoreImplementationUtils:hasCapability(java.io.OutputStream,java.lang.String)" : "* Probe for an output stream having a capability; returns true\n   * if the stream implements {@link StreamCapabilities} and its\n   * {@code hasCapabilities()} method returns true for the capability.\n   * @param out output stream\n   * @param capability capability to probe for\n   * @return true if the stream declares that it supports the capability.",
  "org.apache.hadoop.fs.FileUtil:getDU(java.io.File)" : "* Takes an input dir and returns the du on that local directory. Very basic\n   * implementation.\n   *\n   * @param dir\n   *          The input dir to get the disk space of this local dir\n   * @return The total disk space of the input local directory",
  "org.apache.hadoop.ipc.DecayRpcScheduler:getTotalRawCallVolume()" : null,
  "org.apache.hadoop.ipc.CallerContext:isContextValid()" : null,
  "org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:getLoginAppName()" : null,
  "org.apache.hadoop.util.functional.CloseableTaskPoolSubmitter:<init>(java.util.concurrent.ExecutorService)" : "* Constructor.\n   * @param pool non-null executor.",
  "org.apache.hadoop.fs.shell.Ls:isDisplayECPolicy()" : "* Should EC policies be displayed.\n   * @return true display EC policies, false doesn't display EC policies",
  "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:addHeader(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:recordValues(double[])" : null,
  "org.apache.hadoop.ipc.Server$Call:getRemoteUser()" : null,
  "org.apache.hadoop.fs.BBPartHandle:from(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer$AppendIfExistsOption:<init>(boolean)" : null,
  "org.apache.hadoop.util.SequentialNumber:setIfGreater(long)" : null,
  "org.apache.hadoop.io.BytesWritable:get()" : "* Get the data from the BytesWritable.\n   * @deprecated Use {@link #getBytes()} instead.\n   * @return data from the BytesWritable.",
  "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:set(java.lang.Object)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator:<init>(long,long)" : "* Construct.\n     * @param start start value.\n     * @param excludedFinish halt the iterator once the current value is equal\n     *          to or greater than this.",
  "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:<init>()" : null,
  "org.apache.hadoop.io.BoundedByteArrayOutputStream:write(int)" : null,
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)" : "* Declare that this filesystem connector is always read only.\n   * {@inheritDoc}",
  "org.apache.hadoop.io.Text:write(java.io.DataOutput,int)" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getParameters()" : "The parameter instances.",
  "org.apache.hadoop.fs.ftp.FtpFs:getServerDefaults(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:<init>(org.apache.hadoop.security.authentication.server.AuthenticationHandler)" : null,
  "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:connect()" : null,
  "org.apache.hadoop.fs.FileSystem:listXAttrs(org.apache.hadoop.fs.Path)" : "* Get all of the xattr names for a file or directory.\n   * Only those xattr names which the logged-in user has permissions to view\n   * are returned.\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to get extended attributes\n   * @return List{@literal <String>} of the XAttr names of the file or directory\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(int)" : null,
  "org.apache.hadoop.service.AbstractService:waitForServiceToStop(long)" : null,
  "org.apache.hadoop.ipc.CallQueueManager:swapQueue(java.lang.Class,java.lang.Class,int,java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Replaces active queue with the newly requested one and transfers\n   * all calls to the newQ before returning.\n   *\n   * @param schedulerClass input schedulerClass.\n   * @param queueClassToUse input queueClassToUse.\n   * @param maxSize input maxSize.\n   * @param ns input ns.\n   * @param conf input configuration.",
  "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:<init>(org.apache.hadoop.metrics2.MetricsRecord,org.apache.hadoop.metrics2.MetricsFilter)" : null,
  "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:putAll(java.util.Map)" : null,
  "org.apache.hadoop.http.ProfileServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.util.ReflectionUtils:printThreadInfo(java.io.PrintStream,java.lang.String)" : "* Print all of the thread's information and stack traces.\n   * \n   * @param stream the stream to\n   * @param title a string title for the stack trace",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getAndMoveToFrontDecode()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:mkOneDirWithMode(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.ha.ZKFCRpcServer:cedeActive(int)" : null,
  "org.apache.hadoop.util.PriorityQueue:downHeap()" : null,
  "org.apache.hadoop.util.LightWeightGSet$Values:contains(java.lang.Object)" : null,
  "org.apache.hadoop.util.DataChecksum:newDataChecksum(org.apache.hadoop.util.DataChecksum$Type,int)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)" : null,
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:resolveFullGroupNames(java.lang.String)" : "* Split group names into a set.\n   *\n   * @param groupNames a string representing the user's group names\n   * @return a set of group names",
  "org.apache.hadoop.io.SortedMapWritable:isEmpty()" : null,
  "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:getAlgorithm()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:publishMetricsNow()" : "* Requests an immediate publish of all metrics from sources to sinks.",
  "org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumTimeWithFixedSleep(long,long,java.util.concurrent.TimeUnit)" : "* <p>\n   * Keep trying for a maximum time, waiting a fixed time between attempts,\n   * and then fail by re-throwing the exception.\n   * </p>\n   *\n   * @param timeUnit timeUnit.\n   * @param sleepTime sleepTime.\n   * @param maxTime maxTime.\n   * @return RetryPolicy.",
  "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:needsInput()" : "* Returns true if the input data buffer is empty and\n   * {@link #setInput(byte[], int, int)} should be called to\n   * provide more input.\n   *\n   * @return <code>true</code> if the input data buffer is empty and\n   *         {@link #setInput(byte[], int, int)} should be called in\n   *         order to provide more input.",
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,javax.net.ssl.SSLSocket)" : null,
  "org.apache.hadoop.fs.shell.PathData:openFile(java.lang.String)" : "* Open a file.\n   * @param policy fadvise policy.\n   * @return an input stream\n   * @throws IOException failure",
  "org.apache.hadoop.ipc.Server:setCallQueue(org.apache.hadoop.ipc.CallQueueManager)" : null,
  "org.apache.hadoop.service.AbstractService:removeBlocker(java.lang.String)" : "* Remove a blocker from the blocker map -\n   * this is a no-op if the blocker is not present\n   * @param name the name of the blocker",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:incrSlowRpc()" : "* Increments the Slow RPC counter.",
  "org.apache.hadoop.util.Shell:getQualifiedBinPath(java.lang.String)" : "*  Fully qualify the path to a binary that should be in a known hadoop\n   *  bin location. This is primarily useful for disambiguating call-outs\n   *  to executable sub-components of Hadoop to avoid clashes with other\n   *  executables that may be in the path.  Caveat:  this call doesn't\n   *  just format the path to the bin directory.  It also checks for file\n   *  existence of the composed path. The output of this call should be\n   *  cached by callers.\n   *\n   * @param executable executable\n   * @return executable file reference\n   * @throws FileNotFoundException if the path does not exist\n   * @throws IOException on path canonicalization failures",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.BufferedFSInputStream:readFully(long,byte[],int,int)" : null,
  "org.apache.hadoop.fs.AvroFSInput:<init>(org.apache.hadoop.fs.FSDataInputStream,long)" : "* Construct given an {@link FSDataInputStream} and its length.\n   *\n   * @param in inputstream.\n   * @param len len.",
  "org.apache.hadoop.fs.FileUtil:checkDest(java.lang.String,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.util.LightWeightGSet:get(java.lang.Object)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues()" : null,
  "org.apache.hadoop.http.HttpServer2:getFilterMapping(java.lang.String,java.lang.String[])" : null,
  "org.apache.hadoop.ha.HealthMonitor:tryConnect()" : null,
  "org.apache.hadoop.util.Sets:difference(java.util.Set,java.util.Set)" : "* Returns the difference of two sets as an unmodifiable set.\n   * The returned set contains all elements that are contained by {@code set1}\n   * and not contained by {@code set2}.\n   *\n   * <p>Results are undefined if {@code set1} and {@code set2} are sets based\n   * on different equivalence relations (as {@code HashSet}, {@code TreeSet},\n   * and the keySet of an {@code IdentityHashMap} all are).\n   *\n   * This method is used to find difference for HashSets. For TreeSets with\n   * strict order requirement, recommended method is\n   * {@link #differenceInTreeSets(Set, Set)}.\n   *\n   * @param set1 set1.\n   * @param set2 set2.\n   * @param <E> Generics Type E.\n   * @return a new, empty thread-safe {@code Set}.",
  "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:isSorted()" : null,
  "org.apache.hadoop.ha.BadFencingConfigurationException:<init>(java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.net.NodeBase:setLevel(int)" : "Set this node's level in the tree\n   * @param level the level",
  "org.apache.hadoop.fs.shell.PathData:lookupStat(org.apache.hadoop.fs.FileSystem,java.lang.String,boolean)" : "* Get the FileStatus info\n   * @param ignoreFNF if true, stat will be null if the path doesn't exist\n   * @return FileStatus for the given path\n   * @throws IOException if anything goes wrong",
  "org.apache.hadoop.ha.HealthMonitor:loopUntilConnected()" : null,
  "org.apache.hadoop.util.SequentialNumber:getCurrentValue()" : "@return the current value.",
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getReadLock()" : null,
  "org.apache.hadoop.crypto.key.CachingKeyProvider:deleteKey(java.lang.String)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)" : "* Renews a delegation token from the server end-point using the\n   * configured <code>Authenticator</code> for authentication.\n   *\n   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are\n   * supported.\n   * @param token the authentication token with the Delegation Token to renew.\n   * @param dToken abstract delegation token identifier.\n   * @throws IOException if an IO error occurred.\n   * @throws AuthenticationException if an authentication exception occurred.\n   * @return delegation token long value.",
  "org.apache.hadoop.fs.BlockLocation:setTopologyPaths(java.lang.String[])" : "* Set the network topology paths of the hosts.\n   *\n   * @param topologyPaths topology paths.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:inflateDirect(java.nio.ByteBuffer,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.security.token.TokenIdentifier:getTrackingId()" : "* Returns a tracking identifier that can be used to associate usages of a\n   * token across multiple client sessions.\n   *\n   * Currently, this function just returns an MD5 of {{@link #getBytes()}.\n   *\n   * @return tracking identifier",
  "org.apache.hadoop.fs.Options$CreateOpts$Perms:getValue()" : null,
  "org.apache.hadoop.security.token.DtUtilShell:init(java.lang.String[])" : "* Parse the command line arguments and initialize subcommand.\n   * Also will attempt to perform Kerberos login if both -principal and -keytab\n   * flags are passed in args array.\n   * @param args args.\n   * @return 0 if the argument(s) were recognized, 1 otherwise\n   * @throws Exception Exception.",
  "org.apache.hadoop.crypto.key.KeyProvider$Options:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.UserGroupInformation:doSubjectLogin(javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$LoginParams)" : "* Login a subject with the given parameters.  If the subject is null,\n   * the login context used to create the subject will be attached.\n   * @param subject to login, null for new subject.\n   * @param params for login, null for externally managed ugi.\n   * @return UserGroupInformation for subject\n   * @throws IOException",
  "org.apache.hadoop.fs.AbstractFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)" : "* Get all of the xattrs for a file or directory.\n   * Only those xattrs for which the logged-in user has permissions to view\n   * are returned.\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to get extended attributes\n   * @param names XAttr names.\n   * @return {@literal Map<String, byte[]>} describing the XAttrs of the file\n   * or directory\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.http.HttpServer2:getConnectorAddress(int)" : "* Get the address that corresponds to a particular connector.\n   *\n   * @param index index.\n   * @return the corresponding address for the connector, or null if there's no\n   *         such connector or the connector is not bounded or was closed.",
  "org.apache.hadoop.io.compress.Lz4Codec:getDefaultExtension()" : "* Get the default filename extension for this kind of compression.\n   *\n   * @return <code>.lz4</code>.",
  "org.apache.hadoop.fs.impl.WeakRefMetricsSource:<init>(java.lang.String,org.apache.hadoop.metrics2.MetricsSource)" : "* Constructor.\n   * @param name Name to know when unregistering.\n   * @param source metrics source",
  "org.apache.hadoop.util.DataChecksum:writeValue(java.io.DataOutputStream,boolean)" : "* Writes the current checksum to the stream.\n   * If <i>reset</i> is true, then resets the checksum.\n   *\n   * @param out out.\n   * @param reset reset.\n   * @return number of bytes written. Will be equal to getChecksumSize();\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getCounterReference(java.lang.String)" : "* Get a reference to the atomic instance providing the\n   * value for a specific counter. This is useful if\n   * the value is passed around.\n   * @param key statistic name\n   * @return the reference\n   * @throws NullPointerException if there is no entry of that name",
  "org.apache.hadoop.fs.FileSystem:getUsed()" : "* Return the total size of all files in the filesystem.\n   * @throws IOException IO failure\n   * @return the number of path used.",
  "org.apache.hadoop.fs.QuotaUsage:toString()" : null,
  "org.apache.hadoop.io.SequenceFile$Reader$OnlyHeaderOption:<init>()" : null,
  "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:getCipherSuite()" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:snapshot(org.apache.hadoop.fs.statistics.IOStatistics)" : "* Take a snapshot.\n   *\n   * This completely overwrites the map data with the statistics\n   * from the source.\n   * @param source statistics source.",
  "org.apache.hadoop.io.compress.ZStandardCodec:createInputStream(java.io.InputStream)" : "* Create a {@link CompressionInputStream} that will read from the given\n   * input stream.\n   *\n   * @param in the stream to read compressed bytes from\n   * @return a stream to read uncompressed bytes from\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.Client$Connection:sendPing()" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler:log(java.lang.reflect.Method,boolean,int,int,long,java.lang.Exception)" : null,
  "org.apache.hadoop.util.LimitInputStream:available()" : null,
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$Certificates:getCNs(java.security.cert.X509Certificate)" : null,
  "org.apache.hadoop.io.erasurecode.ECSchema:getCodecName()" : "* Get the codec name\n   * @return codec name",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:initJsonFactory(java.util.Properties)" : null,
  "org.apache.hadoop.ipc.UserIdentityProvider:makeIdentity(org.apache.hadoop.ipc.Schedulable)" : null,
  "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterValues(java.lang.String)" : null,
  "org.apache.hadoop.util.UTF8ByteArrayUtils:findNthByte(byte[],int,int,byte,int)" : "* Find the nth occurrence of the given byte b in a UTF-8 encoded string\n   * @param utf a byte array containing a UTF-8 encoded string\n   * @param start starting offset\n   * @param length the length of byte array\n   * @param b the byte to find\n   * @param n the desired occurrence of the given byte\n   * @return position that nth occurrence of the given byte if exists; otherwise -1",
  "org.apache.hadoop.metrics2.util.Contracts:checkArg(long,boolean,java.lang.Object)" : "* Check an argument for false conditions\n   * @param arg the argument to check\n   * @param expression  the boolean expression for the condition\n   * @param msg the error message if {@code expression} is false\n   * @return the argument for convenience",
  "org.apache.hadoop.io.BytesWritable$Comparator:compare(byte[],int,int,byte[],int,int)" : "* Compare the buffers in serialized form.",
  "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.fs.BatchedRemoteIterator$BatchedListEntries:size()" : null,
  "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,double)" : null,
  "org.apache.hadoop.fs.store.ByteBufferInputStream:checkOpenState()" : "* Check the open state.\n   * @throws IllegalStateException if the stream is closed.",
  "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:withEvaluated(java.util.Map)" : "* Add all evaluated attributes to the current map.\n     * @param value new value\n     * @return the builder",
  "org.apache.hadoop.io.ByteWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.io.compress.DefaultCodec:createCompressor()" : null,
  "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)" : "* Add nfly link to configuration for the given mount table.\n   *\n   * @param conf configuration.\n   * @param mountTableName mount table.\n   * @param src src.\n   * @param settings settings.\n   * @param targets targets.",
  "org.apache.hadoop.util.HostsFileReader:refresh(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.security.alias.LocalKeyStoreProvider:createPermissions(java.lang.String)" : null,
  "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:getKeyStoreType()" : null,
  "org.apache.hadoop.fs.ContentSummary:getQuotaHeaderFields()" : "* Returns the names of the fields used in the quota summary.\n   * \n   * @return names of quota fields as displayed in the header",
  "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)" : "* Construct the preferred type of SequenceFile Writer.\n   * @param fs The configured filesystem. \n   * @param conf The configuration.\n   * @param name The name of the file. \n   * @param keyClass The 'key' type.\n   * @param valClass The 'value' type.\n   * @param compressionType The compression type.\n   * @param progress The Progressable object to track progress.\n   * @return Returns the handle to the constructed SequenceFile Writer.\n   * @throws IOException raised on errors performing I/O.\n   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}\n   *     instead.",
  "org.apache.hadoop.fs.viewfs.ViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)" : null,
  "org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionMinimum(java.lang.String,java.util.function.ToLongFunction)" : "* Add a new evaluator to the minimum statistics.\n   * @param key key of this statistic\n   * @param eval evaluator for the statistic\n   * @return the builder.",
  "org.apache.hadoop.io.LongWritable:<init>()" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:getPadding(long)" : null,
  "org.apache.hadoop.ha.HealthMonitor:addCallback(org.apache.hadoop.ha.HealthMonitor$Callback)" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:init(org.apache.commons.configuration2.SubsetConfiguration)" : null,
  "org.apache.hadoop.fs.FilterFs:checkPath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:incrementDelegationTokenSeqNum()" : "* Obtains the next available sequence number that can be allocated to a Token.\n   * Sequence numbers need to be reserved using the shared sequenceNumberCounter once\n   * the local batch has been exhausted, which handles sequenceNumber allocation\n   * concurrently with other secret managers.\n   * This method ensures that sequence numbers are incremental in a single secret manager,\n   * but not across secret managers.\n   * @return Next available sequence number.",
  "org.apache.hadoop.io.retry.RetryInvocationHandler:isRpcInvocation(java.lang.Object)" : null,
  "org.apache.hadoop.fs.permission.FsPermission:getAclBit()" : "* Returns true if there is also an ACL (access control list).\n   *\n   * @return boolean true if there is also an ACL (access control list).\n   * @deprecated Get acl bit from the {@link org.apache.hadoop.fs.FileStatus}\n   * object.",
  "org.apache.hadoop.io.SequenceFile$Metadata:getMetadata()" : null,
  "org.apache.hadoop.io.WritableUtils:readCompressedString(java.io.DataInput)" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_delete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Collection)" : "* Delete a list of files/objects.\n   * <ul>\n   *   <li>Files must be under the path provided in {@code base}.</li>\n   *   <li>The size of the list must be equal to or less than the page size.</li>\n   *   <li>Directories are not supported; the outcome of attempting to delete\n   *       directories is undefined (ignored; undetected, listed as failures...).</li>\n   *   <li>The operation is not atomic.</li>\n   *   <li>The operation is treated as idempotent: network failures may\n   *        trigger resubmission of the request -any new objects created under a\n   *        path in the list may then be deleted.</li>\n   *    <li>There is no guarantee that any parent directories exist after this call.\n   *    </li>\n   * </ul>\n   * @param fs filesystem\n   * @param base path to delete under.\n   * @param paths list of paths which must be absolute and under the base path.\n   * @return a list of all the paths which couldn't be deleted for a reason other than\n   *          \"not found\" and any associated error message.\n   * @throws UnsupportedOperationException bulk delete under that path is not supported.\n   * @throws IllegalArgumentException if a path argument is invalid.\n   * @throws IOException IO problems including networking, authentication and more.",
  "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolServerSideTranslatorPB:refreshServiceAcl(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.security.proto.RefreshAuthorizationPolicyProtocolProtos$RefreshServiceAclRequestProto)" : null,
  "org.apache.hadoop.io.WritableName:<init>()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:getAsyncChannel()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:getStatistics()" : null,
  "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyCompressor:compress(byte[],int,int)" : "* Fills specified buffer with compressed data. Returns actual number\n   * of bytes of compressed data. A return value of 0 indicates that\n   * needsInput() should be called in order to determine if more input\n   * data is required.\n   *\n   * @param b   Buffer for the compressed data\n   * @param off Start offset of the data\n   * @param len Size of the buffer\n   * @return The actual number of bytes of compressed data.",
  "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$7:suffix()" : null,
  "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$1:suffix()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withMeanStatistics(java.lang.String[])" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getServerDefaults(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping:parseId(java.lang.String)" : "* uid and gid are defined as uint32 in linux. Some systems create\n   * (intended or unintended) <nfsnobody, 4294967294> kind of <name,Id>\n   * mapping, where 4294967294 is 2**32-2 as unsigned int32. As an example,\n   *   https://bugzilla.redhat.com/show_bug.cgi?id=511876.\n   * Because user or group id are treated as Integer (signed integer or int32)\n   * here, the number 4294967294 is out of range. The solution is to convert\n   * uint32 to int32, so to map the out-of-range ID to the negative side of\n   * Integer, e.g. 4294967294 maps to -2 and 4294967295 maps to -1.",
  "org.apache.hadoop.conf.ReconfigurableBase:<init>()" : "* Construct a ReconfigurableBase.",
  "org.apache.hadoop.fs.FileContext:getFileContext(java.net.URI)" : "* Create a FileContext for specified URI using the default config.\n   * \n   * @param defaultFsUri defaultFsUri.\n   * @return a FileContext with the specified URI as the default FS.\n   * \n   * @throws UnsupportedFileSystemException If the file system for\n   *           <code>defaultFsUri</code> is not supported",
  "org.apache.hadoop.io.ReadaheadPool:<init>()" : null,
  "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:writeBuffer(org.apache.hadoop.io.DataOutputBuffer)" : "Workhorse to check and write out compressed data/lengths",
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:currentConfig()" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:loadFromZKCache(boolean)" : "* Load the CuratorCache into the in-memory map. Possible caches to be\n   * loaded are keyCache and tokenCache.\n   *\n   * @param isTokenCache true if loading tokenCache, false if loading keyCache.",
  "org.apache.hadoop.io.erasurecode.ECBlockGroup:getParityBlocks()" : "* Get parity blocks\n   * @return parity blocks",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_means(java.io.Serializable)" : "* Get the means of an IOStatisticsSnapshot.\n   * Each value in the map is the (sample, sum) tuple of the values;\n   * the mean is then calculated by dividing sum/sample wherever sample is non-zero.\n   * @param source source of statistics.\n   * @return a map of mean key to (sample, sum) tuples.",
  "org.apache.hadoop.fs.FileContext:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : "* Rename a snapshot.\n   *\n   * @param path The directory path where the snapshot was taken\n   * @param snapshotOldName Old name of the snapshot\n   * @param snapshotNewName New name of the snapshot\n   *\n   * @throws IOException If an I/O error occurred\n   *\n   * <p>Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws\n   *           undeclared exception to RPC server",
  "org.apache.hadoop.metrics2.MetricsException:<init>(java.lang.Throwable)" : "* Construct the exception with a cause\n   * @param cause of the exception",
  "org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:close()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int,int,long,long)" : null,
  "org.apache.hadoop.fs.FileUtil:unpackEntries(org.apache.commons.compress.archivers.tar.TarArchiveInputStream,org.apache.commons.compress.archivers.tar.TarArchiveEntry,java.io.File)" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:finished()" : "* Returns true if the end of the decompressed\n   * data output stream has been reached.\n   *\n   * @return <code>true</code> if the end of the decompressed\n   *         data output stream has been reached.",
  "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)" : "* Create the named map for keys of the named class.\n     * @deprecated Use Writer(Configuration, Path, Option...) instead.\n     *\n     * @param conf configuration.\n     * @param fs FileSystem.\n     * @param dirName dirName.\n     * @param keyClass keyClass.\n     * @param valClass valClass.\n     * @param compress compress.\n     * @param codec codec.\n     * @param progress progress.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.UserGroupInformation:getShortUserName()" : "* Get the user's login name.\n   * @return the user's name up to the first '/' or '@'.",
  "org.apache.hadoop.util.LightWeightGSet:remove(java.lang.Object)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKMSUrl()" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:listXAttrs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsConfig:getInstanceConfigs(java.lang.String)" : "* Return sub configs for instance specified in the config.\n   * Assuming format specified as follows:<pre>\n   * [type].[instance].[option] = [value]</pre>\n   * Note, '*' is a special default instance, which is excluded in the result.\n   * @param type  of the instance\n   * @return  a map with [instance] as key and config object as value",
  "org.apache.hadoop.io.BytesWritable:hashCode()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:<init>()" : null,
  "org.apache.hadoop.metrics2.AbstractMetric:toString()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withGauges(java.lang.String[])" : null,
  "org.apache.hadoop.io.retry.RetryPolicy$RetryAction:toString()" : null,
  "org.apache.hadoop.fs.shell.SetReplication:processArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.VectoredReadUtils:<init>()" : "* private constructor.",
  "org.apache.hadoop.util.curator.ZKCuratorManager:safeCreate(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode,java.util.List,java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* The src file is on the local disk.  Add it to the filesystem at\n   * the given dst name, removing the source afterwards.\n   * @param src local path\n   * @param dst path\n   * @throws IOException IO failure",
  "org.apache.hadoop.crypto.key.KeyProviderExtension:getKeyVersions(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsConfig:subset(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.BZip2Codec:createDecompressor()" : "* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.\n   *\n   * @return a new decompressor for use by this codec",
  "org.apache.hadoop.fs.viewfs.ChRootedFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:<init>(int,org.apache.hadoop.crypto.CipherSuite,java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.UserProvider:getKeys()" : null,
  "org.apache.hadoop.fs.FileUtil:createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,java.util.Map)" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:releaseBuffer(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.conf.Configuration:getSocketAddr(java.lang.String,java.lang.String,int)" : "* Get the socket address for <code>name</code> property as a\n   * <code>InetSocketAddress</code>.\n   * @param name property name.\n   * @param defaultAddress the default value\n   * @param defaultPort the default port\n   * @return InetSocketAddress",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:numOpenConnections()" : null,
  "org.apache.hadoop.io.compress.BZip2Codec:getDefaultExtension()" : "* .bz2 is recognized as the default extension for compressed BZip2 files\n  *\n  * @return A String telling the default bzip2 file extension",
  "org.apache.hadoop.util.GenericOptionsParser:isParseSuccessful()" : "* Query for the parse operation succeeding.\n   * @return true if parsing the CLI was successful",
  "org.apache.hadoop.fs.shell.Ls:maxLength(int,java.lang.Object)" : null,
  "org.apache.hadoop.io.MapWritable:hashCode()" : null,
  "org.apache.hadoop.fs.BlockLocation:getLength()" : "* Get the length of the block.\n   * @return length of the block.",
  "org.apache.hadoop.security.SaslPlainServer:evaluateResponse(byte[])" : null,
  "org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.RawPathHandle:<init>(java.nio.ByteBuffer)" : "* Store a reference to the given bytes as the serialized form.\n   * @param fd serialized bytes",
  "org.apache.hadoop.fs.impl.prefetch.BufferPool:<init>(int,int,org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics)" : "* Initializes a new instance of the {@code BufferPool} class.\n   * @param size number of buffer in this pool.\n   * @param bufferSize size in bytes of each buffer.\n   * @param prefetchingStatistics statistics for this stream.\n   * @throws IllegalArgumentException if size is zero or negative.\n   * @throws IllegalArgumentException if bufferSize is zero or negative.",
  "org.apache.hadoop.fs.PathExistsException:<init>(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.util.functional.LazyAutoCloseableReference:lazyAutoCloseablefromSupplier(java.util.function.Supplier)" : "* Create from a supplier.\n   * This is not a constructor to avoid ambiguity when a lambda-expression is\n   * passed in.\n   * @param supplier supplier implementation.\n   * @return a lazy reference.\n   * @param <T> type of reference",
  "org.apache.hadoop.fs.GlobFilter:<init>(java.lang.String,org.apache.hadoop.fs.PathFilter)" : "* Creates a glob filter with the specified file pattern and an user filter.\n   *\n   * @param filePattern the file pattern.\n   * @param filter user filter in addition to the glob pattern.\n   * @throws IOException thrown if the file pattern is incorrect.",
  "org.apache.hadoop.util.JvmPauseMonitor:serviceInit(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ha.ZKFailoverController:initZK()" : null,
  "org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.statistics.MeanStatistic:add(org.apache.hadoop.fs.statistics.MeanStatistic)" : "* Add another MeanStatistic.\n   * @param other other value\n   * @return mean statistic.",
  "org.apache.hadoop.util.WeakReferenceMap:clear()" : "* Clear all entries.",
  "org.apache.hadoop.fs.UnsupportedMultipartUploaderException:<init>(java.lang.String)" : "* Constructs exception with the specified detail message.\n   *\n   * @param message exception message.",
  "org.apache.hadoop.metrics2.util.SampleQuantiles:<init>(org.apache.hadoop.metrics2.util.Quantile[])" : null,
  "org.apache.hadoop.security.LdapGroupsMapping:initializeBindUsers()" : null,
  "org.apache.hadoop.crypto.CryptoOutputStream:hflush()" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping:getUidAllowingUnknown(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.CompressionCodecFactory:getCodec(org.apache.hadoop.fs.Path)" : "* Find the relevant compression codec for the given file based on its\n   * filename suffix.\n   * @param file the filename to check\n   * @return the codec object",
  "org.apache.hadoop.http.HttpServer2:addNoCacheFilter(org.eclipse.jetty.servlet.ServletContextHandler)" : null,
  "org.apache.hadoop.util.Options$FSDataInputStreamOption:getValue()" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsLogging$StatisticsToString:toString()" : "* Evaluate and stringify the statistics.\n     * @return a string value.",
  "org.apache.hadoop.security.SecurityUtil:setConfigurationInternal(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.crypto.key.KeyShell$Command:getKeyProvider()" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:toString()" : null,
  "org.apache.hadoop.crypto.CipherOption:<init>(org.apache.hadoop.crypto.CipherSuite)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:initSymbols(java.lang.String)" : null,
  "org.apache.hadoop.util.HostsFileReader$HostDetails:getIncludedHosts()" : null,
  "org.apache.hadoop.util.functional.RemoteIterators:typeCastingRemoteIterator(org.apache.hadoop.fs.RemoteIterator)" : "* Create a RemoteIterator from a RemoteIterator, casting the\n   * type in the process. This is to help with filesystem API\n   * calls where overloading causes confusion (e.g. listStatusIterator())\n   * @param <S> source type\n   * @param <T> result type\n   * @param iterator source\n   * @return a remote iterator",
  "org.apache.hadoop.fs.shell.CommandUtils:formatDescription(java.lang.String,java.lang.String[])" : null,
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addGaugeFunction(java.lang.String,java.util.function.Function)" : "* add a mapping of a key to a gauge function.\n   * @param key the key\n   * @param eval the evaluator",
  "org.apache.hadoop.fs.TrashPolicyDefault:isEnabled()" : null,
  "org.apache.hadoop.metrics2.lib.MutableGaugeLong:incr()" : null,
  "org.apache.hadoop.conf.ConfServlet:parseAcceptHeader(javax.servlet.http.HttpServletRequest)" : null,
  "org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:remove()" : null,
  "org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersionForRpcKind(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)" : null,
  "org.apache.hadoop.util.bloom.CountingBloomFilter:approximateCount(org.apache.hadoop.util.bloom.Key)" : "* This method calculates an approximate count of the key, i.e. how many\n   * times the key was added to the filter. This allows the filter to be\n   * used as an approximate <code>key -&gt; count</code> map.\n   * <p>NOTE: due to the bucket size of this filter, inserting the same\n   * key more than 15 times will cause an overflow at all filter positions\n   * associated with this key, and it will significantly increase the error\n   * rate for this and other keys. For this reason the filter can only be\n   * used to store small count values <code>0 &lt;= N &lt;&lt; 15</code>.\n   * @param key key to be tested\n   * @return 0 if the key is not present. Otherwise, a positive value v will\n   * be returned such that <code>v == count</code> with probability equal to the\n   * error rate of this filter, and <code>v &gt; count</code> otherwise.\n   * Additionally, if the filter experienced an underflow as a result of\n   * {@link #delete(Key)} operation, the return value may be lower than the\n   * <code>count</code> with the probability of the false negative rate of such\n   * filter.",
  "org.apache.hadoop.util.InstrumentedLock:getLock()" : null,
  "org.apache.hadoop.log.LogLevel:printUsage()" : null,
  "org.apache.hadoop.fs.FileSystem$Cache$Key:toString()" : null,
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:drain(java.lang.String)" : "* Calls {@link CryptoExtension#drain(String)} for the given key name on the\n   * underlying {@link CryptoExtension}.\n   *\n   * @param keyName key name.",
  "org.apache.hadoop.security.ShellBasedIdMapping:getUid(java.lang.String)" : null,
  "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:<init>(int,java.util.concurrent.ThreadFactory,java.util.concurrent.RejectedExecutionHandler)" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination:copyStreamToTarget(java.io.InputStream,org.apache.hadoop.fs.shell.PathData)" : "* If direct write is disabled ,copies the stream contents to a temporary\n   * file \"target._COPYING_\". If the copy is successful, the temporary file\n   * will be renamed to the real path, else the temporary file will be deleted.\n   * if direct write is enabled , then creation temporary file is skipped.\n   *\n   * @param in     the input stream for the copy\n   * @param target where to store the contents of the stream\n   * @throws IOException if copy fails",
  "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroDeserializer:close()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:close()" : null,
  "org.apache.hadoop.net.InnerNodeImpl:isLeafParent()" : null,
  "org.apache.hadoop.util.bloom.DynamicBloomFilter:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.ipc.Client$Connection:handleSaslConnectionFailure(int,int,java.io.IOException,java.util.Random,org.apache.hadoop.security.UserGroupInformation)" : "* If multiple clients with the same principal try to connect to the same\n     * server at the same time, the server assumes a replay attack is in\n     * progress. This is a feature of kerberos. In order to work around this,\n     * what is done is that the client backs off randomly and tries to initiate\n     * the connection again. The other problem is to do with ticket expiry. To\n     * handle that, a relogin is attempted.",
  "org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Construct a set reader for the named set.\n     * @param fs input FileSystem.\n     * @param dirName input dirName.\n     * @param conf input Configuration.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getProtocolVersion(java.lang.String,long)" : null,
  "org.apache.hadoop.ha.ZKFailoverController:createReqInfo()" : null,
  "org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials)" : "* Copy all of the credentials from one credential object into another.\n   * Existing secrets and tokens are overwritten.\n   * @param other the credentials to copy",
  "org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,double)" : "* Pass an optional double parameter for the Builder.\n   * This parameter is converted to a long and passed\n   * to {@link #optLong(String, long)} -all\n   * decimal precision is lost.\n   * @param key key.\n   * @param value value.\n   * @return generic type B.\n   * @see #opt(String, String)\n   * @deprecated use {@link #optDouble(String, double)}",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withMaximums(java.lang.String[])" : null,
  "org.apache.hadoop.metrics2.MetricsJsonBuilder:parent()" : null,
  "org.apache.hadoop.util.FindClass:main(java.lang.String[])" : "* Main entry point. \n   * Runs the class via the {@link ToolRunner}, then\n   * exits with an appropriate exit code. \n   * @param args argument list",
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : null,
  "org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorAddress()" : "* Returns an optional separate RPC server address for health checks at the\n   * target node.  If defined, then this address is used by the health monitor\n   * for the {@link HAServiceProtocol#monitorHealth()} and\n   * {@link HAServiceProtocol#getServiceStatus()} calls.  This can be useful for\n   * separating out these calls onto separate RPC handlers to protect against\n   * resource exhaustion in the main RPC handler pool.  If null (which is the\n   * default implementation), then all RPC calls go to the address defined by\n   * {@link #getAddress()}.\n   *\n   * @return IPC address of the lifeline RPC server on the target node, or null\n   *     if no lifeline RPC server is used",
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:moveToFrontCodeAndSend()" : null,
  "org.apache.hadoop.io.SecureIOUtils:checkStat(java.io.File,java.lang.String,java.lang.String,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.security.SecurityUtil:replacePattern(java.lang.String[],java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination:preserveAttributes(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData,boolean)" : "* Preserve the attributes of the source to the target.\n   * The method calls {@link #shouldPreserve(FileAttribute)} to check what\n   * attribute to preserve.\n   * @param src source to preserve\n   * @param target where to preserve attributes\n   * @param preserveRawXAttrs true if raw.* xattrs should be preserved\n   * @throws IOException if fails to preserve attributes",
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)" : "* Construct from a {@link FileContext}.\n   *\n   * @param fc FileContext\n   * @param p path.\n   * @throws IOException failure",
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:getPrevious()" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttrCacheMetric(org.apache.hadoop.metrics2.AbstractMetric,int)" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:set(java.lang.Object)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize()" : null,
  "org.apache.hadoop.fs.shell.find.FilterExpression:isOperator()" : null,
  "org.apache.hadoop.util.NativeCodeLoader:<init>()" : null,
  "org.apache.hadoop.ipc.Server$Connection:isIdle()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:setVerifyChecksum(boolean)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.HarFileSystem:msync()" : null,
  "org.apache.hadoop.fs.HarFileSystem:getFileStatus(org.apache.hadoop.fs.Path)" : "* return the filestatus of files in har archive.\n   * The permission returned are that of the archive\n   * index files. The permissions are not persisted \n   * while creating a hadoop archive.\n   * @param f the path in har filesystem\n   * @return filestatus.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.Sets:addAll(java.util.TreeSet,java.lang.Iterable)" : null,
  "org.apache.hadoop.ipc.metrics.RpcMetrics:<init>(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:getBufferTree(boolean)" : "* Method to get desired buffer tree.\n   * @param isDirect whether the buffer is heap based or direct.\n   * @return corresponding buffer tree.",
  "org.apache.hadoop.fs.FileContext:getLinkTarget(org.apache.hadoop.fs.Path)" : "* Returns the target of the given symbolic link as it was specified\n   * when the link was created.  Links in the path leading up to the\n   * final path component are resolved transparently.\n   *\n   * @param f the path to return the target of\n   * @return The un-interpreted target of the symbolic link.\n   * \n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If path <code>f</code> does not exist\n   * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n   *           not supported\n   * @throws IOException If the given path does not refer to a symlink\n   *           or an I/O error occurred",
  "org.apache.hadoop.fs.FileStatus:validateObject()" : null,
  "org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)" : "* Create a new Path based on the child path resolved against the parent path.\n   *\n   * @param parent the parent path\n   * @param child the child path",
  "org.apache.hadoop.metrics2.source.JvmMetrics:create(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSystem)" : null,
  "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:getThreadPoolQueueSize()" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementCurrentKeyId()" : null,
  "org.apache.hadoop.io.WritableComparator:hashBytes(byte[],int)" : "* Compute hash for binary data.\n   * @param bytes bytes.\n   * @param length length.\n   * @return hash for binary data.",
  "org.apache.hadoop.ipc.RetryCache:<init>(java.lang.String,double,long)" : "* Constructor\n   * @param cacheName name to identify the cache by\n   * @param percentage percentage of total java heap space used by this cache\n   * @param expirationTime time for an entry to expire in nanoseconds",
  "org.apache.hadoop.io.erasurecode.ECBlock:isErased()" : "*\n   * @return true if it's erased due to erasure, otherwise false",
  "org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:<init>()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getPermission()" : null,
  "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:reloadCachedMappings(java.util.List)" : null,
  "org.apache.hadoop.fs.permission.PermissionParser:combineModes(int,boolean)" : null,
  "org.apache.hadoop.ipc.CallQueueManager:parseCapacityWeights(int,java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Read the weights of capacity in callqueue and pass the value to\n   * callqueue constructions.",
  "org.apache.hadoop.util.CloseableReferenceCount:getReferenceCount()" : "* Get the current reference count.\n   *\n   * @return                 The current reference count.",
  "org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException:<init>(int,int)" : null,
  "org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.Path)" : "* convert an array of FileStatus to an array of Path.\n   * If stats if null, return path\n   * @param stats\n   *          an array of FileStatus objects\n   * @param path\n   *          default path to return in stats is null\n   * @return an array of paths corresponding to the input",
  "org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)" : "* Get a proxy connection to a remote server.\n   *\n   * @param <T> Generics Type T.\n   * @param protocol protocol class\n   * @param clientVersion client version\n   * @param addr remote address\n   * @param conf configuration to use\n   * @param connTimeout time in milliseconds before giving up\n   * @return the proxy\n   * @throws IOException if the far end through a RemoteException",
  "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)" : "* Get a protocol proxy that contains a proxy connection to a remote server\n   * and a set of methods that are supported by the server\n   * \n   * @param protocol input protocol.\n   * @param clientVersion input clientVersion.\n   * @param addr input addr.\n   * @param conf input configuration.\n   * @param <T> Generics Type T.\n   * @return a protocol proxy\n   * @throws IOException if the thread is interrupted.",
  "org.apache.hadoop.ha.NodeFencer$FenceMethodWithArg:<init>(org.apache.hadoop.ha.FenceMethod,java.lang.String)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:bytesToHex(byte[],int)" : "* Convert bytes into format like 0x02 02 00 80.\n   * If limit is negative or too large, then all bytes will be converted.\n   *\n   * @param bytes bytes.\n   * @param limit limit.\n   * @return bytesToHex.",
  "org.apache.hadoop.metrics2.lib.MutableRollingAverages$SumAndCount:getSum()" : null,
  "org.apache.hadoop.fs.shell.Ls:isPathOnly()" : "* Should display only paths of files and directories.\n   * @return true display paths only, false display all fields",
  "org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean)" : "* Launch a service catching all exceptions and downgrading them to exit codes\n   * after logging.\n   *\n   * Sets {@link #serviceException} to this value.\n   * @param conf configuration to use\n   * @param instance optional instance of the service.\n   * @param processedArgs command line after the launcher-specific arguments\n   * have been stripped out.\n   * @param addShutdownHook should a shutdown hook be added to terminate\n   * this service on shutdown. Tests should set this to false.\n   * @param execute execute/wait for the service to stop.\n   * @return an exit exception, which will have a status code of 0 if it worked",
  "org.apache.hadoop.ipc.Client$Connection:setupConnection(org.apache.hadoop.security.UserGroupInformation)" : null,
  "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.reflect.Method)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$DataBlock:verifyState(org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState)" : "* Verify that the block is in the declared state.\n     *\n     * @param expected expected state.\n     * @throws IllegalStateException if the DataBlock is in the wrong state",
  "org.apache.hadoop.fs.sftp.SFTPConnectionPool:getFromPool(org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState)" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:setInputFromSavedData()" : "* If a write would exceed the capacity of the direct buffers, it is set\n   * aside to be loaded by this function while the compressed data are\n   * consumed.",
  "org.apache.hadoop.util.Lists:partition(java.util.List,int)" : "* Returns consecutive sub-lists of a list, each of the same size\n   * (the final list may be smaller).\n   * @param originalList original big list.\n   * @param pageSize desired size of each sublist ( last one\n   *                 may be smaller)\n   * @param <T> Generics Type.\n   * @return a list of sub lists.",
  "org.apache.hadoop.util.SysInfoWindows:getNumProcessors()" : "{@inheritDoc}",
  "org.apache.hadoop.util.functional.FutureIO:<init>()" : null,
  "org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String,boolean)" : "* Change the permissions on a file / directory, recursively, if\n   * needed.\n   * @param filename name of the file whose permissions are to change\n   * @param perm permission string\n   * @param recursive true, if permissions should be changed recursively\n   * @return the exit code from the command.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:logUpdateMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String)" : "* Create a mutable rate metric\n   * @param name  of the metric\n   * @param description of the metric\n   * @return a new mutable rate metric object",
  "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:close(java.lang.Object)" : "* Derived classes may implement a way to cleanup each item.",
  "org.apache.hadoop.fs.shell.TouchCommands$Touchz:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.ipc.CallQueueManager:poll(long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:compareTo(org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair)" : null,
  "org.apache.hadoop.io.MapWritable:get(java.lang.Object)" : null,
  "org.apache.hadoop.fs.FSInputChecker:markSupported()" : null,
  "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getNumParityUnits()" : null,
  "org.apache.hadoop.fs.FileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : "* Remove an xattr of a file or directory.\n   * The name must be prefixed with the namespace followed by \".\". For example,\n   * \"user.attr\".\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to remove extended attribute\n   * @param name xattr name\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)" : "* Construct a reader by opening a file from the given file system.\n     * @param fs The file system used to open the file.\n     * @param file The file being read.\n     * @param conf Configuration\n     * @throws IOException raised on errors performing I/O.\n     * @deprecated Use Reader(Configuration, Option...) instead.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackDuration(java.lang.String,long)" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:setJaasConfiguration(org.apache.zookeeper.client.ZKClientConfig)" : null,
  "org.apache.hadoop.fs.Path:checkNotRelative()" : null,
  "org.apache.hadoop.net.NetworkTopology:add(org.apache.hadoop.net.Node)" : "Add a leaf node\n   * Update node counter &amp; rack counter if necessary\n   * @param node node to be added; can be null\n   * @exception IllegalArgumentException if add a node to a leave \n                                         or node to be added is not a leaf",
  "org.apache.hadoop.io.compress.BZip2Codec:getConf()" : "* Return the configuration used by this object.\n   *\n   * @return the configuration object used by this objec.",
  "org.apache.hadoop.crypto.CryptoOutputStream:hsync()" : null,
  "org.apache.hadoop.io.GenericWritable:getConf()" : null,
  "org.apache.hadoop.util.dynamic.BindingUtils:available(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod)" : "* Is a method available?\n   * @param method method to probe\n   * @return true iff the method is found and loaded.",
  "org.apache.hadoop.util.LightWeightCache:put(java.lang.Object)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibFactory:setCompressionLevel(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel)" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarStatus:isDir()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)" : "* Return a set of server default configuration values based on path.\n   * @param f path to fetch server defaults\n   * @return server default configuration values for path\n   * @throws IOException an I/O error occurred",
  "org.apache.hadoop.fs.QuotaUsage$Builder:typeQuota(long[])" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkIntegerMultiple(long,java.lang.String,long,java.lang.String)" : "* Validates that the first value is an integer multiple of the second value.\n   * @param value1 the first value to check.\n   * @param value1Name the name of the first argument.\n   * @param value2 the second value to check.\n   * @param value2Name the name of the second argument.",
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean)" : null,
  "org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.conf.Configuration)" : "* Create a FileContext using the passed config. Generally it is better to use\n   * {@link #getFileContext(URI, Configuration)} instead of this one.\n   * \n   * \n   * @param aConf configration.\n   * @return new FileContext\n   * @throws UnsupportedFileSystemException If file system in the config\n   *           is not supported",
  "org.apache.hadoop.net.NodeBase:<init>()" : "Default constructor",
  "org.apache.hadoop.io.compress.CompressionInputStream:getIOStatistics()" : "* Return any IOStatistics provided by the underlying stream.\n   * @return IO stats from the inner stream.",
  "org.apache.hadoop.fs.shell.Display$Cat:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.FSDataInputStream:getIOStatistics()" : "* Get the IO Statistics of the nested stream, falling back to\n   * null if the stream does not implement the interface\n   * {@link IOStatisticsSource}.\n   * @return an IOStatistics instance or null",
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:startTimer()" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:responses3xx()" : null,
  "org.apache.hadoop.fs.DF:getExecString()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementDelegationTokenSeqNum()" : null,
  "org.apache.hadoop.fs.store.ByteBufferInputStream:read(byte[],int,int)" : "* Read in data.\n   * @param b destination buffer.\n   * @param offset offset within the buffer.\n   * @param length length of bytes to read.\n   * @throws EOFException if the position is negative\n   * @throws IndexOutOfBoundsException if there isn't space for the\n   * amount of data requested.\n   * @throws IllegalArgumentException other arguments are invalid.",
  "org.apache.hadoop.security.alias.LocalKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char[])" : "* escapeString.\n   *\n   * @param str str.\n   * @param escapeChar escapeChar.\n   * @param charsToEscape array of characters to be escaped\n   * @return escapeString.",
  "org.apache.hadoop.io.SequenceFile$Writer$FileSystemOption:getValue()" : null,
  "org.apache.hadoop.fs.FileSystem:truncate(org.apache.hadoop.fs.Path,long)" : "* Truncate the file in the indicated path to the indicated size.\n   * <ul>\n   *   <li>Fails if path is a directory.</li>\n   *   <li>Fails if path does not exist.</li>\n   *   <li>Fails if path is not closed.</li>\n   *   <li>Fails if new size is greater than current size.</li>\n   * </ul>\n   * @param f The path to the file to be truncated\n   * @param newLength The size the file is to be truncated to\n   *\n   * @return <code>true</code> if the file has been truncated to the desired\n   * <code>newLength</code> and is immediately available to be reused for\n   * write operations such as <code>append</code>, or\n   * <code>false</code> if a background process of adjusting the length of\n   * the last block has been started, and clients should wait for it to\n   * complete before proceeding with further file updates.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).",
  "org.apache.hadoop.fs.viewfs.ViewFs$WrappingRemoteIterator:hasNext()" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:isNoop()" : "* @return whether the method is a noop",
  "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.io.CompressedWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttrs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.StringUtils:toUpperCase(java.lang.String)" : "* Converts all of the characters in this String to upper case with\n   * Locale.ENGLISH.\n   *\n   * @param str  string to be converted\n   * @return     the str, converted to uppercase.",
  "org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[],java.util.Map)" : null,
  "org.apache.hadoop.security.UserGroupInformation$LoginParams:getDefaults()" : null,
  "org.apache.hadoop.conf.StorageUnit$1:getDefault(double)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesReadLocalHost()" : null,
  "org.apache.hadoop.util.StringUtils:findNext(java.lang.String,char,char,int,java.lang.StringBuilder)" : "* Finds the first occurrence of the separator character ignoring the escaped\n   * separators starting from the index. Note the substring between the index\n   * and the position of the separator is passed.\n   * @param str the source string\n   * @param separator the character to find\n   * @param escapeChar character used to escape\n   * @param start from where to search\n   * @param split used to pass back the extracted string\n   * @return index.",
  "org.apache.hadoop.security.SaslOutputStream:write(int)" : "* Writes the specified byte to this output stream.\n   * \n   * @param b\n   *          the <code>byte</code>.\n   * @exception IOException\n   *              if an I/O error occurs.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:copyMap(java.util.Map,java.util.Map,java.util.function.Function)" : "* Copy into the dest map all the source entries.\n   * The destination is cleared first.\n   * @param <E> entry type\n   * @param dest destination of the copy\n   * @param source source\n   * @param copyFn function to copy entries\n   * @return the destination.",
  "org.apache.hadoop.ha.ActiveStandbyElector:reEstablishSession()" : null,
  "org.apache.hadoop.conf.Configuration:toString(java.util.List,java.lang.StringBuilder)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO:getShareDeleteFileDescriptor(java.io.File,long)" : "* Create a FileDescriptor that shares delete permission on the\n   * file opened at a given offset, i.e. other process can delete\n   * the file the FileDescriptor is reading. Only Windows implementation\n   * uses the native interface.\n   *\n   * @param f input f.\n   * @param seekOffset input seekOffset.\n   * @return FileDescriptor.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion:getLength()" : null,
  "org.apache.hadoop.service.ServiceOperations:<init>()" : null,
  "org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map,long,boolean)" : "* Create a new instance of the ShellCommandExecutor to execute a command.\n     *\n     * @param execString The command to execute with arguments\n     * @param dir If not-null, specifies the directory which should be set\n     *            as the current working directory for the command.\n     *            If null, the current working directory is not modified.\n     * @param env If not-null, environment of the command will include the\n     *            key-value pairs specified in the map. If null, the current\n     *            environment is not modified.\n     * @param timeout Specifies the time in milliseconds, after which the\n     *                command will be killed and the status marked as timed-out.\n     *                If 0, the command will not be timed out.\n     * @param inheritParentEnv Indicates if the process should inherit the env\n     *                         vars from the parent process or not.",
  "org.apache.hadoop.fs.shell.Tail:dumpFromOffset(org.apache.hadoop.fs.shell.PathData,long)" : null,
  "org.apache.hadoop.util.JvmPauseMonitor:formatMessage(long,java.util.Map,java.util.Map)" : null,
  "org.apache.hadoop.io.MapWritable:<init>(org.apache.hadoop.io.MapWritable)" : "* Copy constructor.\n   * \n   * @param other the map to copy from",
  "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:bufferCapacityUsed()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferData:getFutureStr(java.util.concurrent.Future)" : null,
  "org.apache.hadoop.fs.FSInputStream:validatePositionedReadArgs(long,byte[],int,int)" : "* Validation code, available for use in subclasses.\n   * @param position position: if negative an EOF exception is raised\n   * @param buffer destination buffer\n   * @param offset offset within the buffer\n   * @param length length of bytes to read\n   * @throws EOFException if the position is negative\n   * @throws IndexOutOfBoundsException if there isn't space for the amount of\n   * data requested.\n   * @throws IllegalArgumentException other arguments are invalid.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)" : "* Move the cursor to the new location. The entry returned by the previous\n       * entry() call will be invalid.\n       * \n       * @param l\n       *          new cursor location. It must fall between the begin and end\n       *          location of the scanner.\n       * @throws IOException",
  "org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:getAclEntries(org.apache.hadoop.fs.shell.PathData)" : "* Returns the ACL entries to use in the API call for the given path.  For a\n     * recursive operation, returns all specified ACL entries if the item is a\n     * directory or just the access ACL entries if the item is a file.  For a\n     * non-recursive operation, returns all specified ACL entries.\n     *\n     * @param item PathData path to check\n     * @return List<AclEntry> ACL entries to use in the API call",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:warmUpEncryptedKeys(java.lang.String[])" : null,
  "org.apache.hadoop.util.CrcComposer:update(java.io.DataInputStream,long,long)" : "* Composes {@code numChecksumsToRead} additional CRCs into the current digest\n   * out of {@code checksumIn}, with each CRC expected to correspond to exactly\n   * {@code bytesPerCrc} underlying data bytes.\n   *\n   * @param checksumIn checksumIn.\n   * @param numChecksumsToRead numChecksumsToRead.\n   * @param bytesPerCrc bytesPerCrc.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:blockAddedToFileCache()" : null,
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:getWeight(java.util.List)" : null,
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor:getPermitCount()" : "* Total number of permits.\n   * @return the number of permits as set in the constructor",
  "org.apache.hadoop.util.StringUtils:popFirstNonOption(java.util.List)" : "* From a list of command-line arguments, return the first non-option\n   * argument.  Non-option arguments are those which either come after \n   * a double dash (--) or do not start with a dash.\n   *\n   * @param args  List of arguments.\n   * @return      The first non-option argument, or null if there were none.",
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:getDebugInfo()" : null,
  "org.apache.hadoop.io.SequenceFile$RecordCompressWriter:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])" : null,
  "org.apache.hadoop.ipc.RPC$Server:serverNameFromClass(java.lang.Class)" : "* Get a meaningful and short name for a server based on a java class.\n     *\n     * The rules are defined to support the current naming schema of the\n     * generated protobuf classes where the final class usually an anonymous\n     * inner class of an inner class.\n     *\n     * 1. For simple classes it returns with the simple name of the classes\n     *     (with the name without package name)\n     *\n     * 2. For inner classes, this is the simple name of the inner class.\n     *\n     * 3.  If it is an Object created from a class factory\n     *   E.g., org.apache.hadoop.ipc.TestRPC$TestClass$2\n     * this method returns parent class TestClass.\n     *\n     * 4. If it is an anonymous class E.g., 'org.apache.hadoop.ipc.TestRPC$10'\n     * serverNameFromClass returns parent class TestRPC.\n     *\n     *",
  "org.apache.hadoop.http.ProfileServlet:getLong(javax.servlet.http.HttpServletRequest,java.lang.String)" : null,
  "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:allocateByteBuffer(boolean,int)" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processKeyRemoved(java.lang.String)" : null,
  "org.apache.hadoop.security.UserGroupInformation:setLastLogin(long)" : "* Set the last login time for logged in user\n   * @param loginTime the number of milliseconds since the beginning of time",
  "org.apache.hadoop.util.StopWatch:<init>(org.apache.hadoop.util.Timer)" : "* Used for tests to be able to create a StopWatch which does not follow real\n   * time.\n   * @param timer The timer to base this StopWatch's timekeeping off of.",
  "org.apache.hadoop.io.compress.lz4.Lz4Compressor:reset()" : "* Resets compressor so that a new set of input data can be processed.",
  "org.apache.hadoop.fs.shell.Command:run(java.lang.String[])" : "* Invokes the command handler.  The default behavior is to process options,\n   * expand arguments, and then process each argument.\n   * <pre>\n   * run\n   * |{@literal ->} {@link #processOptions(LinkedList)}\n   * \\{@literal ->} {@link #processRawArguments(LinkedList)}\n   *      |{@literal ->} {@link #expandArguments(LinkedList)}\n   *      |   \\{@literal ->} {@link #expandArgument(String)}*\n   *      \\{@literal ->} {@link #processArguments(LinkedList)}\n   *          |{@literal ->} {@link #processArgument(PathData)}*\n   *          |   |{@literal ->} {@link #processPathArgument(PathData)}\n   *          |   \\{@literal ->} {@link #processPaths(PathData, PathData...)}\n   *          |        \\{@literal ->} {@link #processPath(PathData)}*\n   *          \\{@literal ->} {@link #processNonexistentPath(PathData)}\n   * </pre>\n   * Most commands will chose to implement just\n   * {@link #processOptions(LinkedList)} and {@link #processPath(PathData)}\n   * \n   * @param argv the list of command line arguments\n   * @return the exit code for the command\n   * @throws IllegalArgumentException if called with invalid arguments",
  "org.apache.hadoop.security.ssl.SSLFactory:createSSLServerSocketFactory()" : "* Returns a configured SSLServerSocketFactory.\n   *\n   * @return the configured SSLSocketFactory.\n   * @throws GeneralSecurityException thrown if the SSLSocketFactory could not\n   * be initialized.\n   * @throws IOException thrown if and IO error occurred while loading\n   * the server keystore.",
  "org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.lang.String,int,int,java.util.Map)" : "* Constructor with key parameters provided. Note the extraOptions may contain\n   * additional information for the erasure codec to interpret further.\n   * @param codecName codec name\n   * @param numDataUnits number of data units used in the schema\n   * @param numParityUnits number os parity units used in the schema\n   * @param extraOptions extra options to configure the codec",
  "org.apache.hadoop.util.StringInterner:strongIntern(java.lang.String)" : "* Interns and returns a reference to the representative instance \n   * for any of a collection of string instances that are equal to each other.\n   * Retains strong reference to the instance, \n   * thus preventing it from being garbage-collected. \n   * \n   * @param sample string instance to be interned\n   * @return strong reference to interned string instance",
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:seek(long)" : null,
  "org.apache.hadoop.http.HtmlQuoting:needsQuoting(java.lang.String)" : "* Does the given string need to be quoted?\n   * @param str the string to check\n   * @return does the string contain any of the active html characters?",
  "org.apache.hadoop.fs.HarFileSystem:getUsed(org.apache.hadoop.fs.Path)" : "Return the total size of all files from a specified path.",
  "org.apache.hadoop.fs.ftp.FTPFileSystem:disconnect(org.apache.commons.net.ftp.FTPClient)" : "* Logout and disconnect the given FTPClient. *\n   * \n   * @param client\n   * @throws IOException",
  "org.apache.hadoop.io.InputBuffer:reset(byte[],int)" : "* Resets the data that the buffer reads.\n   * @param input input.\n   * @param length length.",
  "org.apache.hadoop.fs.FSInputStream:readFully(long,byte[],int,int)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAllStoragePolicies()" : null,
  "org.apache.hadoop.fs.FileSystem:newInstance(org.apache.hadoop.conf.Configuration)" : "* Returns a unique configured FileSystem implementation for the default\n   * filesystem of the supplied configuration.\n   * This always returns a new FileSystem object.\n   * @param conf the configuration to use\n   * @return the new FS instance\n   * @throws IOException FS creation or initialization failure.",
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:flushBuffer()" : "* Flush the internal buffer.\n     * \n     * Is this the last call to flushBuffer?\n     * \n     * @throws java.io.IOException",
  "org.apache.hadoop.fs.ftp.FTPInputStream:seek(long)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:makeComparator(java.lang.String)" : null,
  "org.apache.hadoop.ha.StreamPumper:join()" : null,
  "org.apache.hadoop.fs.Trash:moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)" : "* In case of the symlinks or mount points, one has to move the appropriate\n   * trashbin in the actual volume of the path p being deleted.\n   *\n   * Hence we get the file system of the fully-qualified resolved-path and\n   * then move the path p to the trashbin in that volume,\n   * @param fs - the filesystem of path p\n   * @param p - the path being deleted - to be moved to trash\n   * @param conf - configuration\n   * @return false if the item is already in the trash or trash is disabled\n   * @throws IOException on error",
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:checksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt)" : "* Set checksum opt.",
  "org.apache.hadoop.security.authorize.AccessControlList:write(java.io.DataOutput)" : "* Serializes the AccessControlList object",
  "org.apache.hadoop.ipc.Server:refreshCallQueue(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.functional.LazyAtomicReference:isSet()" : "* Is the reference set?\n   * @return true if the reference has been set.",
  "org.apache.hadoop.ipc.CallQueueManager:getServerFailOverEnable(java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Return boolean value configured by property 'ipc.<port>.callqueue.overflow.trigger.failover'\n   * if it is present. If the config is not present, default config\n   * (without port) is used to derive class i.e 'ipc.callqueue.overflow.trigger.failover',\n   * and derived value is returned if configured. Otherwise, default value\n   * {@link CommonConfigurationKeys#IPC_CALLQUEUE_SERVER_FAILOVER_ENABLE_DEFAULT} is returned.\n   *\n   * @param namespace Namespace \"ipc\" + \".\" + Server's listener port.\n   * @param conf Configuration properties.\n   * @return Value returned based on configuration.",
  "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:refresh(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferPool:release(org.apache.hadoop.fs.impl.prefetch.BufferData)" : "* Releases a previously acquired resource.\n   * @param data the {@code BufferData} instance to release.\n   * @throws IllegalArgumentException if data is null.\n   * @throws IllegalArgumentException if data cannot be released due to its state.",
  "org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:<init>(byte[],int,java.lang.Object,long)" : null,
  "org.apache.hadoop.security.ProviderUtils:<init>()" : "* Hidden ctor to ensure that this utility class isn't\n   * instantiated explicitly.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getMasterKeyId()" : null,
  "org.apache.hadoop.io.file.tfile.CompareUtils$ScalarLong:magnitude()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache$Key:<init>(java.net.URI)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:sleepFor(int)" : "* Sleep for the given number of milliseconds.\n   * This is non-static, and separated out, so that unit tests\n   * can override the behavior not to sleep.\n   *\n   * @param sleepMs sleep ms.",
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:release()" : null,
  "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:process(java.nio.ByteBuffer,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.metrics2.MetricsJsonBuilder:<init>(org.apache.hadoop.metrics2.MetricsCollector)" : "* Build an instance.\n   * @param parent parent collector. Unused in this instance; only used for\n   * the {@link #parent()} method",
  "org.apache.hadoop.ha.ActiveStandbyElector:isNodeDoesNotExist(org.apache.zookeeper.KeeperException$Code)" : null,
  "org.apache.hadoop.fs.http.HttpsFileSystem:getFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:tokenFromProto(org.apache.hadoop.security.proto.SecurityProtos$TokenProto)" : "* Create a hadoop token from a protobuf token.\n   * @param tokenProto token\n   * @return a new token",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_getCurrent()" : "* Get the context's {@code IOStatisticsContext} which\n   * implements {@code IOStatisticsSource}.\n   * This is either a thread-local value or a global empty context.\n   * @return instance of {@code IOStatisticsContext}.\n   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",
  "org.apache.hadoop.util.FindClass:loadedClass(java.lang.String,java.lang.Class)" : "* Log that a class has been loaded, and where from.\n   * @param name classname\n   * @param clazz class",
  "org.apache.hadoop.security.authorize.AccessControlList:removeUser(java.lang.String)" : "* Remove user from the names of users allowed for this service.\n   * \n   * @param user\n   *          The user name",
  "org.apache.hadoop.ipc.FairCallQueue:offer(org.apache.hadoop.ipc.Schedulable,long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.util.LightWeightGSet:printDetails(java.io.PrintStream)" : "* Print detailed information of this object.\n   *\n   * @param out out.",
  "org.apache.hadoop.io.SequenceFile$Writer:append(java.lang.Object,java.lang.Object)" : "* Append a key/value pair.\n     * @param key input Object key.\n     * @param val input Object val.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ha.ZKFailoverController:recordActiveAttempt(org.apache.hadoop.ha.ZKFailoverController$ActiveAttemptRecord)" : "* Store the results of the last attempt to become active.\n   * This is used so that, during manually initiated failover,\n   * we can report back the results of the attempt to become active\n   * to the initiator of the failover.",
  "org.apache.hadoop.fs.shell.find.FilterExpression:addChildren(java.util.Deque)" : null,
  "org.apache.hadoop.io.compress.BZip2Codec:createOutputStream(java.io.OutputStream)" : "* Create a {@link CompressionOutputStream} that will write to the given\n   * {@link OutputStream}.\n   *\n   * @param out        the location for the final output stream\n   * @return a stream the user can write uncompressed data to, to have it \n   *         compressed\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.CachingGetSpaceUsed:close()" : null,
  "org.apache.hadoop.crypto.CryptoProtocolVersion:toString()" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem:getFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.FindClass:explainResult(int,java.lang.String)" : "* Explain an error code as part of the usage\n   * @param errorcode error code returned\n   * @param text error text",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:openFile(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.compress.DefaultCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)" : null,
  "org.apache.hadoop.util.SysInfoLinux:getCurrentTime()" : "* Get current time.\n   * @return Unix time stamp in millisecond",
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:<init>(java.io.DataInputStream)" : "* Constructor\n     * \n     * @param in\n     *          The source input stream which contains chunk-encoded data\n     *          stream.",
  "org.apache.hadoop.ipc.Server$Connection:processRpcOutOfBandRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)" : "* Establish RPC connection setup by negotiating SASL if required, then\n     * reading and authorizing the connection header\n     * @param header - RPC header\n     * @param buffer - stream to request payload\n     * @throws RpcServerException - setup failed due to SASL\n     *         negotiation failure, premature or invalid connection context,\n     *         or other state errors. This exception needs to be sent to the \n     *         client.\n     * @throws IOException - failed to send a response back to the client\n     * @throws InterruptedException",
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:writeBufData(byte[],int,int)" : "* Write out a chunk that is a concatenation of the internal buffer plus\n     * user supplied data. This will never be the last block.\n     * \n     * @param data\n     *          User supplied data buffer.\n     * @param offset\n     *          Offset to user data buffer.\n     * @param len\n     *          User data buffer size.",
  "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)" : "Creates a file on the local FS. Pass size as \n     * {@link LocalDirAllocator.SIZE_UNKNOWN} if not known apriori. We\n     *  round-robin over the set of disks (via the configured dirs) and return\n     *  a file on the first path which has enough space. The file is guaranteed\n     *  to go away when the JVM exits.",
  "org.apache.hadoop.fs.FSDataInputStream:seekToNewSource(long)" : "* Seek to the given position on an alternate copy of the data.\n   *\n   * @param  targetPos  position to seek to\n   * @return true if a new source is found, false otherwise",
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:needsPassword()" : null,
  "org.apache.hadoop.metrics2.MetricStringBuilder:toString()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:wrapLocalFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider$TokenSelector:<init>()" : null,
  "org.apache.hadoop.fs.LocalFileSystemPathHandle:bytes()" : null,
  "org.apache.hadoop.util.InstrumentedLock:logWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX:setPmdkSupportState(int)" : null,
  "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getCurrentDirectoryIndex()" : "* Get the current directory index.\n     * @return the current directory index.",
  "org.apache.hadoop.io.SequenceFile$Writer$ProgressableOption:<init>(org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream)" : "* Add a configuration resource. \n   * \n   * The properties of this resource will override properties of previously \n   * added resources, unless they were marked <a href=\"#Final\">final</a>. \n   * \n   * WARNING: The contents of the InputStream will be cached, by this method. \n   * So use this sparingly because it does increase the memory consumption.\n   * \n   * @param in InputStream to deserialize the object from. In will be read from\n   * when a get or set is called next.  After it is read the stream will be\n   * closed.",
  "org.apache.hadoop.tracing.NullTraceScope:<init>()" : null,
  "org.apache.hadoop.net.NetUtils:see(java.lang.String)" : null,
  "org.apache.hadoop.util.HostsFileReader:readFileToSetWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Set)" : null,
  "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:getChunkPosition(long)" : null,
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem:delete(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.http.HttpServer2:stop()" : "* stop the server.\n   *\n   * @throws Exception exception.",
  "org.apache.hadoop.crypto.CryptoInputStream:getPos()" : "Get underlying stream position.",
  "org.apache.hadoop.util.Shell:getRunScriptCommand(java.io.File)" : "* Returns a command to run the given script.  The script interpreter is\n   * inferred by platform: cmd on Windows or bash otherwise.\n   *\n   * @param script File script to run\n   * @return String[] command to run the script",
  "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer)" : null,
  "org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytabAndReturnUGI(java.lang.String,java.lang.String)" : "* Log a user in from a keytab file. Loads a user identity from a keytab\n   * file and login them in. This new user does not affect the currently\n   * logged-in user.\n   * @param user the principal name to load from the keytab\n   * @param path the path to the keytab file\n   * @throws IOException if the keytab file can't be read\n   * @return UserGroupInformation.",
  "org.apache.hadoop.io.UTF8:set(org.apache.hadoop.io.UTF8)" : "* Set to contain the contents of a string.\n   * @param other input other.",
  "org.apache.hadoop.fs.statistics.impl.StubDurationTracker:<init>()" : null,
  "org.apache.hadoop.io.BytesWritable:<init>()" : "* Create a zero-size sequence.",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:incrSentBytes(int)" : null,
  "org.apache.hadoop.net.CachedDNSToSwitchMapping:getUncachedHosts(java.util.List)" : "* @param names a list of hostnames to probe for being cached\n   * @return the hosts from 'names' that have not been cached previously",
  "org.apache.hadoop.fs.shell.Command:isDeprecated()" : "* Is the command deprecated?\n   * @return boolean",
  "org.apache.hadoop.ipc.Server:getLogSlowRPCThresholdTime()" : null,
  "org.apache.hadoop.fs.impl.FlagSet:isEmpty()" : "* Probe for the FlagSet being empty.\n   * @return true if there are no flags set.",
  "org.apache.hadoop.io.MapFile$Reader:binarySearch(org.apache.hadoop.io.WritableComparable)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)" : "* Create a mutable metric that estimates quantiles of a stream of values\n   * @param name of the metric\n   * @param desc metric description\n   * @param sampleName of the metric (e.g., \"Ops\")\n   * @param valueName of the metric (e.g., \"Time\" or \"Latency\")\n   * @param interval rollover interval of estimator in seconds\n   * @return a new quantile estimator object\n   * @throws MetricsException if interval is not a positive integer",
  "org.apache.hadoop.io.DefaultStringifier:load(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)" : "* Restores the object from the configuration.\n   * \n   * @param <K> the class of the item\n   * @param conf the configuration to use\n   * @param keyName the name of the key to use\n   * @param itemClass the class of the item\n   * @return restored object\n   * @throws IOException : forwards Exceptions from the underlying \n   * {@link Serialization} classes.",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsConfig:getPropertyInternal(java.lang.String)" : "* Will poke parents for defaults\n   * @param key to lookup\n   * @return  the value or null",
  "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean)" : null,
  "org.apache.hadoop.security.token.delegation.DelegationKey:setExpiryDate(long)" : null,
  "org.apache.hadoop.crypto.key.KeyProviderExtension:getKeyVersion(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.FsCommand:processRawArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.FileSystem:getDefaultPort()" : "* Get the default port for this FileSystem.\n   * @return the default port or 0 if there isn't one",
  "org.apache.hadoop.io.erasurecode.ECChunk:toBuffers(org.apache.hadoop.io.erasurecode.ECChunk[])" : "* Convert an array of this chunks to an array of ByteBuffers\n   * @param chunks chunks to convert into buffers\n   * @return an array of ByteBuffers",
  "org.apache.hadoop.ipc.ProcessingDetails:setReturnStatus(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getUri()" : "Returns a URI whose scheme and authority identify this FileSystem.",
  "org.apache.hadoop.security.UserGroupInformation:equals(java.lang.Object)" : "* Compare the subjects to see if they are equal to each other.",
  "org.apache.hadoop.fs.HardLink$HardLinkCGUnix:linkCount(java.io.File)" : null,
  "org.apache.hadoop.ipc.Client:getConnection(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.ipc.Client$Call,int,java.util.concurrent.atomic.AtomicBoolean)" : "Get a connection from the pool, or create a new one and add it to the\n   * pool.  Connections to a given ConnectionId are reused.",
  "org.apache.hadoop.util.NativeCrc32:isAvailable()" : "* Return true if the JNI-based native CRC extensions are available.",
  "org.apache.hadoop.fs.impl.prefetch.BlockData:getBlockSize()" : "* Gets the size of each block.\n   * @return the size of each block.",
  "org.apache.hadoop.metrics2.lib.MethodMetric:newGauge(java.lang.Class)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendKey(int)" : "* Obtain an output stream for writing a key into TFile. This may only be\n     * called when there is no active Key appending stream or value appending\n     * stream.\n     * \n     * @param length\n     *          The expected length of the key. If length of the key is not\n     *          known, set length = -1. Otherwise, the application must write\n     *          exactly as many bytes as specified here before calling close on\n     *          the returned output stream.\n     * @return The key appending output stream.\n     * @throws IOException raised on errors performing I/O.\n     *",
  "org.apache.hadoop.util.functional.TaskPool:castAndThrow(java.lang.Exception)" : "* Raise an exception of the declared type.\n   * This method never completes normally.\n   * @param e exception\n   * @param <E> class of exceptions\n   * @throws E a recast exception.",
  "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:incRecordCount()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : "* The specification of this method matches that of\n   * {@link FileContext#renameSnapshot(Path, String, String)}.\n   *\n   * @param path the path.\n   * @param snapshotOldName snapshot old name.\n   * @param snapshotNewName snapshot new name.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.VLongWritable:compareTo(org.apache.hadoop.io.VLongWritable)" : "Compares two VLongWritables.",
  "org.apache.hadoop.io.retry.RetryInvocationHandler:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)" : null,
  "org.apache.hadoop.fs.FilterFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : "* A stream obtained via this call must be closed before using other APIs of\n   * this class or else the invocation will block.",
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addDirLink(java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$INodeDirLink)" : null,
  "org.apache.hadoop.ipc.Server:getConf()" : null,
  "org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)" : "* Return an array containing hostnames, offset and size of\n   * portions of the given file.  For nonexistent\n   * file or regions, {@code null} is returned.\n   *\n   * <pre>\n   *   if f == null :\n   *     result = null\n   *   elif f.getLen() {@literal <=} start:\n   *     result = []\n   *   else result = [ locations(FS, b) for b in blocks(FS, p, s, s+l)]\n   * </pre>\n   * This call is most helpful with and distributed filesystem\n   * where the hostnames of machines that contain blocks of the given file\n   * can be determined.\n   *\n   * The default implementation returns an array containing one element:\n   * <pre>\n   * BlockLocation( { \"localhost:9866\" },  { \"localhost\" }, 0, file.getLen())\n   * </pre>\n   *\n   * In HDFS, if file is three-replicated, the returned array contains\n   * elements like:\n   * <pre>\n   * BlockLocation(offset: 0, length: BLOCK_SIZE,\n   *   hosts: {\"host1:9866\", \"host2:9866, host3:9866\"})\n   * BlockLocation(offset: BLOCK_SIZE, length: BLOCK_SIZE,\n   *   hosts: {\"host2:9866\", \"host3:9866, host4:9866\"})\n   * </pre>\n   *\n   * And if a file is erasure-coded, the returned BlockLocation are logical\n   * block groups.\n   *\n   * Suppose we have a RS_3_2 coded file (3 data units and 2 parity units).\n   * 1. If the file size is less than one stripe size, say 2 * CELL_SIZE, then\n   * there will be one BlockLocation returned, with 0 offset, actual file size\n   * and 4 hosts (2 data blocks and 2 parity blocks) hosting the actual blocks.\n   * 3. If the file size is less than one group size but greater than one\n   * stripe size, then there will be one BlockLocation returned, with 0 offset,\n   * actual file size with 5 hosts (3 data blocks and 2 parity blocks) hosting\n   * the actual blocks.\n   * 4. If the file size is greater than one group size, 3 * BLOCK_SIZE + 123\n   * for example, then the result will be like:\n   * <pre>\n   * BlockLocation(offset: 0, length: 3 * BLOCK_SIZE, hosts: {\"host1:9866\",\n   *   \"host2:9866\",\"host3:9866\",\"host4:9866\",\"host5:9866\"})\n   * BlockLocation(offset: 3 * BLOCK_SIZE, length: 123, hosts: {\"host1:9866\",\n   *   \"host4:9866\", \"host5:9866\"})\n   * </pre>\n   *\n   * @param file FilesStatus to get data from\n   * @param start offset into the given file\n   * @param len length for which to get locations for\n   * @throws IOException IO failure\n   * @return block location array.",
  "org.apache.hadoop.util.VersionInfo:getSrcChecksum()" : "* Get the checksum of the source files from which Hadoop was built.\n   * @return the checksum of the source files",
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:innerSetKeyVersion(java.lang.String,java.lang.String,byte[],java.lang.String)" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,int,long,long,org.apache.hadoop.conf.Configuration)" : "* Construct a reader by the given input stream.\n     * @param in An input stream.\n     * @param buffersize unused\n     * @param start The starting position.\n     * @param length The length being read.\n     * @param conf Configuration\n     * @throws IOException raised on errors performing I/O.\n     * @deprecated Use Reader(Configuration, Reader.Option...) instead.",
  "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addOverallProcessingTime(java.lang.String,long)" : "* Add an overall RPC processing time sample.\n   * @param rpcCallName of the RPC call\n   * @param overallProcessingTime  the overall RPC processing time",
  "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String)" : "* Constructor for the helper class to configure the ZooKeeper client connection.\n     * @param zkPrincipal Optional.",
  "org.apache.hadoop.ipc.RPC:getConnectionIdForProxy(java.lang.Object)" : "* Return the connection ID of the given object. If the provided object is in\n   * fact a protocol translator, we'll get the connection ID of the underlying\n   * proxy object.\n   * \n   * @param proxy the proxy object to get the connection ID of.\n   * @return the connection ID for the provided proxy object.",
  "org.apache.hadoop.ipc.Server:getMaxQueueSize()" : "* The maximum size of the rpc call queue of this server.\n   * @return The maximum size of the rpc call queue.",
  "org.apache.hadoop.util.RateLimitingFactory:create(int)" : "* Create an instance.\n   * If the rate is 0; return the unlimited rate.\n   * @param capacity capacity in permits/second.\n   * @return limiter restricted to the given capacity.",
  "org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)" : "* Construct from a path handle.\n     * @param fileSystem owner\n     * @param pathHandle path handle of file to open.",
  "org.apache.hadoop.io.WritableUtils:writeVInt(java.io.DataOutput,int)" : "* Serializes an integer to a binary stream with zero-compressed encoding.\n   * For -112 {@literal <=} i {@literal <=} 127, only one byte is used with the\n   * actual value.\n   * For other values of i, the first byte value indicates whether the\n   * integer is positive or negative, and the number of bytes that follow.\n   * If the first byte value v is between -113 and -116, the following integer\n   * is positive, with number of bytes that follow are -(v+112).\n   * If the first byte value v is between -121 and -124, the following integer\n   * is negative, with number of bytes that follow are -(v+120). Bytes are\n   * stored in the high-non-zero-byte-first order.\n   *\n   * @param stream Binary output stream\n   * @param i Integer to be serialized\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.Client$Connection:closeConnection()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initMode()" : null,
  "org.apache.hadoop.fs.HardLink$LinkStats:report()" : null,
  "org.apache.hadoop.util.StringUtils:byteDesc(long)" : "* a byte description of the given long interger value.\n   *\n   * @param len len.\n   * @return a byte description of the given long interger value.",
  "org.apache.hadoop.util.Sets:newHashSetWithExpectedSize(int)" : "* Returns a new hash set using the smallest initial table size that can hold\n   * {@code expectedSize} elements without resizing. Note that this is not what\n   * {@link HashSet#HashSet(int)} does, but it is what most users want and\n   * expect it to do.\n   *\n   * <p>This behavior can't be broadly guaranteed, but has been tested with\n   * OpenJDK 1.7 and 1.8.</p>\n   *\n   * @param expectedSize the number of elements you expect to add to the\n   *     returned set\n   * @param <E> Generics Type E.\n   * @return a new, empty hash set with enough capacity to hold\n   *     {@code expectedSize} elements without resizing\n   * @throws IllegalArgumentException if {@code expectedSize} is negative",
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping$PartialGroupNameException:toString()" : null,
  "org.apache.hadoop.ipc.Server:isLogSlowRPC()" : "* Checks if LogSlowRPC is set true.\n   * @return true, if LogSlowRPC is set true, false, otherwise.",
  "org.apache.hadoop.conf.StorageUnit$7:toEBs(double)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isDirectory()" : null,
  "org.apache.hadoop.service.launcher.IrqHandler:getName()" : "* @return the signal name.",
  "org.apache.hadoop.util.GSetByHashMap:get(java.lang.Object)" : null,
  "org.apache.hadoop.util.PureJavaCrc32:<init>()" : "Create a new PureJavaCrc32 object.",
  "org.apache.hadoop.util.LightWeightGSet$SetIterator:next()" : null,
  "org.apache.hadoop.io.InputBuffer:getLength()" : "* Returns the length of the input.\n   * @return length of the input.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:advance()" : "* Move the cursor to the next key-value pair. The entry returned by the\n       * previous entry() call will be invalid.\n       * \n       * @return true if the cursor successfully moves. False when cursor is\n       *         already at the end location and cannot be advanced.\n       * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:getString(java.lang.String)" : null,
  "org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:execute()" : null,
  "org.apache.hadoop.conf.StorageUnit$1:getShortName()" : null,
  "org.apache.hadoop.io.DataInputBuffer:reset(byte[],int)" : "* Resets the data that the buffer reads.\n   *\n   * @param input input.\n   * @param length length.",
  "org.apache.hadoop.io.BloomMapFile$Reader:probablyHasKey(org.apache.hadoop.io.WritableComparable)" : "* Checks if this MapFile has the indicated key. The membership test is\n     * performed using a Bloom filter, so the result has always non-zero\n     * probability of false positives.\n     * @param key key to check\n     * @return  false iff key doesn't exist, true if key probably exists.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:addMeanStatisticSample(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.shell.PathData:findLongestDirPrefix(java.lang.String,java.lang.String,boolean)" : null,
  "org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.Server$Call:<init>(int,int,org.apache.hadoop.ipc.RPC$RpcKind,byte[],org.apache.hadoop.tracing.Span,org.apache.hadoop.ipc.CallerContext)" : null,
  "org.apache.hadoop.fs.GlobFilter:accept(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.HttpExceptionUtils:throwException(java.lang.Throwable)" : null,
  "org.apache.hadoop.io.InputBuffer$Buffer:reset(byte[],int,int)" : null,
  "org.apache.hadoop.fs.shell.find.ExpressionFactory:isExpression(java.lang.String)" : "* Determines whether the given expression name represents and actual\n   * expression.\n   *\n   * @param expressionName\n   *          name of the expression\n   * @return true if expressionName represents an expression",
  "org.apache.hadoop.crypto.CryptoInputStream:readFromUnderlyingStream(java.nio.ByteBuffer)" : "Read data from underlying stream.",
  "org.apache.hadoop.net.TableMapping:getConf()" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:run(boolean)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:getReturnProtoType(java.lang.reflect.Method)" : null,
  "org.apache.hadoop.io.MapFile$Reader:reset()" : "* Re-positions the reader before its first key.\n     *\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])" : null,
  "org.apache.hadoop.service.launcher.InterruptEscalator$ServiceForcedShutdown:run()" : "* Shutdown callback: stop the service and set an atomic boolean\n     * if it stopped within the shutdown time.",
  "org.apache.hadoop.io.NullWritable:get()" : "* Returns the single instance of this class.\n   * @return the single instance of this class.",
  "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$Context:getAndIncrDirNumLastAccessed()" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)" : "* Convenience method, so that we don't open a new connection when using this\n   * method from within another method. Otherwise every API invocation incurs\n   * the overhead of opening/closing a TCP connection.",
  "org.apache.hadoop.ipc.ClientId:getMsb(byte[])" : null,
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMinimumSample(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.RPC$Server:getHighestSupportedProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:<init>()" : null,
  "org.apache.hadoop.util.SequentialNumber:nextValue()" : "Increment and then return the next value.",
  "org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)" : "Construct an RPC server.\n     * @param protocolClass class\n     * @param protocolImpl the instance whose methods will be called\n     * @param conf the configuration to use\n     * @param bindAddress the address to bind on to listen for connection\n     * @param port the port to listen for connections on\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ha.ActiveStandbyElector:preventSessionReestablishmentForTests()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.util.LightWeightCache:remove(java.lang.Object)" : null,
  "org.apache.hadoop.fs.XAttrCodec:decodeValue(java.lang.String)" : "* Decode string representation of a value and check whether it's \n   * encoded. If the given string begins with 0x or 0X, it expresses\n   * a hexadecimal number. If the given string begins with 0s or 0S,\n   * base64 encoding is expected. If the given string is enclosed in \n   * double quotes, the inner string is treated as text. Otherwise \n   * the given string is treated as text. \n   * @param value string representation of the value.\n   * @return byte[] the value\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.dynamic.BindingUtils:checkAvailable(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod)" : "* Require a method to be available.\n   * @param method method to probe\n   * @throws UnsupportedOperationException if the method was not found.",
  "org.apache.hadoop.fs.FileContext:msync()" : "* Synchronize client metadata state.\n   *\n   * @throws IOException If an I/O error occurred.\n   * @throws UnsupportedOperationException If file system for <code>f</code> is\n   *                                       not supported.",
  "org.apache.hadoop.fs.viewfs.NflyFSystem:delete(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.fs.FileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)" : "* Removes all default ACL entries from files and directories.\n   *\n   * @param path Path to modify\n   * @throws IOException if an ACL could not be modified\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:minimums()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getFsStatus()" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:isAction()" : null,
  "org.apache.hadoop.fs.QuotaUsage:hashCode()" : null,
  "org.apache.hadoop.io.TwoDArrayWritable:<init>(java.lang.Class,org.apache.hadoop.io.Writable[][])" : null,
  "org.apache.hadoop.ipc.Server:getRemoteUser()" : "Returns the RPC remote user when invoked inside an RPC.  Note this\n   *  may be different than the current user if called within another doAs\n   *  @return connection's UGI or null if not an RPC",
  "org.apache.hadoop.util.JsonSerialization:getName()" : "* Get the simple name of the class type to be marshalled.\n   * @return the name of the class being marshalled",
  "org.apache.hadoop.io.FloatWritable:<init>(float)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CachedName:<init>(java.lang.String,long)" : null,
  "org.apache.hadoop.conf.Configuration$Parser:handleEndElement()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecodeImpl(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String)" : "* Construct an RPC server.\n     * @param protocolClass - the protocol being registered\n     *     can be null for compatibility with old usage (see below for details)\n     * @param protocolImpl the protocol impl that will be called\n     * @param conf the configuration to use\n     * @param bindAddress the address to bind on to listen for connection\n     * @param port the port to listen for connections on\n     * @param numHandlers the number of method handler threads to run\n     * @param verbose whether each call should be logged\n     * @param secretManager input secretManager.\n     * @param queueSizePerHandler input queueSizePerHandler.\n     * @param portRangeConfig input portRangeConfig.\n     * @param numReaders input numReaders.\n     *\n     * @deprecated use Server#Server(Class, Object,\n     *      Configuration, String, int, int, int, int, boolean, SecretManager)\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.viewfs.ConfigUtil:getConfigViewFsPrefix()" : "* Get the config variable prefix for the default mount table\n   * @return the config variable prefix for the default mount table",
  "org.apache.hadoop.fs.permission.FsPermission:read(java.io.DataInput)" : "* Create and initialize a {@link FsPermission} from {@link DataInput}.\n   *\n   * @param in data input.\n   * @throws IOException raised on errors performing I/O.\n   * @return FsPermission.",
  "org.apache.hadoop.security.authorize.AccessControlList:getString(java.util.Collection)" : "* Returns comma-separated concatenated single String of all strings of\n   * the given set\n   *\n   * @param strings set of strings to concatenate",
  "org.apache.hadoop.io.ByteWritable:set(byte)" : "* Set the value of this ByteWritable.\n   * @param value value.",
  "org.apache.hadoop.fs.shell.FsUsage$Dus:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.crypto.key.KeyShell$CreateCommand:validate()" : null,
  "org.apache.hadoop.fs.shell.FsCommand:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.shell.find.Find:isAncestor(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)" : "Returns true if the target is an ancestor of the source.",
  "org.apache.hadoop.fs.statistics.IOStatisticsSupport:stubDurationTrackerFactory()" : "* Return a stub duration tracker factory whose returned trackers\n   * are always no-ops.\n   *\n   * As singletons are returned, this is very low-cost to use.\n   * @return a duration tracker factory.",
  "org.apache.hadoop.fs.shell.PathData:relativize(java.net.URI,java.net.URI,boolean)" : null,
  "org.apache.hadoop.fs.FsShell:init()" : null,
  "org.apache.hadoop.net.SocketIOWithTimeout:isOpen()" : null,
  "org.apache.hadoop.util.bloom.BloomFilter:<init>(int,int,int)" : "* Constructor\n   * @param vectorSize The vector size of <i>this</i> filter.\n   * @param nbHash The number of hash function to consider.\n   * @param hashType type of the hashing function (see\n   * {@link org.apache.hadoop.util.hash.Hash}).",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:set(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)" : null,
  "org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FSProtos$FsPermissionProto)" : null,
  "org.apache.hadoop.fs.GlobalStorageStatistics:iterator()" : "* Get an iterator that we can use to iterate throw all the global storage\n   * statistics objects.\n   *\n   * @return StorageStatistics Iterator.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackFunctionDuration(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.FunctionRaisingIOE)" : "* Given an IOException raising function/lambda expression,\n   * return a new one which wraps the inner and tracks\n   * the duration of the operation, including whether\n   * it passes/fails.\n   * @param factory factory of duration trackers\n   * @param statistic statistic key\n   * @param inputFn input function\n   * @param <A> type of argument to the input function.\n   * @param <B> return type.\n   * @return a new function which tracks duration and failure.",
  "org.apache.hadoop.io.LongWritable$DecreasingComparator:compare(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferData:getActionFuture()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:<init>(int,int)" : null,
  "org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:writeTo(org.apache.hadoop.ipc.ResponseBuffer)" : null,
  "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.security.RefreshUserMappingsProtocol)" : null,
  "org.apache.hadoop.fs.impl.FutureIOSupport:raiseInnerCause(java.util.concurrent.ExecutionException)" : "* From the inner cause of an execution exception, extract the inner cause\n   * if it is an IOE or RTE.\n   * See {@link FutureIO#raiseInnerCause(ExecutionException)}.\n   * @param e exception.\n   * @param <T> type of return value.\n   * @return nothing, ever.\n   * @throws IOException either the inner IOException, or a wrapper around\n   * any non-Runtime-Exception\n   * @throws RuntimeException if that is the inner cause.",
  "org.apache.hadoop.fs.impl.prefetch.BufferData:setDone()" : "* Indicates that this block is no longer of use and can be reclaimed.",
  "org.apache.hadoop.io.ShortWritable:set(short)" : "* Set the value of this ShortWritable.\n   * @param value input value.",
  "org.apache.hadoop.fs.statistics.IOStatisticsContext:getCurrentIOStatisticsContext()" : "* Get the context's IOStatisticsContext.\n   *\n   * @return instance of IOStatisticsContext for the context.",
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],java.nio.ByteBuffer[],int)" : "* A \"bulk\" version of the solveVandermondeSystem, using ByteBuffer.\n   *\n   * @param x input x.\n   * @param y input y.\n   * @param len input len.",
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:writeChunk(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:getRemaining()" : "* Returns the number of bytes remaining in the input buffer; normally\n   * called when finished() is true to determine amount of post-gzip-stream\n   * data.  Note that, other than the finished state with concatenated data\n   * after the end of the current gzip stream, this will never return a\n   * non-zero value unless called after {@link #setInput(byte[] b, int off,\n   * int len)} and before {@link #decompress(byte[] b, int off, int len)}.\n   * (That is, after {@link #decompress(byte[] b, int off, int len)} it\n   * always returns zero, except in finished state with concatenated data.)\n   *\n   * @return the total (non-negative) number of unprocessed bytes in input",
  "org.apache.hadoop.fs.FsStatus:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getFS()" : null,
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:readFully(long,byte[])" : null,
  "org.apache.hadoop.fs.shell.MoveCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:getPassword()" : "* @return returns password.",
  "org.apache.hadoop.fs.sftp.SFTPConnectionPool:getConnPoolSize()" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:addMaximumSample(java.lang.String,long)" : null,
  "org.apache.hadoop.ipc.Client:stop()" : "Stop all threads related to this client.  No further calls may be made\n   * using this client.",
  "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:counter(org.apache.hadoop.metrics2.MetricsInfo,int)" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping:getName2IdCmdMac(java.lang.String,boolean)" : null,
  "org.apache.hadoop.conf.Configuration:getStringCollection(java.lang.String)" : "* Get the comma delimited values of the <code>name</code> property as \n   * a collection of <code>String</code>s.  \n   * If no such property is specified then empty collection is returned.\n   * <p>\n   * This is an optimized version of {@link #getStrings(String)}\n   * \n   * @param name property name.\n   * @return property value as a collection of <code>String</code>s.",
  "org.apache.hadoop.util.SysInfoWindows:getStorageBytesRead()" : null,
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:cachePut(int,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:getBytesRead()" : "* Returns the total number of compressed bytes input so far, including\n   * gzip header/trailer bytes.\n   *\n   * @return the total (non-negative) number of compressed bytes read so far",
  "org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:prependFileAuthority(java.lang.String)" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsLogging:sortedMap(java.util.Map,java.util.function.Predicate)" : "* Create a sorted (tree) map from an unsorted map.\n   * This incurs the cost of creating a map and that\n   * of inserting every object into the tree.\n   * @param source source map\n   * @param <E> value type\n   * @return a treemap with all the entries.",
  "org.apache.hadoop.util.StringUtils:getTrimmedStringCollectionSplitByEquals(java.lang.String)" : "* Splits an \"=\" separated value <code>String</code>, trimming leading and\n   * trailing whitespace on each value after splitting by comma and new line separator.\n   *\n   * @param str a comma separated <code>String</code> with values, may be null\n   * @return a <code>Map</code> of <code>String</code> keys and values, empty\n   * Collection if null String input.",
  "org.apache.hadoop.util.ExitUtil:terminate(int,java.lang.Throwable)" : "* Like {@link #terminate(int, String)} but uses the given throwable to\n   * build the message to display or throw as an\n   * {@link ExitException}.\n   * <p>\n   * @param status exit code to use if the exception is not an ExitException.\n   * @param t throwable which triggered the termination. If this exception\n   * is an {@link ExitException} its status overrides that passed in.\n   * @throws ExitException if {@link System#exit(int)}  is disabled.",
  "org.apache.hadoop.io.BytesWritable:toString()" : "* Generate the stream of bytes as hex pairs separated by ' '.",
  "org.apache.hadoop.security.UserGroupInformation:checkTGTAndReloginFromKeytab()" : "* Re-login a user from keytab if TGT is expired or is close to expiry.\n   * \n   * @throws IOException raised on errors performing I/O.\n   * @throws KerberosAuthException if it's a kerberos login exception.",
  "org.apache.hadoop.security.LdapGroupsMapping:extractPassword(java.lang.String)" : null,
  "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:getSchemeName()" : null,
  "org.apache.hadoop.fs.store.DataBlocks:<init>()" : null,
  "org.apache.hadoop.conf.StorageUnit$5:toGBs(double)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:close()" : null,
  "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:invoke()" : null,
  "org.apache.hadoop.ipc.Server$Connection:buildSaslNegotiateResponse()" : "* Process the Sasl's Negotiate request, including the optimization of \n     * accelerating token negotiation.\n     * @return the response to Negotiate request - the list of enabled \n     *         authMethods and challenge if the TOKENS are supported. \n     * @throws SaslException - if attempt to generate challenge fails.\n     * @throws IOException - if it fails to create the SASL server for Tokens",
  "org.apache.hadoop.io.SetFile$Reader:get(org.apache.hadoop.io.WritableComparable)" : "* Read the matching key from a set into <code>key</code>.\n     *\n     * @param key input key.\n     * @return Returns <code>key</code>, or null if no match exists.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.shell.CopyCommands$Merge:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.http.HttpRequestLog:<init>()" : null,
  "org.apache.hadoop.net.unix.DomainSocket$DomainChannel:isOpen()" : null,
  "org.apache.hadoop.io.MD5Hash:<init>()" : "Constructs an MD5Hash.",
  "org.apache.hadoop.fs.viewfs.ViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.fs.FSOutputSummer:write(int)" : "Write one byte",
  "org.apache.hadoop.fs.FileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Returns a local file that the user can write output to.  The caller\n   * provides both the eventual target name in this FileSystem\n   * and the local working file path.\n   * If this FileSystem is local, we write directly into the target.  If\n   * the FileSystem is not local, we write into the tmp local area.\n   * @param fsOutputFile path of output file\n   * @param tmpLocalFile path of local tmp file\n   * @throws IOException IO failure\n   * @return the path.",
  "org.apache.hadoop.ipc.Server$ConnectionManager:toArray()" : null,
  "org.apache.hadoop.fs.shell.Head:dumpToOffset(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read()" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem$1:close()" : "* Close the underlying output stream.",
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.fs.permission.AclEntry:getPermission()" : "* Returns the set of permissions in the ACL entry.\n   *\n   * @return FsAction set of permissions in the ACL entry",
  "org.apache.hadoop.fs.GlobPattern:compile(java.lang.String)" : "* Compile glob pattern string\n   * @param globPattern the glob pattern\n   * @return the pattern object",
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:readObject(java.io.ObjectInputStream)" : "* Deserialize by loading each TreeMap, and building concurrent\n   * hash maps from them.\n   *\n   * @param s ObjectInputStream.\n   * @throws IOException raised on errors performing I/O.\n   * @throws ClassNotFoundException class not found exception",
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:replication(short)" : "* Set replication factor.\n   *\n   * @param replica replica.\n   * @return Generics Type B.",
  "org.apache.hadoop.metrics2.source.JvmMetrics:shutdownSingleton()" : "* Shutdown the JvmMetrics singleton. This is not necessary if the JVM itself\n   * is shutdown, but may be necessary for scenarios where JvmMetrics instance\n   * needs to be re-created while the JVM is still around. One such scenario\n   * is unit-testing.",
  "org.apache.hadoop.fs.RawLocalFileSystem:pathToFile(org.apache.hadoop.fs.Path)" : "* Convert a path to a File.\n   *\n   * @param path the path.\n   * @return file.",
  "org.apache.hadoop.io.BloomMapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)" : null,
  "org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:poll(long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.fs.InvalidPathException:<init>(java.lang.String,java.lang.String)" : "* Constructs exception with the specified detail message.\n   * \n   * @param path invalid path.\n   * @param reason Reason <code>path</code> is invalid",
  "org.apache.hadoop.security.LdapGroupsMapping$BindUserInfo:equals(java.lang.Object)" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.Class,java.lang.String,java.lang.Class[])" : "* Checks for a method implementation.\n     * @param targetClass the class to check for an implementation\n     * @param methodName name of a method (different from constructor)\n     * @param argClasses argument classes for the method\n     * @return this Builder for method chaining",
  "org.apache.hadoop.security.Credentials:writeTokenStorageToStream(java.io.DataOutputStream,org.apache.hadoop.security.Credentials$SerializedFormat)" : null,
  "org.apache.hadoop.io.nativeio.NativeIOException:<init>(java.lang.String,int)" : null,
  "org.apache.hadoop.net.SocksSocketFactory:hashCode()" : null,
  "org.apache.hadoop.http.HttpServer2:isAlive()" : "* Test for the availability of the web server\n   * @return true if the web server is started, false otherwise",
  "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:addRow(java.lang.Object[])" : "* Add a row of objects to the table\n     * @param objects the values",
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:setReadLock(java.util.concurrent.locks.Lock)" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:dispatchedTimeMean()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.ipc.ProtocolSignature:getVersion()" : null,
  "org.apache.hadoop.security.alias.UserProvider:getCredentialEntry(java.lang.String)" : null,
  "org.apache.hadoop.ipc.RpcException:<init>(java.lang.String,java.lang.Throwable)" : "* Constructs exception with the specified detail message and cause.\n   * \n   * @param message message.\n   * @param cause that cause this exception\n   * @param cause the cause (can be retried by the {@link #getCause()} method).\n   *          (A <tt>null</tt> value is permitted, and indicates that the cause\n   *          is nonexistent or unknown.)",
  "org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl:<init>(java.lang.Class,java.lang.Object)" : null,
  "org.apache.hadoop.security.authorize.Service:<init>(java.lang.String,java.lang.Class)" : null,
  "org.apache.hadoop.util.ReflectionUtils:getTaskName(long,java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner()" : "* Get a scanner than can scan the whole TFile.\n     * \n     * @return The scanner object. A valid Scanner is always returned even if\n     *         the TFile is empty.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ha.SshFenceByTcpPort:getSshConnectTimeout()" : null,
  "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolServerSideTranslatorPB:refreshSuperUserGroupsConfiguration(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshSuperUserGroupsConfigurationRequestProto)" : null,
  "org.apache.hadoop.ipc.CallQueueManager$CallQueueOverflowException:getCause()" : null,
  "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:equals(java.lang.Object)" : null,
  "org.apache.hadoop.util.StringUtils:getTrimmedStringsSplitByEquals(java.lang.String)" : "* Splits \"=\" separated value <code>String</code>, trimming\n   * leading and trailing whitespace on each value.\n   *\n   * @param str an \"=\" separated <code>String</code> with values,\n   *            may be null\n   * @return an array of <code>String</code> values, empty array if null String\n   *         input",
  "org.apache.hadoop.util.DiskChecker:diskIoCheckWithoutNativeIo(java.io.File)" : "* Try to perform some disk IO by writing to the given file\n   * without using Native IO.\n   *\n   * @param file\n   * @throws IOException if there was a non-retriable error.",
  "org.apache.hadoop.util.IntrusiveCollection:containsAll(java.util.Collection)" : null,
  "org.apache.hadoop.net.TableMapping:<init>()" : null,
  "org.apache.hadoop.ipc.FairCallQueue:put(org.apache.hadoop.ipc.Schedulable)" : null,
  "org.apache.hadoop.ipc.Server:getClientBackoffEnable(java.lang.String,int,org.apache.hadoop.conf.Configuration)" : "* Return boolean value configured by property 'ipc.<port>.backoff.enable'\n   * if it is present. If the config is not present, default config\n   * (without port) is used to derive class i.e 'ipc.backoff.enable',\n   * and derived value is returned if configured. Otherwise, default value\n   * {@link CommonConfigurationKeys#IPC_BACKOFF_ENABLE_DEFAULT} is returned.\n   *\n   * @param namespace Namespace \"ipc\".\n   * @param port Server's listener port.\n   * @param conf Configuration properties.\n   * @return Value returned based on configuration.",
  "org.apache.hadoop.ipc.Server$Connection:setupBadVersionResponse(int)" : "* Try to set up the response to indicate that the client version\n     * is incompatible with the server. This can contain special-case\n     * code to speak enough of past IPC protocols to pass back\n     * an exception to the caller.\n     * @param clientVersion the version the caller is using \n     * @throws IOException",
  "org.apache.hadoop.crypto.key.kms.ValueQueue:getSize(java.lang.String)" : "* Get size of the Queue for keyName. This is only used in unit tests.\n   * @param keyName the key name\n   * @return int queue size. Zero means the queue is empty or the key does not exist.",
  "org.apache.hadoop.fs.FileUtil:setOwner(java.io.File,java.lang.String,java.lang.String)" : "* Set the ownership on a file / directory. User name and group name\n   * cannot both be null.\n   * @param file the file to change\n   * @param username the new user owner name\n   * @param groupname the new group owner name\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.FileContext:resolveAbstractFileSystems(org.apache.hadoop.fs.Path)" : "* Returns the list of AbstractFileSystems accessed in the path. The list may\n   * contain more than one AbstractFileSystems objects in case of symlinks.\n   * \n   * @param f\n   *          Path which needs to be resolved\n   * @return List of AbstractFileSystems accessed in the path\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.ipc.RPC$Server:getProtocolImplMap(org.apache.hadoop.ipc.RPC$RpcKind)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getWorkingDirectory()" : null,
  "org.apache.hadoop.io.UTF8:utf8Length(java.lang.String)" : "Returns the number of bytes required to write this.",
  "org.apache.hadoop.fs.impl.WeakReferenceThreadMap:removeForCurrentThread()" : "* Remove the reference for the current thread.\n   * @return any reference value which existed.",
  "org.apache.hadoop.security.LdapGroupsMapping$BindUserInfo:hashCode()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferPool:distance(org.apache.hadoop.fs.impl.prefetch.BufferData,int)" : null,
  "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence)" : "* Write a line of text to a file. Characters are encoded into bytes using\n   * UTF-8. This utility method opens the file for writing, creating the file if\n   * it does not exist, or overwrites an existing file.\n   *\n   * @param fileContext the files system with which to create the file\n   * @param path the path to the file\n   * @param charseq the char sequence to write to the file\n   *\n   * @return the file context\n   *\n   * @throws NullPointerException if any of the arguments are {@code null}\n   * @throws IOException if an I/O error occurs creating or writing to the file",
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getProviders()" : null,
  "org.apache.hadoop.http.HttpServer2:addMultiException(org.eclipse.jetty.util.MultiException,java.lang.Exception)" : null,
  "org.apache.hadoop.fs.impl.FutureIOSupport:awaitFuture(java.util.concurrent.Future,long,java.util.concurrent.TimeUnit)" : "* Given a future, evaluate it. Raised exceptions are\n   * extracted and handled.\n   * See {@link FutureIO#awaitFuture(Future, long, TimeUnit)}.\n   * @param future future to evaluate\n   * @param <T> type of the result.\n   * @param timeout timeout.\n   * @param unit unit.\n   * @return the result, if all went well.\n   * @throws InterruptedIOException future was interrupted\n   * @throws IOException if something went wrong\n   * @throws RuntimeException any nested RTE thrown\n   * @throws TimeoutException the future timed out.",
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:keySet()" : null,
  "org.apache.hadoop.fs.RawPathHandle:equals(java.lang.Object)" : null,
  "org.apache.hadoop.ipc.Client$Connection:interruptConnectingThread()" : null,
  "org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroupsInternal(java.lang.String)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],byte[][],int[],int,int)" : "* A \"bulk\" version to the solving of Vandermonde System.\n   *\n   * @param x input x.\n   * @param y input y.\n   * @param outputOffsets input outputOffsets.\n   * @param len input len.\n   * @param dataLen input dataLen.",
  "org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.io.SequenceFile$CompressionType)" : "* Create a set naming the element comparator and compression type.\n     *\n     * @param conf input Configuration.\n     * @param fs input FileSystem.\n     * @param dirName input dirName.\n     * @param comparator input comparator.\n     * @param compress input compress.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:build()" : null,
  "org.apache.hadoop.net.NetUtils:normalizeIP2HostName(java.lang.String)" : "* Attempt to normalize the given string to \"host:port\"\n   * if it like \"ip:port\".\n   *\n   * @param ipPort maybe lik ip:port or host:port.\n   * @return host:port",
  "org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path)" : "* List the statuses of the files/directories in the given path \n     * if the path is a directory.\n     * \n     * @param f is the path\n     *\n     * @return an array that contains statuses of the files/directories \n     *         in the given path\n     *\n     * @throws AccessControlException If access is denied\n     * @throws FileNotFoundException If <code>f</code> does not exist\n     * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n     *           not supported\n     * @throws IOException If an I/O error occurred\n     * \n     * Exceptions applicable to file systems accessed over RPC:\n     * @throws RpcClientException If an exception occurred in the RPC client\n     * @throws RpcServerException If an exception occurred in the RPC server\n     * @throws UnexpectedServerException If server implementation throws \n     *           undeclared exception to RPC server",
  "org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String[],java.lang.String)" : null,
  "org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:writeChunk(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newInverseQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)" : "* Create a mutable inverse metric that estimates inverse quantiles of a stream of values\n   * @param name of the metric\n   * @param desc metric description\n   * @param sampleName of the metric (e.g., \"Ops\")\n   * @param valueName of the metric (e.g., \"Rate\")\n   * @param interval rollover interval of estimator in seconds\n   * @return a new inverse quantile estimator object\n   * @throws MetricsException if interval is not a positive integer",
  "org.apache.hadoop.fs.FileSystem$Statistics$6:<init>(org.apache.hadoop.fs.FileSystem$Statistics)" : "* Copy constructor.\n     *\n     * @param other    The input Statistics object which is cloned.",
  "org.apache.hadoop.io.compress.BZip2Codec:setConf(org.apache.hadoop.conf.Configuration)" : "* Set the configuration to be used by this object.\n   *\n   * @param conf the configuration object.",
  "org.apache.hadoop.crypto.key.KeyShell$RollCommand:validate()" : null,
  "org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.ha.ZKFCProtocol)" : null,
  "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:<init>()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:performEncodeImpl(java.nio.ByteBuffer[],int[],int,java.nio.ByteBuffer[],int[])" : null,
  "org.apache.hadoop.io.compress.BlockCompressorStream:rawWriteInt(int)" : null,
  "org.apache.hadoop.io.OutputBuffer$Buffer:write(java.io.InputStream,int)" : null,
  "org.apache.hadoop.util.JvmPauseMonitor$GcTimes:subtract(org.apache.hadoop.util.JvmPauseMonitor$GcTimes)" : null,
  "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:available()" : null,
  "org.apache.hadoop.net.SocketInputStream:read()" : null,
  "org.apache.hadoop.io.compress.bzip2.CRC:initialiseCRC()" : null,
  "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:init(javax.servlet.FilterConfig)" : null,
  "org.apache.hadoop.util.CrcUtil:composeWithMonomial(int,int,int,int)" : "* composeWithMonomial.\n   *\n   * @param crcA crcA.\n   * @param crcB crcB.\n   * @param monomial Precomputed x^(lengthBInBytes * 8) mod {@code mod}\n   * @param mod mod.\n   * @return compose with monomial.",
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:reinit(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.ChecksumFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Rename files/dirs.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:logExpireTokens(java.util.Collection)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:verifyRenameStrategy(java.net.URI,java.net.URI,boolean,org.apache.hadoop.fs.viewfs.ViewFileSystem$RenameStrategy)" : null,
  "org.apache.hadoop.ipc.RetryCache$CacheEntry:<init>(byte[],int,long,boolean)" : null,
  "org.apache.hadoop.ipc.RpcClientUtil:toTraceName(java.lang.String)" : "* Convert an RPC class method to a string.\n   * The format we want is\n   * 'SecondOutermostClassShortName#OutermostClassShortName'.\n   *\n   * For example, if the full class name is:\n   *   org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations\n   *\n   * the format we want is:\n   *   ClientProtocol#getBlockLocations\n   * @param fullName input fullName.\n   * @return toTraceName.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation:getHttpMethod()" : null,
  "org.apache.hadoop.fs.FileContext:processDeleteOnExit()" : "* Delete all the paths that were marked as delete-on-exit.",
  "org.apache.hadoop.fs.FSDataInputStream:getWrappedStream()" : "* Get a reference to the wrapped input stream. Used by unit tests.\n   *\n   * @return the underlying input stream",
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:getRecommendedBufferSize()" : null,
  "org.apache.hadoop.fs.shell.find.Find:isExpression(java.lang.String)" : "Asks the factory whether an expression is recognized.",
  "org.apache.hadoop.fs.impl.FunctionsRaisingIOE:<init>()" : null,
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:<init>(java.util.function.Function)" : "* Construct with the copy function being that supplied in.\n   * @param copyFn copy function.",
  "org.apache.hadoop.fs.FileContext:removeAcl(org.apache.hadoop.fs.Path)" : "* Removes all but the base ACL entries of files and directories.  The entries\n   * for user, group, and others are retained for compatibility with permission\n   * bits.\n   *\n   * @param path Path to modify\n   * @throws IOException if an ACL could not be removed",
  "org.apache.hadoop.util.bloom.HashFunction:clear()" : "Clears <i>this</i> hash function. A NOOP",
  "org.apache.hadoop.fs.shell.CommandWithDestination:setPreserve(boolean)" : "* If true, the last modified time, last access time,\n   * owner, group and permission information of the source\n   * file will be preserved as far as target {@link FileSystem}\n   * implementation allows.\n   *\n   * @param preserve preserve.",
  "org.apache.hadoop.util.LightWeightResizableGSet:resize(int)" : "* Resize the internal table to given capacity.\n   *\n   * @param cap capacity.",
  "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:process(java.nio.ByteBuffer,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.net.AbstractDNSToSwitchMapping:isSingleSwitch()" : "* Predicate that indicates that the switch mapping is known to be\n   * single-switch. The base class returns false: it assumes all mappings are\n   * multi-rack. Subclasses may override this with methods that are more aware\n   * of their topologies.\n   *\n   * <p>\n   *\n   * This method is used when parts of Hadoop need know whether to apply\n   * single rack vs multi-rack policies, such as during block placement.\n   * Such algorithms behave differently if they are on multi-switch systems.\n   * </p>\n   *\n   * @return true if the mapping thinks that it is on a single switch",
  "org.apache.hadoop.io.nativeio.NativeIO:isAvailable()" : "* @return Return true if the JNI-based native IO extensions are available.",
  "org.apache.hadoop.util.ApplicationClassLoader:constructUrlsFromClasspath(java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:copyBytesToLocal(int)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:clearParentZNode()" : "* Clear all of the state held within the parent ZNode.\n   * This recursively deletes everything within the znode as well as the\n   * parent znode itself. It should only be used when it's certain that\n   * no electors are currently participating in the election.\n   *\n   * @throws IOException raised on errors performing I/O.\n   * @throws InterruptedException interrupted exception.",
  "org.apache.hadoop.fs.LocatedFileStatus:setBlockLocations(org.apache.hadoop.fs.BlockLocation[])" : "* Hook for subclasses to lazily set block locations. The {@link #locations}\n   * field should be null before this is called.\n   * @param locations Block locations for this instance.",
  "org.apache.hadoop.crypto.CryptoOutputStream:setDropBehind(java.lang.Boolean)" : null,
  "org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[])" : "* A test method to print out the current user's UGI.\n   * @param args if there are two arguments, read the user from the keytab\n   * and print it out.\n   * @throws Exception Exception.",
  "org.apache.hadoop.security.SaslInputStream:<init>(java.io.InputStream,javax.security.sasl.SaslClient)" : "* Constructs a SASLInputStream from an InputStream and a SaslClient <br>\n   * Note: if the specified InputStream or SaslClient is null, a\n   * NullPointerException may be thrown later when they are used.\n   * \n   * @param inStream\n   *          the InputStream to be processed\n   * @param saslClient\n   *          an initialized SaslClient object",
  "org.apache.hadoop.fs.HarFileSystem:canonicalizeUri(java.net.URI)" : null,
  "org.apache.hadoop.fs.FileContext$FileContextFinalizer:run()" : null,
  "org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)" : "* Constructor for file systems on which symbolic links are not supported\n   *\n   * @param length length.\n   * @param isdir isdir.\n   * @param block_replication block replication.\n   * @param blocksize block size.\n   * @param modification_time modification time.\n   * @param access_time access_time.\n   * @param permission permission.\n   * @param owner owner.\n   * @param group group.\n   * @param path the path.",
  "org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char)" : "* Unescape <code>charToEscape</code> in the string \n   * with the escape char <code>escapeChar</code>\n   * \n   * @param str string\n   * @param escapeChar escape char\n   * @param charToEscape the escaped char\n   * @return an unescaped string",
  "org.apache.hadoop.io.compress.BlockDecompressorStream:decompress(byte[],int,int)" : null,
  "org.apache.hadoop.fs.BufferedFSInputStream:hasCapability(java.lang.String)" : "* If the inner stream supports {@link StreamCapabilities},\n   * forward the probe to it.\n   * Otherwise: return false.\n   *\n   * @param capability string to query the stream support for.\n   * @return true if a capability is known to be supported.",
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:setTmax(int)" : "* @param tmax the tmax to set",
  "org.apache.hadoop.metrics2.MetricsTag:toString()" : null,
  "org.apache.hadoop.fs.shell.find.FilterExpression:toString()" : null,
  "org.apache.hadoop.util.JsonSerialization:fromJson(java.lang.String)" : "* Convert from JSON.\n   *\n   * @param json input\n   * @return the parsed JSON\n   * @throws IOException IO problems\n   * @throws JsonParseException If the input is not well-formatted\n   * @throws JsonMappingException failure to map from the JSON to this class",
  "org.apache.hadoop.fs.ftp.FTPInputStream:reset()" : null,
  "org.apache.hadoop.ha.ZKFailoverController:getParentZnode()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:getTrackingId()" : "* @return returns tracking id.",
  "org.apache.hadoop.http.HttpServer2:addContext(org.eclipse.jetty.servlet.ServletContextHandler,boolean)" : null,
  "org.apache.hadoop.conf.StorageUnit$3:toGBs(double)" : null,
  "org.apache.hadoop.ipc.Server$Call:getFederatedNamespaceState()" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])" : null,
  "org.apache.hadoop.crypto.CipherSuite:getConfigSuffix()" : "* Returns suffix of cipher suite configuration.\n   * @return String configuration suffix",
  "org.apache.hadoop.io.SequenceFile$Sorter:writeFile(org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator,org.apache.hadoop.io.SequenceFile$Writer)" : "* Writes records from RawKeyValueIterator into a file represented by the \n     * passed writer.\n     * @param records the RawKeyValueIterator\n     * @param writer the Writer created earlier \n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.http.CrossOriginFilter:init(javax.servlet.FilterConfig)" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:main(java.lang.String[])" : "* This is the JVM entry point for the service launcher.\n   *\n   * Converts the arguments to a list, then invokes {@link #serviceMain(List)}\n   * @param args command line arguments.",
  "org.apache.hadoop.util.MachineList:<init>(java.util.Collection)" : "*\n   * @param hostEntries collection of separated ip/cidr/host addresses",
  "org.apache.hadoop.util.ComparableVersion$StringItem:comparableQualifier(java.lang.String)" : "* Returns a comparable value for a qualifier.\n         *\n         * This method takes into account the ordering of known qualifiers then unknown qualifiers with lexical ordering.\n         *\n         * just returning an Integer with the index here is faster, but requires a lot of if/then/else to check for -1\n         * or QUALIFIERS.size and then resort to lexical ordering. Most comparisons are decided by the first character,\n         * so this is still fast. If more characters are needed then it requires a lexical sort anyway.\n         *\n         * @param qualifier\n         * @return an equivalent value that can be used with lexical comparison",
  "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:lessThan(java.lang.Object,java.lang.Object)" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination:shouldPreserve(org.apache.hadoop.fs.shell.CommandWithDestination$FileAttribute)" : "* Checks if the input attribute should be preserved or not\n   *\n   * @param attribute - Attribute to check\n   * @return boolean true if attribute should be preserved, false otherwise",
  "org.apache.hadoop.fs.AbstractFileSystem:hashCode()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.ipc.Server$Call:<init>(int,int,java.lang.Void,java.lang.Void,org.apache.hadoop.ipc.RPC$RpcKind,byte[])" : null,
  "org.apache.hadoop.io.MapFile$Writer$ComparatorOption:<init>(org.apache.hadoop.io.WritableComparator)" : null,
  "org.apache.hadoop.io.SortedMapWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[])" : "* Move the cursor to the first entry whose key is greater than or equal\n       * to the input key. Synonymous to lowerBound(key, 0, key.length). The\n       * entry returned by the previous entry() call will be invalid.\n       * \n       * @param key\n       *          The input key\n       * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.shell.find.Name:prepare()" : null,
  "org.apache.hadoop.fs.FileSystem:checkAccessPermissions(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction)" : "* This method provides the default implementation of\n   * {@link #access(Path, FsAction)}.\n   *\n   * @param stat FileStatus to check\n   * @param mode type of access to check\n   * @throws AccessControlException if access is denied\n   * @throws IOException for any error",
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkForErrors(java.lang.String)" : "* If the sink isn't set to ignore errors, throw a {@link MetricsException}\n   * if the stream encountered an exception.  The message parameter will be used\n   * as the new exception's message with the current file name\n   * ({@link #currentFilePath}) appended to it.\n   *\n   * @param message the exception message. The message will have a colon and\n   * the current file name ({@link #currentFilePath}) appended to it.\n   * @throws MetricsException thrown if there was an error and the sink isn't\n   * ignoring errors",
  "org.apache.hadoop.fs.ChecksumFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.bloom.CountingBloomFilter:xor(org.apache.hadoop.util.bloom.Filter)" : null,
  "org.apache.hadoop.tracing.TraceUtils:wrapHadoopConf(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize()" : null,
  "org.apache.hadoop.io.TwoDArrayWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.shell.Command:displayError(java.lang.Exception)" : "* Display an exception prefaced with the command name.  Also increments\n   * the error count for the command which will result in a non-zero exit\n   * code.\n   * @param e exception to display",
  "org.apache.hadoop.io.ArrayPrimitiveWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.metrics2.MetricsSystem$AbstractCallback:postStart()" : null,
  "org.apache.hadoop.util.ProgramDriver:run(java.lang.String[])" : "* This is a driver for the example programs.\n   * It looks at the first command line argument and tries to find an\n   * example program with that name.\n   * If it is found, it calls the main method in that class with the rest \n   * of the command line arguments.\n   * @param args The argument from the user. args[0] is the command to run.\n   * @return -1 on error, 0 on success\n   * @throws NoSuchMethodException  when a particular method cannot be found.\n   * @throws SecurityException security manager to indicate a security violation.\n   * @throws IllegalAccessException for backward compatibility.\n   * @throws IllegalArgumentException if the arg is invalid.\n   * @throws Throwable Anything thrown by the example program's main",
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:getWorkingDirectory()" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:addRawCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder)" : null,
  "org.apache.hadoop.jmx.JMXJsonServletNaNFiltered:extraWrite(java.lang.Object,java.lang.String,com.fasterxml.jackson.core.JsonGenerator)" : null,
  "org.apache.hadoop.crypto.random.OsSecureRandom:next(int)" : null,
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:getPos()" : null,
  "org.apache.hadoop.util.StringUtils:getStrings(java.lang.String)" : "* Returns an arraylist of strings.\n   * @param str the comma separated string values\n   * @return the arraylist of the comma separated string values",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextMarker(long,int)" : "* This method tries to find the marker (passed to it as the first parameter)\n   * in the stream. It can find bit patterns of length &lt;= 63 bits.\n   * Specifically this method is used in CBZip2InputStream to find the end of\n   * block (EOB) delimiter in the stream, starting from the current position\n   * of the stream. If marker is found, the stream position will be at the\n   * byte containing the starting bit of the marker.\n   * @param marker The bit pattern to be found in the stream\n   * @param markerBitLength No of bits in the marker\n   * @return true if the marker was found otherwise false\n   * @throws IOException raised on errors performing I/O.\n   * @throws IllegalArgumentException if marketBitLength is greater than 63",
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(java.lang.String,boolean)" : null,
  "org.apache.hadoop.security.KDiag:printEnv(java.lang.String)" : "* Print an environment variable's name and value; printing\n   * {@link #UNSET} if it is not set.\n   * @param variable environment variable",
  "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:allowVerboseDump()" : "* Allow dump verbose debug info or not.\n   * @return true if verbose debug info is desired, false otherwise",
  "org.apache.hadoop.io.retry.RetryInvocationHandler:getProxyProvider()" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:build(java.lang.Object)" : "* Returns the first valid implementation as a BoundMethod or throws a\n     * RuntimeError if there is none.\n     * @param receiver an Object to receive the method invocation\n     * @return a {@link BoundMethod} with a valid implementation and receiver\n     * @throws IllegalStateException if the method is static\n     * @throws IllegalArgumentException if the receiver's class is incompatible\n     * @throws RuntimeException if no implementation was found",
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_snapshot()" : "* Take a snapshot of the context IOStatistics.\n   * {@link IOStatisticsContext#snapshot()}\n   * @return an instance of {@link IOStatisticsSnapshot}.",
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:getEncryptionKeyName()" : "* @return Name of the encryption key used to encrypt the encrypted key.",
  "org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot:getSuppressedCount()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:initBlock()" : null,
  "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:toString()" : null,
  "org.apache.hadoop.fs.shell.CommandFormat:parse(java.util.List)" : "Parse parameters from the given list of args.  The list is\n   *  destructively modified to remove the options.\n   * \n   * @param args as a list of input arguments",
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:counters()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint:getTargetFileSystemURIs()" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyCompressor:getBytesRead()" : "* Return number of bytes given to this compressor since last reset.",
  "org.apache.hadoop.fs.shell.TouchCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.io.compress.ZStandardCodec:setConf(org.apache.hadoop.conf.Configuration)" : "* Set the configuration to be used by this object.\n   *\n   * @param conf the configuration object.",
  "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)" : "* Copy a file/directory tree within/between filesystems.\n   * <p>\n   * returns true if the operation succeeded. When deleteSource is true,\n   * this means \"after the copy, delete(source) returned true\"\n   * If the destination is a directory, and mkdirs (dest) fails,\n   * the operation will return false rather than raise any exception.\n   * </p>\n   * The overwrite flag is about overwriting files; it has no effect about\n   * handing an attempt to copy a file atop a directory (expect an IOException),\n   * or a directory over a path which contains a file (mkdir will fail, so\n   * \"false\").\n   * <p>\n   * The operation is recursive, and the deleteSource operation takes place\n   * as each subdirectory is copied. Therefore, if an operation fails partway\n   * through, the source tree may be partially deleted.\n   * </p>\n   * @param srcFS source filesystem\n   * @param srcStatus status of source\n   * @param dstFS destination filesystem\n   * @param dst path of source\n   * @param deleteSource delete the source?\n   * @param overwrite overwrite files at destination?\n   * @param conf configuration to use when opening files\n   * @return true if the operation succeeded.\n   * @throws IOException failure",
  "org.apache.hadoop.ipc.Server:getRpcDetailedMetrics()" : null,
  "org.apache.hadoop.ipc.CallQueueManager:put(org.apache.hadoop.ipc.Schedulable)" : "* Insert e into the backing queue or block until we can.  If client\n   * backoff is enabled this method behaves like add which throws if\n   * the queue overflows.\n   * If we block and the queue changes on us, we will insert while the\n   * queue is drained.",
  "org.apache.hadoop.fs.FileSystem:getDefaultUri(org.apache.hadoop.conf.Configuration)" : "* Get the default FileSystem URI from a configuration.\n   * @param conf the configuration to use\n   * @return the uri of the default filesystem",
  "org.apache.hadoop.security.token.delegation.DelegationKey:getKey()" : null,
  "org.apache.hadoop.security.alias.CredentialShell:getCommandUsage()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:msync()" : "* Synchronize client metadata state.\n   * <p>\n   * In some FileSystem implementations such as HDFS metadata\n   * synchronization is essential to guarantee consistency of read requests\n   * particularly in HA setting.\n   * @throws IOException raised on errors performing I/O.\n   * @throws UnsupportedOperationException Unsupported Operation Exception.",
  "org.apache.hadoop.fs.impl.prefetch.BufferData:setReady(org.apache.hadoop.fs.impl.prefetch.BufferData$State[])" : "* Marks the completion of reading data into the buffer.\n   * The buffer cannot be modified once in this state.\n   *\n   * @param expectedCurrentState the collection of states from which transition to READY is allowed.",
  "org.apache.hadoop.io.VersionMismatchException:<init>(byte,byte)" : null,
  "org.apache.hadoop.metrics2.impl.MetricGaugeDouble:visit(org.apache.hadoop.metrics2.MetricsVisitor)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:flush()" : null,
  "org.apache.hadoop.metrics2.lib.MutableQuantiles:getQuantiles()" : "* Returns the array of Quantiles declared in MutableQuantiles.\n   *\n   * @return array of Quantiles",
  "org.apache.hadoop.fs.FileStatus:getPermission()" : "* Get FsPermission associated with the file.\n   * @return permission. If a filesystem does not have a notion of permissions\n   *         or if permissions could not be determined, then default \n   *         permissions equivalent of \"rwxrwxrwx\" is returned.",
  "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String,java.lang.String,java.lang.String,boolean,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)" : "* Constructor for the helper class to configure the ZooKeeper client connection.\n     *\n     * @param zkPrincipal        Optional.\n     * @param kerberosPrincipal  Optional. Use along with kerberosKeytab.\n     * @param kerberosKeytab     Optional. Use along with kerberosPrincipal.\n     * @param sslEnabled         Flag to enable SSL/TLS ZK client connection for each component\n     *                           independently.\n     * @param truststoreKeystore TruststoreKeystore object containing the keystoreLocation,\n     *                           keystorePassword, truststoreLocation, truststorePassword for\n     *                           SSL/TLS connection when sslEnabled is set to true.",
  "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class)" : "* Initialize the registry with all the methods in a protocol\n   * so they all show up in the first snapshot.\n   * Convenient for JMX implementations.\n   * @param protocol the protocol class",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.compress.BlockCompressorStream:compress()" : null,
  "org.apache.hadoop.fs.shell.find.Print:<init>(java.lang.String)" : "* Construct a Print {@link Expression} with the specified suffix.",
  "org.apache.hadoop.fs.shell.FsUsage$Df:getUsagesTable()" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:flush()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:toString()" : null,
  "org.apache.hadoop.util.ReflectionUtils:setJobConf(java.lang.Object,org.apache.hadoop.conf.Configuration)" : "* This code is to support backward compatibility and break the compile  \n   * time dependency of core on mapred.\n   * This should be made deprecated along with the mapred package HADOOP-1230. \n   * Should be removed when mapred package is removed.",
  "org.apache.hadoop.io.file.tfile.Compression:<init>()" : "* Prevent the instantiation of class.",
  "org.apache.hadoop.fs.FileSystem:getHomeDirectory()" : "Return the current user's home directory in this FileSystem.\n   * The default implementation returns {@code \"/user/$USER/\"}.\n   * @return the path.",
  "org.apache.hadoop.io.UTF8:writeChars(java.io.DataOutput,java.lang.String,int,int)" : null,
  "org.apache.hadoop.ipc.Server:setupResponseForWritable(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)" : null,
  "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getChecksumOpt()" : null,
  "org.apache.hadoop.fs.FilterFs:getServerDefaults(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ha.HAServiceTarget:getProxyForAddress(org.apache.hadoop.conf.Configuration,int,int,java.net.InetSocketAddress)" : null,
  "org.apache.hadoop.http.HttpServer2:<init>(org.apache.hadoop.http.HttpServer2$Builder)" : null,
  "org.apache.hadoop.util.Options$PathOption:<init>(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.net.unix.DomainSocketWatcher:toString()" : null,
  "org.apache.hadoop.security.SaslPropertiesResolver:getInstance(org.apache.hadoop.conf.Configuration)" : "* Returns an instance of SaslPropertiesResolver.\n   * Looks up the configuration to see if there is custom class specified.\n   * Constructs the instance by passing the configuration directly to the\n   * constructor to achieve thread safety using final fields.\n   * @param conf configuration.\n   * @return SaslPropertiesResolver",
  "org.apache.hadoop.fs.LocalFileSystemPathHandle:hashCode()" : null,
  "org.apache.hadoop.util.QuickSort:sort(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.util.Tool,java.lang.String[])" : "* Runs the <code>Tool</code> with its <code>Configuration</code>.\n   * \n   * Equivalent to <code>run(tool.getConf(), tool, args)</code>.\n   * \n   * @param tool <code>Tool</code> to run.\n   * @param args command-line arguments to the tool.\n   * @return exit code of the {@link Tool#run(String[])} method.\n   * @throws Exception exception.",
  "org.apache.hadoop.fs.permission.AclStatus$Builder:owner(java.lang.String)" : "* Sets the file owner.\n     *\n     * @param owner String file owner\n     * @return Builder this builder, for call chaining",
  "org.apache.hadoop.security.UserGroupInformation:addTokenIdentifier(org.apache.hadoop.security.token.TokenIdentifier)" : "* Add a TokenIdentifier to this UGI. The TokenIdentifier has typically been\n   * authenticated by the RPC layer as belonging to the user represented by this\n   * UGI.\n   * \n   * @param tokenId\n   *          tokenIdentifier to be added\n   * @return true on successful add of new tokenIdentifier",
  "org.apache.hadoop.fs.shell.Display$AvroFileInputStream:<init>(org.apache.hadoop.fs.FileStatus)" : null,
  "org.apache.hadoop.io.wrappedio.WrappedIO:pathCapabilities_hasPathCapability(java.lang.Object,org.apache.hadoop.fs.Path,java.lang.String)" : "* Does a path have a given capability?\n   * Calls {@link PathCapabilities#hasPathCapability(Path, String)},\n   * mapping IOExceptions to false.\n   * @param fs filesystem\n   * @param path path to query the capability of.\n   * @param capability non-null, non-empty string to query the path for support.\n   * @return true if the capability is supported under that part of the FS.\n   * resolving paths or relaying the call.\n   * @throws IllegalArgumentException invalid arguments",
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readChunk(long,byte[],int,int,byte[])" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:read(long,java.nio.ByteBuffer)" : "* Positioned read using {@link ByteBuffer}s. This method is thread-safe.",
  "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:flush()" : null,
  "org.apache.hadoop.fs.FileUtil:runCommandOnStream(java.io.InputStream,java.lang.String)" : "* Run a command and send the contents of an input stream to it.\n   * @param inputStream Input stream to forward to the shell command\n   * @param command shell command to run\n   * @throws IOException read or write failed\n   * @throws InterruptedException command interrupted\n   * @throws ExecutionException task submit failed",
  "org.apache.hadoop.io.ArrayPrimitiveWritable:getPrimitiveClass(java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:bindToOpenSSLProvider()" : "* Bind to the OpenSSL provider via wildfly.\n   * This MUST be the only place where wildfly classes are referenced,\n   * so ensuring that any linkage problems only surface here where they may\n   * be caught by the initialization code.",
  "org.apache.hadoop.conf.ReconfigurableBase:reconfigureProperty(java.lang.String,java.lang.String)" : "* {@inheritDoc}\n   *\n   * This method makes the change to this objects {@link Configuration}\n   * and calls reconfigurePropertyImpl to update internal data structures.\n   * This method cannot be overridden, subclasses should instead override\n   * reconfigurePropertyImpl.",
  "org.apache.hadoop.util.dynamic.DynMethods$MakeAccessible:run()" : null,
  "org.apache.hadoop.util.ZKUtil:getPermFromString(java.lang.String)" : "* Parse ACL permission string, partially borrowed from\n   * ZooKeeperMain private method",
  "org.apache.hadoop.net.NodeBase:getLevel()" : "@return this node's level in the tree.\n   * E.g. the root of a tree returns 0 and its children return 1",
  "org.apache.hadoop.io.SequenceFile$CompressedBytes:getSize()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:getDebugInfo()" : null,
  "org.apache.hadoop.fs.permission.AclEntry:parseAclSpec(java.lang.String,boolean)" : "* Parses a string representation of an ACL spec into a list of AclEntry\n   * objects. Example: \"user::rwx,user:foo:rw-,group::r--,other::---\"\n   * The expected format of ACL entries in the string parameter is the same\n   * format produced by the {@link #toStringStable()} method.\n   * \n   * @param aclSpec\n   *          String representation of an ACL spec.\n   * @param includePermission\n   *          for setAcl operations this will be true. i.e. AclSpec should\n   *          include permissions.<br>\n   *          But for removeAcl operation it will be false. i.e. AclSpec should\n   *          not contain permissions.<br>\n   *          Example: \"user:foo,group:bar\"\n   * @return Returns list of {@link AclEntry} parsed",
  "org.apache.hadoop.fs.shell.Delete$Expunge:processArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.net.NetUtils:getLocalHostname()" : "* Return hostname without throwing exception.\n   * The returned hostname String format is \"hostname\".\n   * @return hostname",
  "org.apache.hadoop.fs.FSDataOutputStream:setDropBehind(java.lang.Boolean)" : null,
  "org.apache.hadoop.io.MD5Hash:read(java.io.DataInput)" : "* Constructs, reads and returns an instance.\n   * @param in in.\n   * @throws IOException raised on errors performing I/O.\n   * @return MD5Hash.",
  "org.apache.hadoop.net.NetUtils:verifyHostnames(java.lang.String[])" : "* Performs a sanity check on the list of hostnames/IPs to verify they at least\n   * appear to be valid.\n   * @param names - List of hostnames/IPs\n   * @throws UnknownHostException Unknown Host Exception.",
  "org.apache.hadoop.fs.ChecksumFs:truncate(org.apache.hadoop.fs.Path,long)" : null,
  "org.apache.hadoop.util.ShutdownHookManager$HookEntry:<init>(java.lang.Runnable,int,long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.util.DataChecksum:getValue()" : null,
  "org.apache.hadoop.io.ByteWritable:toString()" : null,
  "org.apache.hadoop.util.MachineList:<init>(java.lang.String)" : "* \n   * @param hostEntries comma separated ip/cidr/host addresses",
  "org.apache.hadoop.security.AuthenticationFilterInitializer:getFilterConfigMap(org.apache.hadoop.conf.Configuration,java.lang.String)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)" : null,
  "org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:run()" : null,
  "org.apache.hadoop.io.serializer.WritableSerialization:getSerializer(java.lang.Class)" : null,
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:preferDirectBuffer()" : null,
  "org.apache.hadoop.fs.audit.CommonAuditContext$GlobalIterable:iterator()" : null,
  "org.apache.hadoop.fs.CompositeCrcFileChecksum:getLength()" : null,
  "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.User:getAuthenticationMethod()" : null,
  "org.apache.hadoop.net.NetUtils:quoteHost(java.lang.String)" : "* Quote a hostname if it is not null\n   * @param hostname the hostname; nullable\n   * @return a quoted hostname or {@link #UNKNOWN_HOST} if the hostname is null",
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getServerDefaults(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:getResponseTimeCountInLastWindow()" : null,
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$1:check(java.lang.String[],java.lang.String[],java.lang.String[])" : "* Checks to see if the supplied hostname matches any of the supplied CNs\n     * or \"DNS\" Subject-Alts.  Most implementations only look at the first CN,\n     * and ignore any additional CNs.  Most implementations do look at all of\n     * the \"DNS\" Subject-Alts. The CNs or Subject-Alts may contain wildcards\n     * according to RFC 2818.\n     *\n     * @param cns         CN fields, in order, as extracted from the X.509\n     *                    certificate.\n     * @param subjectAlts Subject-Alt fields of type 2 (\"DNS\"), as extracted\n     *                    from the X.509 certificate.\n     * @param hosts       The array of hostnames to verify.\n     * @throws SSLException If verification failed.",
  "org.apache.hadoop.conf.Configuration:getClass(java.lang.String,java.lang.Class)" : "* Get the value of the <code>name</code> property as a <code>Class</code>.  \n   * If no such property is specified, then <code>defaultValue</code> is \n   * returned.\n   * \n   * @param name the conf key name.\n   * @param defaultValue default value.\n   * @return property value as a <code>Class</code>, \n   *         or <code>defaultValue</code>.",
  "org.apache.hadoop.util.SysInfoLinux:readDiskBlockInformation(java.lang.String,int)" : "* Read /sys/block/diskName/queue/hw_sector_size file, parse and calculate\n   * sector size for a specific disk.\n   * @return sector size of specified disk, or defSector",
  "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:getConf()" : null,
  "org.apache.hadoop.fs.FileContext:getAbstractFileSystem(org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:write(java.lang.String)" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementGauge(java.lang.String,long)" : null,
  "org.apache.hadoop.metrics2.sink.FileSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)" : null,
  "org.apache.hadoop.security.SecurityUtil:getZKAuthInfos(org.apache.hadoop.conf.Configuration,java.lang.String)" : "* Utility method to fetch ZK auth info from the configuration.\n   *\n   * @param conf configuration.\n   * @param configKey config key.\n   * @throws java.io.IOException if the Zookeeper ACLs configuration file\n   * cannot be read\n   * @throws ZKUtil.BadAuthFormatException if the auth format is invalid\n   * @return ZKAuthInfo List.",
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues2(int,int)" : null,
  "org.apache.hadoop.io.NullWritable$Comparator:<init>()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getChrootedPath(org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[])" : "* convert an array of FileStatus to an array of Path\n   *\n   * @param stats\n   *          an array of FileStatus objects\n   * @return an array of paths corresponding to the input",
  "org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:invoke(java.lang.Object[])" : null,
  "org.apache.hadoop.fs.ContentSummary:getSnapshotDirectoryCount()" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:getHelp()" : null,
  "org.apache.hadoop.fs.Path:checkPathArg(java.lang.String)" : null,
  "org.apache.hadoop.ipc.Server$Connection:checkDataLength(int)" : null,
  "org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:checkClosed()" : null,
  "org.apache.hadoop.fs.FileSystem:listStatusIterator(org.apache.hadoop.fs.Path)" : "* Returns a remote iterator so that followup calls are made on demand\n   * while consuming the entries. Each FileSystem implementation should\n   * override this method and provide a more efficient implementation, if\n   * possible.\n   *\n   * Does not guarantee to return the iterator that traverses statuses\n   * of the files in a sorted order.\n   *\n   * @param p target path\n   * @return remote iterator\n   * @throws FileNotFoundException if <code>p</code> does not exist\n   * @throws IOException if any I/O error occurred",
  "org.apache.hadoop.http.ProfileOutputServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.ipc.ClientId:getLsb(byte[])" : null,
  "org.apache.hadoop.util.functional.RemoteIterators:remoteIteratorFromIterator(java.util.Iterator)" : "* Create a remote iterator from a java.util.Iterator.\n   * @param <T> type\n   * @param iterator iterator.\n   * @return a remote iterator",
  "org.apache.hadoop.fs.Stat:getFileStatus()" : null,
  "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:skip(long)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:compress(byte[],int,int)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:createZooKeeper()" : "* Get a new zookeeper client instance. protected so that test class can\n   * inherit and pass in a mock object for zookeeper\n   *\n   * @return new zookeeper client instance\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.retry.CallReturn:getReturnValue()" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsLogging:<init>()" : null,
  "org.apache.hadoop.fs.impl.prefetch.Retryer:updateStatus()" : "* Returns true if status update interval has been reached.\n   *\n   * @return true if status update interval has been reached.",
  "org.apache.hadoop.net.DNSDomainNameResolver:getAllByDomainName(java.lang.String)" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine$Invoker:close()" : null,
  "org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : null,
  "org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean)" : null,
  "org.apache.hadoop.conf.Configuration$Parser:handleEndProperty()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getLinkTarget(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.commons.net.ftp.FTPFile,org.apache.hadoop.fs.Path)" : "* Convert the file information in FTPFile to a {@link FileStatus} object. *\n   * \n   * @param ftpFile\n   * @param parentPath\n   * @return FileStatus",
  "org.apache.hadoop.io.retry.RetryInvocationHandler:handleException(java.lang.reflect.Method,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,long,java.lang.Exception)" : null,
  "org.apache.hadoop.crypto.key.UserProvider:getMetadata(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:<init>(int)" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:getArguments()" : "* Returns the arguments of this expression\n   *\n   * @return list of argument strings",
  "org.apache.hadoop.util.StringUtils:getTrimmedStringCollection(java.lang.String)" : "* Splits a comma separated value <code>String</code>, trimming leading and\n   * trailing whitespace on each value. Duplicate and empty values are removed.\n   *\n   * @param str a comma separated <code>String</code> with values, may be null\n   * @return a <code>Collection</code> of <code>String</code> values, empty\n   *         Collection if null String input",
  "org.apache.hadoop.fs.HarFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$PureJavaComparer:compareTo(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree:breakIntoPathComponents(java.lang.String)" : "* Breaks file path into component names.\n   * @param path\n   * @return array of names component names",
  "org.apache.hadoop.crypto.key.CachingKeyProvider:getCurrentKey(java.lang.String)" : null,
  "org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map,long)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,long,java.lang.String,java.lang.String,long)" : null,
  "org.apache.hadoop.io.UTF8:highSurrogate(int)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getLen()" : null,
  "org.apache.hadoop.metrics2.util.SampleStat$MinMax:reset(org.apache.hadoop.metrics2.util.SampleStat$MinMax)" : null,
  "org.apache.hadoop.io.Text:decode(byte[],int,int)" : null,
  "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getName(java.lang.reflect.Method)" : "* @return Remove the prefix \"get\", if any, from the method name. Return the\n   * capacitalized method name.\"\n   *\n   * @param method input method.",
  "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:toLimits(java.nio.ByteBuffer[])" : null,
  "org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:<init>(org.apache.hadoop.thirdparty.protobuf.Message)" : null,
  "org.apache.hadoop.security.http.XFrameOptionsFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)" : "* Constructs a mapping of configuration properties to be used for filter\n   * initialization.  The mapping includes all properties that start with the\n   * specified configuration prefix.  Property names in the mapping are trimmed\n   * to remove the configuration prefix.\n   *\n   * @param conf configuration to read\n   * @param confPrefix configuration prefix\n   * @return mapping of configuration properties to be used for filter\n   *     initialization",
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:read(long,byte[],int,int)" : null,
  "org.apache.hadoop.security.SecurityUtil:getByName(java.lang.String)" : "* Resolves a host subject to the security requirements determined by\n   * hadoop.security.token.service.use_ip. Optionally logs slow resolutions.\n   * \n   * @param hostname host or ip to resolve\n   * @return a resolved host\n   * @throws UnknownHostException if the host doesn't exist",
  "org.apache.hadoop.util.Shell:getSetPermissionCommand(java.lang.String,boolean)" : "* Return a command to set permission.\n   *\n   * @param perm permission.\n   * @param recursive recursive.\n   * @return set permission command.",
  "org.apache.hadoop.fs.FileSystem$Cache:getDiscardedInstances()" : "* Get the count of discarded instances.\n     * @return the new instance.",
  "org.apache.hadoop.fs.shell.FsUsage$Df:setHumanReadable(boolean)" : null,
  "org.apache.hadoop.net.CachedDNSToSwitchMapping:reloadCachedMappings()" : null,
  "org.apache.hadoop.io.BytesWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.permission.FsPermission:fromShort(short)" : null,
  "org.apache.hadoop.net.ScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration)" : "* {@inheritDoc}.\n   * <p>\n   * This will get called in the superclass constructor, so a check is needed\n   * to ensure that the raw mapping is defined before trying to relaying a null\n   * configuration.\n   * </p>\n   * @param conf input Configuration.",
  "org.apache.hadoop.util.HostsFileReader$HostDetails:getExcludedMap()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:lookup(java.util.Map,java.lang.String)" : "* Get a reference to the map type providing the\n   * value for a specific key, raising an exception if\n   * there is no entry for that key.\n   * @param <T> type of map/return type.\n   * @param map map to look up\n   * @param key statistic name\n   * @return the value\n   * @throws NullPointerException if there is no entry of that name",
  "org.apache.hadoop.ha.StreamPumper:<init>(org.slf4j.Logger,java.lang.String,java.io.InputStream,org.apache.hadoop.ha.StreamPumper$StreamType)" : null,
  "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$6:suffix()" : null,
  "org.apache.hadoop.ha.ZKFailoverController:setLastHealthState(org.apache.hadoop.ha.HealthMonitor$State)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)" : null,
  "org.apache.hadoop.security.token.DtUtilShell$Append:execute()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.RPC:getProtocolEngine(java.lang.Class,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.shell.Ls:formatSize(long)" : null,
  "org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : "* Which blocks were erased ? For XOR it's simple we only allow and return one\n   * erased block, either data or parity.\n   * @param blockGroup blockGroup.\n   * @return output blocks to recover",
  "org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction:setData(java.lang.String,byte[],int)" : null,
  "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:gauges()" : null,
  "org.apache.hadoop.fs.permission.AclStatus:getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry)" : "* Get the effective permission for the AclEntry\n   * @param entry AclEntry to get the effective action\n   * @return FsAction.",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getMeanStatistic(java.lang.String)" : null,
  "org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.lang.String,int,int)" : "* Constructor with key parameters provided.\n   * @param codecName codec name\n   * @param numDataUnits number of data units used in the schema\n   * @param numParityUnits number os parity units used in the schema",
  "org.apache.hadoop.util.dynamic.DynConstructors$MakeAccessible:run()" : null,
  "org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:next()" : null,
  "org.apache.hadoop.util.InstrumentedLock:lockInterruptibly()" : null,
  "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:toString()" : null,
  "org.apache.hadoop.io.SortedMapWritable:get(java.lang.Object)" : null,
  "org.apache.hadoop.ipc.Client$Connection:handleConnectionFailure(int,java.io.IOException)" : null,
  "org.apache.hadoop.security.SaslInputStream:disposeSasl()" : "* Disposes of any system resources or security-sensitive information Sasl\n   * might be using.\n   * \n   * @exception SaslException\n   *              if a SASL error occurs.",
  "org.apache.hadoop.util.Sets:newTreeSet()" : "* Creates a <i>mutable</i>, empty {@code TreeSet} instance sorted by the\n   * natural sort ordering of its elements.\n   *\n   * <p><b>Note:</b> if mutability is not required, use ImmutableSortedSet#of()\n   * instead.</p>\n   *\n   * @param <E> Generics Type E\n   * @return a new, empty {@code TreeSet}",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)" : "* Create a Meta Block and obtain an output stream for adding data into the\n     * block. There can only be one BlockAppender stream active at any time.\n     * Regular Blocks may not be created after the first Meta Blocks. The caller\n     * must call BlockAppender.close() to conclude the block creation.\n     * \n     * @param name\n     *          The name of the Meta Block. The name must not conflict with\n     *          existing Meta Blocks.\n     * @param compressionName\n     *          The name of the compression algorithm to be used.\n     * @return The BlockAppender stream\n     * @throws IOException\n     * @throws MetaBlockAlreadyExists\n     *           If the meta block with the name already exists.",
  "org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.security.token.Token)" : "* Add a token to this UGI\n   * \n   * @param token Token to be added\n   * @return true on successful add of new token",
  "org.apache.hadoop.util.SysInfoWindows:getVirtualMemorySize()" : "{@inheritDoc}",
  "org.apache.hadoop.log.LogLevel$CLI:run(java.lang.String[])" : null,
  "org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator:<init>(java.util.List)" : null,
  "org.apache.hadoop.io.serializer.DeserializerComparator:<init>(org.apache.hadoop.io.serializer.Deserializer)" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:isTokenWatcherEnabled()" : null,
  "org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry:<init>(java.lang.String,char[])" : null,
  "org.apache.hadoop.fs.PathOperationException:<init>(java.lang.String)" : "@param path for the exception",
  "org.apache.hadoop.io.MapWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.io.IntWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.UnionStorageStatistics:isTracked(java.lang.String)" : "* Return true if a statistic is being tracked.\n   *\n   * @return         True only if the statistic is being tracked.",
  "org.apache.hadoop.util.SignalLogger$Handler:handle(sun.misc.Signal)" : "* Handle an incoming signal.\n     *\n     * @param signal    The incoming signal",
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:isSymlink()" : null,
  "org.apache.hadoop.util.LightWeightResizableGSet:<init>(int)" : null,
  "org.apache.hadoop.net.SocketInputStream:close()" : null,
  "org.apache.hadoop.ipc.Server:getAuxiliaryListenerAddresses()" : "* Return the set of all the configured auxiliary socket addresses NameNode\n   * RPC is listening on. If there are none, or it is not configured at all, an\n   * empty set is returned.\n   * @return the set of all the auxiliary addresses on which the\n   *         RPC server is listening on.",
  "org.apache.hadoop.fs.statistics.DurationStatisticSummary:fetchDurationSummary(org.apache.hadoop.fs.statistics.IOStatistics,java.lang.String,boolean)" : "* Fetch the duration timing summary of success or failure operations\n   * from an IO Statistics source.\n   * If the duration key is unknown, the summary will be incomplete.\n   * @param source source of data\n   * @param key duration statistic key\n   * @param success fetch success statistics, or if false, failure stats.\n   * @return a summary of the statistics.",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:getTotalRequests()" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:containsBlock(int)" : "* Indicates whether the given block is in this cache.",
  "org.apache.hadoop.fs.shell.Delete$Rm:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)" : "* Authorize the superuser which is doing doAs.\n   * {@link #authorize(UserGroupInformation, InetAddress)} should be preferred\n   * to avoid possibly re-resolving the ip address.\n   *\n   * @param user ugi of the effective or proxy user which contains a real user\n   * @param remoteAddress the ip address of client\n   * @throws AuthorizationException Authorization Exception.",
  "org.apache.hadoop.ipc.Server$Call:postponeResponse()" : "* Allow a IPC response to be postponed instead of sent immediately\n     * after the handler returns from the proxy method.  The intended use\n     * case is freeing up the handler thread when the response is known,\n     * but an expensive pre-condition must be satisfied before it's sent\n     * to the client.",
  "org.apache.hadoop.crypto.key.KeyShell$RollCommand:execute()" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$RemoteExceptionDependentRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)" : null,
  "org.apache.hadoop.security.UserGroupInformation:getPrimaryGroupName()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable$Internal:<init>()" : null,
  "org.apache.hadoop.security.SaslPropertiesResolver:getServerProperties(java.net.InetAddress,int)" : "* Identify the Sasl Properties to be used for a connection with a  client.\n   * @param clientAddress  client's address\n   * @param ingressPort the port that the client is connecting\n   * @return the sasl properties to be used for the connection.",
  "org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:write(int)" : null,
  "org.apache.hadoop.util.Sets:cast(java.lang.Iterable)" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination$FileAttribute:getAttribute(char)" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:requestTimeTotal()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:passthroughFn(java.io.Serializable)" : "* A passthrough copy operation suitable for immutable\n   * types, including numbers.\n   *\n   * @param <E> type of values.\n   * @param src source object\n   * @return the source object",
  "org.apache.hadoop.ha.FailoverFailedException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.security.UserGroupInformation$TestingGroups:getGroups(java.lang.String)" : null,
  "org.apache.hadoop.fs.impl.PathCapabilitiesSupport:validatePathCapabilityArgs(org.apache.hadoop.fs.Path,java.lang.String)" : "* Validate the arguments to\n   * {@link PathCapabilities#hasPathCapability(Path, String)}.\n   * @param path path to query the capability of.\n   * @param capability non-null, non-empty string to query the path for support.\n   * @return the string to use in a switch statement.\n   * @throws IllegalArgumentException if a an argument is invalid.",
  "org.apache.hadoop.conf.Configuration:loadResources(java.util.Properties,java.util.ArrayList,int,boolean,boolean)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.io.ArrayWritable:toArray()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:getFileDescriptor()" : null,
  "org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(boolean)" : null,
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : "* Which blocks were erased ?\n   * @param blockGroup blockGroup.\n   * @return output blocks to recover",
  "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getBlockSize()" : null,
  "org.apache.hadoop.io.file.tfile.Utils$Version:<init>(java.io.DataInput)" : "* Construct the Version object by reading from the input stream.\n     * \n     * @param in\n     *          input stream\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)" : "* Filter files/directories in the given path using the user-supplied path\n   * filter.\n   * <p>\n   * Does not guarantee to return the List of files/directories status in a\n   * sorted order.\n   *\n   * @param f\n   *          a path name\n   * @param filter\n   *          the user-supplied path filter\n   * @return an array of FileStatus objects for the files under the given path\n   *         after applying the filter\n   * @throws FileNotFoundException when the path does not exist\n   * @throws IOException see specific implementation",
  "org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult:isLastInternalDirLink()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMinimumReference(java.lang.String)" : "* Get a reference to the atomic instance providing the\n   * value for a specific minimum. This is useful if\n   * the value is passed around.\n   * @param key statistic name\n   * @return the reference\n   * @throws NullPointerException if there is no entry of that name",
  "org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])" : "* Create a <code>GenericOptionsParser</code> to parse given options as well \n   * as generic Hadoop options. \n   * \n   * The resulting <code>CommandLine</code> object can be obtained by \n   * {@link #getCommandLine()}.\n   * \n   * @param conf the configuration to modify  \n   * @param options options built by the caller \n   * @param args User-specified arguments\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String,java.lang.String[])" : null,
  "org.apache.hadoop.fs.shell.find.ExpressionFactory:registerExpression(java.lang.Class)" : "* Invokes \"static void registerExpression(FindExpressionFactory)\" on the\n   * given class. This method abstracts the contract between the factory and the\n   * expression class. Do not assume that directly invoking registerExpression\n   * on the given class will have the same effect.\n   *\n   * @param expressionClass\n   *          class to allow an opportunity to register",
  "org.apache.hadoop.io.BooleanWritable$Comparator:compare(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.ipc.ProtocolSignature:getFingerprint(java.lang.reflect.Method[])" : "* Get the hash code of an array of methods\n   * Methods are sorted before hashcode is calculated.\n   * So the returned value is irrelevant of the method order in the array.\n   * \n   * @param methods an array of methods\n   * @return the hash code",
  "org.apache.hadoop.fs.DelegateToFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.security.token.Token:setID(byte[])" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressor(org.apache.hadoop.conf.Configuration)" : "* Return the appropriate implementation of the zlib compressor. \n   * \n   * @param conf configuration\n   * @return the appropriate implementation of the zlib compressor.",
  "org.apache.hadoop.fs.FileSystem:getLinkTarget(org.apache.hadoop.fs.Path)" : "* See {@link FileContext#getLinkTarget(Path)}.\n   * @param f the path.\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   * @throws IOException IO failure.\n   * @return the path.",
  "org.apache.hadoop.conf.ReconfigurationServlet:init()" : null,
  "org.apache.hadoop.io.DataInputByteBuffer:getData()" : null,
  "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:isDone()" : null,
  "org.apache.hadoop.io.ObjectWritable:<init>(java.lang.Class,java.lang.Object)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$WaitableMetricsBuffer:waitTillNotified(long)" : null,
  "org.apache.hadoop.fs.RawPathHandle:readObject(java.io.ObjectInputStream)" : null,
  "org.apache.hadoop.util.JsonSerialization:fromJsonStream(java.io.InputStream)" : "* Read from an input stream.\n   * @param stream stream to read from\n   * @return the parsed entity\n   * @throws IOException IO problems\n   * @throws JsonParseException If the input is not well-formatted\n   * @throws JsonMappingException failure to map from the JSON to this class",
  "org.apache.hadoop.http.HttpServer2:addAsyncProfilerServlet(org.eclipse.jetty.server.handler.ContextHandlerCollection,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)" : null,
  "org.apache.hadoop.util.Preconditions:checkState(boolean,java.util.function.Supplier)" : "* Preconditions that the expression involving one or more parameters to the calling method.\n   *\n   * <p>The message of the exception is {@code msgSupplier.get()}.</p>\n   *\n   * @param expression a boolean expression\n   * @param msgSupplier  the {@link Supplier#get()} set the\n   *                 exception message if valid. Otherwise,\n   *                 the message is {@link #CHECK_STATE_EX_MESSAGE}\n   * @throws IllegalStateException if {@code expression} is false",
  "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:description()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.io.erasurecode.ECChunk:getBuffer()" : "* Convert to ByteBuffer\n   * @return ByteBuffer",
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.Object,java.lang.String[])" : "* Construct a mergeLink or nfly.",
  "org.apache.hadoop.io.SequenceFile:setDefaultCompressionType(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$CompressionType)" : "* Set the default compression type for sequence files.\n   * @param job the configuration to modify\n   * @param val the new compression type (none, block, record)",
  "org.apache.hadoop.fs.FilterFs:listStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileSystem:getDelegationToken(java.lang.String)" : "* Get a new delegation token for this FileSystem.\n   * This is an internal method that should have been declared protected\n   * but wasn't historically.\n   * Callers should use {@link #addDelegationTokens(String, Credentials)}\n   *\n   * @param renewer the account name that is allowed to renew the token.\n   * @return a new delegation token or null if the FS does not support tokens.\n   * @throws IOException on any problem obtaining a token",
  "org.apache.hadoop.fs.permission.AclStatus$Builder:stickyBit(boolean)" : "* Sets sticky bit. If this method is not called, then the builder assumes\n     * false.\n     *\n     * @param stickyBit\n     *          boolean sticky bit\n     * @return Builder this builder, for call chaining",
  "org.apache.hadoop.util.ConfTest:<init>()" : null,
  "org.apache.hadoop.security.KDiag:verifyFileIsValid(java.io.File,java.lang.String,java.lang.String)" : "* Verify that a file is valid: it is a file, non-empty and readable.\n   * @param file file\n   * @param category category for exceptions\n   * @param text text message\n   * @return true if the validation held; false if it did not <i>and</i>\n   * {@link #nofail} has disabled raising exceptions.",
  "org.apache.hadoop.net.DNS:resolveLocalHostname()" : "* Determine the local hostname; retrieving it from cache if it is known\n   * If we cannot determine our host name, return \"localhost\"\n   * @return the local hostname or \"localhost\"",
  "org.apache.hadoop.fs.DelegateToFileSystem:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)" : null,
  "org.apache.hadoop.io.Text:readFields(java.io.DataInput,int)" : null,
  "org.apache.hadoop.security.UserGroupInformation:reattachMetrics()" : "* Reattach the class's metrics to a new metric system.",
  "org.apache.hadoop.security.Credentials:mergeAll(org.apache.hadoop.security.Credentials)" : "* Copy all of the credentials from one credential object into another.\n   * Existing secrets and tokens are not overwritten.\n   * @param other the credentials to copy",
  "org.apache.hadoop.ipc.RPC:getProtocolVersion(java.lang.Class)" : "* Get the protocol version from protocol class.\n   * If the protocol class has a ProtocolAnnotation,\n   * then get the protocol version from the annotation;\n   * otherwise get it from the versionID field of the protocol class.\n   *\n   * @param protocol input protocol.\n   * @return ProtocolVersion.",
  "org.apache.hadoop.crypto.key.KeyShell$CreateCommand:execute()" : null,
  "org.apache.hadoop.metrics2.MetricsSystem$AbstractCallback:postStop()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:preferDirectBuffer()" : "* Tell if direct buffer is preferred or not. It's for callers to\n   * decide how to allocate coding chunk buffers, using DirectByteBuffer or\n   * bytes array. It will return false by default.\n   * @return true if native buffer is preferred for performance consideration,\n   * otherwise false.",
  "org.apache.hadoop.security.Groups:getGroupInternal(java.lang.String)" : "* Get the group memberships of a given user.\n   * If the user's group is not cached, this method may block.\n   * @param user User's name\n   * @return the group memberships of the user as Set\n   * @throws IOException if user does not exist",
  "org.apache.hadoop.net.NetUtils:getConnectAddress(org.apache.hadoop.ipc.Server)" : "* Returns InetSocketAddress that a client can use to \n   * connect to the server. Server.getListenerAddress() is not correct when\n   * the server binds to \"0.0.0.0\". This returns \"hostname:port\" of the server,\n   * or \"127.0.0.1:port\" when the getListenerAddress() returns \"0.0.0.0:port\".\n   * \n   * @param server server.\n   * @return socket address that a client can use to connect to the server.",
  "org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.conf.Configuration)" : "* Create a FileContext with specified FS as default using the specified\n   * config.\n   * \n   * @param defFS default fs.\n   * @param aConf configutration.\n   * @return new FileContext with specified FS as default.",
  "org.apache.hadoop.util.ProtoUtil:readRawVarint32(java.io.DataInput)" : "* Read a variable length integer in the same format that ProtoBufs encodes.\n   * @param in the input stream to read from\n   * @return the integer\n   * @throws IOException if it is malformed or EOF.",
  "org.apache.hadoop.fs.shell.CommandFormat$IllegalNumberOfArgumentsException:getMessage()" : null,
  "org.apache.hadoop.crypto.key.KeyShell$ListCommand:execute()" : null,
  "org.apache.hadoop.security.alias.CredentialShell$CreateCommand:validate()" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long,long,int)" : null,
  "org.apache.hadoop.util.SysInfoWindows:refreshIfNeeded()" : null,
  "org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:checkCreateRSRawEncoder()" : null,
  "org.apache.hadoop.security.UserGroupInformation:createLoginUser(javax.security.auth.Subject)" : null,
  "org.apache.hadoop.ipc.FairCallQueue:add(org.apache.hadoop.ipc.Schedulable)" : null,
  "org.apache.hadoop.security.UserGroupInformation:shouldRelogin()" : null,
  "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:cacheGroupsRefresh()" : null,
  "org.apache.hadoop.io.DataOutputBuffer:write(java.io.DataInput,int)" : "* Writes bytes from a DataInput directly into the buffer.\n   * @param in data input.\n   * @param length length.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongCounter(java.lang.String,java.util.concurrent.atomic.AtomicLong)" : "* Add a counter statistic to dynamically return the\n   * latest value of the source.\n   * @param key key of this statistic\n   * @param source atomic long counter\n   * @return the builder.",
  "org.apache.hadoop.security.KDiag:close()" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination:recursePath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.util.functional.LazyAutoCloseableReference:isClosed()" : "* Is the reference closed?\n   * @return true if the reference is closed.",
  "org.apache.hadoop.metrics2.lib.MutableGaugeInt:toString()" : "* @return  the value of the metric",
  "org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:checkCreateRSRawDecoder()" : null,
  "org.apache.hadoop.fs.FileSystem$Cache:closeAll()" : "* Close all FileSystems in the cache, whether they are marked for\n     * automatic closing or not.\n     * @throws IOException a problem arose closing one or more FileSystem.",
  "org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail:equals(java.lang.Object)" : null,
  "org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration,java.lang.String)" : null,
  "org.apache.hadoop.conf.Configuration:getRaw(java.lang.String)" : "* Get the value of the <code>name</code> property, without doing\n   * <a href=\"#VariableExpansion\">variable expansion</a>.If the key is \n   * deprecated, it returns the value of the first key which replaces \n   * the deprecated key and is not null.\n   * \n   * @param name the property name.\n   * @return the value of the <code>name</code> property or \n   *         its replacing property and null if no such property exists.",
  "org.apache.hadoop.util.ComparableVersion:hashCode()" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getWriteOps()" : null,
  "org.apache.hadoop.fs.audit.CommonAuditContext:setGlobalContextEntry(java.lang.String,java.lang.String)" : "* Set a global entry.\n   * @param key key\n   * @param value value",
  "org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:removeInstance(java.lang.String)" : null,
  "org.apache.hadoop.security.authorize.AccessControlList:<init>(java.lang.String)" : "* Construct a new ACL from a String representation of the same.\n   * \n   * The String is a a comma separated list of users and groups.\n   * The user list comes first and is separated by a space followed \n   * by the group list. For e.g. \"user1,user2 group1,group2\"\n   * \n   * @param aclString String representation of the ACL",
  "org.apache.hadoop.fs.impl.prefetch.BlockData:getSize(int)" : "* Gets the size of the given block.\n   * @param blockNumber the id of the desired block.\n   * @return the size of the given block.",
  "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getAclKey(java.lang.String)" : null,
  "org.apache.hadoop.ha.ZKFailoverController:checkEligibleForFailover()" : "* If the local node is an observer or is unhealthy it\n   * is not eligible for graceful failover.\n   * @throws ServiceFailedException if the node is an observer or unhealthy",
  "org.apache.hadoop.conf.Configuration:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:getStatus(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Get FsStatus for all ViewFsMountPoints matching path for the given\n   * ViewFileSystem.\n   *\n   * Say ViewFileSystem has following mount points configured\n   *  (1) hdfs://NN0_host:port/sales mounted on /dept/sales\n   *  (2) hdfs://NN1_host:port/marketing mounted on /dept/marketing\n   *  (3) hdfs://NN2_host:port/eng_usa mounted on /dept/eng/usa\n   *  (4) hdfs://NN3_host:port/eng_asia mounted on /dept/eng/asia\n   *\n   * For the above config, here is a sample list of paths and their matching\n   * mount points while getting FsStatus\n   *\n   *  Path                  Description                      Matching MountPoint\n   *\n   *  \"/\"                   Root ViewFileSystem lists all    (1), (2), (3), (4)\n   *                         mount points.\n   *\n   *  \"/dept\"               Not a mount point, but a valid   (1), (2), (3), (4)\n   *                         internal dir in the mount tree\n   *                         and resolved down to \"/\" path.\n   *\n   *  \"/dept/sales\"         Matches a mount point            (1)\n   *\n   *  \"/dept/sales/india\"   Path is over a valid mount point (1)\n   *                         and resolved down to\n   *                         \"/dept/sales\"\n   *\n   *  \"/dept/eng\"           Not a mount point, but a valid   (1), (2), (3), (4)\n   *                         internal dir in the mount tree\n   *                         and resolved down to \"/\" path.\n   *\n   *  \"/erp\"                Doesn't match or leads to or\n   *                         over any valid mount points     None\n   *\n   *\n   * @param fileSystem - ViewFileSystem on which mount point exists\n   * @param path - URI for which FsStatus is requested\n   * @return Map of ViewFsMountPoint and FsStatus\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotEmpty(int,java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:getCanonicalServiceName()" : null,
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:throwMetricsException(java.lang.String)" : "* If the sink isn't set to ignore errors, throw a new\n   * {@link MetricsException}.  The message parameter will be used  as the\n   * new exception's message with the current file name\n   * ({@link #currentFilePath}) appended to it.\n   *\n   * @param message the exception message. The message will have a colon and\n   * the current file name ({@link #currentFilePath}) appended to it.",
  "org.apache.hadoop.util.GenericsUtil:toArray(java.lang.Class,java.util.List)" : "* Converts the given <code>List&lt;T&gt;</code> to a an array of \n   * <code>T[]</code>.\n   * @param c the Class object of the items in the list\n   * @param list the list to convert\n   * @param <T> Generics Type T.\n   * @return T Array.",
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState)" : null,
  "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getBytes()" : null,
  "org.apache.hadoop.metrics2.impl.MetricGaugeDouble:value()" : null,
  "org.apache.hadoop.fs.BatchedRemoteIterator$BatchedListEntries:<init>(java.util.List,boolean)" : null,
  "org.apache.hadoop.fs.sftp.SFTPInputStream:available()" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long)" : null,
  "org.apache.hadoop.fs.FsServerDefaults:getTrashInterval()" : null,
  "org.apache.hadoop.fs.shell.Command:postProcessPath(org.apache.hadoop.fs.shell.PathData)" : "* Hook for commands to implement an operation to be applied on each\n   * path for the command after being processed successfully\n   * @param item a {@link PathData} object\n   * @throws IOException if anything goes wrong...",
  "org.apache.hadoop.fs.shell.Ls:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockEntryCount(int)" : null,
  "org.apache.hadoop.security.SecurityUtil:getClientPrincipal(java.lang.Class,org.apache.hadoop.conf.Configuration)" : "* Look up the client principal for a given protocol. It searches all known\n   * SecurityInfo providers.\n   * @param protocol the protocol class to get the information for\n   * @param conf configuration object\n   * @return client principal or null if it has no client principal defined.",
  "org.apache.hadoop.tracing.TraceScope:addKVAnnotation(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)" : "* See {@link FileContext#getFileLinkStatus(Path)}.\n   *\n   * @param f the path.\n   * @throws AccessControlException if access is denied.\n   * @throws FileNotFoundException when the path does not exist.\n   * @throws IOException raised on errors performing I/O.\n   * @throws UnsupportedFileSystemException if there was no known implementation\n   *                                        for the scheme.\n   * @return file status",
  "org.apache.hadoop.util.WeakReferenceMap:getReferenceLostCount()" : "* Get count of references lost as detected\n   * during prune() or get() calls.\n   * @return count of references lost",
  "org.apache.hadoop.io.ObjectWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)" : "* Aggregate the current statistics with the\n   * source reference passed in.\n   *\n   * The operation is synchronized.\n   * @param source source; may be null\n   * @return true if a merge took place.",
  "org.apache.hadoop.fs.viewfs.ConfigUtil:getConfigViewFsPrefix(java.lang.String)" : "* Get the config variable prefix for the specified mount table\n   * @param mountTableName - the name of the mount table\n   * @return the config variable prefix for the specified mount table",
  "org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:fetch()" : "* Fetch: retrieve the next value.\n     * @return true if a new value was found after filtering.\n     * @throws IOException failure in retrieval from source or mapping",
  "org.apache.hadoop.io.AbstractMapWritable:addToMap(java.lang.Class,byte)" : "* Used to add \"predefined\" classes and by Writable to copy \"new\" classes.",
  "org.apache.hadoop.fs.Options$ChecksumOpt:<init>()" : "* Create a uninitialized one",
  "org.apache.hadoop.http.HttpServer2Metrics:create(org.eclipse.jetty.server.handler.StatisticsHandler,int)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:updateFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:getSupportedCipherSuites()" : null,
  "org.apache.hadoop.ipc.RpcWritable$WritableWrapper:<init>(org.apache.hadoop.io.Writable)" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:toString()" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:<init>(java.lang.String,int,org.apache.hadoop.ipc.DecayRpcScheduler)" : null,
  "org.apache.hadoop.net.NetUtils:getPortFromHostPortString(java.lang.String)" : "* Get port as integer from host port string like host:port.\n   *\n   * @param addr host + port string like host:port.\n   * @return an integer value representing the port.\n   * @throws IllegalArgumentException if the input is not in the correct format.",
  "org.apache.hadoop.net.SocketOutputStream$Writer:performIO(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.ipc.ProtobufHelper:protoFromToken(org.apache.hadoop.security.token.Token)" : "* Create a {@code TokenProto} instance\n   * from a hadoop token.\n   * This builds and caches the fields\n   * (identifier, password, kind, service) but not\n   * renewer or any payload.\n   * @param tok token\n   * @return a marshallable protobuf class.",
  "org.apache.hadoop.util.FileBasedIPList:reload()" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:setInternalDirFs(java.lang.Object)" : null,
  "org.apache.hadoop.fs.shell.PathData:toFile()" : "* Get the path to a local file\n   * @return File representing the local path\n   * @throws IllegalArgumentException if this.fs is not the LocalFileSystem",
  "org.apache.hadoop.util.functional.LazyAtomicReference:getConstructor()" : "* Getter for the constructor.\n   * @return the constructor class",
  "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:decr()" : null,
  "org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)" : "* Create a set naming the element class and compression type.\n     *\n     * @param conf input Configuration.\n     * @param fs input FileSystem.\n     * @param dirName input dirName.\n     * @param keyClass input keyClass.\n     * @param compress input compress.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:getAlgorithm()" : null,
  "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:parseTopMetricsTags(java.lang.String)" : "* Parse Custom tags for TopMetrics.\n   *\n   * @param metricName metricName\n   * @return Tags for TopMetrics",
  "org.apache.hadoop.ipc.Client$Connection:getRemoteAddress()" : null,
  "org.apache.hadoop.fs.store.audit.AuditingFunctions:withinAuditSpan(org.apache.hadoop.fs.store.audit.AuditSpan,org.apache.hadoop.util.functional.CallableRaisingIOE)" : "* Given a callable, return a new callable which\n   * activates and deactivates the span around the inner invocation.\n   * @param auditSpan audit span\n   * @param operation operation\n   * @param <T> type of result\n   * @return a new invocation.",
  "org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path)" : "* <p>Return all the files that match filePattern and are not checksum\n     * files. Results are sorted by their names.\n     * \n     * <p>\n     * A filename pattern is composed of <i>regular</i> characters and\n     * <i>special pattern matching</i> characters, which are:\n     *\n     * <dl>\n     *  <dd>\n     *   <dl>\n     *    <dt> <tt> ? </tt>\n     *    <dd> Matches any single character.\n     *\n     *    <dt> <tt> * </tt>\n     *    <dd> Matches zero or more characters.\n     *\n     *    <dt> <tt> [<i>abc</i>] </tt>\n     *    <dd> Matches a single character from character set\n     *     <tt>{<i>a,b,c</i>}</tt>.\n     *\n     *    <dt> <tt> [<i>a</i>-<i>b</i>] </tt>\n     *    <dd> Matches a single character from the character range\n     *     <tt>{<i>a...b</i>}</tt>. Note: character <tt><i>a</i></tt> must be\n     *     lexicographically less than or equal to character <tt><i>b</i></tt>.\n     *\n     *    <dt> <tt> [^<i>a</i>] </tt>\n     *    <dd> Matches a single char that is not from character set or range\n     *     <tt>{<i>a</i>}</tt>.  Note that the <tt>^</tt> character must occur\n     *     immediately to the right of the opening bracket.\n     *\n     *    <dt> <tt> \\<i>c</i> </tt>\n     *    <dd> Removes (escapes) any special meaning of character <i>c</i>.\n     *\n     *    <dt> <tt> {ab,cd} </tt>\n     *    <dd> Matches a string from the string set <tt>{<i>ab, cd</i>} </tt>\n     *\n     *    <dt> <tt> {ab,c{de,fh}} </tt>\n     *    <dd> Matches a string from string set <tt>{<i>ab, cde, cfh</i>}</tt>\n     *\n     *   </dl>\n     *  </dd>\n     * </dl>\n     *\n     * @param pathPattern a glob specifying a path pattern\n     *\n     * @return an array of paths that match the path pattern\n     *\n     * @throws AccessControlException If access is denied\n     * @throws UnsupportedFileSystemException If file system for \n     *         <code>pathPattern</code> is not supported\n     * @throws IOException If an I/O error occurred\n     * \n     * Exceptions applicable to file systems accessed over RPC:\n     * @throws RpcClientException If an exception occurred in the RPC client\n     * @throws RpcServerException If an exception occurred in the RPC server\n     * @throws UnexpectedServerException If server implementation throws \n     *           undeclared exception to RPC server",
  "org.apache.hadoop.fs.statistics.IOStatisticsLogging:mapToString(java.lang.StringBuilder,java.lang.String,java.util.Map,java.lang.String)" : "* Given a map, add its entryset to the string.\n   * The entries are only sorted if the source entryset\n   * iterator is sorted, such as from a TreeMap.\n   * @param sb string buffer to append to\n   * @param type type (for output)\n   * @param map map to evaluate\n   * @param separator separator\n   * @param <E> type of values of the map",
  "org.apache.hadoop.fs.Trash:getCurrentTrashDir(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:reset()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data:<init>(int)" : null,
  "org.apache.hadoop.ipc.Server$Call:deferResponse()" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)" : null,
  "org.apache.hadoop.ipc.Client$IpcStreams:flush()" : null,
  "org.apache.hadoop.util.dynamic.DynMethods:throwIfInstance(java.lang.Throwable,java.lang.Class)" : "* If the given throwable is an instance of E, throw it as an E.\n   * @param t an exception instance\n   * @param excClass an exception class t may be an instance of\n   * @param <E> the type of exception that will be thrown if throwable is an instance\n   * @throws E if t is an instance of E",
  "org.apache.hadoop.util.curator.ZKCuratorManager:getData(java.lang.String,org.apache.zookeeper.data.Stat)" : "* Get the data in a ZNode.\n   * @param path Path of the ZNode.\n   * @param stat stat.\n   * @return The data in the ZNode.\n   * @throws Exception If it cannot contact Zookeeper.",
  "org.apache.hadoop.io.MapFile$Reader:getValueClass()" : "* Returns the class of values in this file.\n     *\n     * @return Value Class.",
  "org.apache.hadoop.ha.ActiveStandbyElector:isStaleClient(java.lang.Object)" : "* The callbacks and watchers pass a reference to the ZK client\n   * which made the original call. We don't want to take action\n   * based on any callbacks from prior clients after we quit\n   * the election.\n   * @param ctx the ZK client passed into the watcher\n   * @return true if it matches the current client",
  "org.apache.hadoop.conf.StorageUnit$4:fromBytes(double)" : null,
  "org.apache.hadoop.fs.DUHelper:getFolderUsage(java.lang.String)" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:close()" : null,
  "org.apache.hadoop.security.http.CrossOriginFilter:destroy()" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:expires()" : null,
  "org.apache.hadoop.io.compress.CompressorStream:write(int)" : null,
  "org.apache.hadoop.fs.FsShell:getCurrentTrashDir(org.apache.hadoop.fs.Path)" : "* Returns the current trash location for the path specified\n   * @param path to be deleted\n   * @return path to the trash\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:removeAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.shell.find.Find$2:<init>()" : "Default constructor for the Find command.",
  "org.apache.hadoop.fs.shell.Command:expandArguments(java.util.LinkedList)" : "*  Expands a list of arguments into {@link PathData} objects.  The default\n   *  behavior is to call {@link #expandArgument(String)} on each element\n   *  which by default globs the argument.  The loop catches IOExceptions,\n   *  increments the error count, and displays the exception.\n   * @param args strings to expand into {@link PathData} objects\n   * @return list of all {@link PathData} objects the arguments\n   * @throws IOException if anything goes wrong...",
  "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:setDictionary(byte[],int,int)" : null,
  "org.apache.hadoop.util.ProtoUtil:makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto,int,int,byte[],org.apache.hadoop.ipc.AlignmentContext)" : null,
  "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,java.lang.String)" : null,
  "org.apache.hadoop.fs.FileUtil:makeSecureShellPath(java.io.File)" : "* Convert a os-native filename to a path that works for the shell\n   * and avoids script injection attacks.\n   * @param file The filename to convert\n   * @return The unix pathname\n   * @throws IOException on windows, there can be problems with the subprocess",
  "org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:registerForDeferredResponse2()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:truncate(org.apache.hadoop.fs.Path,long)" : null,
  "org.apache.hadoop.metrics2.MetricsJsonBuilder:setContext(java.lang.String)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:openFileWithOptions(org.apache.hadoop.fs.PathHandle,org.apache.hadoop.fs.impl.OpenFileParameters)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:registerFailureHandling()" : "* Override point: register this class as the handler for the control-C\n   * and SIGINT interrupts.\n   *\n   * Subclasses can extend this with extra operations, such as\n   * an exception handler:\n   * <pre>\n   *  Thread.setDefaultUncaughtExceptionHandler(\n   *     new YarnUncaughtExceptionHandler());\n   * </pre>",
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.security.SaslRpcClient:getOutputStream(java.io.OutputStream)" : "* Get SASL wrapped OutputStream if SASL QoP requires wrapping,\n   * otherwise return original stream.  Can be called only after\n   * saslConnect() has been called.\n   * \n   * @param out - OutputStream used to make the connection\n   * @return OutputStream that may be using wrapping\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.CallerContext$Builder:build()" : null,
  "org.apache.hadoop.fs.shell.Count:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)" : "* Create the named set for keys of the named class.\n     * @deprecated pass a Configuration too\n     * @param fs input FileSystem.\n     * @param dirName input dirName.\n     * @param keyClass input keyClass.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.crypto.key.KeyShell:main(java.lang.String[])" : "* main() entry point for the KeyShell.  While strictly speaking the\n   * return is void, it will System.exit() with a return code: 0 is for\n   * success and 1 for failure.\n   *\n   * @param args Command line arguments.\n   * @throws Exception raised on errors performing I/O.",
  "org.apache.hadoop.ipc.FairCallQueue:drainTo(java.util.Collection,int)" : "* drainTo defers to each sub-queue. Note that draining from a FairCallQueue\n   * to another FairCallQueue will likely fail, since the incoming calls\n   * may be scheduled differently in the new FairCallQueue. Nonetheless this\n   * method is provided for completeness.",
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getNodePath(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:getValue()" : null,
  "org.apache.hadoop.fs.permission.FsPermission:<init>(java.lang.String)" : "* Construct by given mode, either in octal or symbolic format.\n   * @param mode mode as a string, either in octal or symbolic format\n   * @throws IllegalArgumentException if <code>mode</code> is invalid",
  "org.apache.hadoop.fs.DU$DUShell:startRefresh()" : null,
  "org.apache.hadoop.fs.store.DataBlocks$DataBlock:close()" : null,
  "org.apache.hadoop.fs.Options$HandleOpt$Data:<init>(boolean)" : null,
  "org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:printXAttr(java.lang.String,byte[])" : null,
  "org.apache.hadoop.fs.FSBuilder:mustLong(java.lang.String,long)" : "* Set mandatory long parameter for the Builder.\n   *\n   * @param key key.\n   * @param value value.\n   * @return generic type B.\n   * @see #opt(String, String)",
  "org.apache.hadoop.fs.TrashPolicyDefault:<init>()" : null,
  "org.apache.hadoop.util.KMSUtil:createKeyProviderFromUri(org.apache.hadoop.conf.Configuration,java.net.URI)" : null,
  "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:flush()" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:error(java.lang.Throwable)" : null,
  "org.apache.hadoop.fs.FileSystem$Cache$Key:hashCode()" : null,
  "org.apache.hadoop.fs.Path:toString()" : null,
  "org.apache.hadoop.security.KDiag:<init>(org.apache.hadoop.conf.Configuration,java.io.PrintWriter,java.io.File,java.lang.String,long,boolean)" : null,
  "org.apache.hadoop.fs.BufferedFSInputStream:seek(long)" : null,
  "org.apache.hadoop.ha.FailoverController:preFailoverChecks(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean)" : "* Perform pre-failover checks on the given service we plan to\n   * failover to, eg to prevent failing over to a service (eg due\n   * to it being inaccessible, already active, not healthy, etc).\n   *\n   * An option to ignore toSvc if it claims it is not ready to\n   * become active is provided in case performing a failover will\n   * allow it to become active, eg because it triggers a log roll\n   * so the standby can learn about new blocks and leave safemode.\n   *\n   * @param from currently active service\n   * @param target service to make active\n   * @param forceActive ignore toSvc if it reports that it is not ready\n   * @throws FailoverFailedException if we should avoid failover",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : "* Find the DelegationTokenInformation for the given token id, and verify that\n   * if the token is expired. Note that this method should be called with \n   * acquiring the secret manager's monitor.\n   *\n   * @param identifier identifier.\n   * @throws InvalidToken invalid token exception.\n   * @return DelegationTokenInformation.",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:getRpcSlowCalls()" : "* Returns the number of slow calls.\n   * @return long",
  "org.apache.hadoop.io.Text:append(byte[],int,int)" : "* Append a range of bytes to the end of the given text.\n   *\n   * @param utf8 the data to copy from\n   * @param start the first position to append from utf8\n   * @param len the number of bytes to append",
  "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String)" : "* Create a mutable rate metric\n   * @param name  of the metric\n   * @return a new mutable metric object",
  "org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)" : "* Return an array of FileStatus objects whose path names match pathPattern\n     * and is accepted by the user-supplied path filter. Results are sorted by\n     * their path names.\n     * Return null if pathPattern has no glob and the path does not exist.\n     * Return an empty array if pathPattern has a glob and no path matches it. \n     * \n     * @param pathPattern glob specifying the path pattern\n     * @param filter user-supplied path filter\n     *\n     * @return an array of FileStatus objects\n     *\n     * @throws AccessControlException If access is denied\n     * @throws UnsupportedFileSystemException If file system for \n     *         <code>pathPattern</code> is not supported\n     * @throws IOException If an I/O error occurred\n     * \n     * Exceptions applicable to file systems accessed over RPC:\n     * @throws RpcClientException If an exception occurred in the RPC client\n     * @throws RpcServerException If an exception occurred in the RPC server\n     * @throws UnexpectedServerException If server implementation throws \n     *           undeclared exception to RPC server",
  "org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:<init>()" : "Same as this(0, 0, null)",
  "org.apache.hadoop.security.token.SecretManager:createSecretKey(byte[])" : "* Convert the byte[] to a secret key\n   * @param key the byte[] to create a secret key from\n   * @return the secret key",
  "org.apache.hadoop.fs.PathHandle:toByteArray()" : "* @return Serialized form in bytes.",
  "org.apache.hadoop.fs.FilterFs:setReplication(org.apache.hadoop.fs.Path,short)" : null,
  "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:containsUpperCase(java.lang.Iterable)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)" : null,
  "org.apache.hadoop.fs.FileSystem:cancelDeleteOnExit(org.apache.hadoop.fs.Path)" : "* Cancel the scheduled deletion of the path when the FileSystem is closed.\n   * @param f the path to cancel deletion\n   * @return true if the path was found in the delete-on-exit list.",
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:setDictionary(byte[],int,int)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)" : null,
  "org.apache.hadoop.fs.FileUtil:fullyDeleteOnExit(java.io.File)" : "* Register all files recursively to be deleted on exit.\n   * @param file File/directory to be deleted",
  "org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,java.lang.String,boolean)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.file.tfile.Utils:readVLong(java.io.DataInput)" : null,
  "org.apache.hadoop.metrics2.impl.MetricCounterLong:type()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:listXAttrs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.ArrayFile$Reader:next(org.apache.hadoop.io.Writable)" : "* Read and return the next value in the file.\n     *\n     * @param value value.\n     * @throws IOException raised on errors performing I/O.\n     * @return Writable.",
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.util.ReflectionUtils:cloneWritableInto(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)" : null,
  "org.apache.hadoop.fs.FileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Copy it a file from the remote filesystem to the local one.\n   * @param src path src file in the remote filesystem\n   * @param dst path local destination\n   * @throws IOException IO failure",
  "org.apache.hadoop.metrics2.lib.MutableStat:toString()" : null,
  "org.apache.hadoop.security.KDiag:endln()" : "* Print something at the end of a section",
  "org.apache.hadoop.conf.Configuration:overlay(java.util.Properties,java.util.Properties)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider$EncryptedQueueRefiller:fillQueueForKey(java.lang.String,java.util.Queue,int)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$DataBlock:blockReleased()" : "* A block has been released.",
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:isInternalDir()" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:verifyCanMlock()" : null,
  "org.apache.hadoop.fs.FileAlreadyExistsException:<init>()" : null,
  "org.apache.hadoop.fs.FSDataInputStream:setDropBehind(java.lang.Boolean)" : null,
  "org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException:<init>(int,int)" : null,
  "org.apache.hadoop.net.NetworkTopology$InvalidTopologyException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_pageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Get the maximum number of objects/files to delete in a single request.\n   * @param fileSystem filesystem\n   * @param path path to delete under.\n   * @return a number greater than or equal to zero.\n   * @throws UnsupportedOperationException bulk delete under that path is not supported.\n   * @throws IllegalArgumentException path not valid.\n   * @throws IOException problems resolving paths\n   * @throws RuntimeException invocation failure.",
  "org.apache.hadoop.fs.permission.FsCreateModes:<init>(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:description()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:<init>(java.io.DataInput)" : null,
  "org.apache.hadoop.util.CloseableReferenceCount:unreference()" : "* Decrement the reference count.\n   *\n   * @return          True if the object is closed and has no outstanding\n   *                  references.",
  "org.apache.hadoop.security.token.Token:<init>(byte[],byte[],org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)" : "* Construct a token from the components.\n   * @param identifier the token identifier\n   * @param password the token's password\n   * @param kind the kind of token\n   * @param service the service for this token",
  "org.apache.hadoop.ipc.Server:setupResponseOldVersionFatal(java.io.ByteArrayOutputStream,org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)" : "* Setup response for the IPC Call on Fatal Error from a \n   * client that is using old version of Hadoop.\n   * The response is serialized using the previous protocol's response\n   * layout.\n   * \n   * @param response buffer to serialize the response into\n   * @param call {@link Call} to which we are setting up the response\n   * @param rv return value for the IPC Call, if the call was successful\n   * @param errorClass error class, if the the call failed\n   * @param error error message, if the call failed\n   * @throws IOException",
  "org.apache.hadoop.fs.shell.XAttrCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:getBytesWritten()" : null,
  "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)" : "* Create the named map for keys of the named class.\n     * @deprecated Use Writer(Configuration, Path, Option...) instead.\n     *\n     * @param conf configuration.\n     * @param fs filesystem.\n     * @param dirName dirName.\n     * @param keyClass keyClass.\n     * @param valClass valClass.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:getFailoverCount()" : null,
  "org.apache.hadoop.ipc.metrics.RpcMetrics:shutdown()" : null,
  "org.apache.hadoop.ha.FailoverController:getRpcTimeoutToNewActive(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.DU$DUShell:toString()" : null,
  "org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String,java.lang.String)" : "* See {@link Configuration#get(String, String)}.",
  "org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:getComparator()" : null,
  "org.apache.hadoop.fs.Path:initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:<init>()" : "* Instantiate without setting the statistics.\n   * This is for subclasses which build up the map during their own\n   * construction.",
  "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:addProcessingTime(int,long)" : "* Instrument a Call processing time based on its priority.\n   *\n   * @param priority of the RPC call\n   * @param processingTime of the RPC call in the queue of the priority",
  "org.apache.hadoop.fs.RawLocalFileSystem:createOutputStreamWithMode(org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.conf.ReconfigurableBase:startReconfigurationTask()" : "* Start a reconfiguration task to reload configuration in background.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroSerializer:open(java.io.OutputStream)" : null,
  "org.apache.hadoop.http.HttpServer2:bindForPortRange(org.eclipse.jetty.server.ServerConnector,int)" : "* Bind using port ranges. Keep on looking for a free port in the port range\n   * and throw a bind exception if no port in the configured range binds.\n   * @param listener jetty listener.\n   * @param startPort initial port which is set in the listener.\n   * @throws Exception",
  "org.apache.hadoop.fs.impl.OpenFileParameters:withOptions(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.SysInfoLinux:readProcDisksInfoFile()" : "* Read /proc/diskstats file, parse and calculate amount\n   * of bytes read and written from/to disks.",
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:setRoot(boolean)" : null,
  "org.apache.hadoop.util.ConfigurationHelper:parseEnumSet(java.lang.String,java.lang.String,java.lang.Class,boolean)" : "* Given a comma separated list of enum values,\n   * trim the list, map to enum values in the message (case insensitive)\n   * and return the set.\n   * Special handling of \"*\" meaning: all values.\n   * @param key Configuration object key -used in error messages.\n   * @param valueString value from Configuration\n   * @param enumClass class of enum\n   * @param ignoreUnknown should unknown values be ignored?\n   * @param <E> enum type\n   * @return a mutable set of enum values parsed from the valueString, with any unknown\n   * matches stripped if {@code ignoreUnknown} is true.\n   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,\n   * or there are two entries in the enum which differ only by case.",
  "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:<init>(java.io.DataInput)" : null,
  "org.apache.hadoop.metrics2.util.MetricsCache$Record:getTag(java.lang.String)" : "* Lookup a tag value\n     * @param key name of the tag\n     * @return the tag value",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:configureConnection(java.net.HttpURLConnection)" : null,
  "org.apache.hadoop.metrics2.lib.MutableQuantiles:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)" : "* Instantiates a new {@link MutableQuantiles} for a metric that rolls itself\n   * over on the specified time interval.\n   * \n   * @param name\n   *          of the metric\n   * @param description\n   *          long-form textual description of the metric\n   * @param sampleName\n   *          type of items in the stream (e.g., \"Ops\")\n   * @param valueName\n   *          type of the values\n   * @param interval\n   *          rollover interval (in seconds) of the estimator",
  "org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction)" : "* Construct by the given {@link FsAction}.\n   * @param u user action\n   * @param g group action\n   * @param o other action",
  "org.apache.hadoop.net.NetworkTopology:sortByDistanceUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer)" : "* Sort nodes array by network distance to <i>reader</i>.\n   * <p> using network location. This is used when the reader\n   * is not a datanode. Sorting the nodes based on network distance\n   * from the reader reduces network traffic and improves\n   * performance.\n   * </p>\n   *\n   * @param reader    Node where data will be read\n   * @param nodes     Available replicas with the requested data\n   * @param activeLen Number of active nodes at the front of the array\n   * @param secondarySort a secondary sorting strategy which can inject into\n   *     that point from outside to help sort the same distance.\n   * @param <T> Generics Type T.",
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:<init>()" : null,
  "org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:basePath()" : null,
  "org.apache.hadoop.fs.shell.find.FilterExpression:getPrecedence()" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine:getClient(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.conf.StorageUnit$4:toPBs(double)" : null,
  "org.apache.hadoop.fs.shell.find.FilterExpression:isAction()" : null,
  "org.apache.hadoop.fs.shell.find.Print:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory)" : "Registers this expression with the specified factory.",
  "org.apache.hadoop.io.SequenceFile$CompressedBytes:writeCompressedBytes(java.io.DataOutputStream)" : null,
  "org.apache.hadoop.io.EnumSetWritable:size()" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:getBlockRegion()" : null,
  "org.apache.hadoop.conf.StorageUnit$3:toPBs(double)" : null,
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.shell.Ls:isUseAtime()" : "* Should access time be used rather than modification time.\n   * @return true use access time, false use modification time",
  "org.apache.hadoop.fs.ChecksumFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)" : "* Disable those operations which the checksummed FS blocks.\n   * {@inheritDoc}",
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:performCoding(java.nio.ByteBuffer[],java.nio.ByteBuffer[])" : null,
  "org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials$SerializedFormat)" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:read(byte[])" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getOptionalPathHandle()" : "* Get the optional path handle; may be empty.\n   * @return the optional path handle field.",
  "org.apache.hadoop.ha.HAAdmin:runCmd(java.lang.String[])" : null,
  "org.apache.hadoop.io.SequenceFile$Metadata:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatisticsSnapshot(java.io.Serializable)" : "* Probe for an object being an instance of {@code IOStatisticsSnapshot}.\n   * @param object object to probe\n   * @return true if the object is the right type, false if the classes\n   * were not found or the object is null/of a different type",
  "org.apache.hadoop.security.authorize.AccessControlList:toString()" : "* Returns descriptive way of users and groups that are part of this ACL.\n   * Use {@link #getAclString()} to get the exact String that can be given to\n   * the constructor of AccessControlList to create a new instance.",
  "org.apache.hadoop.metrics2.source.JvmMetrics:registerIfNeeded()" : null,
  "org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,boolean,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.FilterFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.io.compress.CompressionInputStream:<init>(java.io.InputStream)" : "* Create a compression input stream that reads\n   * the decompressed bytes from the given stream.\n   * \n   * @param in The input stream to be compressed.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.FileUtil:isRegularFile(java.io.File,boolean)" : "* Check if the file is regular.\n   * @param file The file being checked.\n   * @param allowLinks Whether to allow matching links.\n   * @return Returns the result of checking whether the file is a regular file.",
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:<init>()" : null,
  "org.apache.hadoop.ipc.RetryCache:newEntry(long,byte[],int)" : null,
  "org.apache.hadoop.fs.statistics.MeanStatistic:copy()" : "* Create a copy of this instance.\n   * @return copy.\n   *",
  "org.apache.hadoop.util.concurrent.AsyncGetFuture:get()" : null,
  "org.apache.hadoop.fs.shell.find.ExpressionFactory:addClass(java.lang.Class,java.lang.String[])" : "* Register the given class as handling the given list of expression names.\n   *\n   * @param expressionClass\n   *          the class implementing the expression names\n   * @param names\n   *          one or more command names that will invoke this class\n   * @throws IOException\n   *           if the expression is not of an expected type",
  "org.apache.hadoop.io.compress.DecompressorStream:skip(long)" : null,
  "org.apache.hadoop.ipc.Client$Connection:sendRpcRequest(org.apache.hadoop.ipc.Client$Call)" : "Initiates a rpc call by sending the rpc request to the remote server.\n     * Note: this is not called from the current thread, but by another\n     * thread, so that if the current thread is interrupted that the socket\n     * state isn't corrupted with a partially written message.\n     * @param call - the rpc request",
  "org.apache.hadoop.security.token.TokenIdentifier:getBytes()" : "* Get the bytes for the token identifier\n   * @return the bytes of the identifier",
  "org.apache.hadoop.net.SocketIOWithTimeout:setTimeout(long)" : null,
  "org.apache.hadoop.util.VersionInfo:_getDate()" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(org.apache.hadoop.crypto.key.JavaKeyStoreProvider)" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarMetaData:parseMetaData()" : null,
  "org.apache.hadoop.fs.FileSystem$DirectoryEntries:<init>(org.apache.hadoop.fs.FileStatus[],byte[],boolean)" : null,
  "org.apache.hadoop.fs.shell.MoveCommands$MoveToLocal:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:executeRunnable(java.lang.Runnable)" : "* @param r runnable to run in future on executor pool\n   * @return future\n   * @throws java.util.concurrent.RejectedExecutionException can be thrown\n   * @throws NullPointerException if r param is null",
  "org.apache.hadoop.crypto.CryptoInputStream:read(java.nio.ByteBuffer)" : "ByteBuffer read.",
  "org.apache.hadoop.http.HttpServer2Metrics:asyncDispatches()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:isViewFileSystemOverloadScheme(org.apache.hadoop.fs.FileSystem)" : "* Check if the FileSystem is a ViewFileSystemOverloadScheme.\n   *\n   * @param fileSystem file system.\n   * @return true if the fileSystem is ViewFileSystemOverloadScheme",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials)" : null,
  "org.apache.hadoop.fs.DU:<init>(java.io.File,long,long,long)" : null,
  "org.apache.hadoop.security.authorize.AccessControlList:<init>(java.lang.String,java.lang.String)" : "* Construct a new ACL from String representation of users and groups\n   * \n   * The arguments are comma separated lists\n   * \n   * @param users comma separated list of users\n   * @param groups comma separated list of groups",
  "org.apache.hadoop.fs.LocalFileSystemPathHandle:verify(org.apache.hadoop.fs.FileStatus)" : null,
  "org.apache.hadoop.crypto.key.kms.ValueQueue:getAtMost(java.lang.String,int)" : "* This removes the \"num\" values currently at the head of the Queue for the\n   * provided key. Will immediately fire the Queue filler function if key\n   * does not exist\n   * How many values are actually returned is governed by the\n   * <code>SyncGenerationPolicy</code> specified by the user.\n   * @param keyName String key name\n   * @param num Minimum number of values to return.\n   * @return {@literal List<E>} values returned\n   * @throws IOException raised on errors performing I/O.\n   * @throws ExecutionException execution exception.",
  "org.apache.hadoop.security.authorize.AccessControlList:readFields(java.io.DataInput)" : "* Deserializes the AccessControlList object",
  "org.apache.hadoop.io.WritableComparator:newKey()" : "* Construct a new {@link WritableComparable} instance.\n   * @return WritableComparable.",
  "org.apache.hadoop.security.UserGroupInformation:getNextTgtRenewalTime(long,long,org.apache.hadoop.io.retry.RetryPolicy)" : "* Get time for next login retry. This will allow the thread to retry with\n   * exponential back-off, until tgt endtime.\n   * Last retry is {@link #kerberosMinSecondsBeforeRelogin} before endtime.\n   *\n   * @param tgtEndTime EndTime of the tgt.\n   * @param now Current time.\n   * @param rp The retry policy.\n   * @return Time for next login retry.",
  "org.apache.hadoop.conf.StorageUnit$2:getSuffixChar()" : null,
  "org.apache.hadoop.conf.ReconfigurationServlet:doPost(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.conf.Configuration$DeprecationContext:getDeprecatedKeyMap()" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:equals(java.lang.Object)" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:isNativeCodeLoaded()" : null,
  "org.apache.hadoop.util.DiskChecker:getFileNameForDiskIoCheck(java.io.File,int)" : "* Generate a path name for a test file under the given directory.\n   *\n   * @return file object.",
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttributes(java.lang.String[])" : null,
  "org.apache.hadoop.security.LdapGroupsMapping:failover(int,int)" : "* Check whether we should fail over to the next LDAP server.\n   * @param attemptsMadeWithSameLdap current number of attempts made\n   *                                 with using same LDAP instance\n   * @param maxAttemptsBeforeFailover maximum number of attempts\n   *                                  before failing over\n   * @return true if we should fail over to the next LDAP server",
  "org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:<init>(java.io.OutputStream,byte[])" : null,
  "org.apache.hadoop.io.IOUtils:wrapException(java.lang.String,java.lang.String,java.io.IOException)" : "* Takes an IOException, file/directory path, and method name and returns an\n   * IOException with the input exception as the cause and also include the\n   * file,method details. The new exception provides the stack trace of the\n   * place where the exception is thrown and some extra diagnostics\n   * information.\n   *\n   * Return instance of same exception if exception class has a public string\n   * constructor; Otherwise return an PathIOException.\n   * InterruptedIOException and PathIOException are returned unwrapped.\n   *\n   * @param path file/directory path\n   * @param methodName method name\n   * @param exception the caught exception.\n   * @return an exception to throw",
  "org.apache.hadoop.io.file.tfile.TFileDumper$Align:format(java.lang.String,int,org.apache.hadoop.io.file.tfile.TFileDumper$Align)" : null,
  "org.apache.hadoop.metrics2.source.JvmMetrics:reattach(org.apache.hadoop.metrics2.MetricsSystem,org.apache.hadoop.metrics2.source.JvmMetrics)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:checkMetricName(java.lang.String)" : null,
  "org.apache.hadoop.net.NetworkTopology:countEmptyRacks()" : null,
  "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:initThreadPoolExecutor()" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:validatePosition(long)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies:isSaslFailure(java.lang.Exception)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree:isRootInternalDir()" : "* @return true if the root represented as internalDir. In LinkMergeSlash,\n   * there will be root to root mapping. So, root does not represent as\n   * internalDir.",
  "org.apache.hadoop.metrics2.lib.MutableGaugeInt:decr(int)" : "* decrement by delta\n   * @param delta of the decrement",
  "org.apache.hadoop.fs.Path:equals(java.lang.Object)" : null,
  "org.apache.hadoop.conf.Configuration$Resource:getResource()" : null,
  "org.apache.hadoop.ipc.Server:setPurgeIntervalNanos(int)" : null,
  "org.apache.hadoop.util.BlockingThreadPoolExecutorService:<init>(int,java.util.concurrent.ThreadPoolExecutor)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:publishMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,boolean)" : "* Publish a metrics snapshot to all the sinks\n   * @param buffer  the metrics snapshot to publish\n   * @param immediate  indicates that we should publish metrics immediately\n   *                   instead of using a separate thread.",
  "org.apache.hadoop.net.NetworkTopology:isOnSameRack(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)" : "Check if two nodes are on the same rack\n   * @param node1 one node (can be null)\n   * @param node2 another node (can be null)\n   * @return true if node1 and node2 are on the same rack; false otherwise\n   * @exception IllegalArgumentException when either node1 or node2 is null, or\n   * node1 or node2 do not belong to the cluster",
  "org.apache.hadoop.fs.Stat:parseExecResult(java.io.BufferedReader)" : null,
  "org.apache.hadoop.fs.FileStatus:setSymlink(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:getGlobalMetrics()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getDurationInfo(java.lang.StringBuilder)" : null,
  "org.apache.hadoop.io.MD5Hash$1:<init>()" : "Constructs an MD5Hash.",
  "org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:<init>(int,java.nio.file.Path,int,long)" : null,
  "org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)" : null,
  "org.apache.hadoop.fs.impl.FlagSet:pathCapabilities()" : "* Generate the list of capabilities.\n   * @return a possibly empty list.",
  "org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization:getReader(java.lang.Class)" : null,
  "org.apache.hadoop.fs.EmptyStorageStatistics:isTracked(java.lang.String)" : null,
  "org.apache.hadoop.util.Shell:<init>(long)" : "* Create an instance with a minimum interval between executions; stderr is\n   * not merged with stdout.\n   * @param interval interval in milliseconds between command executions.",
  "org.apache.hadoop.fs.CachingGetSpaceUsed:getUsed()" : "* @return an estimate of space used in the directory path.",
  "org.apache.hadoop.fs.ChecksumFs:listStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.Server$Call:getHostAddress()" : null,
  "org.apache.hadoop.fs.HarFileSystem:listStatus(org.apache.hadoop.fs.Path)" : "* liststatus returns the children of a directory \n   * after looking up the index files.",
  "org.apache.hadoop.util.bloom.BloomFilter:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsLogging$SourceToString:<init>(org.apache.hadoop.fs.statistics.IOStatisticsSource)" : null,
  "org.apache.hadoop.net.SocketInputStream:read(byte[],int,int)" : null,
  "org.apache.hadoop.fs.FSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int)" : null,
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:append()" : "* Append to an existing file (optional operation).\n   *\n   * @return Generics Type B.",
  "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:getMetaName()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:init()" : null,
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:<init>(java.lang.String,java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$KeyVersion)" : "* Create a new EncryptedKeyVersion.\n     *\n     * @param keyName                  Name of the encryption key used to\n     *                                 encrypt the encrypted key.\n     * @param encryptionKeyVersionName Version name of the encryption key used\n     *                                 to encrypt the encrypted key.\n     * @param encryptedKeyIv           Initialization vector of the encrypted\n     *                                 key. The IV of the encryption key used to\n     *                                 encrypt the encrypted key is derived from\n     *                                 this IV.\n     * @param encryptedKeyVersion      The encrypted encryption key version.",
  "org.apache.hadoop.util.QuickSort:<init>()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:loadGangliaConf(org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaConfType)" : null,
  "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:<init>()" : null,
  "org.apache.hadoop.util.bloom.HashFunction:<init>(int,int,int)" : "* Constructor.\n   * <p>\n   * Builds a hash function that must obey to a given maximum number of returned values and a highest value.\n   * @param maxValue The maximum highest returned value.\n   * @param nbHash The number of resulting hashed values.\n   * @param hashType type of the hashing function (see {@link Hash}).",
  "org.apache.hadoop.ha.ActiveStandbyElector:isNodeExists(org.apache.zookeeper.KeeperException$Code)" : null,
  "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:entrySet()" : null,
  "org.apache.hadoop.fs.shell.CopyCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.metrics2.MetricsSystem$AbstractCallback:preStart()" : null,
  "org.apache.hadoop.conf.Configuration$IntegerRanges:isIncluded(int)" : "* Is the given value in the set of ranges.\n     * @param value the value to check\n     * @return is the value in the ranges?",
  "org.apache.hadoop.io.AbstractMapWritable:addToMap(java.lang.Class)" : "* Add a Class to the maps if it is not already present.\n   * @param clazz clazz.",
  "org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream)" : "* Allow derived classes to directly set the underlying stream.\n   * \n   * @param out Underlying output stream.",
  "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:init(byte[],byte[])" : null,
  "org.apache.hadoop.fs.shell.MoveCommands$Rename:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)" : "* Cancels a delegation token from the server end-point. It does not require\n   * being authenticated by the configured <code>Authenticator</code>.\n   *\n   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs\n   * are supported.\n   * @param token the authentication token with the Delegation Token to cancel.\n   * @param doAsUser the user to do as, which will be the token owner.\n   * @throws IOException if an IO error occurred.",
  "org.apache.hadoop.security.token.Token$PrivateToken:equals(java.lang.Object)" : null,
  "org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator$1:<init>()" : null,
  "org.apache.hadoop.ha.HAAdmin:<init>()" : null,
  "org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:getThisBuilder()" : null,
  "org.apache.hadoop.util.ProgramDriver:<init>()" : null,
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:counters()" : null,
  "org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.util.concurrent.HadoopExecutors:<init>()" : null,
  "org.apache.hadoop.io.IOUtils:fsync(java.nio.channels.FileChannel,boolean)" : "* Ensure that any writes to the given file is written to the storage device\n   * that contains it. This method opens channel on given File and closes it\n   * once the sync is done.\n   * Borrowed from Uwe Schindler in LUCENE-5588\n   * @param channel Channel to sync\n   * @param isDir if true, the given file is a directory (Channel should be\n   *          opened for read and ignore IOExceptions, because not all file\n   *          systems and operating systems allow to fsync on a directory)\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ha.ServiceFailedException:<init>(java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)" : null,
  "org.apache.hadoop.util.LineReader:readCustomLine(org.apache.hadoop.io.Text,int,int)" : "* Read a line terminated by a custom delimiter.",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcProcessingTime(long)" : "* Add an RPC processing time sample\n   * @param processingTime the processing time",
  "org.apache.hadoop.io.compress.Lz4Codec:createDecompressor()" : "* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.\n   *\n   * @return a new decompressor for use by this codec",
  "org.apache.hadoop.fs.AbstractFileSystem:getHomeDirectory()" : "* Return the current user's home directory in this file system.\n   * The default implementation returns \"/user/$USER/\".\n   * \n   * @return current user's home directory.",
  "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:getCertificateChain(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:setInput(byte[],int,int)" : null,
  "org.apache.hadoop.io.ElasticByteBufferPool:getBuffer(boolean,int)" : null,
  "org.apache.hadoop.log.LogLevel$CLI:connect(java.net.URL)" : "* Connect to the URL. Supports HTTP/HTTPS and supports SPNEGO\n     * authentication. It falls back to simple authentication if it fails to\n     * initiate SPNEGO.\n     *\n     * @param url the URL address of the daemon servlet\n     * @return a connected connection\n     * @throws Exception if it can not establish a connection.",
  "org.apache.hadoop.util.StringInterner:internStringsInArray(java.lang.String[])" : "* Interns all the strings in the given array in place,\n   * returning the same array.\n   *\n   * @param strings strings.\n   * @return internStringsInArray.",
  "org.apache.hadoop.security.alias.UserProvider:deleteCredentialEntry(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.lib.Interns:tag(java.lang.String,java.lang.String,java.lang.String)" : "* Get a metrics tag.\n   * @param name  of the tag\n   * @param description of the tag\n   * @param value of the tag\n   * @return an interned metrics tag",
  "org.apache.hadoop.fs.FSDataOutputStream:hflush()" : null,
  "org.apache.hadoop.fs.statistics.DurationStatisticSummary:fetchSuccessSummary(org.apache.hadoop.fs.statistics.IOStatistics,java.lang.String)" : "* Fetch the duration timing summary from an IOStatistics source.\n   * If the duration key is unknown, the summary will be incomplete.\n   * @param source source of data\n   * @param key duration statistic key\n   * @return a summary of the statistics.",
  "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:getBlockRegionList()" : null,
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:<init>(java.io.OutputStream)" : null,
  "org.apache.hadoop.util.functional.LazyAtomicReference:get()" : "* Implementation of {@code Supplier.get()}.\n   * <p>\n   * Invoke {@link #eval()} and convert IOEs to\n   * UncheckedIOException.\n   * <p>\n   * This is the {@code Supplier.get()} implementation, which allows\n   * this class to passed into anything taking a supplier.\n   * @return the value\n   * @throws UncheckedIOException if the constructor raised an IOException.",
  "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:findFirstValidInput(java.lang.Object[])" : "* Find the valid input from all the inputs.\n   *\n   * @param <T> Generics Type T.\n   * @param inputs input buffers to look for valid input\n   * @return the first valid input",
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:getAllStoragePolicies()" : "* Retrieve all the storage policies supported by this file system.\n   *\n   * @return all storage policies supported by this filesystem.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.shell.Command:processRawArguments(java.util.LinkedList)" : "* Allows commands that don't use paths to handle the raw arguments.\n   * Default behavior is to expand the arguments via\n   * {@link #expandArguments(LinkedList)} and pass the resulting list to\n   * {@link #processArguments(LinkedList)} \n   * @param args the list of argument strings\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.authorize.AuthorizationException:printStackTrace()" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])" : null,
  "org.apache.hadoop.fs.shell.Tail:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey(byte[],byte[])" : "* Get a scanner that covers a portion of TFile based on keys.\n     * \n     * @param beginKey\n     *          Begin key of the scan (inclusive). If null, scan from the first\n     *          key-value entry of the TFile.\n     * @param endKey\n     *          End key of the scan (exclusive). If null, scan up to the last\n     *          key-value entry of the TFile.\n     * @return The actual coverage of the returned scanner will cover all keys\n     *         greater than or equal to the beginKey and less than the endKey.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:skip(long)" : null,
  "org.apache.hadoop.fs.FilterFs:open(org.apache.hadoop.fs.Path,int)" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:optDouble(java.lang.String,double)" : "* Set optional double parameter for the Builder.\n   *\n   * @see #opt(String, String)",
  "org.apache.hadoop.security.Credentials:addSecretKey(org.apache.hadoop.io.Text,byte[])" : "* Set the key for an alias.\n   * @param alias the alias for the key\n   * @param key the key bytes",
  "org.apache.hadoop.security.authorize.ProxyUsers:getSip()" : null,
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newSourceName(java.lang.String,boolean)" : null,
  "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:hashCode()" : null,
  "org.apache.hadoop.fs.GlobalStorageStatistics$StorageIterator:next()" : null,
  "org.apache.hadoop.conf.Configuration:getOverlay()" : null,
  "org.apache.hadoop.fs.CachingGetSpaceUsed:setShouldFirstRefresh(boolean)" : "* Reset that if we need to do the first refresh.\n   * @param shouldFirstRefresh The flag value to set.",
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies:exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)" : "* <p>\n   * Keep trying a limited number of times, waiting a growing amount of time between attempts,\n   * and then fail by re-throwing the exception.\n   * The time between attempts is <code>sleepTime</code> mutliplied by a random\n   * number in the range of [0, 2 to the number of retries)\n   * </p>\n   *\n   *\n   * @param timeUnit timeUnit.\n   * @param maxRetries maxRetries.\n   * @param sleepTime sleepTime.\n   * @return RetryPolicy.",
  "org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)" : "* Read the next key/value pair in the file into <code>key</code> and\n     * <code>val</code>.\n     * @return Returns true if such a pair exists and false when at\n     * end of file.\n     *\n     * @param key input key.\n     * @param val input val.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:getCounters()" : null,
  "org.apache.hadoop.conf.StorageUnit$2:getShortName()" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:addUniqueIdentityCount(org.apache.hadoop.metrics2.MetricsRecordBuilder)" : null,
  "org.apache.hadoop.crypto.OpensslCipher:tokenizeTransformation(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.CompressionCodecFactory:removeSuffix(java.lang.String,java.lang.String)" : "* Removes a suffix from a filename, if it has it.\n   * @param filename the filename to strip\n   * @param suffix the suffix to remove\n   * @return the shortened filename",
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:readIntArray(java.io.DataInput)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater:<init>(boolean)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:context()" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:maximums()" : null,
  "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:remainingCapacity()" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:checkPrimitive(java.lang.Class)" : null,
  "org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getEntry(int)" : null,
  "org.apache.hadoop.service.ServiceOperations:stop(org.apache.hadoop.service.Service)" : "* Stop a service.\n   * <p>Do nothing if the service is null or not\n   * in a state in which it can be/needs to be stopped.\n   * <p>\n   * The service state is checked <i>before</i> the operation begins.\n   * This process is <i>not</i> thread safe.\n   * @param service a service or null",
  "org.apache.hadoop.ha.ActiveStandbyElector:isSessionExpired(org.apache.zookeeper.KeeperException$Code)" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkValuesEqual(long,java.lang.String,long,java.lang.String)" : "* Validates that the given two values are equal.\n   * @param value1 the first value to check.\n   * @param value1Name the name of the first argument.\n   * @param value2 the second value to check.\n   * @param value2Name the name of the second argument.",
  "org.apache.hadoop.util.StringUtils:getStrings(java.lang.String,java.lang.String)" : "* Returns an arraylist of strings.\n   * @param str the string values\n   * @param delim delimiter to separate the values\n   * @return the arraylist of the separated string values",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem:getStatus(org.apache.hadoop.fs.Path)" : "* Returns a status object describing the use and capacity of the\n   * filesystem. If the filesystem has multiple partitions, the\n   * use and capacity of the partition pointed to by the specified\n   * path is reflected.\n   * @param p Path for which status should be obtained. null means\n   * the default partition.\n   * @return a FsStatus object\n   * @throws IOException\n   *           see specific implementation",
  "org.apache.hadoop.util.AutoCloseableLock:<init>(java.util.concurrent.locks.Lock)" : "* Wrap provided Lock instance.\n   * @param lock Lock instance to wrap in AutoCloseable API.",
  "org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int)" : "* Util method to build socket addr from either.\n   *   {@literal <host>}\n   *   {@literal <host>:<port>}\n   *   {@literal <fs>://<host>:<port>/<path>}\n   *\n   * @param target target.\n   * @param defaultPort default port.\n   * @return socket addr.",
  "org.apache.hadoop.util.RateLimitingFactory:unlimitedRate()" : "* Get the unlimited rate.\n   * @return a rate limiter which always has capacity.",
  "org.apache.hadoop.ipc.DecayRpcScheduler:addTopNCallerSummary(org.apache.hadoop.metrics2.MetricsRecordBuilder)" : null,
  "org.apache.hadoop.fs.FSOutputSummer:getChecksumSize()" : "@return the size for a checksum.",
  "org.apache.hadoop.fs.BlockLocation:getOffset()" : "* Get the start offset of file associated with this block.\n   * @return start offset of file associated with this block.",
  "org.apache.hadoop.ipc.Server$Call:setDeferredResponse(org.apache.hadoop.io.Writable)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setIssueDate(long)" : null,
  "org.apache.hadoop.util.StringUtils:uriToString(java.net.URI[])" : "* uriToString.\n   * @param uris uris.\n   * @return uriToString.",
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLastKey()" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:newRetryInfo(org.apache.hadoop.io.retry.RetryPolicy,java.lang.Exception,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,boolean,long)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:setDictionary(byte[],int,int)" : null,
  "org.apache.hadoop.fs.FileContext:util()" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:readObject(java.io.ObjectInputStream)" : null,
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numCachingErrors()" : "* Number of errors encountered when caching.\n   *\n   * @return the number of errors encountered when caching.",
  "org.apache.hadoop.crypto.key.KeyProviderExtension:isTransient()" : null,
  "org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:listXAttrs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.log.LogThrottlingHelper:getCurrentStats(java.lang.String,int)" : "* Return the summary information for given index.\n   *\n   * @param recorderName The name of the recorder.\n   * @param idx The index value.\n   * @return The summary information.",
  "org.apache.hadoop.fs.ChecksumFileSystem:getChecksumFile(org.apache.hadoop.fs.Path)" : "* Return the name of the checksum file associated with a file.\n   *\n   * @param file the file path.\n   * @return name of the checksum file associated with a file.",
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_setThreadIOStatisticsContext(java.lang.Object)" : "* Set the IOStatisticsContext for the current thread.\n   * @param statisticsContext IOStatistics context instance for the\n   * current thread. If null, the context is reset.",
  "org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable)" : "* Closes the stream ignoring {@link Throwable}.\n   * Must only be called in cleaning up from exception handlers.\n   *\n   * @param stream the Stream to close",
  "org.apache.hadoop.fs.Options$CreateOpts$BytesPerChecksum:getValue()" : null,
  "org.apache.hadoop.metrics2.lib.MutableStat:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)" : "* Construct a sample statistics metric\n   * @param name        of the metric\n   * @param description of the metric\n   * @param sampleName  of the metric (e.g. \"Ops\")\n   * @param valueName   of the metric (e.g. \"Time\", \"Latency\")\n   * @param extended    create extended stats (stdev, min/max etc.) by default.",
  "org.apache.hadoop.security.http.RestCsrfPreventionFilter$ServletFilterHttpInteraction:getMethod()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:initBlock()" : null,
  "org.apache.hadoop.util.RunJar:createClassLoader(java.io.File,java.io.File)" : "* Creates a classloader based on the environment that was specified by the\n   * user. If HADOOP_USE_CLIENT_CLASSLOADER is specified, it creates an\n   * application classloader that provides the isolation of the user class space\n   * from the hadoop classes and their dependencies. It forms a class space for\n   * the user jar as well as the HADOOP_CLASSPATH. Otherwise, it creates a\n   * classloader that simply adds the user jar to the classpath.",
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getServerDefaults()" : null,
  "org.apache.hadoop.io.AbstractMapWritable:copy(org.apache.hadoop.io.Writable)" : "* Used by child copy constructors.\n   * @param other other.",
  "org.apache.hadoop.fs.viewfs.ViewFs:listXAttrs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileUtil:setExecutable(java.io.File,boolean)" : "* Platform independent implementation for {@link File#setExecutable(boolean)}\n   * File#setExecutable does not work as expected on Windows.\n   * Note: revoking execute permission on folders does not have the same\n   * behavior on Windows as on Unix platforms. Creating, deleting or renaming\n   * a file within that folder will still succeed on Windows.\n   * @param f input file\n   * @param executable executable.\n   * @return true on success, false otherwise",
  "org.apache.hadoop.io.InputBuffer$Buffer:<init>()" : null,
  "org.apache.hadoop.fs.FSBuilder:mustDouble(java.lang.String,double)" : "* Set mandatory double parameter for the Builder.\n   *\n   * @param key key.\n   * @param value value.\n   * @return generic type B.\n   * @see #opt(String, String)",
  "org.apache.hadoop.ipc.Server$Call:getRemotePort()" : null,
  "org.apache.hadoop.io.WritableComparator:readVInt(byte[],int)" : "* Reads a zero-compressed encoded integer from a byte array and returns it.\n   * @param bytes byte array with the encoded integer\n   * @param start start index\n   * @throws IOException raised on errors performing I/O.\n   * @return deserialized integer",
  "org.apache.hadoop.fs.PartialListing:get()" : "* Partial listing of the path being listed. In the case where the path is\n   * a file. The list will be a singleton with the file itself.\n   *\n   * @return Partial listing of the path being listed.\n   * @throws IOException if there was an exception getting the listing.",
  "org.apache.hadoop.ipc.Server$Call:getDetailedMetricsName()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSystemSource()" : null,
  "org.apache.hadoop.util.Preconditions:checkArgument(boolean,java.lang.String,java.lang.Object[])" : "* Ensures the truth of an expression involving one or more parameters to the calling method.\n   *\n   * <p>The message of the exception is {@code String.format(f, m)}.</p>\n   *\n   * @param expression a boolean expression\n   * @param errorMsg  the {@link String#format(String, Object...)}\n   *                 exception message if valid. Otherwise,\n   *                 the message is {@link #CHECK_ARGUMENT_EX_MESSAGE}\n   * @param errorMsgArgs the optional values for the formatted exception message.\n   * @throws IllegalArgumentException if {@code expression} is false",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:setWriteChecksum(boolean)" : null,
  "org.apache.hadoop.ipc.ProcessingDetails:<init>(java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.crypto.random.OpensslSecureRandom:<init>()" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Reader:getDefaultCompressionName()" : "* Get the name of the default compression algorithm.\n     * \n     * @return the name of the default compression algorithm.",
  "org.apache.hadoop.ha.ActiveStandbyElector:becomeActive()" : null,
  "org.apache.hadoop.io.UTF8:compareTo(org.apache.hadoop.io.UTF8)" : "Compare two UTF8s.",
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,java.nio.ByteBuffer[],java.nio.ByteBuffer[])" : null,
  "org.apache.hadoop.fs.BBPartHandle:bytes()" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class,int)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:incRecordIndex()" : null,
  "org.apache.hadoop.util.ReadWriteDiskValidator:checkStatus(java.io.File)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:hashCode()" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping$PassThroughMap:<init>()" : null,
  "org.apache.hadoop.ipc.Server$Connection:getTrueCause(java.io.IOException)" : "* Some exceptions ({@link RetriableException} and {@link StandbyException})\n     * that are wrapped as a cause of parameter e are unwrapped so that they can\n     * be sent as the true cause to the client side. In case of\n     * {@link InvalidToken} we go one level deeper to get the true cause.\n     * \n     * @param e the exception that may have a cause we want to unwrap.\n     * @return the true cause for some exceptions.",
  "org.apache.hadoop.io.file.tfile.BCFile:<init>()" : "* Prevent the instantiation of BCFile objects.",
  "org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsPermission)" : "* Copy constructor\n   * \n   * @param other other permission",
  "org.apache.hadoop.util.LightWeightCache:setExpirationTime(org.apache.hadoop.util.LightWeightCache$Entry,long)" : null,
  "org.apache.hadoop.metrics2.util.Quantile:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.lang.String[])" : "* Varargs version of the entry point for testing and other in-JVM use.\n   * Hands off to {@link #serviceMain(List)}\n   * @param args command line arguments.",
  "org.apache.hadoop.security.SaslInputStream:<init>(java.io.InputStream,javax.security.sasl.SaslServer)" : "* Constructs a SASLInputStream from an InputStream and a SaslServer <br>\n   * Note: if the specified InputStream or SaslServer is null, a\n   * NullPointerException may be thrown later when they are used.\n   * \n   * @param inStream\n   *          the InputStream to be processed\n   * @param saslServer\n   *          an initialized SaslServer object",
  "org.apache.hadoop.fs.FileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)" : "* Set the source path to satisfy storage policy.\n   * @param path The source path referring to either a directory or a file.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:finish()" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekToEnd()" : "* Seek to the end of the scanner. The entry returned by the previous\n       * entry() call will be invalid.\n       * \n       * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.net.AbstractDNSToSwitchMapping:<init>()" : "* Create an unconfigured instance",
  "org.apache.hadoop.fs.FileSystem:getUsed(org.apache.hadoop.fs.Path)" : "* Return the total size of all files from a specified path.\n   * @param path the path.\n   * @throws IOException IO failure\n   * @return the number of path content summary.",
  "org.apache.hadoop.ipc.Server$Call:isOpen()" : "* Indicates whether the call has been processed. Always true unless\n     * overridden.\n     *\n     * @return true",
  "org.apache.hadoop.ipc.RPC$Server$VerProtocolImpl:<init>(long,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)" : null,
  "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:available()" : null,
  "org.apache.hadoop.security.alias.KeyStoreProvider:getInputStreamForFile()" : null,
  "org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getMetadata(java.lang.String)" : null,
  "org.apache.hadoop.crypto.CryptoStreamUtils:getInputStreamOffset(java.io.InputStream)" : "* If input stream is {@link org.apache.hadoop.fs.Seekable}, return it's\n   * current position, otherwise return 0;\n   *\n   * @param in wrapper.\n   * @return current position, otherwise return 0.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.Text:find(java.lang.String,int)" : "* Finds any occurrence of <code>what</code> in the backing\n   * buffer, starting as position <code>start</code>. The starting\n   * position is measured in bytes and the return value is in\n   * terms of byte position in the buffer. The backing buffer is\n   * not converted to a string for this operation.\n   *\n   * @param what input what.\n   * @param start input start.\n   * @return byte position of the first occurrence of the search\n   *         string in the UTF-8 buffer or -1 if not found",
  "org.apache.hadoop.util.ExitUtil$ExitException:getExitCode()" : null,
  "org.apache.hadoop.fs.ContentSummary$Builder:spaceConsumed(long)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:tags()" : null,
  "org.apache.hadoop.conf.ReconfigurableBase:isPropertyReconfigurable(java.lang.String)" : "* {@inheritDoc}\n   *\n   * Subclasses may wish to override this with a more efficient implementation.",
  "org.apache.hadoop.ipc.RpcWritable$Buffer:writeTo(org.apache.hadoop.ipc.ResponseBuffer)" : null,
  "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refresh(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)" : null,
  "org.apache.hadoop.util.CleanerUtil:unmapHackImpl()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationByRecordNum(long)" : null,
  "org.apache.hadoop.conf.StorageUnit$3:getShortName()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)" : null,
  "org.apache.hadoop.util.functional.TaskPool$Builder:suppressExceptions()" : "* Suppress exceptions from tasks.\n     * RemoteIterator exceptions are not suppressable.\n     * @return the builder.",
  "org.apache.hadoop.fs.shell.find.Result:negate()" : "* Negate this result.\n   * @return Result.",
  "org.apache.hadoop.fs.impl.prefetch.BlockManager:<init>(org.apache.hadoop.fs.impl.prefetch.BlockData)" : "* Constructs an instance of {@code BlockManager}.\n   *\n   * @param blockData information about each block of the underlying file.\n   *\n   * @throws IllegalArgumentException if blockData is null.",
  "org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)" : "* Construct an RPC server.\n     * @param instance the instance whose methods will be called\n     * @param conf the configuration to use\n     * @param bindAddress the address to bind on to listen for connection\n     * @param port the port to listen for connections on\n     * \n     * @deprecated Use #Server(Class, Object, Configuration, String, int)\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.crypto.CryptoOutputStream:getIOStatistics()" : null,
  "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : "* Create a file with the provided permission.\n   *\n   * The permission of the file is set to be the provided permission as in\n   * setPermission, not permission{@literal &~}umask\n   *\n   * The HDFS implementation is implemented using two RPCs.\n   * It is understood that it is inefficient,\n   * but the implementation is thread-safe. The other option is to change the\n   * value of umask in configuration to be 0, but it is not thread-safe.\n   *\n   * @param fs FileSystem\n   * @param file the name of the file to be created\n   * @param permission the permission of the file\n   * @return an output stream\n   * @throws IOException IO failure",
  "org.apache.hadoop.util.ShutdownThreadsHelper:shutdownExecutorService(java.util.concurrent.ExecutorService,long)" : "* shutdownExecutorService.\n   *\n   * @param service {@link ExecutorService to be shutdown}\n   * @param timeoutInMs time to wait for {@link\n   * ExecutorService#awaitTermination(long, java.util.concurrent.TimeUnit)}\n   *                    calls in milli seconds.\n   * @return <tt>true</tt> if the service is terminated,\n   * <tt>false</tt> otherwise\n   * @throws InterruptedException if the thread is interrupted.",
  "org.apache.hadoop.io.compress.CodecPool:returnDecompressor(org.apache.hadoop.io.compress.Decompressor)" : "* Return the {@link Decompressor} to the pool.\n   * \n   * @param decompressor the <code>Decompressor</code> to be returned to the \n   *                     pool",
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:hashCode()" : null,
  "org.apache.hadoop.fs.FilterFs:getUriDefaultPort()" : null,
  "org.apache.hadoop.fs.HarFileSystem:open(org.apache.hadoop.fs.PathHandle,int)" : null,
  "org.apache.hadoop.crypto.CryptoStreamUtils:getBufferSize(org.apache.hadoop.conf.Configuration)" : "* Read crypto buffer size.\n   *\n   * @param conf configuration.\n   * @return hadoop.security.crypto.buffer.size.",
  "org.apache.hadoop.ipc.Server:bind(java.net.ServerSocket,java.net.InetSocketAddress,int,org.apache.hadoop.conf.Configuration,java.lang.String)" : null,
  "org.apache.hadoop.fs.FilterFs:getXAttrs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence,java.nio.charset.Charset)" : "* Write a line of text to a file. Characters are encoded into bytes using the\n   * specified charset. This utility method opens the file for writing, creating\n   * the file if it does not exist, or overwrites an existing file.\n   *\n   * @param fs the file context with which to create the file\n   * @param path the path to the file\n   * @param charseq the char sequence to write to the file\n   * @param cs the charset to use for encoding\n   *\n   * @return the file context\n   *\n   * @throws NullPointerException if any of the arguments are {@code null}\n   * @throws IOException if an I/O error occurs creating or writing to the file",
  "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:withFilter(java.util.Collection)" : "* Declare the fields to filter.\n     * @param fields iterable of field names.\n     * @return the builder",
  "org.apache.hadoop.fs.AvroFSInput:read(byte[],int,int)" : null,
  "org.apache.hadoop.fs.DelegateToFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)" : null,
  "org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket,long)" : "* Returns OutputStream for the socket. If the socket has an associated\n   * SocketChannel then it returns a \n   * {@link SocketOutputStream} with the given timeout. If the socket does not\n   * have a channel, {@link Socket#getOutputStream()} is returned. In the later\n   * case, the timeout argument is ignored and the write will wait until \n   * data is available.<br><br>\n   * \n   * Any socket created using socket factories returned by {@link NetUtils},\n   * must use this interface instead of {@link Socket#getOutputStream()}.\n   * \n   * @see Socket#getChannel()\n   * \n   * @param socket socket.\n   * @param timeout timeout in milliseconds. This may not always apply. zero\n   *        for waiting as long as necessary.\n   * @return OutputStream for writing to the socket.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.SequenceFile:getDefaultCompressionType(org.apache.hadoop.conf.Configuration)" : "* Get the compression type for the reduce outputs\n   * @param job the job config to look in\n   * @return the kind of compression to use",
  "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$4:suffix()" : null,
  "org.apache.hadoop.io.serializer.JavaSerializationComparator:compare(java.io.Serializable,java.io.Serializable)" : null,
  "org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)" : "* Filter files/directories in the given list of paths using user-supplied\n     * path filter.\n     * \n     * @param files is a list of paths\n     * @param filter is the filter\n     *\n     * @return a list of statuses for the files under the given paths after\n     *         applying the filter\n     *\n     * @throws AccessControlException If access is denied\n     * @throws FileNotFoundException If a file in <code>files</code> does not \n     *           exist\n     * @throws IOException If an I/O error occurred\n     * \n     * Exceptions applicable to file systems accessed over RPC:\n     * @throws RpcClientException If an exception occurred in the RPC client\n     * @throws RpcServerException If an exception occurred in the RPC server\n     * @throws UnexpectedServerException If server implementation throws \n     *           undeclared exception to RPC server",
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(java.lang.String,java.lang.String,java.lang.String,boolean)" : "* Add a tag to the metrics\n   * @param name  of the tag\n   * @param description of the tag\n   * @param value of the tag\n   * @param override  existing tag if true\n   * @return the registry (for keep adding tags)",
  "org.apache.hadoop.fs.RawPathHandle:toString()" : null,
  "org.apache.hadoop.util.functional.ConsumerRaisingIOE:andThen(org.apache.hadoop.util.functional.ConsumerRaisingIOE)" : "* after calling {@link #accept(Object)},\n   * invoke the next consumer in the chain.\n   * @param next next consumer\n   * @return the chain.",
  "org.apache.hadoop.util.BlockingThreadPoolExecutorService:newDaemonThreadFactory(java.lang.String)" : "* Get a named {@link ThreadFactory} that just builds daemon threads.\n   *\n   * @param prefix name prefix for all threads created from the factory\n   * @return a thread factory that creates named, daemon threads with\n   * the supplied exception handler and normal priority",
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getSymlink()" : null,
  "org.apache.hadoop.crypto.OpensslCipher:checkState()" : "Check whether context is initialized.",
  "org.apache.hadoop.io.EnumSetWritable:add(java.lang.Object)" : null,
  "org.apache.hadoop.fs.FileSystemStorageStatistics:reset()" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType)" : null,
  "org.apache.hadoop.fs.ContentSummary$Builder:typeQuota(org.apache.hadoop.fs.StorageType,long)" : null,
  "org.apache.hadoop.fs.viewfs.RegexMountPoint:getSrcPattern()" : null,
  "org.apache.hadoop.fs.FilterFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)" : null,
  "org.apache.hadoop.util.Shell:setEnvironment(java.util.Map)" : "* Set the environment for the command.\n   * @param env Mapping of environment variables",
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementGauge(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem$2:close()" : null,
  "org.apache.hadoop.io.MapWritable:size()" : null,
  "org.apache.hadoop.io.ElasticByteBufferPool$Key:<init>(int,long)" : null,
  "org.apache.hadoop.util.VersionInfo:getUrl()" : "* Get the URL for the Hadoop repository.\n   * @return the URL of the Hadoop repository",
  "org.apache.hadoop.security.SecurityUtil:login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)" : "* Login as a principal specified in config. Substitute $host in user's Kerberos principal \n   * name with hostname. If non-secure mode - return. If no keytab available -\n   * bail out with an exception\n   * \n   * @param conf\n   *          conf to use\n   * @param keytabFileKey\n   *          the key to look for keytab file in conf\n   * @param userNameKey\n   *          the key to look for user's Kerberos principal name in conf\n   * @param hostname\n   *          hostname to use for substitution\n   * @throws IOException if the config doesn't specify a keytab",
  "org.apache.hadoop.security.http.RestCsrfPreventionFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)" : "* Constructs a mapping of configuration properties to be used for filter\n   * initialization.  The mapping includes all properties that start with the\n   * specified configuration prefix.  Property names in the mapping are trimmed\n   * to remove the configuration prefix.\n   *\n   * @param conf configuration to read\n   * @param confPrefix configuration prefix\n   * @return mapping of configuration properties to be used for filter\n   *     initialization",
  "org.apache.hadoop.crypto.key.kms.ValueQueue$NamedRunnable:cancel()" : null,
  "org.apache.hadoop.util.Progress:addPhase()" : "* Adds a node to the tree. Gives equal weightage to all phases.\n   * @return Progress.",
  "org.apache.hadoop.service.LoggingStateChangeListener:<init>(org.slf4j.Logger)" : "* Log events to the given log\n   * @param log destination for events",
  "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:close()" : null,
  "org.apache.hadoop.fs.DF:getAvailable()" : "@return the usable space remaining on the filesystem in bytes.",
  "org.apache.hadoop.io.compress.DefaultCodec:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getKeyStore()" : null,
  "org.apache.hadoop.metrics2.MetricsTag:<init>(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)" : "* Construct the tag with name, description and value\n   * @param info  of the tag\n   * @param value of the tag",
  "org.apache.hadoop.fs.shell.find.FindOptions:getOut()" : "* Returns the output stream to be used.\n   *\n   * @return output stream to be used",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:incrRpcCallSuccesses()" : "* One RPC call success event.",
  "org.apache.hadoop.fs.FileContext:setReplication(org.apache.hadoop.fs.Path,short)" : "* Set replication for an existing file.\n   * \n   * @param f file name\n   * @param replication new replication\n   *\n   * @return true if successful\n   *\n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If file <code>f</code> does not exist\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server",
  "org.apache.hadoop.conf.Configuration:dumpConfiguration(org.apache.hadoop.conf.Configuration,java.lang.String,java.io.Writer)" : "*  Writes properties and their attributes (final and resource)\n   *  to the given {@link Writer}.\n   *  <ul>\n   *  <li>\n   *  When propertyName is not empty, and the property exists\n   *  in the configuration, the format of the output would be,\n   *  <pre>\n   *  {\n   *    \"property\": {\n   *      \"key\" : \"key1\",\n   *      \"value\" : \"value1\",\n   *      \"isFinal\" : \"key1.isFinal\",\n   *      \"resource\" : \"key1.resource\"\n   *    }\n   *  }\n   *  </pre>\n   *  </li>\n   *\n   *  <li>\n   *  When propertyName is null or empty, it behaves same as\n   *  {@link #dumpConfiguration(Configuration, Writer)}, the\n   *  output would be,\n   *  <pre>\n   *  { \"properties\" :\n   *      [ { key : \"key1\",\n   *          value : \"value1\",\n   *          isFinal : \"key1.isFinal\",\n   *          resource : \"key1.resource\" },\n   *        { key : \"key2\",\n   *          value : \"value2\",\n   *          isFinal : \"ke2.isFinal\",\n   *          resource : \"key2.resource\" }\n   *       ]\n   *   }\n   *  </pre>\n   *  </li>\n   *\n   *  <li>\n   *  When propertyName is not empty, and the property is not\n   *  found in the configuration, this method will throw an\n   *  {@link IllegalArgumentException}.\n   *  </li>\n   *  </ul>\n   *  <p>\n   * @param config the configuration\n   * @param propertyName property name\n   * @param out the Writer to write to\n   * @throws IOException raised on errors performing I/O.\n   * @throws IllegalArgumentException when property name is not\n   *   empty and the property is not found in configuration\n   *",
  "org.apache.hadoop.net.DNSDomainNameResolver:getHostnameByIP(java.net.InetAddress)" : null,
  "org.apache.hadoop.fs.FileUtil:fullyDelete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Recursively delete a directory.\n   *\n   * @param fs {@link FileSystem} on which the path is present\n   * @param dir directory to recursively delete\n   * @throws IOException raised on errors performing I/O.\n   * @deprecated Use {@link FileSystem#delete(Path, boolean)}",
  "org.apache.hadoop.fs.permission.AclStatus:getEntries()" : "* Returns the list of all ACL entries, ordered by their natural ordering.\n   *\n   * @return List&lt;AclEntry&gt; unmodifiable ordered list of all ACL entries",
  "org.apache.hadoop.metrics2.lib.MutableRollingAverages:add(java.lang.String,long)" : "* @param name\n   *          name of metric\n   * @param value\n   *          value of metric",
  "org.apache.hadoop.metrics2.lib.MutableStat:resetMinMax()" : "* Reset the all time min max of the metric",
  "org.apache.hadoop.util.DiskChecker:mkdirsWithExistsCheck(java.io.File)" : "* The semantics of mkdirsWithExistsCheck method is different from the mkdirs\n   * method provided in the Sun's java.io.File class in the following way:\n   * While creating the non-existent parent directories, this method checks for\n   * the existence of those directories if the mkdir fails at any point (since\n   * that directory might have just been created by some other process).\n   * If both mkdir() and the exists() check fails for any seemingly\n   * non-existent directory, then we signal an error; Sun's mkdir would signal\n   * an error (return false) if a directory it is attempting to create already\n   * exists or the mkdir fails.\n   * @param dir\n   * @return true on success, false on failure",
  "org.apache.hadoop.util.ZKUtil:parseAuth(java.lang.String)" : "* Parse a comma-separated list of authentication mechanisms. Each\n   * such mechanism should be of the form 'scheme:auth' -- the same\n   * syntax used for the 'addAuth' command in the ZK CLI.\n   * \n   * @param authString the comma-separated auth mechanisms\n   * @return a list of parsed authentications\n   * @throws BadAuthFormatException if the auth format is invalid",
  "org.apache.hadoop.metrics2.impl.MetricGaugeLong:type()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.FSInputChecker:read(byte[],int,int)" : "* Read checksum verified bytes from this byte-input stream into \n   * the specified byte array, starting at the given offset.\n   *\n   * <p> This method implements the general contract of the corresponding\n   * <code>{@link InputStream#read(byte[], int, int) read}</code> method of\n   * the <code>{@link InputStream}</code> class.  As an additional\n   * convenience, it attempts to read as many bytes as possible by repeatedly\n   * invoking the <code>read</code> method of the underlying stream.  This\n   * iterated <code>read</code> continues until one of the following\n   * conditions becomes true: <ul>\n   *\n   *   <li> The specified number of bytes have been read,\n   *\n   *   <li> The <code>read</code> method of the underlying stream returns\n   *   <code>-1</code>, indicating end-of-file.\n   *\n   * </ul> If the first <code>read</code> on the underlying stream returns\n   * <code>-1</code> to indicate end-of-file then this method returns\n   * <code>-1</code>.  Otherwise this method returns the number of bytes\n   * actually read.\n   *\n   * @param      b     destination buffer.\n   * @param      off   offset at which to start storing bytes.\n   * @param      len   maximum number of bytes to read.\n   * @return     the number of bytes read, or <code>-1</code> if the end of\n   *             the stream has been reached.\n   * @exception  IOException  if an I/O error occurs.\n   *             ChecksumException if any checksum error occurs",
  "org.apache.hadoop.security.ProviderUtils:locatePassword(java.lang.String,java.lang.String)" : "* The password is either found in the environment or in a file. This\n   * routine implements the logic for locating the password in these\n   * locations.\n   *\n   * @param envWithPass  The name of the environment variable that might\n   *                     contain the password. Must not be null.\n   * @param fileWithPass The name of a file that could contain the password.\n   *                     Can be null.\n   * @return The password as a char []; null if not found.\n   * @throws IOException If fileWithPass is non-null and points to a\n   * nonexistent file or a file that fails to open and be read properly.",
  "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:maybeStripWrappedQuotes(java.lang.String)" : "* Strip any quotes from around a header.\n   * This is needed when processing log entries.\n   * @param header field.\n   * @return field without quotes.",
  "org.apache.hadoop.fs.HarFileSystem:getConf()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getSummary(boolean)" : null,
  "org.apache.hadoop.security.UserGroupInformation:getSubject()" : "* Get the underlying subject from this ugi.\n   * @return the subject that represents this user.",
  "org.apache.hadoop.net.unix.DomainSocket:setAttribute(int,int)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getCompressedSize()" : "* Get the compressed size of the block.\n       * \n       * @return compressed size of the block.",
  "org.apache.hadoop.metrics2.util.MetricsCache$Record:tags()" : "* @return the entry set of the tags of the record",
  "org.apache.hadoop.log.LogThrottlingHelper:getLogSupressionMessage(org.apache.hadoop.log.LogThrottlingHelper$LogAction)" : "* Helper function to create a message about how many log statements were\n   * suppressed in the provided log action. If no statements were suppressed,\n   * this returns an empty string. The message has the format (without quotes):\n   *\n   * <p>' (suppressed logging <i>{suppression_count}</i> times)'</p>\n   *\n   * @param action The log action to produce a message about.\n   * @return A message about suppression within this action.",
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_create(java.lang.Object)" : "* Create a new {@link IOStatisticsSnapshot} instance.\n   * @param source optional source statistics\n   * @return an IOStatisticsSnapshot.\n   * @throws ClassCastException if the {@code source} is not null and not an IOStatistics instance",
  "org.apache.hadoop.fs.AbstractFileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Create a file system instance for the specified uri using the conf. The\n   * conf is used to find the class name that implements the file system. The\n   * conf is also passed to the file system for its configuration.\n   *\n   * @param uri URI of the file system\n   * @param conf Configuration for the file system\n   * \n   * @return Returns the file system for the given URI\n   *\n   * @throws UnsupportedFileSystemException file system for <code>uri</code> is\n   *           not found",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMaps(java.util.Map,java.util.Map,java.util.function.BiFunction,java.util.function.Function)" : "* Aggregate two maps so that the destination.\n   * @param <E> type of values\n   * @param dest destination map.\n   * @param other other map\n   * @param aggregateFn function to aggregate the values.\n   * @param copyFn function to copy the value",
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:getTmax()" : "* @return the tmax",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$2:unit()" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getLargeReadOps()" : null,
  "org.apache.hadoop.security.SaslRpcClient:<init>(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)" : "* Create a SaslRpcClient that can be used by a RPC client to negotiate\n   * SASL authentication with a RPC server\n   * @param ugi - connecting user\n   * @param protocol - RPC protocol\n   * @param serverAddr - InetSocketAddress of remote server\n   * @param conf - Configuration",
  "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:canonicalizeUri(java.net.URI)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:doSync()" : "Do the sync checks.",
  "org.apache.hadoop.util.DiskChecker:checkAccessByFileMethods(java.io.File)" : "* Checks that the current running process can read, write, and execute the\n   * given directory by using methods of the File object.\n   * \n   * @param dir File to check\n   * @throws DiskErrorException if dir is not readable, not writable, or not\n   *   executable",
  "org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile()" : "* Read /proc/meminfo, parse and compute memory information only once.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:reset()" : "* Reset all data structures and mutable state.",
  "org.apache.hadoop.util.bloom.BloomFilter:and(org.apache.hadoop.util.bloom.Filter)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:getIOStatistics()" : "* Get the IO Statistics of the nested stream, falling back to\n     * null if the stream does not implement the interface\n     * {@link IOStatisticsSource}.\n     * @return an IOStatistics instance or null",
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:finalize()" : null,
  "org.apache.hadoop.fs.ContentSummary$Builder:typeQuota(long[])" : null,
  "org.apache.hadoop.ipc.FairCallQueue:isServerFailOverEnabled()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication()" : null,
  "org.apache.hadoop.util.CleanerUtil:getCleaner()" : "* Reference to a BufferCleaner that does unmapping.\n   * @return {@code null} if not supported.",
  "org.apache.hadoop.fs.VectoredReadUtils:mergeSortedRanges(java.util.List,int,int,int)" : "* Merge sorted ranges to optimize the access from the underlying file\n   * system.\n   * The motivations are that:\n   * <ul>\n   *   <li>Upper layers want to pass down logical file ranges.</li>\n   *   <li>Fewer reads have better performance.</li>\n   *   <li>Applications want callbacks as ranges are read.</li>\n   *   <li>Some file systems want to round ranges to be at checksum boundaries.</li>\n   * </ul>\n   *\n   * @param sortedRanges already sorted list of ranges based on offset.\n   * @param chunkSize round the start and end points to multiples of chunkSize\n   * @param minimumSeek the smallest gap that we should seek over in bytes\n   * @param maxSize the largest combined file range in bytes\n   * @return the list of sorted CombinedFileRanges that cover the input",
  "org.apache.hadoop.util.SysInfoWindows:getStorageBytesWritten()" : null,
  "org.apache.hadoop.fs.permission.PermissionStatus:toString()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues0(int,int)" : null,
  "org.apache.hadoop.security.alias.UserProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep:getOutputBlocks()" : null,
  "org.apache.hadoop.io.compress.CompressionCodecFactory:setCodecClasses(org.apache.hadoop.conf.Configuration,java.util.List)" : "* Sets a list of codec classes in the configuration. In addition to any\n   * classes specified using this method, {@link CompressionCodec} classes on\n   * the classpath are discovered using a Java ServiceLoader.\n   * @param conf the configuration to modify\n   * @param classes the list of classes to set",
  "org.apache.hadoop.security.ShellBasedIdMapping:loadFullMaps()" : null,
  "org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:close()" : "* Close: closes any upload stream and byteArray provided in the\n     * constructor.\n     *\n     * @throws IOException inherited exception.",
  "org.apache.hadoop.net.InnerNodeImpl:<init>(java.lang.String)" : "* Construct an InnerNode from a path-like string.\n   * @param path input path.",
  "org.apache.hadoop.ha.HAAdmin:createReqInfo()" : null,
  "org.apache.hadoop.io.MultipleIOException$Builder:build()" : "* @return null if nothing is added to this builder;\n     *         otherwise, return an {@link IOException}",
  "org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:advanceIndex()" : "* Advances the index, which will change the current index\n   * if called enough times.",
  "org.apache.hadoop.jmx.JMXJsonServlet:init()" : "* Initialize this servlet.",
  "org.apache.hadoop.ipc.Server:get()" : "@return Returns the server instance called under or null.  May be called under\n   * {@link #call(Writable, long)} implementations, and under {@link Writable}\n   * methods of paramters and return values.  Permits applications to access\n   * the server context.",
  "org.apache.hadoop.io.ArrayPrimitiveWritable$Internal:<init>(java.lang.Object)" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler$Counters:isZeros()" : null,
  "org.apache.hadoop.metrics2.impl.MetricGaugeInt:type()" : null,
  "org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:<init>(java.io.InputStream)" : "* Stream constructor, file and byteArray field will be null.\n     *\n     * @param uploadStream stream to upload.",
  "org.apache.hadoop.ipc.Server$Call:setFederatedNamespaceState(org.apache.hadoop.thirdparty.protobuf.ByteString)" : null,
  "org.apache.hadoop.fs.TrashPolicyDefault:moveToTrash(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileSystem:globStatus(org.apache.hadoop.fs.Path)" : "* <p>Return all the files that match filePattern and are not checksum\n   * files. Results are sorted by their names.\n   *\n   * <p>\n   * A filename pattern is composed of <i>regular</i> characters and\n   * <i>special pattern matching</i> characters, which are:\n   *\n   * <dl>\n   *  <dd>\n   *   <dl>\n   *    <dt> <tt> ? </tt>\n   *    <dd> Matches any single character.\n   *\n   *    <dt> <tt> * </tt>\n   *    <dd> Matches zero or more characters.\n   *\n   *    <dt> <tt> [<i>abc</i>] </tt>\n   *    <dd> Matches a single character from character set\n   *     <tt>{<i>a,b,c</i>}</tt>.\n   *\n   *    <dt> <tt> [<i>a</i>-<i>b</i>] </tt>\n   *    <dd> Matches a single character from the character range\n   *     <tt>{<i>a...b</i>}</tt>.  Note that character <tt><i>a</i></tt> must be\n   *     lexicographically less than or equal to character <tt><i>b</i></tt>.\n   *\n   *    <dt> <tt> [^<i>a</i>] </tt>\n   *    <dd> Matches a single character that is not from character set or range\n   *     <tt>{<i>a</i>}</tt>.  Note that the <tt>^</tt> character must occur\n   *     immediately to the right of the opening bracket.\n   *\n   *    <dt> <tt> \\<i>c</i> </tt>\n   *    <dd> Removes (escapes) any special meaning of character <i>c</i>.\n   *\n   *    <dt> <tt> {ab,cd} </tt>\n   *    <dd> Matches a string from the string set <tt>{<i>ab, cd</i>} </tt>\n   *\n   *    <dt> <tt> {ab,c{de,fh}} </tt>\n   *    <dd> Matches a string from the string set <tt>{<i>ab, cde, cfh</i>}</tt>\n   *\n   *   </dl>\n   *  </dd>\n   * </dl>\n   *\n   * @param pathPattern a glob specifying a path pattern\n\n   * @return an array of paths that match the path pattern\n   * @throws IOException IO failure",
  "org.apache.hadoop.util.functional.FutureIO:raiseInnerCause(java.util.concurrent.CompletionException)" : "* Extract the cause of a completion failure and rethrow it if an IOE\n   * or RTE.\n   * @param e exception.\n   * @param <T> type of return value.\n   * @return nothing, ever.\n   * @throws IOException either the inner IOException, or a wrapper around\n   * any non-Runtime-Exception\n   * @throws RuntimeException if that is the inner cause.",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:handleKind(org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getThisBuilder()" : "* Return the concrete implementation of the builder instance.",
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainSimpleSort(org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data,int,int,int)" : "* This is the most hammered method of this class.\n  *\n  * <p>\n  * This is the version using unrolled loops. Normally I never use such ones\n  * in Java code. The unrolling has shown a noticable performance improvement\n  * on JRE 1.4.2 (Linux i586 / HotSpot Client). Of course it depends on the\n  * JIT compiler of the vm.\n  * </p>",
  "org.apache.hadoop.util.Shell:isSetsidSupported()" : "* Look for <code>setsid</code>.\n   * @return true if <code>setsid</code> was present",
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:isRecursive()" : "* Return true to create the parent directories if they do not exist.\n   *\n   * @return if create the parent directories if they do not exist true,not false.",
  "org.apache.hadoop.fs.FilterFileSystem:open(org.apache.hadoop.fs.PathHandle,int)" : null,
  "org.apache.hadoop.fs.FileSystem:setQuota(org.apache.hadoop.fs.Path,long,long)" : "* Set quota for the given {@link Path}.\n   *\n   * @param src the target path to set quota for\n   * @param namespaceQuota the namespace quota (i.e., # of files/directories)\n   *                       to set\n   * @param storagespaceQuota the storage space quota to set\n   * @throws IOException IO failure",
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,java.lang.String[])" : "* Set a string array as mandatory option.\n   *\n   * @see #must(String, String)",
  "org.apache.hadoop.http.lib.StaticUserWebFilter$User:hashCode()" : null,
  "org.apache.hadoop.fs.FileSystem:setReplication(org.apache.hadoop.fs.Path,short)" : "* Set the replication for an existing file.\n   * If a filesystem does not support replication, it will always\n   * return true: the check for a file existing may be bypassed.\n   * This is the default behavior.\n   * @param src file name\n   * @param replication new replication\n   * @throws IOException an IO failure.\n   * @return true if successful, or the feature in unsupported;\n   *         false if replication is supported but the file does not exist,\n   *         or is a directory",
  "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:start(java.util.List)" : "* Start the connection to the ZooKeeper ensemble.\n   * @param authInfos List of authentication keys.\n   * @throws IOException If the connection cannot be started.",
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:getUnits()" : "* @return the units",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:readAByte(java.io.InputStream)" : "* This method reads a Byte from the compressed stream. Whenever we need to\n  * read from the underlying compressed stream, this method should be called\n  * instead of directly calling the read method of the underlying compressed\n  * stream. This method does important record keeping to have the statistic\n  * that how many bytes have been read off the compressed stream.",
  "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:metrics()" : null,
  "org.apache.hadoop.io.WritableComparator:readInt(byte[],int)" : "* Parse an integer from a byte array.\n   * @param bytes bytes.\n   * @param start start.\n   * @return integer from a byte array",
  "org.apache.hadoop.util.StringUtils:limitDecimalTo2(double)" : "* limitDecimalTo2.\n   *\n   * @param d double param.\n   * @return string value (\"%.2f\").\n   * @deprecated use StringUtils.format(\"%.2f\", d).",
  "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:shouldRetry(java.lang.Exception,int,int,boolean)" : null,
  "org.apache.hadoop.net.SocketOutputStream:setTimeout(int)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesRead(long)" : "* Increment the bytes read in the statistics.\n     * @param newBytes the additional bytes read",
  "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String)" : null,
  "org.apache.hadoop.fs.ContentSummary:getDirectoryCount()" : "@return the directory count",
  "org.apache.hadoop.service.launcher.ServiceLauncher:isClassnameDefined()" : "* Probe for service classname being defined.\n   * @return true if the classname is set",
  "org.apache.hadoop.net.NetUtils:wrapException(java.lang.String,int,java.lang.String,int,java.io.IOException)" : "* Take an IOException , the local host port and remote host port details and\n   * return an IOException with the input exception as the cause and also\n   * include the host details. The new exception provides the stack trace of the\n   * place where the exception is thrown and some extra diagnostics information.\n   * If the exception is of type BindException, ConnectException,\n   * UnknownHostException, SocketTimeoutException or has a String constructor,\n   * return a new one of the same type; Otherwise return an IOException.\n   *\n   * @param destHost target host (nullable)\n   * @param destPort target port\n   * @param localHost local host (nullable)\n   * @param localPort local port\n   * @param exception the caught exception.\n   * @return an exception to throw",
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:blockSize(long)" : "* Set block size.\n   *\n   * @param blkSize block size.\n   * @return B Generics Type.",
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:applyToIOStatisticsSnapshot(java.io.Serializable,org.apache.hadoop.util.functional.FunctionRaisingIOE)" : "* Apply a function to an object which may be an IOStatisticsSnapshot.\n   * @param <T> return type\n   * @param source statistics snapshot\n   * @param fun function to invoke if {@code source} is valid.\n   * @return the applied value\n   * @throws UncheckedIOException Any IO exception.\n   * @throws IllegalArgumentException if the supplied class is not a snapshot",
  "org.apache.hadoop.fs.ContentSummary$Builder:<init>()" : null,
  "org.apache.hadoop.security.token.DtUtilShell$Remove:execute()" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:addTimedOperation(java.lang.String,java.time.Duration)" : null,
  "org.apache.hadoop.fs.http.HttpFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.PathIsNotDirectoryException:<init>(java.lang.String)" : "@param path for the exception",
  "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)" : null,
  "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:processArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.ChecksumException:<init>(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.permission.FsPermission:valueOf(java.lang.String)" : "* Create a FsPermission from a Unix symbolic permission string\n   * @param unixSymbolicPermission e.g. \"-rw-rw-rw-\"\n   * @return FsPermission.",
  "org.apache.hadoop.log.LogThrottlingHelper$NoLogAction:getStats(int)" : null,
  "org.apache.hadoop.metrics2.source.JvmMetrics:getGcUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder)" : null,
  "org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer:lessThanUnsigned(long,long)" : "* Returns true if x1 is less than x2, when both values are treated as\n       * unsigned.",
  "org.apache.hadoop.fs.Options$CreateOpts$ChecksumParam:getValue()" : null,
  "org.apache.hadoop.util.PriorityQueue:put(java.lang.Object)" : "* Adds an Object to a PriorityQueue in log(size) time.\n   * If one tries to add more objects than maxSize from initialize\n   * a RuntimeException (ArrayIndexOutOfBound) is thrown.\n   * @param element element.",
  "org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String)" : "@param path for the exception",
  "org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int,int)" : null,
  "org.apache.hadoop.io.compress.DefaultCodec:createOutputStream(java.io.OutputStream)" : null,
  "org.apache.hadoop.fs.CachingGetSpaceUsed:initRefreshThread(boolean)" : "* RunImmediately should set true, if we skip the first refresh.\n   * @param runImmediately The param default should be false.",
  "org.apache.hadoop.metrics2.lib.MutableQuantiles:setQuantileInfos(int)" : "* Initialize quantileInfos array.\n   *\n   * @param length of the quantileInfos array.",
  "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:pack(java.util.Collection)" : null,
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:meanStatistics()" : null,
  "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])" : "* Construct the preferred type of SequenceFile Writer.\n   * @param fc The context for the specified file.\n   * @param conf The configuration.\n   * @param name The name of the file.\n   * @param keyClass The 'key' type.\n   * @param valClass The 'value' type.\n   * @param compressionType The compression type.\n   * @param codec The compression codec.\n   * @param metadata The metadata of the file.\n   * @param createFlag gives the semantics of create: overwrite, append etc.\n   * @param opts file creation options; see {@link CreateOpts}.\n   * @return Returns the handle to the constructed SequenceFile Writer.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:emptyStatisticsStore()" : "* Get the shared instance of the immutable empty statistics\n   * store.\n   * @return an empty statistics object.",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getMountPathInfo(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)" : "* Gets the mount path info, which contains the target file system and\n   * remaining path to pass to the target file system.\n   *\n   * @param path the path.\n   * @param conf configuration.\n   * @return mount path info.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.HarFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.compress.ZStandardCodec:getCompressionBufferSize(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.ShutdownHookManager$HookEntry:getTimeout()" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:needsInput()" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:getRemaining()" : "* <p>Returns the number of bytes remaining in the input buffers;\n   * normally called when finished() is true to determine amount of post-stream\n   * data.</p>\n   *\n   * @return the total (non-negative) number of unprocessed bytes in input",
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerComplete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)" : "* The upload complete operation.\n   * @param multipartUploadId the ID of the upload\n   * @param filePath path\n   * @param handleMap map of handles\n   * @return the path handle\n   * @throws IOException failure",
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:release()" : null,
  "org.apache.hadoop.io.WritableFactories:<init>()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.fs.FSDataOutputStream:abort()" : "* Invoke {@code abort()} on the wrapped stream if it\n   * is Abortable, otherwise raise an\n   * {@code UnsupportedOperationException}.\n   * @throws UnsupportedOperationException if not available.\n   * @return the result.",
  "org.apache.hadoop.fs.FileContext:satisfyStoragePolicy(org.apache.hadoop.fs.Path)" : "* Set the source path to satisfy storage policy.\n   * @param path The source path referring to either a directory or a file.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.fs.FileSystem:setDefaultUri(org.apache.hadoop.conf.Configuration,java.net.URI)" : "* Set the default FileSystem URI in a configuration.\n   * @param conf the configuration to alter\n   * @param uri the new default filesystem uri",
  "org.apache.hadoop.crypto.CryptoOutputStream:close()" : null,
  "org.apache.hadoop.net.TableMapping:reloadCachedMappings()" : null,
  "org.apache.hadoop.fs.FileSystem:createDataInputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Create instance of the standard {@link FSDataInputStreamBuilder} for the\n   * given filesystem and path.\n   * @param fileSystem owner\n   * @param path path to read\n   * @return a builder.",
  "org.apache.hadoop.io.IOUtils:skipFully(java.io.InputStream,long)" : "* Similar to readFully(). Skips bytes in a loop.\n   * @param in The InputStream to skip bytes from\n   * @param len number of bytes to skip.\n   * @throws IOException if it could not skip requested number of bytes \n   * for any reason (including EOF)",
  "org.apache.hadoop.security.token.SecretManager:generateSecret()" : "* Generate a new random secret key.\n   * @return the new key",
  "org.apache.hadoop.fs.FsUrlStreamHandler:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler:getConnectionId()" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping:<init>(java.util.Map,java.util.Map)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)" : null,
  "org.apache.hadoop.fs.Path:getParentUtil()" : null,
  "org.apache.hadoop.security.http.XFrameOptionsFilter:init(javax.servlet.FilterConfig)" : null,
  "org.apache.hadoop.fs.sftp.SFTPConnectionPool:connect(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getAllStoragePolicies()" : null,
  "org.apache.hadoop.security.alias.CredentialShell:main(java.lang.String[])" : "* Main program.\n   *\n   * @param args\n   *          Command line arguments\n   * @throws Exception exception.",
  "org.apache.hadoop.security.http.RestCsrfPreventionFilter$ServletFilterHttpInteraction:getHeader(java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Constructor.\n     * @param fileSystem owner\n     * @param p path to create",
  "org.apache.hadoop.fs.viewfs.NflyFSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.util.ExitUtil$HaltException:<init>(int,java.lang.Throwable)" : null,
  "org.apache.hadoop.fs.FileSystem:appendFile(org.apache.hadoop.fs.Path)" : "* Create a Builder to append a file.\n   * @param path file path.\n   * @return a {@link FSDataOutputStreamBuilder} to build file append request.",
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:gauge(org.apache.hadoop.metrics2.MetricsInfo,float)" : null,
  "org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.io.WritableUtils:cloneInto(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)" : "* Make a copy of the writable object using serialization to a buffer.\n   * @param dst the object to copy from\n   * @param src the object to copy into, which is destroyed\n   * @throws IOException raised on errors performing I/O.\n   * @deprecated use ReflectionUtils.cloneInto instead.",
  "org.apache.hadoop.ipc.RpcServerException:<init>(java.lang.String)" : "* Constructs exception with the specified detail message.\n   * @param message detailed message.",
  "org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String,boolean)" : "* Create an InetSocketAddress from the given target string and\n   * default port. If the string cannot be parsed correctly, the\n   * <code>configName</code> parameter is used as part of the\n   * exception message, allowing the user to better diagnose\n   * the misconfiguration.\n   *\n   * @param target a string of either \"host\" or \"host:port\"\n   * @param defaultPort the default port if <code>target</code> does not\n   *                    include a port number\n   * @param configName the name of the configuration from which\n   *                   <code>target</code> was loaded. This is used in the\n   *                   exception message in the case that parsing fails.\n   * @param useCacheIfPresent Whether use cache when create URI\n   * @return  socket addr",
  "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory:<init>(org.apache.hadoop.fs.statistics.DurationTrackerFactory,org.apache.hadoop.fs.statistics.DurationTrackerFactory)" : null,
  "org.apache.hadoop.fs.PathIOException:formatPath(java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : "* Get an xattr name and value for a file or directory.\n   * The name must be prefixed with the namespace followed by \".\". For example,\n   * \"user.attr\".\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to get extended attribute\n   * @param name xattr name.\n   * @return byte[] xattr value.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.fs.viewfs.ViewFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* This constructor has the signature needed by\n   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.\n   *\n   * @param theUri which must be that of ViewFs\n   * @param conf\n   * @throws IOException\n   * @throws URISyntaxException",
  "org.apache.hadoop.util.dynamic.DynConstructors:methodName(java.lang.Class,java.lang.Class[])" : null,
  "org.apache.hadoop.fs.permission.ScopedAclEntries:getDefaultEntries()" : "* Returns default entries.\n   *\n   * @return List&lt;AclEntry&gt; containing just default entries, or an empty\n   * list if there are no default entries",
  "org.apache.hadoop.util.dynamic.DynConstructors:formatProblems(java.util.Map)" : null,
  "org.apache.hadoop.fs.impl.FlagSet:isImmutable()" : "* Is the FlagSet immutable?\n   * @return true iff the FlagSet is immutable.",
  "org.apache.hadoop.util.JsonSerialization:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Load from a Hadoop filesystem.\n   * @param fs filesystem\n   * @param path path\n   * @return a loaded object\n   * @throws PathIOException JSON parse problem\n   * @throws IOException IO problems",
  "org.apache.hadoop.util.ToolRunner:confirmPrompt(java.lang.String)" : "* Print out a prompt to the user, and return true if the user\n   * responds with \"y\" or \"yes\". (case insensitive).\n   *\n   * @param prompt prompt.\n   * @throws IOException raised on errors performing I/O.\n   * @return if the user\n   *         responds with \"y\" or \"yes\". (case insensitive) true,\n   *         not false.",
  "org.apache.hadoop.util.GSetByHashMap:put(java.lang.Object)" : null,
  "org.apache.hadoop.fs.DF:<init>(java.io.File,long)" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_aggregate(java.lang.Object)" : "* Aggregate into the IOStatistics context the statistics passed in via\n   * IOStatistics/source parameter.\n   * <p>\n   * Returns false if the source is null or does not contain any statistics.\n   * @param source implementation of {@link IOStatisticsSource} or {@link IOStatistics}\n   * @return true if the the source object was aggregated.",
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])" : "* Encode with inputs and generates outputs. More see above.\n   *\n   * @param inputs input buffers to read data from\n   * @param outputs output buffers to put the encoded data into, read to read\n   *                after the call\n   * @throws IOException if the encoder is closed.",
  "org.apache.hadoop.fs.FilterFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.fs.impl.FileRangeImpl:<init>(long,int,java.lang.Object)" : "* Create.\n   * @param offset offset in file\n   * @param length length of data to read.\n   * @param reference nullable reference to store in the range.",
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:getAlgorithm()" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetOutputBuffers(byte[][],int[],int)" : "* Initialize the output buffers with ZERO bytes.",
  "org.apache.hadoop.crypto.CryptoOutputStream:write(byte[],int,int)" : "* Encryption is buffer based.\n   * If there is enough room in {@link #inBuffer}, then write to this buffer.\n   * If {@link #inBuffer} is full, then do encryption and write data to the\n   * underlying stream.\n   * @param b the data.\n   * @param off the start offset in the data.\n   * @param len the number of bytes to write.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.RawLocalFileSystem:deprecatedGetFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.shell.Ls:isHumanReadable()" : "* Should file sizes be returned in human readable format rather than bytes?\n   * @return true is human readable, false if bytes",
  "org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String)" : "* Returns the default (first) host name associated by the default\n   * nameserver with the address bound to the specified network interface\n   * \n   * @param strInterface\n   *            The name of the network interface to query (e.g. eth0).\n   *            Must not be null.\n   * @return The default host name associated with IPs bound to the network\n   *         interface\n   * @throws UnknownHostException\n   *             If one is encountered while querying the default interface",
  "org.apache.hadoop.fs.HarFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* not implemented.",
  "org.apache.hadoop.ipc.ResponseBuffer:ensureCapacity(int)" : null,
  "org.apache.hadoop.io.retry.AsyncCallHandler:initAsyncCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue)" : null,
  "org.apache.hadoop.security.authorize.AuthorizationException:<init>()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:truncate(org.apache.hadoop.fs.Path,long)" : null,
  "org.apache.hadoop.util.concurrent.AsyncGet$Util:wait(java.lang.Object,long,java.util.concurrent.TimeUnit)" : "* Use {@link #get(long, TimeUnit)} timeout parameters to wait.\n     * @param obj object.\n     * @param timeout timeout.\n     * @param unit unit.\n     * @throws InterruptedException if the thread is interrupted.",
  "org.apache.hadoop.fs.FileSystem$Cache:getInternal(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Cache$Key)" : "* Get the FS instance if the key maps to an instance, creating and\n     * initializing the FS if it is not found.\n     * If this is the first entry in the map and the JVM is not shutting down,\n     * this registers a shutdown hook to close filesystems, and adds this\n     * FS to the {@code toAutoClose} set if {@code \"fs.automatic.close\"}\n     * is set in the configuration (default: true).\n     * @param uri filesystem URI\n     * @param conf configuration\n     * @param key key to store/retrieve this FileSystem in the cache\n     * @return a cached or newly instantiated FileSystem.\n     * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.fs.shell.CommandFormat:getOptValue(java.lang.String)" : "* get the option's value\n   *\n   * @param option option name\n   * @return option value\n   * if option exists, but no value assigned, return \"\"\n   * if option not exists, return null",
  "org.apache.hadoop.crypto.key.KeyProvider$Options:setDescription(java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:reencryptEncryptedKeys(java.util.List)" : null,
  "org.apache.hadoop.security.UserGroupInformation:isLoginKeytabBased()" : "* Did the login happen via keytab.\n   * @return true or false\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[])" : "* Move the cursor to the first entry whose key is greater than or equal\n       * to the input key. Synonymous to seekTo(key, 0, key.length). The entry\n       * returned by the previous entry() call will be invalid.\n       * \n       * @param key\n       *          The input key\n       * @return true if we find an equal key.\n       * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:getInternalDirFs()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:toString()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfMul(byte,byte)" : null,
  "org.apache.hadoop.util.ShutdownHookManager:executeShutdown()" : "* Execute the shutdown.\n   * This is exposed purely for testing: do not invoke it.\n   * @return the number of shutdown hooks which timed out.",
  "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:<init>(java.io.DataOutputStream,int)" : "* Constructor.\n     * \n     * @param out\n     *          the underlying output stream.\n     * @param size\n     *          The total # of bytes to be written as a single chunk.\n     * @throws java.io.IOException\n     *           if an I/O error occurs.",
  "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:logout()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getIntList(java.lang.Iterable)" : null,
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:addFunction(java.lang.String,java.util.function.Function)" : "* add a mapping of a key to a function.\n   * @param key the key\n   * @param eval the evaluator",
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:createMaps()" : "* Create the maps.",
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configure(java.lang.String)" : null,
  "org.apache.hadoop.conf.StorageUnit$5:toPBs(double)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)" : null,
  "org.apache.hadoop.net.InnerNodeImpl:getNumOfLeaves()" : null,
  "org.apache.hadoop.fs.HarFileSystem:getUri()" : "* Returns the uri of this filesystem.\n   * The uri is of the form \n   * har://underlyingfsschema-host:port/pathintheunderlyingfs",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:memSync(org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion)" : null,
  "org.apache.hadoop.security.token.DtFileOperations:stripPrefix(java.lang.String)" : "Let the DtFetcher code add the appropriate prefix if HTTP/S is used.",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:loaded()" : "* Is the wrapped IO class loaded?\n   * @return true if the wrappedIO class was found and loaded.",
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:getBlockNumber()" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping:checkSupportedPlatform()" : null,
  "org.apache.hadoop.util.LightWeightGSet:getIndex(java.lang.Object)" : null,
  "org.apache.hadoop.fs.LocatedFileStatus:compareTo(org.apache.hadoop.fs.FileStatus)" : "* Compare this FileStatus to another FileStatus\n   * @param   o the FileStatus to be compared.\n   * @return  a negative integer, zero, or a positive integer as this object\n   *   is less than, equal to, or greater than the specified object.",
  "org.apache.hadoop.ipc.WritableRpcEngine:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)" : null,
  "org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.MapFile$Reader:createDataFileReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])" : "* Override this method to specialize the type of\n     * {@link SequenceFile.Reader} returned.\n     *\n     * @param dataFile data file.\n     * @param conf configuration.\n     * @param options options.\n     * @throws IOException raised on errors performing I/O.\n     * @return SequenceFile.Reader.",
  "org.apache.hadoop.ha.ZKFailoverController:gracefulFailoverToYou()" : "* Coordinate a graceful failover to this node.\n   * @throws ServiceFailedException if the node fails to become active\n   * @throws IOException some other error occurs",
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:<init>(org.apache.hadoop.crypto.key.KeyProvider$Metadata)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getBlockSize()" : null,
  "org.apache.hadoop.ipc.Server$Connection:decRpcCount()" : null,
  "org.apache.hadoop.security.IngressPortBasedResolver:getServerProperties(java.net.InetAddress,int)" : "* Identify the Sasl Properties to be used for a connection with a client.\n   * @param clientAddress client's address\n   * @param ingressPort the port that the client is connecting\n   * @return the sasl properties to be used for the connection.",
  "org.apache.hadoop.ipc.Server$Connection:switchToSimple()" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_create()" : "* Create a new {@code IOStatisticsSnapshot} instance.\n   * @return an empty IOStatisticsSnapshot.\n   * @throws UnsupportedOperationException if the IOStatistics classes were not found",
  "org.apache.hadoop.fs.store.DataBlocks$DataBlock:startUpload()" : "* Switch to the upload state and return a stream for uploading.\n     * Base class calls {@link #enterState(DestState, DestState)} to\n     * manage the state machine.\n     *\n     * @return the stream.\n     * @throws IOException trouble",
  "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getBlockData()" : "* @return The object holding blocks data info for the underlying file.",
  "org.apache.hadoop.fs.FSDataOutputStream:getPos()" : "* Get the current position in the output stream.\n   *\n   * @return the current position in the output stream",
  "org.apache.hadoop.util.LimitInputStream:read()" : null,
  "org.apache.hadoop.ipc.RPC$Server:call(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String,org.apache.hadoop.io.Writable,long)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,int,int[],java.nio.ByteBuffer[],java.nio.ByteBuffer[])" : null,
  "org.apache.hadoop.fs.FileContext:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : "* Set owner of a path (i.e. a file or a directory). The parameters username\n   * and groupname cannot both be null.\n   * \n   * @param f The path\n   * @param username If it is null, the original username remains unchanged.\n   * @param groupname If it is null, the original groupname remains unchanged.\n   * \n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If <code>f</code> does not exist\n   * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n   *           not supported\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server\n   * \n   * RuntimeExceptions:\n   * @throws HadoopIllegalArgumentException If <code>username</code> or\n   *           <code>groupname</code> is invalid.",
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:setInputFromSavedData()" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:posixFadviseIfPossible(java.lang.String,java.io.FileDescriptor,long,long,int)" : null,
  "org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:<init>(org.apache.hadoop.ipc.RPC$Server)" : null,
  "org.apache.hadoop.conf.StorageUnit$6:toGBs(double)" : null,
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:flush()" : null,
  "org.apache.hadoop.http.HtmlQuoting:quoteHtmlChars(java.io.OutputStream,byte[],int,int)" : "* Quote all of the active HTML characters in the given string as they\n   * are added to the buffer.\n   * @param output the stream to write the output to\n   * @param buffer the byte array to take the characters from\n   * @param off the index of the first byte to quote\n   * @param len the number of bytes to quote\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:selectiveClearing(org.apache.hadoop.util.bloom.Key,short)" : "* Performs the selective clearing for a given key.\n   * @param k The false positive key to remove from <i>this</i> retouched Bloom filter.\n   * @param scheme The selective clearing scheme to apply.",
  "org.apache.hadoop.io.MapWritable:remove(java.lang.Object)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getHomeDirectory()" : null,
  "org.apache.hadoop.fs.FileSystem:setVerifyChecksum(boolean)" : "* Set the verify checksum flag. This is only applicable if the\n   * corresponding filesystem supports checksums.\n   * By default doesn't do anything.\n   * @param verifyChecksum Verify checksum flag",
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:write(int)" : null,
  "org.apache.hadoop.util.DataChecksum$ChecksumNull:<init>()" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:reinit(org.apache.hadoop.conf.Configuration)" : "* Prepare the compressor to be used in a new stream with settings defined in\n   * the given Configuration. It will reset the compressor's compression level\n   * and compression strategy.\n   *\n   * @param conf Configuration storing new settings",
  "org.apache.hadoop.fs.statistics.IOStatisticsSupport:snapshotIOStatistics()" : "* Create a snapshot statistics instance ready to aggregate data.\n   *\n   * The instance can be serialized, and its\n   * {@code toString()} method lists all the values.\n   * @return an empty snapshot",
  "org.apache.hadoop.ipc.Server$Call:getCallerContext()" : null,
  "org.apache.hadoop.fs.HardLink:<init>()" : null,
  "org.apache.hadoop.util.Shell:getSignalKillCommand(int,java.lang.String)" : "* Return a command to send a signal to a given pid.\n   *\n   * @param code code.\n   * @param pid pid.\n   * @return signal kill command.",
  "org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpChunk(org.apache.hadoop.io.erasurecode.ECChunk)" : "* Print data in hex format in a chunk.\n   * @param chunk chunk.",
  "org.apache.hadoop.fs.AbstractFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])" : "* The specification of this method matches that of\n   * {@link FileContext#rename(Path, Path, Options.Rename...)} except that Path\n   * f must be for this file system.\n   *\n   * @param src src.\n   * @param dst dst.\n   * @param options options.\n   * @throws AccessControlException access control exception.\n   * @throws FileAlreadyExistsException file already exists exception.\n   * @throws FileNotFoundException file not found exception.\n   * @throws ParentNotDirectoryException parent not directory exception.\n   * @throws UnresolvedLinkException unresolved link exception.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.conf.ReconfigurationServlet:getParams(javax.servlet.http.HttpServletRequest)" : null,
  "org.apache.hadoop.io.file.tfile.Utils$Version:write(java.io.DataOutput)" : "* Write the objec to a DataOutput. The serialized format of the Version is\n     * major version followed by minor version, both as big-endian short\n     * integers.\n     * \n     * @param out\n     *          The DataOutput object.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.token.Token$PrivateToken:hashCode()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getBlockLocations()" : null,
  "org.apache.hadoop.security.authorize.Service:getProtocol()" : "* Get the protocol for the service\n   * @return the {@link Class} for the protocol",
  "org.apache.hadoop.security.authorize.ProxyUsers:getDefaultImpersonationProvider()" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:getIOStatistics()" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:close()" : "Close the file.",
  "org.apache.hadoop.util.ShutdownHookManager$1:<init>()" : null,
  "org.apache.hadoop.net.NetUtils:wrapWithMessage(java.io.IOException,java.lang.String)" : null,
  "org.apache.hadoop.io.BooleanWritable:readFields(java.io.DataInput)" : "",
  "org.apache.hadoop.fs.FileContext:listXAttrs(org.apache.hadoop.fs.Path)" : "* Get all of the xattr names for a file or directory.\n   * Only those xattr names which the logged-in user has permissions to view\n   * are returned.\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to get extended attributes\n   * @return List{@literal <}String{@literal >} of the XAttr names of the\n   * file or directory\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:permission(org.apache.hadoop.fs.permission.FsPermission)" : "* Set permission for the file.",
  "org.apache.hadoop.util.ExitUtil$ExitException:<init>(int,java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.net.InnerNodeImpl:createParentNode(java.lang.String)" : "* Creates a parent node to be added to the list of children.\n   * Creates a node using the InnerNode four argument constructor specifying\n   * the name, location, parent, and level of this node.\n   *\n   * <p>To be overridden in subclasses for specific InnerNode implementations,\n   * as alternative to overriding the full {@link #add(Node)} method.\n   *\n   * @param parentName The name of the parent node\n   * @return A new inner node\n   * @see InnerNodeImpl(String, String, InnerNode, int)",
  "org.apache.hadoop.util.DataChecksum:getChecksumSize(int)" : "* the required checksum size given the data length.\n   * @param dataSize data size.\n   * @return the required checksum size given the data length.",
  "org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults()" : null,
  "org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)" : "* Set an xattr of a file or directory.\n   * The name must be prefixed with the namespace followed by \".\". For example,\n   * \"user.attr\".\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to modify\n   * @param name xattr name.\n   * @param value xattr value.\n   * @param flag xattr set flag\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.crypto.CipherSuite:toString()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)" : "* For subclasses externalizing the storage, for example Zookeeper\n   * based implementations.\n   *\n   * @param key DelegationKey.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.jmx.JMXJsonServlet:writeAttribute(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO:stripDomain(java.lang.String)" : "* The Windows logon name has two part, NetBIOS domain name and\n   * user account name, of the format DOMAIN\\UserName. This method\n   * will remove the domain part of the full logon name.\n   *\n   * @param name the full principal name containing the domain\n   * @return name with domain removed\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.audit.CommonAuditContext:getGlobalContextEntries()" : "* Get an iterator over the global entries.\n   * Thread safe.\n   * @return an iterable to enumerate the values.",
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm:getName()" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:getVersion()" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.String,java.lang.String,java.lang.Class[])" : "* Checks for an implementation, first finding the given class by name.\n     * @param className name of a class\n     * @param methodName name of a method (different from constructor)\n     * @param argClasses argument classes for the method\n     * @return this Builder for method chaining",
  "org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)" : null,
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongMaximum(java.lang.String,java.util.concurrent.atomic.AtomicLong)" : "* Add a maximum statistic to dynamically return the\n   * latest value of the source.\n   * @param key key of this statistic\n   * @param source atomic long maximum\n   * @return the builder.",
  "org.apache.hadoop.fs.audit.CommonAuditContext:noteEntryPoint(java.lang.Object)" : "* Add the entry point as a context entry with the key\n   * {@link AuditConstants#PARAM_COMMAND}\n   * if it has not  already been recorded.\n   * This is called via ToolRunner but may be used at any\n   * other entry point.\n   * @param tool object loaded/being launched.",
  "org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.String)" : "* Create a domain name resolver to convert the domain name in the config to\n   * the actual IP addresses of the Namenode/Router/RM.\n   *\n   * @param conf Configuration to get the resolver from.\n   * @param uri the url that the resolver will be used against\n   * @param configKey The config key name suffixed with\n   *                  the nameservice/yarnservice.\n   * @return Domain name resolver.",
  "org.apache.hadoop.conf.ConfigRedactor:redactXml(java.lang.String,java.lang.String)" : "* Given a key / value pair, decides whether or not to redact and returns\n   * either the original value or text indicating it has been redacted.\n   *\n   * @param key param key.\n   * @param value param value, will return if conditions permit.\n   * @return Original value, or text indicating it has been redacted",
  "org.apache.hadoop.ipc.Server$Connection:setServiceClass(int)" : "* Set service class for connection\n     * @param serviceClass the serviceClass to set",
  "org.apache.hadoop.fs.shell.Mkdir:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:loadConf()" : "* Return the supplied configuration for testing or otherwise load a new\n   * configuration.\n   *\n   * @return the configuration to use",
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getUriDefaultPort()" : null,
  "org.apache.hadoop.fs.store.EtagChecksum:toString()" : null,
  "org.apache.hadoop.fs.permission.FsPermission:getErasureCodedBit()" : "* Returns true if the file or directory is erasure coded.\n   *\n   * @return if the file or directory is\n   * erasure coded true, not false.\n   * @deprecated Get ec bit from the {@link org.apache.hadoop.fs.FileStatus}\n   * object.",
  "org.apache.hadoop.security.UserGroupInformation:isSecurityEnabled()" : "* Determine if UserGroupInformation is using Kerberos to determine\n   * user identities or is relying on simple authentication\n   * \n   * @return true if UGI is working in a secure environment",
  "org.apache.hadoop.service.launcher.InterruptEscalator:interrupted(org.apache.hadoop.service.launcher.IrqHandler$InterruptData)" : null,
  "org.apache.hadoop.security.http.CrossOriginFilter:isMethodAllowed(java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.kms.ValueQueue:<init>(int,float,long,int,org.apache.hadoop.crypto.key.kms.ValueQueue$SyncGenerationPolicy,org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller)" : "* Constructor takes the following tunable configuration parameters\n   * @param numValues The number of values cached in the Queue for a\n   *    particular key.\n   * @param lowWatermark The ratio of (number of current entries/numValues)\n   *    below which the <code>fillQueueForKey()</code> funciton will be\n   *    invoked to fill the Queue.\n   * @param expiry Expiry time after which the Key and associated Queue are\n   *    evicted from the cache.\n   * @param numFillerThreads Number of threads to use for the filler thread\n   * @param policy The SyncGenerationPolicy to use when client\n   *    calls \"getAtMost\"\n   * @param refiller implementation of the QueueRefiller",
  "org.apache.hadoop.fs.FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)" : "* The src files is on the local disk.  Add it to filesystem at\n   * the given dst name, removing the source afterwards.\n   * @param srcs source paths\n   * @param dst path\n   * @throws IOException IO failure",
  "org.apache.hadoop.io.serializer.SerializationFactory:<init>(org.apache.hadoop.conf.Configuration)" : "* <p>\n   * Serializations are found by reading the <code>io.serializations</code>\n   * property from <code>conf</code>, which is a comma-delimited list of\n   * classnames.\n   * </p>\n   *\n   * @param conf configuration.",
  "org.apache.hadoop.metrics2.MetricsTag:hashCode()" : null,
  "org.apache.hadoop.fs.FilterFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.statistics.IOStatistics)" : "* Instantiate.\n   * @param name storage statistics name.\n   * @param scheme FS scheme; may be null.\n   * @param ioStatistics IOStatistics source.",
  "org.apache.hadoop.ipc.RpcClientUtil:getVersionSignatureMap(java.net.InetSocketAddress,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.util.FastNumberFormat:format(java.lang.StringBuilder,long,int)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getAclStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:getCodecName()" : null,
  "org.apache.hadoop.fs.FsShellPermissions$Chgrp:parseOwnerGroup(java.lang.String)" : null,
  "org.apache.hadoop.fs.ftp.FTPException:<init>(java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet:<init>()" : null,
  "org.apache.hadoop.security.token.SecretManager$InvalidToken:<init>(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.impl.MetricCounterLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)" : null,
  "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:getGroupsForUser(java.lang.String)" : null,
  "org.apache.hadoop.util.RateLimitingFactory$RestrictedRateLimiting:<init>(int)" : "* Constructor.\n     * @param capacityPerSecond capacity in permits/second.",
  "org.apache.hadoop.util.dynamic.BindingUtils:loadClass(java.lang.ClassLoader,java.lang.String)" : "* Load a class by name.\n   * @param cl classloader to use.\n   * @param className classname\n   * @return the class or null if it could not be loaded.",
  "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForMethod(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)" : null,
  "org.apache.hadoop.security.Groups:getBackgroundRefreshRunning()" : null,
  "org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings(java.util.List)" : null,
  "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory:trackDuration(java.lang.String,long)" : null,
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:createKeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider)" : "* Creates a <code>KeyProviderCryptoExtension</code> using a given\n   * {@link KeyProvider}.\n   * <p>\n   * If the given <code>KeyProvider</code> implements the\n   * {@link CryptoExtension} interface the <code>KeyProvider</code> itself\n   * will provide the extension functionality.\n   * If the given <code>KeyProvider</code> implements the\n   * {@link KeyProviderExtension} interface and the KeyProvider being\n   * extended by the <code>KeyProvider</code> implements the\n   * {@link CryptoExtension} interface, the KeyProvider being extended will\n   * provide the extension functionality. Otherwise, a default extension\n   * implementation will be used.\n   *\n   * @param keyProvider <code>KeyProvider</code> to use to create the\n   * <code>KeyProviderCryptoExtension</code> extension.\n   * @return a <code>KeyProviderCryptoExtension</code> instance using the\n   * given <code>KeyProvider</code>.",
  "org.apache.hadoop.fs.FileSystem:createDataOutputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Create instance of the standard FSDataOutputStreamBuilder for the\n   * given filesystem and path.\n   * @param fileSystem owner\n   * @param path path to create\n   * @return a builder.",
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:updatePos(boolean)" : null,
  "org.apache.hadoop.fs.shell.CommandFormat$UnknownOptionException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.ReadaheadPool:readaheadStream(java.lang.String,java.io.FileDescriptor,long,long,long,org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest)" : "* Issue a request to readahead on the given file descriptor.\n   * \n   * @param identifier a textual identifier that will be used in error\n   * messages (e.g. the file name)\n   * @param fd the file descriptor to read ahead\n   * @param curPos the current offset at which reads are being issued\n   * @param readaheadLength the configured length to read ahead\n   * @param maxOffsetToRead the maximum offset that will be readahead\n   *        (useful if, for example, only some segment of the file is\n   *        requested by the user). Pass {@link Long#MAX_VALUE} to allow\n   *        readahead to the end of the file.\n   * @param lastReadahead the result returned by the previous invocation\n   *        of this function on this file descriptor, or null if this is\n   *        the first call\n   * @return an object representing this outstanding request, or null\n   *        if no readahead was performed",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValueLength()" : "* Get the length of the value. isValueLengthKnown() must be tested\n         * true.\n         * \n         * @return the length of the value.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.metrics2.util.Servers:<init>()" : "* This class is not intended to be instantiated",
  "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:add(java.lang.String,long)" : "* Add a rate sample for a rate metric.\n   * @param name of the rate metric\n   * @param elapsed time",
  "org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])" : null,
  "org.apache.hadoop.fs.VectoredReadUtils:roundDown(long,int)" : "* Calculates floor value of offset based on chunk size.\n   * @param offset file offset.\n   * @param chunkSize file chunk size.\n   * @return  floor value.",
  "org.apache.hadoop.tools.TableListing$Builder:<init>()" : "* Create a new Builder.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:checkKey()" : "* check whether we have already successfully obtained the key. It also\n       * initializes the valueInputStream.",
  "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isFile()" : null,
  "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheUpdated()" : null,
  "org.apache.hadoop.crypto.CryptoStreamUtils:checkBufferSize(org.apache.hadoop.crypto.CryptoCodec,int)" : "* Check and floor buffer size.\n   *\n   * @param codec crypto codec.\n   * @param bufferSize the size of the buffer to be used.\n   * @return calc buffer size.",
  "org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)" : "* The constructor with all the necessary info.\n   * @param inputBlocks inputBlocks.\n   * @param outputBlocks outputBlocks.\n   * @param rawEncoder rawEncoder.",
  "org.apache.hadoop.conf.Configuration:getStreamReader(org.apache.hadoop.conf.Configuration$Resource,boolean)" : null,
  "org.apache.hadoop.util.hash.Hash:hash(byte[],int)" : "* Calculate a hash using all bytes from the input argument,\n   * and a provided seed value.\n   * @param bytes input bytes\n   * @param initval seed value\n   * @return hash value",
  "org.apache.hadoop.ha.ZKFailoverController:startRPC()" : null,
  "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:toString()" : null,
  "org.apache.hadoop.io.ObjectWritable:readObject(java.io.DataInput,org.apache.hadoop.io.ObjectWritable,org.apache.hadoop.conf.Configuration)" : "* Read a {@link Writable}, {@link String}, primitive type, or an array of\n   * the preceding.\n   *\n   * @param in DataInput.\n   * @param objectWritable objectWritable.\n   * @param conf configuration.\n   * @return Object.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry:toString()" : null,
  "org.apache.hadoop.fs.FileContext:<init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.UserGroupInformation$RealUser:<init>(org.apache.hadoop.security.UserGroupInformation)" : null,
  "org.apache.hadoop.util.CrcComposer:newCrcComposer(org.apache.hadoop.util.DataChecksum$Type,long)" : "* Returns a CrcComposer which will collapse all ingested CRCs into a single\n   * value.\n   *\n   * @param type type.\n   * @param bytesPerCrcHint bytesPerCrcHint.\n   * @throws IOException raised on errors performing I/O.\n   * @return a CrcComposer which will collapse all ingested CRCs into a single value.",
  "org.apache.hadoop.util.Options$ClassOption:<init>(java.lang.Class)" : null,
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(int)" : null,
  "org.apache.hadoop.fs.Options$CreateOpts$BytesPerChecksum:<init>(short)" : null,
  "org.apache.hadoop.net.StandardSocketFactory:createSocket(java.lang.String,int)" : null,
  "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:builder()" : "* Get a builder.\n   * @return a new builder.",
  "org.apache.hadoop.ipc.RetriableException:<init>(java.lang.Exception)" : null,
  "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:confChanged(org.apache.hadoop.conf.Configuration)" : "This method gets called everytime before any read/write to make sure\n     * that any change to localDirs is reflected immediately.",
  "org.apache.hadoop.io.VLongWritable:<init>(long)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMinimumSample(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getFileLength()" : "* Calculate length of file if not already cached.\n     * @return file length.\n     * @throws IOException any IOE.",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.security.alias.LocalKeyStoreProvider:initFileSystem(java.net.URI)" : null,
  "org.apache.hadoop.fs.FsShell:newShellInstance()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.util.Time:monotonicNowNanos()" : "* Same as {@link #monotonicNow()} but returns its result in nanoseconds.\n   * Note that this is subject to the same resolution constraints as\n   * {@link System#nanoTime()}.\n   * @return a monotonic clock that counts in nanoseconds.",
  "org.apache.hadoop.net.NetUtils:getFreeSocketPort()" : "* Return a free port number. There is no guarantee it will remain free, so\n   * it should be used immediately.\n   *\n   * @return A free port for binding a local socket",
  "org.apache.hadoop.fs.Options$CreateOpts:<init>()" : null,
  "org.apache.hadoop.metrics2.lib.MutableGauge:info()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:setThreadIOStatisticsContext(org.apache.hadoop.fs.statistics.IOStatisticsContext)" : "* Set the IOStatisticsContext for the current thread.\n   * @param statisticsContext IOStatistics context instance for the\n   * current thread. If null, the context is reset.",
  "org.apache.hadoop.ipc.RPC$Builder:build()" : "* @return Build the RPC Server.\n     * @throws IOException on error\n     * @throws HadoopIllegalArgumentException when mandatory fields are not set",
  "org.apache.hadoop.ipc.Server$Listener:doRead(java.nio.channels.SelectionKey)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem:getUri()" : null,
  "org.apache.hadoop.io.WritableComparator:hashBytes(byte[],int,int)" : "* Compute hash for binary data.\n   * @param bytes bytes.\n   * @param offset offset.\n   * @param length length.\n   * @return hash for binary data.",
  "org.apache.hadoop.tools.CommandShell:run(java.lang.String[])" : null,
  "org.apache.hadoop.util.SysInfoLinux:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,long)" : "* Constructor which allows assigning the /proc/ directories. This will be\n   * used only in unit tests.\n   * @param procfsMemFile fake file for /proc/meminfo\n   * @param procfsCpuFile fake file for /proc/cpuinfo\n   * @param procfsStatFile fake file for /proc/stat\n   * @param procfsNetFile fake file for /proc/net/dev\n   * @param procfsDisksFile fake file for /proc/diskstats\n   * @param jiffyLengthInMillis fake jiffy length value",
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:isLastBlock()" : "* Determines whether the current block is the last block in this file.\n   *\n   * @return true if the current block is the last block in this file, false otherwise.",
  "org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char[])" : "* unEscapeString.\n   * @param str str.\n   * @param escapeChar escapeChar.\n   * @param charsToEscape array of characters to unescape\n   * @return escape string.",
  "org.apache.hadoop.security.token.Token:setPassword(byte[])" : null,
  "org.apache.hadoop.metrics2.lib.MutableGaugeLong:value()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.Client$Connection:run()" : null,
  "org.apache.hadoop.ha.HAServiceStatus:setReadyToBecomeActive()" : null,
  "org.apache.hadoop.conf.StorageUnit$2:toTBs(double)" : null,
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,java.lang.String[],java.lang.String[])" : null,
  "org.apache.hadoop.ipc.Server:setupResponseForProtobuf(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)" : null,
  "org.apache.hadoop.fs.ChecksumFs:getChecksumFile(org.apache.hadoop.fs.Path)" : "* Return the name of the checksum file associated with a file.\n   *\n   * @param file the file path.\n   * @return the checksum file associated with a file.",
  "org.apache.hadoop.fs.store.EtagChecksum:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.util.HttpExceptionUtils:createServletExceptionResponse(javax.servlet.http.HttpServletResponse,int,java.lang.Throwable)" : "* Creates a HTTP servlet response serializing the exception in it as JSON.\n   *\n   * @param response the servlet response\n   * @param status the error code to set in the response\n   * @param ex the exception to serialize in the response\n   * @throws IOException thrown if there was an error while creating the\n   * response",
  "org.apache.hadoop.util.bloom.DynamicBloomFilter:getActiveStandardBF()" : "* Returns the active standard Bloom filter in <i>this</i> dynamic Bloom filter.\n   * @return BloomFilter The active standard Bloom filter.\n   * \t\t\t <code>Null</code> otherwise.",
  "org.apache.hadoop.fs.AbstractFileSystem:createMultipartUploader(org.apache.hadoop.fs.Path)" : "* Create a multipart uploader.\n   * @param basePath file path under which all files are uploaded\n   * @return a MultipartUploaderBuilder object to build the uploader\n   * @throws IOException if some early checks cause IO failures.\n   * @throws UnsupportedOperationException if support is checked early.",
  "org.apache.hadoop.fs.Globber:authorityFromPath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[],int,int)" : "* Move the cursor to the first entry whose key is greater than or equal\n       * to the input key. The entry returned by the previous entry() call will\n       * be invalid.\n       * \n       * @param key\n       *          The input key\n       * @param keyOffset\n       *          offset in the key buffer.\n       * @param keyLen\n       *          key buffer length.\n       * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:tooManyConnectionFailures()" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyCompressor:setDictionary(byte[],int,int)" : "* Does nothing.",
  "org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:toString()" : null,
  "org.apache.hadoop.security.token.delegation.DelegationKey:<init>()" : "Default constructore required for Writable",
  "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:getIOStatistics()" : "* Ask the inner stream for their IOStatistics.\n   * @return any IOStatistics offered by the inner stream.",
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerMaximum(java.lang.String,java.util.concurrent.atomic.AtomicInteger)" : "* Add a maximum statistic to dynamically return the\n   * latest value of the source.\n   * @param key key of this statistic\n   * @param source atomic int maximum\n   * @return the builder.",
  "org.apache.hadoop.fs.viewfs.NflyFSystem:processThrowable(org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode,java.lang.String,java.lang.Throwable,java.util.List,org.apache.hadoop.fs.Path[])" : null,
  "org.apache.hadoop.security.SecurityUtil$TruststoreKeystore:<init>(org.apache.hadoop.conf.Configuration)" : "* Configuration for the ZooKeeper connection when SSL/TLS is enabled.\n     * When a value is not configured, ensure that empty string is set instead of null.\n     *\n     * @param conf ZooKeeper Client configuration",
  "org.apache.hadoop.fs.FileContext$Util:listFiles(org.apache.hadoop.fs.Path,boolean)" : "* List the statuses and block locations of the files in the given path.\n     * \n     * If the path is a directory, \n     *   if recursive is false, returns files in the directory;\n     *   if recursive is true, return files in the subtree rooted at the path.\n     *   The subtree is traversed in the depth-first order.\n     * If the path is a file, return the file's status and block locations.\n     * Files across symbolic links are also returned.\n     * \n     * @param f is the path\n     * @param recursive if the subdirectories need to be traversed recursively\n     *\n     * @return an iterator that traverses statuses of the files\n     * If any IO exception (for example a sub-directory gets deleted while\n     * listing is being executed), next() or hasNext() of the returned iterator\n     * may throw a RuntimeException with the IO exception as the cause.\n     *\n     * @throws AccessControlException If access is denied\n     * @throws FileNotFoundException If <code>f</code> does not exist\n     * @throws UnsupportedFileSystemException If file system for <code>f</code>\n     *         is not supported\n     * @throws IOException If an I/O error occurred\n     * \n     * Exceptions applicable to file systems accessed over RPC:\n     * @throws RpcClientException If an exception occurred in the RPC client\n     * @throws RpcServerException If an exception occurred in the RPC server\n     * @throws UnexpectedServerException If server implementation throws \n     *           undeclared exception to RPC server",
  "org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByExactName(java.lang.String)" : null,
  "org.apache.hadoop.fs.Options$CreateOpts$BlockSize:<init>(long)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:getNativeFileLinkStatus(org.apache.hadoop.fs.Path,boolean)" : "* Calls out to platform's native stat(1) implementation to get file metadata\n   * (permissions, user, group, atime, mtime, etc). This works around the lack\n   * of lstat(2) in Java 6.\n   * \n   *  Currently, the {@link Stat} class used to do this only supports Linux\n   *  and FreeBSD, so the old {@link #deprecatedGetFileLinkStatusInternal(Path)}\n   *  implementation (deprecated) remains further OS support is added.\n   *\n   * @param f File to stat\n   * @param dereference whether to dereference symlinks\n   * @return FileStatus of f\n   * @throws IOException",
  "org.apache.hadoop.fs.EmptyStorageStatistics:getLong(java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)" : null,
  "org.apache.hadoop.fs.FSInputChecker:resetState()" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:file(org.apache.hadoop.fs.Path)" : "* Create an option to specify the path name of the sequence file.\n     * @param value the path to read\n     * @return a new option",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingStdDev()" : "* Return Standard Deviation of the Processing Time.\n   * @return  double",
  "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:calculateIV(byte[],long,byte[])" : null,
  "org.apache.hadoop.io.file.tfile.TFile:makeComparator(java.lang.String)" : "* Make a raw comparator from a string name.\n   * \n   * @param name\n   *          Comparator name\n   * @return A RawComparable comparator.",
  "org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:<init>(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod,java.lang.Object)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:addArguments(java.util.Deque)" : null,
  "org.apache.hadoop.fs.FileSystem:getCanonicalUri()" : "* Return a canonicalized form of this FileSystem's URI.\n   *\n   * The default implementation simply calls {@link #canonicalizeUri(URI)}\n   * on the filesystem's own URI, so subclasses typically only need to\n   * implement that method.\n   *\n   * @see #canonicalizeUri(URI)\n   * @return the URI of this filesystem.",
  "org.apache.hadoop.metrics2.util.MetricsCache$Record:metricsEntrySet()" : "* @return entry set of metrics",
  "org.apache.hadoop.fs.FileUtil:createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Map)" : "* Create a jar file at the given path, containing a manifest with a classpath\n   * that references all specified entries.\n   *\n   * Some platforms may have an upper limit on command line length.  For example,\n   * the maximum command line length on Windows is 8191 characters, but the\n   * length of the classpath may exceed this.  To work around this limitation,\n   * use this method to create a small intermediate jar with a manifest that\n   * contains the full classpath.  It returns the absolute path to the new jar,\n   * which the caller may set as the classpath for a new process.\n   *\n   * Environment variable evaluation is not supported within a jar manifest, so\n   * this method expands environment variables before inserting classpath entries\n   * to the manifest.  The method parses environment variables according to\n   * platform-specific syntax (%VAR% on Windows, or $VAR otherwise).  On Windows,\n   * environment variables are case-insensitive.  For example, %VAR% and %var%\n   * evaluate to the same value.\n   *\n   * Specifying the classpath in a jar manifest does not support wildcards, so\n   * this method expands wildcards internally.  Any classpath entry that ends\n   * with * is translated to all files at that path with extension .jar or .JAR.\n   *\n   * @param inputClassPath String input classpath to bundle into the jar manifest\n   * @param pwd Path to working directory to save jar\n   * @param targetDir path to where the jar execution will have its working dir\n   * @param callerEnv Map {@literal <}String, String{@literal >} caller's\n   * environment variables to use for expansion\n   * @return String[] with absolute path to new jar in position 0 and\n   *   unexpanded wild card entry path in position 1\n   * @throws IOException if there is an I/O error while writing the jar file",
  "org.apache.hadoop.fs.impl.FsLinkResolution:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.FsLinkResolution$FsLinkResolutionFunction)" : "* Apply the given function to the resolved path under the the supplied\n   * FileContext.\n   * @param fileContext file context to resolve under\n   * @param path path to resolve\n   * @param fn function to invoke\n   * @param <T> return type.\n   * @return the return value of the function as revoked against the resolved\n   * path.\n   * @throws UnresolvedLinkException link resolution failure\n   * @throws IOException other IO failure.",
  "org.apache.hadoop.ipc.RefreshRegistry:unregister(java.lang.String,org.apache.hadoop.ipc.RefreshHandler)" : "* Remove the registered object for a given identity.\n   * @param identifier the resource to unregister\n   * @param handler input handler.\n   * @return the true if removed",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache$Key:hashCode()" : null,
  "org.apache.hadoop.crypto.key.kms.ValueQueue:getLock(java.lang.String)" : "* Get the stripped lock given a key name.\n   *\n   * @param keyName The key name.",
  "org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:drain(java.lang.String)" : null,
  "org.apache.hadoop.fs.store.EtagChecksum:<init>()" : "* Create with an empty etag.",
  "org.apache.hadoop.fs.AbstractFileSystem:open(org.apache.hadoop.fs.Path)" : "* The specification of this method matches that of\n   * {@link FileContext#open(Path)} except that Path f must be for this\n   * file system.\n   *\n   * @param f the path.\n   * @throws AccessControlException access control exception.\n   * @throws FileNotFoundException file not found exception.\n   * @throws UnresolvedLinkException unresolved link exception.\n   * @throws IOException raised on errors performing I/O.\n   * @return input stream.",
  "org.apache.hadoop.ipc.RpcScheduler:addResponseTime(java.lang.String,int,int,int)" : "* This method only exists to maintain backwards compatibility with old\n   * implementations. It will not be called by any Hadoop code, and should not\n   * be implemented by new implementations.\n   *\n   * @param name input name.\n   * @param priorityLevel input priorityLevel.\n   * @param queueTime input queueTime.\n   * @param processingTime input processingTime.\n   * @throws UnsupportedOperationException\n   *         the requested operation is not supported.\n   * @deprecated Use\n   * {@link #addResponseTime(String, Schedulable, ProcessingDetails)} instead.",
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:finished()" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:isChecksumFile(org.apache.hadoop.fs.Path)" : "* Return true if file is a checksum file name.\n   *\n   * @param file the file path.\n   * @return if file is a checksum file true, not false.",
  "org.apache.hadoop.security.FastSaslServerFactory:getMechanismNames(java.util.Map)" : null,
  "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:read(long,byte[],int,int)" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsFile(java.nio.file.Path,java.lang.String)" : "* Validates that the given path exists and is a file.\n   * @param path the path to check.\n   * @param argName the name of the argument being validated.",
  "org.apache.hadoop.ipc.CallQueueManager:getDefaultQueueCapacityWeights(int)" : "* By default, queue capacity is the same for all priority levels.\n   *\n   * @param priorityLevels number of levels\n   * @return default weights",
  "org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:getCoderName()" : null,
  "org.apache.hadoop.net.NetworkTopology:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.net.InnerNode$Factory)" : null,
  "org.apache.hadoop.net.InnerNodeImpl:getLoc(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.TouchCommands$Touch:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.net.NetUtils:getHostname()" : "* Return hostname without throwing exception.\n   * The returned hostname String format is \"hostname/ip address\".\n   * @return hostname",
  "org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.Lock,long,long,org.apache.hadoop.util.Timer)" : null,
  "org.apache.hadoop.util.ConfTest:parseConf(java.io.InputStream)" : null,
  "org.apache.hadoop.fs.BBUploadHandle:hashCode()" : null,
  "org.apache.hadoop.tools.GetGroupsBase:<init>(org.apache.hadoop.conf.Configuration)" : "* Create an instance of this tool using the given configuration.\n   * @param conf configuration.",
  "org.apache.hadoop.fs.impl.prefetch.BufferData:<init>(int,java.nio.ByteBuffer)" : "* Constructs an instances of this class.\n   *\n   * @param blockNumber Number of the block associated with this buffer.\n   * @param buffer The buffer associated with this block.\n   *\n   * @throws IllegalArgumentException if blockNumber is negative.\n   * @throws IllegalArgumentException if buffer is null.",
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferData:getBuffer()" : "* Gets the buffer associated with this block.\n   *\n   * @return the buffer associated with this block.",
  "org.apache.hadoop.ha.HAAdmin:checkManualStateManagementOK(org.apache.hadoop.ha.HAServiceTarget)" : "* Ensure that we are allowed to manually manage the HA state of the target\n   * service. If automatic failover is configured, then the automatic\n   * failover controllers should be doing state management, and it is generally\n   * an error to use the HAAdmin command line to do so.\n   * \n   * @param target the target to check\n   * @return true if manual state management is allowed",
  "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:flush(int,int,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,boolean)" : null,
  "org.apache.hadoop.security.SecurityUtil:isOriginalTGT(javax.security.auth.kerberos.KerberosTicket)" : "* Check whether the server principal is the TGS's principal\n   * @param ticket the original TGT (the ticket that is obtained when a \n   * kinit is done)\n   * @return true or false",
  "org.apache.hadoop.util.Progress:<init>()" : "Creates a new root node.",
  "org.apache.hadoop.security.UserGroupInformation:forceReloginFromKeytab()" : "* Force re-Login a user in from a keytab file irrespective of the last login\n   * time. Loads a user identity from a keytab file and logs them in. They\n   * become the currently logged-in user. This method assumes that\n   * {@link #loginUserFromKeytab(String, String)} had happened already. The\n   * Subject field of this UserGroupInformation object is updated to have the\n   * new credentials.\n   *\n   * @throws IOException raised on errors performing I/O.\n   * @throws KerberosAuthException on a failure",
  "org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.conf.StorageSize:<init>(org.apache.hadoop.conf.StorageUnit,double)" : "* Constucts a Storage Measure, which contains the value and the unit of\n   * measure.\n   *\n   * @param unit - Unit of Measure\n   * @param value - Numeric value.",
  "org.apache.hadoop.util.functional.CloseableTaskPoolSubmitter:submit(java.lang.Runnable)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter:sort(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* The backwards compatible interface to sort.\n     * @param inFile the input file to sort.\n     * @param outFile the sorted output file.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeDirLink:getLink()" : null,
  "org.apache.hadoop.fs.QuotaUsage:getSpaceQuota()" : "* Return (disk) space quota.\n   *\n   * @return space quota.",
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:skip(long)" : null,
  "org.apache.hadoop.fs.ClusterStorageCapacityExceededException:<init>(java.lang.Throwable)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:unmapBlock(long,long)" : null,
  "org.apache.hadoop.security.LdapGroupsMapping:getGroupsSet(java.lang.String)" : null,
  "org.apache.hadoop.security.Groups:getUserToGroupsMappingService(org.apache.hadoop.conf.Configuration)" : "* Get the groups being used to map user-to-groups.\n   * @param conf configuration.\n   * @return the groups being used to map user-to-groups.",
  "org.apache.hadoop.net.TableMapping$RawTableMapping:load()" : null,
  "org.apache.hadoop.conf.ReconfigurationTaskStatus:<init>(long,long,java.util.Map)" : null,
  "org.apache.hadoop.fs.FileContext:getStatistics(java.net.URI)" : "* Get the statistics for a particular file system\n   * \n   * @param uri\n   *          the uri to lookup the statistics. Only scheme and authority part\n   *          of the uri are used as the key to store and lookup.\n   * @return a statistics object",
  "org.apache.hadoop.io.erasurecode.ECChunk:toBytesArray()" : "* Convert to a bytes array, just for test usage.\n   * @return bytes array",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,float)" : "* Set optional float parameter for the Builder.\n   *\n   * @see #opt(String, String)",
  "org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand:getUsage()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:useQueryStringForDelegationToken()" : "* Returns if delegation token is transmitted as a HTTP header.\n   *\n   * @return <code>TRUE</code> if the token is transmitted in the URL query\n   * string, <code>FALSE</code> if the delegation token is transmitted using the\n   * {@link DelegationTokenAuthenticator#DELEGATION_TOKEN_HEADER} HTTP header.",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.file.tfile.Utils:readVInt(java.io.DataInput)" : "* Decoding the variable-length integer. Synonymous to\n   * <code>(int)Utils#readVLong(in)</code>.\n   * \n   * @param in\n   *          input stream\n   * @return the decoded integer\n   * @throws IOException raised on errors performing I/O.\n   * \n   * @see Utils#readVLong(DataInput)",
  "org.apache.hadoop.io.file.tfile.BCFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.conf.Configuration)" : "* Constructor\n     * \n     * @param fin\n     *          FS input stream.\n     * @param fileLength\n     *          Length of the corresponding file\n     * @throws IOException",
  "org.apache.hadoop.fs.FileSystem$Statistics:getRemoteReadTime()" : "* Get total time taken in ms for bytes read from remote.\n     * @return time taken in ms for remote bytes read.",
  "org.apache.hadoop.io.MapFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType)" : null,
  "org.apache.hadoop.ipc.Client:getCallId()" : null,
  "org.apache.hadoop.util.FileBasedIPList:readLines(java.lang.String)" : "* Reads the lines in a file.\n   * @param fileName\n   * @return lines in a String array; null if the file does not exist or if the\n   * file name is null\n   * @throws IOException",
  "org.apache.hadoop.fs.Stat:isAvailable()" : "* Whether Stat is supported on the current platform.\n   * @return if is available true, not false.",
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:rejectUnknownMandatoryKeys(java.util.Collection,java.lang.String)" : "* Reject a configuration if one or more mandatory keys are\n   * not in the set of mandatory keys.\n   * The first invalid key raises the exception; the order of the\n   * scan and hence the specific key raising the exception is undefined.\n   * @param knownKeys a possibly empty collection of known keys\n   * @param extraErrorText extra error text to include.\n   * @throws IllegalArgumentException if any key is unknown.",
  "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:aggregateLocalStatesToGlobalMetrics(java.util.concurrent.ConcurrentMap)" : "* Aggregates the thread's local samples into the global metrics. The caller\n   * should ensure its thread safety.",
  "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:<init>(org.apache.hadoop.ipc.RetryCache)" : null,
  "org.apache.hadoop.tracing.TraceScope:<init>(org.apache.hadoop.tracing.Span)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults()" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:setInput(byte[],int,int)" : null,
  "org.apache.hadoop.security.UserGroupInformation:setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod)" : "* Sets the authentication method in the subject\n   * \n   * @param authMethod authMethod.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:verifyToken(org.apache.hadoop.security.token.Token)" : null,
  "org.apache.hadoop.fs.FileSystem:setWriteChecksum(boolean)" : "* Set the write checksum flag. This is only applicable if the\n   * corresponding filesystem supports checksums.\n   * By default doesn't do anything.\n   * @param writeChecksum Write checksum flag",
  "org.apache.hadoop.util.DataChecksum:toString()" : null,
  "org.apache.hadoop.net.SocketIOWithTimeout:connect(java.nio.channels.SocketChannel,java.net.SocketAddress,int)" : "* The contract is similar to {@link SocketChannel#connect(SocketAddress)} \n   * with a timeout.\n   * \n   * @see SocketChannel#connect(SocketAddress)\n   * \n   * @param channel - this should be a {@link SelectableChannel}\n   * @param endpoint\n   * @throws IOException",
  "org.apache.hadoop.conf.Configuration:getEnumSet(java.lang.String,java.lang.Class,boolean)" : "* Build an enumset from a comma separated list of values.\n   * Case independent.\n   * Special handling of \"*\" meaning: all values.\n   * @param key key to look for\n   * @param enumClass class of enum\n   * @param ignoreUnknown should unknown values raise an exception?\n   * @return a mutable set of the identified enum values declared in the configuration\n   * @param <E> enumeration type\n   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,\n   *           or there are two entries in the enum which differ only by case.",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getActualUgi()" : null,
  "org.apache.hadoop.util.SignalLogger$Handler:<init>(java.lang.String,org.slf4j.Logger)" : null,
  "org.apache.hadoop.io.serializer.avro.AvroSerialization:getDeserializer(java.lang.Class)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.http.HttpFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:getBytesPerSum()" : "* Return the bytes Per Checksum.\n   *\n   * @return bytes per check sum.",
  "org.apache.hadoop.fs.audit.CommonAuditContext:put(java.lang.String,java.util.function.Supplier)" : "* Put a context entry dynamically evaluated on demand.\n   * Important: as these supplier methods are long-lived,\n   * the supplier function <i>MUST NOT</i> be part of/refer to\n   * any object instance of significant memory size.\n   * Applications SHOULD remove references when they are\n   * no longer needed.\n   * When logged at TRACE, prints the key and stack trace of the caller,\n   * to allow for debugging of any problems.\n   * @param key key\n   * @param value new value\n   * @return old value or null",
  "org.apache.hadoop.security.token.Token:buildCacheKey()" : null,
  "org.apache.hadoop.util.Lists:newArrayList(java.util.Iterator)" : "* Creates a <i>mutable</i> {@code ArrayList} instance containing the\n   * given elements; a very thin shortcut for creating an empty list\n   * and then calling Iterators#addAll.\n   *\n   * @param <E> Generics Type E.\n   * @param elements elements.\n   * @return ArrayList Generics Type E.",
  "org.apache.hadoop.ipc.Server$Call:sendResponse()" : null,
  "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:waitAsyncValue(long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.util.ReflectionUtils:getFactory(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.ProtobufHelper:<init>()" : null,
  "org.apache.hadoop.util.MachineList:includes(java.net.InetAddress)" : "* Accepts an inet address and return true if address is in the list.\n   * @param address address.\n   * @return true if address is part of the list",
  "org.apache.hadoop.util.HostsFileReader:refresh()" : null,
  "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:<init>()" : null,
  "org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)" : "* Make a call, passing <code>rpcRequest</code>, to the IPC server defined by\n   * <code>remoteId</code>, returning the rpc response.\n   *\n   * @param rpcKind\n   * @param rpcRequest -  contains serialized method and method parameters\n   * @param remoteId - the target rpc server\n   * @param serviceClass - service class for RPC\n   * @param fallbackToSimpleAuth - set to true or false during this method to\n   *   indicate if a secure client falls back to simple auth\n   * @param alignmentContext - state alignment context\n   * @return the rpc response\n   * Throws exceptions if there are network problems or if the remote code\n   * threw an exception.",
  "org.apache.hadoop.fs.ChecksumFileSystem:openFile(org.apache.hadoop.fs.Path)" : "* This is overridden to ensure that this class's\n   * {@link #openFileWithOptions}() method is called, and so ultimately\n   * its {@link #open(Path, int)}.\n   *\n   * {@inheritDoc}",
  "org.apache.hadoop.io.compress.CompressionCodec$Util:createOutputStreamWithCodecPool(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration,java.io.OutputStream)" : "* Create an output stream with a codec taken from the global CodecPool.\n     *\n     * @param codec       The codec to use to create the output stream.\n     * @param conf        The configuration to use if we need to create a new codec.\n     * @param out         The output stream to wrap.\n     * @return            The new output stream\n     * @throws IOException",
  "org.apache.hadoop.io.SortedMapWritable:subMap(java.lang.Object,java.lang.Object)" : null,
  "org.apache.hadoop.io.compress.CodecPool:getLeasedCompressorsCount(org.apache.hadoop.io.compress.CompressionCodec)" : "* Return the number of leased {@link Compressor}s for this\n   * {@link CompressionCodec}.\n   *\n   * @param codec codec.\n   * @return the number of leased.",
  "org.apache.hadoop.util.CombinedIPWhiteList:isIn(java.lang.String)" : null,
  "org.apache.hadoop.security.token.DtFileOperations:appendTokenFiles(java.util.ArrayList,java.lang.String,org.apache.hadoop.conf.Configuration)" : "Append tokens from list of files in local filesystem, saving to last file.\n   *  @param tokenFiles list of local File objects.  Last file holds the output.\n   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output\n   *  @param conf Configuration object passed along.\n   *  @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.Server$Responder:processResponse(java.util.LinkedList,boolean)" : null,
  "org.apache.hadoop.conf.Configuration:getClassByNameOrNull(java.lang.String)" : "* Load a class by name, returning null rather than throwing an exception\n   * if it couldn't be loaded. This is to avoid the overhead of creating\n   * an exception.\n   * \n   * @param name the class name\n   * @return the class object, or null if it could not be found.",
  "org.apache.hadoop.ha.ActiveStandbyElector:parentZNodeExists()" : "* @return true if the configured parent znode exists\n   * @throws IOException raised on errors performing I/O.\n   * @throws InterruptedException interrupted exception.",
  "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:hasCapacity(long)" : null,
  "org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:toString()" : null,
  "org.apache.hadoop.util.StopWatch:now(java.util.concurrent.TimeUnit)" : "* now.\n   *\n   * @param timeUnit timeUnit.\n   * @return current elapsed time in specified timeunit.",
  "org.apache.hadoop.fs.FSDataInputStream:unbuffer()" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:getBytesWritten()" : "* Returns the total number of compressed bytes output so far.\n   *\n   * @return the total (non-negative) number of compressed bytes output so far",
  "org.apache.hadoop.ipc.Server$Call:run()" : null,
  "org.apache.hadoop.ipc.ProtocolProxy:fetchServerMethods(java.lang.reflect.Method)" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping:getUserName(int,java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:hashCode()" : null,
  "org.apache.hadoop.ipc.FairCallQueue:put(java.lang.Object)" : null,
  "org.apache.hadoop.conf.Configuration:getenv(java.lang.String)" : "* Get the environment variable value if\n   * {@link #restrictSystemProps} does not block this.\n   * @param name environment variable name.\n   * @return the value or null if either it is unset or access forbidden.",
  "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:build()" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,java.lang.String[])" : "* Set an array of string values as optional parameter for the Builder.\n   *\n   * @see #opt(String, String)",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>()" : "* Creates a new compressor with a default values for the\n   * compression block size and work factor.  Compressed data will be\n   * generated in bzip2 format.",
  "org.apache.hadoop.util.ShutdownHookManager$HookEntry:getTimeUnit()" : null,
  "org.apache.hadoop.util.DataChecksum:compare(byte[],int)" : "* Compares the checksum located at buf[offset] with the current checksum.\n    *\n    * @param buf buf.\n    * @param offset offset.\n    * @return true if the checksum matches and false otherwise.",
  "org.apache.hadoop.fs.BBPartHandle:hashCode()" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem:workSet()" : null,
  "org.apache.hadoop.metrics2.MetricStringBuilder:tuple(java.lang.String,java.lang.String)" : "* Add any key,val pair to the string, between the prefix and suffix,\n   * separated by the separator.\n   * @param key key\n   * @param value value\n   * @return this instance",
  "org.apache.hadoop.metrics2.source.JvmMetricsInfo:toString()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:open(org.apache.hadoop.fs.Path,int)" : null,
  "org.apache.hadoop.io.UTF8:writeString(java.io.DataOutput,java.lang.String)" : "* @return Write a UTF-8 encoded string.\n   *\n   * @see DataOutput#writeUTF(String)\n   * @param out input out.\n   * @param s input s.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.UserGroupInformation:isInitialized()" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)" : null,
  "org.apache.hadoop.fs.audit.CommonAuditContext:init()" : "* Initialize.",
  "org.apache.hadoop.crypto.key.KeyProviderExtension:getKeyProvider()" : null,
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:createTokenIdent(byte[])" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getSourceAdapter(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:finalize()" : null,
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:toString()" : null,
  "org.apache.hadoop.util.functional.LazyAtomicReference:toString()" : null,
  "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:hashCode()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getDoAs(javax.servlet.http.HttpServletRequest)" : null,
  "org.apache.hadoop.fs.LocalFileSystemPathHandle:<init>(java.lang.String,java.util.Optional)" : null,
  "org.apache.hadoop.log.LogLevel$CLI:doGetLevel()" : "* Send HTTP/HTTPS request to get log level.\n     *\n     * @throws HadoopIllegalArgumentException if arguments are invalid.\n     * @throws Exception if unable to connect",
  "org.apache.hadoop.fs.viewfs.NflyFSystem:getWorkingDirectory()" : null,
  "org.apache.hadoop.security.KerberosAuthException:<init>(java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:isCompressed()" : "* Returns true if values are compressed.\n     * @return if values are compressed true, not false.",
  "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:next()" : null,
  "org.apache.hadoop.util.functional.FunctionalIO:toUncheckedFunction(org.apache.hadoop.util.functional.FunctionRaisingIOE)" : "* Convert a {@link FunctionRaisingIOE} as a {@link Supplier}.\n   * @param fun function to wrap\n   * @param <T> type of input\n   * @param <R> type of return value.\n   * @return a new function which invokes the inner function and wraps\n   * exceptions.",
  "org.apache.hadoop.io.retry.AsyncCallHandler:setLowerLayerAsyncReturn(org.apache.hadoop.util.concurrent.AsyncGet)" : "* For the lower rpc layers to set the async return value.\n   * @param asyncReturn asyncReturn.",
  "org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.crypto.key.KeyShell$Command:printProviderWritten()" : null,
  "org.apache.hadoop.ipc.Client$Call:toString()" : null,
  "org.apache.hadoop.util.XMLUtils:newSecureSAXParserFactory()" : "* This method should be used if you need a {@link SAXParserFactory}. Use this method\n   * instead of {@link SAXParserFactory#newInstance()}. The factory that is returned has\n   * secure configuration enabled.\n   *\n   * @return a {@link SAXParserFactory} with secure configuration enabled\n   * @throws ParserConfigurationException if the {@code JAXP} parser does not support the\n   * secure configuration\n   * @throws SAXException if there are another issues when creating the factory",
  "org.apache.hadoop.fs.permission.AclEntryType:toStringStable()" : "* Returns a string representation guaranteed to be stable across versions to\n   * satisfy backward compatibility requirements, such as for shell command\n   * output or serialization.\n   *\n   * @return stable, backward compatible string representation",
  "org.apache.hadoop.fs.ftp.FTPFileSystem:getTransferMode(org.apache.hadoop.conf.Configuration)" : "* Set FTP's transfer mode based on configuration. Valid values are\n   * STREAM_TRANSFER_MODE, BLOCK_TRANSFER_MODE and COMPRESSED_TRANSFER_MODE.\n   * <p>\n   * Defaults to BLOCK_TRANSFER_MODE.\n   *\n   * @param conf\n   * @return",
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read(byte[],int,int)" : null,
  "org.apache.hadoop.fs.HarFileSystem$LruCache:<init>(int)" : null,
  "org.apache.hadoop.http.HttpServer2:addJerseyResourcePackage(java.lang.String,java.lang.String,java.util.Map)" : "* Add a Jersey resource package.\n   * @param packageName The Java package name containing the Jersey resource.\n   * @param pathSpec The path spec for the servlet\n   * @param params properties and features for ResourceConfig",
  "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getSymlink()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:addRecord(org.apache.hadoop.metrics2.MetricsInfo)" : null,
  "org.apache.hadoop.fs.LocalDirAllocator:removeContext(java.lang.String)" : "* Removes the context from the context config items.\n   * \n   * @param contextCfgItemName contextCfgItemName.",
  "org.apache.hadoop.ha.HAAdmin:transitionToActive(org.apache.commons.cli.CommandLine)" : null,
  "org.apache.hadoop.fs.LocalFileSystem:<init>(org.apache.hadoop.fs.FileSystem)" : null,
  "org.apache.hadoop.net.NetworkTopology:isSameParents(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)" : "* Compare the parents of each node for equality\n   * \n   * <p>To be overridden in subclasses for specific NetworkTopology \n   * implementations, as alternative to overriding the full \n   * {@link #isOnSameRack(Node, Node)} method.\n   * \n   * @param node1 the first node to compare\n   * @param node2 the second node to compare\n   * @return true if their parents are equal, false otherwise\n   * \n   * @see #isOnSameRack(Node, Node)",
  "org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:getConnectionId()" : "* Since this is incapable of returning multiple connection IDs, simply\n     * return the first one. In most cases, the connection ID should be the same\n     * for all proxies.",
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:med3(byte,byte,byte)" : null,
  "org.apache.hadoop.net.InnerNodeImpl:remove(org.apache.hadoop.net.Node)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getRawSize()" : "* Get the uncompressed size of the block.\n       * \n       * @return uncompressed size of the block.",
  "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:invoke(java.lang.Object,java.lang.Object[])" : null,
  "org.apache.hadoop.net.unix.DomainSocketWatcher:addNotificationSocket(java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet)" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.FileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : "* Rename a snapshot.\n   * @param path The directory path where the snapshot was taken\n   * @param snapshotOldName Old name of the snapshot\n   * @param snapshotNewName New name of the snapshot\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:writeImpl(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.InvalidRequestException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)" : "* Create the named file.\n     * @deprecated Use \n     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} \n     *   instead.\n     * @param fs input filesystem.\n     * @param conf input configuration.\n     * @param name input name.\n     * @param keyClass input keyClass.\n     * @param valClass input valClass.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.DataInputBuffer$Buffer:reset(byte[],int,int)" : null,
  "org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)" : null,
  "org.apache.hadoop.fs.LocalDirAllocator:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)" : "Creates a temporary file in the local FS. Pass size as -1 if not known \n   *  apriori. We round-robin over the set of disks (via the configured dirs) \n   *  and select the first complete path which has enough space. A file is\n   *  created on this directory. The file is guaranteed to go away when the\n   *  JVM exits.\n   *  @param pathStr prefix for the temporary file\n   *  @param size the size of the file that is going to be written\n   *  @param conf the Configuration object\n   *  @return a unique temporary file\n   *  @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.DoubleWritable:<init>(double)" : null,
  "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)" : "* Construct the preferred type of SequenceFile Writer.\n   * @param fs The configured filesystem. \n   * @param conf The configuration.\n   * @param name The name of the file. \n   * @param keyClass The 'key' type.\n   * @param valClass The 'value' type.\n   * @return Returns the handle to the constructed SequenceFile Writer.\n   * @throws IOException raised on errors performing I/O.\n   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}\n   *     instead.",
  "org.apache.hadoop.io.Text$Comparator:<init>()" : null,
  "org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:equals(java.lang.Object)" : "Override equals to avoid findbugs warnings",
  "org.apache.hadoop.fs.FileStatus:write(java.io.DataOutput)" : "* Write instance encoded as protobuf to stream.\n   * @param out Output stream\n   * @see PBHelper#convert(FileStatus)\n   * @deprecated Use the {@link PBHelper} and protobuf serialization directly.",
  "org.apache.hadoop.fs.impl.OpenFileParameters:withStatus(org.apache.hadoop.fs.FileStatus)" : null,
  "org.apache.hadoop.fs.FileContext:getTracer()" : null,
  "org.apache.hadoop.metrics2.util.SampleStat:reset(long,double,double,org.apache.hadoop.metrics2.util.SampleStat$MinMax)" : null,
  "org.apache.hadoop.util.DataChecksum:mapByteToChecksumType(int)" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyCompressor:compressDirectBuf()" : null,
  "org.apache.hadoop.util.Progress:addPhases(int)" : "* Adds n nodes to the tree. Gives equal weightage to all phases.\n   *\n   * @param n n.",
  "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getByteString(byte[])" : "* Get the byte string of a non-null byte array.\n   * If the array is 0 bytes long, return a singleton to reduce object allocation.\n   * @param bytes bytes to convert.\n   * @return the protobuf byte string representation of the array.",
  "org.apache.hadoop.io.retry.CallReturn:getState()" : null,
  "org.apache.hadoop.io.MultipleIOException$Builder:add(java.lang.Throwable)" : "* Add the given {@link Throwable} to the exception list.\n     * @param t Throwable.",
  "org.apache.hadoop.util.bloom.HashFunction:hash(org.apache.hadoop.util.bloom.Key)" : "* Hashes a specified key into several integers.\n   * @param k The specified key.\n   * @return The array of hashed values.",
  "org.apache.hadoop.security.UserGroupInformation:getCredentials()" : "* Obtain the tokens in credentials form associated with this user.\n   * \n   * @return Credentials of tokens associated with this user",
  "org.apache.hadoop.fs.shell.SetReplication:waitForReplication()" : "* Wait for all files in waitList to have replication number equal to rep.",
  "org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String)" : "* Change the permissions on a filename.\n   * @param filename the name of the file to change\n   * @param perm the permission string\n   * @return the exit code from the command\n   * @throws IOException raised on errors performing I/O.\n   * @throws InterruptedException command interrupted.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:doDelegationTokenOperation(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation,java.lang.String,org.apache.hadoop.security.token.Token,boolean,java.lang.String)" : null,
  "org.apache.hadoop.service.launcher.ServiceShutdownHook:<init>(org.apache.hadoop.service.Service)" : "* Create an instance.\n   * @param service the service",
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getOptions()" : null,
  "org.apache.hadoop.fs.shell.Display$TextRecordInputStream:close()" : null,
  "org.apache.hadoop.io.SecureIOUtils$AlreadyExistsException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$2:<init>(org.apache.hadoop.fs.FileSystem$Statistics)" : "* Copy constructor.\n     *\n     * @param other    The input Statistics object which is cloned.",
  "org.apache.hadoop.io.compress.PassthroughCodec:getConf()" : null,
  "org.apache.hadoop.ipc.RetryCache:clear(org.apache.hadoop.ipc.RetryCache)" : null,
  "org.apache.hadoop.metrics2.util.MetricsCache$Record:metrics()" : "* @deprecated use metricsEntrySet() instead\n     * @return entry set of metrics",
  "org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:getPos()" : null,
  "org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:open(java.io.InputStream)" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:needsInput()" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:exists(java.lang.String)" : "* Check if a ZNode exists.\n   * @param path Path of the ZNode.\n   * @return If the ZNode exists.\n   * @throws Exception If it cannot contact Zookeeper.",
  "org.apache.hadoop.io.WritableUtils:decodeVIntSize(byte)" : "* Parse the first byte of a vint/vlong to determine the number of bytes\n   * @param value the first byte of the vint/vlong\n   * @return the total number of bytes (1 to 9)",
  "org.apache.hadoop.net.SocksSocketFactory:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(short[],java.lang.String)" : "* Validates that the given array is not null and has at least one element.\n   * @param array the argument reference to validate.\n   * @param argName the name of the argument being validated.",
  "org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpMatrix(byte[],int,int)" : null,
  "org.apache.hadoop.crypto.key.kms.ValueQueue:submitRefillTask(java.lang.String,java.util.Queue)" : null,
  "org.apache.hadoop.fs.Path:mergePaths(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Merge 2 paths such that the second path is appended relative to the first.\n   * The returned path has the scheme and authority of the first path.  On\n   * Windows, the drive specification in the second path is discarded.\n   * \n   * @param path1 the first path\n   * @param path2 the second path, to be appended relative to path1\n   * @return the merged path",
  "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:setOptions(org.apache.hadoop.fs.shell.find.FindOptions)" : null,
  "org.apache.hadoop.util.NativeCrc32:verifyChunkedSumsByteArray(int,int,byte[],int,byte[],int,int,java.lang.String,long)" : null,
  "org.apache.hadoop.service.launcher.AbstractLaunchableService:bindArgs(org.apache.hadoop.conf.Configuration,java.util.List)" : null,
  "org.apache.hadoop.fs.DelegationTokenRenewer:run()" : null,
  "org.apache.hadoop.ha.ZKFailoverController:waitForActiveAttempt(int,long)" : "* Wait until one of the following events:\n   * <ul>\n   * <li>Another thread publishes the results of an attempt to become active\n   * using {@link #recordActiveAttempt(ActiveAttemptRecord)}</li>\n   * <li>The node enters bad health status</li>\n   * <li>The specified timeout elapses</li>\n   * </ul>\n   * \n   * @param timeoutMillis number of millis to wait\n   * @param onlyAfterNanoTime accept attempt records only after a given\n   * timestamp. Use this parameter to ignore the old attempt records from a\n   * previous fail-over attempt.\n   * @return the published record, or null if the timeout elapses or the\n   * service becomes unhealthy \n   * @throws InterruptedException if the thread is interrupted.",
  "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:processErasures(int[])" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processRetryInfo()" : null,
  "org.apache.hadoop.util.CrcUtil:galoisFieldMultiply(int,int,int)" : "* Galois field multiplication of {@code p} and {@code q} with the\n   * generator polynomial {@code m} as the modulus.\n   *\n   * @param m The little-endian polynomial to use as the modulus when\n   *     multiplying p and q, with implicit \"1\" bit beyond the bottom bit.",
  "org.apache.hadoop.util.Shell:getReadlinkCommand(java.lang.String)" : "* Return a command to read the target of the a symbolic link.\n   *\n   * @param link link.\n   * @return read link command.",
  "org.apache.hadoop.util.functional.Tuples:<init>()" : null,
  "org.apache.hadoop.http.HttpServer2$Builder:addEndpoint(java.net.URI)" : "* Add an endpoint that the HTTP server should listen to.\n     *\n     * @param endpoint\n     *          the endpoint of that the HTTP server should listen to. The\n     *          scheme specifies the protocol (i.e. HTTP / HTTPS), the host\n     *          specifies the binding address, and the port specifies the\n     *          listening port. Unspecified or zero port means that the server\n     *          can listen to any port.\n     * @return Builder.",
  "org.apache.hadoop.fs.ParentNotDirectoryException:<init>()" : null,
  "org.apache.hadoop.net.ConnectTimeoutException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:getExecutor()" : null,
  "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:beforeExecute(java.lang.Thread,java.lang.Runnable)" : null,
  "org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeyVersions(java.lang.String)" : null,
  "org.apache.hadoop.conf.Configuration:main(java.lang.String[])" : "For debugging.  List non-default properties to the terminal and exit.\n   * @param args the argument to be parsed.\n   * @throws Exception exception.",
  "org.apache.hadoop.service.ServiceStateModel:checkStateTransition(java.lang.String,org.apache.hadoop.service.Service$STATE,org.apache.hadoop.service.Service$STATE)" : "* Check that a state tansition is valid and\n   * throw an exception if not\n   * @param name name of the service (can be null)\n   * @param state current state\n   * @param proposed proposed new state",
  "org.apache.hadoop.util.curator.ZKCuratorManager:getZKAcls(org.apache.hadoop.conf.Configuration)" : "* Utility method to fetch the ZK ACLs from the configuration.\n   *\n   * @param conf configuration.\n   * @throws java.io.IOException if the Zookeeper ACLs configuration file\n   * cannot be read\n   * @return acl list.",
  "org.apache.hadoop.ipc.RPC$Server:getSupportedProtocolVersions(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:getFailoverOrRetrySleepTime(int)" : "* @return 0 if this is our first failover/retry (i.e., retry immediately),\n     *         sleep exponentially otherwise",
  "org.apache.hadoop.net.InnerNodeImpl:getNextAncestorName(org.apache.hadoop.net.Node)" : null,
  "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:updateRenewalTime(long)" : "* Set a new time for the renewal.\n     * It can only be called when the action is not in the queue or any\n     * collection because the hashCode may change\n     * @param delay the renewal time",
  "org.apache.hadoop.metrics2.lib.MutableCounterLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:isOperator()" : null,
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getPermission()" : null,
  "org.apache.hadoop.net.InnerNodeImpl:isAncestor(org.apache.hadoop.net.Node)" : "Judge if this node is an ancestor of node <i>n</i>.\n   *\n   * @param n a node\n   * @return true if this node is an ancestor of <i>n</i>",
  "org.apache.hadoop.fs.HarFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)" : "* Get block locations from the underlying fs and fix their\n   * offsets and lengths.\n   * @param file the input file status to get block locations\n   * @param start the start of the desired range in the contained file\n   * @param len the length of the desired range\n   * @return block locations for this segment of file\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.viewfs.ChRootedFs:delete(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.ipc.Server$ConnectionManager:stopIdleScan()" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$5:<init>(org.apache.hadoop.fs.FileSystem$Statistics)" : "* Copy constructor.\n     *\n     * @param other    The input Statistics object which is cloned.",
  "org.apache.hadoop.fs.FsShell:getUsagePrefix()" : null,
  "org.apache.hadoop.conf.StorageUnit$4:getShortName()" : null,
  "org.apache.hadoop.fs.shell.CommandFormat:<init>(java.lang.String,int,int,java.lang.String[])" : "* @deprecated use replacement since name is an unused parameter\n   * @param name of command, but never used\n   * @param min see replacement\n   * @param max see replacement\n   * @param possibleOpt see replacement\n   * @see #CommandFormat(int, int, String...)",
  "org.apache.hadoop.ipc.ProtobufHelper:getRemoteException(com.google.protobuf.ServiceException)" : "* Return the IOException thrown by the remote server wrapped in\n   * ServiceException as cause.\n   * @param se ServiceException that wraps IO exception thrown by the server\n   * @return Exception wrapped in ServiceException or\n   *         a new IOException that wraps the unexpected ServiceException.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Convenience Constructor for apps to call directly.\n   * @param theUri which must be that of ViewFileSystem\n   * @param conf conf configuration.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.GcTimeMonitor$GcData:update(long,long,long,long,int)" : null,
  "org.apache.hadoop.ipc.Server$Connection:checkRpcHeaders(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto)" : "* Verify RPC header is valid\n     * @param header - RPC request header\n     * @throws RpcServerException - header contains invalid values",
  "org.apache.hadoop.io.WritableComparator:get(java.lang.Class,org.apache.hadoop.conf.Configuration)" : "* Get a comparator for a {@link WritableComparable} implementation.\n   * @param c class.\n   * @param conf configuration.\n   * @return WritableComparator.",
  "org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:getValue()" : null,
  "org.apache.hadoop.util.ExitUtil$HaltException:getExitCode()" : null,
  "org.apache.hadoop.util.DataChecksum:getCrcPolynomialForType(org.apache.hadoop.util.DataChecksum$Type)" : "* getCrcPolynomialForType.\n   *\n   * @param type type.\n   * @return the int representation of the polynomial associated with the\n   *     CRC {@code type}, suitable for use with further CRC arithmetic.\n   * @throws IOException if there is no CRC polynomial applicable\n   *     to the given {@code type}.",
  "org.apache.hadoop.ipc.WritableRpcEngine$Server:log(java.lang.String)" : null,
  "org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Construct a map reader for the named map.\n     * @deprecated\n     *\n     * @param fs FileSystem.\n     * @param dirName dirName.\n     * @param conf configuration.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.FileUtil:copy(java.io.File,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)" : "* Copy local files to a FileSystem.\n   *\n   * @param src src.\n   * @param dstFS dstFs.\n   * @param dst dst.\n   * @param deleteSource delete source.\n   * @param conf configuration.\n   * @throws IOException raised on errors performing I/O.\n   * @return true if the operation succeeded.",
  "org.apache.hadoop.net.NetworkTopology:countNumOfAvailableNodes(java.lang.String,java.util.Collection)" : "return the number of leaves in <i>scope</i> but not in <i>excludedNodes</i>\n   * if scope starts with ~, return the number of nodes that are not\n   * in <i>scope</i> and <i>excludedNodes</i>; \n   * @param scope a path string that may start with ~\n   * @param excludedNodes a list of nodes\n   * @return number of available nodes",
  "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:<init>(java.util.List)" : null,
  "org.apache.hadoop.util.IntrusiveCollection:removeElement(org.apache.hadoop.util.IntrusiveCollection$Element)" : null,
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:<init>(org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder,org.apache.hadoop.fs.FileSystem)" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExists(java.nio.file.Path,java.lang.String)" : "* Validates that the given path exists.\n   * @param path the path to check.\n   * @param argName the name of the argument being validated.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:open(org.apache.hadoop.fs.Path,int)" : null,
  "org.apache.hadoop.util.ZKUtil:resolveConfIndirection(java.lang.String)" : "* Because ZK ACLs and authentication information may be secret,\n   * allow the configuration values to be indirected through a file\n   * by specifying the configuration as \"@/path/to/file\". If this\n   * syntax is used, this function will return the contents of the file\n   * as a String.\n   * \n   * @param valInConf the value from the Configuration \n   * @return either the same value, or the contents of the referenced\n   * file if the configured value starts with \"@\"\n   * @throws IOException if the file cannot be read",
  "org.apache.hadoop.ipc.Client$ConnectionId:getTcpLowLatency()" : "use low-latency QoS bits over TCP",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createServiceURL(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.Shell:checkIsBashSupported()" : null,
  "org.apache.hadoop.fs.FilterFs:removeDefaultAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.Client$IpcStreams:<init>(java.net.Socket,int)" : null,
  "org.apache.hadoop.io.DefaultStringifier:close()" : null,
  "org.apache.hadoop.security.UserGroupInformation:trimLoginMethod(java.lang.String)" : "* remove the login method that is followed by a space from the username\n   * e.g. \"jack (auth:SIMPLE)\" {@literal ->} \"jack\"\n   *\n   * @param userName userName.\n   * @return userName without login method",
  "org.apache.hadoop.security.authorize.AuthorizationException:printStackTrace(java.io.PrintWriter)" : null,
  "org.apache.hadoop.io.compress.CompressionCodecFactory:<init>(org.apache.hadoop.conf.Configuration)" : "* Find the codecs specified in the config value io.compression.codecs \n   * and register them. Defaults to gzip and deflate.\n   *\n   * @param conf configuration.",
  "org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,java.lang.Object,java.util.Map)" : "* Create a proxy for an interface of an implementation class\n   * using the a set of retry policies specified by method name.\n   * If no retry policy is defined for a method then a default of\n   * {@link RetryPolicies#TRY_ONCE_THEN_FAIL} is used.\n   * \n   * @param iface the interface that the retry will implement\n   * @param <T> T.\n   * @param implementation the instance whose methods should be retried\n   * @param methodNameToPolicyMap a map of method names to retry policies\n   * @return the retry proxy",
  "org.apache.hadoop.service.launcher.InterruptEscalator:lookup(java.lang.String)" : "* Look up the handler for a signal.\n   * @param signalName signal name\n   * @return a handler if found",
  "org.apache.hadoop.service.ServiceStateModel:getState()" : "* Query the service state. This is a non-blocking operation.\n   * @return the state",
  "org.apache.hadoop.io.MapWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.util.AsyncDiskService:shutdown()" : "* Gracefully start the shut down of all ThreadPools.",
  "org.apache.hadoop.security.SaslPropertiesResolver:getClientProperties(java.net.InetAddress,int)" : "* Identify the Sasl Properties to be used for a connection with a server.\n   * @param serverAddress server's address\n   * @param ingressPort the port that is used to connect to server\n   * @return the sasl properties to be used for the connection.",
  "org.apache.hadoop.fs.FilterFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:readFloatArray(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.PathExistsException:<init>(java.lang.String)" : "@param path for the exception",
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:hashCode()" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Load IOStatisticsSnapshot from a Hadoop filesystem.\n   * @param fs filesystem\n   * @param path path\n   * @return the loaded snapshot\n   * @throws UncheckedIOException Any IO exception.\n   * @throws UnsupportedOperationException if the IOStatistics classes were not found",
  "org.apache.hadoop.fs.shell.find.Find:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : "* Register the names for the count command\n   * \n   * @param factory the command factory that will instantiate this class",
  "org.apache.hadoop.fs.FileSystem:printStatistics()" : "* Print all statistics for all file systems to {@code System.out}\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationNear(long)" : "* Get the location pointing to the beginning of the first key-value pair in\n     * a compressed block whose byte offset in the TFile is greater than or\n     * equal to the specified offset.\n     * \n     * @param offset\n     *          the user supplied offset.\n     * @return the location to the corresponding entry; or end() if no such\n     *         entry exists.",
  "org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String)" : "* Takes input as a comma separated list of files\n   * and verifies if they exist. It defaults for file:///\n   * if the files specified do not have a scheme.\n   * it returns the paths uri converted defaulting to file:///.\n   * So an input of  /home/user/file1,/home/user/file2 would return\n   * file:///home/user/file1,file:///home/user/file2.\n   *\n   * This method does not recognize wildcards.\n   *\n   * @param files the input files argument\n   * @return a comma-separated list of validated and qualified paths, or null\n   * if the input files argument is null",
  "org.apache.hadoop.fs.HarFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : "* Not implemented.",
  "org.apache.hadoop.crypto.key.CachingKeyProvider:<init>(org.apache.hadoop.crypto.key.KeyProvider,long,long)" : null,
  "org.apache.hadoop.fs.LocalFileSystem:<init>()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)" : null,
  "org.apache.hadoop.fs.VectoredReadUtils:isOrderedDisjoint(java.util.List,int,int)" : "* Is the given input list.\n   * <ul>\n   *   <li>already sorted by offset</li>\n   *   <li>each range is more than minimumSeek apart</li>\n   *   <li>the start and end of each range is a multiple of chunkSize</li>\n   * </ul>\n   *\n   * @param input the list of input ranges.\n   * @param chunkSize the size of the chunks that the offset and end must align to.\n   * @param minimumSeek the minimum distance between ranges.\n   * @return true if we can use the input list as is.",
  "org.apache.hadoop.io.WritableComparator:define(java.lang.Class,org.apache.hadoop.io.WritableComparator)" : "* Register an optimized comparator for a {@link WritableComparable}\n   * implementation. Comparators registered with this method must be\n   * thread-safe.\n   * @param c class.\n   * @param comparator WritableComparator.",
  "org.apache.hadoop.fs.FileSystem:getXAttrs(org.apache.hadoop.fs.Path)" : "* Get all of the xattr name/value pairs for a file or directory.\n   * Only those xattrs which the logged-in user has permissions to view\n   * are returned.\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to get extended attributes\n   * @return Map describing the XAttrs of the file or directory\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.fs.FilterFileSystem:removeAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.statistics.MeanStatistic:clone()" : null,
  "org.apache.hadoop.fs.FileSystem$DirListingIterator:next()" : null,
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:containsKey(java.lang.Object)" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping:clearNameMaps()" : null,
  "org.apache.hadoop.fs.shell.CommandFormat:getOpts()" : "Returns all the options that are set\n   * \n   * @return Set{@literal <}String{@literal >} of the enabled options",
  "org.apache.hadoop.service.CompositeService:addService(org.apache.hadoop.service.Service)" : "* Add the passed {@link Service} to the list of services managed by this\n   * {@link CompositeService}\n   * @param service the {@link Service} to be added",
  "org.apache.hadoop.ha.ActiveStandbyElector:connectToZooKeeper()" : "* Get a new zookeeper client instance. protected so that test class can\n   * inherit and mock out the zookeeper instance\n   * \n   * @return new zookeeper client instance\n   * @throws IOException raised on errors performing I/O.\n   * @throws KeeperException zookeeper connectionloss exception",
  "org.apache.hadoop.conf.StorageUnit$6:toTBs(double)" : null,
  "org.apache.hadoop.io.DefaultStringifier:loadArray(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)" : "* Restores the array of objects from the configuration.\n   * \n   * @param <K> the class of the item\n   * @param conf the configuration to use\n   * @param keyName the name of the key to use\n   * @param itemClass the class of the item\n   * @return restored object\n   * @throws IOException : forwards Exceptions from the underlying \n   * {@link Serialization} classes.",
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:getHomeDirectory(com.jcraft.jsch.ChannelSftp)" : "* Convenience method, so that we don't open a new connection when using this\n   * method from within another method. Otherwise every API invocation incurs\n   * the overhead of opening/closing a TCP connection.",
  "org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:getThisBuilder()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(byte[])" : "* Copy value into user-supplied buffer. User supplied buffer must be\n         * large enough to hold the whole value. The value part of the key-value\n         * pair pointed by the current cursor is not cached and can only be\n         * examined once. Calling any of the following functions more than once\n         * without moving the cursor will result in exception:\n         * {@link #getValue(byte[])}, {@link #getValue(byte[], int)},\n         * {@link #getValueStream}.\n         *\n         * @param buf buf.\n         * @return the length of the value. Does not require\n         *         isValueLengthKnown() to be true.\n         * @throws IOException raised on errors performing I/O.\n         *",
  "org.apache.hadoop.fs.FileSystem:getAllStoragePolicies()" : "* Retrieve all the storage policies supported by this file system.\n   *\n   * @return all storage policies supported by this filesystem.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.ipc.RpcClientUtil:getProtocolMetaInfoProxy(java.lang.Object,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.GenericWritable:toString()" : null,
  "org.apache.hadoop.ipc.Server$Connection:createSaslServer(org.apache.hadoop.security.SaslRpcServer$AuthMethod)" : null,
  "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.ipc.RefreshCallQueueProtocol)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)" : "* The specification of this method matches that of\n   * {@link FileContext#access(Path, FsAction)}\n   * except that an UnresolvedLinkException may be thrown if a symlink is\n   * encountered in the path.\n   *\n   * @param path the path.\n   * @param mode fsaction mode.\n   * @throws AccessControlException access control exception.\n   * @throws FileNotFoundException file not found exception.\n   * @throws UnresolvedLinkException unresolved link exception.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.conf.Configuration:loadProps(java.util.Properties,int,boolean)" : "* Loads the resource at a given index into the properties.\n   * @param props the object containing the loaded properties.\n   * @param startIdx the index where the new resource has been added.\n   * @param fullReload flag whether we do complete reload of the conf instead\n   *                   of just loading the new resource.",
  "org.apache.hadoop.net.NetUtils:getHostNameOfIP(java.lang.String)" : "* Attempt to obtain the host name of the given string which contains\n   * an IP address and an optional port.\n   * \n   * @param ipPort string of form ip[:port]\n   * @return Host name or null if the name can not be determined",
  "org.apache.hadoop.fs.FilterFs:getFileLinkStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String,byte)" : null,
  "org.apache.hadoop.conf.Configuration:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)" : "* Gets the Storage Size from the config, or returns the defaultValue. The\n   * unit of return value is specified in target unit.\n   *\n   * @param name - Key Name\n   * @param defaultValue - Default Value -- e.g. 100MB\n   * @param targetUnit - The units that we want result to be in.\n   * @return double -- formatted in target Units",
  "org.apache.hadoop.fs.FutureDataInputStreamBuilder:withFileStatus(org.apache.hadoop.fs.FileStatus)" : "* A FileStatus may be provided to the open request.\n   * It is up to the implementation whether to use this or not.\n   * @param status status: may be null\n   * @return the builder.",
  "org.apache.hadoop.fs.FSOutputSummer:getDataChecksum()" : null,
  "org.apache.hadoop.io.VLongWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.conf.StorageUnit:multiply(double,double)" : "* Using BigDecimal so we can throw if we are overflowing the Long.Max.\n   *\n   * @param first - First Num.\n   * @param second - Second Num.\n   * @return Returns a double",
  "org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:addEntry(org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry)" : null,
  "org.apache.hadoop.metrics2.lib.MutableGaugeLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)" : null,
  "org.apache.hadoop.ipc.Server:setLogSlowRPCThresholdTime(long)" : null,
  "org.apache.hadoop.fs.FilterFs:getUri()" : null,
  "org.apache.hadoop.fs.ftp.FTPInputStream:close()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues5(int,int)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:mkdirs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:needsDictionary()" : null,
  "org.apache.hadoop.crypto.random.OsSecureRandom:nextBytes(byte[])" : null,
  "org.apache.hadoop.fs.FSInputChecker:set(boolean,java.util.zip.Checksum,int,int)" : "* Set the checksum related parameters\n   * @param verifyChecksum whether to verify checksum\n   * @param sum which type of checksum to use\n   * @param maxChunkSize maximun chunk size\n   * @param checksumSize checksum size",
  "org.apache.hadoop.fs.FileContext:getHomeDirectory()" : "* Return the current user's home directory in this file system.\n   * The default implementation returns \"/user/$USER/\".\n   * @return the home directory",
  "org.apache.hadoop.io.DataInputBuffer$Buffer:read(byte[],int,int)" : "* {@inheritDoc}",
  "org.apache.hadoop.util.bloom.DynamicBloomFilter:toString()" : null,
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Load IOStatisticsSnapshot from a Hadoop filesystem.\n   * @param fs filesystem\n   * @param path path\n   * @return the loaded snapshot\n   * @throws UncheckedIOException Any IO exception.",
  "org.apache.hadoop.service.launcher.ServiceLauncher:createConfiguration()" : "* Override point: create the base configuration for the service.\n   *\n   * Subclasses can override to create HDFS/YARN configurations etc.\n   * @return the configuration to use as the service initializer.",
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)" : null,
  "org.apache.hadoop.http.PrometheusServlet:getPrometheusSink()" : null,
  "org.apache.hadoop.net.NetworkTopology:getNumOfNonEmptyRacks()" : "@return the number of nonempty racks",
  "org.apache.hadoop.util.ProcessUtils:<init>()" : null,
  "org.apache.hadoop.net.DNS:getHosts(java.lang.String)" : "* Returns all the host names associated by the default nameserver with the\n   * address bound to the specified network interface\n   * \n   * @param strInterface\n   *            The name of the network interface to query (e.g. eth0)\n   * @return The list of host names associated with IPs bound to the network\n   *         interface\n   * @throws UnknownHostException\n   *             If one is encountered while querying the default interface\n   *",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:clone()" : "* @see java.lang.Object#clone()",
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:reset()" : null,
  "org.apache.hadoop.util.RunJar:unJarAndSave(java.io.InputStream,java.io.File,java.lang.String,java.util.regex.Pattern)" : "* Unpack matching files from a jar. Entries inside the jar that do\n   * not match the given pattern will be skipped. Keep also a copy\n   * of the entire jar in the same directory for backward compatibility.\n   * TODO remove this feature in a new release and do only unJar\n   *\n   * @param inputStream the jar stream to unpack\n   * @param toDir the destination directory into which to unpack the jar\n   * @param unpackRegex the pattern to match jar entries against\n   * @param name name.\n   *\n   * @throws IOException if an I/O error has occurred or toDir\n   * cannot be created and does not already exist",
  "org.apache.hadoop.io.SortedMapWritable:size()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)" : "* Set the current working directory for the given file system. All relative\n   * paths will be resolved relative to it.\n   * \n   * @param newDir new dir.",
  "org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)" : "* Merges the contents of files passed in Path[]\n     * @param inNames the array of path names\n     * @param tempDir the directory for creating temp files during merge\n     * @param deleteInputs true if the input files should be deleted when \n     * unnecessary\n     * @return RawKeyValueIteratorMergeQueue\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.FsServerDefaults:getBytesPerChecksum()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getCached(int)" : null,
  "org.apache.hadoop.conf.StorageUnit$6:getShortName()" : null,
  "org.apache.hadoop.fs.Path:isAbsoluteAndSchemeAuthorityNull()" : "* Returns true if the path component (i.e. directory) of this URI is\n   * absolute <strong>and</strong> the scheme is null, <b>and</b> the authority\n   * is null.\n   *\n   * @return whether the path is absolute and the URI has no scheme nor\n   * authority parts",
  "org.apache.hadoop.http.HttpServer2Metrics:responsesBytesTotal()" : null,
  "org.apache.hadoop.conf.ReconfigurationException:<init>()" : "* Create a new instance of {@link ReconfigurationException}.",
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeByPiggyBack(byte[][],int[],byte[],int,byte[],int,int)" : null,
  "org.apache.hadoop.http.HttpServer2$QuotingInputFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)" : null,
  "org.apache.hadoop.io.compress.SnappyCodec:createDecompressor()" : "* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.\n   *\n   * @return a new decompressor for use by this codec",
  "org.apache.hadoop.fs.RawLocalFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.metrics2.util.SampleStat$MinMax:min()" : null,
  "org.apache.hadoop.util.SysInfoLinux:getCpuFrequency()" : "{@inheritDoc}",
  "org.apache.hadoop.metrics2.MetricsTag:info()" : "* @return the info object of the tag",
  "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:preferDirectBuffer()" : null,
  "org.apache.hadoop.service.launcher.InterruptEscalator$ServiceForcedShutdown:<init>(org.apache.hadoop.service.Service,int)" : null,
  "org.apache.hadoop.ha.ShellCommandFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateGauges(java.lang.Long,java.lang.Long)" : "* Add two gauges.\n   * @param l left value\n   * @param r right value\n   * @return aggregate value",
  "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.String)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:finish()" : null,
  "org.apache.hadoop.net.ScriptBasedMapping:getRawMapping()" : "* Get the cached mapping and convert it to its real type\n   * @return the inner raw script mapping.",
  "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,long)" : null,
  "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:deserialize(java.lang.Object)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newRatesWithAggregation(java.lang.String)" : null,
  "org.apache.hadoop.util.functional.TaskPool$Builder:runSingleThreaded(org.apache.hadoop.util.functional.TaskPool$Task)" : "* Single threaded execution.\n     * @param task task to execute\n     * @param <E> exception which may be raised in execution.\n     * @return true if the operation executed successfully\n     * @throws E any exception raised.\n     * @throws IOException IOExceptions raised by remote iterator or in execution.",
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)" : "* Create a mutable long integer gauge\n   * @param info  metadata of the metric\n   * @param iVal  initial value\n   * @return a new gauge object",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getBytesRead()" : "* Returns the total number of compressed bytes input so far.\n   *\n   * @return the total (non-negative) number of compressed bytes input so far",
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:skip(long)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLocationByRecordNum(long)" : null,
  "org.apache.hadoop.util.SysInfoWindows:getAvailableVirtualMemorySize()" : "{@inheritDoc}",
  "org.apache.hadoop.util.VersionInfo:getRevision()" : "* Get the Git commit hash of the repository when compiled.\n   * @return the commit hash, eg. \"18f64065d5db6208daf50b02c1b5ed4ee3ce547a\"",
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)" : null,
  "org.apache.hadoop.fs.FileContext:getFileStatus(org.apache.hadoop.fs.Path)" : "* Return a file status object that represents the path.\n   * @param f The path we want information from\n   *\n   * @return a FileStatus object\n   *\n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If <code>f</code> does not exist\n   * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n   *           not supported\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server",
  "org.apache.hadoop.ha.HAAdmin:gracefulFailoverThroughZKFCs(org.apache.hadoop.ha.HAServiceTarget)" : "* Initiate a graceful failover by talking to the target node's ZKFC.\n   * This sends an RPC to the ZKFC, which coordinates the failover.\n   *\n   * @param toNode the node to fail to\n   * @return status code (0 for success)\n   * @throws IOException if failover does not succeed",
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],javax.net.ssl.SSLSocket)" : null,
  "org.apache.hadoop.io.NullWritable$Comparator:compare(byte[],int,int,byte[],int,int)" : "* Compare the buffers in serialized form.",
  "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:getEngineId()" : null,
  "org.apache.hadoop.util.SysInfoLinux:getConf(java.lang.String)" : null,
  "org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)" : null,
  "org.apache.hadoop.fs.shell.CopyCommands$Merge:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.conf.Configuration:addTags(java.util.Properties)" : "* Add tags defined in HADOOP_TAGS_SYSTEM, HADOOP_TAGS_CUSTOM.\n   * @param prop properties.",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:getMetricsTimeUnit()" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:getConnectionId()" : null,
  "org.apache.hadoop.fs.ChecksumFs:listLocatedStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:getUsage()" : null,
  "org.apache.hadoop.fs.GlobalStorageStatistics:put(java.lang.String,org.apache.hadoop.fs.GlobalStorageStatistics$StorageStatisticsProvider)" : "* Create or return the StorageStatistics object with the given name.\n   *\n   * @param name        The storage statistics object name.\n   * @param provider    An object which can create a new StorageStatistics\n   *                      object if needed.\n   * @return            The StorageStatistics object with the given name.\n   * @throws RuntimeException  If the StorageStatisticsProvider provides a null\n   *                           object or a new StorageStatistics object with the\n   *                           wrong name.",
  "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:setRecordFilter(org.apache.hadoop.metrics2.MetricsFilter)" : null,
  "org.apache.hadoop.io.SequenceFile$Metadata:get(org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopTimer()" : null,
  "org.apache.hadoop.ipc.FairCallQueue:offer(java.lang.Object,long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.crypto.OpensslCipher:doFinal(java.nio.ByteBuffer)" : "* Finishes a multiple-part operation. The data is encrypted or decrypted,\n   * depending on how this cipher was initialized.\n   * <p>\n   * The result is stored in the output buffer. Upon return, the output buffer's\n   * position will have advanced by n, where n is the value returned by this\n   * method; the output buffer's limit will not have changed.\n   * </p>\n   * If <code>output.remaining()</code> bytes are insufficient to hold the result,\n   * a <code>ShortBufferException</code> is thrown.\n   * <p>\n   * Upon finishing, this method resets this cipher object to the state it was\n   * in when previously initialized. That is, the object is available to encrypt\n   * or decrypt more data.\n   * </p>\n   * If any exception is thrown, this cipher object need to be reset before it\n   * can be used again.\n   *\n   * @param output the output ByteBuffer\n   * @return int number of bytes stored in <code>output</code>\n   * @throws ShortBufferException      if there is insufficient space in the output buffer.\n   * @throws IllegalBlockSizeException This exception is thrown when the length\n   *                                   of data provided to a block cipher is incorrect.\n   * @throws BadPaddingException       This exception is thrown when a particular\n   *                                   padding mechanism is expected for the input\n   *                                   data but the data is not padded properly.",
  "org.apache.hadoop.service.AbstractService:serviceStop()" : "* Actions called during the transition to the STOPPED state.\n   *\n   * This method will only ever be called once during the lifecycle of\n   * a specific service instance.\n   *\n   * Implementations do not need to be synchronized as the logic\n   * in {@link #stop()} prevents re-entrancy.\n   *\n   * Implementations MUST write this to be robust against failures, including\n   * checks for null references -and for the first failure to not stop other\n   * attempts to shut down parts of the service.\n   *\n   * @throws Exception if needed -these will be caught and logged.",
  "org.apache.hadoop.fs.FilterFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.shell.Command:<init>()" : "Constructor",
  "org.apache.hadoop.fs.shell.Delete$Rm:canBeSafelyDeleted(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2DecompressorType(org.apache.hadoop.conf.Configuration)" : "* Return the appropriate type of the bzip2 decompressor. \n   * \n   * @param conf configuration\n   * @return the appropriate type of the bzip2 decompressor.",
  "org.apache.hadoop.io.file.tfile.TFile:getFSInputBufferSize(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBlockSize(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.Server$Listener:doStop()" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processTokenAddOrUpdate(byte[])" : null,
  "org.apache.hadoop.fs.permission.FsPermission:toString()" : null,
  "org.apache.hadoop.util.JsonSerialization:save(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Object,boolean)" : "* Save to a Hadoop filesystem.\n   * @param fs filesystem\n   * @param path path\n   * @param overwrite should any existing file be overwritten\n   * @param instance instance\n   * @throws IOException IO exception.",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:getNumInProcessHandler()" : null,
  "org.apache.hadoop.io.wrappedio.WrappedIO:bulkDelete_pageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Get the maximum number of objects/files to delete in a single request.\n   * @param fs filesystem\n   * @param path path to delete under.\n   * @return a number greater than or equal to zero.\n   * @throws UnsupportedOperationException bulk delete under that path is not supported.\n   * @throws IllegalArgumentException path not valid.\n   * @throws UncheckedIOException if an IOE was raised.",
  "org.apache.hadoop.util.PriorityQueue:top()" : "* Returns the least element of the PriorityQueue in constant time.\n   *\n   * @return T Generics Type T.",
  "org.apache.hadoop.io.MapWritable:containsKey(java.lang.Object)" : null,
  "org.apache.hadoop.fs.shell.Command:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)" : "*  Provides a hook for handling paths that don't exist.  By default it\n   *  will throw an exception.  Primarily overriden by commands that create\n   *  paths such as mkdir or touch.\n   *  @param item the {@link PathData} that doesn't exist\n   *  @throws FileNotFoundException if arg is a path and it doesn't exist\n   *  @throws IOException if anything else goes wrong...",
  "org.apache.hadoop.io.compress.lz4.Lz4Compressor:compress(byte[],int,int)" : "* Fills specified buffer with compressed data. Returns actual number\n   * of bytes of compressed data. A return value of 0 indicates that\n   * needsInput() should be called in order to determine if more input\n   * data is required.\n   *\n   * @param b   Buffer for the compressed data\n   * @param off Start offset of the data\n   * @param len Size of the buffer\n   * @return The actual number of bytes of compressed data.",
  "org.apache.hadoop.conf.Configuration$IntegerRanges:<init>()" : null,
  "org.apache.hadoop.fs.FSOutputSummer:write(byte[],int,int)" : "* Writes <code>len</code> bytes from the specified byte array \n   * starting at offset <code>off</code> and generate a checksum for\n   * each data chunk.\n   *\n   * <p> This method stores bytes from the given array into this\n   * stream's buffer before it gets checksumed. The buffer gets checksumed \n   * and flushed to the underlying output stream when all data \n   * in a checksum chunk are in the buffer.  If the buffer is empty and\n   * requested length is at least as large as the size of next checksum chunk\n   * size, this method will checksum and write the chunk directly \n   * to the underlying output stream.  Thus it avoids unnecessary data copy.\n   *\n   * @param      b     the data.\n   * @param      off   the start offset in the data.\n   * @param      len   the number of bytes to write.\n   * @exception  IOException  if an I/O error occurs.",
  "org.apache.hadoop.io.erasurecode.CodecRegistry:getCoders(java.lang.String)" : "* Get all coder factories of the given codec.\n   * @param codecName the name of codec\n   * @return a list of all coder factories, null if not exist",
  "org.apache.hadoop.fs.permission.FsPermission:toOctal()" : "* Returns the FsPermission in an octal format.\n   *\n   * @return short Unlike {@link #toShort()} which provides a binary\n   * representation, this method returns the standard octal style permission.",
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:setFirstKey(byte[],int,int)" : null,
  "org.apache.hadoop.fs.DelegateToFileSystem:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)" : null,
  "org.apache.hadoop.ipc.RemoteException:toString()" : null,
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_retrieve(java.lang.Object)" : "* Extract the IOStatistics from an object in a serializable form.\n   * @param source source object, may be null/not a statistics source/instance\n   * @return {@link IOStatisticsSnapshot} or null if the object is null/doesn't have statistics",
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:getCallCostSnapshot()" : null,
  "org.apache.hadoop.crypto.key.KeyProviderExtension:getCurrentKey(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.TouchCommands$Touchz:touchz(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getPathAsString()" : null,
  "org.apache.hadoop.io.SequenceFile:getBufferSize(org.apache.hadoop.conf.Configuration)" : "Get the configured buffer size",
  "org.apache.hadoop.util.bloom.CountingBloomFilter:<init>(int,int,int)" : "* Constructor\n   * @param vectorSize The vector size of <i>this</i> filter.\n   * @param nbHash The number of hash function to consider.\n   * @param hashType type of the hashing function (see\n   * {@link org.apache.hadoop.util.hash.Hash}).",
  "org.apache.hadoop.conf.Configuration:getProperty(java.lang.String)" : "* Get a system property value if\n   * {@link #restrictSystemProps} does not block this.\n   * @param key property key\n   * @return the value or null if either it is unset or access forbidden.",
  "org.apache.hadoop.fs.shell.PathData:uriToString(java.net.URI,boolean)" : null,
  "org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:build()" : "* Perform the open operation.\n     * Returns a future which, when get() or a chained completion\n     * operation is invoked, will supply the input stream of the file\n     * referenced by the path/path handle.\n     * @return a future to the input stream.\n     * @throws IOException early failure to open\n     * @throws UnsupportedOperationException if the specific operation\n     * is not supported.\n     * @throws IllegalArgumentException if the parameters are not valid.",
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long,long,int)" : "* constructors for har input stream.\n     * @param fs the underlying filesystem\n     * @param p The path in the underlying filesystem\n     * @param start the start position in the part file\n     * @param length the length of valid data in the part file\n     * @param bufsize the buffer size\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:adjustPriorityQueue(org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor)" : null,
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : "* Removes the existing TokenInformation from the SQL database to\n   * invalidate it.\n   * @param ident TokenInformation to remove from the SQL database.",
  "org.apache.hadoop.fs.ChecksumFs:<init>(org.apache.hadoop.fs.AbstractFileSystem)" : null,
  "org.apache.hadoop.util.Options$FSDataInputStreamOption:<init>(org.apache.hadoop.fs.FSDataInputStream)" : null,
  "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:meanStatistics()" : null,
  "org.apache.hadoop.util.Shell$ShellTimeoutTimerTask:<init>(org.apache.hadoop.util.Shell)" : null,
  "org.apache.hadoop.security.UserGroupInformation$LoginParams:<init>()" : null,
  "org.apache.hadoop.io.AbstractMapWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.ha.HAAdmin:parseOpts(java.lang.String,org.apache.commons.cli.Options,java.lang.String[])" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod$1:<init>(java.lang.reflect.Method,java.lang.String)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,int)" : "* Create a mutable integer gauge\n   * @param info  metadata of the metric\n   * @param iVal  initial value\n   * @return a new gauge object",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.util.ReflectionUtils:setContentionTracing(boolean)" : null,
  "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:setShouldLog()" : null,
  "org.apache.hadoop.fs.FileUtil:unTar(java.io.File,java.io.File)" : "* Given a Tar File as input it will untar the file in a the untar directory\n   * passed as the second parameter\n   *\n   * This utility will untar \".tar\" files and \".tar.gz\",\"tgz\" files.\n   *\n   * @param inFile The tar file as input.\n   * @param untarDir The untar directory where to untar the tar file.\n   * @throws IOException an exception occurred.",
  "org.apache.hadoop.fs.FileStatus:isSnapshotEnabled()" : "* Check if directory is Snapshot enabled or not.\n   *\n   * @return true if directory is snapshot enabled",
  "org.apache.hadoop.fs.FilterFileSystem:openFile(org.apache.hadoop.fs.PathHandle)" : null,
  "org.apache.hadoop.fs.shell.Count:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,org.apache.hadoop.tools.TableListing$Justification,boolean)" : "* Add a new field to the Table under construction.\n     *\n     * @param title Field title.\n     * @param justification Right or left justification. Defaults to left.\n     * @param wrap Width at which to auto-wrap the content of the cell.\n     *        Defaults to Integer.MAX_VALUE.\n     * @return This Builder object",
  "org.apache.hadoop.fs.shell.find.Find:<init>()" : "Default constructor for the Find command.",
  "org.apache.hadoop.security.authorize.AccessControlList:removeGroup(java.lang.String)" : "* Remove group from the names of groups allowed for this service.\n   * \n   * @param group\n   *          The group name",
  "org.apache.hadoop.ha.ActiveStandbyElector:quitElection(boolean)" : "* Any service instance can drop out of the election by calling quitElection. \n   * <br>\n   * This will lose any leader status, if held, and stop monitoring of the lock\n   * node. <br>\n   * If the instance wants to participate in election again, then it needs to\n   * call joinElection(). <br>\n   * This allows service instances to take themselves out of rotation for known\n   * impending unavailable states (e.g. long GC pause or software upgrade).\n   * \n   * @param needFence true if the underlying daemon may need to be fenced\n   * if a failover occurs due to dropping out of the election.",
  "org.apache.hadoop.fs.Globber:glob()" : null,
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:monitorHealth()" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:setDelegationTokenSeqNum(int)" : null,
  "org.apache.hadoop.security.Credentials$SerializedFormat:valueOf(int)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:getCodecName()" : null,
  "org.apache.hadoop.fs.StorageType:parseStorageType(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.util.LimitInputStream:<init>(java.io.InputStream,long)" : null,
  "org.apache.hadoop.fs.DelegateToFileSystem:listStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean)" : "* Make a call, passing <code>rpcRequest</code>, to the IPC server defined by\n   * <code>remoteId</code>, returning the rpc respond.\n   *\n   * @param rpcKind - input rpcKind.\n   * @param rpcRequest -  contains serialized method and method parameters\n   * @param remoteId - the target rpc server\n   * @param fallbackToSimpleAuth - set to true or false during this method to\n   *   indicate if a secure client falls back to simple auth\n   * @return the rpc response\n   * Throws exceptions if there are network problems or if the remote code\n   * threw an exception.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.BufferedFSInputStream:read(long,byte[],int,int)" : null,
  "org.apache.hadoop.io.SequenceFile$UncompressedBytes:<init>()" : null,
  "org.apache.hadoop.net.NodeBase:set(java.lang.String,java.lang.String)" : "* set this node's name and location\n   * @param name the (nullable) name -which cannot contain the {@link #PATH_SEPARATOR}\n   * @param location the location",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:incrReceivedBytes(int)" : null,
  "org.apache.hadoop.util.bloom.Key:getBytes()" : "@return byte[] The value of <i>this</i> key.",
  "org.apache.hadoop.fs.Options$CreateOpts$Progress:getValue()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* The src file is on the local disk.  Add it to FS at\n   * the given dst name.\n   * delSrc indicates if the source should be removed",
  "org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:finish()" : null,
  "org.apache.hadoop.util.ShutdownHookManager:<init>()" : null,
  "org.apache.hadoop.io.file.tfile.Utils$Version:compareTo(org.apache.hadoop.io.file.tfile.Utils$Version)" : "* Compare this version with another version.",
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],int[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)" : "* The constructor with all the necessary info.\n   * @param inputBlocks inputBlocks.\n   * @param erasedIndexes the indexes of erased blocks in inputBlocks array\n   * @param outputBlocks outputBlocks.\n   * @param rawDecoder underlying RS decoder for hitchhiker decoding\n   * @param rawEncoder underlying XOR encoder for hitchhiker decoding",
  "org.apache.hadoop.io.ArrayPrimitiveWritable:<init>(java.lang.Class)" : "* Construct an instance of known type but no value yet\n   * for use with type-specific wrapper classes.\n   *\n   * @param componentType componentType.",
  "org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder:getBestComparer()" : "* Returns the Unsafe-using Comparer, or falls back to the pure-Java\n     * implementation if unable to do so.",
  "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh()" : "* Refresh the netgroup cache",
  "org.apache.hadoop.fs.FSInputChecker:read()" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementMinimum(java.lang.String,long)" : null,
  "org.apache.hadoop.security.token.delegation.DelegationKey:write(java.io.DataOutput)" : "",
  "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:<init>(int,org.apache.hadoop.crypto.CipherSuite)" : null,
  "org.apache.hadoop.security.SaslPlainServer$SecurityProvider:<init>()" : null,
  "org.apache.hadoop.util.StringUtils:getFormattedTimeWithDiff(java.lang.String,long,long)" : "* Formats time in ms and appends difference (finishTime - startTime)\n   * as returned by formatTimeDiff().\n   * If finish time is 0, empty string is returned, if start time is 0\n   * then difference is not appended to return value.\n   * @param formattedFinishTime formattedFinishTime to use\n   * @param finishTime finish time\n   * @param startTime start time\n   * @return formatted value.",
  "org.apache.hadoop.fs.RawLocalFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)" : "* Sets the {@link Path}'s last modified time and last access time to\n   * the given valid times.\n   *\n   * @param mtime the modification time to set (only if no less than zero).\n   * @param atime the access time to set (only if no less than zero).\n   * @throws IOException if setting the times fails.",
  "org.apache.hadoop.io.SequenceFile$Writer:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getRecommendedBufferSize()" : null,
  "org.apache.hadoop.io.MapFile$Writer:checkKey(org.apache.hadoop.io.WritableComparable)" : null,
  "org.apache.hadoop.fs.InvalidPathHandleException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.net.NetworkTopology:getNode(java.lang.String)" : "Given a string representation of a node, return its reference\n   * \n   * @param loc\n   *          a path-like string representation of a node\n   * @return a reference to the node; null if the node is not in the tree",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultBlockSize(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.ShutdownHookManager$HookEntry:getHook()" : null,
  "org.apache.hadoop.ipc.Server$FatalRpcServerException:toString()" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.conf.Configuration$Parser:handleStartElement()" : null,
  "org.apache.hadoop.util.RateLimitingFactory:<init>()" : null,
  "org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.BlockLocation[])" : "* Constructor\n   * \n   * @param length a file's length\n   * @param isdir if the path is a directory\n   * @param block_replication the file's replication factor\n   * @param blocksize a file's block size\n   * @param modification_time a file's modification time\n   * @param access_time a file's access time\n   * @param permission a file's permission\n   * @param owner a file's owner\n   * @param group a file's group\n   * @param symlink symlink if the path is a symbolic link\n   * @param path the path's qualified name\n   * @param locations a file's block locations",
  "org.apache.hadoop.fs.FsUrlStreamHandlerFactory:createURLStreamHandler(java.lang.String)" : null,
  "org.apache.hadoop.util.Shell$ShellCommandExecutor:getOutput()" : "Get the output of the shell command.",
  "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig)" : null,
  "org.apache.hadoop.ipc.Server:isRpcInvocation()" : "@return Return true if the invocation was through an RPC.",
  "org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:close()" : null,
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:readBlock(org.apache.hadoop.fs.impl.prefetch.BufferData,boolean,org.apache.hadoop.fs.impl.prefetch.BufferData$State[])" : null,
  "org.apache.hadoop.fs.FilterFileSystem:openFile(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.crypto.OpensslCipher:getLoadingFailureReason()" : null,
  "org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.String,java.lang.Object[])" : null,
  "org.apache.hadoop.fs.FSBuilder:optDouble(java.lang.String,double)" : "* Set optional double parameter for the Builder.\n   *\n   * @param key key.\n   * @param value value.\n   * @return generic type B.\n   * @see #opt(String, String)",
  "org.apache.hadoop.fs.FSDataOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics)" : null,
  "org.apache.hadoop.io.retry.AsyncCallHandler:getAsyncReturn()" : "* @return the async return value from {@link AsyncCallHandler}.\n   * @param <T> T.\n   * @param <R> R.",
  "org.apache.hadoop.fs.AbstractFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)" : "* The specification of this method matches that of\n   * {@link FileContext#getFileLinkStatus(Path)}\n   * except that an UnresolvedLinkException may be thrown if a symlink is  \n   * encountered in the path leading up to the final path component.\n   * If the file system does not support symlinks then the behavior is\n   * equivalent to {@link AbstractFileSystem#getFileStatus(Path)}.\n   *\n   * @param f the path.\n   * @throws AccessControlException access control exception.\n   * @throws FileNotFoundException file not found exception.\n   * @throws UnsupportedFileSystemException UnSupported File System Exception.\n   * @throws IOException raised on errors performing I/O.\n   * @return file status.",
  "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:refreshUserToGroupsMappings()" : null,
  "org.apache.hadoop.util.StopWatch:<init>()" : null,
  "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.erasurecode.ECSchema:hashCode()" : null,
  "org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:hashCode()" : "* Similarly, remoteExceptionToRetry is ignored as part of hashCode since it\n     * does not affect connection failure handling.",
  "org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)" : "* The src files are on the local disk.  Add it to the filesystem at\n   * the given dst name.\n   * delSrc indicates if the source should be removed\n   * @param delSrc whether to delete the src\n   * @param overwrite whether to overwrite an existing file\n   * @param srcs array of paths which are source\n   * @param dst path\n   * @throws IOException IO failure",
  "org.apache.hadoop.io.ArrayFile$Writer:append(org.apache.hadoop.io.Writable)" : "* Append a value to the file.\n     * @param value value.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.SaslRpcClient:saslConnect(org.apache.hadoop.ipc.Client$IpcStreams)" : "* Do client side SASL authentication with server via the given IpcStreams.\n   *\n   * @param ipcStreams ipcStreams.\n   * @return AuthMethod used to negotiate the connection\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.Server:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.net.InetAddress)" : "* Authorize the incoming client connection.\n   * \n   * @param user client user\n   * @param protocolName - the protocol\n   * @param addr InetAddress of incoming connection\n   * @throws AuthorizationException when the client isn't authorized to talk the protocol",
  "org.apache.hadoop.util.SysInfoWindows:getCpuUsagePercentage()" : "{@inheritDoc}",
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:add(java.lang.String,long)" : "* Add sample to a stat metric by name.\n   * @param name  of the metric\n   * @param value of the snapshot to add",
  "org.apache.hadoop.crypto.CryptoInputStream:returnDecryptor(org.apache.hadoop.crypto.Decryptor)" : "Return decryptor to pool",
  "org.apache.hadoop.io.file.tfile.BCFile$Magic:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.ha.ZKFailoverController:initHM()" : null,
  "org.apache.hadoop.fs.FileContext:createMultipartUploader(org.apache.hadoop.fs.Path)" : "* Create a multipart uploader.\n   * @param basePath file path under which all files are uploaded\n   * @return a MultipartUploaderBuilder object to build the uploader\n   * @throws IOException if some early checks cause IO failures.\n   * @throws UnsupportedOperationException if support is checked early.",
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:<init>(org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters)" : "* Constructs an instance of a {@code CachingBlockManager}.\n   *\n   * @param blockManagerParameters params for block manager.\n   * @throws IllegalArgumentException if bufferPoolSize is zero or negative.",
  "org.apache.hadoop.security.alias.CredentialShell$ListCommand:getUsage()" : null,
  "org.apache.hadoop.security.KDiag:warn(java.lang.String,java.lang.String,java.lang.Object[])" : "* Print a message as an warning\n   * @param category error category\n   * @param message format string\n   * @param args list of arguments",
  "org.apache.hadoop.io.SequenceFile$Writer$StreamOption:<init>(org.apache.hadoop.fs.FSDataOutputStream)" : null,
  "org.apache.hadoop.fs.FileSystem:areSymlinksEnabled()" : null,
  "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:diskCheckFailed()" : "* Increase the failure count and update the last failure timestamp.",
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:shutdown()" : null,
  "org.apache.hadoop.ipc.Server:buildNegotiateResponse(java.util.List)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:fatalError(java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:incrementReadOps(int)" : "* Increment the number of read operations.\n     * @param count number of read operations",
  "org.apache.hadoop.security.http.CrossOriginFilter:getAllowedMethodsHeader()" : null,
  "org.apache.hadoop.ipc.Client$ConnectionId:getDoPing()" : null,
  "org.apache.hadoop.ha.ZKFCRpcServer:start()" : null,
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:permission(org.apache.hadoop.fs.permission.FsPermission)" : "* Set permission for the file.\n   *\n   * @param perm permission.\n   * @return B Generics Type.",
  "org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean,boolean,java.util.List)" : "Return the string representation of the object in the output format.\n   * if qOption is false, output directory count, file count, and content size;\n   * if qOption is true, output quota and remaining quota as well.\n   * if hOption is false, file sizes are returned in bytes\n   * if hOption is true, file sizes are returned in human readable\n   * if tOption is true, display the quota by storage types\n   * if tOption is false, same logic with #toString(boolean,boolean)\n   * if xOption is false, output includes the calculation from snapshots\n   * if xOption is true, output excludes the calculation from snapshots\n   *\n   * @param qOption a flag indicating if quota needs to be printed or not\n   * @param hOption a flag indicating if human readable output is to be used\n   * @param tOption a flag indicating if display quota by storage types\n   * @param xOption a flag indicating if calculation from snapshots is to be\n   *                included in the output\n   * @param types Storage types to display\n   * @return the string representation of the object",
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:decompress(byte[],int,int)" : null,
  "org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:expandArgument(java.lang.String)" : null,
  "org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text,int,int)" : "* Read one line from the InputStream into the given Text.\n   *\n   * @param str the object to store the given line (without newline)\n   * @param maxLineLength the maximum number of bytes to store into str;\n   *  the rest of the line is silently discarded.\n   * @param maxBytesToConsume the maximum number of bytes to consume\n   *  in this call.  This is only a hint, because if the line cross\n   *  this threshold, we allow it to happen.  It can overshoot\n   *  potentially by as much as one buffer length.\n   *\n   * @return the number of bytes read including the (longest) newline\n   * found.\n   *\n   * @throws IOException if the underlying stream throws",
  "org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)" : "* Create the named file with write-progress reporter.\n     * @deprecated Use \n     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} \n     *   instead.\n     * @param fs input filesystem.\n     * @param conf input configuration.\n     * @param name input name.\n     * @param keyClass input keyClass.\n     * @param valClass input valClass.\n     * @param bufferSize input bufferSize.\n     * @param replication input replication.\n     * @param blockSize input blockSize.\n     * @param progress input progress.\n     * @param metadata input metadata.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getLinkType()" : null,
  "org.apache.hadoop.ipc.RpcClientUtil:convertProtocolSignatureProtos(java.util.List)" : null,
  "org.apache.hadoop.crypto.random.OpensslSecureRandom:next(int)" : "* Generates an integer containing the user-specified number of\n   * random bits (right justified, with leading zeros).\n   *\n   * @param numBits number of random bits to be generated, where\n   * 0 {@literal <=} <code>numBits</code> {@literal <=} 32.\n   *\n   * @return int an <code>int</code> containing the user-specified number\n   * of random bits (right justified, with leading zeros).",
  "org.apache.hadoop.fs.viewfs.ViewFs:getStoragePolicy(org.apache.hadoop.fs.Path)" : "* Retrieve the storage policy for a given file or directory.\n   *\n   * @param src file or directory path.\n   * @return storage policy for give file.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.ObjectWritable:getConf()" : null,
  "org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter,boolean)" : "* Filesystem constructor for use by {@link GlobBuilder}.\n   * @param fs filesystem\n   * @param pathPattern path pattern\n   * @param filter optional filter\n   * @param resolveSymlinks should symlinks be resolved.",
  "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:<init>(int)" : null,
  "org.apache.hadoop.net.InnerNodeImpl:getNumOfChildren()" : "@return the number of children this node has.",
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:finish()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:convertToByteArrayState()" : "* Convert to a ByteArrayDecodingState when it's backed by on-heap arrays.",
  "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:create(org.apache.hadoop.ipc.RetryCache)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:<init>(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir,long,org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.fs.viewfs.InodeTree,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.HarFileSystem:archivePath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.LightWeightResizableGSet:remove(java.lang.Object)" : null,
  "org.apache.hadoop.util.hash.MurmurHash:getInstance()" : null,
  "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:run()" : null,
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:setChanged(boolean)" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination:processPathArgument(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>()" : "* Creates a new compressor with the default compression level.\n   * Compressed data will be generated in ZLIB format.",
  "org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo:getSource()" : null,
  "org.apache.hadoop.fs.permission.AclEntry:parseAclEntry(java.lang.String,boolean)" : "* Parses a string representation of an ACL into a AclEntry object.<br>\n   * The expected format of ACL entries in the string parameter is the same\n   * format produced by the {@link #toStringStable()} method.\n   * \n   * @param aclStr\n   *          String representation of an ACL.<br>\n   *          Example: \"user:foo:rw-\"\n   * @param includePermission\n   *          for setAcl operations this will be true. i.e. Acl should include\n   *          permissions.<br>\n   *          But for removeAcl operation it will be false. i.e. Acl should not\n   *          contain permissions.<br>\n   *          Example: \"user:foo,group:bar,mask::\"\n   * @return Returns an {@link AclEntry} object",
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)" : null,
  "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,int)" : null,
  "org.apache.hadoop.io.WritableComparator:compare(byte[],int,int,byte[],int,int)" : "Optimization hook.  Override this to make SequenceFile.Sorter's scream.\n   *\n   * <p>The default implementation reads the data into two {@link\n   * WritableComparable}s (using {@link\n   * Writable#readFields(DataInput)}, then calls {@link\n   * #compare(WritableComparable,WritableComparable)}.",
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:getCodec()" : null,
  "org.apache.hadoop.io.EnumSetWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.util.MachineList$InetAddressFactory:getByName(java.lang.String)" : null,
  "org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)" : "* Make a copy of a writable object using serialization to a buffer.\n   *\n   * @param <T> Generics Type T.\n   * @param orig The object to copy\n   * @param conf input Configuration.\n   * @return The copied object",
  "org.apache.hadoop.fs.FSInputChecker:readChecksumChunk(byte[],int,int)" : null,
  "org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand:execute()" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayPeriodMillis(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.http.ProfileOutputServlet:sanitize(java.lang.String)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:mkdirsWithOptionalPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.util.WeakReferenceMap:noteLost(java.lang.Object)" : "* Notify the reference lost callback.\n   * @param key key of lost reference",
  "org.apache.hadoop.ipc.Server$ConnectionManager:scheduleIdleScanTask()" : null,
  "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:<init>(int,int)" : null,
  "org.apache.hadoop.io.MapFile:fix(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.conf.Configuration)" : "* This method attempts to fix a corrupt MapFile by re-creating its index.\n   * @param fs filesystem\n   * @param dir directory containing the MapFile data and index\n   * @param keyClass key class (has to be a subclass of Writable)\n   * @param valueClass value class (has to be a subclass of Writable)\n   * @param dryrun do not perform any changes, just report what needs to be done\n   * @param conf configuration.\n   * @return number of valid entries in this MapFile, or -1 if no fixing was needed\n   * @throws Exception Exception.",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String)" : "* Open a file.\n   * <p>\n   * If the WrappedIO class is found, use it.\n   * <p>\n   * If not, falls back to the classic {@code fs.open(Path)} call.\n   * @param fs filesystem\n   * @param status file status\n   * @param readPolicies read policy to use\n   * @return the input stream\n   * @throws IOException any IO failure.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:measureDurationOfInvocation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)" : "* Given an IOException raising callable/lambda expression,\n   * execute it and update the relevant statistic,\n   * returning the measured duration.\n   *\n   * {@link #trackDurationOfInvocation(DurationTrackerFactory, String, InvocationRaisingIOE)}\n   * with the duration returned for logging etc.; added as a new\n   * method to avoid linking problems with any code calling the existing\n   * method.\n   *\n   * @param factory factory of duration trackers\n   * @param statistic statistic key\n   * @param input input callable.\n   * @return the duration of the operation, as measured by the duration tracker.\n   * @throws IOException IO failure.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:toStringStable()" : null,
  "org.apache.hadoop.util.DataChecksum:getBytesPerChecksum()" : null,
  "org.apache.hadoop.io.SortedMapWritable:containsKey(java.lang.Object)" : null,
  "org.apache.hadoop.metrics2.util.SampleStat:copyTo(org.apache.hadoop.metrics2.util.SampleStat)" : "* Copy the values to other (saves object creation and gc.)\n   * @param other the destination to hold our values",
  "org.apache.hadoop.security.SaslPropertiesResolver:getDefaultProperties()" : "* The default Sasl Properties read from the configuration\n   * @return sasl Properties",
  "org.apache.hadoop.util.Shell:getCheckProcessIsAliveCommand(java.lang.String)" : "* Return a command for determining if process with specified pid is alive.\n   * @param pid process ID\n   * @return a <code>kill -0</code> command or equivalent",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeysMetadata(java.lang.String[])" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:metrics()" : null,
  "org.apache.hadoop.util.Time:monotonicNow()" : "* Current time from some arbitrary time base in the past, counting in\n   * milliseconds, and not affected by settimeofday or similar system clock\n   * changes.  This is appropriate to use when computing how much longer to\n   * wait for an interval to expire.\n   * This function can return a negative value and it must be handled correctly\n   * by callers. See the documentation of System#nanoTime for caveats.\n   * @return a monotonic clock that counts in milliseconds.",
  "org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet,org.apache.hadoop.fs.viewfs.FsGetter)" : "* Creates a new Nfly instance.\n   *\n   * @param uris the list of uris in the mount point\n   * @param conf configuration object\n   * @param minReplication minimum copies to commit a write op\n   * @param nflyFlags modes such readMostRecent\n   * @param fsGetter to get the file system instance with the given uri\n   * @throws IOException",
  "org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesReadByDistance(int,long)" : "* Increment the bytes read by the network distance in the statistics\n     * In the common network topology setup, distance value should be an even\n     * number such as 0, 2, 4, 6. To make it more general, we group distance\n     * by {1, 2}, {3, 4} and {5 and beyond} for accounting.\n     * @param distance the network distance\n     * @param newBytes the additional bytes read",
  "org.apache.hadoop.service.launcher.ServiceLauncher:getService()" : "* Get the service.\n   *\n   * Null until\n   * {@link #coreServiceLaunch(Configuration, Service, List, boolean, boolean)}\n   * has completed.\n   * @return the service",
  "org.apache.hadoop.fs.shell.find.BaseExpression:getFileStatus(org.apache.hadoop.fs.shell.PathData,int)" : "* Returns the {@link FileStatus} from the {@link PathData} item. If the\n   * current options require links to be followed then the returned file status\n   * is that of the linked file.\n   *\n   * @param item\n   *          PathData\n   * @param depth\n   *          current depth in the process directories\n   * @return FileStatus\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.Options$BooleanOption:<init>(boolean)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:stopThreads()" : null,
  "org.apache.hadoop.fs.shell.Truncate:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:getMetricsServers()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getContentSummary(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.Classpath:terminate(int,java.lang.String)" : "* Prints a message to stderr and exits with a status code.\n   *\n   * @param status exit code\n   * @param msg message",
  "org.apache.hadoop.io.IOUtils:cleanupWithLogger(org.slf4j.Logger,java.io.Closeable[])" : "* Close the Closeable objects and <b>ignore</b> any {@link Throwable} or\n   * null pointers. Must only be used for cleanup in exception handlers.\n   *\n   * @param logger the log to record problems to at debug level. Can be null.\n   * @param closeables the objects to close",
  "org.apache.hadoop.ipc.DecayRpcScheduler:addCost(java.lang.Object,long)" : "* Adjust the stored cost for a given identity.\n   *\n   * @param identity the identity of the user whose cost should be adjusted\n   * @param costDelta the cost to add for the given identity",
  "org.apache.hadoop.util.functional.Tuples$Tuple:<init>(java.lang.Object,java.lang.Object)" : null,
  "org.apache.hadoop.ipc.CallerContext:toString()" : null,
  "org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:isRack()" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:getPosition()" : "* @return Return the current byte position in the input file.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getErasedIndexes(org.apache.hadoop.io.erasurecode.ECBlock[])" : "* Get indexes of erased blocks from inputBlocks\n   * @param inputBlocks inputBlocks.\n   * @return indexes of erased blocks from inputBlocks",
  "org.apache.hadoop.metrics2.MetricStringBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)" : null,
  "org.apache.hadoop.fs.LocatedFileStatus:<init>()" : null,
  "org.apache.hadoop.fs.statistics.DurationTrackerFactory:trackDuration(java.lang.String)" : "* Initiate a duration tracking operation by creating/returning\n   * an object whose {@code close()} call will\n   * update the statistics.\n   * The expected use is within a try-with-resources clause.\n   * @param key statistic key\n   * @return an object to close after an operation completes.",
  "org.apache.hadoop.security.KerberosAuthException:setKeytabFile(java.lang.String)" : null,
  "org.apache.hadoop.util.bloom.BloomFilter:not()" : null,
  "org.apache.hadoop.io.retry.RetryUtils:getDefaultRetryPolicy(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String,java.lang.String)" : "* Return the default retry policy set in conf.\n   * \n   * If the value retryPolicyEnabledKey is set to false in conf,\n   * use TRY_ONCE_THEN_FAIL.\n   * \n   * Otherwise, get the MultipleLinearRandomRetry policy specified in the conf\n   * and then\n   * (1) use multipleLinearRandomRetry for\n   *     - remoteExceptionToRetry, or\n   *     - IOException other than RemoteException, or\n   *     - ServiceException; and\n   * (2) use TRY_ONCE_THEN_FAIL for\n   *     - non-remoteExceptionToRetry RemoteException, or\n   *     - non-IOException.\n   *     \n   *\n   * @param conf configuration.\n   * @param retryPolicyEnabledKey     conf property key for enabling retry\n   * @param defaultRetryPolicyEnabled default retryPolicyEnabledKey conf value \n   * @param retryPolicySpecKey        conf property key for retry policy spec\n   * @param defaultRetryPolicySpec    default retryPolicySpecKey conf value\n   * @param remoteExceptionToRetry    The particular RemoteException to retry\n   * @return the default retry policy.",
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:get(int)" : "* Gets the block having the given {@code blockNumber}.\n   *\n   * @throws IllegalArgumentException if blockNumber is negative.",
  "org.apache.hadoop.security.LdapGroupsMapping:lookupPosixGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext)" : "* Look up groups using posixGroups semantics. Use posix gid/uid to find\n   * groups of the user.\n   *\n   * @param result the result object returned from the prior user lookup.\n   * @param c the context object of the LDAP connection.\n   * @return an object representing the search result.\n   *\n   * @throws NamingException if the server does not support posixGroups\n   * semantics.",
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues4()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockData:getFileSize()" : "* Gets the size of the associated file.\n   * @return the size of the associated file.",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:getPmdkLibPath()" : null,
  "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:add(java.lang.Object,java.lang.reflect.Method)" : "Add {@link MutableMetric} for a method annotated with {@link Metric}",
  "org.apache.hadoop.util.Progress:phase()" : "* Returns the current sub-node executing.\n   * @return Progress.",
  "org.apache.hadoop.ipc.Server$Call:getHostInetAddress()" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:<init>(int)" : "* Creates a new compressor.\n   *\n   * @param directBufferSize size of the direct buffer to be used.",
  "org.apache.hadoop.fs.AbstractFileSystem:getBaseUri(java.net.URI)" : null,
  "org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(java.io.File)" : "* Create the directory if it doesn't exist and check that dir is\n   * readable, writable and executable. Perform some disk IO to\n   * ensure that the disk is usable for writes.\n   *\n   * @param dir dir.\n   * @throws DiskErrorException disk problem.",
  "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:isTracked(java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.Compression:getCompressionAlgorithmByName(java.lang.String)" : null,
  "org.apache.hadoop.fs.Options$CreateOpts$BlockSize:getValue()" : null,
  "org.apache.hadoop.io.ObjectWritable$NullInstance:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.security.token.DtFileOperations:printTokenFile(java.io.File,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration,java.io.PrintStream)" : "Print out a Credentials file from the local filesystem.\n   *  @param tokenFile a local File object.\n   *  @param alias print only tokens matching alias (null matches all).\n   *  @param conf Configuration object passed along.\n   *  @param out print to this stream.\n   *  @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ha.PowerShellFencer:buildPSScript(java.lang.String,java.lang.String)" : "* Build a PowerShell script to kill a java.exe process in a remote machine.\n   *\n   * @param processName Name of the process to kill. This is an attribute in\n   *                    CommandLine.\n   * @param host Host where the process is.\n   * @return Path of the PowerShell script.",
  "org.apache.hadoop.util.concurrent.HadoopExecutors:newSingleThreadExecutor(java.util.concurrent.ThreadFactory)" : null,
  "org.apache.hadoop.io.Text:hashCode()" : null,
  "org.apache.hadoop.conf.Configuration:getResource(java.lang.String)" : "* Get the {@link URL} for the named resource.\n   * \n   * @param name resource name.\n   * @return the url for the named resource.",
  "org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings()" : null,
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:setDelegationTokenSeqNum(int)" : "* Updates the value of the last reserved sequence number.\n   * @param seqNum Value to update the sequence number to.",
  "org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)" : "* Construct an IPC client with the default SocketFactory.\n   * @param valueClass input valueClass.\n   * @param conf input Configuration.",
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:analyze(java.lang.StringBuilder)" : null,
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongGauge(java.lang.String,java.util.concurrent.atomic.AtomicLong)" : "* Add a gauge statistic to dynamically return the\n   * latest value of the source.\n   * @param key key of this statistic\n   * @param source atomic long gauge\n   * @return the builder.",
  "org.apache.hadoop.fs.impl.AbstractMultipartUploader:<init>(org.apache.hadoop.fs.Path)" : "* Instantiate.\n   * @param basePath base path",
  "org.apache.hadoop.ha.HealthMonitor:isHealthCheckFailedException(java.lang.Throwable)" : null,
  "org.apache.hadoop.ipc.ClientCache:clearCache()" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibFactory:loadNativeZLib()" : "* Load native library and set the flag whether to use native library. The\n   * method is also used for reset the flag modified by setNativeZlibLoaded",
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_maximums(java.io.Serializable)" : "* Get the maximums of an IOStatisticsSnapshot.\n   * @param source source of statistics.\n   * @return the map of maximums.",
  "org.apache.hadoop.io.BloomMapFile$Writer:initBloomFilter(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:newInstance(java.lang.Class,java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Create an object for the given class and initialize it from conf.\n   * @param theClass class of which an object is created\n   * @param conf Configuration\n   * @return a new object",
  "org.apache.hadoop.metrics2.lib.Interns:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)" : "* Get a metrics tag.\n   * @param info  of the tag\n   * @param value of the tag\n   * @return an interned metrics tag",
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.ha.ShellCommandFencer:addTargetInfoAsEnvVars(org.apache.hadoop.ha.HAServiceTarget,java.util.Map)" : "* Add information about the target to the the environment of the\n   * subprocess.\n   * \n   * @param target\n   * @param environment",
  "org.apache.hadoop.util.IntrusiveCollection:remove(java.lang.Object)" : null,
  "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:<init>(org.apache.hadoop.fs.statistics.DurationTracker,org.apache.hadoop.fs.statistics.DurationTracker)" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:counters()" : null,
  "org.apache.hadoop.fs.shell.find.FindOptions:setIn(java.io.InputStream)" : "* Sets the input stream to be used.\n   *\n   * @param in input stream to be used",
  "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:addWriteFileLatency(long)" : "* Add the file write latency to {@link MutableQuantiles} metrics.\n   *\n   * @param writeLatency file write latency in microseconds",
  "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getTarget()" : null,
  "org.apache.hadoop.net.unix.DomainSocketWatcher:kick()" : "* Wake up the DomainSocketWatcher thread.",
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getPrefetched(int)" : null,
  "org.apache.hadoop.ha.FailoverController:failover(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean,boolean)" : "* Failover from service 1 to service 2. If the failover fails\n   * then try to failback.\n   *\n   * @param fromSvc currently active service\n   * @param toSvc service to make active\n   * @param forceFence to fence fromSvc even if not strictly necessary\n   * @param forceActive try to make toSvc active even if it is not ready\n   * @throws FailoverFailedException if the failover fails",
  "org.apache.hadoop.util.LightWeightCache:isExpired(org.apache.hadoop.util.LightWeightCache$Entry,long)" : null,
  "org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecByClassName(java.lang.String)" : "* Find the relevant compression codec for the codec's canonical class name.\n   * @param classname the canonical class name of the codec\n   * @return the codec object",
  "org.apache.hadoop.util.Sets:newHashSet(java.lang.Object[])" : "* Creates a <i>mutable</i> {@code HashSet} instance initially containing\n   * the given elements.\n   *\n   * <p><b>Note:</b> if elements are non-null and won't be added or removed\n   * after this point, use ImmutableSet#of() or ImmutableSet#copyOf(Object[])\n   * instead. If {@code E} is an {@link Enum} type, use\n   * {@link EnumSet#of(Enum, Enum[])} instead. Otherwise, strongly consider\n   * using a {@code LinkedHashSet} instead, at the cost of increased memory\n   * footprint, to get deterministic iteration behavior.</p>\n   *\n   * <p>This method is just a small convenience, either for\n   * {@code newHashSet(}{@link Arrays#asList}{@code (...))}, or for creating an\n   * empty set then calling {@link Collections#addAll}.</p>\n   *\n   * @param <E> Generics Type E.\n   * @param elements the elements that the set should contain.\n   * @return a new, empty thread-safe {@code Set}",
  "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int)" : "* Create an FSDataOutputStream at the indicated Path.\n   * @param f the file to create\n   * @param overwrite if a path with this name already exists, then if true,\n   *   the file will be overwritten, and if false an error will be thrown.\n   * @param bufferSize the size of the buffer to be used.\n   * @throws IOException IO failure\n   * @return output stream.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfOperation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE)" : "* Given an IOException raising callable/lambda expression,\n   * return a new one which wraps the inner and tracks\n   * the duration of the operation, including whether\n   * it passes/fails.\n   * @param factory factory of duration trackers\n   * @param statistic statistic key\n   * @param input input callable.\n   * @param <B> return type.\n   * @return a new callable which tracks duration and failure.",
  "org.apache.hadoop.io.compress.PassthroughCodec:<init>()" : null,
  "org.apache.hadoop.fs.shell.Display$Cat:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.ipc.Server$Connection:getHostAddress()" : null,
  "org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingSampleCount()" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:minimums()" : null,
  "org.apache.hadoop.security.UserGroupInformation:logoutUserFromKeytab()" : "* Log the current user out who previously logged in using keytab.\n   * This method assumes that the user logged in by calling\n   * {@link #loginUserFromKeytab(String, String)}.\n   *\n   * @throws IOException raised on errors performing I/O.\n   * @throws KerberosAuthException if a failure occurred in logout,\n   * or if the user did not log in by invoking loginUserFromKeyTab() before.",
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:init(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoots(boolean)" : "* Get all the trash roots for current user or all users.\n   *\n   * When FORCE_INSIDE_MOUNT_POINT is set to true, we also return trash roots\n   * under the root of each mount point, with their viewFS paths.\n   *\n   * @param allUsers return trash roots for all users if true.\n   * @return all Trash root directories.",
  "org.apache.hadoop.io.SequenceFile$Reader:getDeserializer(org.apache.hadoop.io.serializer.SerializationFactory,java.lang.Class)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:getServerRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind)" : null,
  "org.apache.hadoop.io.FloatWritable:compareTo(org.apache.hadoop.io.FloatWritable)" : "Compares two FloatWritables.",
  "org.apache.hadoop.service.ServiceStateException:convert(java.lang.String,java.lang.Throwable)" : "* Convert any exception into a {@link RuntimeException}.\n   * If the caught exception is already of that type, it is typecast to a\n   * {@link RuntimeException} and returned.\n   *\n   * All other exception types are wrapped in a new instance of\n   * {@code ServiceStateException}.\n   * @param text text to use if a new exception is created\n   * @param fault exception or throwable\n   * @return a {@link RuntimeException} to rethrow",
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkPositiveInteger(long,java.lang.String)" : "* Validates that the given integer argument is not zero or negative.\n   * @param value the argument value to validate\n   * @param argName the name of the argument being validated.",
  "org.apache.hadoop.io.ShortWritable:write(java.io.DataOutput)" : "write short value",
  "org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)" : null,
  "org.apache.hadoop.util.HeapSort:sort(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.fs.FileContext:isSameFS(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Are qualSrc and qualDst of the same file system?\n   * @param qualPath1 - fully qualified path\n   * @param qualPath2 - fully qualified path\n   * @return is same fs true,not false.",
  "org.apache.hadoop.ha.ZKFCRpcServer:getAddress()" : null,
  "org.apache.hadoop.fs.FileContext:setAcl(org.apache.hadoop.fs.Path,java.util.List)" : "* Fully replaces ACL of files and directories, discarding all existing\n   * entries.\n   *\n   * @param path Path to modify\n   * @param aclSpec List{@literal <}AclEntry{@literal >} describing\n   * modifications, must include entries for user, group, and others for\n   * compatibility with permission bits.\n   * @throws IOException if an ACL could not be modified",
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:meanStatistics()" : null,
  "org.apache.hadoop.util.SysInfoWindows:getSystemInfoInfoFromShell()" : null,
  "org.apache.hadoop.metrics2.source.JvmMetrics:getMemoryUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder)" : null,
  "org.apache.hadoop.util.NativeCodeLoader:isNativeCodeLoaded()" : "* Check if native-hadoop code is loaded for this platform.\n   * \n   * @return <code>true</code> if native-hadoop is loaded, \n   *         else <code>false</code>",
  "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket()" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:requestsActive()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:removeDefaultAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getCallQueue()" : "* Fetch the current call queue from the weak reference delegate. If there\n     * is no delegate, or the delegate is empty, this will return null.",
  "org.apache.hadoop.util.SysInfoWindows:getCumulativeCpuTime()" : "{@inheritDoc}",
  "org.apache.hadoop.fs.viewfs.ChRootedFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.io.SetFile$Reader:next(org.apache.hadoop.io.WritableComparable)" : "* Read the next key in a set into <code>key</code>.\n     *\n     * @param key input key.\n     * @return Returns true if such a key exists\n     *    and false when at the end of the set.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.UTF8ByteArrayUtils:findNthByte(byte[],byte,int)" : "* Find the nth occurrence of the given byte b in a UTF-8 encoded string\n   * @param utf a byte array containing a UTF-8 encoded string\n   * @param b the byte to find\n   * @param n the desired occurrence of the given byte\n   * @return position that nth occurrence of the given byte if exists; otherwise -1",
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:deleteKey(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.lib.MutableQuantiles$RolloverSample:run()" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:getHomeDirectory()" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:<init>(long,long,long)" : null,
  "org.apache.hadoop.io.wrappedio.WrappedIO:byteBufferPositionedReadable_readFullyAvailable(java.io.InputStream)" : "* Probe to see if the input stream is an instance of ByteBufferPositionedReadable.\n   * If the stream is an FSDataInputStream, the wrapped stream is checked.\n   * @param in input stream\n   * @return true if the stream implements the interface (including a wrapped stream)\n   * and that it declares the stream capability.",
  "org.apache.hadoop.io.ObjectWritable:get()" : "* Return the instance, or null if none.\n   * @return the instance, or null if none.",
  "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:getRandom()" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:getData()" : "* Get all statistics data.\n     * MR or other frameworks can use the method to get all statistics at once.\n     * @return the StatisticsData",
  "org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision)" : null,
  "org.apache.hadoop.fs.store.EtagChecksum:getBytes()" : null,
  "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:init(java.lang.Class)" : "* Initialize the metrics for JMX with protocol methods\n   * @param protocol the protocol class",
  "org.apache.hadoop.util.LightWeightResizableGSet:getIterator(java.util.function.Consumer)" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(double,java.lang.String,double,double)" : "* Validates that the given value is within the given range of values.\n   * @param value the value to check.\n   * @param valueName the name of the argument.\n   * @param minValueInclusive inclusive lower limit for the value.\n   * @param maxValueInclusive inclusive upper limit for the value.",
  "org.apache.hadoop.util.DirectBufferPool:returnBuffer(java.nio.ByteBuffer)" : "* Return a buffer into the pool. After being returned,\n   * the buffer may be recycled, so the user must not\n   * continue to use it in any way.\n   * @param buf the buffer to return",
  "org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : null,
  "org.apache.hadoop.util.Lists:newArrayListWithExpectedSize(int)" : "* Creates an {@code ArrayList} instance to hold {@code estimatedSize}\n   * elements, <i>plus</i> an unspecified amount of padding;\n   * you almost certainly mean to call {@link\n   * #newArrayListWithCapacity} (see that method for further advice on usage).\n   *\n   * @param estimatedSize an estimate of the eventual {@link List#size()}\n   *     of the new list.\n   * @return a new, empty {@code ArrayList}, sized appropriately to hold the\n   *     estimated number of elements.\n   * @throws IllegalArgumentException if {@code estimatedSize} is negative.\n   *\n   * @param <E> Generics Type E.",
  "org.apache.hadoop.fs.BufferedFSInputStream:<init>(org.apache.hadoop.fs.FSInputStream,int)" : "* Creates a <code>BufferedFSInputStream</code>\n   * with the specified buffer size,\n   * and saves its  argument, the input stream\n   * <code>in</code>, for later use.  An internal\n   * buffer array of length  <code>size</code>\n   * is created and stored in <code>buf</code>.\n   *\n   * @param   in     the underlying input stream.\n   * @param   size   the buffer size.\n   * @exception IllegalArgumentException if size {@literal <=} 0.",
  "org.apache.hadoop.ipc.Server$Responder:incPending()" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Compressor:setInput(byte[],int,int)" : "* Sets input data for compression.\n   * This should be called whenever #needsInput() returns\n   * <code>true</code> indicating that more input data is required.\n   *\n   * @param b   Input data\n   * @param off Start offset\n   * @param len Length",
  "org.apache.hadoop.fs.shell.Command:exitCodeForError()" : "* The exit code to be returned if any errors occur during execution.\n   * This method is needed to account for the inconsistency in the exit\n   * codes returned by various commands.\n   * @return a non-zero exit code",
  "org.apache.hadoop.conf.StorageUnit$1:toPBs(double)" : null,
  "org.apache.hadoop.util.ApplicationClassLoader:<init>(java.net.URL[],java.lang.ClassLoader,java.util.List)" : null,
  "org.apache.hadoop.util.StringUtils:getTrimmedStringCollection(java.lang.String,java.lang.String)" : "* Returns a collection of strings, trimming leading and trailing whitespace\n   * on each value. Duplicates are not removed.\n   *\n   * @param str\n   *          String separated by delim.\n   * @param delim\n   *          Delimiter to separate the values in str.\n   * @return Collection of string values.",
  "org.apache.hadoop.io.SequenceFile$Reader:getMetadata()" : "* Returns the metadata object of the file.\n     * @return metadata.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:getThreadSpecificIOStatisticsContext(long)" : "* Get thread ID specific IOStatistics values if\n   * statistics are enabled and the thread ID is in the map.\n   * @param testThreadId thread ID.\n   * @return IOStatisticsContext if found in the map.",
  "org.apache.hadoop.crypto.CryptoInputStream:cleanDecryptorPool()" : null,
  "org.apache.hadoop.fs.shell.CommandFactory:addClass(java.lang.Class,java.lang.String[])" : "* Register the given class as handling the given list of command\n   * names.\n   * @param cmdClass the class implementing the command names\n   * @param names one or more command names that will invoke this class",
  "org.apache.hadoop.ipc.ProtobufRpcEngine2:getAsyncReturnMessage()" : null,
  "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheCleared()" : "* One cache cleared",
  "org.apache.hadoop.service.launcher.ServiceLauncher:instantiateService(org.apache.hadoop.conf.Configuration)" : "* @return Instantiate the service defined in {@code serviceClassName}.\n   *\n   * Sets the {@code configuration} field\n   * to the the value of {@code conf},\n   * and the {@code service} field to the service created.\n   *\n   * @param conf configuration to use",
  "org.apache.hadoop.security.UserGroupInformation:<init>(javax.security.auth.Subject)" : "* Create a UserGroupInformation for the given subject.\n   * This does not change the subject or acquire new credentials.\n   *\n   * The creator of subject is responsible for renewing credentials.\n   * @param subject the user's subject",
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:buildStatic()" : "* Returns the first valid implementation as a StaticMethod or throws a\n     * RuntimeException if there is none.\n     * @return a {@link StaticMethod} with a valid implementation\n     * @throws IllegalStateException if the method is not static\n     * @throws RuntimeException if no implementation was found",
  "org.apache.hadoop.fs.statistics.MeanStatistic:<init>(org.apache.hadoop.fs.statistics.MeanStatistic)" : "* Create from another statistic.\n   * @param that source",
  "org.apache.hadoop.util.StringUtils:popOptionWithArgument(java.lang.String,java.util.List)" : "* From a list of command-line arguments, remove both an option and the \n   * next argument.\n   *\n   * @param name  Name of the option to remove.  Example: -foo.\n   * @param args  List of arguments.\n   * @return      null if the option was not found; the value of the \n   *              option otherwise.\n   * @throws IllegalArgumentException if the option's argument is not present",
  "org.apache.hadoop.fs.store.ByteBufferInputStream:read()" : null,
  "org.apache.hadoop.fs.FileSystem:checkPath(org.apache.hadoop.fs.Path)" : "* Check that a Path belongs to this FileSystem.\n   *\n   * The base implementation performs case insensitive equality checks\n   * of the URIs' schemes and authorities. Subclasses may implement slightly\n   * different checks.\n   * @param path to check\n   * @throws IllegalArgumentException if the path is not considered to be\n   * part of this FileSystem.\n   *",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(long)" : "* Advance cursor by n positions within the block.\n       * \n       * @param n\n       *          Number of key-value pairs to skip in block.\n       * @throws IOException",
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSources()" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:shouldRetry(org.apache.zookeeper.KeeperException$Code)" : null,
  "org.apache.hadoop.util.hash.MurmurHash:hash(byte[],int,int,int)" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:read(byte[],int,int)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:getCompressionName()" : null,
  "org.apache.hadoop.fs.shell.Display$AvroFileInputStream:read()" : "* Read a single byte from the stream.",
  "org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:cedeActive(int)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)" : "* The specification of this method matches that of\n   * {@link FileContext#listStatus(Path)} except that Path f must be for this\n   * file system.\n   *\n   * @param f path.\n   * @throws AccessControlException access control exception.\n   * @throws FileNotFoundException file not found exception.\n   * @throws UnresolvedLinkException unresolved link exception.\n   * @throws IOException raised on errors performing I/O.\n   * @return FileStatus Iterator.",
  "org.apache.hadoop.fs.FileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : "* Delete a snapshot of a directory.\n   * @param path  The directory that the to-be-deleted snapshot belongs to\n   * @param snapshotName The name of the snapshot\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:<init>()" : null,
  "org.apache.hadoop.http.HttpServer2:setThreads(int,int)" : "* Set the min, max number of worker threads (simultaneous connections).\n   *\n   * @param min min.\n   * @param max max.",
  "org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:getCurrentBuffersCount(boolean)" : "* Get current buffers count in the pool.\n   * @param isDirect whether we want to count the heap or direct buffers.\n   * @return count of buffers.",
  "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getDescription()" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:createPersistentNode(java.lang.String)" : null,
  "org.apache.hadoop.util.InstrumentedReadLock:startLockTiming()" : "* Starts timing for the instrumented read lock.\n   * It records the time to ThreadLocal.",
  "org.apache.hadoop.metrics2.util.MetricsCache$Record:getMetricInstance(java.lang.String)" : "* Lookup a metric instance\n     * @param key name of the metric\n     * @return the metric instance",
  "org.apache.hadoop.fs.FileStatus:getReplication()" : "* Get the replication factor of a file.\n   * @return the replication factor of a file.",
  "org.apache.hadoop.ipc.RetryCache$CacheEntry:isSuccess()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferData:throwIfStateIncorrect(org.apache.hadoop.fs.impl.prefetch.BufferData$State[])" : "* Helper that asserts the current state is one of the expected values.\n   *\n   * @param states the collection of allowed states.\n   *\n   * @throws IllegalArgumentException if states is null.",
  "org.apache.hadoop.fs.QuotaUsage:getSpaceConsumed()" : "* Return (disk) space consumed.\n   *\n   * @return space consumed.",
  "org.apache.hadoop.fs.permission.FsPermission:getCachePoolDefault()" : "* Get the default permission for cache pools.\n   *\n   * @return CachePoolDefault FsPermission.",
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:duration()" : null,
  "org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.Class[])" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:getACL(java.lang.String)" : "* Get ACLs for a ZNode.\n   * @param path Path of the ZNode.\n   * @return The list of ACLs.\n   * @throws Exception If it cannot contact Zookeeper.",
  "org.apache.hadoop.http.HttpServer2:addPrometheusServlet(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:gauge(org.apache.hadoop.metrics2.MetricsInfo,long)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:getRequestHeader()" : null,
  "org.apache.hadoop.net.AbstractDNSToSwitchMapping:isMappingSingleSwitch(org.apache.hadoop.net.DNSToSwitchMapping)" : "* Query for a {@link DNSToSwitchMapping} instance being on a single\n   * switch.\n   * <p>\n   * This predicate simply assumes that all mappings not derived from\n   * this class are multi-switch.\n   * @param mapping the mapping to query\n   * @return true if the base class says it is single switch, or the mapping\n   * is not derived from this class.",
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.security.KDiag:validateUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation)" : "* Validate the UGI: verify it is kerberized.\n   * @param messagePrefix message in exceptions\n   * @param user user to validate",
  "org.apache.hadoop.io.IntWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.io.file.tfile.ByteArray:buffer()" : "* @return the underlying buffer.",
  "org.apache.hadoop.fs.StorageType:asList()" : null,
  "org.apache.hadoop.util.Classpath:main(java.lang.String[])" : "* Main entry point.\n   *\n   * @param args command-line arguments",
  "org.apache.hadoop.crypto.CryptoInputStream:seek(long)" : "Seek to a position.",
  "org.apache.hadoop.fs.store.audit.AuditingFunctions:<init>()" : null,
  "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:write(int)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getPermission()" : null,
  "org.apache.hadoop.fs.StorageType:getNonTransientTypes()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:getAllowableBlockSize(int)" : null,
  "org.apache.hadoop.ipc.CallQueueManager:size()" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:isRoot()" : null,
  "org.apache.hadoop.io.ArrayWritable:<init>(java.lang.Class,org.apache.hadoop.io.Writable[])" : null,
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:setPassword(char[])" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:shouldRetry(java.lang.Exception,int,int,boolean)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(org.apache.hadoop.metrics2.MetricsInfo,long)" : "* Create a mutable long integer counter\n   * @param info  metadata of the metric\n   * @param iVal  initial value\n   * @return a new counter object",
  "org.apache.hadoop.fs.ChecksumFileSystem:getSumBufferSize(int,int)" : null,
  "org.apache.hadoop.util.functional.TaskPool:waitFor(java.util.Collection,int)" : "* Wait for all the futures to complete; there's a small sleep between\n   * each iteration; enough to yield the CPU.\n   * @param futures futures.\n   * @param sleepInterval Interval in milliseconds to await completion.",
  "org.apache.hadoop.io.VLongWritable:set(long)" : "* Set the value of this LongWritable.\n   * @param value input value.",
  "org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)" : null,
  "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getLength()" : null,
  "org.apache.hadoop.util.Lists:newArrayList()" : "* Creates a <i>mutable</i>, empty {@code ArrayList} instance.\n   *\n   * @param <E> Generics Type E.\n   * @return ArrayList Generics Type E.",
  "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:containsKey(java.lang.Object)" : null,
  "org.apache.hadoop.io.compress.Lz4Codec:getDecompressorType()" : "* Get the type of {@link Decompressor} needed by this {@link CompressionCodec}.\n   *\n   * @return the type of decompressor needed by this codec.",
  "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createTrustManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,java.lang.String,long)" : null,
  "org.apache.hadoop.crypto.key.kms.ValueQueue$NamedRunnable:isCanceled()" : null,
  "org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.Class,java.lang.Class[])" : null,
  "org.apache.hadoop.http.HttpServer2$Builder:createHttpChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration)" : null,
  "org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:createDecoder()" : null,
  "org.apache.hadoop.ha.HAAdmin:parseOpts(java.lang.String,org.apache.commons.cli.Options,java.lang.String[],java.util.Map)" : null,
  "org.apache.hadoop.conf.Configuration:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)" : "* Set the socket address a client can use to connect for the\n   * <code>name</code> property as a <code>host:port</code>.  The wildcard\n   * address is replaced with the local host's address.\n   * @param name property name.\n   * @param addr InetSocketAddress of a listener to store in the given property\n   * @return InetSocketAddress for clients to connect",
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getProxyForCallback(org.apache.hadoop.metrics2.MetricsSystem$Callback)" : null,
  "org.apache.hadoop.io.MapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)" : "* Return the value for the named key, or null if none exists.\n     * @param key key.\n     * @param val val.\n     * @return Writable if such a pair exists true,not false.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:createEncoder()" : null,
  "org.apache.hadoop.metrics2.lib.MutableStat:setExtended(boolean)" : "* Set whether to display the extended stats (stdev, min/max etc.) or not\n   * @param extended enable/disable displaying extended stats",
  "org.apache.hadoop.metrics2.impl.MetricsConfig:getPluginLoader()" : null,
  "org.apache.hadoop.http.HttpServer2$StackServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.fs.FSDataOutputStream:getIOStatistics()" : "* Get the IO Statistics of the nested stream, falling back to\n   * empty statistics if the stream does not implement the interface\n   * {@link IOStatisticsSource}.\n   * @return an IOStatistics instance.",
  "org.apache.hadoop.crypto.key.KeyProvider:needsPassword()" : "* Does this provider require a password? This means that a password is\n   * required for normal operation, and it has not been found through normal\n   * means. If true, the password should be provided by the caller using\n   * setPassword().\n   * @return Whether or not the provider requires a password\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.ssl.FileMonitoringTimerTask:run()" : null,
  "org.apache.hadoop.ipc.Client$ConnectionId:getAddress()" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyCompressor:getBytesWritten()" : "* Return number of bytes consumed by callers of compress since last reset.",
  "org.apache.hadoop.fs.GetSpaceUsed$Builder:getPath()" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,long)" : "* Create a mutable long integer gauge\n   * @param name  of the metric\n   * @param desc  metric description\n   * @param iVal  initial value\n   * @return a new gauge object",
  "org.apache.hadoop.ha.NodeFencer:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)" : null,
  "org.apache.hadoop.http.HttpServer2:openListeners()" : "* Open the main listener for the server\n   * @throws Exception",
  "org.apache.hadoop.net.NodeBase:getPathComponents(org.apache.hadoop.net.Node)" : "* Get the path components of a node.\n   * @param node a non-null node\n   * @return the path of a node",
  "org.apache.hadoop.io.InputBuffer$Buffer:getLength()" : null,
  "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getSchema()" : null,
  "org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : null,
  "org.apache.hadoop.jmx.JMXJsonServlet:isInstrumentationAccessAllowed(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.fs.impl.BackReference:<init>(java.lang.Object)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager)" : "* Sets an external <code>DelegationTokenSecretManager</code> instance to\n   * manage creation and verification of Delegation Tokens.\n   * <p>\n   * This is useful for use cases where secrets must be shared across multiple\n   * services.\n   *\n   * @param secretManager a <code>DelegationTokenSecretManager</code> instance",
  "org.apache.hadoop.fs.Options$CreateOpts$BufferSize:<init>(int)" : null,
  "org.apache.hadoop.fs.http.HttpFileSystem:listStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.OutputBuffer$Buffer:getLength()" : null,
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:warmUpEncryptedKeys(java.lang.String[])" : "* Notifies the Underlying CryptoExtension implementation to warm up any\n   * implementation specific caches for the specified KeyVersions\n   * @param keyNames Arrays of key Names\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.DecayRpcScheduler:getTopCallers(int)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],int[],int)" : "* Given a Vandermonde matrix V[i][j]=x[j]^i and vector y, solve for z such\n   * that Vz=y. The output z will be placed in y.\n   *\n   * @param x   the vector which describe the Vandermonde matrix\n   * @param y   right-hand side of the Vandermonde system equation. will be\n   *            replaced the output in this vector\n   * @param len consider x and y only from 0...len-1",
  "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:counters()" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:getConf()" : null,
  "org.apache.hadoop.metrics2.impl.SinkQueue:clear()" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:gauge(org.apache.hadoop.metrics2.MetricsInfo,int)" : null,
  "org.apache.hadoop.fs.ZeroCopyUnavailableException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)" : "* Constructs a server listening on the named port and address.  Parameters passed must\n   * be of the named class.  The <code>handlerCount</code> determines\n   * the number of handler threads that will be used to process calls.\n   * If queueSizePerHandler or numReaders are not -1 they will be used instead of parameters\n   * from configuration. Otherwise the configuration will be picked up.\n   * \n   * If rpcRequestClass is null then the rpcRequestClass must have been \n   * registered via {@link #registerProtocolEngine(RPC.RpcKind,\n   *  Class, RPC.RpcInvoker)}\n   * This parameter has been retained for compatibility with existing tests\n   * and usage.\n   *\n   * @param bindAddress input bindAddress.\n   * @param port input port.\n   * @param rpcRequestClass input rpcRequestClass.\n   * @param handlerCount input handlerCount.\n   * @param numReaders input numReaders.\n   * @param queueSizePerHandler input queueSizePerHandler.\n   * @param conf input Configuration.\n   * @param serverName input serverName.\n   * @param secretManager input secretManager.\n   * @param portRangeConfig input portRangeConfig.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isDirectory()" : null,
  "org.apache.hadoop.ipc.RetryCache:incrCacheClearedCounter()" : null,
  "org.apache.hadoop.fs.impl.FsLinkResolution:next(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : "* Modifies ACL entries of files and directories.  This method can add new ACL\n   * entries or modify the permissions on existing ACL entries.  All existing\n   * ACL entries that are not specified in this call are retained without\n   * changes.  (Modifications are merged into the current ACL.)\n   *\n   * @param path Path to modify\n   * @param aclSpec List{@literal <AclEntry>} describing modifications\n   * @throws IOException if an ACL could not be modified",
  "org.apache.hadoop.io.file.tfile.Utils:lowerBound(java.util.List,java.lang.Object)" : "* Lower bound binary search. Find the index to the first element in the list\n   * that compares greater than or equal to key.\n   * \n   * @param <T>\n   *          Type of the input key.\n   * @param list\n   *          The list\n   * @param key\n   *          The input key.\n   * @return The index to the desired element if it exists; or list.size()\n   *         otherwise.",
  "org.apache.hadoop.fs.RawLocalFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])" : "* Hook to implement support for {@link PathHandle} operations.\n   * @param stat Referent in the target FileSystem\n   * @param opts Constraints that determine the validity of the\n   *            {@link PathHandle} reference.",
  "org.apache.hadoop.fs.FilterFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)" : null,
  "org.apache.hadoop.util.ComparableVersion:parseVersion(java.lang.String)" : null,
  "org.apache.hadoop.tracing.Tracer:close()" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)" : "* Convenience method, so that we don't open a new connection when using this\n   * method from within another method. Otherwise every API invocation incurs\n   * the overhead of opening/closing a TCP connection.",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.WeakReferenceMap:containsKey(java.lang.Object)" : "* Does the map have a valid reference for this object?\n   * no-side effects: there's no attempt to notify or cleanup\n   * if the reference is null.\n   * @param key key to look up\n   * @return true if there is a valid reference.",
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:cancelPrefetches()" : "* Requests cancellation of any previously issued prefetch requests.",
  "org.apache.hadoop.security.NetgroupCache:isCached(java.lang.String)" : "* Returns true if a given netgroup is cached\n   *\n   * @param group check if this group is cached\n   * @return true if group is cached, false otherwise",
  "org.apache.hadoop.ipc.Server:queueCall(org.apache.hadoop.ipc.Server$Call)" : null,
  "org.apache.hadoop.ipc.CallQueueManager:convertSchedulerClass(java.lang.Class)" : null,
  "org.apache.hadoop.security.HadoopKerberosName:setConfiguration(org.apache.hadoop.conf.Configuration)" : "* Set the static configuration to get and evaluate the rules.\n   * <p>\n   * IMPORTANT: This method does a NOP if the rules have been set already.\n   * If there is a need to reset the rules, the {@link KerberosName#setRules(String)}\n   * method should be invoked directly.\n   * \n   * @param conf the new configuration\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.store.DataBlocks$DataBlock:innerClose()" : "* Inner close logic for subclasses to implement.\n     *\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.conf.Configuration:getConfResourceAsInputStream(java.lang.String)" : "* Get an input stream attached to the configuration resource with the\n   * given <code>name</code>.\n   * \n   * @param name configuration resource name.\n   * @return an input stream attached to the resource.",
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$Certificates:getDNSSubjectAlts(java.security.cert.X509Certificate)" : "* Extracts the array of SubjectAlt DNS names from an X509Certificate.\n       * Returns null if there aren't any.\n       * <p>\n       * Note:  Java doesn't appear able to extract international characters\n       * from the SubjectAlts.  It can only extract international characters\n       * from the CN field.\n       * <p>\n       * (Or maybe the version of OpenSSL I'm using to test isn't storing the\n       * international characters correctly in the SubjectAlts?).\n       *\n       * @param cert X509Certificate\n       * @return Array of SubjectALT DNS names stored in the certificate.",
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getKeyFromZK(int)" : null,
  "org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String)" : "* Adds the deprecated key to the global deprecation map when no custom\n   * message is provided.\n   * It does not override any existing entries in the deprecation map.\n   * This is to be used only by the developers in order to add deprecation of\n   * keys, and attempts to call this method after loading resources once,\n   * would lead to <tt>UnsupportedOperationException</tt>\n   * \n   * If you have multiple deprecation entries to add, it is more efficient to\n   * use #addDeprecations(DeprecationDelta[] deltas) instead.\n   *\n   * @param key Key that is to be deprecated\n   * @param newKey key that takes up the value of deprecated key",
  "org.apache.hadoop.tracing.Span:<init>()" : null,
  "org.apache.hadoop.util.LightWeightCache:evictEntries()" : "Evict entries in order to enforce the size limit of the cache.",
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:noPasswordWarning()" : null,
  "org.apache.hadoop.fs.audit.CommonAuditContext:getGlobalContextEntry(java.lang.String)" : "* Get a global entry.\n   * @param key key\n   * @return value or null",
  "org.apache.hadoop.conf.Configuration:writeXml(java.lang.String,java.io.Writer,org.apache.hadoop.conf.Configuration)" : "* Write out the non-default properties in this configuration to the\n   * given {@link Writer}.\n   * <ul>\n   * <li>\n   * When property name is not empty and the property exists in the\n   * configuration, this method writes the property and its attributes\n   * to the {@link Writer}.\n   * </li>\n   *\n   * <li>\n   * When property name is null or empty, this method writes all the\n   * configuration properties and their attributes to the {@link Writer}.\n   * </li>\n   *\n   * <li>\n   * When property name is not empty but the property doesn't exist in\n   * the configuration, this method throws an {@link IllegalArgumentException}.\n   * </li>\n   * </ul>\n   * @param propertyName xml property name.\n   * @param out the writer to write to.\n   * @param config configuration.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.Daemon:getRunnable()" : null,
  "org.apache.hadoop.metrics2.lib.MutableGaugeLong:toString()" : "* @return  the value of the metric",
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_counters(java.io.Serializable)" : "* Get the counters of an IOStatisticsSnapshot.\n   * @param source source of statistics.\n   * @return the map of counters.",
  "org.apache.hadoop.io.WritableUtils:toByteArray(org.apache.hadoop.io.Writable[])" : "* Convert writables to a byte array.\n   * @param writables input writables.\n   * @return ByteArray.",
  "org.apache.hadoop.metrics2.util.Contracts:checkArg(double,boolean,java.lang.Object)" : "* Check an argument for false conditions\n   * @param arg the argument to check\n   * @param expression  the boolean expression for the condition\n   * @param msg the error message if {@code expression} is false\n   * @return the argument for convenience",
  "org.apache.hadoop.ipc.RemoteException:unwrapRemoteException(java.lang.Class[])" : "* If this remote exception wraps up one of the lookupTypes\n   * then return this exception.\n   * <p>\n   * Unwraps any IOException.\n   * \n   * @param lookupTypes the desired exception class. may be null.\n   * @return IOException, which is either the lookupClass exception or this.",
  "org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:getSchemeName()" : null,
  "org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:reset()" : null,
  "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:close()" : "Close the file.",
  "org.apache.hadoop.ipc.RPC$Server$ProtoNameVer:<init>(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seek(long)" : null,
  "org.apache.hadoop.util.AutoCloseableLock:acquire()" : "* A wrapper method that makes a call to {@code lock()} of the underlying\n   * {@code ReentrantLock} object.\n   *\n   * Acquire teh lock it is not held by another thread, then sets\n   * lock held count to one, then returns immediately.\n   *\n   * If the current thread already holds the lock, increase the lock\n   * help count by one and returns immediately.\n   *\n   * If the lock is held by another thread, the current thread is\n   * suspended until the lock has been acquired by current thread.\n   *\n   * @return The {@code ReentrantLock} object itself. This is to\n   * support try-with-resource syntax.",
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:stopThreads()" : null,
  "org.apache.hadoop.security.KerberosAuthException:<init>(java.lang.Throwable)" : null,
  "org.apache.hadoop.fs.Trash:expunge()" : "* Delete old checkpoint(s).\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.MetricsJsonBuilder:add(org.apache.hadoop.metrics2.AbstractMetric)" : null,
  "org.apache.hadoop.fs.FileStatus:toString()" : null,
  "org.apache.hadoop.fs.FileContext:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)" : "* Make(create) a directory and all the non-existent parents.\n   * \n   * @param dir - the dir to make\n   * @param permission - permissions is set permission{@literal &~}umask\n   * @param createParent - if true then missing parent dirs are created if false\n   *          then parent must exist\n   * \n   * @throws AccessControlException If access is denied\n   * @throws FileAlreadyExistsException If directory <code>dir</code> already\n   *           exists\n   * @throws FileNotFoundException If parent of <code>dir</code> does not exist\n   *           and <code>createParent</code> is false\n   * @throws ParentNotDirectoryException If parent of <code>dir</code> is not a\n   *           directory\n   * @throws UnsupportedFileSystemException If file system for <code>dir</code>\n   *         is not supported\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server\n   * \n   * RuntimeExceptions:\n   * @throws InvalidPathException If path <code>dir</code> is not valid",
  "org.apache.hadoop.util.SysInfoLinux:getPhysicalMemorySize()" : "{@inheritDoc}",
  "org.apache.hadoop.io.compress.AlreadyClosedException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.ipc.Client$ConnectionId:getPingInterval()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.KDiag:println(java.lang.String,java.lang.Object[])" : "* Print a line of output. This goes to any output file, or\n   * is logged at info. The output is flushed before and after, to\n   * try and stay in sync with JRE logging.\n   *\n   * @param format format string\n   * @param args any arguments",
  "org.apache.hadoop.fs.Path:startPositionWithoutWindowsDrive(java.lang.String)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$OtherThanRemoteAndSaslExceptionDependentRetry:shouldRetry(java.lang.Exception,int,int,boolean)" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec:createDecompressor()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:resolvePath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numAvailable()" : "* Number of ByteBuffers available to be acquired.\n   *\n   * @return the number of available buffers.",
  "org.apache.hadoop.ipc.CallerContext:getCurrent()" : null,
  "org.apache.hadoop.util.Shell:runCommand()" : "* Run the command.\n   *\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)" : null,
  "org.apache.hadoop.fs.UploadHandle:toByteArray()" : "* @return Serialized from in bytes.",
  "org.apache.hadoop.security.alias.CredentialShell$CheckCommand:getUsage()" : null,
  "org.apache.hadoop.util.HostsFileReader:setIncludesFile(java.lang.String)" : null,
  "org.apache.hadoop.fs.impl.BackReference:toString()" : null,
  "org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object,java.lang.Object)" : "* <p>Preconditions that the specified argument is not {@code null},\n   * throwing a NPE exception otherwise.\n   *\n   * <p>The message of the exception is {@code errorMessage}.</p>\n   *\n   * @param <T> the object type\n   * @param obj  the object to check\n   * @param errorMessage  the message associated with the NPE\n   * @return the validated object\n   * @throws NullPointerException if the object is {@code null}\n   * @see #checkNotNull(Object, String, Object...)",
  "org.apache.hadoop.util.JsonSerialization:<init>(java.lang.Class,boolean,boolean)" : "* Create an instance bound to a specific type.\n   * @param classType class to marshall\n   * @param failOnUnknownProperties fail if an unknown property is encountered.\n   * @param pretty generate pretty (indented) output?",
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.io.UTF8:toStringChecked()" : "* Convert to a string, checking for valid UTF8.\n   * @return the converted string\n   * @throws UTFDataFormatException if the underlying bytes contain invalid\n   * UTF8 data.",
  "org.apache.hadoop.io.SequenceFile$Writer:ownStream()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getResolvedQualifiedPath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.ObjectWritable:getStaticProtobufMethod(java.lang.Class,java.lang.String,java.lang.Class[])" : null,
  "org.apache.hadoop.fs.http.HttpFileSystem:delete(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:setTimes(org.apache.hadoop.fs.Path,long,long)" : null,
  "org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:performFailover(java.lang.Object)" : null,
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm:getCompressor()" : null,
  "org.apache.hadoop.io.compress.bzip2.CRC:<init>()" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:readShortArray(java.io.DataInput)" : null,
  "org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text,int)" : "* Read from the InputStream into the given Text.\n   * @param str the object to store the given line\n   * @param maxLineLength the maximum number of bytes to store into str.\n   * @return the number of bytes read including the newline\n   * @throws IOException if the underlying stream throws",
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,java.security.cert.X509Certificate)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.fs.shell.Command:isSorted()" : "* Whether the directory listing for a path should be sorted.?\n   * @return true/false.",
  "org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:isInternalDir()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsPutUByte(int)" : null,
  "org.apache.hadoop.fs.FSOutputSummer:hasCapability(java.lang.String)" : null,
  "org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:getInputBlocks()" : null,
  "org.apache.hadoop.net.SocketInputStream$Reader:performIO(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.security.UserGroupInformation:initialize(org.apache.hadoop.conf.Configuration,boolean)" : "* Initialize UGI and related classes.\n   * @param conf the configuration to use",
  "org.apache.hadoop.util.LightWeightGSet$SetIterator:nextNonemptyEntry()" : "Find the next nonempty entry starting at (index + 1).",
  "org.apache.hadoop.fs.ftp.FTPFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:fenceOldActive()" : "* If there is a breadcrumb node indicating that another node may need\n   * fencing, try to fence that node.\n   * @return the Stat of the breadcrumb node that was read, or null\n   * if no breadcrumb node existed",
  "org.apache.hadoop.metrics2.impl.SinkQueue:_dequeue()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:getOutstandingBufferCount()" : "* Get count of outstanding buffers.\n     *\n     * @return the current buffer count.",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:<init>(boolean,int)" : "* Creates a new decompressor.\n   * @param conserveMemory conserveMemory.\n   * @param directBufferSize directBufferSize.",
  "org.apache.hadoop.util.IdentityHashStore:putInternal(java.lang.Object,java.lang.Object)" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:getCallId()" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:drain(java.lang.String)" : null,
  "org.apache.hadoop.fs.sftp.SFTPInputStream:checkNotClosed()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getBufferPoolSize()" : "* @return The size of the in-memory cache.",
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:<init>()" : null,
  "org.apache.hadoop.security.CompositeGroupsMapping:getGroups(java.lang.String)" : "* Returns list of groups for a user.\n   * \n   * @param user get groups for this user\n   * @return list of groups for a given user",
  "org.apache.hadoop.io.DataInputBuffer:<init>(org.apache.hadoop.io.DataInputBuffer$Buffer)" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:createRootDirRecursively(java.lang.String,java.util.List)" : "* Utility function to ensure that the configured base znode exists.\n   * This recursively creates the znode as well as all of its parents.\n   * @param path Path of the znode to create.\n   * @param zkAcl ACLs for ZooKeeper.\n   * @throws Exception If it cannot create the file.",
  "org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String)" : "* Randomly choose a node.\n   *\n   * @param scope range of nodes from which a node will be chosen\n   * @return the chosen node\n   *\n   * @see #chooseRandom(String, Collection)",
  "org.apache.hadoop.conf.StorageSize:parse(java.lang.String)" : null,
  "org.apache.hadoop.conf.Configuration:findSubVariable(java.lang.String)" : "* This is a manual implementation of the following regex\n   * \"\\\\$\\\\{[^\\\\}\\\\$\\u0020]+\\\\}\". It can be 15x more efficient than\n   * a regex matcher as demonstrated by HADOOP-11506. This is noticeable with\n   * Hadoop apps building on the assumption Configuration#get is an O(1)\n   * hash table lookup, especially when the eval is a long string.\n   *\n   * @param eval a string that may contain variables requiring expansion.\n   * @return a 2-element int array res such that\n   * eval.substring(res[0], res[1]) is \"var\" for the left-most occurrence of\n   * ${var} in eval. If no variable is found -1, -1 is returned.",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2CompressorType(org.apache.hadoop.conf.Configuration)" : "* Return the appropriate type of the bzip2 compressor. \n   * \n   * @param conf configuration\n   * @return the appropriate type of the bzip2 compressor.",
  "org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:postProcessPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping:getName2IdCmdNIX(java.lang.String,boolean)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:maybeUpdateMaximum(java.util.concurrent.atomic.AtomicLong,long)" : "* Update a maximum value tracked in an atomic long.\n   * This is thread safe -it uses compareAndSet to ensure\n   * that Thread T1 whose sample is greater than the current\n   * value never overwrites an update from thread T2 whose\n   * sample was also higher -and which completed first.\n   * @param dest destination for all changes.\n   * @param sample sample to update.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token:getDelegationToken()" : null,
  "org.apache.hadoop.conf.ReconfigurableBase:getReconfigurationTaskStatus()" : null,
  "org.apache.hadoop.io.Text$1:<init>()" : "* Construct an empty text string.",
  "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:getBytesRead()" : "* Returns the total number of uncompressed bytes input so far.\n   *\n   * @return the total (non-negative) number of uncompressed bytes input so far",
  "org.apache.hadoop.ipc.Server:isClientBackoffEnabled()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Called after a new FileSystem instance is constructed.\n   * @param name a uri whose authority section names the host, port, etc.\n   *   for this FileSystem\n   * @param conf the configuration",
  "org.apache.hadoop.fs.FileSystem:removeAcl(org.apache.hadoop.fs.Path)" : "* Removes all but the base ACL entries of files and directories.  The entries\n   * for user, group, and others are retained for compatibility with permission\n   * bits.\n   *\n   * @param path Path to modify\n   * @throws IOException if an ACL could not be removed\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.fs.FilterFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)" : "Called after a new FileSystem instance is constructed.\n   * @param name a uri whose authority section names the host, port, etc.\n   *   for this FileSystem\n   * @param conf the configuration",
  "org.apache.hadoop.ipc.ExternalCall:waitForCompletion()" : null,
  "org.apache.hadoop.io.compress.DecompressorStream:getCompressedData()" : null,
  "org.apache.hadoop.metrics2.util.SampleStat$MinMax:max()" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:finished()" : "* Returns true if the end of the gzip substream (single \"member\") has been\n   * reached.",
  "org.apache.hadoop.util.functional.RemoteIterators$MaybeClose:close()" : null,
  "org.apache.hadoop.fs.permission.AclEntry:<init>(org.apache.hadoop.fs.permission.AclEntryType,java.lang.String,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.AclEntryScope)" : "* Private constructor.\n   *\n   * @param type AclEntryType ACL entry type\n   * @param name String optional ACL entry name\n   * @param permission FsAction set of permissions in the ACL entry\n   * @param scope AclEntryScope scope of the ACL entry",
  "org.apache.hadoop.ha.PowerShellFencer:checkArgs(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.CodecPool:createCache(java.lang.Class)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:createWithRetries(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode)" : null,
  "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)" : "* Create an FSDataOutputStream at the indicated Path with write-progress\n   * reporting.\n   * Files are overwritten by default.\n   * @param f the file to create\n   * @param replication the replication factor\n   * @param progress to report progress\n   * @throws IOException IO failure\n   * @return output stream.",
  "org.apache.hadoop.util.SysInfoWindows:getNumCores()" : "{@inheritDoc}",
  "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.io.File,boolean,org.apache.hadoop.conf.Configuration)" : "Copy FileSystem files to local files.",
  "org.apache.hadoop.io.erasurecode.ECChunk:isAllZero()" : null,
  "org.apache.hadoop.io.WritableUtils:skipCompressedByteArray(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferData:getState()" : "* Gets the state of this block.\n   *\n   * @return the state of this block.",
  "org.apache.hadoop.ha.FailoverFailedException:<init>(java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.security.SaslOutputStream:disposeSasl()" : "* Disposes of any system resources or security-sensitive information Sasl\n   * might be using.\n   * \n   * @exception SaslException\n   *              if a SASL error occurs.",
  "org.apache.hadoop.security.token.SecretManager:checkAvailableForRead()" : "* No-op if the secret manager is available for reading tokens, throw a\n   * StandbyException otherwise.\n   * \n   * @throws StandbyException if the secret manager is not available to read\n   *         tokens",
  "org.apache.hadoop.fs.FSBuilder:must(java.lang.String,long)" : "* Set mandatory long option.\n   *\n   * @param key key.\n   * @param value value.\n   * @return generic type B.\n   * @see #must(String, String)",
  "org.apache.hadoop.util.ShutdownHookManager:get()" : "* Return <code>ShutdownHookManager</code> singleton.\n   *\n   * @return <code>ShutdownHookManager</code> singleton.",
  "org.apache.hadoop.fs.LocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.DataChecksum:verifyChunked(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,java.nio.ByteBuffer,int,java.nio.ByteBuffer,java.lang.String,long)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:getChecksumFileLength(org.apache.hadoop.fs.Path,long)" : "* Return the length of the checksum file given the size of the\n   * actual file.\n   *\n   * @param file the file path.\n   * @param fileSize file size.\n   * @return checksum length.",
  "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:tags()" : null,
  "org.apache.hadoop.io.file.tfile.TFileDumper:<init>()" : null,
  "org.apache.hadoop.io.GenericWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.FilterFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.io.SortedMapWritable:headMap(java.lang.Object)" : null,
  "org.apache.hadoop.fs.shell.find.Print:<init>()" : null,
  "org.apache.hadoop.metrics2.lib.MethodMetric:isInt(java.lang.Class)" : null,
  "org.apache.hadoop.ipc.RpcClientUtil$ProtoSigCacheKey:<init>(java.net.InetSocketAddress,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.crypto.JceCtrCryptoCodec:getProvider()" : null,
  "org.apache.hadoop.service.AbstractService:notifyListeners()" : "* Notify local and global listeners of state changes.\n   * Exceptions raised by listeners are NOT passed up.",
  "org.apache.hadoop.fs.shell.PathData:parentExists()" : "* Test if the parent directory exists\n   * @return boolean indicating parent exists\n   * @throws IOException upon unexpected error",
  "org.apache.hadoop.fs.FilterFileSystem:getXAttrs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.DiskValidatorFactory:<init>()" : null,
  "org.apache.hadoop.net.unix.DomainSocket:connect(java.lang.String)" : "* Create a new DomainSocket connected to the given path.\n   *\n   * @param path              The path to connect to.\n   * @throws IOException      If there was an I/O error performing the connect.\n   *\n   * @return                  The new DomainSocket.",
  "org.apache.hadoop.io.file.tfile.BCFile$Writer$MetaBlockRegister:register(long,long,long)" : null,
  "org.apache.hadoop.security.SaslRpcServer:encodeIdentifier(byte[])" : null,
  "org.apache.hadoop.service.Service$STATE:toString()" : "* Get the name of a state\n     * @return the state's name",
  "org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:createDescriptor(java.lang.String,int)" : "* Create a shared file descriptor which will be both readable and writable.\n   *\n   * @param info           Information to include in the path of the \n   *                         generated descriptor.\n   * @param length         The starting file length.\n   *\n   * @return               The file descriptor, wrapped in a FileInputStream.\n   * @throws IOException   If there was an I/O or configuration error creating\n   *                         the descriptor.",
  "org.apache.hadoop.ipc.DefaultRpcScheduler:shouldBackOff(org.apache.hadoop.ipc.Schedulable)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferData:getChecksum(java.nio.ByteBuffer)" : "* Computes CRC32 checksum of the given buffer's contents.\n   *\n   * @param buffer the buffer whose content's checksum is to be computed.\n   * @return the computed checksum.",
  "org.apache.hadoop.net.ScriptBasedMapping:toString()" : null,
  "org.apache.hadoop.util.VersionInfo:getUser()" : "* The user that compiled Hadoop.\n   * @return the username of the user",
  "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getCurrentPos()" : "* Get the current position in file.\n       * \n       * @return The current byte offset in underlying file.\n       * @throws IOException",
  "org.apache.hadoop.io.WritableComparator:readLong(byte[],int)" : "* Parse a long from a byte array.\n   * @param bytes bytes.\n   * @param start start.\n   * @return long from a byte array",
  "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:getProgress()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:refresh(java.lang.String,java.lang.String[])" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:getFailException()" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:reset()" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:addChildren(java.util.Deque)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:name()" : null,
  "org.apache.hadoop.io.erasurecode.ECSchema:getNumParityUnits()" : "* Get required parity units count in a coding group\n   * @return count of parity units",
  "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.conf.Configuration:setPattern(java.lang.String,java.util.regex.Pattern)" : "* Set the given property to <code>Pattern</code>.\n   * If the pattern is passed as null, sets the empty pattern which results in\n   * further calls to getPattern(...) returning the default value.\n   *\n   * @param name property name\n   * @param pattern new value",
  "org.apache.hadoop.ipc.ProtobufRpcEngine2:clearClientCache()" : null,
  "org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:getLoadingFailureReason()" : null,
  "org.apache.hadoop.fs.shell.CopyCommands$Cp:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:incrementCurrentKeyId()" : "* Obtains the next available delegation key id that can be allocated to a DelegationKey.\n   * Delegation key id need to be reserved using the shared delegationKeyIdCounter,\n   * which handles keyId allocation concurrently with other secret managers.\n   * @return Next available delegation key id.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator)" : "* Creates an <code>DelegationTokenAuthenticatedURL</code>.\n   *\n   * @param authenticator the {@link DelegationTokenAuthenticator} instance to\n   * use, if <code>null</code> the default one will be used.",
  "org.apache.hadoop.conf.StorageUnit$1:getSuffixChar()" : null,
  "org.apache.hadoop.util.functional.CommonCallableSupplier:get()" : null,
  "org.apache.hadoop.conf.ReconfigurationUtil$PropertyChange:<init>(java.lang.String,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:nflyStatus()" : null,
  "org.apache.hadoop.fs.statistics.FileSystemStatisticNames:<init>()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:cancelPrefetches()" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:getNodePath(java.lang.String,java.lang.String)" : "* Get the path for a ZNode.\n   * @param root Root of the ZNode.\n   * @param nodeName Name of the ZNode.\n   * @return Path for the ZNode.",
  "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:<init>(java.io.DataInput)" : null,
  "org.apache.hadoop.ipc.RpcWritable:wrap(java.lang.Object)" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSupport:retrieveIOStatistics(java.lang.Object)" : null,
  "org.apache.hadoop.tools.TableListing:<init>(org.apache.hadoop.tools.TableListing$Column[],boolean,int)" : null,
  "org.apache.hadoop.conf.ConfigRedactor:configIsSensitive(java.lang.String)" : "* Matches given config key against patterns and determines whether or not\n   * it should be considered sensitive enough to redact in logs and other\n   * plaintext displays.\n   *\n   * @param key\n   * @return True if parameter is considered sensitive",
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readVectored(java.util.List,java.util.function.IntFunction)" : "* Vectored read.\n     * If the file has no checksums: delegate to the underlying stream.\n     * If the file is checksummed: calculate the checksum ranges as\n     * well as the data ranges, read both, and validate the checksums\n     * as well as returning the data.\n     * @param ranges the byte ranges to read\n     * @param allocate the function to allocate ByteBuffer\n     * @throws IOException",
  "org.apache.hadoop.security.token.Token:isPrivate()" : "* Whether this is a private token.\n   * @return false always for non-private tokens",
  "org.apache.hadoop.fs.ChecksumFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.metrics2.filter.GlobFilter:compile(java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:close()" : "* Close the scanner. Release all resources. The behavior of using the\n       * scanner after calling close is not defined. The entry returned by the\n       * previous entry() call will be invalid.",
  "org.apache.hadoop.io.ObjectWritable:toString()" : null,
  "org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:getDependency(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:initialize(java.lang.String)" : "* Convenience method to initialize the metrics system\n   * @param prefix  for the metrics system configuration\n   * @return the metrics system instance",
  "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])" : "*  Validate outputs decoded from inputs, by decoding an input back from\n   *  those outputs and comparing it with the original one.\n   * @param inputs input buffers used for decoding\n   * @param erasedIndexes indexes of erased units used for decoding\n   * @param outputs decoded output buffers\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.FastByteComparisons:lexicographicalComparerJavaImpl()" : null,
  "org.apache.hadoop.metrics2.util.SampleStat:toString()" : null,
  "org.apache.hadoop.security.Credentials:numberOfTokens()" : "* @return number of Tokens in the in-memory map",
  "org.apache.hadoop.fs.shell.find.FilterExpression:apply(org.apache.hadoop.fs.shell.PathData,int)" : null,
  "org.apache.hadoop.fs.FsShell:<init>()" : "* Default ctor with no configuration.  Be sure to invoke\n   * {@link #setConf(Configuration)} with a valid configuration prior\n   * to running commands.",
  "org.apache.hadoop.io.SequenceFile$Writer:syncFs()" : "* flush all currently written data to the file system.\n     * @deprecated Use {@link #hsync()} or {@link #hflush()} instead\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.impl.MetricGaugeFloat:<init>(org.apache.hadoop.metrics2.MetricsInfo,float)" : null,
  "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:chooseEngineClientAlias(java.lang.String[],java.security.Principal[],javax.net.ssl.SSLEngine)" : null,
  "org.apache.hadoop.fs.shell.Head:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Get a FileSystem for this URI's scheme and authority.\n   * <ol>\n   * <li>\n   *   If the configuration has the property\n   *   {@code \"fs.$SCHEME.impl.disable.cache\"} set to true,\n   *   a new instance will be created, initialized with the supplied URI and\n   *   configuration, then returned without being cached.\n   * </li>\n   * <li>\n   *   If the there is a cached FS instance matching the same URI, it will\n   *   be returned.\n   * </li>\n   * <li>\n   *   Otherwise: a new FS instance will be created, initialized with the\n   *   configuration and URI, cached and returned to the caller.\n   * </li>\n   * </ol>\n   * @param uri uri of the filesystem.\n   * @param conf configrution.\n   * @return filesystem instance.\n   * @throws IOException if the FileSystem cannot be instantiated.",
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(org.apache.hadoop.conf.Configuration)" : "* Creates a new compressor, taking settings from the configuration.\n   * @param conf configuration.",
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsForUserCommand(java.lang.String)" : "* Returns just the shell command to be used to fetch a user's groups list.\n   * This is mainly separate to make some tests easier.\n   * @param userName The username that needs to be passed into the command built\n   * @return An appropriate shell command with arguments",
  "org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:valueOf(org.apache.hadoop.security.SaslRpcServer$AuthMethod)" : null,
  "org.apache.hadoop.metrics2.lib.MutableStat:setUpdateTimeStamp(boolean)" : "* Set whether to update the snapshot time or not.\n   * @param updateTimeStamp enable update stats snapshot timestamp",
  "org.apache.hadoop.fs.permission.FsPermission:<init>(int)" : "* Construct by the given mode.\n   *\n   * octal mask is applied.\n   *\n   *<pre>\n   *              before mask     after mask    file type   sticky bit\n   *\n   *    octal     100644            644         file          no\n   *    decimal    33188            420\n   *\n   *    octal     101644           1644         file          yes\n   *    decimal    33700           1420\n   *\n   *    octal      40644            644         directory     no\n   *    decimal    16804            420\n   *\n   *    octal      41644           1644         directory     yes\n   *    decimal    17316           1420\n   *</pre>\n   *\n   * 100644 becomes 644 while 644 remains as 644\n   *\n   * @param mode Mode is supposed to come from the result of native stat() call.\n   *             It contains complete permission information: rwxrwxrwx, sticky\n   *             bit, whether it is a directory or a file, etc. Upon applying\n   *             mask, only permission and sticky bit info will be kept because\n   *             they are the only parts to be used for now.\n   * @see #FsPermission(short mode)",
  "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:buildHttpReferrer()" : "* Build the referrer string.\n   * This includes dynamically evaluating all of the evaluated\n   * attributes.\n   * If there is an error creating the string it will be logged once\n   * per entry, and \"\" returned.\n   * @return a referrer string or \"\"",
  "org.apache.hadoop.ipc.ProtobufRpcEngine:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:getNullIndexes(java.lang.Object[])" : "* Get indexes array for items marked as null, either erased or\n   * not to read.\n   * @return indexes array",
  "org.apache.hadoop.fs.shell.FsUsage$Du:processPathArgument(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.util.MergeSort:<init>(java.util.Comparator)" : null,
  "org.apache.hadoop.util.UTF8ByteArrayUtils:findByte(byte[],int,int,byte)" : "* Find the first occurrence of the given byte b in a UTF-8 encoded string\n   * @param utf a byte array containing a UTF-8 encoded string\n   * @param start starting offset\n   * @param end ending position\n   * @param b the byte to find\n   * @return position that first byte occurs, otherwise -1",
  "org.apache.hadoop.net.SocksSocketFactory:getConf()" : null,
  "org.apache.hadoop.net.NodeBase:setParent(org.apache.hadoop.net.Node)" : "Set this node's parent\n   * @param parent the parent",
  "org.apache.hadoop.fs.LocatedFileStatus:<init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.BlockLocation[])" : "* Constructor \n   * @param stat a file status\n   * @param locations a file's block locations",
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:orNoop()" : "* If no implementation has been found, adds a NOOP method.\n     * <p>\n     * Note: calls to impl will not match after this method is called!\n     * @return this Builder for method chaining",
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newMutableRollingAverages(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.shell.Truncate:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:abort(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<init>(java.util.List,java.util.function.Consumer,java.util.function.Consumer)" : "* Create file monitoring task to be scheduled using a standard\n   * Java {@link java.util.Timer} instance.\n   *\n   * @param filePaths The path to the file to monitor.\n   * @param onFileChange The function to call when the file has changed.\n   * @param onChangeFailure The function to call when an exception is\n   *                       thrown during the file change processing.",
  "org.apache.hadoop.io.MapFile$Writer:setIndexInterval(int)" : "* Sets the index interval.\n     * @see #getIndexInterval()\n     *\n     * @param interval interval.",
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:add(java.lang.String,org.apache.hadoop.metrics2.lib.MutableMetric)" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:addChild(org.apache.hadoop.fs.shell.find.Expression)" : "* Add a single argument to this expression. The argument is popped off the\n   * head of the expressions.\n   *\n   * @param expr\n   *          child to add to the expression",
  "org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping:clear()" : null,
  "org.apache.hadoop.fs.FileSystem:<init>()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.conf.Configuration:getClass(java.lang.String,java.lang.Class,java.lang.Class)" : "* Get the value of the <code>name</code> property as a <code>Class</code>\n   * implementing the interface specified by <code>xface</code>.\n   *   \n   * If no such property is specified, then <code>defaultValue</code> is \n   * returned.\n   * \n   * An exception is thrown if the returned class does not implement the named\n   * interface. \n   * \n   * @param name the conf key name.\n   * @param defaultValue default value.\n   * @param xface the interface implemented by the named class.\n   * @param <U> Interface class type.\n   * @return property value as a <code>Class</code>, \n   *         or <code>defaultValue</code>.",
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])" : "* This is the client side invoker of RPC method. It only throws\n     * ServiceException, since the invocation proxy expects only\n     * ServiceException to be thrown by the method in case protobuf service.\n     *\n     * ServiceException has the following causes:\n     * <ol>\n     * <li>Exceptions encountered on the client side in this method are\n     * set as cause in ServiceException as is.</li>\n     * <li>Exceptions from the server are wrapped in RemoteException and are\n     * set as cause in ServiceException</li>\n     * </ol>\n     *\n     * Note that the client calling protobuf RPC methods, must handle\n     * ServiceException by getting the cause from the ServiceException. If the\n     * cause is RemoteException, then unwrap it to get the exception thrown by\n     * the server.",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:gauges()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:compareCursorKeyTo(org.apache.hadoop.io.file.tfile.RawComparable)" : "* Internal API. Comparing the key at cursor to user-specified key.\n       * \n       * @param other\n       *          user-specified key.\n       * @return negative if key at cursor is smaller than user key; 0 if equal;\n       *         and positive if key at cursor greater than user key.\n       * @throws IOException",
  "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:chooseClientAlias(java.lang.String[],java.security.Principal[],java.net.Socket)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)" : null,
  "org.apache.hadoop.io.DataOutputBuffer:getLength()" : "* Returns the length of the valid data currently in the buffer.\n   * @return length.",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:minimums()" : null,
  "org.apache.hadoop.fs.ContentSummary:getSnapshotSpaceConsumed()" : null,
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor:toString()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:deprecatedGetFileLinkStatusInternal(org.apache.hadoop.fs.Path)" : "* Deprecated. Remains for legacy support. Should be removed when {@link Stat}\n   * gains support for Windows and other operating systems.",
  "org.apache.hadoop.security.SaslInputStream:skip(long)" : "* Skips <code>n</code> bytes of input from the bytes that can be read from\n   * this input stream without blocking.\n   * \n   * <p>\n   * Fewer bytes than requested might be skipped. The actual number of bytes\n   * skipped is equal to <code>n</code> or the result of a call to\n   * {@link #available()}, whichever is smaller. If\n   * <code>n</code> is less than zero, no bytes are skipped.\n   * \n   * <p>\n   * The actual number of bytes skipped is returned.\n   * \n   * @param n\n   *          the number of bytes to be skipped.\n   * @return the actual number of bytes skipped.\n   * @exception IOException\n   *              if an I/O error occurs.",
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(long[],java.lang.String)" : "* Validates that the given array is not null and has at least one element.\n   * @param array the argument reference to validate.\n   * @param argName the name of the argument being validated.",
  "org.apache.hadoop.fs.AvroFSInput:seek(long)" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:invoke(java.lang.Object[])" : null,
  "org.apache.hadoop.ha.NodeFencer:create(org.apache.hadoop.conf.Configuration,java.lang.String)" : null,
  "org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.ipc.CallQueueManager:isServerFailOverEnabled()" : null,
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getFlags()" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(byte[],java.lang.String)" : "* Validates that the given array is not null and has at least one element.\n   * @param array the argument reference to validate.\n   * @param argName the name of the argument being validated.",
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsPutInt(int)" : null,
  "org.apache.hadoop.ipc.Client$Call:setException(java.io.IOException)" : "Set the exception when there is an error.\n     * Notify the caller the call is done.\n     * \n     * @param error exception thrown by the call; either local or remote",
  "org.apache.hadoop.util.WeakReferenceMap:get(java.lang.Object)" : "* Get the value, creating if needed.\n   * @param key key.\n   * @return an instance.",
  "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterNames()" : "* Return the set of parameter names, quoting each name.",
  "org.apache.hadoop.io.MD5Hash:compareTo(org.apache.hadoop.io.MD5Hash)" : "Compares this object with the specified object for order.",
  "org.apache.hadoop.security.UserGroupInformation:getLogin()" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersion(java.lang.String,byte[])" : null,
  "org.apache.hadoop.fs.TrashPolicyDefault:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecodeImpl(byte[][],int[],int,int[],byte[][],int[])" : null,
  "org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:setDictionary(byte[],int,int)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.conf.Configuration$Parser:parseNext()" : null,
  "org.apache.hadoop.metrics2.util.SampleStat$MinMax:add(double)" : null,
  "org.apache.hadoop.io.UTF8:readChars(java.io.DataInput,java.lang.StringBuilder,int)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind:fromShortName(java.lang.String)" : null,
  "org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByName(java.lang.String)" : "* Create an InetAddress with a fully qualified hostname of the given\n     * hostname.  InetAddress does not qualify an incomplete hostname that\n     * is resolved via the domain search list.\n     * {@link InetAddress#getCanonicalHostName()} will fully qualify the\n     * hostname, but it always return the A record whereas the given hostname\n     * may be a CNAME.\n     * \n     * @param host a hostname or ip address\n     * @return InetAddress with the fully qualified hostname or ip\n     * @throws UnknownHostException if host does not exist",
  "org.apache.hadoop.ipc.Client:createCall(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:increaseRemoteReadTime(long)" : "* Increment the time taken to read bytes from remote in the statistics.\n     * @param durationMS time taken in ms to read bytes from remote",
  "org.apache.hadoop.security.token.Token:getClassForIdentifier(org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.util.StopWatch:start()" : "* Start to measure times and make the state of stopwatch running.\n   * @return this instance of StopWatch.",
  "org.apache.hadoop.fs.statistics.IOStatisticsSupport:stubDurationTracker()" : "* Get a stub duration tracker.\n   * @return a stub tracker.",
  "org.apache.hadoop.metrics2.sink.FileSink:init(org.apache.commons.configuration2.SubsetConfiguration)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:getValue()" : null,
  "org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.ipc.CallerContext$Builder:isValid(java.lang.String)" : "* Whether the field is valid.\n     * @param field one of the fields in context.\n     * @return true if the field is not null or empty.",
  "org.apache.hadoop.ipc.ResponseBuffer:capacity()" : null,
  "org.apache.hadoop.fs.FilterFs:getHomeDirectory()" : null,
  "org.apache.hadoop.fs.FileStatus:setPermission(org.apache.hadoop.fs.permission.FsPermission)" : "* Sets permission.\n   * @param permission if permission is null, default value is set",
  "org.apache.hadoop.io.SecureIOUtils:openForRandomRead(java.io.File,java.lang.String,java.lang.String,java.lang.String)" : "* @return Open the given File for random read access, verifying the expected user/\n   * group constraints if security is enabled.\n   * \n   * Note that this function provides no additional security checks if hadoop\n   * security is disabled, since doing the checks would be too expensive when\n   * native libraries are not available.\n   * \n   * @param f file that we are trying to open\n   * @param mode mode in which we want to open the random access file\n   * @param expectedOwner the expected user owner for the file\n   * @param expectedGroup the expected group owner for the file\n   * @throws IOException if an IO error occurred or if the user/group does\n   * not match when security is enabled.",
  "org.apache.hadoop.fs.FileSystem:fixName(java.lang.String)" : "Update old-format filesystem names, for back-compatibility.  This should\n   * eventually be replaced with a checkName() method that throws an exception\n   * for old-format names.",
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:getSummary(java.lang.StringBuilder)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetBuffer(byte[],int,int)" : "* Ensure the buffer (either input or output) ready to read or write with ZERO\n   * bytes fully in specified length of len.\n   * @param buffer bytes array buffer\n   * @return the buffer itself",
  "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getSrc()" : null,
  "org.apache.hadoop.net.DNS:getIPs(java.lang.String,boolean)" : "* Returns all the IPs associated with the provided interface, if any, in\n   * textual form.\n   * \n   * @param strInterface\n   *            The name of the network interface or sub-interface to query\n   *            (eg eth0 or eth0:0) or the string \"default\"\n   * @param returnSubinterfaces\n   *            Whether to return IPs associated with subinterfaces of\n   *            the given interface\n   * @return A string vector of all the IPs associated with the provided\n   *         interface. The local host IP is returned if the interface\n   *         name \"default\" is specified or there is an I/O error looking\n   *         for the given interface.\n   * @throws UnknownHostException\n   *             If the given interface is invalid\n   *",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:getRenewDate()" : "* @return returns renew date.",
  "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:grow(org.apache.hadoop.io.SequenceFile$ValueBytes[],int)" : null,
  "org.apache.hadoop.metrics2.util.SampleQuantiles:toString()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : "* The specification of this method matches that of\n   * {@link FileContext#createSnapshot(Path, String)}.\n   *\n   * @param path the path.\n   * @param snapshotName snapshot name.\n   * @throws IOException raised on errors performing I/O.\n   * @return path.",
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:setDictionary(byte[],int,int)" : null,
  "org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,long,boolean)" : "* Copies count bytes from one stream to another.\n   *\n   * @param in InputStream to read from\n   * @param out OutputStream to write to\n   * @param count number of bytes to copy\n   * @param close whether to close the streams\n   * @throws IOException if bytes can not be read or written",
  "org.apache.hadoop.fs.BlockLocation:setNames(java.lang.String[])" : "* Set the names (host:port) hosting this block.\n   * @param names names.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.util.InstrumentedReadLock:unlock()" : null,
  "org.apache.hadoop.fs.shell.find.Name:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory)" : "Registers this expression with the specified factory.",
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:exists(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)" : "* Convenience method, so that we don't open a new connection when using this\n   * method from within another method. Otherwise every API invocation incurs\n   * the overhead of opening/closing a TCP connection.\n   * @throws IOException",
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:compress(byte[],int,int)" : null,
  "org.apache.hadoop.security.AccessControlException:<init>(java.lang.String)" : "* Constructs an {@link AccessControlException}\n   * with the specified detail message.\n   * @param s the detail message.",
  "org.apache.hadoop.fs.shell.Delete$Rm:expandArgument(java.lang.String)" : null,
  "org.apache.hadoop.fs.impl.FlagSet:copy()" : "* Create a copy of the FlagSet.\n   * @return a new mutable instance with a separate copy of the flags",
  "org.apache.hadoop.fs.FileSystem:newInstance(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)" : "* Returns the FileSystem for this URI's scheme and authority and the\n   * given user. Internally invokes {@link #newInstance(URI, Configuration)}\n   * @param uri uri of the filesystem.\n   * @param conf the configuration to use\n   * @param user to perform the get as\n   * @return filesystem instance\n   * @throws IOException if the FileSystem cannot be instantiated.\n   * @throws InterruptedException If the {@code UGI.doAs()} call was\n   *         somehow interrupted.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addFallbackLink(org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)" : null,
  "org.apache.hadoop.io.file.tfile.Compression$FinishOnFlushCompressionStream:<init>(org.apache.hadoop.io.compress.CompressionOutputStream)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:getInterface()" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibFactory:getCompressionStrategy(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.shell.PathData:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Creates an object to wrap the given parameters as fields.  The string\n   * used to create the path will be recorded since the Path object does not\n   * return exactly the same string used to initialize it\n   * @param pathString a string for a path\n   * @param conf the configuration file\n   * @throws IOException if anything goes wrong...",
  "org.apache.hadoop.fs.ChecksumFileSystem:delete(org.apache.hadoop.fs.Path,boolean)" : "* Implement the delete(Path, boolean) in checksum\n   * file system.",
  "org.apache.hadoop.fs.viewfs.ViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:addCallVolumePerPriority(org.apache.hadoop.metrics2.MetricsRecordBuilder)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)" : null,
  "org.apache.hadoop.fs.FileSystem:createNewFile(org.apache.hadoop.fs.Path)" : "* Creates the given Path as a brand-new zero-length file.  If\n   * create fails, or if it already existed, return false.\n   * <i>Important: the default implementation is not atomic</i>\n   * @param f path to use for create\n   * @throws IOException IO failure\n   * @return if create new file success true,not false.",
  "org.apache.hadoop.io.UTF8:<init>(java.lang.String)" : "* Construct from a given string.\n   * @param string input string.",
  "org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int)" : null,
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withMeanStatisticFunction(java.lang.String,java.util.function.Function)" : "* Add a new evaluator to the mean statistics.\n   *\n   * This is a function which must return the mean and the sample count.\n   * @param key key of this statistic\n   * @param eval evaluator for the statistic\n   * @return the builder.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:getRecordNum()" : "* Get the RecordNum corresponding to the entry pointed by the cursor.\n       * @return The RecordNum corresponding to the entry pointed by the cursor.\n       * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:deserialize(java.io.Serializable)" : null,
  "org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.File,java.io.File,boolean)" : null,
  "org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FileStatus)" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromMemory(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:invokeChecked(java.lang.Object[])" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:coreServiceLaunch(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.io.MapFile$Writer$KeyClassOption:<init>(java.lang.Class)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:resolveInternal(java.lang.String)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:maybeUpdateMinimum(java.util.concurrent.atomic.AtomicLong,long)" : "* Update a maximum value tracked in an atomic long.\n   * This is thread safe -it uses compareAndSet to ensure\n   * that Thread T1 whose sample is greater than the current\n   * value never overwrites an update from thread T2 whose\n   * sample was also higher -and which completed first.\n   * @param dest destination for all changes.\n   * @param sample sample to update.",
  "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:getFS()" : null,
  "org.apache.hadoop.jmx.JMXJsonServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : "* Process a GET request for the specified resource.\n   * \n   * @param request\n   *          The servlet request we are processing\n   * @param response\n   *          The servlet response we are creating",
  "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:<init>(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler,org.apache.hadoop.io.retry.AsyncCallHandler)" : null,
  "org.apache.hadoop.fs.FsShell:getCurrentTrashDir()" : "* Returns the Trash object associated with this shell.\n   * @return Path to the trash\n   * @throws IOException upon error",
  "org.apache.hadoop.conf.StorageUnit$2:toKBs(double)" : null,
  "org.apache.hadoop.ipc.RemoteException:getClassName()" : "* @return the class name for the wrapped exception; may be null if none was given.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStore:addSample(java.lang.String,long)" : "* Add a statistics sample as a min, max and mean and count.\n   * @param key key to add.\n   * @param count count.",
  "org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)" : null,
  "org.apache.hadoop.service.launcher.InterruptEscalator:getService()" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:getAverageResponseTime()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:delete(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class,boolean)" : "* Add an internal servlet in the server, specifying whether or not to\n   * protect with Kerberos authentication.\n   * Note: This method is to be used for adding servlets that facilitate\n   * internal communication and not for user facing functionality. For\n   * servlets added using this method, filters (except internal Kerberos\n   * filters) are not enabled.\n   *\n   * @param name The name of the servlet (can be passed as null)\n   * @param pathSpec The path spec for the servlet\n   * @param clazz The servlet class\n   * @param requireAuth Require Kerberos authenticate to access servlet",
  "org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat)" : "* interface implementation of Zookeeper callback for monitor (exists)",
  "org.apache.hadoop.ipc.DecayRpcScheduler:getTotalCallSnapshot()" : null,
  "org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.MetricsTag)" : null,
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:getCodec()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listXAttrs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.metrics2.util.SampleQuantiles:insertBatch()" : "* Merges items from buffer into the samples array in one pass.\n   * This is more efficient than doing an insert on every item.",
  "org.apache.hadoop.metrics2.util.Contracts:checkArg(java.lang.Object,boolean,java.lang.Object)" : "* Check an argument for false conditions\n   * @param <T> type of the argument\n   * @param arg the argument to check\n   * @param expression  the boolean expression for the condition\n   * @param msg the error message if {@code expression} is false\n   * @return the argument for convenience",
  "org.apache.hadoop.conf.Configuration:setLong(java.lang.String,long)" : "* Set the value of the <code>name</code> property to a <code>long</code>.\n   * \n   * @param name property name.\n   * @param value <code>long</code> value of the property.",
  "org.apache.hadoop.ipc.RpcServerException:<init>(java.lang.String,java.lang.Throwable)" : "* Constructs exception with the specified detail message and cause.\n   * \n   * @param message message.\n   * @param cause the cause (can be retried by the {@link #getCause()} method).\n   *          (A <tt>null</tt> value is permitted, and indicates that the cause\n   *          is nonexistent or unknown.)",
  "org.apache.hadoop.metrics2.lib.MutableRates:<init>(org.apache.hadoop.metrics2.lib.MetricsRegistry)" : null,
  "org.apache.hadoop.http.HttpServer2:constructBindException(org.eclipse.jetty.server.ServerConnector,java.io.IOException)" : "* Create bind exception by wrapping the bind exception thrown.\n   * @param listener\n   * @param ex\n   * @return",
  "org.apache.hadoop.fs.FileUtil:replaceFile(java.io.File,java.io.File)" : "* Move the src file to the name specified by target.\n   * @param src the source file\n   * @param target the target file\n   * @exception IOException If this operation fails",
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getOptionalPath()" : "* Get the optional path; may be empty.\n   * @return the optional path field.",
  "org.apache.hadoop.security.token.delegation.DelegationKey:hashCode()" : null,
  "org.apache.hadoop.fs.GetSpaceUsed$Builder:getInterval()" : null,
  "org.apache.hadoop.fs.VectoredReadUtils:readInDirectBuffer(org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer,org.apache.hadoop.util.functional.Function4RaisingIOE)" : "* Read bytes from stream into a byte buffer using an\n   * intermediate byte array.\n   *   <pre>\n   *     (position, buffer, buffer-offset, length): Void\n   *     position:= the position within the file to read data.\n   *     buffer := a buffer to read fully `length` bytes into.\n   *     buffer-offset := the offset within the buffer to write data\n   *     length := the number of bytes to read.\n   *   </pre>\n   * The passed in function MUST block until the required length of\n   * data is read, or an exception is thrown.\n   * @param range range to read\n   * @param buffer buffer to fill.\n   * @param operation operation to use for reading data.\n   * @throws IOException any IOE.",
  "org.apache.hadoop.fs.shell.Truncate:waitForRecovery()" : "* Wait for all files in waitList to have length equal to newLength.",
  "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:setInput(byte[],int,int)" : "* Sets input data for decompression.\n   * This should be called if and only if {@link #needsInput()} returns\n   * <code>true</code> indicating that more input data is required.\n   * (Both native and non-native versions of various Decompressors require\n   * that the data passed in via <code>b[]</code> remain unmodified until\n   * the caller is explicitly notified--via {@link #needsInput()}--that the\n   * buffer may be safely modified.  With this requirement, an extra\n   * buffer-copy can be avoided.)\n   *\n   * @param b   Input data\n   * @param off Start offset\n   * @param len Length",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Called after a new FileSystem instance is constructed.\n   * @param theUri a uri whose authority section names the host, port, etc. for\n   *        this FileSystem\n   * @param conf the configuration",
  "org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException:getMessage()" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:takeLock(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry$LockType)" : "* Take the read or write lock.\n     *\n     * @param lockType type of the lock.",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcEnQueueTime(long)" : "* Sometimes, the request time observed by the client is much longer than\n   * the queue + process time on the RPC server.Perhaps the RPC request\n   * 'waiting enQueue' took too long on the RPC server, so we should add\n   * enQueue time to RpcMetrics. See HADOOP-18840 for details.\n   * Add an RPC enqueue time sample\n   * @param enQTime the queue time",
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesWritten()" : null,
  "org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)" : "* Write a UTF8 encoded string to out.\n   *\n   * @param out input out.\n   * @param s input s.\n   * @throws IOException raised on errors performing I/O.\n   * @return a UTF8 encoded string to out.",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:ioStatisticsContextAvailable()" : "* Are the IOStatisticsContext methods and classes available?\n   * @return true if the relevant methods are loaded.",
  "org.apache.hadoop.fs.impl.FSBuilderSupport:getLong(java.lang.String,long)" : "* Get a long value with resilience to unparseable values.\n   * @param key key to log\n   * @param defVal default value\n   * @return long value",
  "org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:write(byte[],int,int)" : null,
  "org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)" : null,
  "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:cacheGroupsAdd(java.util.List)" : null,
  "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.reflect.Field)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferPool:acquire(int)" : "* Acquires a {@code ByteBuffer}; blocking if necessary until one becomes available.\n   * @param blockNumber the id of the block to acquire.\n   * @return the acquired block's {@code BufferData}.",
  "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:setContext(java.lang.String)" : null,
  "org.apache.hadoop.fs.store.EtagChecksum:getLength()" : null,
  "org.apache.hadoop.io.compress.SnappyCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)" : "* Create a {@link CompressionOutputStream} that will write to the given\n   * {@link OutputStream} with the given {@link Compressor}.\n   *\n   * @param out        the location for the final output stream\n   * @param compressor compressor to use\n   * @return a stream the user can write uncompressed data to have it compressed\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.impl.MetricGaugeDouble:type()" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:takeLock(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry$LockType,long,java.util.concurrent.TimeUnit)" : "* Try to take the read or write lock within the given timeout.\n     *\n     * @param lockType type of the lock.\n     * @param timeout the time to wait for the given lock.\n     * @param unit the time unit of the timeout argument.\n     * @return true if the lock of the given lock type was acquired.",
  "org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FSProtos$FileStatusProto)" : null,
  "org.apache.hadoop.metrics2.impl.MsInfo:toString()" : null,
  "org.apache.hadoop.fs.HarFileSystem:resolvePath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.metrics2.impl.SinkQueue:setConsumerLock()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)" : null,
  "org.apache.hadoop.util.VersionInfo:_getBranch()" : null,
  "org.apache.hadoop.fs.permission.FsPermission$1:<init>()" : null,
  "org.apache.hadoop.io.MapFile$Reader:seek(org.apache.hadoop.io.WritableComparable)" : "* Positions the reader at the named key, or if none such exists, at the\n     * first entry after the named key.  Returns true iff the named key exists\n     * in this map.\n     *\n     * @param key key.\n     * @throws IOException raised on errors performing I/O.\n     * @return if the named key exists in this map true, not false.",
  "org.apache.hadoop.ipc.Server:getCallQueueLen()" : "* The number of rpc calls in the queue.\n   * @return The number of rpc calls in the queue.",
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:<init>(org.apache.hadoop.fs.viewfs.ChRootedFileSystem,org.apache.hadoop.fs.FileStatus)" : null,
  "org.apache.hadoop.fs.shell.find.FilterExpression:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.conf.Configuration:writeXml(java.io.OutputStream)" : "* Write out the non-default properties in this configuration to the given\n   * {@link OutputStream} using UTF-8 encoding.\n   *\n   * @param out the output stream to write to.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.fs.FileContext:deleteOnExit(org.apache.hadoop.fs.Path)" : "* Mark a path to be deleted on JVM shutdown.\n   * \n   * @param f the existing path to delete.\n   *\n   * @return  true if deleteOnExit is successful, otherwise false.\n   *\n   * @throws AccessControlException If access is denied\n   * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n   *           not supported\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server",
  "org.apache.hadoop.fs.shell.find.Result:combine(org.apache.hadoop.fs.shell.find.Result)" : "* Returns the combination of this and another result.\n   * @param other other.\n   * @return result.",
  "org.apache.hadoop.io.file.tfile.TFile:<init>()" : null,
  "org.apache.hadoop.util.IntrusiveCollection$IntrusiveIterator:remove()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:createHuffmanDecodingTables(int,int)" : "* Called by recvDecodingTables() exclusively.",
  "org.apache.hadoop.crypto.key.UserProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.FileContext:listCorruptFileBlocks(org.apache.hadoop.fs.Path)" : "* List CorruptFile Blocks.\n   *\n   * @param path the path.\n   * @return an iterator over the corrupt files under the given path\n   * (may contain duplicates if a file has more than one corrupt block)\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.ipc.ProtobufWrapperLegacy:<init>(java.lang.Object)" : "* Construct.\n   * The type of the parameter is Object so as to keep the casting internal\n   * to this class.\n   * @param message message to wrap.\n   * @throws IllegalArgumentException if the class is not a protobuf message.",
  "org.apache.hadoop.fs.FileStatus:hasAcl()" : "* Tell whether the underlying file or directory has ACLs set.\n   *\n   * @return true if the underlying file or directory has ACLs set.",
  "org.apache.hadoop.fs.store.DataBlocks$BlockFactory:close()" : "* Implement any close/cleanup operation.\n     * Base class is a no-op.\n     *\n     * @throws IOException Inherited exception; implementations should\n     *                     avoid raising it.",
  "org.apache.hadoop.util.DiskValidatorFactory:getInstance(java.lang.Class)" : "* Returns a {@link DiskValidator} instance corresponding to the passed clazz.\n   * @param clazz a class extends {@link DiskValidator}\n   * @return disk validator.",
  "org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumCountWithProportionalSleep(int,long,java.util.concurrent.TimeUnit)" : "* <p>\n   * Keep trying a limited number of times, waiting a growing amount of time between attempts,\n   * and then fail by re-throwing the exception.\n   * The time between attempts is <code>sleepTime</code> mutliplied by the number of tries so far.\n   * </p>\n   *\n   * @param sleepTime sleepTime.\n   * @param maxRetries maxRetries.\n   * @param timeUnit timeUnit.\n   * @return RetryPolicy.",
  "org.apache.hadoop.crypto.CipherSuite:getAlgorithmBlockSize()" : "* @return size of an algorithm block in bytes",
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:<init>(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation)" : null,
  "org.apache.hadoop.security.UserGroupInformation:createProxyUserForTesting(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.String[])" : "* Create a proxy user UGI for testing HDFS and MapReduce\n   * \n   * @param user\n   *          the full user principal name for effective user\n   * @param realUser\n   *          UGI of the real user\n   * @param userGroups\n   *          the names of the groups that the user belongs to\n   * @return a fake user for running unit tests",
  "org.apache.hadoop.util.VersionInfo:getDate()" : "* The date that Hadoop was compiled.\n   * @return the compilation date in unix date format",
  "org.apache.hadoop.fs.Options$HandleOpt$Location:allowChange()" : "* Tracks whether any changes to file location are permitted.\n       * @return True if relocation in the namespace is allowed, false\n       * otherwise.",
  "org.apache.hadoop.util.curator.ZKCuratorManager:validateSslConfiguration(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.LightWeightGSet$SetIterator:ensureNext()" : null,
  "org.apache.hadoop.fs.GlobalStorageStatistics$StorageIterator:remove()" : null,
  "org.apache.hadoop.fs.HarFileSystem:getCanonicalUri()" : "* Used for delegation token related functionality. Must delegate to\n   * underlying file system.",
  "org.apache.hadoop.security.Credentials:readTokenStorageFile(java.io.File,org.apache.hadoop.conf.Configuration)" : "* Convenience method for reading a token storage file and loading its Tokens.\n   * @param filename filename.\n   * @param conf configuration.\n   * @throws IOException raised on errors performing I/O.\n   * @return Token.",
  "org.apache.hadoop.metrics2.util.SampleQuantiles:snapshot()" : "* Get a snapshot of the current values of all the tracked quantiles.\n   * \n   * @return snapshot of the tracked quantiles. If no items are added\n   * to the estimator, returns null.",
  "org.apache.hadoop.util.ProtoUtil:getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto)" : null,
  "org.apache.hadoop.util.ServletUtil:getParameter(javax.servlet.ServletRequest,java.lang.String)" : "* Get a parameter from a ServletRequest.\n   * Return null if the parameter contains only white spaces.\n   *\n   * @param request request.\n   * @param name name.\n   * @return get a parameter from a ServletRequest.",
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:readFile(java.nio.file.Path,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:removeAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.ConfTest:checkConf(java.io.InputStream)" : null,
  "org.apache.hadoop.tools.TableListing$Column:addRow(java.lang.String)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:bufferSize(int)" : null,
  "org.apache.hadoop.net.SocksSocketFactory:<init>()" : "* Default empty constructor (for use with the reflection API).",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:getBlockIndex()" : null,
  "org.apache.hadoop.util.VersionInfo:_getUrl()" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithFixedSleep:calculateSleepTime(int)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTopTokenRealOwners(int)" : "* Return top token real owners list as well as the tokens count.\n   *\n   * @param n top number of users\n   * @return map of owners to counts",
  "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:publishMetricsFromQueue()" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:maximums()" : null,
  "org.apache.hadoop.ipc.Server:getDelimitedLength(org.apache.hadoop.thirdparty.protobuf.Message)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem:notFoundStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.file.tfile.Utils:writeVLong(java.io.DataOutput,long)" : "* Encoding a Long integer into a variable-length encoding format.\n   * <ul>\n   * <li>if n in [-32, 127): encode in one byte with the actual value.\n   * Otherwise,\n   * <li>if n in [-20*2^8, 20*2^8): encode in two bytes: byte[0] = n/256 - 52;\n   * byte[1]=n&amp;0xff. Otherwise,\n   * <li>if n IN [-16*2^16, 16*2^16): encode in three bytes: byte[0]=n/2^16 -\n   * 88; byte[1]=(n&gt;&gt;8)&amp;0xff; byte[2]=n&amp;0xff. Otherwise,\n   * <li>if n in [-8*2^24, 8*2^24): encode in four bytes: byte[0]=n/2^24 - 112;\n   * byte[1] = (n&gt;&gt;16)&amp;0xff; byte[2] = (n&gt;&gt;8)&amp;0xff;\n   * byte[3]=n&amp;0xff.\n   * Otherwise:\n   * <li>if n in [-2^31, 2^31): encode in five bytes: byte[0]=-125; byte[1] =\n   * (n&gt;&gt;24)&amp;0xff; byte[2]=(n&gt;&gt;16)&amp;0xff;\n   * byte[3]=(n&gt;&gt;8)&amp;0xff; byte[4]=n&amp;0xff;\n   * <li>if n in [-2^39, 2^39): encode in six bytes: byte[0]=-124; byte[1] =\n   * (n&gt;&gt;32)&amp;0xff; byte[2]=(n&gt;&gt;24)&amp;0xff;\n   * byte[3]=(n&gt;&gt;16)&amp;0xff; byte[4]=(n&gt;&gt;8)&amp;0xff;\n   * byte[5]=n&amp;0xff\n   * <li>if n in [-2^47, 2^47): encode in seven bytes: byte[0]=-123; byte[1] =\n   * (n&gt;&gt;40)&amp;0xff; byte[2]=(n&gt;&gt;32)&amp;0xff;\n   * byte[3]=(n&gt;&gt;24)&amp;0xff; byte[4]=(n&gt;&gt;16)&amp;0xff;\n   * byte[5]=(n&gt;&gt;8)&amp;0xff; byte[6]=n&amp;0xff;\n   * <li>if n in [-2^55, 2^55): encode in eight bytes: byte[0]=-122; byte[1] =\n   * (n&gt;&gt;48)&amp;0xff; byte[2] = (n&gt;&gt;40)&amp;0xff;\n   * byte[3]=(n&gt;&gt;32)&amp;0xff; byte[4]=(n&gt;&gt;24)&amp;0xff; byte[5]=\n   * (n&gt;&gt;16)&amp;0xff; byte[6]=(n&gt;&gt;8)&amp;0xff; byte[7]=n&amp;0xff;\n   * <li>if n in [-2^63, 2^63): encode in nine bytes: byte[0]=-121; byte[1] =\n   * (n&gt;&gt;54)&amp;0xff; byte[2] = (n&gt;&gt;48)&amp;0xff;\n   * byte[3] = (n&gt;&gt;40)&amp;0xff; byte[4]=(n&gt;&gt;32)&amp;0xff;\n   * byte[5]=(n&gt;&gt;24)&amp;0xff; byte[6]=(n&gt;&gt;16)&amp;0xff; byte[7]=\n   * (n&gt;&gt;8)&amp;0xff; byte[8]=n&amp;0xff;\n   * </ul>\n   * \n   * @param out\n   *          output stream\n   * @param n\n   *          the integer number\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createConnection(java.net.URL,java.lang.String)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:setDictionary(byte[],int,int)" : null,
  "org.apache.hadoop.metrics2.MetricsJsonBuilder:add(org.apache.hadoop.metrics2.MetricsTag)" : null,
  "org.apache.hadoop.net.NetworkTopology:getDistanceByPath(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)" : "Return the distance between two nodes by comparing their network paths\n   * without checking if they belong to the same ancestor node by reference.\n   * It is assumed that the distance from one node to its parent is 1\n   * The distance between two nodes is calculated by summing up their distances\n   * to their closest common ancestor.\n   * @param node1 one node\n   * @param node2 another node\n   * @return the distance between node1 and node2",
  "org.apache.hadoop.http.HttpServer2:addDefaultServlets(org.apache.hadoop.conf.Configuration)" : "* Add default servlets.\n   * @param configuration the hadoop configuration",
  "org.apache.hadoop.io.WritableUtils:writeStringArray(java.io.DataOutput,java.lang.String[])" : null,
  "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:getLogger()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* When ViewFileSystemOverloadScheme scheme and target uri scheme are\n     * matching, it will not take advantage of FileSystem cache as it will\n     * create instance directly. For caching needs please set\n     * \"fs.viewfs.enable.inner.cache\" to true.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)" : "* Aggregate those statistics which the store is tracking;\n   * ignore the rest.\n   *\n   * @param source statistics; may be null\n   * @return true if a statistics reference was supplied/aggregated.",
  "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsBlockedAcls(java.lang.Class)" : null,
  "org.apache.hadoop.fs.Globber:getPathComponents(java.lang.String)" : "* Translate an absolute path into a list of path components.\n   * We merge double slashes into a single slash here.\n   * POSIX root path, i.e. '/', does not get an entry in the list.",
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:writeTrailer(byte[],int,int)" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:getIOStatistics()" : null,
  "org.apache.hadoop.io.EnumSetWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:replication(short)" : null,
  "org.apache.hadoop.ipc.Server:getNumReaders()" : "* The number of reader threads for this server.\n   * @return The number of reader threads.",
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:isFile()" : null,
  "org.apache.hadoop.fs.shell.Count:<init>()" : "Constructor",
  "org.apache.hadoop.fs.RawLocalFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:close()" : null,
  "org.apache.hadoop.fs.DelegateToFileSystem:getUriDefaultPort()" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:exitWithMessage(int,java.lang.String)" : "* Exit with a printed message. \n   * @param status status code\n   * @param message message message to print before exiting\n   * @throws ExitUtil.ExitException if exceptions are disabled",
  "org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer:run()" : null,
  "org.apache.hadoop.fs.shell.FsUsage$Du:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.metrics2.sink.GraphiteSink:init(org.apache.commons.configuration2.SubsetConfiguration)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementGauge(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* The src file is on the local disk.  Add it to the filesystem at\n   * the given dst name.\n   * delSrc indicates if the source should be removed\n   * @param delSrc whether to delete the src\n   * @param src path\n   * @param dst path\n   * @throws IOException IO failure.",
  "org.apache.hadoop.util.functional.CallableRaisingIOE:unchecked()" : "* Apply unchecked.\n   * @return the evaluated call\n   * @throws UncheckedIOException IOE raised.",
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:counter(org.apache.hadoop.metrics2.MetricsInfo,int)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:supportsSymlinks()" : null,
  "org.apache.hadoop.util.DiskChecker$DefaultFileIoProvider:write(java.io.FileOutputStream,byte[])" : "* See {@link FileOutputStream#write(byte[])}.",
  "org.apache.hadoop.fs.InvalidPathHandleException:<init>(java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.net.DNS:resolveLocalHostIPAddress()" : "* Get the IPAddress of the local host as a string.\n   * This will be a loop back value if the local host address cannot be\n   * determined.\n   * If the loopback address of \"localhost\" does not resolve, then the system's\n   * network is in such a state that nothing is going to work. A message is\n   * logged at the error level and a null pointer returned, a pointer\n   * which will trigger failures later on the application\n   * @return the IPAddress of the local host or null for a serious problem.",
  "org.apache.hadoop.ha.ZKFailoverController:cedeActive(int)" : "* Request from graceful failover to cede active role. Causes\n   * this ZKFC to transition its local node to standby, then quit\n   * the election for the specified period of time, after which it\n   * will rejoin iff it is healthy.",
  "org.apache.hadoop.security.alias.CredentialProvider:isTransient()" : "* Indicates whether this provider represents a store\n   * that is intended for transient use - such as the UserProvider\n   * is. These providers are generally used to provide job access to\n   * passwords rather than for long term storage.\n   * @return true if transient, false otherwise",
  "org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(int,int)" : "* Create a BoundedByteArrayOutputStream with the specified\n   * capacity and limit.\n   * @param capacity The capacity of the underlying byte array\n   * @param limit The maximum limit upto which data can be written",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.ipc.Server$Listener$Reader:shutdown()" : null,
  "org.apache.hadoop.fs.FileUtil:listFiles(java.io.File)" : "* A wrapper for {@link File#listFiles()}. This java.io API returns null\n   * when a dir is not a directory or for any I/O error. Instead of having\n   * null check everywhere File#listFiles() is used, we will add utility API\n   * to get around this problem. For the majority of cases where we prefer\n   * an IOException to be thrown.\n   * @param dir directory for which listing should be performed\n   * @return list of files or empty list\n   * @exception IOException for invalid directory or for a bad disk.",
  "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:getComparatorString()" : null,
  "org.apache.hadoop.io.ObjectWritable$NullInstance:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ha.HAServiceStatus:<init>(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState)" : null,
  "org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator:<init>()" : null,
  "org.apache.hadoop.io.LongWritable:get()" : "* Return the value of this LongWritable.\n   * @return value of this LongWritable.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:getID()" : "* ID of this context.\n   * @return ID.",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:mapBlock(java.lang.String,long,boolean)" : null,
  "org.apache.hadoop.ipc.RefreshResponse:setSenderName(java.lang.String)" : "* Optionally set the sender of this RefreshResponse.\n   * This helps clarify things when multiple handlers respond.\n   * @param name The name of the sender",
  "org.apache.hadoop.util.StringUtils:formatPercent(double,int)" : "* Format a percentage for presentation to the user.\n   * @param fraction the percentage as a fraction, e.g. 0.1 = 10%\n   * @param decimalPlaces the number of decimal places\n   * @return a string representation of the percentage",
  "org.apache.hadoop.util.NativeCrc32:calculateChunkedSums(int,int,java.nio.ByteBuffer,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.ipc.ProcessingDetails:set(org.apache.hadoop.ipc.ProcessingDetails$Timing,long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)" : null,
  "org.apache.hadoop.fs.Globber:getFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.http.CrossOriginFilter:doCrossFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO:getOperatingSystemPageSize()" : "* @return the operating system's page size.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withMinimums(java.lang.String[])" : null,
  "org.apache.hadoop.fs.DelegateToFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:finish()" : null,
  "org.apache.hadoop.ipc.Client:checkAsyncCall()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getMountTableConfigLoader(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.shell.CommandFormat:getOpt(java.lang.String)" : "Return if the option is set or not\n   * \n   * @param option String representation of an option\n   * @return true is the option is set; false otherwise",
  "org.apache.hadoop.fs.DUHelper:main(java.lang.String[])" : null,
  "org.apache.hadoop.fs.shell.CopyCommands$Cp:popPreserveOption(java.util.List)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:<init>(int)" : "* Constructs a resource pool of the given size.\n   *\n   * @param size the size of this pool. Cannot be changed post creation.\n   *\n   * @throws IllegalArgumentException if size is zero or negative.",
  "org.apache.hadoop.conf.Configuration:setRestrictSystemProperties(boolean)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$MountPoint:<init>(java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)" : null,
  "org.apache.hadoop.util.ThreadUtil:getResourceAsStream(java.lang.ClassLoader,java.lang.String)" : "* Convenience method that returns a resource as inputstream from the\n   * classpath using given classloader.\n   * <p>\n   *\n   * @param cl ClassLoader to be used to retrieve resource.\n   * @param resourceName resource to retrieve.\n   *\n   * @throws IOException thrown if resource cannot be loaded\n   * @return inputstream with the resource.",
  "org.apache.hadoop.util.StringUtils:getFormattedTimeWithDiff(org.apache.commons.lang3.time.FastDateFormat,long,long)" : "* Formats time in ms and appends difference (finishTime - startTime)\n   * as returned by formatTimeDiff().\n   * If finish time is 0, empty string is returned, if start time is 0\n   * then difference is not appended to return value.\n   *\n   * @param dateFormat date format to use\n   * @param finishTime finish time\n   * @param startTime  start time\n   * @return formatted value.",
  "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getMaxBlocksCount()" : "* @return The max blocks count to be kept in cache at any time.",
  "org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:equals(java.lang.Object)" : "* remoteExceptionToRetry is ignored as part of equals since it does not\n     * affect connection failure handling.",
  "org.apache.hadoop.crypto.key.UserProvider:deleteKey(java.lang.String)" : null,
  "org.apache.hadoop.crypto.CipherSuite:getName()" : "* @return name of cipher suite, as in {@link javax.crypto.Cipher}",
  "org.apache.hadoop.fs.shell.Delete$Rm:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesReadDistanceOfThreeOrFour()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager)" : "* Sets an external <code>DelegationTokenSecretManager</code> instance to\n   * manage creation and verification of Delegation Tokens.\n   * <p>\n   * This is useful for use cases where secrets must be shared across multiple\n   * services.\n   *\n   * @param secretManager a <code>DelegationTokenSecretManager</code> instance",
  "org.apache.hadoop.metrics2.util.SampleQuantiles:query(double)" : "* Get the estimated value at the specified quantile.\n   * \n   * @param quantile Queried quantile, e.g. 0.50 or 0.99.\n   * @return Estimated value at that quantile.",
  "org.apache.hadoop.service.ServiceStateException:getExitCode()" : null,
  "org.apache.hadoop.ha.FailoverController:createReqInfo()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferPool:canRelease(org.apache.hadoop.fs.impl.prefetch.BufferData)" : null,
  "org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:cedeActive(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.ZKFCProtocolProtos$CedeActiveRequestProto)" : null,
  "org.apache.hadoop.fs.HardLink:createIOException(java.io.File,java.lang.String,java.lang.String,int,java.lang.Exception)" : null,
  "org.apache.hadoop.fs.local.LocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* This constructor has the signature needed by\n   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.\n   * \n   * @param theUri which must be that of localFs\n   * @param conf\n   * @throws IOException\n   * @throws URISyntaxException",
  "org.apache.hadoop.fs.AbstractFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)" : "* Fully replaces ACL of files and directories, discarding all existing\n   * entries.\n   *\n   * @param path Path to modify\n   * @param aclSpec List{@literal <AclEntry>} describing modifications, must\n   * include entries for user, group, and others for compatibility with\n   * permission bits.\n   * @throws IOException if an ACL could not be modified",
  "org.apache.hadoop.fs.HarFileSystem:append(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.permission.AclUtil:isMinimalAcl(java.util.List)" : "* Checks if the given entries represent a minimal ACL (contains exactly 3\n   * entries).\n   *\n   * @param entries List&lt;AclEntry&gt; entries to check\n   * @return boolean true if the entries represent a minimal ACL",
  "org.apache.hadoop.fs.FileSystem$Statistics:incrementLargeReadOps(int)" : "* Increment the number of large read operations.\n     * @param count number of large read operations",
  "org.apache.hadoop.net.NetUtils:getHostDetailsAsString(java.lang.String,int,java.lang.String)" : "* Get the host details as a string\n   * @param destHost destinatioon host (nullable)\n   * @param destPort destination port\n   * @param localHost local host (nullable)\n   * @return a string describing the destination host:port and the local host",
  "org.apache.hadoop.util.DataChecksum:getHeader()" : null,
  "org.apache.hadoop.security.UserGroupInformation:isHadoopLogin()" : "* Is the ugi managed by the UGI or an external subject?\n   * @return true if managed by UGI.",
  "org.apache.hadoop.net.NetUtils:createSocketAddrUnresolved(java.lang.String)" : null,
  "org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision,long)" : null,
  "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Constructor.\n   * @param fileSystem owner FS.\n   * @param path path",
  "org.apache.hadoop.log.LogLevel$Servlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : "* Opens an FSDataOutputStream at the indicated Path with write-progress\n   * reporting. Same as create(), except fails if parent directory doesn't\n   * already exist.\n   * @param f the file name to open\n   * @param permission file permission\n   * @param overwrite if a file with this name already exists, then if true,\n   * the file will be overwritten, and if false an error will be thrown.\n   * @param bufferSize the size of the buffer to be used.\n   * @param replication required block replication for the file.\n   * @param blockSize block size\n   * @param progress the progress reporter\n   * @throws IOException IO failure\n   * @see #setPermission(Path, FsPermission)\n   * @return output stream.",
  "org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:<init>(org.apache.hadoop.security.UserGroupInformation$LoginParams)" : null,
  "org.apache.hadoop.metrics2.MetricsTag:name()" : null,
  "org.apache.hadoop.crypto.key.KeyProvider$Metadata:addVersion()" : null,
  "org.apache.hadoop.util.Timer:now()" : "* Current system time.  Do not use this to calculate a duration or interval\n   * to sleep, because it will be broken by settimeofday.  Instead, use\n   * monotonicNow.\n   * @return current time in msec.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.util.ExitUtil:resetFirstExitException()" : "* Reset the tracking of process termination. This is for use in unit tests\n   * where one test in the suite expects an exit but others do not.",
  "org.apache.hadoop.ipc.RetryCache$CacheEntry:getNext()" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,double)" : null,
  "org.apache.hadoop.util.CpuTimeTracker:getCumulativeCpuTime()" : "* Obtain the cumulative CPU time since the system is on.\n   * @return cumulative CPU time in milliseconds",
  "org.apache.hadoop.io.SequenceFile$Writer:blockSize(long)" : null,
  "org.apache.hadoop.security.Credentials:numberOfSecretKeys()" : "* @return number of keys in the in-memory map",
  "org.apache.hadoop.util.ComparableVersion:compareTo(org.apache.hadoop.util.ComparableVersion)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:constructReasonString(int)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.util.LightWeightGSet:toString()" : null,
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getPath()" : null,
  "org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.io.erasurecode.CodecUtil:createCodec(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)" : null,
  "org.apache.hadoop.conf.Configuration:convertStorageUnit(double,org.apache.hadoop.conf.StorageUnit,org.apache.hadoop.conf.StorageUnit)" : "* convert the value from one storage unit to another.\n   *\n   * @param value - value\n   * @param sourceUnit - Source unit to convert from\n   * @param targetUnit - target unit.\n   * @return double.",
  "org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Object,java.lang.Throwable,org.apache.hadoop.io.retry.CallReturn$State)" : null,
  "org.apache.hadoop.io.erasurecode.ErasureCodeNative:<init>()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:listCorruptFileBlocks(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:close()" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:toString()" : null,
  "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:getDelay(java.util.concurrent.TimeUnit)" : "Get the delay until this event should happen.",
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem:listStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:bind(java.lang.Object)" : "* Returns this method as a BoundMethod for the given receiver.\n     * @param receiver an Object to receive the method invocation\n     * @return a {@link BoundMethod} for this method and the receiver\n     * @throws IllegalStateException if the method is static\n     * @throws IllegalArgumentException if the receiver's class is incompatible",
  "org.apache.hadoop.net.SocketIOWithTimeout:getChannel()" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:hashCode()" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:disconnect(com.jcraft.jsch.ChannelSftp)" : "* Logout and disconnect the given channel.\n   *\n   * @param client\n   * @throws IOException",
  "org.apache.hadoop.util.LambdaUtils:eval(java.util.concurrent.CompletableFuture,java.util.concurrent.Callable)" : "* Utility method to evaluate a callable and fill in the future\n   * with the result or the exception raised.\n   * Once this method returns, the future will have been evaluated to\n   * either a return value or an exception.\n   * @param <T> type of future\n   * @param result future for the result.\n   * @param call callable to invoke.\n   * @return the future passed in",
  "org.apache.hadoop.io.SortedMapWritable:remove(java.lang.Object)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:getProtocolImpl(org.apache.hadoop.ipc.RPC$Server,java.lang.String,long)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferData:toString()" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:available()" : null,
  "org.apache.hadoop.util.DataChecksum:<init>(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,int)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.io.MD5Hash:setDigest(java.lang.String)" : "* Sets the digest value from a hex string.\n   * @param hex hex.",
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getThisBuilder()" : "* Get the cast builder.\n   * @return this object, typecast",
  "org.apache.hadoop.util.PureJavaCrc32C:reset()" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(java.lang.String,java.lang.String,long)" : "* Create a mutable long integer counter\n   * @param name  of the metric\n   * @param desc  metric description\n   * @param iVal  initial value\n   * @return a new counter object",
  "org.apache.hadoop.fs.store.ByteBufferInputStream:mark(int)" : null,
  "org.apache.hadoop.fs.shell.find.And:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory)" : "Registers this expression with the specified factory.",
  "org.apache.hadoop.io.file.tfile.TFile$Writer$ValueRegister:close()" : null,
  "org.apache.hadoop.io.retry.AsyncCallHandler:getLowerLayerAsyncReturn()" : null,
  "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:create(int)" : null,
  "org.apache.hadoop.fs.Options$CreateOpts:blockSize(long)" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:connect()" : "* Connecting by using configuration parameters.\n   *\n   * @return An FTPClient instance\n   * @throws IOException",
  "org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)" : "Merge the provided files.\n     * @param inFiles the array of input path names\n     * @param outFile the final output file\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.ContentSummary:getHeader(boolean)" : "Return the header of the output.\n   * if qOption is false, output directory count, file count, and content size;\n   * if qOption is true, output quota and remaining quota as well.\n   * \n   * @param qOption a flag indicating if quota needs to be printed or not\n   * @return the header of the output",
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:setInput(byte[],int,int)" : "* Sets input data for decompression.\n   * This should be called if and only if {@link #needsInput()} returns\n   * <code>true</code> indicating that more input data is required.\n   * (Both native and non-native versions of various Decompressors require\n   * that the data passed in via <code>b[]</code> remain unmodified until\n   * the caller is explicitly notified--via {@link #needsInput()}--that the\n   * buffer may be safely modified.  With this requirement, an extra\n   * buffer-copy can be avoided.)\n   *\n   * @param b   Input data\n   * @param off Start offset\n   * @param len Length",
  "org.apache.hadoop.util.ExitUtil:haltCalled()" : "* @return true if halt has been called.",
  "org.apache.hadoop.http.HttpServer2Metrics:asyncRequestsWaitingMax()" : null,
  "org.apache.hadoop.fs.permission.AclEntry:getScope()" : "* Returns the scope of the ACL entry.\n   *\n   * @return AclEntryScope scope of the ACL entry",
  "org.apache.hadoop.io.InputBuffer:reset(byte[],int,int)" : "* Resets the data that the buffer reads.\n   * @param input input.\n   * @param start start.\n   * @param length length.",
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:resolvePartialGroupNames(java.lang.String,java.lang.String,java.lang.String)" : "* Attempt to partially resolve group names.\n   *\n   * @param userName the user's name\n   * @param errMessage error message from the shell command\n   * @param groupNames the incomplete list of group names\n   * @return a set of resolved group names\n   * @throws PartialGroupNameException if the resolution fails or times out",
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:close()" : null,
  "org.apache.hadoop.net.SocketInputWrapper:getReadableByteChannel()" : "* @return an underlying ReadableByteChannel implementation.\n   * @throws IllegalStateException if this socket does not have a channel",
  "org.apache.hadoop.ha.ZKFailoverController:doCedeActive(int)" : null,
  "org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:hasNext()" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:close()" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(java.net.URI,org.apache.hadoop.crypto.key.kms.KMSClientProvider[],org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)" : "Append a key/value pair.",
  "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getRemoteException(org.apache.hadoop.thirdparty.protobuf.ServiceException)" : "* Return the IOException thrown by the remote server wrapped in\n   * ServiceException as cause.\n   * The signature of this method changes with updates to the hadoop-thirdparty\n   * shaded protobuf library.\n   * @param se ServiceException that wraps IO exception thrown by the server\n   * @return Exception wrapped in ServiceException or\n   * a new IOException that wraps the unexpected ServiceException.",
  "org.apache.hadoop.util.GSetByHashMap:clear()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:idempotentOrAtMostOnce(java.lang.reflect.Method)" : null,
  "org.apache.hadoop.io.Text:charAt(int)" : "* Returns the Unicode Scalar Value (32-bit integer value)\n   * for the character at <code>position</code>. Note that this\n   * method avoids using the converter or doing String instantiation.\n   *\n   * @param position input position.\n   * @return the Unicode scalar value at position or -1\n   *          if the position is invalid or points to a\n   *          trailing byte",
  "org.apache.hadoop.fs.ftp.FTPFileSystem:setDataConnectionMode(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.conf.Configuration)" : "* Set the FTPClient's data connection mode based on configuration. Valid\n   * values are ACTIVE_LOCAL_DATA_CONNECTION_MODE,\n   * PASSIVE_LOCAL_DATA_CONNECTION_MODE and PASSIVE_REMOTE_DATA_CONNECTION_MODE.\n   * <p>\n   * Defaults to ACTIVE_LOCAL_DATA_CONNECTION_MODE.\n   *\n   * @param client\n   * @param conf\n   * @throws IOException",
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initSystemMBean()" : null,
  "org.apache.hadoop.metrics2.lib.Interns$CacheWith2Keys:add(java.lang.Object,java.lang.Object)" : null,
  "org.apache.hadoop.fs.FileUtil:grantPermissions(java.io.File)" : null,
  "org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket)" : "* Same as getOutputStream(socket, 0). Timeout of zero implies write will\n   * wait until data is available.<br><br>\n   * \n   * From documentation for {@link #getOutputStream(Socket, long)} : <br>\n   * Returns OutputStream for the socket. If the socket has an associated\n   * SocketChannel then it returns a \n   * {@link SocketOutputStream} with the given timeout. If the socket does not\n   * have a channel, {@link Socket#getOutputStream()} is returned. In the later\n   * case, the timeout argument is ignored and the write will wait until \n   * data is available.<br><br>\n   * \n   * Any socket created using socket factories returned by {@link NetUtils},\n   * must use this interface instead of {@link Socket#getOutputStream()}.\n   * \n   * @see #getOutputStream(Socket, long)\n   * \n   * @param socket socket.\n   * @return OutputStream for writing to the socket.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.LightWeightGSet:computeCapacity(double,java.lang.String)" : "* Let t = percentage of max memory.\n   * Let e = round(log_2 t).\n   * Then, we choose capacity = 2^e/(size of reference),\n   * unless it is outside the close interval [1, 2^30].\n   *\n   * @param mapName mapName.\n   * @param percentage percentage.\n   * @return compute capacity.",
  "org.apache.hadoop.fs.CompositeCrcFileChecksum:toString()" : null,
  "org.apache.hadoop.util.functional.Tuples$Tuple:setValue(java.lang.Object)" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_toJsonString(java.io.Serializable)" : "* Save IOStatisticsSnapshot to a JSON string.\n   * @param snapshot statistics; may be null or of an incompatible type\n   * @return JSON string value or null if source is not an IOStatisticsSnapshot\n   * @throws UncheckedIOException Any IO/jackson exception.\n   * @throws UnsupportedOperationException if the IOStatistics classes were not found",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileChecksum(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:selectDelegationToken(java.net.URL,org.apache.hadoop.security.Credentials)" : "* Select a delegation token from all tokens in credentials, based on url.\n   *\n   * @param url url.\n   * @param creds credentials.\n   * @return token.",
  "org.apache.hadoop.security.SaslRpcServer$QualityOfProtection:getSaslQop()" : null,
  "org.apache.hadoop.io.Text:bytesToCodePoint(java.nio.ByteBuffer)" : "* @return Returns the next code point at the current position in\n   * the buffer. The buffer's position will be incremented.\n   * Any mark set on this buffer will be changed by this method!\n   *\n   * @param bytes input bytes.",
  "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,long,long)" : "* Constructor\n   * \n   * @param in\n   *          The FSDataInputStream we connect to.\n   * @param offset\n   *          Beginning offset of the region.\n   * @param length\n   *          Length of the region.\n   * \n   *          The actual length of the region may be smaller if (off_begin +\n   *          length) goes beyond the end of FS input stream.",
  "org.apache.hadoop.crypto.key.UserProvider:rollNewVersion(java.lang.String,byte[])" : null,
  "org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream)" : null,
  "org.apache.hadoop.security.UserGroupInformation:getKerberosLoginRenewalExecutor()" : null,
  "org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getWriter(java.lang.Class)" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:responses2xx()" : null,
  "org.apache.hadoop.io.OutputBuffer:write(java.io.InputStream,int)" : "* Writes bytes from a InputStream directly into the buffer.\n   * @param in input in.\n   * @param length input length.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.net.NetUtils:isValidSubnet(java.lang.String)" : "* isValidSubnet.\n   * @param subnet subnet.\n   * @return true if the given string is a subnet specified\n   *     using CIDR notation, false otherwise",
  "org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:toByteArray()" : "* Convert to a byte array.\n     * If the data is stored in a file, it will be read and returned.\n     * If the data was passed in via an input stream (which happens if the\n     * data is stored in a bytebuffer) then it will be converted to a byte\n     * array -which will then be cached for any subsequent use.\n     *\n     * @return byte[] after converting the uploadBlock.\n     * @throws IOException throw if an exception is caught while reading\n     *                     File/InputStream or closing InputStream.",
  "org.apache.hadoop.metrics2.util.Servers:parse(java.lang.String,int)" : "* Parses a space and/or comma separated sequence of server specifications\n   * of the form <i>hostname</i> or <i>hostname:port</i>.  If\n   * the specs string is null, defaults to localhost:defaultPort.\n   *\n   * @param specs   server specs (see description)\n   * @param defaultPort the default port if not specified\n   * @return a list of InetSocketAddress objects.",
  "org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:<init>(java.io.DataOutputStream,byte[])" : "* Constructor.\n     * \n     * @param out\n     *          the underlying output stream.\n     * @param buf\n     *          user-supplied buffer. The buffer would be used exclusively by\n     *          the ChunkEncoder during its life cycle.",
  "org.apache.hadoop.util.functional.FunctionalIO:<init>()" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:checkStream()" : null,
  "org.apache.hadoop.util.Daemon:<init>(java.lang.Runnable)" : "* Construct a daemon thread.\n   * @param runnable runnable.",
  "org.apache.hadoop.tracing.Tracer:getCurrentSpan()" : "*\n   * Return active span.\n   * @return org.apache.hadoop.tracing.Span",
  "org.apache.hadoop.util.HostsFileReader:readFirstTagValue(org.w3c.dom.Element,java.lang.String)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:end()" : null,
  "org.apache.hadoop.fs.impl.OpenFileParameters:<init>()" : null,
  "org.apache.hadoop.ipc.Server$Connection:getRemotePort()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getContentSummary(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)" : null,
  "org.apache.hadoop.fs.Path:getParent()" : "* Returns the parent of a path or null if at root. Better alternative is\n   * {@link #getOptionalParentPath()} to handle nullable value for root path.\n   *\n   * @return the parent of a path or null if at root",
  "org.apache.hadoop.http.HttpServer2$Builder:build()" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:compress(byte[],int,int)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:getStatistics(java.net.URI)" : "* Get the statistics for a particular file system.\n   * \n   * @param uri\n   *          used as key to lookup STATISTICS_TABLE. Only scheme and authority\n   *          part of the uri are used.\n   * @return a statistics object",
  "org.apache.hadoop.ipc.ProtobufWrapperLegacy:isUnshadedProtobufMessage(java.lang.Object)" : "* Is a message an unshaded protobuf message?\n   * @param payload payload\n   * @return true if protobuf.jar is on the classpath and the payload is a Message",
  "org.apache.hadoop.io.compress.DecompressorStream:close()" : null,
  "org.apache.hadoop.io.SequenceFile$Metadata:hashCode()" : null,
  "org.apache.hadoop.fs.store.DataBlocks$DataBlock:blockAllocated()" : "* A block has been allocated.",
  "org.apache.hadoop.util.Preconditions:checkState(boolean)" : "* Ensures the truth of an expression involving the state of the calling instance\n   * without involving any parameters to the calling method.\n   *\n   * @param expression a boolean expression\n   * @throws IllegalStateException if {@code expression} is false",
  "org.apache.hadoop.fs.viewfs.ChRootedFs:open(org.apache.hadoop.fs.Path,int)" : null,
  "org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit()" : null,
  "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:bind(java.lang.Object)" : null,
  "org.apache.hadoop.crypto.OpensslCipher:clean()" : "Forcibly clean the context.",
  "org.apache.hadoop.crypto.CryptoInputStream:getTmpBuf()" : null,
  "org.apache.hadoop.jmx.JMXJsonServlet:doTrace(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : "* Disable TRACE method to avoid TRACE vulnerability.",
  "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:getPiggyBacksFromInput(java.nio.ByteBuffer[],int[],int,int,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)" : null,
  "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.File,boolean,org.apache.hadoop.conf.Configuration)" : "* Copy FileSystem files to local files.\n   *\n   * @param srcFS srcFs.\n   * @param src src.\n   * @param dst dst.\n   * @param deleteSource delete source.\n   * @param conf configuration.\n   * @throws IOException raised on errors performing I/O.\n   * @return true if the operation succeeded.",
  "org.apache.hadoop.io.InputBuffer:<init>()" : "Constructs a new empty buffer.",
  "org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : "* Create the local directory if necessary, also ensure permissions\n   * allow it to be read from and written into. Perform some diskIO\n   * to ensure that the disk is usable for writes. \n   *\n   * @param localFS local filesystem\n   * @param dir directory\n   * @param expected permission\n   * @throws DiskErrorException disk problem.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.DecompressorStream:mark(int)" : null,
  "org.apache.hadoop.ipc.Client$ConnectionId:setAddress(java.net.InetSocketAddress)" : "* This is used to update the remote address when an address change is detected.  This method\n     * ensures that the {@link #hashCode()} won't change.\n     *\n     * @param address the updated address\n     * @throws IllegalArgumentException if the hostname or port doesn't match\n     * @see Connection#updateAddress()",
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_gauges(java.io.Serializable)" : "* Get the gauges of an IOStatisticsSnapshot.\n   * @param source source of statistics.\n   * @return the map of gauges.",
  "org.apache.hadoop.fs.store.ByteBufferInputStream:close()" : "* After the stream is closed, set the local reference to the byte\n   * buffer to null; this guarantees that future attempts to use\n   * stream methods will fail.",
  "org.apache.hadoop.conf.StorageUnit$2:fromBytes(double)" : null,
  "org.apache.hadoop.util.MergeSort:swap(int[],int,int)" : null,
  "org.apache.hadoop.io.AbstractMapWritable:<init>()" : "constructor.",
  "org.apache.hadoop.fs.FilterFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)" : "* Create the named map using the named key comparator.\n     * @deprecated Use Writer(Configuration, Path, Option...)} instead.\n     *\n     * @param conf configuration.\n     * @param fs filesystem.\n     * @param dirName dirName.\n     * @param comparator comparator.\n     * @param valClass valClass.\n     * @param compress CompressionType.\n     * @param progress progress.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.Client$Connection$1:run()" : null,
  "org.apache.hadoop.util.InstrumentedWriteLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long,org.apache.hadoop.util.Timer)" : null,
  "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:close()" : null,
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:internalReset()" : null,
  "org.apache.hadoop.ipc.ProtocolSignature:<init>(long,int[])" : "* Constructor\n   * \n   * @param version server version\n   * @param methodHashcodes hash codes of the methods supported by server",
  "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:<init>(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler)" : null,
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.RawPathHandle:bytes()" : null,
  "org.apache.hadoop.metrics2.lib.MutableQuantiles:setInterval(int)" : "* Set the rollover interval (in seconds) of the estimator.\n   *\n   * @param pIntervalSecs of the estimator.",
  "org.apache.hadoop.ipc.CallQueueManager:peek()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:snapshotMap(java.util.Map)" : "* Take a snapshot of a supplied map, where the copy option simply\n   * uses the existing value.\n   *\n   * For this to be safe, the map must refer to immutable objects.\n   * @param source source map\n   * @param <E> type of values.\n   * @return a new map referencing the same values.",
  "org.apache.hadoop.fs.FileSystem:getStatistics(java.lang.String,java.lang.Class)" : "* Get the statistics for a particular file system.\n   * @param scheme scheme.\n   * @param cls the class to lookup\n   * @return a statistics object\n   * @deprecated use {@link #getGlobalStorageStatistics()}",
  "org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set,org.apache.hadoop.fs.BlockLocation[])" : "* Constructor.\n   *\n   * @param length a file's length\n   * @param isdir if the path is a directory\n   * @param block_replication the file's replication factor\n   * @param blocksize a file's block size\n   * @param modification_time a file's modification time\n   * @param access_time a file's access time\n   * @param permission a file's permission\n   * @param owner a file's owner\n   * @param group a file's group\n   * @param symlink symlink if the path is a symbolic link\n   * @param path the path's qualified name\n   * @param attr Attribute flags (See {@link FileStatus.AttrFlags}).\n   * @param locations a file's block locations",
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:<init>(java.lang.String)" : "* Construct the registry with a record name\n   * @param name  of the record of the metrics",
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementCounter(java.lang.String,long)" : null,
  "org.apache.hadoop.io.erasurecode.ECSchema:getNumDataUnits()" : "* Get required data units count in a coding group\n   * @return count of data units",
  "org.apache.hadoop.security.alias.UserProvider:getAliases()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],int[])" : "* Given a Vandermonde matrix V[i][j]=x[j]^i and vector y, solve for z such\n   * that Vz=y. The output z will be placed in y.\n   *\n   * @param x the vector which describe the Vandermonde matrix\n   * @param y right-hand side of the Vandermonde system equation. will be\n   *          replaced the output in this vector",
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<init>(int)" : "* Creates a new decompressor.\n   * @param bufferSize bufferSize.",
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_create()" : "* Create a new {@link IOStatisticsSnapshot} instance.\n   * @return an empty IOStatisticsSnapshot.",
  "org.apache.hadoop.fs.FileSystem$Cache$Key:isEqual(java.lang.Object,java.lang.Object)" : null,
  "org.apache.hadoop.ipc.Server$Call:abortResponse(java.lang.Throwable)" : null,
  "org.apache.hadoop.ipc.Server$FatalRpcServerException:<init>(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,java.io.IOException)" : null,
  "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:toLowerCase(javax.servlet.http.HttpServletRequest)" : null,
  "org.apache.hadoop.conf.ConfServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.io.Text:find(java.lang.String)" : null,
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionCounter(java.lang.String,java.util.function.ToLongFunction)" : "* Add a new evaluator to the counter statistics.\n   * @param key key of this statistic\n   * @param eval evaluator for the statistic\n   * @return the builder.",
  "org.apache.hadoop.fs.shell.Ls:getOrderComparator()" : "* Get the comparator to be used for sorting files.\n   * @return comparator",
  "org.apache.hadoop.io.wrappedio.WrappedIO:<init>()" : null,
  "org.apache.hadoop.util.functional.RemoteIterators:toList(org.apache.hadoop.fs.RemoteIterator)" : "* Build a list from a RemoteIterator.\n   * @param source source iterator\n   * @param <T> type\n   * @return a list of the values.\n   * @throws IOException if the source RemoteIterator raises it.",
  "org.apache.hadoop.fs.FilterFileSystem:getAclStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.UserGroupInformation:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod)" : "* Sets the authentication method in the subject\n   * \n   * @param authMethod authMethod.",
  "org.apache.hadoop.ipc.Client$ConnectionId:<init>(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ha.NodeFencer:createFenceMethod(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseDoneBlocks()" : "* Releases resources for any blocks marked as 'done'.",
  "org.apache.hadoop.fs.FileRange:createFileRange(long,int)" : "* Factory method to create a FileRange object.\n   * @param offset starting offset of the range.\n   * @param length length of the range.\n   * @return a new instance of FileRangeImpl.",
  "org.apache.hadoop.security.FastSaslClientFactory:createSaslClient(java.lang.String[],java.lang.String,java.lang.String,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write(byte[],int,int)" : null,
  "org.apache.hadoop.io.compress.DefaultCodec:createDecompressor()" : null,
  "org.apache.hadoop.conf.StorageUnit$4:toMBs(double)" : null,
  "org.apache.hadoop.fs.FileUtil:checkDependencies(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileContext:resolve(org.apache.hadoop.fs.Path)" : "* Resolves all symbolic links in the specified path.\n   * Returns the new path object.\n   *\n   * @param f the path.\n   * @throws FileNotFoundException If <code>f</code> does not exist.\n   * @throws UnresolvedLinkException If unresolved link occurred.\n   * @throws AccessControlException If access is denied.\n   * @throws IOException If an I/O error occurred.\n   * @return resolve path.",
  "org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)" : null,
  "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String)" : "* Gets unix groups and netgroups for the user.\n   *\n   * It gets all unix groups as returned by id -Gn but it\n   * only returns netgroups that are used in ACLs (there is\n   * no way to get all netgroups for a given user, see\n   * documentation for getent netgroup)",
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getThisBuilder()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:isPermissionLoaded()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues1(int,int)" : null,
  "org.apache.hadoop.util.Preconditions:checkArgument(boolean)" : "* Ensures the truth of an expression involving one or more parameters to the calling method.\n   *\n   * @param expression a boolean expression\n   * @throws IllegalArgumentException if {@code expression} is false",
  "org.apache.hadoop.fs.DelegateToFileSystem:getInitialWorkingDirectory()" : null,
  "org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:proxyName()" : null,
  "org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:shutdown(org.slf4j.Logger,long,java.util.concurrent.TimeUnit)" : "* Utility to shutdown the {@link ExecutorService} used by this class. Will wait up to a\n   * certain timeout for the ExecutorService to gracefully shutdown.\n   *\n   * @param logger Logger\n   * @param timeout the maximum time to wait\n   * @param unit the time unit of the timeout argument",
  "org.apache.hadoop.conf.StorageUnit$3:toEBs(double)" : null,
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:<init>()" : "* Construct with the copy function being simple passthrough.",
  "org.apache.hadoop.ha.ActiveStandbyElector:monitorLockNodeAsync()" : null,
  "org.apache.hadoop.io.WritableFactories:newInstance(java.lang.Class)" : "* Create a new instance of a class with a defined factory.\n   * @param c input c.\n   * @return a new instance of a class with a defined factory.",
  "org.apache.hadoop.io.compress.ZStandardCodec:getConf()" : "* Return the configuration used by this object.\n   *\n   * @return the configuration object used by this object.",
  "org.apache.hadoop.util.ComparableVersion:parseItem(boolean,java.lang.String)" : null,
  "org.apache.hadoop.security.User:toString()" : null,
  "org.apache.hadoop.conf.StorageUnit$6:toMBs(double)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:listStatusIterator(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.RefreshRegistry:handlerName(org.apache.hadoop.ipc.RefreshHandler)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)" : null,
  "org.apache.hadoop.util.FindClass:getResource(java.lang.String)" : "* Get the resource\n   * @param name resource name\n   * @return URL or null for not found",
  "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:prometheusName(java.lang.String,java.lang.String)" : "* Convert CamelCase based names to lower-case names where the separator\n   * is the underscore, to follow prometheus naming conventions.\n   *\n   * @param metricName metricName.\n   * @param recordName recordName.\n   * @return prometheusName.",
  "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:containsValue(java.lang.Object)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : null,
  "org.apache.hadoop.fs.impl.FsLinkResolution:<init>(org.apache.hadoop.fs.impl.FsLinkResolution$FsLinkResolutionFunction)" : "* Construct an instance with the given function.\n   * @param fn function to invoke.",
  "org.apache.hadoop.security.token.Token:identifierToString(java.lang.StringBuilder)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.security.authorize.RefreshAuthorizationPolicyProtocol)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:makeAbsolute(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:optLong(java.lang.String,long)" : null,
  "org.apache.hadoop.util.SysInfoWindows:getNetworkBytesWritten()" : "{@inheritDoc}",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)" : "* Reject the concat operation; forward the rest to the viewed FS.\n   * @param path path to query the capability of.\n   * @param capability string to query the stream support for.\n   * @return the capability\n   * @throws IOException if there is no resolved FS, or it raises an IOE.",
  "org.apache.hadoop.http.PrometheusServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.io.BooleanWritable:<init>(boolean)" : "* @param value value.",
  "org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,java.lang.Class)" : "* Construct &amp; cache an IPC client with the user-provided SocketFactory\n   * if no cached client exists.\n   * \n   * @param conf Configuration\n   * @param factory SocketFactory for client socket\n   * @param valueClass Class of the expected response\n   * @return an IPC client",
  "org.apache.hadoop.metrics2.impl.MetricCounterInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)" : null,
  "org.apache.hadoop.ipc.CallQueueManager:offer(java.lang.Object,long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invokeChecked(java.lang.Object,java.lang.Object[])" : null,
  "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])" : null,
  "org.apache.hadoop.util.ServletUtil:parseLongParam(javax.servlet.ServletRequest,java.lang.String)" : "* parseLongParam.\n   *\n   * @param request request.\n   * @param param param.\n   * @return a long value as passed in the given parameter, throwing\n   * an exception if it is not present or if it is not a valid number.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.ReflectionUtils:getDeclaredMethodsIncludingInherited(java.lang.Class)" : "* Gets all the declared methods of a class including methods declared in\n   * superclasses.\n   *\n   * @param clazz clazz.\n   * @return Method List.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)" : "* Sort and merge using an arbitrary {@link RawComparator}.\n     * @param fs input FileSystem.\n     * @param comparator input RawComparator.\n     * @param keyClass input keyClass.\n     * @param valClass input valClass.\n     * @param conf input Configuration.",
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:<init>()" : null,
  "org.apache.hadoop.fs.HarFileSystem:getHarHash(org.apache.hadoop.fs.Path)" : "* the hash of the path p inside  the filesystem\n   * @param p the path in the harfilesystem\n   * @return the hash code of the path.",
  "org.apache.hadoop.ipc.Server$ConnectionManager:closeIdle(boolean)" : null,
  "org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration)" : "* Set the configuration and extract the configuration parameters of interest\n     * @param conf the new configuration",
  "org.apache.hadoop.io.MapFile$Reader:midKey()" : "* Get the key at approximately the middle of the file. Or null if the\n     *  file is empty.\n     *\n     * @throws IOException raised on errors performing I/O.\n     * @return WritableComparable.",
  "org.apache.hadoop.conf.StorageUnit$1:toEBs(double)" : null,
  "org.apache.hadoop.fs.CompositeCrcFileChecksum:getAlgorithmName()" : null,
  "org.apache.hadoop.ha.HAAdmin:getTargetIds(java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.FastSaslClientFactory:getMechanismNames(java.util.Map)" : null,
  "org.apache.hadoop.security.CompositeGroupsMapping:loadMappingProviders()" : null,
  "org.apache.hadoop.io.erasurecode.ErasureCodeNative:getLoadingFailureReason()" : null,
  "org.apache.hadoop.fs.ContentSummary:getErasureCodingPolicy()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoot(org.apache.hadoop.fs.Path)" : "* Get the trash root directory for current user when the path\n   * specified is deleted.\n   *\n   * If FORCE_INSIDE_MOUNT_POINT flag is not set, return the default trash root\n   * from targetFS.\n   *\n   * When FORCE_INSIDE_MOUNT_POINT is set to true,\n   * <ol>\n   *   <li>\n   *     If the trash root for path p is in the same mount point as path p,\n   *       and one of:\n   *       <ol>\n   *         <li>The mount point isn't at the top of the target fs.</li>\n   *         <li>The resolved path of path is root (in fallback FS).</li>\n   *         <li>The trash isn't in user's target fs home directory\n   *            get the corresponding viewFS path for the trash root and return\n   *            it.\n   *         </li>\n   *       </ol>\n   *   </li>\n   *   <li>\n   *     else, return the trash root under the root of the mount point\n   *     (/{mntpoint}/.Trash/{user}).\n   *   </li>\n   * </ol>\n   *\n   * These conditions handle several different important cases:\n   * <ul>\n   *   <li>File systems may need to have more local trash roots, such as\n   *         encryption zones or snapshot roots.</li>\n   *   <li>The fallback mount should use the user's home directory.</li>\n   *   <li>Cloud storage systems should not use trash in an implicity defined\n   *        home directory, per a container, unless it is the fallback fs.</li>\n   * </ul>\n   *\n   * @param path the trash root of the path to be determined.\n   * @return the trash root path.",
  "org.apache.hadoop.fs.shell.CommandFormat:addOptionWithValue(java.lang.String)" : "* add option with value\n   *\n   * @param option option name",
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:decompress(byte[],int,int)" : null,
  "org.apache.hadoop.io.SequenceFile$Reader$LengthOption:<init>(long)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.RpcWritable$Buffer:getValue(java.lang.Object)" : null,
  "org.apache.hadoop.conf.Configuration$Resource:isParserRestricted()" : null,
  "org.apache.hadoop.fs.ContentSummary:toErasureCodingPolicy()" : "* @return Constant-width String representation of Erasure Coding Policy",
  "org.apache.hadoop.fs.viewfs.RegexMountPoint:resolve(java.lang.String,boolean)" : "* Get resolved path from regex mount points.\n   *  E.g. link: ^/user/(?<username>\\\\w+) => s3://$user.apache.com/_${user}\n   *  srcPath: is /user/hadoop/dir1\n   *  resolveLastComponent: true\n   *  then return value is s3://hadoop.apache.com/_hadoop\n   * @param srcPath - the src path to resolve\n   * @param resolveLastComponent - whether resolve the path after last `/`\n   * @return mapped path of the mount point.",
  "org.apache.hadoop.util.SysInfoLinux:getStorageBytesRead()" : null,
  "org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand:validate()" : null,
  "org.apache.hadoop.io.IntWritable:get()" : "* Return the value of this IntWritable.\n   * @return value of this IntWritable.",
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<init>()" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:asyncRequestsWaiting()" : null,
  "org.apache.hadoop.conf.StorageUnit$2:toMBs(double)" : null,
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:blockNumber()" : "* Gets the id of the current block.\n   *\n   * @return the id of the current block.",
  "org.apache.hadoop.metrics2.source.JvmMetrics$Singleton:shutdown()" : null,
  "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)" : "* Add a LinkFallback to the config for the specified mount table.\n   *\n   * @param conf configuration.\n   * @param mountTableName mountTable.\n   * @param target targets.",
  "org.apache.hadoop.fs.DelegateToFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)" : "* Open a file by delegating to\n   * {@link FileSystem#openFileWithOptions(Path, org.apache.hadoop.fs.impl.OpenFileParameters)}.\n   * @param path path to the file\n   * @param parameters open file parameters from the builder.\n   *\n   * @return a future which will evaluate to the opened file.ControlAlpha\n   * @throws IOException failure to resolve the link.\n   * @throws IllegalArgumentException unknown mandatory key",
  "org.apache.hadoop.security.KerberosAuthException:setTicketCacheFile(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:setIncludePattern(com.google.re2j.Pattern)" : null,
  "org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:close()" : null,
  "org.apache.hadoop.fs.FileSystem:clearStatistics()" : "* Reset all statistics for all file systems.",
  "org.apache.hadoop.conf.Configuration:handleDeprecation(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String)" : "* Checks for the presence of the property <code>name</code> in the\n   * deprecation map. Returns the first of the list of new keys if present\n   * in the deprecation map or the <code>name</code> itself. If the property\n   * is not presently set but the property map contains an entry for the\n   * deprecated key, the value of the deprecated key is set as the value for\n   * the provided property name.\n   *\n   * Also updates properties and overlays with deprecated keys, if the new\n   * key does not already exist.\n   *\n   * @param deprecations deprecation context\n   * @param name the property name\n   * @return the first property in the list of properties mapping\n   *         the <code>name</code> or the <code>name</code> itself.",
  "org.apache.hadoop.util.Preconditions:checkArgument(boolean,java.util.function.Supplier)" : "* Preconditions that the expression involving one or more parameters to the calling method.\n   *\n   * <p>The message of the exception is {@code msgSupplier.get()}.</p>\n   *\n   * @param expression a boolean expression\n   * @param msgSupplier  the {@link Supplier#get()} set the\n   *                 exception message if valid. Otherwise,\n   *                 the message is {@link #CHECK_ARGUMENT_EX_MESSAGE}\n   * @throws IllegalArgumentException if {@code expression} is false",
  "org.apache.hadoop.fs.sftp.SFTPConnectionPool:shutdown()" : "Shutdown the connection pool and close all open connections.",
  "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:flush()" : null,
  "org.apache.hadoop.ipc.Server$RpcCall:doResponse(java.lang.Throwable,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:getReason()" : null,
  "org.apache.hadoop.fs.FileSystem:loadFileSystems()" : "* Load the filesystem declarations from service resources.\n   * This is a synchronized operation.",
  "org.apache.hadoop.conf.StorageUnit$5:toEBs(double)" : null,
  "org.apache.hadoop.fs.Path:getOptionalParentPath()" : "* Returns the parent of a path as {@link Optional} or\n   * {@link Optional#empty()} i.e an empty Optional if at root.\n   *\n   * @return Parent of path wrappen in {@link Optional}.\n   * {@link Optional#empty()} i.e an empty Optional if at root.",
  "org.apache.hadoop.ipc.Client$ConnectionId:equals(java.lang.Object)" : null,
  "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:kill(org.apache.hadoop.util.Daemon)" : null,
  "org.apache.hadoop.security.alias.CredentialShell$CreateCommand:execute()" : null,
  "org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:build()" : null,
  "org.apache.hadoop.io.file.tfile.TFile:getFSOutputBufferSize(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getAccessTime()" : null,
  "org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingMean()" : null,
  "org.apache.hadoop.ipc.Server$RpcCall:toString()" : null,
  "org.apache.hadoop.util.GenericOptionsParser:getLibJars(org.apache.hadoop.conf.Configuration)" : "* If libjars are set in the conf, parse the libjars.\n   * @param conf input Configuration.\n   * @return libjar urls\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:<init>(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Kind,int)" : null,
  "org.apache.hadoop.service.launcher.IrqHandler$InterruptData:toString()" : null,
  "org.apache.hadoop.fs.InvalidPathException:<init>(java.lang.String)" : "* Constructs exception with the specified detail message.\n   * \n   * @param path invalid path.",
  "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$6:unit()" : null,
  "org.apache.hadoop.io.serializer.JavaSerialization:getDeserializer(java.lang.Class)" : null,
  "org.apache.hadoop.util.ApplicationClassLoader:loadClass(java.lang.String)" : null,
  "org.apache.hadoop.security.alias.LocalKeyStoreProvider:stashOriginalFilePermissions()" : null,
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:createCache(int,org.apache.hadoop.fs.statistics.DurationTrackerFactory)" : null,
  "org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.lang.String)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:gauges()" : null,
  "org.apache.hadoop.ipc.ProtobufWrapperLegacy:writeTo(org.apache.hadoop.ipc.ResponseBuffer)" : null,
  "org.apache.hadoop.ipc.CallQueueManager$CallQueueOverflowException:<init>(java.io.IOException,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)" : null,
  "org.apache.hadoop.conf.StorageSize:getUnit()" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Compressor:finished()" : "* Returns true if the end of the compressed\n   * data output stream has been reached.\n   *\n   * @return <code>true</code> if the end of the compressed\n   *         data output stream has been reached.",
  "org.apache.hadoop.conf.Configuration:getPasswordFromConfig(java.lang.String)" : "* Fallback to clear text passwords in configuration.\n   * @param name the property name.\n   * @return clear text password or null",
  "org.apache.hadoop.io.compress.CompressionCodecFactory:addCodec(org.apache.hadoop.io.compress.CompressionCodec)" : null,
  "org.apache.hadoop.io.UTF8:hashCode()" : null,
  "org.apache.hadoop.security.Groups$TimerToTickerAdapter:read()" : null,
  "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)" : "* Construct the preferred type of SequenceFile Writer.\n   * @param fs The configured filesystem.\n   * @param conf The configuration.\n   * @param name The name of the file.\n   * @param keyClass The 'key' type.\n   * @param valClass The 'value' type.\n   * @param bufferSize buffer size for the underlaying outputstream.\n   * @param replication replication factor for the file.\n   * @param blockSize block size for the file.\n   * @param compressionType The compression type.\n   * @param codec The compression codec.\n   * @param progress The Progressable object to track progress.\n   * @param metadata The metadata of the file.\n   * @return Returns the handle to the constructed SequenceFile Writer.\n   * @throws IOException raised on errors performing I/O.\n   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}\n   *     instead.",
  "org.apache.hadoop.fs.LocalDirAllocator:obtainContext(java.lang.String)" : "This method must be used to obtain the dir allocation context for a \n   * particular value of the context name. The context name must be an item\n   * defined in the Configuration object for which we want to control the \n   * dir allocations (e.g., <code>mapred.local.dir</code>). The method will\n   * create a context for that name if it doesn't already exist.",
  "org.apache.hadoop.crypto.key.KeyProvider$Options:setAttributes(java.util.Map)" : null,
  "org.apache.hadoop.io.WritableFactories:setFactory(java.lang.Class,org.apache.hadoop.io.WritableFactory)" : "* Define a factory for a class.\n   * @param c input c.\n   * @param factory input factory.",
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:finalize()" : "* Overriden to close the stream.",
  "org.apache.hadoop.fs.AbstractFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : "* Removes ACL entries from files and directories.  Other ACL entries are\n   * retained.\n   *\n   * @param path Path to modify\n   * @param aclSpec List{@literal <AclEntry>} describing entries to remove\n   * @throws IOException if an ACL could not be modified",
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:setContext(java.lang.String)" : "* Set the metrics context tag\n   * @param name of the context\n   * @return the registry itself as a convenience",
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:getSlope()" : "* @return the slope of a visited metric. Slope is positive for counters and\n   *         null for others",
  "org.apache.hadoop.util.IntrusiveCollection:iterator()" : "* Get an iterator over the list.  This can be used to remove elements.\n   * It is not safe to do concurrent modifications from other threads while\n   * using this iterator.\n   * \n   * @return         The iterator.",
  "org.apache.hadoop.security.CompositeGroupsMapping:prepareConf(java.lang.String)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.http.HttpServer2:getWebAppsPath(java.lang.String)" : "* Get the pathname to the webapps files.\n   * @param appName eg \"secondary\" or \"datanode\"\n   * @return the pathname as a URL\n   * @throws FileNotFoundException if 'webapps' directory cannot be found\n   *   on CLASSPATH or in the development location.",
  "org.apache.hadoop.fs.FileContext:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)" : "* Set the storage policy for a given file or directory.\n   *\n   * @param path file or directory path.\n   * @param policyName the name of the target storage policy. The list\n   *                   of supported Storage policies can be retrieved\n   *                   via {@link #getAllStoragePolicies}.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.fs.Options$Rename:valueOf(byte)" : null,
  "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:refreshServiceAcl()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:getWorkingDirectory()" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:<init>()" : null,
  "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:getLongStatistics()" : "* Take a snapshot of the current counter values\n   * and return an iterator over them.\n   * @return all the counter statistics.",
  "org.apache.hadoop.util.bloom.CountingBloomFilter:delete(org.apache.hadoop.util.bloom.Key)" : "* Removes a specified key from <i>this</i> counting Bloom filter.\n   * <p>\n   * <b>Invariant</b>: nothing happens if the specified key does not belong to <i>this</i> counter Bloom filter.\n   * @param key The key to remove.",
  "org.apache.hadoop.fs.ContentSummary:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.FSInputChecker:read1(byte[],int,int)" : null,
  "org.apache.hadoop.util.FindClass:printStack(java.lang.Throwable,java.lang.String,java.lang.Object[])" : "* print a stack trace with text\n   * @param e the exception to print\n   * @param text text to print",
  "org.apache.hadoop.ha.ServiceFailedException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.security.authorize.AccessControlList:isWildCardACLValue(java.lang.String)" : "* Checks whether ACL string contains wildcard\n   *\n   * @param aclString check this ACL string for wildcard\n   * @return true if ACL string contains wildcard false otherwise",
  "org.apache.hadoop.ipc.RetryCache$CacheEntry:toString()" : null,
  "org.apache.hadoop.conf.Configuration:iterator()" : "* Get an {@link Iterator} to go through the list of <code>String</code> \n   * key-value pairs in the configuration.\n   * \n   * @return an iterator over the entries.",
  "org.apache.hadoop.io.compress.SnappyCodec:createCompressor()" : "* Create a new {@link Compressor} for use by this {@link CompressionCodec}.\n   *\n   * @return a new compressor for use by this codec",
  "org.apache.hadoop.io.nativeio.NativeIOException:<init>(java.lang.String,org.apache.hadoop.io.nativeio.Errno)" : null,
  "org.apache.hadoop.conf.ReconfigurationServlet:printHeader(java.io.PrintWriter,java.lang.String)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.conf.Configuration:get(java.lang.String)" : "* Get the value of the <code>name</code> property, <code>null</code> if\n   * no such property exists. If the key is deprecated, it returns the value of\n   * the first key which replaces the deprecated key and is not null.\n   * \n   * Values are processed for <a href=\"#VariableExpansion\">variable expansion</a> \n   * before being returned.\n   *\n   * As a side effect get loads the properties from the sources if called for\n   * the first time as a lazy init.\n   * \n   * @param name the property name, will be trimmed before get value.\n   * @return the value of the <code>name</code> or its replacing property, \n   *         or null if no such property exists.",
  "org.apache.hadoop.fs.ContentSummary:<init>(org.apache.hadoop.fs.ContentSummary$Builder)" : "* Constructor for ContentSummary.Builder.\n   *\n   * @param builder builder.",
  "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getInstance(java.lang.String,int,org.apache.hadoop.ipc.DecayRpcScheduler)" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler:close()" : null,
  "org.apache.hadoop.fs.CreateFlag:validate(java.lang.Object,boolean,java.util.EnumSet)" : "* Validate the CreateFlag for create operation\n   * @param path Object representing the path; usually String or {@link Path}\n   * @param pathExists pass true if the path exists in the file system\n   * @param flag set of CreateFlag\n   * @throws IOException on error\n   * @throws HadoopIllegalArgumentException if the CreateFlag is invalid",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* This provider expects URIs in the following form :\n     * {@literal kms://<PROTO>@<AUTHORITY>/<PATH>}\n     *\n     * where :\n     * - PROTO = http or https\n     * - AUTHORITY = {@literal <HOSTS>[:<PORT>]}\n     * - HOSTS = {@literal <HOSTNAME>[;<HOSTS>]}\n     * - HOSTNAME = string\n     * - PORT = integer\n     *\n     * This will always create a {@link LoadBalancingKMSClientProvider}\n     * if the uri is correct.",
  "org.apache.hadoop.ipc.Server$Call:setPriorityLevel(int)" : null,
  "org.apache.hadoop.fs.shell.find.FilterExpression:addArguments(java.util.Deque)" : null,
  "org.apache.hadoop.io.nativeio.NativeIOException:toString()" : null,
  "org.apache.hadoop.util.Timer:monotonicNowNanos()" : "* Same as {@link #monotonicNow()} but returns its result in nanoseconds.\n   * Note that this is subject to the same resolution constraints as\n   * {@link System#nanoTime()}.\n   * @return a monotonic clock that counts in nanoseconds.",
  "org.apache.hadoop.fs.FileSystem:createFile(org.apache.hadoop.fs.Path)" : "* Create a new FSDataOutputStreamBuilder for the file with path.\n   * Files are overwritten by default.\n   *\n   * @param path file path\n   * @return a FSDataOutputStreamBuilder object to build the file\n   *\n   * HADOOP-14384. Temporarily reduce the visibility of method before the\n   * builder interface becomes stable.",
  "org.apache.hadoop.fs.FilterFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)" : null,
  "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:addCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall)" : null,
  "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:open(java.io.InputStream)" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadFromPath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Try loading from the user specified path, else load from the backup\n   * path in case Exception is not due to bad/wrong password.\n   * @param path Actual path to load from\n   * @param backupPath Backup path (_OLD)\n   * @return The permissions of the loaded file\n   * @throws NoSuchAlgorithmException\n   * @throws CertificateException\n   * @throws IOException",
  "org.apache.hadoop.security.ShellBasedIdMapping:getId2NameCmdNIX(int,boolean)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setSequenceNumber(int)" : null,
  "org.apache.hadoop.fs.UnionStorageStatistics:<init>(java.lang.String,org.apache.hadoop.fs.StorageStatistics[])" : null,
  "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:isLoginSuccess()" : "Get the login status.",
  "org.apache.hadoop.http.HtmlQuoting:needsQuoting(byte[],int,int)" : "* Does the given string need to be quoted?\n   * @param data the string to check\n   * @param off the starting position\n   * @param len the number of bytes to check\n   * @return does the string contain any of the active html characters?",
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToHeadOfLinkedList(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)" : "* Add the given entry to the head of the linked list.\n   *\n   * @param entry Block entry to add.",
  "org.apache.hadoop.io.SequenceFile$Sorter:sort(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)" : "* Perform a file sort from a set of input files into an output file.\n     * @param inFiles the files to be sorted\n     * @param outFile the sorted output file\n     * @param deleteInput should the input files be deleted as they are read?\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:initializeAuthHandler(java.lang.String,javax.servlet.FilterConfig)" : null,
  "org.apache.hadoop.fs.FileSystem:getPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])" : "* Create a durable, serializable handle to the referent of the given\n   * entity.\n   * @param stat Referent in the target FileSystem\n   * @param opt If absent, assume {@link HandleOpt#path()}.\n   * @throws IllegalArgumentException If the FileStatus does not belong to\n   *         this FileSystem\n   * @throws UnsupportedOperationException If {@link #createPathHandle}\n   *         not overridden by subclass.\n   * @throws UnsupportedOperationException If this FileSystem cannot enforce\n   *         the specified constraints.\n   * @return path handle.",
  "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean,org.apache.zookeeper.client.ZKClientConfig)" : null,
  "org.apache.hadoop.security.KerberosAuthException:getMessage()" : null,
  "org.apache.hadoop.util.Progress:complete()" : "Completes this node, moving the parent node to its next child.",
  "org.apache.hadoop.fs.DelegationTokenRenewer:reset()" : null,
  "org.apache.hadoop.conf.StorageUnit$3:getDefault(double)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:removeAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.HarFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.CrcComposer:update(byte[],int,int,long)" : "* Composes length / CRC_SIZE_IN_BYTES more CRCs from crcBuffer, with\n   * each CRC expected to correspond to exactly {@code bytesPerCrc} underlying\n   * data bytes.\n   *\n   * @param crcBuffer crcBuffer.\n   * @param offset offset.\n   * @param length must be a multiple of the expected byte-size of a CRC.\n   * @param bytesPerCrc bytesPerCrc.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:toString()" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:getKeyClassName()" : "@return Returns the name of the key class.",
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:deleteBlockFileAndEvictCache(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)" : "* Delete cache file as part of the block cache LRU eviction.\n   *\n   * @param elementToPurge Block entry to evict.",
  "org.apache.hadoop.io.compress.CompressionCodec$Util:createInputStreamWithCodecPool(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration,java.io.InputStream)" : "* Create an input stream with a codec taken from the global CodecPool.\n     *\n     * @param codec       The codec to use to create the input stream.\n     * @param conf        The configuration to use if we need to create a new codec.\n     * @param in          The input stream to wrap.\n     * @return            The new input stream\n     * @throws IOException",
  "org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:getFileLength()" : null,
  "org.apache.hadoop.metrics2.lib.MutableCounterInt:incr()" : null,
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:convert(org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAServiceStateProto)" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:startupShutdownMessage(java.lang.String,java.util.List)" : "* @return Build a log message for starting up and shutting down.\n   * @param classname the class of the server\n   * @param args arguments",
  "org.apache.hadoop.ipc.ClientId:toString(byte[])" : "* @return Convert a clientId byte[] to string.\n   * @param clientId input clientId.",
  "org.apache.hadoop.security.token.DtUtilShell$Print:getUsage()" : null,
  "org.apache.hadoop.fs.QuotaUsage:toString(boolean,boolean,java.util.List)" : "* Return the string representation of the object in the output format.\n   * if hOption is false file sizes are returned in bytes\n   * if hOption is true file sizes are returned in human readable\n   *\n   * @param hOption a flag indicating if human readable output if to be used\n   * @param tOption type option.\n   * @param types storage types.\n   * @return the string representation of the object.",
  "org.apache.hadoop.util.functional.FunctionalIO:extractIOExceptions(java.util.function.Supplier)" : "* Invoke the supplier, catching any {@code UncheckedIOException} raised,\n   * extracting the inner IOException and rethrowing it.\n   * @param call call to invoke\n   * @param <T> type of result\n   * @return result\n   * @throws IOException if the call raised an IOException wrapped by an UncheckedIOException.",
  "org.apache.hadoop.fs.permission.FsCreateModes:getUnmasked()" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToLinkedListAndEvictIfRequired(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)" : "* Add the given entry to the head of the linked list and if the LRU cache size\n   * exceeds the max limit, evict tail of the LRU linked list.\n   *\n   * @param entry Block entry to add.",
  "org.apache.hadoop.fs.FsStatus:getCapacity()" : "* Return the capacity in bytes of the file system.\n   * @return capacity.",
  "org.apache.hadoop.metrics2.lib.DefaultMetricsFactory:getAnnotatedMetricsFactory()" : null,
  "org.apache.hadoop.util.Shell:checkHadoopHome()" : "*  Centralized logic to discover and validate the sanity of the Hadoop\n   *  home directory.\n   *\n   *  This does a lot of work so it should only be called\n   *  privately for initialization once per process.\n   *\n   * @return A directory that exists and via was specified on the command line\n   * via <code>-Dhadoop.home.dir</code> or the <code>HADOOP_HOME</code>\n   * environment variable.\n   * @throws FileNotFoundException if the properties are absent or the specified\n   * path is not a reference to a valid directory.",
  "org.apache.hadoop.fs.FileContext:getAclStatus(org.apache.hadoop.fs.Path)" : "* Gets the ACLs of files and directories.\n   *\n   * @param path Path to get\n   * @return RemoteIterator{@literal <}AclStatus{@literal >} which returns\n   *         each AclStatus\n   * @throws IOException if an ACL could not be read",
  "org.apache.hadoop.ipc.RPC:getServerAddress(java.lang.Object)" : "* @return Returns the server address for a given proxy.\n   * @param proxy input proxy.",
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getInnerStatistics()" : null,
  "org.apache.hadoop.fs.shell.FsUsage$Du:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.metrics2.util.Metrics2Util$TopN:<init>(int)" : null,
  "org.apache.hadoop.metrics2.lib.MutableStat:add(long)" : "* Add a snapshot to the metric.\n   * @param value of the metric",
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:available()" : null,
  "org.apache.hadoop.security.Credentials:writeProtobufOutputStream(java.io.DataOutputStream)" : null,
  "org.apache.hadoop.conf.Configuration:<init>(boolean)" : "A new configuration where the behavior of reading from the default \n   * resources can be turned off.\n   * \n   * If the parameter {@code loadDefaults} is false, the new instance\n   * will not load resources from the default files. \n   * @param loadDefaults specifies whether to load from the default files",
  "org.apache.hadoop.security.LdapGroupsMapping:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)" : "* Passwords should not be stored in configuration. Use\n   * {@link #getPasswordFromCredentialProviders(\n   *            Configuration, String, String)}\n   * to avoid reading passwords from a configuration file.",
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.metrics2.lib.MethodMetric:<init>(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.MetricsInfo,org.apache.hadoop.metrics2.annotation.Metric$Type)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsBuffer:iterator()" : null,
  "org.apache.hadoop.io.ElasticByteBufferPool$Key:hashCode()" : null,
  "org.apache.hadoop.fs.FileSystem:closeAll()" : "* Close all cached FileSystem instances. After this operation, they\n   * may not be used in any operations.\n   *\n   * @throws IOException a problem arose closing one or more filesystem.",
  "org.apache.hadoop.net.unix.DomainSocket:sendFileDescriptors(java.io.FileDescriptor[],byte[],int,int)" : "* Send some FileDescriptor objects to the process on the other side of this\n   * socket.\n   * \n   * @param descriptors       The file descriptors to send.\n   * @param jbuf              Some bytes to send.  You must send at least\n   *                          one byte.\n   * @param offset            The offset in the jbuf array to start at.\n   * @param length            Length of the jbuf array to use.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.Options$HandleOpt$Data:toString()" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatisticsSource(java.lang.Object)" : "* Probe for an object being an instance of {@code IOStatisticsSource}.\n   * @param object object to probe\n   * @return true if the object is the right type, false if the classes\n   * were not found or the object is null/of a different type",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackUpdateToken(org.apache.hadoop.util.functional.InvocationRaisingIOE)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:readUByte(byte[],int)" : null,
  "org.apache.hadoop.io.EnumSetWritable:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.UserGroupInformation$KeytabRenewalRunnable:relogin()" : null,
  "org.apache.hadoop.metrics2.MetricsJsonBuilder:tuple(java.lang.String,java.lang.Object)" : null,
  "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:set(java.lang.String,java.lang.String)" : "* Set an attribute. If the value is non-null/empty,\n   * it will be used as a query parameter.\n   *\n   * @param key key to set\n   * @param value value.",
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:getWorkingDirectory(com.jcraft.jsch.ChannelSftp)" : "* Convenience method, so that we don't open a new connection when using this\n   * method from within another method. Otherwise every API invocation incurs\n   * the overhead of opening/closing a TCP connection.",
  "org.apache.hadoop.io.SequenceFile$Reader:seekToCurrentValue()" : "* Position valLenIn/valIn to the 'value' \n     * corresponding to the 'current' key",
  "org.apache.hadoop.fs.FileSystem:close()" : "* Close this FileSystem instance.\n   * Will release any held locks, delete all files queued for deletion\n   * through calls to {@link #deleteOnExit(Path)}, and remove this FS instance\n   * from the cache, if cached.\n   *\n   * After this operation, the outcome of any method call on this FileSystem\n   * instance, or any input/output stream created by it is <i>undefined</i>.\n   * @throws IOException IO failure",
  "org.apache.hadoop.ha.ZKFailoverController:initRPC()" : null,
  "org.apache.hadoop.fs.ContentSummary:getSnapshotLength()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:pairedTrackerFactory(org.apache.hadoop.fs.statistics.DurationTrackerFactory,org.apache.hadoop.fs.statistics.DurationTrackerFactory)" : "* Create a DurationTrackerFactory which aggregates the tracking\n   * of two other factories.\n   * @param first first tracker factory\n   * @param second second tracker factory\n   * @return a factory",
  "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxySuperuserUserConfKey(java.lang.String)" : "* Returns configuration key for effective usergroups allowed for a superuser\n   * \n   * @param userName name of the superuser\n   * @return configuration key for superuser usergroups",
  "org.apache.hadoop.ipc.DecayRpcScheduler:decayCurrentCosts()" : "* Decay the stored costs for each user and clean as necessary.\n   * This method should be called periodically in order to keep\n   * costs current.",
  "org.apache.hadoop.http.HttpServer2:getFilterHolder(java.lang.String,java.lang.String,java.util.Map)" : null,
  "org.apache.hadoop.fs.FileUtil:symLink(java.lang.String,java.lang.String)" : "* Create a soft link between a src and destination\n   * only on a local disk. HDFS does not support this.\n   * On Windows, when symlink creation fails due to security\n   * setting, we will log a warning. The return code in this\n   * case is 2.\n   *\n   * @param target the target for symlink\n   * @param linkname the symlink\n   * @return 0 on success\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)" : "* Construct a map reader for the named map using the named comparator.\n     * @deprecated\n     *\n     * @param fs FileSystem.\n     * @param dirName dirName.\n     * @param comparator WritableComparator.\n     * @param conf Configuration.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getKerberosEntry()" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Reader:close()" : "* Finishing reading the BCFile. Release all resources.",
  "org.apache.hadoop.fs.AbstractFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)" : "* Set the storage policy for a given file or directory.\n   *\n   * @param path file or directory path.\n   * @param policyName the name of the target storage policy. The list\n   *                   of supported Storage policies can be retrieved\n   *                   via {@link #getAllStoragePolicies}.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.statistics.MeanStatistic:isEmpty()" : "* Is a statistic empty?\n   * @return true if the sample count is 0",
  "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:cloneBufferData(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FsUrlConnection:connect()" : null,
  "org.apache.hadoop.tools.CommandShell:printShellUsage()" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:toString()" : null,
  "org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.slf4j.Logger,org.apache.hadoop.security.UserGroupInformation)" : "* Log all (current, real, login) UGI and token info into specified log.\n   * @param ugi - UGI\n   * @param log - log.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.tracing.Span:addKVAnnotation(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:innerClose()" : null,
  "org.apache.hadoop.io.Text:getLength()" : "* Returns the number of bytes in the byte array.",
  "org.apache.hadoop.net.InnerNodeImpl:getLeaf(int,org.apache.hadoop.net.Node)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:hashCode()" : null,
  "org.apache.hadoop.security.Credentials:readTokenStorageStream(java.io.DataInputStream)" : "* Convenience method for reading a token from a DataInputStream.\n   *\n   * @param in DataInputStream.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.util.SampleStat:total()" : "* @return the total of all samples added",
  "org.apache.hadoop.io.serializer.DeserializerComparator:compare(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:getConnectionId()" : null,
  "org.apache.hadoop.fs.BBUploadHandle:bytes()" : null,
  "org.apache.hadoop.fs.http.HttpsFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)" : "* Declare that this filesystem connector is always read only.\n   * {@inheritDoc}",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:isAvailable()" : "* Is the wrapped IO class loaded?\n   * @return true if the instance is loaded.",
  "org.apache.hadoop.util.Shell:getGroupsIDForUserCommand(java.lang.String)" : "* A command to get a given user's group id list.\n   * The command will get the user's primary group\n   * first and finally get the groups list which includes the primary group.\n   * i.e. the user's primary group will be included twice.\n   * This command does not support Windows and will only return group names.\n   *\n   * @param user user.\n   * @return groups id for user command.",
  "org.apache.hadoop.ipc.DecayRpcScheduler$DecayTask:<init>(org.apache.hadoop.ipc.DecayRpcScheduler,java.util.Timer)" : null,
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getProgress()" : null,
  "org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)" : null,
  "org.apache.hadoop.security.UserGroupInformation:isLoginTicketBased()" : "* Did the login happen via ticket cache.\n   * @return true or false\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.CallerContext$Builder:append(java.lang.String)" : "* Append new field to the context.\n     * @param field one of fields to append.\n     * @return the builder.",
  "org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer:close()" : null,
  "org.apache.hadoop.ha.HAServiceStatus:getNotReadyReason()" : null,
  "org.apache.hadoop.ipc.Client$IpcStreams:close()" : null,
  "org.apache.hadoop.util.DataChecksum:newCrc32C()" : "* The flag is volatile to avoid synchronization here.\n   * Re-entrancy is unlikely except in failure mode (and inexpensive).",
  "org.apache.hadoop.io.serializer.JavaSerializationComparator:<init>()" : null,
  "org.apache.hadoop.security.alias.JavaKeyStoreProvider:getKeyStoreType()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsConfig:<init>(org.apache.commons.configuration2.Configuration,java.lang.String)" : null,
  "org.apache.hadoop.util.DiskChecker:mkdirsWithExistsAndPermissionCheck(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : "* Create the directory or check permissions if it already exists.\n   *\n   * The semantics of mkdirsWithExistsAndPermissionCheck method is different\n   * from the mkdirs method provided in the Sun's java.io.File class in the\n   * following way:\n   * While creating the non-existent parent directories, this method checks for\n   * the existence of those directories if the mkdir fails at any point (since\n   * that directory might have just been created by some other process).\n   * If both mkdir() and the exists() check fails for any seemingly\n   * non-existent directory, then we signal an error; Sun's mkdir would signal\n   * an error (return false) if a directory it is attempting to create already\n   * exists or the mkdir fails.\n   *\n   * @param localFS local filesystem\n   * @param dir directory to be created or checked\n   * @param expected expected permission\n   * @throws IOException",
  "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:gauges()" : null,
  "org.apache.hadoop.fs.PathIOException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)" : "* Use a subclass of PathIOException if possible.\n   * @param path for the exception\n   * @param error custom string to use an the error text\n   * @param cause cause of exception.",
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:flush()" : null,
  "org.apache.hadoop.fs.FileSystem:getAclStatus(org.apache.hadoop.fs.Path)" : "* Gets the ACL of a file or directory.\n   *\n   * @param path Path to get\n   * @return AclStatus describing the ACL of the file or directory\n   * @throws IOException if an ACL could not be read\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.ipc.Client$ConnectionId:getRetryPolicy()" : null,
  "org.apache.hadoop.io.NullWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.metrics2.impl.MsInfo:description()" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String,org.apache.hadoop.io.file.tfile.Compression$Algorithm)" : null,
  "org.apache.hadoop.fs.viewfs.RegexMountPoint:getPathToResolve(java.lang.String,boolean)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setCounter(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:bulkDelete(java.util.Collection)" : "* {@inheritDoc}.\n     * The default impl just calls {@code FileSystem.delete(path, false)}\n     * on the single path in the list.",
  "org.apache.hadoop.util.bloom.DynamicBloomFilter:addRow()" : "* Adds a new row to <i>this</i> dynamic Bloom filter.",
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.ObjectWritable:getDeclaredClass()" : "* Return the class this is meant to be.\n   * @return the class this is meant to be.",
  "org.apache.hadoop.util.DataChecksum:calculateChunkedSums(java.nio.ByteBuffer,java.nio.ByteBuffer)" : "* Calculate checksums for the given data.\n   * \n   * The 'mark' of the ByteBuffer parameters may be modified by this function,\n   * but the position is maintained.\n   * \n   * @param data the DirectByteBuffer pointing to the data to checksum.\n   * @param checksums the DirectByteBuffer into which checksums will be\n   *                  stored. Enough space must be available in this\n   *                  buffer to put the checksums.",
  "org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,long)" : "* Set optional long parameter for the Builder.\n   *\n   * @param key key.\n   * @param value value.\n   * @return generic type B.\n   * @deprecated use  {@link #optLong(String, long)} where possible.",
  "org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:close()" : "* Set the finished time and then update the statistics.\n   * If the operation failed then the key + .failures counter will be\n   * incremented by one.\n   * The operation min/mean/max values will be updated with the duration;\n   * on a failure these will all be the .failures metrics.",
  "org.apache.hadoop.io.file.tfile.Utils:writeString(java.io.DataOutput,java.lang.String)" : "* Write a String as a VInt n, followed by n Bytes as in Text format.\n   * \n   * @param out out.\n   * @param s s.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:close()" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:close()" : null,
  "org.apache.hadoop.conf.Configuration:setIfUnset(java.lang.String,java.lang.String)" : "* Sets a property if it is currently unset.\n   * @param name the property name\n   * @param value the new value",
  "org.apache.hadoop.util.DiskValidatorFactory:getInstance(java.lang.String)" : "* Returns {@link DiskValidator} instance corresponding to its name.\n   * The diskValidator parameter can be \"basic\" for {@link BasicDiskValidator}\n   * or \"read-write\" for {@link ReadWriteDiskValidator}.\n   * @param diskValidator canonical class name, for example, \"basic\"\n   * @throws DiskErrorException if the class cannot be located\n   * @return disk validator.",
  "org.apache.hadoop.ipc.DecayRpcScheduler:getTotalServiceUserCallVolume()" : null,
  "org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:createDecoder()" : null,
  "org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(byte[])" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,boolean)" : "* Convenience method, so that we don't open a new connection when using this\n   * method from within another method. Otherwise every API invocation incurs\n   * the overhead of opening/closing a TCP connection.",
  "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:metrics()" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReference:cleanUp()" : "* Performs clean-up action when the associated thread is garbage\n       * collected.",
  "org.apache.hadoop.fs.Options$CreateOpts$ChecksumParam:<init>(org.apache.hadoop.fs.Options$ChecksumOpt)" : null,
  "org.apache.hadoop.fs.Path:getName()" : "* Returns the final component of this path.\n   *\n   * @return the final component of this path",
  "org.apache.hadoop.ipc.Server$Call:doResponse(java.lang.Throwable)" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : "* Convenience method, so that we don't open a new connection when using this\n   * method from within another method. Otherwise every API invocation incurs\n   * the overhead of opening/closing a TCP connection.",
  "org.apache.hadoop.security.Groups:getBackgroundRefreshSuccess()" : null,
  "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:shouldLog()" : null,
  "org.apache.hadoop.io.WritableUtils:readCompressedStringArray(java.io.DataInput)" : null,
  "org.apache.hadoop.io.BooleanWritable:set(boolean)" : "* Set the value of the BooleanWritable.\n   * @param value value.",
  "org.apache.hadoop.fs.viewfs.ViewFs:getFileStatus(org.apache.hadoop.fs.Path)" : "* {@inheritDoc}\n   *\n   * If the given path is a symlink(mount link), the path will be resolved to a\n   * target path and it will get the resolved path's FileStatus object. It will\n   * not be represented as a symlink and isDirectory API returns true if the\n   * resolved path is a directory, false otherwise.",
  "org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.String)" : "* Create an exception with the specific exit code and text.\n   * @param exitCode exit code\n   * @param message message to use in exception",
  "org.apache.hadoop.metrics2.lib.MutableQuantiles$RolloverSample:<init>(org.apache.hadoop.metrics2.lib.MutableQuantiles)" : null,
  "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:<init>()" : null,
  "org.apache.hadoop.io.DoubleWritable$Comparator:compare(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReference:getData()" : null,
  "org.apache.hadoop.util.NativeCrc32:verifyChunkedSums(int,int,java.nio.ByteBuffer,java.nio.ByteBuffer,java.lang.String,long)" : "* Verify the given buffers of data and checksums, and throw an exception\n   * if any checksum is invalid. The buffers given to this function should\n   * have their position initially at the start of the data, and their limit\n   * set at the end of the data. The position, limit, and mark are not\n   * modified.\n   * \n   * @param bytesPerSum the chunk size (eg 512 bytes)\n   * @param checksumType the DataChecksum type constant (NULL is not supported)\n   * @param sums the DirectByteBuffer pointing at the beginning of the\n   *             stored checksums\n   * @param data the DirectByteBuffer pointing at the beginning of the\n   *             data to check\n   * @param basePos the position in the file where the data buffer starts \n   * @param fileName the name of the file being verified\n   * @throws ChecksumException if there is an invalid checksum",
  "org.apache.hadoop.fs.permission.PermissionParser:applyOctalPattern(java.util.regex.Matcher)" : null,
  "org.apache.hadoop.util.Shell:getQualifiedBinInner(java.io.File,java.lang.String)" : "* Inner logic of {@link #getQualifiedBin(String)}, accessible\n   * for tests.\n   * @param hadoopHomeDir home directory (assumed to be valid)\n   * @param executable executable\n   * @return path to the binary\n   * @throws FileNotFoundException if the executable was not found/valid",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)" : "* Renews a delegation token from the server end-point using the\n   * configured <code>Authenticator</code> for authentication.\n   *\n   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are\n   * supported.\n   * @param token the authentication token with the Delegation Token to renew.\n   * @param doAsUser the user to do as, which will be the token owner.\n   * @throws IOException if an IO error occurred.\n   * @throws AuthenticationException if an authentication exception occurred.\n   * @return delegation token long value.",
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)" : "* Construct from a {@link FileContext}.\n   *\n   * @param fc FileContext\n   * @param p path.\n   * @throws IOException failure",
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:mayThrow(java.util.List)" : null,
  "org.apache.hadoop.util.JvmPauseMonitor$Monitor:run()" : null,
  "org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)" : "* Iterates over the given expanded paths and invokes\n   * {@link #processPath(PathData)} on each element. If \"recursive\" is true,\n   * will do a post-visit DFS on directories.\n   * @param parent if called via a recurse, will be the parent dir, else null\n   * @param itemsIterator a iterator of {@link PathData} objects to process\n   * @throws IOException if anything goes wrong...",
  "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.ipc.Server:setupResponse(org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)" : null,
  "org.apache.hadoop.security.token.SecretManager:createPassword(byte[],javax.crypto.SecretKey)" : "* Compute HMAC of the identifier using the secret key and return the \n   * output as password\n   * @param identifier the bytes of the identifier\n   * @param key the secret key\n   * @return the bytes of the generated password",
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:stringifySecurityProperty(java.lang.String)" : "* Turn a security property into a nicely formatted set of <i>name=value</i>\n   * strings, allowing for either the property or the configuration not to be\n   * set.\n   *\n   * @param property the property to stringify\n   * @return the stringified property",
  "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:hashCode()" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)" : "* Called with a source and target destination pair\n   * @param src for the operation\n   * @param dst for the operation\n   * @throws IOException if anything goes wrong",
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable)" : null,
  "org.apache.hadoop.util.IntrusiveCollection:toArray(java.lang.Object[])" : null,
  "org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])" : null,
  "org.apache.hadoop.security.token.Token:isPrivateCloneOf(org.apache.hadoop.io.Text)" : "* Whether this is a private clone of a public token.\n   * @param thePublicService the public service name\n   * @return false always for non-private tokens",
  "org.apache.hadoop.metrics2.sink.GraphiteSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)" : null,
  "org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthorizationFailures()" : null,
  "org.apache.hadoop.util.ConfTest:terminate(int,java.lang.String)" : null,
  "org.apache.hadoop.conf.Configuration$IntegerRanges:iterator()" : null,
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_toPrettyString(java.lang.Object)" : "* Convert IOStatistics to a string form, with all the metrics sorted\n   * and empty value stripped.\n   * @param statistics A statistics instance; may be null\n   * @return string value or the empty string if null",
  "org.apache.hadoop.conf.StorageUnit$4:toKBs(double)" : null,
  "org.apache.hadoop.fs.FileSystem$3:<init>()" : null,
  "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:toString()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.fs.FilterFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.metrics2.util.SampleStat:mean()" : "* @return  the arithmetic mean of the samples",
  "org.apache.hadoop.util.DataChecksum:verifyChunked(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,byte[],int,int,int,byte[],int,java.lang.String,long)" : "* Implementation of chunked verification specifically on byte arrays. This\n   * is to avoid the copy when dealing with ByteBuffers that have array backing.",
  "org.apache.hadoop.ipc.CallerContext:getContext()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(byte[],int,int)" : null,
  "org.apache.hadoop.util.ComparableVersion$IntegerItem:getType()" : null,
  "org.apache.hadoop.conf.Configuration:getTrimmedStringCollection(java.lang.String)" : "* Get the comma delimited values of the <code>name</code> property as \n   * a collection of <code>String</code>s, trimmed of the leading and trailing whitespace.  \n   * If no such property is specified then empty <code>Collection</code> is returned.\n   *\n   * @param name property name.\n   * @return property value as a collection of <code>String</code>s, or empty <code>Collection</code>",
  "org.apache.hadoop.fs.AbstractFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)" : "* Unset the storage policy set for a given file or directory.\n   * @param src file or directory path.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.FileSystem:getDefaultReplication()" : "* Get the default replication.\n   * @return the replication; the default value is \"1\".\n   * @deprecated use {@link #getDefaultReplication(Path)} instead",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKeyInternal(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)" : null,
  "org.apache.hadoop.fs.FSLinkResolver:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)" : "* Performs the operation specified by the next function, calling it\n   * repeatedly until all symlinks in the given path are resolved.\n   * @param fc FileContext used to access file systems.\n   * @param path The path to resolve symlinks on.\n   * @return Generic type determined by the implementation of next.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.SysInfo:newInstance()" : "* Return default OS instance.\n   * @throws UnsupportedOperationException If cannot determine OS.\n   * @return Default instance for the detected OS.",
  "org.apache.hadoop.tools.GetGroupsBase:run(java.lang.String[])" : "* Get the groups for the users given and print formatted output to the\n   * {@link PrintStream} configured earlier.",
  "org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)" : "* Create the named file with write-progress reporter.\n     * @deprecated Use \n     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} \n     *   instead.\n     * @param fs input filesystem.\n     * @param conf input configuration.\n     * @param name input name.\n     * @param keyClass input keyClass.\n     * @param valClass input valClass.\n     * @param progress input progress.\n     * @param metadata input metadata.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.http.HttpsFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:getUsage()" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyCompressor:setInputFromSavedData()" : "* If a write would exceed the capacity of the direct buffers, it is set\n   * aside to be loaded by this function while the compressed data are\n   * consumed.",
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:generateMTFValues()" : null,
  "org.apache.hadoop.util.bloom.DynamicBloomFilter:or(org.apache.hadoop.util.bloom.Filter)" : null,
  "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read()" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:createGenericOptionsParser(org.apache.hadoop.conf.Configuration,java.lang.String[])" : "* Override point: create a generic options parser or subclass thereof.\n   * @param conf Hadoop configuration\n   * @param argArray array of arguments\n   * @return a generic options parser to parse the arguments\n   * @throws IOException on any failure",
  "org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Copy file from src to dest. See\n     * {@link #copy(Path, Path, boolean, boolean)}\n     *\n     * @param src src.\n     * @param dst dst.\n     * @throws AccessControlException If access is denied.\n     * @throws FileAlreadyExistsException If file <code>src</code> already exists.\n     * @throws FileNotFoundException if next file does not exist any more.\n     * @throws ParentNotDirectoryException If parent of <code>src</code> is not a\n     * directory.\n     * @throws UnsupportedFileSystemException If file system for\n     * <code>src/dst</code> is not supported.\n     * @throws IOException If an I/O error occurred.\n     * @return if success copy true, not false.",
  "org.apache.hadoop.ha.HAServiceTarget:getZKFCProxy(org.apache.hadoop.conf.Configuration,int)" : "* @return a proxy to the ZKFC which is associated with this HA service.\n   * @param conf configuration.\n   * @param timeoutMs timeout in milliseconds.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.shell.Test:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.service.launcher.ServiceShutdownHook:shutdown()" : "* Shutdown operation.\n   * <p>\n   * Subclasses may extend it, but it is primarily\n   * made available for testing.\n   * @return true if the service was stopped and no exception was raised.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:fsGetter()" : "* This method is overridden because in ViewFileSystemOverloadScheme if\n   * overloaded scheme matches with mounted target fs scheme, file system\n   * should be created without going into {@literal fs.<scheme>.impl} based\n   * resolution. Otherwise it will end up in an infinite loop as the target\n   * will be resolved again to ViewFileSystemOverloadScheme as\n   * {@literal fs.<scheme>.impl} points to ViewFileSystemOverloadScheme.\n   * So, below method will initialize the\n   * {@literal fs.viewfs.overload.scheme.target.<scheme>.impl}.\n   * Other schemes can follow fs.newInstance",
  "org.apache.hadoop.ipc.Server:getSchedulerClass(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:exit(int,java.lang.String)" : "* Exit the JVM.\n   *\n   * This is method can be overridden for testing, throwing an \n   * exception instead. Any subclassed method MUST raise an \n   * {@code ExitException} instance/subclass.\n   * The service launcher code assumes that after this method is invoked,\n   * no other code in the same method is called.\n   * @param exitCode code to exit\n   * @param message input message.",
  "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolServerSideTranslatorPB:refreshCallQueue(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.proto.RefreshCallQueueProtocolProtos$RefreshCallQueueRequestProto)" : null,
  "org.apache.hadoop.fs.QuotaUsage:formatSize(long,boolean)" : "* Formats a size to be human readable or in bytes.\n   * @param size value to be formatted\n   * @param humanReadable flag indicating human readable or not\n   * @return String representation of the size",
  "org.apache.hadoop.io.MapFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)" : null,
  "org.apache.hadoop.http.HtmlQuoting:main(java.lang.String[])" : null,
  "org.apache.hadoop.ipc.CallQueueManager:setClientBackoffEnabled(boolean)" : null,
  "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createSocket(java.net.InetAddress,int)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.Client$Connection:cleanupCalls()" : null,
  "org.apache.hadoop.fs.shell.find.Name:<init>()" : "Creates a case sensitive name expression.",
  "org.apache.hadoop.fs.Options$HandleOpt:<init>()" : null,
  "org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(int)" : null,
  "org.apache.hadoop.ha.HAAdmin:getAllServiceState()" : null,
  "org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread:run()" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader:windowBits()" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:readCharArray(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:next()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:release(java.lang.Object)" : "* Releases a previously acquired resource.\n   *\n   * @throws IllegalArgumentException if item is null.",
  "org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.Throwable,java.lang.String,java.lang.Object[])" : null,
  "org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException:getMessage()" : null,
  "org.apache.hadoop.net.unix.DomainSocket$DomainChannel:close()" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:serviceCreationFailure(java.lang.Exception)" : "* Generate an exception announcing a failure to create the service.\n   * @param exception inner exception.\n   * @return a new exception, with the exit code\n   * {@link LauncherExitCodes#EXIT_SERVICE_CREATION_FAILURE}",
  "org.apache.hadoop.service.AbstractService:getName()" : null,
  "org.apache.hadoop.fs.viewfs.RegexMountPoint:getSrcPathRegex()" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:isBlockCompressed()" : "* Returns true if records are block-compressed.\n     * @return if records are block-compressed true, not false.",
  "org.apache.hadoop.util.StringUtils:isAlpha(java.lang.String)" : "* <p>Checks if the String contains only unicode letters.</p>\n   *\n   * <p><code>null</code> will return <code>false</code>.\n   * An empty String (length()=0) will return <code>true</code>.</p>\n   *\n   * <pre>\n   * StringUtils.isAlpha(null)   = false\n   * StringUtils.isAlpha(\"\")     = true\n   * StringUtils.isAlpha(\"  \")   = false\n   * StringUtils.isAlpha(\"abc\")  = true\n   * StringUtils.isAlpha(\"ab2c\") = false\n   * StringUtils.isAlpha(\"ab-c\") = false\n   * </pre>\n   *\n   * @param str  the String to check, may be null\n   * @return <code>true</code> if only contains letters, and is non-null",
  "org.apache.hadoop.util.GSetByHashMap:iterator()" : null,
  "org.apache.hadoop.ha.HAAdmin$UsageInfo:<init>(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:release()" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping:isInteger(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDirectDecompressor(org.apache.hadoop.conf.Configuration)" : "* Return the appropriate implementation of the zlib direct decompressor. \n   * \n   * @param conf configuration\n   * @return the appropriate implementation of the zlib decompressor.",
  "org.apache.hadoop.service.launcher.IrqHandler:<init>(java.lang.String,org.apache.hadoop.service.launcher.IrqHandler$Interrupted)" : "* Create an IRQ handler bound to the specific interrupt.\n   * @param name signal name\n   * @param handler handler",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[])" : "* Compare the entry key to another key. Synonymous to compareTo(key, 0,\n         * key.length).\n         * \n         * @param buf\n         *          The key buffer.\n         * @return comparison result between the entry key with the input key.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listXAttrs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.DUHelper:<init>()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:rollLogDirIfNeeded()" : "* Check the current directory against the time stamp.  If they're not\n   * the same, create a new directory and a new log file in that directory.\n   *\n   * @throws MetricsException thrown if an error occurs while creating the\n   * new directory or new log file",
  "org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.CompositeCrcFileChecksum:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPath(org.apache.hadoop.fs.Path)" : "* Validate a path.\n   * @param path path to check.",
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:doOp(org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$ProviderCallable,int,boolean)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem:listLocatedStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.HarFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)" : "* Declare that this filesystem connector is always read only.\n   * {@inheritDoc}",
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:<init>()" : null,
  "org.apache.hadoop.ipc.Server$Connection:saslProcess(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)" : "* Process saslMessage and send saslResponse back\n     * @param saslMessage received SASL message\n     * @throws RpcServerException setup failed due to SASL negotiation\n     *         failure, premature or invalid connection context, or other state \n     *         errors. This exception needs to be sent to the client. This \n     *         exception will wrap {@link RetriableException}, \n     *         {@link InvalidToken}, {@link StandbyException} or \n     *         {@link SaslException}.\n     * @throws IOException if sending reply fails\n     * @throws InterruptedException",
  "org.apache.hadoop.fs.permission.AclEntry$Builder:setName(java.lang.String)" : "* Sets the optional ACL entry name.\n     *\n     * @param name String optional ACL entry name\n     * @return Builder this builder, for call chaining",
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:incrementWriteOps(int)" : "* Increment the number of write operations.\n     * @param count number of write operations",
  "org.apache.hadoop.fs.http.HttpsFileSystem:listStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:hashCode()" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Iterable,java.lang.String)" : "* Validates that the given buffer is not null and has non-zero capacity.\n   * @param <T> the type of iterable's elements.\n   * @param iter the argument reference to validate.\n   * @param argName the name of the argument being validated.",
  "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:memoryAllocated(int)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:getLastKey()" : "* Get the last key in the TFile.\n     * \n     * @return The last key in the TFile.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)" : "* Create a MD5FileChecksum.\n   *\n   * @param bytesPerCRC bytesPerCRC.\n   * @param crcPerBlock crcPerBlock.\n   * @param md5 md5.",
  "org.apache.hadoop.ipc.DecayRpcScheduler:getIdentity(org.apache.hadoop.ipc.Schedulable)" : null,
  "org.apache.hadoop.fs.store.audit.AuditingFunctions:withinAuditSpan(org.apache.hadoop.fs.store.audit.AuditSpan,org.apache.hadoop.util.functional.InvocationRaisingIOE)" : "* Given an invocation, return a new invocation which\n   * activates and deactivates the span around the inner invocation.\n   * @param auditSpan audit span\n   * @param operation operation\n   * @return a new invocation.",
  "org.apache.hadoop.conf.Configuration:putIntoUpdatingResource(java.lang.String,java.lang.String[])" : null,
  "org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String,java.lang.String)" : "* Construct a Path from components.\n   *\n   * @param scheme the scheme\n   * @param authority the authority\n   * @param path the path",
  "org.apache.hadoop.metrics2.lib.MethodMetric:nameFrom(java.lang.reflect.Method)" : null,
  "org.apache.hadoop.metrics2.util.MetricsCache:<init>(int)" : "* Construct a metrics cache\n   * @param maxRecsPerName  limit of the number records per record name",
  "org.apache.hadoop.io.file.tfile.TFile$Reader:getEntryComparator()" : "* Get a Comparator object to compare Entries. It is useful when you want\n     * stores the entries in a collection (such as PriorityQueue) and perform\n     * sorting or comparison among entries based on the keys without copying out\n     * the key.\n     * \n     * @return An Entry Comparator..",
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getChecksumOpt()" : null,
  "org.apache.hadoop.fs.FileStatus:getGroup()" : "* Get the group associated with the file.\n   * @return group for the file. The string could be empty if there is no\n   *         notion of group of a file in a filesystem or if it could not \n   *         be determined (rare).",
  "org.apache.hadoop.metrics2.lib.MutableInverseQuantiles:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)" : "* Instantiates a new {@link MutableInverseQuantiles} for a metric that rolls itself\n   * over on the specified time interval.\n   *\n   * @param name          of the metric\n   * @param description   long-form textual description of the metric\n   * @param sampleName    type of items in the stream (e.g., \"Ops\")\n   * @param valueName     type of the values\n   * @param intervalSecs  rollover interval (in seconds) of the estimator",
  "org.apache.hadoop.fs.FilterFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:decompress(java.nio.ByteBuffer,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.util.Shell:getWinUtilsPath()" : "* Locate the winutils binary, or fail with a meaningful\n   * exception and stack trace as an RTE.\n   * This method is for use in methods which don't explicitly throw\n   * an <code>IOException</code>.\n   * @return the path to {@link #WINUTILS_EXE}\n   * @throws RuntimeException if the path is not resolvable",
  "org.apache.hadoop.ipc.RemoteException:<init>(java.lang.String,java.lang.String,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto)" : "* @param className wrapped exception, may be null\n   * @param msg may be null\n   * @param erCode may be null",
  "org.apache.hadoop.metrics2.impl.MetricGaugeInt:value()" : null,
  "org.apache.hadoop.fs.shell.Command:expandArgument(java.lang.String)" : "* Expand the given argument into a list of {@link PathData} objects.\n   * The default behavior is to expand globs.  Commands may override to\n   * perform other expansions on an argument.\n   * @param arg string pattern to expand\n   * @return list of {@link PathData} objects\n   * @throws IOException if anything goes wrong...",
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:rollNewVersion(java.lang.String,byte[])" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:performDecodeImpl(java.nio.ByteBuffer[],int[],int,int[],java.nio.ByteBuffer[],int[])" : null,
  "org.apache.hadoop.util.LightWeightGSet:clear()" : null,
  "org.apache.hadoop.crypto.OpensslCipher:getInstance(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.DefaultCodec:createInputStream(java.io.InputStream)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:memCopy(byte[],long,boolean,long)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators:filteringRemoteIterator(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)" : "* Create a RemoteIterator from a RemoteIterator and a filter\n   * function which returns true for every element to be passed\n   * through.\n   * <p>\n   * Elements are filtered in the hasNext() method; if not used\n   * the filtering will be done on demand in the {@code next()}\n   * call.\n   * </p>\n   * @param <S> type\n   * @param iterator source\n   * @param filter filter\n   * @return a remote iterator",
  "org.apache.hadoop.fs.HarFileSystem:<init>(org.apache.hadoop.fs.FileSystem)" : "* Constructor to create a HarFileSystem with an\n   * underlying filesystem.\n   * @param fs underlying file system",
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,float)" : null,
  "org.apache.hadoop.net.NetUtils:getStaticResolution(java.lang.String)" : "* Retrieves the resolved name for the passed host. The resolved name must\n   * have been set earlier using \n   * {@link NetUtils#addStaticResolution(String, String)}\n   * @param host the hostname or IP use to instantiate the object.\n   * @return the resolution",
  "org.apache.hadoop.io.TwoDArrayWritable:<init>(java.lang.Class)" : null,
  "org.apache.hadoop.ipc.RpcWritable$Buffer:remaining()" : null,
  "org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo:<init>(java.lang.String,int,java.lang.String)" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)" : "* Construct a client-side proxy object that implements the named protocol,\n   * talking to a server at the named address. \n   * @param <T> Generics Type T\n   * @param protocol input protocol.\n   * @param clientVersion input clientVersion.\n   * @param addr input addr.\n   * @param ticket input ticket.\n   * @param conf input configuration.\n   * @param factory input factory.\n   * @param rpcTimeout input rpcTimeout.\n   * @param connectionRetryPolicy input connectionRetryPolicy.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.SequentialNumber:skipTo(long)" : "* Skip to the new value.\n   * @param newValue newValue.\n   * @throws IllegalStateException\n   *         Cannot skip to less than the current value.",
  "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:close()" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:needsDictionary()" : "* Returns <code>false</code>.\n   *\n   * @return <code>false</code>.",
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getDelegationKey(int)" : null,
  "org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedMethods(javax.servlet.FilterConfig)" : null,
  "org.apache.hadoop.fs.FSInputChecker:readAndDiscard(int)" : "* Like read(byte[], int, int), but does not provide a dest buffer,\n   * so the read data is discarded.\n   * @param      len maximum number of bytes to read.\n   * @return     the number of bytes read.\n   * @throws     IOException  if an I/O error occurs.",
  "org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:<init>(int,boolean)" : null,
  "org.apache.hadoop.fs.QuotaUsage:<init>(org.apache.hadoop.fs.QuotaUsage$Builder)" : "Build the instance based on the builder.\n   * @param builder bulider.",
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:clear()" : null,
  "org.apache.hadoop.fs.Path:validateObject()" : "* Validate the contents of a deserialized Path, so as\n   * to defend against malicious object streams.\n   * @throws InvalidObjectException if there's no URI",
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSinks()" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:setData(java.lang.String,java.lang.String,int)" : "* Set data into a ZNode.\n   * @param path Path of the ZNode.\n   * @param data Data to set as String.\n   * @param version Version of the data to store.\n   * @throws Exception If it cannot contact Zookeeper.",
  "org.apache.hadoop.fs.shell.find.BaseExpression:getArgument(int)" : "* Returns the argument at the given position (starting from 1).\n   *\n   * @param position\n   *          argument to be returned\n   * @return requested argument\n   * @throws IOException\n   *           if the argument doesn't exist or is null",
  "org.apache.hadoop.security.token.Token:hashCode()" : null,
  "org.apache.hadoop.security.alias.LocalKeyStoreProvider:keystoreExists()" : null,
  "org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)" : "* Get the value of the <code>name</code>. If the key is deprecated,\n   * it returns the value of the first key which replaces the deprecated key\n   * and is not null.\n   * If no such property exists,\n   * then <code>defaultValue</code> is returned.\n   * \n   * @param name property name, will be trimmed before get value.\n   * @param defaultValue default value.\n   * @return property value, or <code>defaultValue</code> if the property \n   *         doesn't exist.",
  "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMeanStatistic(java.lang.String)" : "* Get a mean statistic.\n   * @param key statistic name\n   * @return the reference\n   * @throws NullPointerException if there is no entry of that name",
  "org.apache.hadoop.net.SocketInputStream$Reader:<init>(java.nio.channels.ReadableByteChannel,long)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:reset()" : "* Resets everything including the input buffers (user and direct).",
  "org.apache.hadoop.util.SysInfoLinux:main(java.lang.String[])" : "* Test the {@link SysInfoLinux}.\n   *\n   * @param args - arguments to this calculator test",
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getMyFs()" : null,
  "org.apache.hadoop.fs.FileSystem:openFileWithOptions(org.apache.hadoop.fs.PathHandle,org.apache.hadoop.fs.impl.OpenFileParameters)" : "* Execute the actual open file operation.\n   * The base implementation performs a blocking\n   * call to {@link #open(Path, int)} in this call;\n   * the actual outcome is in the returned {@code CompletableFuture}.\n   * This avoids having to create some thread pool, while still\n   * setting up the expectation that the {@code get()} call\n   * is needed to evaluate the result.\n   * @param pathHandle path to the file\n   * @param parameters open file parameters from the builder.\n   * @return a future which will evaluate to the opened file.\n   * @throws IOException failure to resolve the link.\n   * @throws IllegalArgumentException unknown mandatory key\n   * @throws UnsupportedOperationException PathHandles are not supported.\n   * This may be deferred until the future is evaluated.",
  "org.apache.hadoop.ipc.Server$RpcCall:getRemotePort()" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:recomputeScheduleCache()" : "* Update the scheduleCache to match current conditions in callCosts.",
  "org.apache.hadoop.fs.impl.WeakRefMetricsSource:toString()" : null,
  "org.apache.hadoop.ipc.Client$ConnectionId:getMaxRetriesOnSasl()" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:checkCreateRSRawEncoder()" : null,
  "org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)" : "* Construct a set reader for the named set using the named comparator.\n     * @param fs input FileSystem.\n     * @param dirName input dirName.\n     * @param comparator input comparator.\n     * @param conf input Configuration.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.conf.Configuration:setBooleanIfUnset(java.lang.String,boolean)" : "* Set the given property, if it is currently unset.\n   * @param name property name\n   * @param value new value",
  "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:build()" : "* Build.\n     * @return an HttpReferrerAuditHeader",
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:needsDictionary()" : null,
  "org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : "* This method is overridden to restrict HTTP authentication schemes\n   * available for delegation token management functionality. The\n   * authentication schemes to be used for delegation token management are\n   * configured using {@link DELEGATION_TOKEN_SCHEMES_PROPERTY}\n   *\n   * The basic logic here is to check if the current request is for delegation\n   * token management. If yes then check if the request contains an\n   * \"Authorization\" header. If it is missing, then return the HTTP 401\n   * response with WWW-Authenticate header for each scheme configured for\n   * delegation token management.\n   *\n   * It is also possible for a client to preemptively send Authorization header\n   * for a scheme not configured for delegation token management. We detect\n   * this case and return the HTTP 401 response with WWW-Authenticate header\n   * for each scheme configured for delegation token management.\n   *\n   * If a client has sent a request with \"Authorization\" header for a scheme\n   * configured for delegation token management, then it is forwarded to\n   * underlying {@link MultiSchemeAuthenticationHandler} for actual\n   * authentication.\n   *\n   * Finally all other requests (excluding delegation token management) are\n   * forwarded to underlying {@link MultiSchemeAuthenticationHandler} for\n   * actual authentication.",
  "org.apache.hadoop.fs.FileSystem:newInstance(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Returns the FileSystem for this URI's scheme and authority.\n   * The entire URI is passed to the FileSystem instance's initialize method.\n   * This always returns a new FileSystem object.\n   * @param uri FS URI\n   * @param config configuration to use\n   * @return the new FS instance\n   * @throws IOException FS creation or initialization failure.",
  "org.apache.hadoop.fs.CachingGetSpaceUsed:running()" : "* Is the background thread running.",
  "org.apache.hadoop.io.compress.zlib.ZlibFactory:getLibraryName()" : null,
  "org.apache.hadoop.fs.impl.FlagSet:hasCapability(java.lang.String)" : "* Is a flag enabled?\n   * @param capability string to query the stream support for.\n   * @return true if the capability maps to an enum value and\n   * that value is set.",
  "org.apache.hadoop.util.VersionInfo:_getBuildVersion()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getSource(java.lang.String)" : null,
  "org.apache.hadoop.http.lib.StaticUserWebFilter$User:equals(java.lang.Object)" : null,
  "org.apache.hadoop.security.UserGroupInformation:getOSLoginModuleName()" : null,
  "org.apache.hadoop.security.SecurityUtil:getComponents(java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:delete(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping:updateMapInternal(org.apache.hadoop.thirdparty.com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map)" : "* Get the list of users or groups returned by the specified command,\n   * and save them in the corresponding map.\n   *\n   * @param map map.\n   * @param mapName mapName.\n   * @param command command.\n   * @param staticMapping staticMapping.\n   * @param regex regex.\n   * @throws IOException raised on errors performing I/O.\n   * @return updateMapInternal.",
  "org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:<init>(java.lang.Thread$UncaughtExceptionHandler)" : "* Create an instance delegating to the supplied handler if\n   * the exception is considered \"simple\".\n   * @param delegate a delegate exception handler.",
  "org.apache.hadoop.crypto.CipherSuite:convert(java.lang.String)" : "* Convert to CipherSuite from name, {@link #algoBlockSize} is fixed for\n   * certain cipher suite, just need to compare the name.\n   * @param name cipher suite name\n   * @return CipherSuite cipher suite",
  "org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String)" : "* Create a Meta Block and obtain an output stream for adding data into the\n     * block. The Meta Block will be compressed with the same compression\n     * algorithm as data blocks. There can only be one BlockAppender stream\n     * active at any time. Regular Blocks may not be created after the first\n     * Meta Blocks. The caller must call BlockAppender.close() to conclude the\n     * block creation.\n     * \n     * @param name\n     *          The name of the Meta Block. The name must not conflict with\n     *          existing Meta Blocks.\n     * @return The BlockAppender stream\n     * @throws MetaBlockAlreadyExists\n     *           If the meta block with the name already exists.\n     * @throws IOException",
  "org.apache.hadoop.ipc.Server$Listener:doAccept(java.nio.channels.SelectionKey)" : null,
  "org.apache.hadoop.metrics2.lib.MutableGaugeInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)" : null,
  "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:becomeActive()" : null,
  "org.apache.hadoop.metrics2.util.MetricsCache:update(org.apache.hadoop.metrics2.MetricsRecord,boolean)" : "* Update the cache and return the current cached record\n   * @param mr the update record\n   * @param includingTags cache tag values (for later lookup by name) if true\n   * @return the updated cache record",
  "org.apache.hadoop.fs.permission.FsAction:implies(org.apache.hadoop.fs.permission.FsAction)" : "* Return true if this action implies that action.\n   * @param that FsAction that.\n   * @return if implies true,not false.",
  "org.apache.hadoop.fs.shell.TouchCommands$Touch:updateTime(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.ipc.Server$ConnectionManager:getUserToConnectionsMap()" : null,
  "org.apache.hadoop.fs.shell.Command:runAll()" : "* For each source path, execute the command\n   * \n   * @return 0 if it runs successfully; -1 if it fails",
  "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])" : null,
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:innerSetCredential(java.lang.String,char[])" : null,
  "org.apache.hadoop.fs.StorageStatistics:getName()" : "* Get the name of this StorageStatistics object.\n   * @return name of this StorageStatistics object",
  "org.apache.hadoop.util.SysInfoLinux:readProcCpuInfoFile()" : "* Read /proc/cpuinfo, parse and calculate CPU information.",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration)" : "* Check if native-bzip2 code is loaded &amp; initialized correctly and\n   * can be loaded for this job.\n   * \n   * @param conf configuration\n   * @return <code>true</code> if native-bzip2 is loaded &amp; initialized\n   *         and can be loaded for this job, else <code>false</code>",
  "org.apache.hadoop.util.HostsFileReader:readFileToSet(java.lang.String,java.lang.String,java.util.Set)" : null,
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:isSupported()" : null,
  "org.apache.hadoop.security.KerberosAuthException:setPrincipal(java.lang.String)" : null,
  "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:prefetchOperationCompleted()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getUri()" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:getTotalCallVolume()" : null,
  "org.apache.hadoop.service.launcher.ServiceShutdownHook:unregister()" : "* Unregister the hook.",
  "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:setIncludeTagPattern(java.lang.String,com.google.re2j.Pattern)" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:decompress(byte[],int,int)" : "* Fills specified buffer with uncompressed data. Returns actual number\n   * of bytes of uncompressed data. A return value of 0 indicates that\n   * {@link #needsInput()} should be called in order to determine if more\n   * input data is required.\n   *\n   * @param b   Buffer for the compressed data\n   * @param off Start offset of the data\n   * @param len Size of the buffer\n   * @return The actual number of bytes of uncompressed data.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.DataChecksum:calculateChunkedSums(byte[],int,int,byte[],int)" : "* Implementation of chunked calculation specifically on byte arrays. This\n   * is to avoid the copy when dealing with ByteBuffers that have array backing.\n   *\n   * @param data data.\n   * @param dataOffset dataOffset.\n   * @param dataLength dataLength.\n   * @param sums sums.\n   * @param sumsOffset sumsOffset.",
  "org.apache.hadoop.util.functional.LazyAtomicReference:eval()" : "* Get the value, constructing it if needed.\n   * @return the value\n   * @throws IOException on any evaluation failure\n   * @throws NullPointerException if the evaluated function returned null.",
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:write(byte[],int,int)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:<init>(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)" : null,
  "org.apache.hadoop.fs.FileContext:clearStatistics()" : "* Clears all the statistics stored in AbstractFileSystem, for all the file\n   * systems.",
  "org.apache.hadoop.fs.RawLocalFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])" : null,
  "org.apache.hadoop.security.NetgroupCache:getGroups()" : null,
  "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:getValueCount()" : null,
  "org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$INode:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation)" : null,
  "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:getCount()" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)" : "* An iterator which combines filtering with transformation.\n     * All source elements for which filter = true are returned,\n     * transformed via the mapper.\n     * @param source source iterator.\n     * @param filter filter predicate.",
  "org.apache.hadoop.security.ssl.SSLFactory:getHostnameVerifier(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.Count:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : "* Register the names for the count command\n   * @param factory the command factory that will instantiate this class",
  "org.apache.hadoop.fs.BatchedRemoteIterator:next()" : null,
  "org.apache.hadoop.io.SortedMapWritable:putAll(java.util.Map)" : null,
  "org.apache.hadoop.log.LogThrottlingHelper:record(double[])" : "* Record some set of values at the current time into this helper. Note that\n   * this does <i>not</i> actually write information to any log. Instead, this\n   * will return a LogAction indicating whether or not the caller should write\n   * to its own log. The LogAction will additionally contain summary information\n   * about the values specified since the last time the caller was expected to\n   * write to its log.\n   *\n   * <p>Specifying multiple values will maintain separate summary statistics\n   * about each value. For example:\n   * <pre>{@code\n   *   helper.record(1, 0);\n   *   LogAction action = helper.record(3, 100);\n   *   action.getStats(0); // == 2\n   *   action.getStats(1); // == 50\n   * }</pre>\n   *\n   * @param values The values about which to maintain summary information. Every\n   *               time this method is called, the same number of values must\n   *               be specified.\n   * @return A LogAction indicating whether or not the caller should write to\n   *         its log.",
  "org.apache.hadoop.security.KDiag:printDefaultRealm()" : "* Get the default realm.\n   * <p>\n   * Not having a default realm may be harmless, so is noted at info.\n   * All other invocation failures are downgraded to warn, as\n   * follow-on actions may still work.\n   * Failure to invoke the method via introspection is considered a failure,\n   * as it's a sign of JVM compatibility issues that may have other \n   * consequences",
  "org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.util.List)" : null,
  "org.apache.hadoop.util.Shell$ShellCommandExecutor:parseExecResult(java.io.BufferedReader)" : null,
  "org.apache.hadoop.io.WritableUtils:getVIntSize(long)" : "* Get the encoded length if an integer is stored in a variable-length format.\n   * @param i input i.\n   * @return the encoded length",
  "org.apache.hadoop.fs.store.LogExactlyOnce:<init>(org.slf4j.Logger)" : null,
  "org.apache.hadoop.security.http.RestCsrfPreventionFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,long)" : "* Get a proxy connection to a remote server.\n   *\n   * @param <T> Generics Type T.\n   * @param protocol protocol class\n   * @param clientVersion client version\n   * @param addr remote address\n   * @param conf configuration to use\n   * @param rpcTimeout timeout for each RPC\n   * @param timeout time in milliseconds before giving up\n   * @return the proxy\n   * @throws IOException if the far end through a RemoteException",
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:decompress(java.nio.ByteBuffer,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:<init>()" : null,
  "org.apache.hadoop.fs.ftp.FtpFs:getServerDefaults()" : null,
  "org.apache.hadoop.fs.ByteBufferUtil:streamHasByteBufferRead(java.io.InputStream)" : "* Determine if a stream can do a byte buffer read via read(ByteBuffer buf)",
  "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfoByNonNativeIO()" : null,
  "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$7:unit()" : null,
  "org.apache.hadoop.util.ProtoUtil:convert(org.apache.hadoop.ipc.RPC$RpcKind)" : null,
  "org.apache.hadoop.net.SocketOutputStream:<init>(java.nio.channels.WritableByteChannel,long)" : "* Create a new ouput stream with the given timeout. If the timeout\n   * is zero, it will be treated as infinite timeout. The socket's\n   * channel will be configured to be non-blocking.\n   * \n   * @param channel \n   *        Channel for writing, should also be a {@link SelectableChannel}.  \n   *        The channel will be configured to be non-blocking.\n   * @param timeout timeout in milliseconds. must not be negative.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:setInput(byte[],int,int)" : null,
  "org.apache.hadoop.ipc.FairCallQueue:poll()" : "* poll() provides no strict consistency: it is possible for poll to return\n   * null even though an element is in the queue.",
  "org.apache.hadoop.util.RateLimitingFactory$RestrictedRateLimiting:acquire(int)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$ExceptionDependentRetry:shouldRetry(java.lang.Exception,int,int,boolean)" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompressDirect(java.nio.ByteBuffer,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:unregisterSource(java.lang.String)" : null,
  "org.apache.hadoop.util.bloom.Filter:add(java.util.Collection)" : "* Adds a collection of keys to <i>this</i> filter.\n   * @param keys The collection of keys.",
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getBlockSize()" : null,
  "org.apache.hadoop.ha.NodeFencer:parseMethod(org.apache.hadoop.conf.Configuration,java.lang.String)" : null,
  "org.apache.hadoop.conf.ReconfigurableBase:setReconfigurationUtil(org.apache.hadoop.conf.ReconfigurationUtil)" : null,
  "org.apache.hadoop.fs.shell.find.And:addChildren(java.util.Deque)" : null,
  "org.apache.hadoop.metrics2.MetricsSystem$AbstractCallback:preStop()" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:generateEncryptedKey(java.lang.String)" : null,
  "org.apache.hadoop.util.dynamic.DynConstructors$Builder:<init>(java.lang.Class)" : null,
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(byte[],int,int)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:endBlock()" : null,
  "org.apache.hadoop.io.Text:clear()" : "* Clear the string to empty.\n   *\n   * <em>Note</em>: For performance reasons, this call does not clear the\n   * underlying byte array that is retrievable via {@link #getBytes()}.\n   * In order to free the byte-array memory, call {@link #set(byte[])}\n   * with an empty byte array (For example, <code>new byte[0]</code>).",
  "org.apache.hadoop.util.Progress:toString(java.lang.StringBuilder)" : null,
  "org.apache.hadoop.util.HttpExceptionUtils:createJerseyExceptionResponse(javax.ws.rs.core.Response$Status,java.lang.Throwable)" : "* Creates a HTTP JAX-RPC response serializing the exception in it as JSON.\n   *\n   * @param status the error code to set in the response\n   * @param ex the exception to serialize in the response\n   * @return the JAX-RPC response with the set error and JSON encoded exception",
  "org.apache.hadoop.fs.FileStatus:readFields(java.io.DataInput)" : "* Read instance encoded as protobuf from stream.\n   * @param in Input stream\n   * @see PBHelper#convert(FileStatus)\n   * @deprecated Use the {@link PBHelper} and protobuf serialization directly.",
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:releaseLock(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry$LockType)" : "* Release the read or write lock.\n     *\n     * @param lockType type of the lock.",
  "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:isJaasConfigurationSet(org.apache.zookeeper.client.ZKClientConfig)" : null,
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongMinimum(java.lang.String,java.util.concurrent.atomic.AtomicLong)" : "* Add a minimum statistic to dynamically return the\n   * latest value of the source.\n   * @param key key of this statistic\n   * @param source atomic long minimum\n   * @return the builder.",
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:setUnits(java.lang.String)" : "* @param units the units to set",
  "org.apache.hadoop.fs.store.DataBlocks:validateWriteArgs(byte[],int,int)" : "* Validate args to a write command. These are the same validation checks\n   * expected for any implementation of {@code OutputStream.write()}.\n   *\n   * @param b   byte array containing data.\n   * @param off offset in array where to start.\n   * @param len number of bytes to be written.\n   * @throws NullPointerException      for a null buffer\n   * @throws IndexOutOfBoundsException if indices are out of range\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.http.lib.StaticUserWebFilter:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:toString()" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:getDelegationTokens(java.lang.String)" : "* Get one or more delegation tokens associated with the filesystem. Normally\n   * a file system returns a single delegation token. A file system that manages\n   * multiple file systems underneath, could return set of delegation tokens for\n   * all the file systems it manages\n   * \n   * @param renewer the account name that is allowed to renew the token.\n   * @return List of delegation tokens.\n   *   If delegation tokens not supported then return a list of size zero.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.CompositeGroupsMapping:cacheGroupsAdd(java.util.List)" : "* Adds groups to cache, no need to do that for this provider\n   *\n   * @param groups unused",
  "org.apache.hadoop.fs.GetSpaceUsed$Builder:setPath(java.io.File)" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_minimums(java.io.Serializable)" : "* Get the minimums of an IOStatisticsSnapshot.\n   * @param source source of statistics.\n   * @return the map of minimums.",
  "org.apache.hadoop.fs.QuotaUsage:setSpaceConsumed(long)" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:fileSystem_openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.FileStatus,java.lang.Long,java.util.Map)" : "* OpenFile assistant, easy reflection-based access to\n   * {@code FileSystem#openFile(Path)} and blocks\n   * awaiting the operation completion.\n   * @param fs filesystem\n   * @param path path\n   * @param policy read policy\n   * @param status optional file status\n   * @param length optional file length\n   * @param options nullable map of other options\n   * @return stream of the opened file\n   * @throws IOException if the operation was attempted and failed.",
  "org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:read(byte[],int,int)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:close()" : null,
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:getEncryptedKeyIv()" : "* @return Initialization vector of the encrypted key. The IV of the\n     * encryption key used to encrypt the encrypted key is derived from this\n     * IV.",
  "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:flush()" : null,
  "org.apache.hadoop.ipc.Server:getTotalRequestsPerSecond()" : null,
  "org.apache.hadoop.metrics2.lib.MutableMetric:changed()" : "* @return  true if metric is changed since last snapshot",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:obtainDelegationTokenAuthenticator(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listXAttrs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileStatus:getSymlink()" : "* @return The contents of the symbolic link.\n   *\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.SaslPropertiesResolver:getServerProperties(java.net.InetAddress)" : "* Identify the Sasl Properties to be used for a connection with a  client.\n   * @param clientAddress client's address\n   * @return the sasl properties to be used for the connection.",
  "org.apache.hadoop.fs.FSDataOutputStream:hasCapability(java.lang.String)" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:build()" : "* Returns the first valid implementation as a UnboundMethod or throws a\n     * RuntimeError if there is none.\n     * @return a {@link UnboundMethod} with a valid implementation\n     * @throws RuntimeException if no implementation was found",
  "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:releaseBuffer(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider:getCurrentKey(java.lang.String)" : "* Get the current version of the key, which should be used for encrypting new\n   * data.\n   * @param name the base name of the key\n   * @return the version name of the current version of the key or null if the\n   *    key version doesn't exist\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.DF:getPercentUsed()" : "@return the amount of the volume full, as a percent.",
  "org.apache.hadoop.tracing.TraceConfiguration:<init>()" : null,
  "org.apache.hadoop.util.Shell:getUsersForNetgroupCommand(java.lang.String)" : "* A command to get a given netgroup's user list.\n   *\n   * @param netgroup net group.\n   * @return users for net group command.",
  "org.apache.hadoop.fs.FileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)" : "* Return an array of {@link FileStatus} objects whose path names match\n   * {@code pathPattern} and is accepted by the user-supplied path filter.\n   * Results are sorted by their path names.\n   *\n   * @param pathPattern a glob specifying the path pattern\n   * @param filter a user-supplied path filter\n   * @return null if {@code pathPattern} has no glob and the path does not exist\n   *         an empty array if {@code pathPattern} has a glob and no path\n   *         matches it else an array of {@link FileStatus} objects matching the\n   *         pattern\n   * @throws IOException if any I/O error occurs when fetching file status",
  "org.apache.hadoop.fs.PathIOException:withFullyQualifiedPath(java.lang.String)" : null,
  "org.apache.hadoop.security.UserGroupInformation:addCredentials(org.apache.hadoop.security.Credentials)" : "* Add the given Credentials to this user.\n   * @param credentials of tokens and secrets",
  "org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)" : null,
  "org.apache.hadoop.util.HostsFileReader:getLazyLoadedHostDetails()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)" : null,
  "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,int,byte[])" : "* Create a line reader that reads from the given stream using the\n   * given buffer-size, and using a custom delimiter of array of\n   * bytes.\n   * @param in The input stream\n   * @param bufferSize Size of the read buffer\n   * @param recordDelimiterBytes The delimiter",
  "org.apache.hadoop.io.erasurecode.rawcoder.EncodingState:checkParameters(java.lang.Object[],java.lang.Object[])" : "* Check and validate decoding parameters, throw exception accordingly.\n   * @param inputs input buffers to check\n   * @param outputs output buffers to check",
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getBufferSize()" : null,
  "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:<init>(javax.net.ssl.SSLSocketFactory)" : null,
  "org.apache.hadoop.ha.HealthMonitor:setLastServiceStatus(org.apache.hadoop.ha.HAServiceStatus)" : null,
  "org.apache.hadoop.util.Lists:<init>()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)" : "* Open a file with the given set of options.\n   * The base implementation performs a blocking\n   * call to {@link #open(Path, int)}in this call;\n   * the actual outcome is in the returned {@code CompletableFuture}.\n   * This avoids having to create some thread pool, while still\n   * setting up the expectation that the {@code get()} call\n   * is needed to evaluate the result.\n   * @param path path to the file\n   * @param parameters open file parameters from the builder.\n   * @return a future which will evaluate to the opened file.\n   * @throws IOException failure to resolve the link.\n   * @throws IllegalArgumentException unknown mandatory key",
  "org.apache.hadoop.conf.StorageUnit$5:getDefault(double)" : null,
  "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:<init>(java.lang.String,org.apache.hadoop.util.DiskValidator)" : null,
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getServiceStatus(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusRequestProto)" : null,
  "org.apache.hadoop.ipc.Server:call(org.apache.hadoop.io.Writable,long)" : "* Called for each call. \n   * @deprecated Use  {@link #call(RPC.RpcKind, String,\n   *  Writable, long)} instead\n   * @param param input param.\n   * @param receiveTime input receiveTime.\n   * @throws Exception if any error occurs.\n   * @return Call",
  "org.apache.hadoop.metrics2.impl.MetricsConfig:getClassName(java.lang.String)" : null,
  "org.apache.hadoop.net.CachedDNSToSwitchMapping:getSwitchMap()" : "* Get the (host x switch) map.\n   * @return a copy of the cached map of hosts to rack",
  "org.apache.hadoop.fs.FilterFs:createMultipartUploader(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarMetaData:getMasterIndexTimestamp()" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping:getId2NameCmdMac(int,boolean)" : null,
  "org.apache.hadoop.ipc.FairCallQueue:offerQueues(int,org.apache.hadoop.ipc.Schedulable,boolean)" : "* Offer the element to queue of the given or lower priority.\n   * @param priority - starting queue priority\n   * @param e - element to add\n   * @param includeLast - whether to attempt last queue\n   * @return boolean if added to a queue",
  "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator)" : null,
  "org.apache.hadoop.io.DataOutputBuffer$Buffer:getData()" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:getFileDescriptor()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeTokenForOwnerStats(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : "* Remove token stats to the owner to token count mapping.\n   *\n   * @param id",
  "org.apache.hadoop.fs.shell.Display$Cat:getInputStream(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarMetaData:getVersion()" : null,
  "org.apache.hadoop.util.InstrumentedLock:getTimer()" : null,
  "org.apache.hadoop.ipc.ProcessingDetails:get(org.apache.hadoop.ipc.ProcessingDetails$Timing)" : null,
  "org.apache.hadoop.http.HtmlQuoting:quoteOutputStream(java.io.OutputStream)" : "* Return an output stream that quotes all of the output.\n   * @param out the stream to write the quoted output to\n   * @return a new stream that the application show write to\n   * @throws IOException if the underlying output fails",
  "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState)" : null,
  "org.apache.hadoop.util.LightWeightCache:iterator()" : null,
  "org.apache.hadoop.util.InstrumentedWriteLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long)" : null,
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:setCurrentKeyId(int)" : "* Updates the value of the last delegation key id.\n   * @param keyId Value to update the delegation key id to.",
  "org.apache.hadoop.io.DoubleWritable:<init>()" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeByPiggyBack(java.nio.ByteBuffer[],java.nio.ByteBuffer,java.nio.ByteBuffer,int)" : null,
  "org.apache.hadoop.util.Lists:saturatedCast(long)" : "* Returns the {@code int} nearest in value to {@code value}.\n   *\n   * @param value any {@code long} value.\n   * @return the same value cast to {@code int} if it is in the range of the\n   *     {@code int} type, {@link Integer#MAX_VALUE} if it is too large,\n   *     or {@link Integer#MIN_VALUE} if it is too small.",
  "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:getPiggyBackForDecode(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int,int,int,int)" : null,
  "org.apache.hadoop.io.compress.BlockDecompressorStream:rawReadInt()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:listCorruptFileBlocks(org.apache.hadoop.fs.Path)" : "* @return an iterator over the corrupt files under the given path\n   * (may contain duplicates if a file has more than one corrupt block)\n   * @param path the path.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.IdentityHashStore:getElementIndex(java.lang.Object)" : null,
  "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,float)" : null,
  "org.apache.hadoop.util.ShutdownHookManager$HookEntry:<init>(java.lang.Runnable,int)" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadAndReturnPerm(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.Path:makeQualified(org.apache.hadoop.fs.FileSystem)" : "* Returns a qualified path object for the {@link FileSystem}'s working\n   * directory.\n   *  \n   * @param fs the target FileSystem\n   * @return a qualified path object for the FileSystem's working directory\n   * @deprecated use {@link #makeQualified(URI, Path)}",
  "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:getDelegationToken(java.lang.String)" : null,
  "org.apache.hadoop.fs.PathIOException:<init>(java.lang.String,java.lang.String)" : "* Avoid using this method.  Use a subclass of PathIOException if\n   * possible.\n   * @param path for the exception\n   * @param error custom string to use an the error text",
  "org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:getMessage()" : null,
  "org.apache.hadoop.metrics2.lib.MethodMetric:isFloat(java.lang.Class)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:useStatIfAvailable()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:create()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:clearConfigs()" : null,
  "org.apache.hadoop.fs.GetSpaceUsed$Builder:getInitialUsed()" : null,
  "org.apache.hadoop.io.compress.SnappyCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)" : "* Create a {@link CompressionInputStream} that will read from the given\n   * {@link InputStream} with the given {@link Decompressor}.\n   *\n   * @param in           the stream to read compressed bytes from\n   * @param decompressor decompressor to use\n   * @return a stream to read uncompressed bytes from\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.DelegateToFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:getChildren()" : "* Returns the children of this expression.\n   *\n   * @return list of child expressions",
  "org.apache.hadoop.conf.StorageUnit:divide(double,double)" : "* Using BigDecimal to avoid issues with overflow and underflow.\n   *\n   * @param value - value\n   * @param divisor - divisor.\n   * @return -- returns a double that represents this value",
  "org.apache.hadoop.fs.FsShell:main(java.lang.String[])" : "* main() has some simple utility methods\n   * @param argv the command and its arguments\n   * @throws Exception upon error",
  "org.apache.hadoop.fs.viewfs.NflyFSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.hash.Hash:getHashType(org.apache.hadoop.conf.Configuration)" : "* This utility method converts the name of the configured\n   * hash type to a symbolic constant.\n   * @param conf configuration\n   * @return one of the predefined constants",
  "org.apache.hadoop.security.alias.KeyStoreProvider:keystoreExists()" : null,
  "org.apache.hadoop.fs.FileUtil:rename(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])" : null,
  "org.apache.hadoop.metrics2.lib.MutableGaugeLong:decr()" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getOptionalKeys()" : "* Get all the keys that are set as optional keys.\n   * @return optional keys.",
  "org.apache.hadoop.fs.shell.FsUsage$Df:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder)" : null,
  "org.apache.hadoop.ipc.Server:getQueueClassPrefix()" : null,
  "org.apache.hadoop.fs.Options$ChecksumOpt:toString()" : null,
  "org.apache.hadoop.fs.Path:<init>(java.net.URI)" : "* Construct a path from a URI\n   *\n   * @param aUri the source URI",
  "org.apache.hadoop.fs.VectoredReadUtils:sortRangeList(java.util.List)" : "* Sort the input ranges by offset; no validation is done.\n   * @param input input ranges.\n   * @return a new list of the ranges, sorted by offset.",
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:rollLogDir()" : "* Create a new directory based on the current interval and a new log file in\n   * that directory.\n   *\n   * @throws IOException thrown if an error occurs while creating the\n   * new directory or new log file",
  "org.apache.hadoop.ipc.Server:getRpcMetrics()" : "* Returns a handle to the rpcMetrics (required in tests)\n   * @return rpc metrics",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getIOStatistics()" : null,
  "org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.util.RunJar:skipUnjar()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:readOnlyMountTable(java.lang.String,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getMetadata(java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:deriveIV(byte[])" : "* Derive the initialization vector (IV) for the encryption key from the IV\n     * of the encrypted key. This derived IV is used with the encryption key to\n     * decrypt the encrypted key.\n     * <p>\n     * The alternative to this is using the same IV for both the encryption key\n     * and the encrypted key. Even a simple symmetric transformation like this\n     * improves security by avoiding IV re-use. IVs will also be fairly unique\n     * among different EEKs.\n     *\n     * @param encryptedKeyIV of the encrypted key (i.e. {@link\n     * #getEncryptedKeyIv()})\n     * @return IV for the encryption key",
  "org.apache.hadoop.conf.ConfigurationWithLogging:getInt(java.lang.String,int)" : "* See {@link Configuration#getInt(String, int)}.",
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getUnixGroups(java.lang.String)" : "* Get the current user's group list from Unix by running the command 'groups'\n   * NOTE. For non-existing user it will return EMPTY list.\n   *\n   * @param user get groups for this user\n   * @return the groups list that the <code>user</code> belongs to. The primary\n   *         group is returned first.\n   * @throws IOException if encounter any error when running the command",
  "org.apache.hadoop.util.StringUtils:humanReadableInt(long)" : "* Given an integer, return a string that is in an approximate, but human \n   * readable format. \n   * @param number the number to format\n   * @return a human readable form of the integer\n   *\n   * @deprecated use {@link TraditionalBinaryPrefix#long2String(long, String, int)}.",
  "org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec)" : null,
  "org.apache.hadoop.util.NativeCrc32:calculateChunkedSumsByteArray(int,int,byte[],int,byte[],int,int)" : null,
  "org.apache.hadoop.fs.shell.find.ExpressionFactory:getExpression(java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Get an instance of the requested expression\n   *\n   * @param expressionName\n   *          name of the command to lookup\n   * @param conf\n   *          the Hadoop configuration\n   * @return the {@link Expression} or null if the expression is unknown",
  "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)" : "* Copy files between FileSystems.\n   *\n   * @param srcFS srcFs.\n   * @param src src.\n   * @param dstFS dstFs.\n   * @param dst dst.\n   * @param deleteSource delete source.\n   * @param overwrite overwrite.\n   * @param conf configuration.\n   * @throws IOException raised on errors performing I/O.\n   * @return true if the operation succeeded.",
  "org.apache.hadoop.security.UserGroupInformation:isFromTicket()" : "*  Is this user logged in from a ticket (but no keytab) managed by the UGI?\n   * @return true if the credentials are from a ticket cache.",
  "org.apache.hadoop.util.Daemon:<init>(java.lang.ThreadGroup,java.lang.Runnable)" : "* Construct a daemon thread to be part of a specified thread group.\n   * @param group thread group.\n   * @param runnable runnable.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getOwner()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:publishAsStorageStatistics(java.lang.String,java.lang.String,org.apache.hadoop.fs.statistics.IOStatistics)" : "* Publish the IOStatistics as a set of storage statistics.\n   * This is dynamic.\n   * @param name storage statistics name.\n   * @param scheme FS scheme; may be null.\n   * @param source IOStatistics source.\n   * @return a dynamic storage statistics object.",
  "org.apache.hadoop.util.bloom.DynamicBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key)" : null,
  "org.apache.hadoop.util.Shell:joinThread(java.lang.Thread)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressorType(org.apache.hadoop.conf.Configuration)" : "* Return the appropriate type of the zlib decompressor. \n   * \n   * @param conf configuration\n   * @return the appropriate type of the zlib decompressor.",
  "org.apache.hadoop.service.CompositeService:serviceStart()" : null,
  "org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)" : "* Construct &amp; cache an IPC client with the user-provided SocketFactory\n   * if no cached client exists. Default response type is ObjectWritable.\n   * \n   * @param conf Configuration\n   * @param factory SocketFactory for client socket\n   * @return an IPC client",
  "org.apache.hadoop.net.InnerNodeImpl:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$BlockFactory:getConf()" : "* Configuration.\n     *\n     * @return config passed to create the factory.",
  "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:getGroups(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.find.FindOptions:setFollowArgLink(boolean)" : "* Sets flag indicating whether command line symbolic links should be\n   * followed.\n   *\n   * @param followArgLink true indicates follow links",
  "org.apache.hadoop.security.UserGroupInformation:getCurrentUser()" : "* Return the current user, including any doAs in the current stack.\n   * @return the current user\n   * @throws IOException if login fails",
  "org.apache.hadoop.ipc.Client$ConnectionId:getTcpNoDelay()" : "disable nagle's algorithm",
  "org.apache.hadoop.fs.FilterFs:removeAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState)" : null,
  "org.apache.hadoop.metrics2.lib.Interns:info(java.lang.String,java.lang.String)" : "* Get a metric info object.\n   * @param name Name of metric info object\n   * @param description Description of metric info object\n   * @return an interned metric info object",
  "org.apache.hadoop.fs.FilterFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.net.NodeBase:setNetworkLocation(java.lang.String)" : "Set this node's network location\n   * @param location the location",
  "org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:<init>(java.lang.String,java.lang.String,java.lang.String)" : "* Creates a reloadable trustmanager. The trustmanager reloads itself\n   * if the underlying trustore file has changed.\n   *\n   * @param type type of truststore file, typically 'jks'.\n   * @param location local path to the truststore file.\n   * @param password password of the truststore file.\n   * changed, in milliseconds.\n   * @throws IOException thrown if the truststore could not be initialized due\n   * to an IO error.\n   * @throws GeneralSecurityException thrown if the truststore could not be\n   * initialized due to a security error.",
  "org.apache.hadoop.fs.FilterFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:getPrecedence()" : null,
  "org.apache.hadoop.fs.GlobFilter$1:accept(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.CreateFlag:validate(java.util.EnumSet)" : "* Validate the CreateFlag and throw exception if it is invalid\n   * @param flag set of CreateFlag\n   * @throws HadoopIllegalArgumentException if the CreateFlag is invalid",
  "org.apache.hadoop.security.UserGroupInformation:isFromKeytab()" : "* Is this user logged in from a keytab file managed by the UGI?\n   * @return true if the credentials are from a keytab file.",
  "org.apache.hadoop.util.Progress:getProgress()" : "* Returns progress in this node. get() would give overall progress of the\n   * root node(not just given current node).\n   *\n   * @return progress.",
  "org.apache.hadoop.fs.shell.FsUsage:setHumanReadable(boolean)" : null,
  "org.apache.hadoop.util.CleanerUtil:<init>()" : null,
  "org.apache.hadoop.fs.Globber:doGlob()" : null,
  "org.apache.hadoop.security.SaslRpcClient:selectSaslClient(java.util.List)" : "* Instantiate a sasl client for the first supported auth type in the\n   * given list.  The auth type must be defined, enabled, and the user\n   * must possess the required credentials, else the next auth is tried.\n   * \n   * @param authTypes to attempt in the given order\n   * @return SaslAuth of instantiated client\n   * @throws AccessControlException - client doesn't support any of the auths\n   * @throws IOException - misc errors",
  "org.apache.hadoop.fs.permission.PermissionStatus:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.io.ObjectWritable:readObject(java.io.DataInput,org.apache.hadoop.conf.Configuration)" : "* Read a {@link Writable}, {@link String}, primitive type, or an array of\n   * the preceding.\n   *\n   * @param conf configuration.\n   * @param in DataInput.\n   * @return Object.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.UserGroupInformation:setLoginUser(org.apache.hadoop.security.UserGroupInformation)" : null,
  "org.apache.hadoop.fs.FileUtil:unTarUsingJava(java.io.File,java.io.File,boolean)" : null,
  "org.apache.hadoop.fs.shell.find.Find:isPathRecursable(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.viewfs.RegexMountPoint:getDstPath()" : null,
  "org.apache.hadoop.security.UserGroupInformation:getUserName()" : "* Get the user's full principal name.\n   * @return the user's full principal name.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:equals(java.lang.Object)" : "* Compare whether this and other points to the same key value.",
  "org.apache.hadoop.conf.StorageUnit$1:toBytes(double)" : null,
  "org.apache.hadoop.ipc.Server:addAuxiliaryListener(int)" : null,
  "org.apache.hadoop.fs.FileSystem:createMultipartUploader(org.apache.hadoop.fs.Path)" : "* Create a multipart uploader.\n   * @param basePath file path under which all files are uploaded\n   * @return a MultipartUploaderBuilder object to build the uploader\n   * @throws IOException if some early checks cause IO failures.\n   * @throws UnsupportedOperationException if support is checked early.",
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:init(org.apache.commons.configuration2.SubsetConfiguration)" : null,
  "org.apache.hadoop.metrics2.source.JvmMetrics:getGcInfo(java.lang.String)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferPool:close()" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:seek(long)" : null,
  "org.apache.hadoop.io.IntWritable$Comparator:compare(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.util.ComparableVersion$IntegerItem:toString()" : null,
  "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:becomeStandby()" : null,
  "org.apache.hadoop.fs.shell.Display$Text:getInputStream(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:<init>(java.lang.String,int,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback,int,boolean,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)" : "* Create a new ActiveStandbyElector object <br>\n   * The elector is created by providing to it the Zookeeper configuration, the\n   * parent znode under which to create the znode and a reference to the\n   * callback interface. <br>\n   * The parent znode name must be the same for all service instances and\n   * different across services. <br>\n   * After the leader has been lost, a new leader will be elected after the\n   * session timeout expires. Hence, the app must set this parameter based on\n   * its needs for failure response time. The session timeout must be greater\n   * than the Zookeeper disconnect timeout and is recommended to be 3X that\n   * value to enable Zookeeper to retry transient disconnections. Setting a very\n   * short session timeout may result in frequent transitions between active and\n   * standby states during issues like network outages/GS pauses.\n   * \n   * @param zookeeperHostPorts\n   *          ZooKeeper hostPort for all ZooKeeper servers\n   * @param zookeeperSessionTimeout\n   *          ZooKeeper session timeout\n   * @param parentZnodeName\n   *          znode under which to create the lock\n   * @param acl\n   *          ZooKeeper ACL's\n   * @param authInfo a list of authentication credentials to add to the\n   *                 ZK connection\n   * @param app\n   *          reference to callback interface object\n   * @param failFast\n   *          whether need to add the retry when establishing ZK connection.\n   * @param maxRetryNum max Retry Num\n   * @param truststoreKeystore truststore keystore, that we will use for ZK if SSL/TLS is enabled\n   * @throws IOException\n   *          raised on errors performing I/O.\n   * @throws HadoopIllegalArgumentException\n   *          if valid data is not supplied.\n   * @throws KeeperException\n   *          other zookeeper operation errors.",
  "org.apache.hadoop.metrics2.util.MetricsCache$RecordCache:removeEldestEntry(java.util.Map$Entry)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:snapshot()" : "* Returns a snapshot of the current thread's IOStatistics.\n   *\n   * @return IOStatisticsSnapshot of the context.",
  "org.apache.hadoop.fs.viewfs.RegexMountPoint:<init>(org.apache.hadoop.fs.viewfs.InodeTree,java.lang.String,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.util.InstrumentedLock:startLockTiming()" : "* Starts timing for the instrumented lock.",
  "org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.LocatedFileStatus:hashCode()" : "* Returns a hash code value for the object, which is defined as\n   * the hash code of the path name.\n   *\n   * @return  a hash code value for the path name.",
  "org.apache.hadoop.metrics2.util.Quantile:hashCode()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:remainder(java.nio.ByteBuffer[],int[])" : "* The \"bulk\" version of the remainder, using ByteBuffer.\n   * Warning: This function will modify the \"dividend\" inputs.\n   *\n   * @param dividend dividend.\n   * @param divisor divisor.",
  "org.apache.hadoop.io.SetFile:<init>()" : null,
  "org.apache.hadoop.security.SecurityUtil:setSslConfiguration(org.apache.zookeeper.client.ZKClientConfig,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)" : "* Configure ZooKeeper Client with SSL/TLS connection.\n   * @param zkClientConfig ZooKeeper Client configuration\n   * @param truststoreKeystore truststore keystore, that we use to set the SSL configurations\n   * @throws ConfigurationException if the SSL configs are empty",
  "org.apache.hadoop.tools.CommandShell:setErr(java.io.PrintStream)" : null,
  "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)" : null,
  "org.apache.hadoop.security.token.Token:getIdentifier()" : "* Get the token identifier's byte representation.\n   * @return the token identifier's byte representation",
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:setDictionary(byte[],int,int)" : "* Does nothing.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:getAggregator()" : "* Get the IOStatisticsAggregator of the context.\n   * @return the instance of IOStatisticsAggregator for this context.",
  "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)" : "* Get a protocol proxy that contains a proxy connection to a remote server\n   * and a set of methods that are supported by the server.\n   *\n   * @param protocol protocol\n   * @param clientVersion client's version\n   * @param addr server address\n   * @param ticket security ticket\n   * @param conf configuration\n   * @param factory socket factory\n   * @param rpcTimeout max time for each rpc; 0 means no timeout\n   * @param connectionRetryPolicy retry policy\n   * @param fallbackToSimpleAuth set to true or false during calls to indicate\n   *   if a secure client falls back to simple auth\n   * @param alignmentContext state alignment context\n   * @param <T> Generics Type T.\n   * @return the proxy\n   * @throws IOException if any error occurs",
  "org.apache.hadoop.http.HttpServer2Metrics:dispatchedTimeMax()" : null,
  "org.apache.hadoop.fs.FileSystem:listCorruptFileBlocks(org.apache.hadoop.fs.Path)" : "* List corrupted file blocks.\n   *\n   * @param path the path.\n   * @return an iterator over the corrupt files under the given path\n   * (may contain duplicates if a file has more than one corrupt block)\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).\n   * @throws IOException IO failure",
  "org.apache.hadoop.util.Lists:addAll(java.util.Collection,java.util.Iterator)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:isCompressed()" : null,
  "org.apache.hadoop.conf.ConfServlet:getConfFromContext()" : "* Return the Configuration of the daemon hosting this servlet.\n   * This is populated when the HttpServer starts.",
  "org.apache.hadoop.ipc.CallQueueManager:stringRepr(java.lang.Object)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:isViewFileSystem(org.apache.hadoop.fs.FileSystem)" : "* Check if the FileSystem is a ViewFileSystem.\n   *\n   * @param fileSystem file system.\n   * @return true if the fileSystem is ViewFileSystem",
  "org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],int,int,byte[],int,int)" : "* Adding a new key-value pair to TFile.\n     * \n     * @param key\n     *          buffer for key.\n     * @param koff\n     *          offset in key buffer.\n     * @param klen\n     *          length of key.\n     * @param value\n     *          buffer for value.\n     * @param voff\n     *          offset in value buffer.\n     * @param vlen\n     *          length of value.\n     * @throws IOException\n     *           Upon IO errors.\n     *           <p>\n     *           If an exception is thrown, the TFile will be in an inconsistent\n     *           state. The only legitimate call after that would be close",
  "org.apache.hadoop.fs.FileSystem$Statistics:getBytesRead()" : "* Get the total number of bytes read.\n     * @return the number of bytes",
  "org.apache.hadoop.util.ProgramDriver:printUsage(java.util.Map)" : null,
  "org.apache.hadoop.io.compress.DecompressorStream:available()" : null,
  "org.apache.hadoop.crypto.key.KeyProvider$Metadata:<init>(java.lang.String,int,java.lang.String,java.util.Map,java.util.Date,int)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:getKeyNear(long)" : "* Get a sample key that is within a block whose starting offset is greater\n     * than or equal to the specified offset.\n     * \n     * @param offset\n     *          The file offset.\n     * @return the key that fits the requirement; or null if no such key exists\n     *         (which could happen if the offset is close to the end of the\n     *         TFile).\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.retry.AsyncCallHandler$1:isDone()" : null,
  "org.apache.hadoop.ipc.Server$Connection:authorizeConnection()" : "* Authorize proxy users to access this server\n     * @throws RpcServerException - user is not allowed to proxy",
  "org.apache.hadoop.crypto.UnsupportedCodecException:<init>()" : "Default constructor",
  "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:getProxy()" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:getRequestHeader()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfoByNativeIO()" : null,
  "org.apache.hadoop.metrics2.impl.MetricGaugeFloat:visit(org.apache.hadoop.metrics2.MetricsVisitor)" : null,
  "org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.concurrent.CompletableFuture)" : "* Wait for a single of future to complete, extracting IOEs afterwards.\n   *\n   * @param <T> Generics Type T.\n   * @param future future to wait for.\n   * @throws IOException      if one of the called futures raised an IOE.\n   * @throws RuntimeException if one of the futures raised one.",
  "org.apache.hadoop.fs.shell.AclCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:build()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:getType()" : "* Returns the ViewFileSystem type.\n   *\n   * @return <code>viewfs</code>",
  "org.apache.hadoop.fs.Options$ChecksumOpt:getChecksumType()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:toString()" : null,
  "org.apache.hadoop.io.FloatWritable$Comparator:<init>()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockData:throwIfInvalidBlockNumber(int)" : null,
  "org.apache.hadoop.fs.permission.FsPermission:getGroupAction()" : "* @return Return group {@link FsAction}.",
  "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithProportionalSleep:<init>(int,long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfSupplier(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,java.util.function.Supplier)" : "* Given a Java supplier, evaluate it while\n   * tracking the duration of the operation and success/failure.\n   * @param factory factory of duration trackers\n   * @param statistic statistic key\n   * @param input input callable.\n   * @param <B> return type.\n   * @return the output of the supplier.",
  "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxySuperuserGroupConfKey(java.lang.String)" : "* Returns configuration key for effective groups allowed for a superuser\n   * \n   * @param userName name of the superuser\n   * @return configuration key for superuser groups",
  "org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:iterator()" : null,
  "org.apache.hadoop.io.SequenceFile$Writer$FileSystemOption:<init>(org.apache.hadoop.fs.FileSystem)" : null,
  "org.apache.hadoop.util.concurrent.HadoopExecutors:newScheduledThreadPool(int)" : null,
  "org.apache.hadoop.security.NullGroupsMapping:cacheGroupsAdd(java.util.List)" : "* Nothing is returned, so nothing is cached.\n   * @param groups ignored",
  "org.apache.hadoop.fs.AbstractFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : "* Get an xattr for a file or directory.\n   * The name must be prefixed with the namespace followed by \".\". For example,\n   * \"user.attr\".\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to get extended attribute\n   * @param name xattr name.\n   * @return byte[] xattr value.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.ZStandardCodec:createDirectDecompressor()" : null,
  "org.apache.hadoop.util.HostsFileReader:readXmlFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:addToCache(int)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:merge(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileStatus[])" : null,
  "org.apache.hadoop.io.SequenceFile$Writer$MetadataOption:getValue()" : null,
  "org.apache.hadoop.net.NetworkTopology:getRandom()" : null,
  "org.apache.hadoop.io.DataOutputOutputStream:<init>(java.io.DataOutput)" : null,
  "org.apache.hadoop.metrics2.lib.MutableGaugeInt:value()" : null,
  "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)" : "* Get a protocol proxy that contains a proxy connection to a remote server\n   * and a set of methods that are supported by the server.\n   *\n   * @param <T> Generics Type T\n   * @param protocol protocol class\n   * @param clientVersion client's version\n   * @param connId client connection identifier\n   * @param conf configuration\n   * @param factory socket factory\n   * @return the protocol proxy\n   * @throws IOException if the far end through a RemoteException",
  "org.apache.hadoop.fs.FilterFileSystem:appendFile(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:getMetric(java.lang.String)" : "* Get a metric by given directory name.\n   *\n   * @param dirName directory name\n   * @return the metric",
  "org.apache.hadoop.metrics2.lib.MutableCounterInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)" : null,
  "org.apache.hadoop.service.CompositeService:removeService(org.apache.hadoop.service.Service)" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,boolean)" : "* Convenience method, so that we don't open a new connection when using this\n   * method from within another method. Otherwise every API invocation incurs\n   * the overhead of opening/closing a TCP connection.",
  "org.apache.hadoop.ipc.Server:doKerberosRelogin()" : null,
  "org.apache.hadoop.metrics2.sink.StatsDSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getCurrentTokensSize()" : "* Total count of active delegation tokens.\n   *\n   * @return currentTokens.size.",
  "org.apache.hadoop.crypto.CryptoInputStream:readFully(long,byte[])" : null,
  "org.apache.hadoop.fs.FSDataOutputStream:close()" : "* Close the underlying output stream.",
  "org.apache.hadoop.fs.statistics.MeanStatistic:setSamplesAndSum(long,long)" : "* Set the sum and samples.\n   * Synchronized.\n   * @param sampleCount new sample count.\n   * @param newSum new sum",
  "org.apache.hadoop.util.hash.JenkinsHash:getInstance()" : null,
  "org.apache.hadoop.net.NetUtils:getSocketFactory(org.apache.hadoop.conf.Configuration,java.lang.Class)" : "* Get the socket factory for the given class according to its\n   * configuration parameter\n   * <tt>hadoop.rpc.socket.factory.class.&lt;ClassName&gt;</tt>. When no\n   * such parameter exists then fall back on the default socket factory as\n   * configured by <tt>hadoop.rpc.socket.factory.class.default</tt>. If\n   * this default socket factory is not configured, then fall back on the JVM\n   * default socket factory.\n   * \n   * @param conf the configuration\n   * @param clazz the class (usually a {@link VersionedProtocol})\n   * @return a socket factory",
  "org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:next()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.DataInputByteBuffer:reset(java.nio.ByteBuffer[])" : null,
  "org.apache.hadoop.util.bloom.BloomFilter:xor(org.apache.hadoop.util.bloom.Filter)" : null,
  "org.apache.hadoop.io.DataOutputBuffer:getData()" : "* Returns the current contents of the buffer.\n   *  Data is only valid to {@link #getLength()}.\n   *\n   * @return data byte.",
  "org.apache.hadoop.fs.store.DataBlocks$DataBlockByteArrayOutputStream:getInputStream()" : "* InputStream backed by the internal byte array.\n     *\n     * @return ByteArrayInputStream instance.",
  "org.apache.hadoop.fs.statistics.MeanStatistic:mean()" : "* Get the arithmetic mean value.\n   * @return the mean",
  "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:requestBuffer(int)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:mkdirs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:isNativeCodeLoaded()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:getFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:negate()" : "* Negate the values of all statistics.",
  "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:<init>(org.apache.hadoop.metrics2.MetricsInfo,float)" : null,
  "org.apache.hadoop.util.StringUtils:replaceTokens(java.lang.String,java.util.regex.Pattern,java.util.Map)" : "* Matches a template string against a pattern, replaces matched tokens with\n   * the supplied replacements, and returns the result.  The regular expression\n   * must use a capturing group.  The value of the first capturing group is used\n   * to look up the replacement.  If no replacement is found for the token, then\n   * it is replaced with the empty string.\n   * \n   * For example, assume template is \"%foo%_%bar%_%baz%\", pattern is \"%(.*?)%\",\n   * and replacements contains 2 entries, mapping \"foo\" to \"zoo\" and \"baz\" to\n   * \"zaz\".  The result returned would be \"zoo__zaz\".\n   * \n   * @param template String template to receive replacements\n   * @param pattern Pattern to match for identifying tokens, must use a capturing\n   *   group\n   * @param replacements Map&lt;String, String&gt; mapping tokens identified by\n   * the capturing group to their replacement values\n   * @return String template with replacements",
  "org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:<init>(java.lang.Class,java.lang.Object)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)" : null,
  "org.apache.hadoop.security.SaslRpcClient:getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)" : "* Get the remote server's principal.  The value will be obtained from\n   * the config and cross-checked against the server's advertised principal.\n   * \n   * @param authType of the SASL client\n   * @return String of the server's principal\n   * @throws IOException - error determining configured principal",
  "org.apache.hadoop.io.retry.CallReturn:<init>(org.apache.hadoop.io.retry.CallReturn$State)" : null,
  "org.apache.hadoop.util.JvmPauseMonitor$GcTimes:<init>(java.lang.management.GarbageCollectorMXBean)" : null,
  "org.apache.hadoop.fs.DelegationTokenRenewer:addRenewAction(org.apache.hadoop.fs.FileSystem)" : "* Add a renew action to the queue.\n   *\n   * @param <T> generic type T.\n   * @param fs file system.\n   * @return renew action.\n   *",
  "org.apache.hadoop.util.curator.ZKCuratorManager:safeSetData(java.lang.String,byte[],int,java.util.List,java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.ComparableVersion$IntegerItem:isNull()" : null,
  "org.apache.hadoop.util.InstrumentedLock:check(long,long,boolean)" : "* Log a warning if the lock was held for too long.\n   *\n   * Should be invoked by the caller immediately AFTER releasing the lock.\n   *\n   * @param acquireTime  - timestamp just after acquiring the lock.\n   * @param releaseTime - timestamp just before releasing the lock.\n   * @param checkLockHeld checkLockHeld.",
  "org.apache.hadoop.io.SecureIOUtils:forceSecureOpenFSDataInputStream(java.io.File,java.lang.String,java.lang.String)" : "* Same as openFSDataInputStream except that it will run even if security is\n   * off. This is used by unit tests.\n   *\n   * @param file input file.\n   * @param expectedOwner input expectedOwner.\n   * @param expectedGroup input expectedGroup.\n   * @throws IOException raised on errors performing I/O.\n   * @return FSDataInputStream.",
  "org.apache.hadoop.util.ZKUtil$BadAclFormatException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:putBuffer(java.nio.ByteBuffer)" : "* Return buffer to the pool.\n   * @param buffer buffer to be returned.",
  "org.apache.hadoop.io.WritableName:setName(java.lang.Class,java.lang.String)" : "* Set the name that a class should be known as to something other than the\n   * class name.\n   *\n   * @param writableClass input writableClass.\n   * @param name input name.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addTimedOperation(java.lang.String,java.time.Duration)" : null,
  "org.apache.hadoop.io.file.tfile.CompareUtils$ScalarLong:<init>(long)" : null,
  "org.apache.hadoop.util.FindClass:createClassInstance(java.lang.String)" : "* Create an instance of a class\n   * @param name classname\n   * @return the outcome",
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:getUnderlyingProxyObject()" : null,
  "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:getAppName()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:add(int[],int[])" : "* Compute the sum of two polynomials. The index in the array corresponds to\n   * the power of the entry. For example p[0] is the constant term of the\n   * polynomial p.\n   *\n   * @param p input polynomial\n   * @param q input polynomial\n   * @return polynomial represents p+q",
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:minimumFnRemove(int[])" : "* Chooses the bit position that minimizes the number of false negative generated.\n   * @param h The different bit positions.\n   * @return The position that minimizes the number of false negative generated.",
  "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getOptions()" : null,
  "org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration,long)" : null,
  "org.apache.hadoop.fs.FSBuilder:must(java.lang.String,int)" : "* Set mandatory int option.\n   *\n   * @param key key.\n   * @param value value.\n   * @return generic type B.\n   * @see #must(String, String)",
  "org.apache.hadoop.fs.store.DataBlocks:createFactory(java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String)" : "* Create a factory.\n   *\n   * @param keyToBufferDir Key to buffer directory config for a FS.\n   * @param configuration  factory configurations.\n   * @param name           factory name -the option from {@link CommonConfigurationKeys}.\n   * @return the factory, ready to be initialized.\n   * @throws IllegalArgumentException if the name is unknown.",
  "org.apache.hadoop.ipc.Server:internalQueueCall(org.apache.hadoop.ipc.Server$Call)" : null,
  "org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.hadoop.service.Service)" : "* Stop a service; if it is null do nothing. Exceptions are caught and\n   * logged at warn level. (but not Throwables). This operation is intended to\n   * be used in cleanup operations\n   *\n   * @param service a service; may be null\n   * @return any exception that was caught; null if none was.",
  "org.apache.hadoop.ipc.Server$Handler:run()" : null,
  "org.apache.hadoop.fs.audit.CommonAuditContext:remove(java.lang.String)" : "* Remove a context entry.\n   * @param key key",
  "org.apache.hadoop.util.functional.BiFunctionRaisingIOE:unchecked(java.lang.Object,java.lang.Object)" : "* Apply unchecked.\n   * @param t argument\n   * @param u argument 2\n   * @return the evaluated function\n   * @throws UncheckedIOException IOE raised.",
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:acceptableCountryWildcard(java.lang.String)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumParityUnits()" : null,
  "org.apache.hadoop.security.http.RestCsrfPreventionFilter$ServletFilterHttpInteraction:proceed()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)" : null,
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:createForDecryption(java.lang.String,java.lang.String,byte[],byte[])" : "* Factory method to create a new EncryptedKeyVersion that can then be\n     * passed into {@link #decryptEncryptedKey}. Note that the fields of the\n     * returned EncryptedKeyVersion will only partially be populated; it is not\n     * necessarily suitable for operations besides decryption.\n     *\n     * @param keyName Key name of the encryption key use to encrypt the\n     *                encrypted key.\n     * @param encryptionKeyVersionName Version name of the encryption key used\n     *                                 to encrypt the encrypted key.\n     * @param encryptedKeyIv           Initialization vector of the encrypted\n     *                                 key. The IV of the encryption key used to\n     *                                 encrypt the encrypted key is derived from\n     *                                 this IV.\n     * @param encryptedKeyMaterial     Key material of the encrypted key.\n     * @return EncryptedKeyVersion suitable for decryption.",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:end()" : null,
  "org.apache.hadoop.service.CompositeService:serviceInit(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:checkDeclaredComponentType(java.lang.Class)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:next()" : null,
  "org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget)" : null,
  "org.apache.hadoop.ipc.ProtocolSignature:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:valueOf(char)" : "* The TraditionalBinaryPrefix object corresponding to the symbol.\n     *\n     * @param symbol symbol.\n     * @return traditional binary prefix object.",
  "org.apache.hadoop.util.hash.Hash:hash(byte[])" : "* Calculate a hash using all bytes from the input argument, and\n   * a seed of -1.\n   * @param bytes input bytes\n   * @return hash value",
  "org.apache.hadoop.log.LogLevel:isValidProtocol(java.lang.String)" : null,
  "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:checkClientTrusted(java.security.cert.X509Certificate[],java.lang.String)" : null,
  "org.apache.hadoop.fs.FSOutputSummer:flush()" : "* Checksums all complete data chunks and flushes them to the underlying\n   * stream. If there is a trailing partial chunk, it is not flushed and is\n   * maintained in the buffer.",
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:end()" : null,
  "org.apache.hadoop.io.MD5Hash:<init>(byte[])" : "* Constructs an MD5Hash with a specified value.\n   * @param digest digest.",
  "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:getAndSetAccessed()" : null,
  "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:getCipherSuite()" : null,
  "org.apache.hadoop.fs.FsServerDefaults:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.util.RateLimitingFactory$NoRateLimiting:acquire(int)" : null,
  "org.apache.hadoop.fs.InternalOperations:rename(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])" : null,
  "org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroupsSet(java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.ByteArray:<init>(org.apache.hadoop.io.BytesWritable)" : "* Constructing a ByteArray from a {@link BytesWritable}.\n   * \n   * @param other other.",
  "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:close()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>(long,byte[])" : null,
  "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long)" : null,
  "org.apache.hadoop.util.concurrent.AsyncGetFuture:<init>(org.apache.hadoop.util.concurrent.AsyncGet)" : null,
  "org.apache.hadoop.fs.shell.Test:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.fs.DelegateToFileSystem:getFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileSystem:getCanonicalServiceName()" : "* Get a canonical service name for this FileSystem.\n   * The token cache is the only user of the canonical service name,\n   * and uses it to lookup this FileSystem's service tokens.\n   * If the file system provides a token of its own then it must have a\n   * canonical name, otherwise the canonical name can be null.\n   *\n   * Default implementation: If the FileSystem has child file systems\n   * (such as an embedded file system) then it is assumed that the FS has no\n   * tokens of its own and hence returns a null name; otherwise a service\n   * name is built using Uri and port.\n   *\n   * @return a service string that uniquely identifies this file system, null\n   *         if the filesystem does not implement tokens\n   * @see SecurityUtil#buildDTServiceName(URI, int)",
  "org.apache.hadoop.io.file.tfile.TFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.conf.Configuration)" : "* Constructor\n     * \n     * @param fsdis\n     *          FS input stream of the TFile.\n     * @param fileLength\n     *          The length of TFile. This is required because we have no easy\n     *          way of knowing the actual size of the input file through the\n     *          File input stream.\n     * @param conf configuration.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.impl.FlagSet:buildFlagSet(java.lang.Class,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)" : "* Build a FlagSet from a comma separated list of values.\n   * Case independent.\n   * Special handling of \"*\" meaning: all values.\n   * @param enumClass class of enum\n   * @param conf configuration\n   * @param key key to look for\n   * @param ignoreUnknown should unknown values raise an exception?\n   * @param <E> enumeration type\n   * @return a mutable FlagSet\n   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,\n   * or there are two entries in the enum which differ only by case.",
  "org.apache.hadoop.ipc.Server$Connection:processSaslMessage(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)" : "* Process a saslMessge.\n     * @param saslMessage received SASL message\n     * @return the sasl response to send back to client\n     * @throws SaslException if authentication or generating response fails, \n     *                       or SASL protocol mixup\n     * @throws IOException if a SaslServer cannot be created\n     * @throws AccessControlException if the requested authentication type \n     *         is not supported or trying to re-attempt negotiation.\n     * @throws InterruptedException",
  "org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getSchema(java.lang.Object)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:getReadOps()" : "* Get the number of file system read operations such as list files.\n     * @return number of read operations",
  "org.apache.hadoop.ha.ActiveStandbyElector:<init>(java.lang.String,int,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback,int,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)" : "* Create a new ActiveStandbyElector object <br>\n   * The elector is created by providing to it the Zookeeper configuration, the\n   * parent znode under which to create the znode and a reference to the\n   * callback interface. <br>\n   * The parent znode name must be the same for all service instances and\n   * different across services. <br>\n   * After the leader has been lost, a new leader will be elected after the\n   * session timeout expires. Hence, the app must set this parameter based on\n   * its needs for failure response time. The session timeout must be greater\n   * than the Zookeeper disconnect timeout and is recommended to be 3X that\n   * value to enable Zookeeper to retry transient disconnections. Setting a very\n   * short session timeout may result in frequent transitions between active and\n   * standby states during issues like network outages/GS pauses.\n   * \n   * @param zookeeperHostPorts\n   *          ZooKeeper hostPort for all ZooKeeper servers\n   * @param zookeeperSessionTimeout\n   *          ZooKeeper session timeout\n   * @param parentZnodeName\n   *          znode under which to create the lock\n   * @param acl\n   *          ZooKeeper ACL's\n   * @param authInfo a list of authentication credentials to add to the\n   *                 ZK connection\n   * @param app\n   *          reference to callback interface object\n   * @param maxRetryNum maxRetryNum.\n   * @param truststoreKeystore truststore keystore, that we will use for ZK if SSL/TLS is enabled\n   * @throws IOException raised on errors performing I/O.\n   * @throws HadoopIllegalArgumentException\n   *         if valid data is not supplied.\n   * @throws KeeperException\n   *         other zookeeper operation errors.",
  "org.apache.hadoop.ipc.CallQueueManager:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)" : null,
  "org.apache.hadoop.ipc.Server:getCallQueue()" : null,
  "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:close()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getLinkTarget(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FilterFs:delete(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:generateEncryptedKey(java.lang.String)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:getConf()" : "Returns the configuration of this file.",
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:writeRun()" : null,
  "org.apache.hadoop.crypto.OpensslCipher$Transform:<init>(java.lang.String,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getConf()" : null,
  "org.apache.hadoop.util.DataChecksum:newDataChecksum(java.io.DataInputStream)" : "* This constructs a DataChecksum by reading HEADER_LEN bytes from input\n   * stream <i>in</i>.\n   *\n   * @param in data input stream.\n   * @throws IOException raised on errors performing I/O.\n   * @return DataChecksum by reading HEADER_LEN\n   *         bytes from input stream.",
  "org.apache.hadoop.io.compress.GzipCodec:createDecompressor()" : null,
  "org.apache.hadoop.fs.Globber:schemeFromPath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.ipc.Server$Call:getClientStateId()" : null,
  "org.apache.hadoop.io.FloatWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.conf.StorageUnit$4:getLongName()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultReplication(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.erasurecode.ECSchema:extractIntOption(java.lang.String,java.util.Map)" : null,
  "org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.List)" : "* Wait for a list of futures to complete. If the list is empty,\n   * return immediately.\n   *\n   * @param futures list of futures.\n   * @param <T> Generics Type T.\n   * @throws IOException      if one of the called futures raised an IOE.\n   * @throws RuntimeException if one of the futures raised one.",
  "org.apache.hadoop.security.UserGroupInformation:ensureInitialized()" : "* A method to initialize the fields that depend on a configuration.\n   * Must be called before useKerberos or groups is used.",
  "org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,float)" : "* This parameter is converted to a long and passed\n   * to {@link #optLong(String, long)} -all\n   * decimal precision is lost.\n   *\n   * @param key key.\n   * @param value value.\n   * @return generic type B.\n   * @see #opt(String, String)\n   * @deprecated use {@link #optDouble(String, double)}",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:compareTo(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)" : "* @see java.lang.Comparable#compareTo(java.lang.Object)",
  "org.apache.hadoop.ipc.RetriableException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.security.token.Token:getRenewer()" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarMetaData:getPartFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.erasurecode.CodecUtil:hasCodec(java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:isManaged(org.apache.hadoop.security.token.Token)" : null,
  "org.apache.hadoop.fs.store.EtagChecksum:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:getFileSystem(org.apache.hadoop.fs.shell.PathData)" : "* Returns the {@link FileSystem} associated with the {@link PathData} item.\n   *\n   * @param item PathData\n   * @return FileSystem\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.conf.StorageUnit$2:getLongName()" : null,
  "org.apache.hadoop.conf.Configured:getConf()" : null,
  "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])" : "* Create a new Writer with the given options.\n   * @param conf the configuration to use\n   * @param opts the options to create the file with\n   * @return a new Writer\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer,boolean)" : "* Sort nodes array by network distance to <i>reader</i>.\n   * <p>\n   * As an additional twist, we also randomize the nodes at each network\n   * distance. This helps with load balancing when there is data skew.\n   * And it helps choose node with more fast storage type.\n   *\n   * @param reader    Node where data will be read\n   * @param nodes     Available replicas with the requested data\n   * @param activeLen Number of active nodes at the front of the array\n   * @param nonDataNodeReader True if the reader is not a datanode",
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm:returnCompressor(org.apache.hadoop.io.compress.Compressor)" : null,
  "org.apache.hadoop.util.SequentialNumber:setCurrentValue(long)" : "* Set current value.\n   * @param value value.",
  "org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)" : "Get a path from the local FS. Pass size as \n   *  SIZE_UNKNOWN if not known apriori. We\n   *  round-robin over the set of disks (via the configured dirs) and return\n   *  the first complete path which has enough space \n   *  @param pathStr the requested path (this will be created on the first \n   *  available disk)\n   *  @param size the size of the file that is going to be written\n   *  @param conf the Configuration object\n   *  @return the complete path to the file on a local disk\n   *  @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.file.tfile.TFile:getChunkBufferSize(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.net.unix.DomainSocketWatcher$Entry:<init>(org.apache.hadoop.net.unix.DomainSocket,org.apache.hadoop.net.unix.DomainSocketWatcher$Handler)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$9:<init>(org.apache.hadoop.fs.FileSystem$Statistics)" : "* Copy constructor.\n     *\n     * @param other    The input Statistics object which is cloned.",
  "org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,boolean)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKeyStream()" : "* Streaming access to the key. Useful for desrializing the key into\n         * user objects.\n         * \n         * @return The input stream.",
  "org.apache.hadoop.io.ArrayFile$Reader:get(long,org.apache.hadoop.io.Writable)" : "* Return the <code>n</code>th value in the file.\n     * @param n n key.\n     * @param value value.\n     * @throws IOException raised on errors performing I/O.\n     * @return writable.",
  "org.apache.hadoop.http.ProfileServlet:getAsyncProfilerHome()" : null,
  "org.apache.hadoop.fs.permission.FsCreateModes:hashCode()" : null,
  "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.lang.String,int)" : null,
  "org.apache.hadoop.fs.GlobPattern:matches(java.lang.CharSequence)" : "* Match input against the compiled glob pattern\n   * @param s input chars\n   * @return true for successful matches",
  "org.apache.hadoop.io.SequenceFile$Writer$SyncIntervalOption:<init>(int)" : null,
  "org.apache.hadoop.security.ssl.SSLFactory:getHostnameVerifier()" : "* Returns the hostname verifier it should be used in HttpsURLConnections.\n   *\n   * @return the hostname verifier.",
  "org.apache.hadoop.metrics2.util.SampleStat:reset()" : null,
  "org.apache.hadoop.fs.statistics.StreamStatisticNames:<init>()" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:setMinimum(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.viewfs.HCFSMountTableConfigLoader:load(java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Loads the mount-table configuration from hadoop compatible file system and\n   * add the configuration items to given configuration. Mount-table\n   * configuration format should be suffixed with version number.\n   * Format: {@literal mount-table.<versionNumber>.xml}\n   * Example: mount-table.1.xml\n   * When user wants to update mount-table, the expectation is to upload new\n   * mount-table configuration file with monotonically increasing integer as\n   * version number. This API loads the highest version number file. We can\n   * also configure single file path directly.\n   *\n   * @param mountTableConfigPath : A directory path where mount-table files\n   *          stored or a mount-table file path. We recommend to configure\n   *          directory with the mount-table version files.\n   * @param conf : to add the mount table as resource.",
  "org.apache.hadoop.util.CloseableReferenceCount:<init>()" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:setReadahead(java.lang.Long)" : null,
  "org.apache.hadoop.tools.CommandShell:getOut()" : null,
  "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$5:suffix()" : null,
  "org.apache.hadoop.fs.FileSystem:getLocal(org.apache.hadoop.conf.Configuration)" : "* Get the local FileSystem.\n   * @param conf the configuration to configure the FileSystem with\n   * if it is newly instantiated.\n   * @return a LocalFileSystem\n   * @throws IOException if somehow the local FS cannot be instantiated.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)" : "* Creates an <code>DelegationTokenAuthenticatedURL</code>.\n   *\n   * @param authenticator the {@link DelegationTokenAuthenticator} instance to\n   * use, if <code>null</code> the default one will be used.\n   * @param connConfigurator a connection configurator.",
  "org.apache.hadoop.ipc.RefreshRegistry:<init>()" : null,
  "org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:<init>()" : "* Constructor. The mapping is not ready to use until\n     * {@link #setConf(Configuration)} has been called",
  "org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:<init>(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod)" : null,
  "org.apache.hadoop.fs.FileSystem:createBulkDelete(org.apache.hadoop.fs.Path)" : "* Create a bulk delete operation.\n   * The default implementation returns an instance of {@link DefaultBulkDeleteOperation}.\n   * @param path base path for the operation.\n   * @return an instance of the bulk delete.\n   * @throws IllegalArgumentException any argument is invalid.\n   * @throws IOException if there is an IO problem.",
  "org.apache.hadoop.util.concurrent.ExecutorHelper:<init>()" : null,
  "org.apache.hadoop.conf.StorageUnit$7:getDefault(double)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getIOStatistics()" : "* Get the IO Statistics of the nested stream, falling back to\n     * null if the stream does not implement the interface\n     * {@link IOStatisticsSource}.\n     * @return an IOStatistics instance or null",
  "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:toString()" : null,
  "org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getInetAddressByName(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.find.FindOptions:getMaxDepth()" : "* Returns the maximum depth for applying expressions.\n   *\n   * @return maximum depth",
  "org.apache.hadoop.fs.FileContext:resolveIntermediate(org.apache.hadoop.fs.Path)" : "* Resolves all symbolic links in the specified path leading up \n   * to, but not including the final path component.\n   * @param f path to resolve\n   * @return the new path object.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.net.NetworkTopology:hasClusterEverBeenMultiRack()" : "* @return true if this cluster has ever consisted of multiple racks, even if\n   *         it is not now a multi-rack cluster.",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:streamCapabilities_hasCapability(java.lang.Object,java.lang.String)" : "* Does an object implement {@code StreamCapabilities} and, if so,\n   * what is the result of the probe for the capability?\n   * Calls {@code StreamCapabilities#hasCapability(String)},\n   * @param object object to probe\n   * @param capability capability string\n   * @return true iff the object implements StreamCapabilities and the capability is\n   * declared available.",
  "org.apache.hadoop.tools.GetGroupsBase:<init>(org.apache.hadoop.conf.Configuration,java.io.PrintStream)" : "* Used exclusively for testing.\n   * \n   * @param conf The configuration to use.\n   * @param out The PrintStream to write to, instead of System.out",
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getAccessTime()" : null,
  "org.apache.hadoop.io.nativeio.NativeIO:getOwner(java.io.FileDescriptor)" : null,
  "org.apache.hadoop.fs.FileSystem$Cache:closeAll(boolean)" : "* Close all FileSystem instances in the Cache.\n     * @param onlyAutomatic only close those that are marked for automatic closing\n     * @throws IOException a problem arose closing one or more FileSystem.",
  "org.apache.hadoop.fs.http.HttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.net.NetworkTopology:init(org.apache.hadoop.net.InnerNode$Factory)" : null,
  "org.apache.hadoop.fs.Options$OpenFileOptions:<init>()" : null,
  "org.apache.hadoop.metrics2.lib.MutableMetric:clearChanged()" : "* Clear the changed flag in the snapshot operations",
  "org.apache.hadoop.fs.FileEncryptionInfo:toStringStable()" : "* A frozen version of {@link #toString()} to be backward compatible.\n   * When backward compatibility is not needed, use {@link #toString()}, which\n   * provides more info and is supposed to evolve.\n   * Don't change this method except for major revisions.\n   *\n   * NOTE:\n   * Currently this method is used by CLI for backward compatibility.\n   *\n   * @return stable string.",
  "org.apache.hadoop.conf.Configuration:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)" : "* Set the value of <code>name</code> to the given time duration. This\n   * is equivalent to <code>set(&lt;name&gt;, value + &lt;time suffix&gt;)</code>.\n   * @param name Property name\n   * @param value Time duration\n   * @param unit Unit of time",
  "org.apache.hadoop.util.Sets:newConcurrentHashSet()" : "* Creates a thread-safe set backed by a hash map. The set is backed by a\n   * {@link ConcurrentHashMap} instance, and thus carries the same concurrency\n   * guarantees.\n   *\n   * <p>Unlike {@code HashSet}, this class does NOT allow {@code null} to be\n   * used as an element. The set is serializable.\n   *\n   * @param <E> Generics Type.\n   * @return a new, empty thread-safe {@code Set}",
  "org.apache.hadoop.fs.shell.find.Find:isStop(org.apache.hadoop.fs.shell.PathData)" : "Returns true if the {@link PathData} item is in the stop set.",
  "org.apache.hadoop.ipc.RetryCache:setState(org.apache.hadoop.ipc.RetryCache$CacheEntry,boolean)" : null,
  "org.apache.hadoop.io.LongWritable$DecreasingComparator:compare(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.WritableComparable)" : null,
  "org.apache.hadoop.security.SecurityUtil:login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)" : "* Login as a principal specified in config. Substitute $host in\n   * user's Kerberos principal name with a dynamically looked-up fully-qualified\n   * domain name of the current host.\n   * \n   * @param conf\n   *          conf to use\n   * @param keytabFileKey\n   *          the key to look for keytab file in conf\n   * @param userNameKey\n   *          the key to look for user's Kerberos principal name in conf\n   * @throws IOException if login fails",
  "org.apache.hadoop.fs.LocalFileSystemPathHandle:getPath()" : null,
  "org.apache.hadoop.net.unix.DomainSocketWatcher:isClosed()" : null,
  "org.apache.hadoop.security.UserGroupInformation:forceReloginFromTicketCache()" : "* Force re-Login a user in from the ticket cache irrespective of the last\n   * login time. This method assumes that login had happened already. The\n   * Subject field of this UserGroupInformation object is updated to have the\n   * new credentials.\n   *\n   * @throws IOException\n   *           raised on errors performing I/O.\n   * @throws KerberosAuthException\n   *           on a failure",
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:read(org.apache.hadoop.fs.impl.prefetch.BufferData)" : null,
  "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refreshWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatistics(java.lang.Object)" : "* Probe for an object being an instance of {@code IOStatisticsSource}.\n   * @param object object to probe\n   * @return true if the object is the right type, false if the classes\n   * were not found or the object is null/of a different type",
  "org.apache.hadoop.fs.shell.Command:processArguments(java.util.LinkedList)" : "*  Processes the command's list of expanded arguments.\n   *  {@link #processArgument(PathData)} will be invoked with each item\n   *  in the list.  The loop catches IOExceptions, increments the error\n   *  count, and displays the exception.\n   *  @param args a list of {@link PathData} to process\n   *  @throws IOException if anything goes wrong...",
  "org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl:isShadedPBImpl()" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : null,
  "org.apache.hadoop.fs.FileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])" : "* Set an xattr of a file or directory.\n   * The name must be prefixed with the namespace followed by \".\". For example,\n   * \"user.attr\".\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to modify\n   * @param name xattr name.\n   * @param value xattr value.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:getBytesWritten()" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_retrieve(java.lang.Object)" : "* Extract the IOStatistics from an object in a serializable form.\n   * @param source source object, may be null/not a statistics source/instance\n   * @return {@code IOStatisticsSnapshot} or null if the object is null/doesn't have statistics\n   * @throws UnsupportedOperationException if the IOStatistics classes were not found",
  "org.apache.hadoop.ipc.Server$RpcCall:setDeferredError(java.lang.Throwable)" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])" : null,
  "org.apache.hadoop.io.WritableFactories:getFactory(java.lang.Class)" : "* Define a factory for a class.\n   * @param c input c.\n   * @return a factory for a class.",
  "org.apache.hadoop.fs.FileUtil:getJarsInDirectory(java.lang.String)" : "* Returns all jars that are in the directory. It is useful in expanding a\n   * wildcard path to return all jars from the directory to use in a classpath.\n   * It operates only on local paths.\n   *\n   * @param path the path to the directory. The path may include the wildcard.\n   * @return the list of jars as URLs, or an empty list if there are no jars, or\n   * the directory does not exist locally",
  "org.apache.hadoop.ipc.Client:incCount()" : "* Increment this client's reference count",
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrSharedCount(org.apache.curator.framework.recipes.shared.SharedCount,int)" : null,
  "org.apache.hadoop.conf.Configuration:getRange(java.lang.String,java.lang.String)" : "* Parse the given attribute as a set of integer ranges.\n   * @param name the attribute name\n   * @param defaultValue the default value if it is not set\n   * @return a new set of ranges from the configured value",
  "org.apache.hadoop.ipc.RpcWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.security.SaslRpcClient:useWrap()" : null,
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read()" : null,
  "org.apache.hadoop.io.BoundedByteArrayOutputStream:write(byte[],int,int)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO:getMemlockLimit()" : "* Get the maximum number of bytes that can be locked into memory at any\n   * given point.\n   *\n   * @return 0 if no bytes can be locked into memory;\n   *         Long.MAX_VALUE if there is no limit;\n   *         The number of bytes that can be locked into memory otherwise.",
  "org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.Class,org.apache.hadoop.conf.Configuration)" : "* Creates an instance of the requested {@link Expression} class.\n   *\n   * @param expressionClass\n   *          {@link Expression} class to be instantiated\n   * @param conf\n   *          the Hadoop configuration\n   * @return a new instance of the requested {@link Expression} class",
  "org.apache.hadoop.fs.Path:toUri()" : "* Convert this Path to a URI.\n   *\n   * @return this Path as a URI",
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateInfoCache(java.lang.Iterable)" : null,
  "org.apache.hadoop.ipc.Server$RpcCall:sendDeferedResponse()" : "* Send a deferred response, ignoring errors.",
  "org.apache.hadoop.fs.GlobFilter:hasPattern()" : null,
  "org.apache.hadoop.fs.HarFileSystem:getFileHarStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.metrics2.util.MBeans:<init>()" : null,
  "org.apache.hadoop.fs.permission.UmaskParser:<init>(java.lang.String)" : null,
  "org.apache.hadoop.conf.Configuration:parse(java.net.URL,boolean)" : null,
  "org.apache.hadoop.crypto.key.UserProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)" : null,
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:aggregate(org.apache.hadoop.fs.statistics.IOStatistics)" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination:setVerifyChecksum(boolean)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read0()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)" : null,
  "org.apache.hadoop.fs.permission.RawParser:<init>(java.lang.String)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine$Server:processCall(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)" : "* This implementation is same as\n     * ProtobufRpcEngine2.Server.ProtobufInvoker#call(..)\n     * except this implementation uses non-shaded protobuf classes from legacy\n     * protobuf version (default 2.5.0).",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX:getCacheManipulator()" : null,
  "org.apache.hadoop.fs.shell.find.FindOptions:isFollowArgLink()" : "* Should command line symbolic links be follows?\n   *\n   * @return true indicates links should be followed",
  "org.apache.hadoop.util.functional.CommonCallableSupplier:submit(java.util.concurrent.Executor,java.util.concurrent.Callable)" : "* Submit a callable into a completable future.\n   * RTEs are rethrown.\n   * Non RTEs are caught and wrapped; IOExceptions to\n   * {@code RuntimeIOException} instances.\n   * @param executor executor.\n   * @param call     call to invoke\n   * @param <T>      type\n   * @return the future to wait for",
  "org.apache.hadoop.ipc.DefaultRpcScheduler:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.http.HttpServer2:loadListeners()" : null,
  "org.apache.hadoop.fs.FileSystem$Cache:remove(org.apache.hadoop.fs.FileSystem$Cache$Key,org.apache.hadoop.fs.FileSystem)" : null,
  "org.apache.hadoop.fs.shell.CopyCommands$Merge:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:<init>(long,long)" : "* Constructor.\n   * @param threadId thread ID\n   * @param id instance ID.",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getEncKeyQueueSize(java.lang.String)" : null,
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:randomRemove()" : null,
  "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:getWrapped()" : "* Get at the wrapped inner statistics.\n   * @return the wrapped value",
  "org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)" : "* Filter files/directories in the given path using the user-supplied path\n     * filter.\n     * \n     * @param f is the path name\n     * @param filter is the user-supplied path filter\n     *\n     * @return an array of FileStatus objects for the files under the given path\n     *         after applying the filter\n     *\n     * @throws AccessControlException If access is denied\n     * @throws FileNotFoundException If <code>f</code> does not exist\n     * @throws UnsupportedFileSystemException If file system for \n     *         <code>pathPattern</code> is not supported\n     * @throws IOException If an I/O error occurred\n     * \n     * Exceptions applicable to file systems accessed over RPC:\n     * @throws RpcClientException If an exception occurred in the RPC client\n     * @throws RpcServerException If an exception occurred in the RPC server\n     * @throws UnexpectedServerException If server implementation throws \n     *           undeclared exception to RPC server",
  "org.apache.hadoop.fs.shell.Ls:initialiseOrderComparator()" : "* Initialise the comparator to be used for sorting files. If multiple options\n   * are selected then the order is chosen in the following precedence: -\n   * Modification time (or access time if requested) - File size - File name",
  "org.apache.hadoop.ipc.Server$ConnectionManager:startIdleScan()" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:mustLong(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.permission.FsPermission:createImmutable(short)" : "* Create an immutable {@link FsPermission} object.\n   * @param permission permission.\n   * @return FsPermission.",
  "org.apache.hadoop.util.concurrent.HadoopExecutors:newFixedThreadPool(int)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)" : null,
  "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:getChecksumFilePos(long)" : null,
  "org.apache.hadoop.http.HttpServer2:addHandlerAtFront(org.eclipse.jetty.server.Handler)" : "* Add the given handler to the front of the list of handlers.\n   *\n   * @param handler The handler to add",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:needsInput()" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader,int)" : "* Creates a new decompressor.\n   * @param header header.\n   * @param directBufferSize directBufferSize.",
  "org.apache.hadoop.security.KDiag:failif(boolean,java.lang.String,java.lang.String,java.lang.Object[])" : "* Conditional failure with string formatted arguments.\n   * There is no chek for the {@link #nofail} value.\n   * @param condition failure condition\n   * @param category category for exception\n   * @param message string formatting message\n   * @param args any arguments for the formatting\n   * @throws KerberosDiagsFailure containing the formatted text\n   *         if the condition was met",
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkAppend(org.apache.hadoop.fs.FileSystem)" : "* Test whether the file system supports append and return the answer.\n   *\n   * @param fs the target file system",
  "org.apache.hadoop.fs.shell.Ls:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree:getLinkEntries(java.util.List)" : "* Get collection of linkEntry. Sort mount point based on alphabetical order of the src paths.\n   * The purpose is to group nested paths(shortest path always comes first) during INodeTree creation.\n   * E.g. /foo is nested with /foo/bar so an INodeDirLink will be created at /foo.\n   * @param linkEntries input linkEntries\n   * @return sorted linkEntries",
  "org.apache.hadoop.http.HttpServer2Metrics:dispatchedTimeStdDev()" : null,
  "org.apache.hadoop.io.SortedMapWritable:clear()" : null,
  "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:blockEvictedFromFileCache()" : null,
  "org.apache.hadoop.fs.FileStatus:setPath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:appendPrefix(org.apache.hadoop.metrics2.MetricsRecord,java.lang.StringBuilder)" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec:createInputStream(java.io.InputStream)" : null,
  "org.apache.hadoop.fs.FileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)" : "* Set an xattr of a file or directory.\n   * The name must be prefixed with the namespace followed by \".\". For example,\n   * \"user.attr\".\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to modify\n   * @param name xattr name.\n   * @param value xattr value.\n   * @param flag xattr set flag\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.util.WeakReferenceMap:toString()" : null,
  "org.apache.hadoop.fs.Trash:expungeImmediately()" : "* Delete all trash immediately.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:isManagementOperation(javax.servlet.http.HttpServletRequest)" : "* This method checks if the given HTTP request corresponds to a management\n   * operation.\n   *\n   * @param request The HTTP request\n   * @return true if the given HTTP request corresponds to a management\n   *         operation false otherwise\n   * @throws IOException In case of I/O error.",
  "org.apache.hadoop.fs.Options$CreateOpts:createParent()" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:cachedOrComputedPriorityLevel(java.lang.Object)" : "* Returns the priority level for a given identity by first trying the cache,\n   * then computing it.\n   * @param identity an object responding to toString and hashCode\n   * @return integer scheduling decision from 0 to numLevels - 1",
  "org.apache.hadoop.util.GcTimeMonitor$GcData:clone()" : null,
  "org.apache.hadoop.metrics2.util.SampleQuantiles:insert(long)" : "* Add a new value from the stream.\n   * \n   * @param v v.",
  "org.apache.hadoop.util.ProgramDriver$ProgramDescription:getDescription()" : null,
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$PrefetchTask:<init>(org.apache.hadoop.fs.impl.prefetch.BufferData,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager,java.time.Instant)" : null,
  "org.apache.hadoop.fs.BBPartHandle:<init>(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.ipc.Server$Responder:run()" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:close()" : null,
  "org.apache.hadoop.security.UserGroupInformation:getRealUserOrSelf(org.apache.hadoop.security.UserGroupInformation)" : "* If this is a proxy user, get the real user. Otherwise, return\n   * this user.\n   * @param user the user to check\n   * @return the real user or self",
  "org.apache.hadoop.io.Text:equals(java.lang.Object)" : "* Returns true iff <code>o</code> is a Text with the same length and same\n   * contents.",
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfMulTab()" : "* Get the big GF multiply table so utilize it efficiently.\n   * @return the big GF multiply table",
  "org.apache.hadoop.fs.shell.CopyCommands$Put:expandArgument(java.lang.String)" : null,
  "org.apache.hadoop.fs.impl.AbstractMultipartUploader:getBasePath()" : null,
  "org.apache.hadoop.security.token.DtUtilShell:maybeDoLoginFromKeytabAndPrincipal(java.lang.String[])" : "* Parse arguments looking for Kerberos keytab/principal.\n   * If both are found: remove both from the argument list and attempt login.\n   * If only one of the two is found: remove it from argument list, log warning\n   * and do not attempt login.\n   * If neither is found: return original args array, doing nothing.\n   * Return the pruned args array if either flag is present.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setMasterKeyId(int)" : null,
  "org.apache.hadoop.fs.StorageStatistics$LongStatistic:getName()" : "* @return    The name of this statistic.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:storeNewToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)" : null,
  "org.apache.hadoop.security.SaslOutputStream:write(byte[],int,int)" : "* Writes <code>len</code> bytes from the specified byte array starting at\n   * offset <code>off</code> to this output stream.\n   * \n   * @param inBuf\n   *          the data.\n   * @param off\n   *          the start offset in the data.\n   * @param len\n   *          the number of bytes to write.\n   * @exception IOException\n   *              if an I/O error occurs.",
  "org.apache.hadoop.ipc.Client$ConnectionId:getRpcTimeout()" : null,
  "org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text)" : "* Read from the InputStream into the given Text.\n   * @param str the object to store the given line\n   * @return the number of bytes read including the newline\n   * @throws IOException if the underlying stream throws",
  "org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable,boolean)" : "* Positions the reader at the named key, or if none such exists, at the\n     * key that falls just before or just after dependent on how the\n     * <code>before</code> parameter is set.\n     * \n     * @param before - IF true, and <code>key</code> does not exist, position\n     * file at entry that falls just before <code>key</code>.  Otherwise,\n     * position file at record that sorts just after.\n     * @return  0   - exact match found\n     *          < 0 - positioned at next record\n     *          1   - no more records in file",
  "org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:getInputStream()" : "* Get the output stream for BlockAppender's consumption.\n       * \n       * @return the output stream suitable for writing block data.",
  "org.apache.hadoop.util.Preconditions:<init>()" : null,
  "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)" : "Get a path from the local FS. If size is known, we go\n     *  round-robin over the set of disks (via the configured dirs) and return\n     *  the first complete path which has enough space.\n     *  \n     *  If size is not known, use roulette selection -- pick directories\n     *  with probability proportional to their available space.",
  "org.apache.hadoop.security.ShellBasedIdMapping:updateStaticMapping()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:allowChangeInputs()" : "* Allow change into input buffers or not while perform encoding/decoding.\n   * @return true if it's allowed to change inputs, false otherwise",
  "org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String,boolean,boolean)" : null,
  "org.apache.hadoop.io.UTF8:<init>()" : null,
  "org.apache.hadoop.ipc.Server$AuthProtocol:valueOf(int)" : null,
  "org.apache.hadoop.ipc.ResponseBuffer:writeTo(java.io.OutputStream)" : null,
  "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:remove()" : null,
  "org.apache.hadoop.util.StringUtils:stringToPath(java.lang.String[])" : "* stringToPath.\n   * @param str str.\n   * @return path array.",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:counters()" : null,
  "org.apache.hadoop.ipc.RpcServerException:getRpcStatusProto()" : "* @return get the rpc status corresponding to this exception.",
  "org.apache.hadoop.fs.GlobPattern:error(java.lang.String,java.lang.String,int)" : null,
  "org.apache.hadoop.util.StringUtils:stringifyException(java.lang.Throwable)" : "* Make a string representation of the exception.\n   * @param e The exception to stringify\n   * @return A string with exception name and call stack.",
  "org.apache.hadoop.fs.ftp.FTPInputStream:read(byte[],int,int)" : null,
  "org.apache.hadoop.fs.FilterFs:getEnclosingRoot(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.User:getLogin()" : "* Returns login object\n   * @return login",
  "org.apache.hadoop.util.ClassUtil:findClassLocation(java.lang.Class)" : "* Find the absolute location of the class.\n   *\n   * @param clazz the class to find.\n   * @return the class file with absolute location, or null.",
  "org.apache.hadoop.net.SocketIOWithTimeout:waitForIO(int)" : "* This is similar to {@link #doIO(ByteBuffer, int)} except that it\n   * does not perform any I/O. It just waits for the channel to be ready\n   * for I/O as specified in ops.\n   * \n   * @param ops Selection Ops used for waiting\n   * \n   * @throws SocketTimeoutException \n   *         if select on the channel times out.\n   * @throws IOException\n   *         if any other I/O error occurs.",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetUByte()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:add(int,int)" : "* Compute the sum of two fields\n   *\n   * @param x input field\n   * @param y input field\n   * @return result of addition",
  "org.apache.hadoop.ipc.Client:decAndGetCount()" : "* Decrement this client's reference count",
  "org.apache.hadoop.util.XMLUtils:newSecureSAXTransformerFactory()" : "* This method should be used if you need a {@link SAXTransformerFactory}. Use this method\n   * instead of {@link SAXTransformerFactory#newInstance()}. The factory that is returned has\n   * secure configuration enabled.\n   *\n   * @return a {@link SAXTransformerFactory} with secure configuration enabled\n   * @throws TransformerConfigurationException if the {@code JAXP} transformer does not\n   * support the secure configuration",
  "org.apache.hadoop.util.Shell:getWinUtilsFile()" : "* Get a file reference to winutils.\n   * Always raises an exception if there isn't one\n   * @return the file instance referring to the winutils bin.\n   * @throws FileNotFoundException on any failure to locate that file.",
  "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],long,long)" : "* Constructor with host, name, network topology, offset and length.\n   * @param names names.\n   * @param hosts hosts.\n   * @param topologyPaths topologyPaths.\n   * @param offset offset.\n   * @param length length.",
  "org.apache.hadoop.util.StringUtils:simpleHostname(java.lang.String)" : "* Given a full hostname, return the word upto the first dot.\n   * @param fullHostname the full hostname\n   * @return the hostname to the first dot",
  "org.apache.hadoop.fs.shell.Mkdir:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.permission.FsPermission:setUMask(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.permission.FsPermission)" : "* Set the user file creation mask (umask)\n   * @param conf configuration.\n   * @param umask umask.",
  "org.apache.hadoop.fs.FSError:<init>(java.lang.Throwable)" : null,
  "org.apache.hadoop.util.Lists:newArrayList(java.lang.Iterable)" : "* Creates a <i>mutable</i> {@code ArrayList} instance containing the\n   * given elements; a very thin shortcut for creating an empty list then\n   * calling Iterables#addAll.\n   *\n   * @param <E> Generics Type E.\n   * @param elements elements.\n   * @return ArrayList Generics Type E.",
  "org.apache.hadoop.security.UserGroupInformation:getUGIFromTicketCache(java.lang.String,java.lang.String)" : "* Create a UserGroupInformation from a Kerberos ticket cache.\n   * \n   * @param user                The principal name to load from the ticket\n   *                            cache\n   * @param ticketCache     the path to the ticket cache file\n   *\n   * @throws IOException        if the kerberos login fails\n   * @return UserGroupInformation.",
  "org.apache.hadoop.fs.FileContext:removeDefaultAcl(org.apache.hadoop.fs.Path)" : "* Removes all default ACL entries from files and directories.\n   *\n   * @param path Path to modify\n   * @throws IOException if an ACL could not be modified",
  "org.apache.hadoop.security.UserGroupInformation$RealUser:toString()" : null,
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$5:check(java.lang.String[],java.lang.String[],java.lang.String[])" : "* Checks to see if the supplied hostname matches any of the supplied CNs\n     * or \"DNS\" Subject-Alts.  Most implementations only look at the first CN,\n     * and ignore any additional CNs.  Most implementations do look at all of\n     * the \"DNS\" Subject-Alts. The CNs or Subject-Alts may contain wildcards\n     * according to RFC 2818.\n     *\n     * @param cns         CN fields, in order, as extracted from the X.509\n     *                    certificate.\n     * @param subjectAlts Subject-Alt fields of type 2 (\"DNS\"), as extracted\n     *                    from the X.509 certificate.\n     * @param hosts       The array of hostnames to verify.\n     * @throws SSLException If verification failed.",
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:initFileSystem(java.net.URI)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:toString()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)" : "* Obtain an output stream for creating a meta block. This function may not\n     * be called when there is a key append stream or value append stream\n     * active. No more key-value insertion is allowed after a meta data block\n     * has been added to TFile.\n     * \n     * @param name\n     *          Name of the meta block.\n     * @param compressName\n     *          Name of the compression algorithm to be used. Must be one of the\n     *          strings returned by\n     *          {@link TFile#getSupportedCompressionAlgorithms()}.\n     * @return A DataOutputStream that can be used to write Meta Block data.\n     *         Closing the stream would signal the ending of the block.\n     * @throws IOException raised on errors performing I/O.\n     * @throws MetaBlockAlreadyExists\n     *           the Meta Block with the same name already exists.",
  "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:<init>()" : "* Constructor. The mapping is not ready to use until\n     * {@link #setConf(Configuration)} has been called",
  "org.apache.hadoop.fs.statistics.StoreStatisticNames:<init>()" : null,
  "org.apache.hadoop.io.Text:copyBytes()" : "* @return Get a copy of the bytes that is exactly the length of the data.\n   * See {@link #getBytes()} for faster access to the underlying array.",
  "org.apache.hadoop.io.SequenceFile$Reader:openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int,long)" : "* Override this method to specialize the type of\n     * {@link FSDataInputStream} returned.\n     * @param fs The file system used to open the file.\n     * @param file The file being read.\n     * @param bufferSize The buffer size used to read the file.\n     * @param length The length being read if it is {@literal >=} 0.\n     *               Otherwise, the length is not available.\n     * @return The opened stream.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.ShortWritable$Comparator:compare(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.util.functional.FutureIO:raiseInnerCause(java.util.concurrent.ExecutionException)" : "* From the inner cause of an execution exception, extract the inner cause\n   * if it is an IOE or RTE.\n   * This will always raise an exception, either the inner IOException,\n   * an inner RuntimeException, or a new IOException wrapping the raised\n   * exception.\n   * @param e exception.\n   * @param <T> type of return value.\n   * @return nothing, ever.\n   * @throws IOException either the inner IOException, or a wrapper around\n   * any non-Runtime-Exception\n   * @throws RuntimeException if that is the inner cause.",
  "org.apache.hadoop.fs.permission.ScopedAclEntries:<init>(java.util.List)" : "* Creates a new ScopedAclEntries from the given list.  It is assumed that the\n   * list is already sorted such that all access entries precede all default\n   * entries.\n   *\n   * @param aclEntries List&lt;AclEntry&gt; to separate",
  "org.apache.hadoop.security.token.Token:<init>(org.apache.hadoop.security.token.Token)" : "* Clone a token.\n   * @param other the token to clone",
  "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:renewDelegationToken(org.apache.hadoop.security.token.Token)" : null,
  "org.apache.hadoop.ipc.Server$ConnectionManager:size()" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:needsInput()" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping:updateMapIncr(int,boolean)" : null,
  "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:init(byte[],byte[])" : null,
  "org.apache.hadoop.fs.HarFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(long,byte[],int,int)" : "* implementing position readable.",
  "org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:<init>(java.lang.String,int)" : null,
  "org.apache.hadoop.crypto.OpensslCipher$AlgMode:get(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredKeys()" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:hasNext()" : null,
  "org.apache.hadoop.ipc.Server$Call:<init>(org.apache.hadoop.ipc.Server$Call)" : null,
  "org.apache.hadoop.util.Progress:set(float)" : "* Called during execution on a leaf node to set its progress.\n   * @param progress progress.",
  "org.apache.hadoop.util.SysInfoLinux:getNetworkBytesRead()" : "{@inheritDoc}",
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues7(int)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:getChildren()" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeys()" : null,
  "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:getLong(java.lang.String)" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getDelegationTokenSeqNum()" : null,
  "org.apache.hadoop.io.MapFile$Writer:comparator(org.apache.hadoop.io.WritableComparator)" : null,
  "org.apache.hadoop.conf.Configuration:setDeprecatedProperties()" : "* Sets all deprecated properties that are not currently set but have a\n   * corresponding new property that is set. Useful for iterating the\n   * properties when all deprecated properties for currently set properties\n   * need to be present.",
  "org.apache.hadoop.io.WritableComparator:readFloat(byte[],int)" : "* Parse a float from a byte array.\n   * @param bytes bytes.\n   * @param start start.\n   * @return float from a byte array",
  "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean)" : "* Create an FSDataOutputStream at the indicated Path.\n   * @param f the file to create\n   * @param overwrite if a file with this name already exists, then if true,\n   *   the file will be overwritten, and if false an exception will be thrown.\n   * @throws IOException IO failure\n   * @return output stream.",
  "org.apache.hadoop.ipc.RetryCache:setState(org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload,boolean,java.lang.Object)" : null,
  "org.apache.hadoop.ipc.Server:setSocketSendBufSize(int)" : "* Sets the socket buffer size used for responding to RPCs.\n   * @param size input size.",
  "org.apache.hadoop.util.Shell:getProcess()" : "get the current sub-process executing the given command.\n   * @return process executing the command",
  "org.apache.hadoop.fs.BlockLocation:setStorageIds(java.lang.String[])" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.StringUtils:formatTime(long)" : "* \n   * Given the time in long milliseconds, returns a \n   * String in the format Xhrs, Ymins, Z sec. \n   * \n   * @param timeDiff The time difference to format\n   * @return formatTime String.",
  "org.apache.hadoop.util.InstrumentedLock:tryLock(long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.io.VLongWritable:<init>()" : null,
  "org.apache.hadoop.fs.LocalDirAllocator:getCurrentDirectoryIndex()" : "* Get the current directory index for the given configuration item.\n   * @return the current directory index for the given configuration item.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:<init>(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.FSInputChecker:skip(long)" : "* Skips over and discards <code>n</code> bytes of data from the\n   * input stream.\n   *\n   * <p>This method may skip more bytes than are remaining in the backing\n   * file. This produces no exception and the number of bytes skipped\n   * may include some number of bytes that were beyond the EOF of the\n   * backing file. Attempting to read from the stream after skipping past\n   * the end will result in -1 indicating the end of the file.\n   *\n   *<p>If <code>n</code> is negative, no bytes are skipped.\n   *\n   * @param      n   the number of bytes to be skipped.\n   * @return     the actual number of bytes skipped.\n   * @exception  IOException  if an I/O error occurs.\n   *             ChecksumException if the chunk to skip to is corrupted",
  "org.apache.hadoop.ipc.RPC:stopProxy(java.lang.Object)" : "* Stop the proxy. Proxy must either implement {@link Closeable} or must have\n   * associated {@link RpcInvocationHandler}.\n   * \n   * @param proxy\n   *          the RPC proxy object to be stopped\n   * @throws HadoopIllegalArgumentException\n   *           if the proxy does not implement {@link Closeable} interface or\n   *           does not have closeable {@link InvocationHandler}",
  "org.apache.hadoop.security.SaslPlainServer$SaslPlainServerFactory:createSaslServer(java.lang.String,java.lang.String,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler)" : null,
  "org.apache.hadoop.service.LoggingStateChangeListener:<init>()" : "* Log events to the static log for this class",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDelegationToken(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:<init>(int)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.util.ExitUtil:getFirstHaltException()" : "* @return the first {@code HaltException} thrown, null if none thrown yet.",
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,double)" : "* Set optional double parameter for the Builder.\n   *\n   * @see #opt(String, String)",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStore:incrementCounter(java.lang.String)" : "* Increment a counter by one.\n   *\n   * No-op if the counter is unknown.\n   * @param key statistics key\n   * @return old value or, if the counter is unknown: 0",
  "org.apache.hadoop.fs.FileUtil:unZip(java.io.File,java.io.File)" : "* Given a File input it will unzip it in the unzip directory.\n   * passed as the second parameter\n   * @param inFile The zip file as input\n   * @param unzipDir The unzip directory where to unzip the zip file.\n   * @throws IOException An I/O exception has occurred",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String,java.lang.String)" : "* Requests a delegation token using the configured <code>Authenticator</code>\n   * for authentication.\n   *\n   * @param url the URL to get the delegation token from. Only HTTP/S URLs are\n   * supported.\n   * @param token the authentication token being used for the user where the\n   * Delegation token will be stored.\n   * @param renewer the renewer user.\n   * @param doAsUser the user to do as, which will be the token owner.\n   * @return a delegation token.\n   * @throws IOException if an IO error occurred.\n   * @throws AuthenticationException if an authentication exception occurred.",
  "org.apache.hadoop.fs.permission.FsPermission:getStickyBit()" : null,
  "org.apache.hadoop.io.IntWritable:<init>(int)" : null,
  "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:getLogger()" : null,
  "org.apache.hadoop.io.erasurecode.ECChunk:<init>(java.nio.ByteBuffer,int,int)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* The src file is under FS, and the dst is on the local disk.\n   * Copy it from FS control to the local dst name.",
  "org.apache.hadoop.util.DiskChecker:checkDir(java.io.File)" : "* Create the directory if it doesn't exist and check that dir is readable,\n   * writable and executable\n   *  \n   * @param dir dir.\n   * @throws DiskErrorException disk problem.",
  "org.apache.hadoop.fs.shell.PathData:hashCode()" : null,
  "org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,java.net.SocketAddress,int)" : "* Like {@link NetUtils#connect(Socket, SocketAddress, int)} but\n   * also takes a local address and port to bind the socket to. \n   * \n   * @param socket socket.\n   * @param endpoint the remote address\n   * @param localAddr the local address to bind the socket to\n   * @param timeout timeout in milliseconds\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.crypto.CryptoStreamUtils:checkCodec(org.apache.hadoop.crypto.CryptoCodec)" : "* AES/CTR/NoPadding or SM4/CTR/NoPadding is required.\n   *\n   * @param codec crypto codec.",
  "org.apache.hadoop.ipc.RetryCache:lock()" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:close()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:isEqual(java.lang.Object,java.lang.Object)" : null,
  "org.apache.hadoop.ipc.ProtocolProxy:isMethodSupported(java.lang.String,java.lang.Class[])" : "* Check if a method is supported by the server or not.\n   * \n   * @param methodName a method's name in String format\n   * @param parameterTypes a method's parameter types\n   * @return true if the method is supported by the server\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.MapFile$Writer:progressable(org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.fs.DU:refresh()" : null,
  "org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)" : null,
  "org.apache.hadoop.io.WritableComparator:readDouble(byte[],int)" : "* Parse a double from a byte array.\n   * @param bytes bytes.\n   * @param start start.\n   * @return double from a byte array.",
  "org.apache.hadoop.net.SocketOutputStream:isOpen()" : null,
  "org.apache.hadoop.io.ObjectWritable:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.crypto.key.KeyProviderExtension:getMetadata(java.lang.String)" : null,
  "org.apache.hadoop.fs.FileContext:checkDest(java.lang.String,org.apache.hadoop.fs.Path,boolean)" : "* Check if copying srcName to dst would overwrite an existing \n   * file or directory.\n   * @param srcName File or directory to be copied.\n   * @param dst Destination to copy srcName to.\n   * @param overwrite Whether it's ok to overwrite an existing file. \n   * @throws AccessControlException If access is denied.\n   * @throws IOException If dst is an existing directory, or dst is an \n   * existing file and the overwrite option is not passed.",
  "org.apache.hadoop.util.SysInfoLinux:getNumProcessors()" : "{@inheritDoc}",
  "org.apache.hadoop.metrics2.util.MetricsCache:<init>()" : null,
  "org.apache.hadoop.ipc.Client$Connection:close()" : "Close the connection.",
  "org.apache.hadoop.ipc.ClientCache:stopClient(org.apache.hadoop.ipc.Client)" : "* Stop a RPC client connection \n   * A RPC client is closed only when its reference count becomes zero.\n   *\n   * @param client input client.",
  "org.apache.hadoop.security.SaslRpcServer:<init>(org.apache.hadoop.security.SaslRpcServer$AuthMethod)" : null,
  "org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path)" : "* Opens an FSDataInputStream at the indicated Path using\n   * default buffersize.\n   * @param f the file name to open\n   *\n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If file <code>f</code> does not exist\n   * @throws UnsupportedFileSystemException If file system for <code>f</code>\n   *         is not supported\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server\n   * @return input stream.",
  "org.apache.hadoop.fs.ChecksumFileSystem:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,int[],boolean,org.apache.hadoop.conf.Configuration)" : "* Create a FairCallQueue.\n   * @param priorityLevels the total size of all multi-level queue\n   *                       priority policies\n   * @param capacity the total size of all sub-queues\n   * @param ns the prefix to use for configuration\n   * @param capacityWeights the weights array for capacity allocation\n   *                        among subqueues\n   * @param serverFailOverEnabled whether or not to enable callqueue overflow trigger failover\n   *                              for stateless servers when RPC call queue is filled\n   * @param conf the configuration to read from\n   * Notes: Each sub-queue has a capacity of `capacity / numSubqueues`.\n   * The first or the highest priority sub-queue has an excess capacity\n   * of `capacity % numSubqueues`",
  "org.apache.hadoop.util.functional.RemoteIterators:remoteIteratorFromSingleton(java.lang.Object)" : "* Create an iterator from a singleton.\n   * @param singleton instance\n   * @param <T> type\n   * @return a remote iterator",
  "org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:createDecoder()" : null,
  "org.apache.hadoop.io.compress.CompressionOutputStream:<init>(java.io.OutputStream)" : "* Create a compression output stream that writes\n   * the compressed bytes to the given stream.\n   * @param out out.",
  "org.apache.hadoop.ipc.DecayRpcScheduler:computePriorityLevel(long,java.lang.Object)" : "* Given the cost for an identity, compute a scheduling decision.\n   *\n   * @param cost the cost for an identity\n   * @param identity the identity of the user\n   * @return scheduling decision from 0 to numLevels - 1",
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:setDictionary(byte[],int,int)" : null,
  "org.apache.hadoop.net.NodeBase:locationToDepth(java.lang.String)" : null,
  "org.apache.hadoop.fs.FsShell:createOptionTableListing()" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod$1:asStatic()" : "* Returns this method as a StaticMethod.\n     * @return a {@link StaticMethod} for this method\n     * @throws IllegalStateException if the method is not static",
  "org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)" : "* Add a named token to this UGI\n   * \n   * @param alias Name of the token\n   * @param token Token to be added\n   * @return true on successful add of new token",
  "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:encodeData(byte[],int,byte[][],int[],byte[][],int[])" : "* Encode a group of inputs data and generate the outputs. It's also used for\n   * decoding because, in this implementation, encoding and decoding are\n   * unified.\n   *\n   * The algorithm is ported from Intel ISA-L library for compatible. It\n   * leverages Java auto-vectorization support for performance.\n   *\n   * @param gfTables gfTables.\n   * @param dataLen dataLen.\n   * @param inputs inputs.\n   * @param inputOffsets inputOffsets.\n   * @param outputs outputs.\n   * @param outputOffsets outputOffsets.",
  "org.apache.hadoop.io.erasurecode.CodecUtil:getCodecClassName(org.apache.hadoop.conf.Configuration,java.lang.String)" : null,
  "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:skip(long)" : "* Skips over and discards <code>n</code> bytes of data from the\n     * input stream.\n     *\n     * The <code>skip</code> method skips over some smaller number of bytes\n     * when reaching end of file before <code>n</code> bytes have been skipped.\n     * The actual number of bytes skipped is returned.  If <code>n</code> is\n     * negative, no bytes are skipped.\n     *\n     * @param      n   the number of bytes to be skipped.\n     * @return     the actual number of bytes skipped.\n     * @exception  IOException  if an I/O error occurs.\n     *             ChecksumException if the chunk to skip to is corrupted",
  "org.apache.hadoop.ha.SshFenceByTcpPort:cleanup(com.jcraft.jsch.ChannelExec)" : null,
  "org.apache.hadoop.fs.impl.FSBuilderSupport:getPositiveLong(java.lang.String,long)" : "* Get a long value with resilience to unparseable values.\n   * Negative values are replaced with the default.\n   * @param key key to log\n   * @param defVal default value\n   * @return long value",
  "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:isDone()" : "@return true if the call is done; otherwise, return false.",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:renew(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:setInput(byte[],int,int)" : null,
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor$CallableWithPermitRelease:call()" : null,
  "org.apache.hadoop.io.WritableUtils:displayByteArray(byte[])" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Compressor:needsInput()" : "* Returns true if the input data buffer is empty and\n   * #setInput() should be called to provide more input.\n   *\n   * @return <code>true</code> if the input data buffer is empty and\n   *         #setInput() should be called in order to provide more input.",
  "org.apache.hadoop.fs.FilterFs:listLocatedStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:getRecords()" : null,
  "org.apache.hadoop.service.LoggingStateChangeListener:stateChanged(org.apache.hadoop.service.Service)" : "* Callback for a state change event: log it\n   * @param service the service that has changed.",
  "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:loadFrom(java.nio.file.Path)" : null,
  "org.apache.hadoop.fs.LocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.metrics2.util.SampleQuantiles:allowableError(int)" : "* Specifies the allowable error for this rank, depending on which quantiles\n   * are being targeted.\n   * \n   * This is the f(r_i, n) function from the CKMS paper. It's basically how wide\n   * the range of this rank can be.\n   * \n   * @param rank\n   *          the index in the list of samples",
  "org.apache.hadoop.fs.store.ByteBufferInputStream:isOpen()" : "* Is the stream open?\n   * @return true if the stream has not been closed.",
  "org.apache.hadoop.security.alias.LocalKeyStoreProvider:getInputStreamForFile()" : null,
  "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)" : "* Create a MD5FileChecksum.\n   *\n   * @param bytesPerCRC bytesPerCRC.\n   * @param crcPerBlock crcPerBlock.\n   * @param md5 md5.",
  "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getLocalDirAllocator()" : "* @return The local dir allocator instance.",
  "org.apache.hadoop.io.MD5Hash:hashCode()" : "Returns a hash code value for this object.\n   * Only uses the first 4 bytes, since md5s are evenly distributed.",
  "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool$SelectorInfo:<init>(java.nio.channels.spi.SelectorProvider,java.nio.channels.Selector)" : null,
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getCandidateTokensForCleanup()" : "* Obtain a list of tokens that will be considered for cleanup, based on the last\n   * time the token was updated in SQL. This list may include tokens that are not\n   * expired and should not be deleted (e.g. if the token was last renewed using a\n   * higher renewal interval).\n   * The number of results is limited to reduce performance impact. Some level of\n   * contention is expected when multiple routers run cleanup simultaneously.\n   * @return Map of tokens that have not been updated in SQL after the token renewal\n   *         period.",
  "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyStore(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(int,int,int)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:getHomeDirectory()" : null,
  "org.apache.hadoop.ipc.Server$Listener$Reader:doRunLoop()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:stripOutRoot(org.apache.hadoop.fs.Path)" : "*  \n   * Strip out the root from the path.\n   * \n   * @param p - fully qualified path p\n   * @return -  the remaining path  without the beginning /",
  "org.apache.hadoop.net.AbstractDNSToSwitchMapping:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getReplication()" : null,
  "org.apache.hadoop.fs.permission.AclStatus:hashCode()" : null,
  "org.apache.hadoop.crypto.CryptoStreamUtils:freeDB(java.nio.ByteBuffer)" : "* Forcibly free the direct buffer.\n   *\n   * @param buffer buffer.",
  "org.apache.hadoop.util.ProtoUtil:getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto)" : null,
  "org.apache.hadoop.ha.SshFenceByTcpPort:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,boolean,java.lang.Class,java.lang.Class,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,int)" : "Initialize.",
  "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:setMetricFilter(org.apache.hadoop.metrics2.MetricsFilter)" : null,
  "org.apache.hadoop.util.Shell:appendScriptExtension(java.lang.String)" : "* Returns a script file name with the given basename.\n   *\n   * The file extension is inferred by platform:\n   * <code>\".cmd\"</code> on Windows, or <code>\".sh\"</code> otherwise.\n   *\n   * @param basename String script file basename\n   * @return String script file name",
  "org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String[])" : "* Adds the deprecated key to the global deprecation map when no custom\n   * message is provided.\n   * It does not override any existing entries in the deprecation map.\n   * This is to be used only by the developers in order to add deprecation of\n   * keys, and attempts to call this method after loading resources once,\n   * would lead to <tt>UnsupportedOperationException</tt>\n   * \n   * If a key is deprecated in favor of multiple keys, they are all treated as \n   * aliases of each other, and setting any one of them resets all the others \n   * to the new value.\n   * \n   * If you have multiple deprecation entries to add, it is more efficient to\n   * use #addDeprecations(DeprecationDelta[] deltas) instead.\n   *\n   * @param key Key that is to be deprecated\n   * @param newKeys list of keys that take up the values of deprecated key\n   * @deprecated use {@link #addDeprecation(String key, String newKey)} instead",
  "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool$SelectorInfo:close()" : null,
  "org.apache.hadoop.fs.shell.Ls:<init>()" : null,
  "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:unpack(org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto)" : null,
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:internalReset()" : null,
  "org.apache.hadoop.http.HttpServer2:bindListener(org.eclipse.jetty.server.ServerConnector)" : "* Bind listener by closing and opening the listener.\n   * @param listener\n   * @throws Exception",
  "org.apache.hadoop.util.ComparableVersion:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)" : null,
  "org.apache.hadoop.ipc.Server:getRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind)" : null,
  "org.apache.hadoop.fs.impl.WeakReferenceThreadMap:<init>(java.util.function.Function,java.util.function.Consumer)" : null,
  "org.apache.hadoop.util.ShutdownHookManager:addShutdownHook(java.lang.Runnable,int,long,java.util.concurrent.TimeUnit)" : "*\n   * Adds a shutdownHook with a priority and timeout the higher the priority\n   * the earlier will run. ShutdownHooks with same priority run\n   * in a non-deterministic order. The shutdown hook will be terminated if it\n   * has not been finished in the specified period of time.\n   *\n   * @param shutdownHook shutdownHook <code>Runnable</code>\n   * @param priority priority of the shutdownHook\n   * @param timeout timeout of the shutdownHook\n   * @param unit unit of the timeout <code>TimeUnit</code>",
  "org.apache.hadoop.util.bloom.Filter:<init>(int,int,int)" : "* Constructor.\n   * @param vectorSize The vector size of <i>this</i> filter.\n   * @param nbHash The number of hash functions to consider.\n   * @param hashType type of the hashing function (see {@link Hash}).",
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:calculateSlope(org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)" : null,
  "org.apache.hadoop.metrics2.MetricsRecordBuilder:endRecord()" : "* Syntactic sugar to add multiple records in a collector in a one liner.\n   * @return the parent metrics collector object",
  "org.apache.hadoop.fs.FileSystem$Statistics:getScheme()" : "* Get the uri scheme associated with this statistics object.\n     * @return the schema associated with this set of statistics",
  "org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:toString()" : null,
  "org.apache.hadoop.security.UserGroupInformation$TestingGroups:setUserGroups(java.lang.String,java.lang.String[])" : null,
  "org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream)" : null,
  "org.apache.hadoop.ipc.RPC$Builder:setBindAddress(java.lang.String)" : "* @return Default: 0.0.0.0.\n     * @param bindAddress input bindAddress.",
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path)" : "* {@inheritDoc}\n     *\n     * Note: listStatus on root(\"/\") considers listing from fallbackLink if\n     * available. If the same directory name is present in configured mount\n     * path as well as in fallback link, then only the configured mount path\n     * will be listed in the returned result.",
  "org.apache.hadoop.net.InnerNodeImpl$Factory:newInnerNode(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.util.SampleQuantiles:clear()" : "* Resets the estimator, clearing out all previously inserted items",
  "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsAdd(java.util.List)" : "* Add a group to cache, only netgroups are cached\n   *\n   * @param groups list of group names to add to cache",
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:start()" : null,
  "org.apache.hadoop.io.DataInputByteBuffer$Buffer:getPosition()" : null,
  "org.apache.hadoop.net.NetworkTopology:normalizeNetworkLocationPath(java.lang.String)" : "Normalize a path by stripping off any trailing {@link #PATH_SEPARATOR}.\n   * @param path path to normalize.\n   * @return the normalised path\n   * If <i>path</i>is null or empty {@link #ROOT} is returned\n   * @throws IllegalArgumentException if the first character of a non empty path\n   * is not {@link #PATH_SEPARATOR}",
  "org.apache.hadoop.io.EnumSetWritable$1:<init>()" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesReadErasureCoded()" : null,
  "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:getLogger()" : null,
  "org.apache.hadoop.fs.DF:main(java.lang.String[])" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:preserveInput(boolean)" : "* Whether to delete the files when no longer needed.\n       * @param preserve input boolean preserve.",
  "org.apache.hadoop.ipc.Server$Connection:doSaslReply(java.lang.Exception)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:name()" : null,
  "org.apache.hadoop.fs.permission.RawParser:getPermission()" : null,
  "org.apache.hadoop.io.MD5Hash:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.io.compress.Lz4Codec:getCompressorType()" : "* Get the type of {@link Compressor} needed by this {@link CompressionCodec}.\n   *\n   * @return the type of compressor needed by this codec.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setRealUser(org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.crypto.CryptoOutputStream:updateEncryptor()" : "Update the {@link #encryptor}: calculate counter and {@link #padding}.",
  "org.apache.hadoop.security.ShellBasedIdMapping$PassThroughMap:get(java.lang.Object)" : null,
  "org.apache.hadoop.util.HostsFileReader:getHostDetails(java.util.Set,java.util.Set)" : "* Retrieve an atomic view of the included and excluded hosts.\n   *\n   * @param includes set to populate with included hosts\n   * @param excludes set to populate with excluded hosts\n   * @deprecated use {@link #getHostDetails() instead}",
  "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)" : "* Get a protocol proxy that contains a proxy connection to a remote server\n   * and a set of methods that are supported by the server\n   *\n   * @param <T> Generics Type T.\n   * @param protocol protocol class\n   * @param clientVersion client version\n   * @param addr remote address\n   * @param ticket user group information\n   * @param conf configuration to use\n   * @param factory socket factory\n   * @return the protocol proxy\n   * @throws IOException if the far end through a RemoteException",
  "org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler:handle(javax.security.auth.callback.Callback[])" : null,
  "org.apache.hadoop.security.SecurityUtil:setSslConfiguration(org.apache.zookeeper.client.ZKClientConfig,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore,org.apache.zookeeper.common.ClientX509Util)" : null,
  "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:resolvePropertyName(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String)" : "* Resolves a property name to its client/server version if applicable.\n   * <p>\n   * NOTE: This method is public for testing purposes.\n   *\n   * @param mode client/server mode.\n   * @param template property name template.\n   * @return the resolved property name.",
  "org.apache.hadoop.util.hash.Hash:getInstance(org.apache.hadoop.conf.Configuration)" : "* Get a singleton instance of hash function of a type\n   * defined in the configuration.\n   * @param conf current configuration\n   * @return defined hash type, or null if type is invalid",
  "org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoder(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : "* Create RS raw decoder according to configuration.\n   * @param conf configuration\n   * @param coderOptions coder options that's used to create the coder\n   * @param codec the codec to use. If null, will use the default codec\n   * @return raw decoder",
  "org.apache.hadoop.metrics2.AbstractMetric:description()" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction:commit()" : null,
  "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(java.lang.Iterable)" : null,
  "org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.FsShell$UnknownCommandException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep:finish()" : null,
  "org.apache.hadoop.net.NetworkTopology:isChildScope(java.lang.String,java.lang.String)" : "* Checks whether one scope is contained in the other scope.\n   * @param parentScope the parent scope to check\n   * @param childScope  the child scope which needs to be checked.\n   * @return true if childScope is contained within the parentScope",
  "org.apache.hadoop.io.UTF8$Comparator:compare(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumAllUnits()" : null,
  "org.apache.hadoop.security.KDiag:flush()" : "* Flush all active output channels, including {@Code System.err},\n   * so as to stay in sync with any JRE log messages.",
  "org.apache.hadoop.fs.FSInputChecker:mark(int)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:add(java.lang.Object,java.lang.reflect.Field)" : "* Change the declared field {@code field} in {@code source} Object to\n   * {@link MutableMetric}",
  "org.apache.hadoop.util.functional.FutureIO:awaitFuture(java.util.concurrent.Future,long,java.util.concurrent.TimeUnit)" : "* Given a future, evaluate it.\n   * <p>\n   * Any exception generated in the future is\n   * extracted and rethrown.\n   * </p>\n   * @param future future to evaluate\n   * @param timeout timeout to wait.\n   * @param unit time unit.\n   * @param <T> type of the result.\n   * @return the result, if all went well.\n   * @throws InterruptedIOException waiting for future completion was interrupted\n   * @throws CancellationException if the future itself was cancelled\n   * @throws IOException if something went wrong\n   * @throws RuntimeException any nested RTE thrown\n   * @throws TimeoutException the future timed out.",
  "org.apache.hadoop.tools.CommandShell$SubCommand:validate()" : null,
  "org.apache.hadoop.fs.FSDataInputStream:read(long,byte[],int,int)" : "* Read bytes from the given position in the stream to the given buffer.\n   *\n   * @param position  position in the input stream to seek\n   * @param buffer    buffer into which data is read\n   * @param offset    offset into the buffer in which data is written\n   * @param length    maximum number of bytes to read\n   * @return total number of bytes read into the buffer, or <code>-1</code>\n   *         if there is no more data because the end of the stream has been\n   *         reached",
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,java.lang.String)" : "* Set optional Builder parameter.",
  "org.apache.hadoop.io.AbstractMapWritable:setConf(org.apache.hadoop.conf.Configuration)" : "@param conf the conf to set",
  "org.apache.hadoop.io.FloatWritable:hashCode()" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2:getClient(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.FsTracer:get(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.ThreadUtil:getResourceAsStream(java.lang.String)" : "* Convenience method that returns a resource as inputstream from the\n   * classpath.\n   * <p>\n   * Uses the Thread's context classloader to load resource.\n   *\n   * @param resourceName resource to retrieve.\n   *\n   * @throws IOException thrown if resource cannot be loaded\n   * @return inputstream with the resource.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:resolvePath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:<init>(long,long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.util.SysInfoWindows:now()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])" : "* Set an xattr of a file or directory.\n   * The name must be prefixed with the namespace followed by \".\". For example,\n   * \"user.attr\".\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to modify\n   * @param name xattr name.\n   * @param value xattr value.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.FileUtil:checkReturnValue(boolean,java.io.File,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.io.IOUtils:readFullyToByteArray(java.io.DataInput)" : "* Reads a DataInput until EOF and returns a byte array.  Make sure not to\n   * pass in an infinite DataInput or this will never return.\n   *\n   * @param in A DataInput\n   * @return a byte array containing the data from the DataInput\n   * @throws IOException on I/O error, other than EOF",
  "org.apache.hadoop.ha.SshFenceByTcpPort$LogAdapter:isEnabled(int)" : null,
  "org.apache.hadoop.ipc.RetryCache:getCacheName()" : "* @return This method returns cache name for metrics.",
  "org.apache.hadoop.crypto.key.KeyProvider$Options:getCipher()" : null,
  "org.apache.hadoop.conf.StorageUnit$4:getSuffixChar()" : null,
  "org.apache.hadoop.fs.Stat:getExecString()" : null,
  "org.apache.hadoop.conf.Configuration$Parser:parse()" : null,
  "org.apache.hadoop.http.HttpServer2:getDefaultHeaders()" : null,
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:getServiceStatus()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : "* The specification of this method matches that of  \n   * {@link FileContext#createSymlink(Path, Path, boolean)};\n   *\n   * @param target target.\n   * @param link link.\n   * @param createParent create parent.\n   * @throws IOException raised on errors performing I/O.\n   * @throws UnresolvedLinkException unresolved link exception.",
  "org.apache.hadoop.ipc.FairCallQueue:getOverflowedCalls()" : null,
  "org.apache.hadoop.fs.permission.PermissionStatus:createImmutable(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)" : "* Create an immutable {@link PermissionStatus} object.\n   * @param user user.\n   * @param group group.\n   * @param permission permission.\n   * @return PermissionStatus.",
  "org.apache.hadoop.util.Sets:union(java.util.Set,java.util.Set)" : "* Returns the union of two sets as an unmodifiable set.\n   * The returned set contains all elements that are contained in either\n   * backing set.\n   *\n   * <p>Results are undefined if {@code set1} and {@code set2} are sets\n   * based on different equivalence relations (as {@link HashSet},\n   * {@link TreeSet}, and the {@link Map#keySet} of an\n   * {@code IdentityHashMap} all are).\n   *\n   * @param set1 set1.\n   * @param set2 set2.\n   * @param <E> Generics Type E.\n   * @return a new, empty thread-safe {@code Set}.",
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:setCurator(org.apache.curator.framework.CuratorFramework)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:next()" : null,
  "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:newInstance(java.lang.Object[])" : null,
  "org.apache.hadoop.conf.Configuration:isDeprecated(java.lang.String)" : "* checks whether the given <code>key</code> is deprecated.\n   * \n   * @param key the parameter which is to be checked for deprecation\n   * @return <code>true</code> if the key is deprecated and \n   *         <code>false</code> otherwise.",
  "org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:shouldRetry(java.lang.Exception,int,int,boolean)" : null,
  "org.apache.hadoop.ha.HAServiceStatus:setNotReadyToBecomeActive(java.lang.String)" : null,
  "org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:toString()" : null,
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:recursive()" : "* Create the parent directory if they do not exist.\n   *\n   * @return B Generics Type.",
  "org.apache.hadoop.conf.StorageUnit$6:toKBs(double)" : null,
  "org.apache.hadoop.io.file.tfile.Utils$Version:<init>(short,short)" : "* Constructor.\n     * \n     * @param major\n     *          major version.\n     * @param minor\n     *          minor version.",
  "org.apache.hadoop.conf.Configuration:getClasses(java.lang.String,java.lang.Class[])" : "* Get the value of the <code>name</code> property\n   * as an array of <code>Class</code>.\n   * The value of the property specifies a list of comma separated class names.  \n   * If no such property is specified, then <code>defaultValue</code> is \n   * returned.\n   * \n   * @param name the property name.\n   * @param defaultValue default value.\n   * @return property value as a <code>Class[]</code>, \n   *         or <code>defaultValue</code>.",
  "org.apache.hadoop.fs.shell.Display$AvroFileInputStream:close()" : "* Close the stream.",
  "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:builder()" : "* Get the builder.\n   * This must be used after the constructor has been invoked to create\n   * the actual builder: it allows for subclasses to do things after\n   * construction.\n   *\n   * @return FutureDataInputStreamBuilder.",
  "org.apache.hadoop.fs.PositionedReadable:minSeekForVectorReads()" : "* What is the smallest reasonable seek?\n   * @return the minimum number of bytes",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSMetadata:<init>(java.lang.String,int,java.lang.String,java.util.Map,java.util.Date,int)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues6(int,int)" : null,
  "org.apache.hadoop.ipc.CallerContext$Builder:checkFieldSeparator(java.lang.String)" : "* Check whether the separator is legal.\n     * The illegal separators include '\\t', '\\n', '='.\n     * Throw IllegalArgumentException if the separator is Illegal.\n     * @param separator the separator of fields.",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:getTotalRequestsPerSecond()" : null,
  "org.apache.hadoop.util.StringUtils:formatTimeSortable(long)" : "*\n   * Given the time in long milliseconds, returns a String in the sortable\n   * format Xhrs, Ymins, Zsec. X, Y, and Z are always two-digit. If the time is\n   * more than 100 hours ,it is displayed as 99hrs, 59mins, 59sec.\n   *\n   * @param timeDiff The time difference to format\n   * @return format time sortable.",
  "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.Socket,java.lang.String,int,boolean)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:vswap(int[],int,int,int)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:checkPath(org.apache.hadoop.fs.Path)" : "Check that a Path belongs to this FileSystem.",
  "org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(int)" : "* Create a BoundedByteArrayOutputStream with the specified\n   * capacity\n   * @param capacity The capacity of the underlying byte array",
  "org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesReadErasureCoded(long)" : "* Increment the bytes read on erasure-coded files in the statistics.\n     * @param newBytes the additional bytes read",
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:prefetch(int)" : null,
  "org.apache.hadoop.fs.FileUtil:canExecute(java.io.File)" : "* Platform independent implementation for {@link File#canExecute()}\n   * @param f input file\n   * @return On Unix, same as {@link File#canExecute()}\n   *         On Windows, true if process has execute access on the path",
  "org.apache.hadoop.io.compress.SnappyCodec:getConf()" : "* Return the configuration used by this object.\n   *\n   * @return the configuration object used by this objec.",
  "org.apache.hadoop.fs.DUHelper:check(java.lang.String)" : null,
  "org.apache.hadoop.ipc.Server:internalQueueCall(org.apache.hadoop.ipc.Server$Call,boolean)" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToPrettyString(org.apache.hadoop.fs.statistics.IOStatistics)" : "* Convert IOStatistics to a string form, with all the metrics sorted\n   * and empty value stripped.\n   * This is more expensive than the simple conversion, so should only\n   * be used for logging/output where it's known/highly likely that the\n   * caller wants to see the values. Not for debug logging.\n   * @param statistics A statistics instance.\n   * @return string value or the empty string if null",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeys()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:afterDecryption(org.apache.hadoop.crypto.Decryptor,java.nio.ByteBuffer,long,byte[])" : "* This method is executed immediately after decryption. Check whether \n   * decryptor should be updated and recalculate padding if needed.",
  "org.apache.hadoop.ipc.Client:setPingInterval(org.apache.hadoop.conf.Configuration,int)" : "* set the ping interval value in configuration\n   * \n   * @param conf Configuration\n   * @param pingInterval the ping interval",
  "org.apache.hadoop.ipc.Server$Connection:incRpcCount()" : null,
  "org.apache.hadoop.fs.FileSystem:getTrashRoots(boolean)" : "* Get all the trash roots for current user or all users.\n   *\n   * @param allUsers return trash roots for all users if true.\n   * @return all the trash root directories.\n   *         Default FileSystem returns .Trash under users' home directories if\n   *         {@code /user/$USER/.Trash} exists.",
  "org.apache.hadoop.io.ByteWritable:<init>(byte)" : null,
  "org.apache.hadoop.fs.FileSystem$DirListingIterator:fetchMore()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:<init>(org.apache.hadoop.fs.FileSystem)" : null,
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:createOrAppendLogFile(org.apache.hadoop.fs.Path)" : "* Create a new log file and return the {@link FSDataOutputStream}. If a\n   * file with the specified path already exists, open the file for append\n   * instead.\n   *\n   * Once the file is open, update {@link #currentFSOutStream},\n   * {@link #currentOutStream}, and {@#link #currentFilePath}.\n   *\n   * @param initial the target path\n   * @throws IOException thrown if the call to see the append operation fails.",
  "org.apache.hadoop.ipc.RpcClientException:<init>(java.lang.String,java.lang.Throwable)" : "* Constructs exception with the specified detail message and cause.\n   * \n   * @param message message.\n   * @param cause that cause this exception\n   * @param cause the cause (can be retried by the {@link #getCause()} method).\n   *          (A <tt>null</tt> value is permitted, and indicates that the cause\n   *          is nonexistent or unknown.)",
  "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)" : "* Construct the preferred type of 'raw' SequenceFile Writer.\n   * @param conf The configuration.\n   * @param out The stream on top which the writer is to be constructed.\n   * @param keyClass The 'key' type.\n   * @param valClass The 'value' type.\n   * @param compressionType The compression type.\n   * @param codec The compression codec.\n   * @return Returns the handle to the constructed SequenceFile Writer.\n   * @throws IOException raised on errors performing I/O.\n   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}\n   *     instead.",
  "org.apache.hadoop.ipc.Server:getSchedulerClass(java.lang.String,int,org.apache.hadoop.conf.Configuration)" : "* Return class configured by property 'ipc.<port>.scheduler.impl' if it is\n   * present. If the config is not present, and if property\n   * 'ipc.<port>.callqueue.impl' represents FairCallQueue class,\n   * return DecayRpcScheduler. If config 'ipc.<port>.callqueue.impl'\n   * does not have value FairCallQueue, default config (without port) is used\n   * to derive class i.e 'ipc.scheduler.impl'. If default config is also not\n   * present, default class {@link DefaultRpcScheduler} is returned.\n   *\n   * @param namespace Namespace \"ipc\".\n   * @param port Server's listener port.\n   * @param conf Configuration properties.\n   * @return Class returned based on configuration.",
  "org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:failed()" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:updateProgress(long)" : null,
  "org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable:setRunRenewalLoop(boolean)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferPool:numAvailable()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,java.lang.Object)" : null,
  "org.apache.hadoop.security.alias.CredentialShell$CheckCommand:execute()" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(int)" : null,
  "org.apache.hadoop.security.token.Token$TrivialRenewer:renew(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:startUpload()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsConfig:loadFirst(java.lang.String,java.lang.String[])" : "* Load configuration from a list of files until the first successful load\n   * @param conf  the configuration object\n   * @param files the list of filenames to try\n   * @return  the configuration object",
  "org.apache.hadoop.net.AbstractDNSToSwitchMapping:getSwitchMap()" : "* Get a copy of the map (for diagnostics)\n   * @return a clone of the map or null for none known",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:getInstance()" : "* Get the single instance of this class.\n   * @return a shared, empty instance.",
  "org.apache.hadoop.ha.ZKFailoverController:confirmFormat()" : null,
  "org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)" : "* Finds the record that is the closest match to the specified key.\n     * Returns <code>key</code> or if it does not exist, at the first entry\n     * after the named key.\n     * \n     * @param key key that we're trying to find.\n     * @param val data value if key is found.\n     * @return the key that was the closest match or null if eof.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.BufferedFSInputStream:getIOStatistics()" : null,
  "org.apache.hadoop.fs.shell.find.FindOptions:setConfiguration(org.apache.hadoop.conf.Configuration)" : "* Set the {@link Configuration}\n   *\n   * @param configuration {@link Configuration}",
  "org.apache.hadoop.conf.Configuration$Parser:handleInclude()" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:createOptions()" : "* Override point: create an options instance to combine with the \n   * standard options set.\n   * <i>Important. Synchronize uses of {@link Option}</i>\n   * with {@code Option.class}\n   * @return the new options",
  "org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,java.lang.String[])" : "* Create a <code>GenericOptionsParser</code> to parse only the generic\n   * Hadoop arguments.\n   * \n   * The array of string arguments other than the generic arguments can be \n   * obtained by {@link #getRemainingArgs()}.\n   * \n   * @param conf the <code>Configuration</code> to modify.\n   * @param args command-line arguments.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.Client$Connection:disposeSasl()" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:<init>()" : null,
  "org.apache.hadoop.fs.impl.prefetch.PrefetchConstants:<init>()" : null,
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionMaximum(java.lang.String,java.util.function.ToLongFunction)" : "* Add a new evaluator to the maximum statistics.\n   * @param key key of this statistic\n   * @param eval evaluator for the statistic\n   * @return the builder.",
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:getKind()" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:reset()" : "* Resets everything including the input buffers (user and direct).",
  "org.apache.hadoop.io.SecureIOUtils:openForRead(java.io.File,java.lang.String,java.lang.String)" : "* Open the given File for read access, verifying the expected user/group\n   * constraints if security is enabled.\n   *\n   * @return Note that this function provides no additional checks if Hadoop\n   * security is disabled, since doing the checks would be too expensive\n   * when native libraries are not available.\n   *\n   * @param f the file that we are trying to open\n   * @param expectedOwner the expected user owner for the file\n   * @param expectedGroup the expected group owner for the file\n   * @throws IOException if an IO Error occurred, or security is enabled and\n   * the user/group does not match",
  "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:getDefaultQueueWeights(int)" : "* Creates default weights for each queue. The weights are 2^N.",
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getRemoteReadTimeMS()" : null,
  "org.apache.hadoop.fs.FileContext:getFSofPath(org.apache.hadoop.fs.Path)" : "* Get the file system of supplied path.\n   * \n   * @param absOrFqPath - absolute or fully qualified path\n   * @return the file system of the path\n   * \n   * @throws UnsupportedFileSystemException If the file system for\n   *           <code>absOrFqPath</code> is not supported.\n   * @throws IOException If the file system for <code>absOrFqPath</code> could\n   *         not be instantiated.",
  "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:setConf(org.apache.hadoop.conf.Configuration)" : "* Set the configuration and extract the configuration parameters of interest\n     * @param conf the new configuration",
  "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.FileContext$Util:listStatus(java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)" : null,
  "org.apache.hadoop.security.User:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,javax.security.auth.login.LoginContext)" : null,
  "org.apache.hadoop.fs.permission.PermissionStatus:write(java.io.DataOutput,java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)" : "* Serialize a {@link PermissionStatus} from its base components.\n   * @param out out.\n   * @param username username.\n   * @param groupname groupname.\n   * @param permission FsPermission.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.FloatWritable:set(float)" : "* Set the value of this FloatWritable.\n   * @param value value.",
  "org.apache.hadoop.metrics2.sink.StatsDSink:close()" : null,
  "org.apache.hadoop.crypto.key.KeyProvider:close()" : "* Can be used by implementing classes to close any resources\n   * that require closing",
  "org.apache.hadoop.util.Progress:setParent(org.apache.hadoop.util.Progress)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* The src file is under FS, and the dst is on the local disk.\n   * Copy it from FS control to the local dst name.\n   * delSrc indicates if the src will be removed or not.",
  "org.apache.hadoop.crypto.key.kms.ValueQueue:shutdown()" : "* Cleanly shutdown",
  "org.apache.hadoop.conf.Configuration:logDeprecationOnce(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:write(byte[],int,int)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.ShortWritable:readFields(java.io.DataInput)" : "read the short value",
  "org.apache.hadoop.fs.FileSystem:methodNotSupported()" : "* Helper method that throws an {@link UnsupportedOperationException} for the\n   * current {@link FileSystem} method being called.",
  "org.apache.hadoop.fs.viewfs.ViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.conf.StorageUnit$5:toMBs(double)" : null,
  "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addDeferredProcessingTime(java.lang.String,long)" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:setDmax(int)" : "* @param dmax the dmax to set",
  "org.apache.hadoop.util.VersionInfo:<init>(java.lang.String)" : null,
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$3:check(java.lang.String[],java.lang.String[],java.lang.String[])" : "* Checks to see if the supplied hostname matches any of the supplied CNs\n     * or \"DNS\" Subject-Alts.  Most implementations only look at the first CN,\n     * and ignore any additional CNs.  Most implementations do look at all of\n     * the \"DNS\" Subject-Alts. The CNs or Subject-Alts may contain wildcards\n     * according to RFC 2818.\n     *\n     * @param cns         CN fields, in order, as extracted from the X.509\n     *                    certificate.\n     * @param subjectAlts Subject-Alt fields of type 2 (\"DNS\"), as extracted\n     *                    from the X.509 certificate.\n     * @param hosts       The array of hostnames to verify.\n     * @throws SSLException If verification failed.",
  "org.apache.hadoop.ha.ZKFCRpcServer:<init>(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,org.apache.hadoop.ha.ZKFailoverController,org.apache.hadoop.security.authorize.PolicyProvider)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.http.HttpServer2:addFilterPathMapping(java.lang.String,org.eclipse.jetty.servlet.ServletContextHandler)" : "* Add the path spec to the filter path mapping.\n   * @param pathSpec The path spec\n   * @param webAppCtx The WebApplicationContext to add to",
  "org.apache.hadoop.fs.Path:normalizePath(java.lang.String,java.lang.String)" : "* Normalize a path string to use non-duplicated forward slashes as\n   * the path separator and remove any trailing path separators.\n   *\n   * @param scheme the URI scheme. Used to deduce whether we\n   * should replace backslashes or not\n   * @param path the scheme-specific part\n   * @return the normalized path string",
  "org.apache.hadoop.fs.Trash:getEmptier()" : "* Return a {@link Runnable} that periodically empties the trash of all\n   * users, intended to be run by the superuser.\n   *\n   * @throws IOException on raised on errors performing I/O.\n   * @return Runnable.",
  "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getTestProvider()" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:<init>(int,int,int)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:<init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:close()" : "* Shutdown valueQueue executor threads",
  "org.apache.hadoop.io.BytesWritable:<init>(byte[])" : "* Create a BytesWritable using the byte array as the initial value.\n   * @param bytes This array becomes the backing storage for the object.",
  "org.apache.hadoop.fs.store.audit.AuditingFunctions:withinAuditSpan(org.apache.hadoop.fs.store.audit.AuditSpan,org.apache.hadoop.util.functional.FunctionRaisingIOE)" : "* Given a function, return a new function which\n   * activates and deactivates the span around the inner one.\n   * @param auditSpan audit span\n   * @param operation operation\n   * @param <T> Generics Type T.\n   * @param <R> Generics Type R.\n   * @return a new invocation.",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(java.lang.String,java.lang.String,int)" : "* Create a mutable integer counter\n   * @param name  of the metric\n   * @param desc  metric description\n   * @param iVal  initial value\n   * @return a new counter object",
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumErasedBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : "* Get the number of erased blocks in the block group.\n   * @param blockGroup blockGroup.\n   * @return number of erased blocks",
  "org.apache.hadoop.fs.FilterFileSystem:getChildFileSystems()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getOwner()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:setReplication(org.apache.hadoop.fs.Path,short)" : "* Set replication for an existing file.\n   * \n   * @param src file name\n   * @param replication new replication\n   * @throws IOException raised on errors performing I/O.\n   * @return true if successful;\n   *         false if file does not exist or is a directory",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMinimum(java.lang.String,long)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:allowVerboseDump()" : "* Allow to dump verbose info during encoding/decoding.\n   * @return true if it's allowed to do verbose dump, false otherwise.",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartA()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:snapshotMetrics(org.apache.hadoop.metrics2.impl.MetricsSourceAdapter,org.apache.hadoop.metrics2.impl.MetricsBufferBuilder)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem:createFileSystem(java.net.URI[],org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.viewfs.FsGetter)" : "* Initializes an nfly mountpoint in viewfs.\n   *\n   * @param uris destinations to replicate writes to\n   * @param conf file system configuration\n   * @param settings comma-separated list of k=v pairs.\n   * @return an Nfly filesystem\n   * @throws IOException",
  "org.apache.hadoop.util.dynamic.BindingUtils:<init>()" : null,
  "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:calculateIV(byte[],long,byte[])" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable)" : "* @return Read the next key in the file into <code>key</code>, skipping its\n     * value.True if another entry exists, and false at end of file.\n     *\n     * @param key key.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.MapWritable:putAll(java.util.Map)" : null,
  "org.apache.hadoop.fs.sftp.SFTPInputStream:close()" : null,
  "org.apache.hadoop.fs.shell.find.Print$Print0:<init>()" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKeySets(java.lang.String[])" : null,
  "org.apache.hadoop.util.Options$FSDataOutputStreamOption:getValue()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.permission.FsPermission:getDirDefault()" : "* Get the default permission for directory.\n   *\n   * @return DirDefault FsPermission.",
  "org.apache.hadoop.io.MapFile$Writer:close()" : "Close the map.",
  "org.apache.hadoop.ipc.Server:wrapWithSasl(org.apache.hadoop.ipc.Server$RpcCall)" : null,
  "org.apache.hadoop.security.KDiag:validateShortName()" : "* Verify whether auth_to_local rules transform a principal name\n   * <p>\n   * Having a local user name \"bar@foo.com\" may be harmless, so it is noted at\n   * info. However if what was intended is a transformation to \"bar\"\n   * it can be difficult to debug, hence this check.",
  "org.apache.hadoop.fs.shell.PathData:getPathDataForChild(org.apache.hadoop.fs.shell.PathData)" : "* Creates a new object for a child entry in this directory\n   * @param child the basename will be appended to this object's path\n   * @return PathData for the child\n   * @throws IOException if this object does not exist or is not a directory",
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:flush()" : null,
  "org.apache.hadoop.service.ServiceStateException:convert(java.lang.Throwable)" : "* Convert any exception into a {@link RuntimeException}.\n   * All other exception types are wrapped in a new instance of\n   * {@code ServiceStateException}.\n   * @param fault exception or throwable\n   * @return a {@link RuntimeException} to rethrow",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:referenceLostContext(java.lang.Long)" : "* In case of reference loss for IOStatisticsContext.\n   * @param key ThreadID.",
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMaximumReference(java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree:getHomeDirPrefixValue()" : "*\n   * @return home dir value from mount table; null if no config value\n   * was found.",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getInstance()" : "* Get the single instance.\n   * @return an instance.",
  "org.apache.hadoop.util.KMSUtil:createKeyProvider(org.apache.hadoop.conf.Configuration,java.lang.String)" : "* Creates a new KeyProvider from the given Configuration\n   * and configuration key name.\n   *\n   * @param conf Configuration\n   * @param configKeyName The configuration key name\n   * @return new KeyProvider, or null if no provider was found.\n   * @throws IOException if the KeyProvider is improperly specified in\n   *                             the Configuration",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:numOpenConnectionsPerUser()" : null,
  "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh()" : "* Refresh the netgroup cache",
  "org.apache.hadoop.fs.viewfs.ViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToStandby(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto)" : null,
  "org.apache.hadoop.io.serializer.SerializationFactory:getDeserializer(java.lang.Class)" : null,
  "org.apache.hadoop.fs.FileContext:getWorkingDirectory()" : "* Gets the working directory for wd-relative names (such a \"foo/bar\").\n   * @return the path.",
  "org.apache.hadoop.fs.LocalFileSystemPathHandle:equals(java.lang.Object)" : null,
  "org.apache.hadoop.metrics2.lib.MutableCounterLong:incr()" : null,
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:reset()" : null,
  "org.apache.hadoop.util.dynamic.DynConstructors$Builder:buildChecked()" : null,
  "org.apache.hadoop.conf.ReconfigurableBase:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])" : "* This is the client side invoker of RPC method. It only throws\n     * ServiceException, since the invocation proxy expects only\n     * ServiceException to be thrown by the method in case protobuf service.\n     * \n     * ServiceException has the following causes:\n     * <ol>\n     * <li>Exceptions encountered on the client side in this method are \n     * set as cause in ServiceException as is.</li>\n     * <li>Exceptions from the server are wrapped in RemoteException and are\n     * set as cause in ServiceException</li>\n     * </ol>\n     * \n     * Note that the client calling protobuf RPC methods, must handle\n     * ServiceException by getting the cause from the ServiceException. If the\n     * cause is RemoteException, then unwrap it to get the exception thrown by\n     * the server.",
  "org.apache.hadoop.util.SysInfoLinux:readProcStatFile()" : "* Read /proc/stat file, parse and calculate cumulative CPU.",
  "org.apache.hadoop.util.ChunkedArrayList:getMaxChunkSize()" : null,
  "org.apache.hadoop.conf.Configuration:toString()" : null,
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(java.util.Collection)" : "* Adds a collection of false positive information to <i>this</i> retouched Bloom filter.\n   * @param coll The collection of false positive.",
  "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:setDelegate(org.apache.hadoop.ipc.DecayRpcScheduler)" : null,
  "org.apache.hadoop.fs.FSDataOutputStream$PositionCache:close()" : null,
  "org.apache.hadoop.ipc.Server$ConnectionManager:decrUserConnections(java.lang.String)" : null,
  "org.apache.hadoop.conf.Configuration:getStrings(java.lang.String)" : "* Get the comma delimited values of the <code>name</code> property as \n   * an array of <code>String</code>s.  \n   * If no such property is specified then <code>null</code> is returned.\n   * \n   * @param name property name.\n   * @return property value as an array of <code>String</code>s, \n   *         or <code>null</code>.",
  "org.apache.hadoop.io.compress.ZStandardCodec:createOutputStream(java.io.OutputStream)" : "* Create a {@link CompressionOutputStream} that will write to the given\n   * {@link OutputStream}.\n   *\n   * @param out the location for the final output stream\n   * @return a stream the user can write uncompressed data to have compressed\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.Text:set(byte[])" : "* Set to a utf8 byte array. If the length of <code>utf8</code> is\n   * <em>zero</em>, actually clear {@link #bytes} and any existing\n   * data is lost.\n   *\n   * @param utf8 input utf8.",
  "org.apache.hadoop.fs.LocalFileSystem:getRaw()" : null,
  "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsAcls(java.lang.Class)" : null,
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem:getUri()" : null,
  "org.apache.hadoop.fs.Options$ChecksumOpt:<init>(org.apache.hadoop.util.DataChecksum$Type,int)" : "* Normal ctor\n     * @param type checksum type\n     * @param size bytes per checksum",
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read()" : null,
  "org.apache.hadoop.fs.HarFileSystem:getUsed()" : null,
  "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence)" : "* Write a line of text to a file. Characters are encoded into bytes using\n   * UTF-8. This utility method opens the file for writing, creating the file if\n   * it does not exist, or overwrites an existing file.\n   *\n   * @param fs the files system with which to create the file\n   * @param path the path to the file\n   * @param charseq the char sequence to write to the file\n   *\n   * @return the file system\n   *\n   * @throws NullPointerException if any of the arguments are {@code null}\n   * @throws IOException if an I/O error occurs creating or writing to the file",
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:generateEncryptedKey(java.lang.String)" : "* Generates a key material and encrypts it using the given key version name\n   * and initialization vector. The generated key material is of the same\n   * length as the <code>KeyVersion</code> material and is encrypted using the\n   * same cipher.\n   * <p>\n   * NOTE: The generated key is not stored by the <code>KeyProvider</code>\n   *\n   * @param encryptionKeyName The latest KeyVersion of this key's material will\n   * be encrypted.\n   * @return EncryptedKeyVersion with the generated key material, the version\n   * name is 'EEK' (for Encrypted Encryption Key)\n   * @throws IOException thrown if the key material could not be generated\n   * @throws GeneralSecurityException thrown if the key material could not be \n   * encrypted because of a cryptographic issue.",
  "org.apache.hadoop.fs.Path:makeQualified(java.net.URI,org.apache.hadoop.fs.Path)" : "* Returns a qualified path object.\n   *\n   * @param defaultUri if this path is missing the scheme or authority\n   * components, borrow them from this URI\n   * @param workingDir if this path isn't absolute, treat it as relative to this\n   * working directory\n   * @return this path if it contains a scheme and authority and is absolute, or\n   * a new path that includes a path and authority and is fully qualified",
  "org.apache.hadoop.fs.FilterFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.io.ArrayWritable:toString()" : null,
  "org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:<init>(java.io.InputStream)" : "* Buffer an input stream with the default buffer size of 8k.\n   * @param in input stream",
  "org.apache.hadoop.util.CloseableReferenceCount:reference()" : "* Increment the reference count.\n   *\n   * @throws ClosedChannelException      If the status is closed.",
  "org.apache.hadoop.fs.shell.find.Name:setCaseSensitive(boolean)" : null,
  "org.apache.hadoop.conf.Configuration:getStrings(java.lang.String,java.lang.String[])" : "* Get the comma delimited values of the <code>name</code> property as \n   * an array of <code>String</code>s.  \n   * If no such property is specified then default value is returned.\n   * \n   * @param name property name.\n   * @param defaultValue The default value\n   * @return property value as an array of <code>String</code>s, \n   *         or default value.",
  "org.apache.hadoop.http.HttpServer2:addDefaultApps(org.eclipse.jetty.server.handler.ContextHandlerCollection,java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Add default apps.\n   *\n   * @param parent contexthandlercollection.\n   * @param appDir The application directory\n   * @param conf configuration.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.ShellBasedIdMapping:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$SupportState:getMessage()" : null,
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsSet(java.lang.String)" : null,
  "org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,boolean)" : null,
  "org.apache.hadoop.io.DefaultStringifier:storeArray(org.apache.hadoop.conf.Configuration,java.lang.Object[],java.lang.String)" : "* Stores the array of items in the configuration with the given keyName.\n   * \n   * @param <K> the class of the item\n   * @param conf the configuration to use \n   * @param items the objects to be stored\n   * @param keyName the name of the key to use\n   * @throws IndexOutOfBoundsException if the items array is empty\n   * @throws IOException : forwards Exceptions from the underlying \n   * {@link Serialization} classes.",
  "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getCipher()" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtLevel(org.slf4j.Logger,java.lang.String,java.lang.Object)" : "* A method to log IOStatistics from a source at different levels.\n   *\n   * @param log    Logger for logging.\n   * @param level  LOG level.\n   * @param source Source to LOG.",
  "org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:getMetaByName(java.lang.String)" : null,
  "org.apache.hadoop.net.SocketInputStream:setTimeout(long)" : null,
  "org.apache.hadoop.http.HttpServer2:getAttribute(java.lang.String)" : "* Get the value in the webapp context.\n   * @param name The name of the attribute\n   * @return The value of the attribute",
  "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:sink()" : null,
  "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getConf()" : "* Returns the configuration of the factory.\n   *\n   * @return the configuration of the factory.",
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel:compressionLevel()" : null,
  "org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:<init>(java.util.concurrent.ExecutorService)" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:getSlope()" : "* @return the slope",
  "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:init(org.apache.commons.configuration2.SubsetConfiguration)" : null,
  "org.apache.hadoop.fs.shell.find.ExpressionFactory:getExpressionFactory()" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec:getDefaultExtension()" : null,
  "org.apache.hadoop.ha.NodeFencer:parseMethods(org.apache.hadoop.conf.Configuration,java.lang.String)" : null,
  "org.apache.hadoop.fs.statistics.MeanStatistic:set(org.apache.hadoop.fs.statistics.MeanStatistic)" : "* Set the statistic to the values of another.\n   * Synchronized.\n   * @param other the source.",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:getMode()" : null,
  "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:<init>(int)" : "* Create a table w/o headers\n     * @param columns number of columns",
  "org.apache.hadoop.fs.statistics.MeanStatistic:addSample(long)" : "* Add a sample.\n   * Thread safe.\n   * @param value value to add to the sum",
  "org.apache.hadoop.io.DoubleWritable$Comparator:<init>()" : null,
  "org.apache.hadoop.ipc.Server$Connection:getServer()" : null,
  "org.apache.hadoop.fs.shell.PathData:openForSequentialIO()" : "* Open a file for sequential IO.\n   * <p>\n   * This uses FileSystem.openFile() to request sequential IO;\n   * the file status is also passed in.\n   * Filesystems may use to optimize their IO.\n   * </p>\n   * @return an input stream\n   * @throws IOException failure",
  "org.apache.hadoop.util.Options$BooleanOption:getValue()" : null,
  "org.apache.hadoop.service.CompositeService:addIfService(java.lang.Object)" : "* If the passed object is an instance of {@link Service},\n   * add it to the list of services managed by this {@link CompositeService}\n   * @param object object.\n   * @return true if a service is added, false otherwise.",
  "org.apache.hadoop.util.ProcessUtils:runCmdAsync(java.util.List)" : null,
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:initFs()" : "* Initialize the connection to HDFS and create the base directory. Also\n   * launch the flush thread.",
  "org.apache.hadoop.security.SaslRpcServer$AuthMethod:write(java.io.DataOutput)" : "* Write to out.\n     * @param out DataOutput.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.SaslOutputStream:write(byte[])" : "* Writes <code>b.length</code> bytes from the specified byte array to this\n   * output stream.\n   * <p>\n   * The <code>write</code> method of <code>SASLOutputStream</code> calls the\n   * <code>write</code> method of three arguments with the three arguments\n   * <code>b</code>, <code>0</code>, and <code>b.length</code>.\n   * \n   * @param b\n   *          the data.\n   * @exception NullPointerException\n   *              if <code>b</code> is null.\n   * @exception IOException\n   *              if an I/O error occurs.",
  "org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:printExtendedAclEntry(org.apache.hadoop.fs.permission.AclStatus,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.AclEntry)" : "* Prints a single extended ACL entry.  If the mask restricts the\n     * permissions of the entry, then also prints the restricted version as the\n     * effective permissions.  The mask applies to all named entries and also\n     * the unnamed group entry.\n     * @param aclStatus AclStatus for the path\n     * @param fsPerm FsPermission for the path\n     * @param entry AclEntry extended ACL entry to print",
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:close()" : null,
  "org.apache.hadoop.conf.StorageUnit$4:toEBs(double)" : null,
  "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:close()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:getUri(java.net.URI,java.lang.String,boolean,int)" : "* Get the URI for the file system based on the given URI. The path, query\n   * part of the given URI is stripped out and default file system port is used\n   * to form the URI.\n   * \n   * @param uri FileSystem URI.\n   * @param authorityNeeded if true authority cannot be null in the URI. If\n   *          false authority must be null.\n   * @param defaultPort default port to use if port is not specified in the URI.\n   * \n   * @return URI of the file system\n   * \n   * @throws URISyntaxException <code>uri</code> has syntax error",
  "org.apache.hadoop.util.VersionInfo:getBuildVersion()" : "* Returns the buildVersion which includes version,\n   * revision, user and date.\n   * @return the buildVersion",
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:createCredentialEntry(java.lang.String,char[])" : null,
  "org.apache.hadoop.fs.FileSystem:getStatus()" : "* Returns a status object describing the use and capacity of the\n   * filesystem. If the filesystem has multiple partitions, the\n   * use and capacity of the root partition is reflected.\n   *\n   * @return a FsStatus object\n   * @throws IOException\n   *           see specific implementation",
  "org.apache.hadoop.io.BloomMapFile$Reader:initBloomFilter(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:flush()" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:extractKMSPath(java.net.URI)" : null,
  "org.apache.hadoop.http.HttpServer2$Builder:createHttpsChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration)" : null,
  "org.apache.hadoop.fs.FilterFs:getCanonicalServiceName()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getHttpUserGroupInformationInContext()" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkRequired(boolean,java.lang.String)" : "* Validates that the expression (that checks a required field is present) is true.\n   * @param isPresent indicates whether the given argument is present.\n   * @param argName the name of the argument being validated.",
  "org.apache.hadoop.service.AbstractService:resetGlobalListeners()" : "* Package-scoped method for testing -resets the global listener list",
  "org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytab(java.lang.String,java.lang.String)" : "* Log a user in from a keytab file. Loads a user identity from a keytab\n   * file and logs them in. They become the currently logged-in user.\n   * @param user the principal name to load from the keytab\n   * @param path the path to the keytab file\n   * @throws IOException raised on errors performing I/O.\n   * @throws KerberosAuthException if it's a kerberos login exception.",
  "org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)" : "* Convenience method, so that we don't open a new connection when using this\n   * method from within another method. Otherwise every API invocation incurs\n   * the overhead of opening/closing a TCP connection.",
  "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:size()" : "* Number of rows excluding header \n     * @return rows",
  "org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)" : "* Return an array containing hostnames, offset and size of\n   * portions of the given file.  For a nonexistent\n   * file or regions, {@code null} is returned.\n   *\n   * This call is most helpful with location-aware distributed\n   * filesystems, where it returns hostnames of machines that\n   * contain the given file.\n   *\n   * A FileSystem will normally return the equivalent result\n   * of passing the {@code FileStatus} of the path to\n   * {@link #getFileBlockLocations(FileStatus, long, long)}\n   *\n   * @param p path is used to identify an FS since an FS could have\n   *          another FS that it could be delegating the call to\n   * @param start offset into the given file\n   * @param len length for which to get locations for\n   * @throws FileNotFoundException when the path does not exist\n   * @throws IOException IO failure\n   * @return block location array.",
  "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)" : "* Construct from a {@link FileContext}.\n   *\n   * @param fc FileContext\n   * @param path path.\n   * @throws IOException failure",
  "org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String,java.lang.String)" : "* Get the value of the <code>name</code> property as a trimmed <code>String</code>, \n   * <code>defaultValue</code> if no such property exists. \n   * See @{Configuration#getTrimmed} for more details.\n   * \n   * @param name          the property name.\n   * @param defaultValue  the property default value.\n   * @return              the value of the <code>name</code> or defaultValue\n   *                      if it is not set.",
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:finished()" : null,
  "org.apache.hadoop.util.VersionInfo:_getVersion()" : null,
  "org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecByName(java.lang.String)" : "* Find the relevant compression codec for the codec's canonical class name\n   * or by codec alias.\n   * <p>\n   * Codec aliases are case insensitive.\n   * <p>\n   * The code alias is the short class name (without the package name).\n   * If the short class name ends with 'Codec', then there are two aliases for\n   * the codec, the complete short class name and the short class name without\n   * the 'Codec' ending. For example for the 'GzipCodec' codec class name the\n   * alias are 'gzip' and 'gzipcodec'.\n   *\n   * @param codecName the canonical class name of the codec\n   * @return the codec object",
  "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,int)" : null,
  "org.apache.hadoop.conf.Configuration:substituteVars(java.lang.String)" : "* Attempts to repeatedly expand the value {@code expr} by replacing the\n   * left-most substring of the form \"${var}\" in the following precedence order\n   * <ol>\n   *   <li>by the value of the environment variable \"var\" if defined</li>\n   *   <li>by the value of the Java system property \"var\" if defined</li>\n   *   <li>by the value of the configuration key \"var\" if defined</li>\n   * </ol>\n   *\n   * If var is unbounded the current state of expansion \"prefix${var}suffix\" is\n   * returned.\n   * <p>\n   * This function also detects self-referential substitutions, i.e.\n   * <pre>\n   *   {@code\n   *   foo.bar = ${foo.bar}\n   *   }\n   * </pre>\n   * If a cycle is detected then the original expr is returned. Loops\n   * involving multiple substitutions are not detected.\n   *\n   * In order not to introduce breaking changes (as Oozie for example contains a method with the\n   * same name and same signature) do not make this method public, use substituteCommonVariables\n   * in this case.\n   *\n   * @param expr the literal value of a config key\n   * @return null if expr is null, otherwise the value resulting from expanding\n   * expr using the algorithm above.\n   * @throws IllegalArgumentException when more than\n   * {@link Configuration#MAX_SUBST} replacements are required",
  "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:pad()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getUriDefaultPort()" : null,
  "org.apache.hadoop.ha.HAServiceTarget:getProxy(org.apache.hadoop.conf.Configuration,int)" : "* @return a proxy to connect to the target HA Service.\n   * @param timeoutMs timeout in milliseconds.\n   * @param conf Configuration.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ha.ActiveStandbyElector:getZKSessionIdForTests()" : null,
  "org.apache.hadoop.security.SaslPlainServer:getNegotiatedProperty(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:setInputFromSavedData()" : null,
  "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:<init>(org.apache.hadoop.fs.statistics.IOStatistics)" : "* Instantiate.\n   * @param wrapped nullable wrapped statistics.",
  "org.apache.hadoop.io.retry.RetryInvocationHandler:getRetryPolicy(java.lang.reflect.Method)" : null,
  "org.apache.hadoop.util.StringUtils:escapeHTML(java.lang.String)" : "* Escapes HTML Special characters present in the string.\n     * @param string param string.\n     * @return HTML Escaped String representation",
  "org.apache.hadoop.io.DefaultStringifier:store(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.String)" : "* Stores the item in the configuration with the given keyName.\n   * \n   * @param <K>  the class of the item\n   * @param conf the configuration to store\n   * @param item the object to be stored\n   * @param keyName the name of the key to use\n   * @throws IOException : forwards Exceptions from the underlying \n   * {@link Serialization} classes.",
  "org.apache.hadoop.io.compress.snappy.SnappyCompressor:<init>()" : "* Creates a new compressor with the default buffer size.",
  "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:<init>(org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolPB)" : null,
  "org.apache.hadoop.fs.FileSystem:isFile(org.apache.hadoop.fs.Path)" : "True iff the named path is a regular file.\n   * Note: Avoid using this method. Instead reuse the FileStatus\n   * returned by {@link #getFileStatus(Path)} or listStatus() methods.\n   *\n   * @param f path to check\n   * @throws IOException IO failure\n   * @deprecated Use {@link #getFileStatus(Path)} instead\n   * @return if f is file true, not false.",
  "org.apache.hadoop.metrics2.impl.MetricsConfig:keys()" : null,
  "org.apache.hadoop.util.Options$IntegerOption:getValue()" : null,
  "org.apache.hadoop.io.IOUtils:wrapWithMessage(java.io.IOException,java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:getEncryptionKeyVersionName()" : "* @return Version name of the encryption key used to encrypt the encrypted\n     * key.",
  "org.apache.hadoop.fs.RawLocalFileSystem:truncate(org.apache.hadoop.fs.Path,long)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:<init>(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir,long,org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.viewfs.InodeTree)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:write(byte[],int,int)" : null,
  "org.apache.hadoop.fs.FileSystem:getReplication(org.apache.hadoop.fs.Path)" : "* Get the replication factor.\n   *\n   * @deprecated Use {@link #getFileStatus(Path)} instead\n   * @param src file name\n   * @return file replication\n   * @throws FileNotFoundException if the path does not resolve.\n   * @throws IOException an IO failure",
  "org.apache.hadoop.crypto.key.KeyProviderExtension:toString()" : null,
  "org.apache.hadoop.security.HadoopKerberosName:<init>(java.lang.String)" : "* Create a name from the full Kerberos principal name.\n   * @param name name.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setAtomicLong(java.util.concurrent.atomic.AtomicLong,long)" : "* Set an atomic long to a value.\n   * @param aLong atomic long; may be null\n   * @param value value to set to",
  "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getOwner()" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep:getInputBlocks()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttrCacheTag(org.apache.hadoop.metrics2.MetricsTag,int)" : null,
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:getCodec()" : null,
  "org.apache.hadoop.io.VIntWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:clearBit(int)" : "* Clears a specified bit in the bit vector and keeps up-to-date the KeyList vectors.\n   * @param index The position of the bit to clear.",
  "org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])" : "*  Iterates over the given expanded paths and invokes\n   *  {@link #processPath(PathData)} on each element.  If \"recursive\" is true,\n   *  will do a post-visit DFS on directories.\n   *  @param parent if called via a recurse, will be the parent dir, else null\n   *  @param items a list of {@link PathData} objects to process\n   *  @throws IOException if anything goes wrong...",
  "org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:release()" : "* Clear the buffer pool thus releasing all the buffers.\n   * The caller must remove all references of\n   * existing buffers before calling this method to avoid\n   * memory leaks.",
  "org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:invokeChecked(java.lang.Object[])" : null,
  "org.apache.hadoop.util.Shell:getAllShells()" : "* Static method to return a Set of all <code>Shell</code> objects.\n   *\n   * @return all shells set.",
  "org.apache.hadoop.fs.viewfs.ChRootedFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.util.Shell:getGroupsForUserCommand(java.lang.String)" : "* A command to get a given user's groups list.\n   * If the OS is not WINDOWS, the command will get the user's primary group\n   * first and finally get the groups list which includes the primary group.\n   * i.e. the user's primary group will be included twice.\n   *\n   * @param user user.\n   * @return groups for user command.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:createNewInstance(java.lang.Long)" : "* Creating a new IOStatisticsContext instance for a FS to be used.\n   * @param key Thread ID that represents which thread the context belongs to.\n   * @return an instance of IOStatisticsContext.",
  "org.apache.hadoop.util.functional.RemoteIterators$MaybeClose:<init>(java.lang.Object)" : "* Construct.\n     * @param o object to close.",
  "org.apache.hadoop.io.SequenceFile$Reader$FileOption:<init>(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:bindCommandOptions()" : "* Set the {@link #commandOptions} field to the result of\n   * {@link #createOptions()}; protected for subclasses and test access.",
  "org.apache.hadoop.fs.RawLocalFileSystem:<init>()" : null,
  "org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.net.InetAddress)" : "* Identify the Sasl Properties to be used for a connection with a client.\n   * @param clientAddress client's address\n   * @return the sasl properties to be used for the connection.",
  "org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,boolean)" : "* Finds the record that is the closest match to the specified key.\n     * \n     * @param key       - key that we're trying to find\n     * @param val       - data value if key is found\n     * @param before    - IF true, and <code>key</code> does not exist, return\n     * the first entry that falls just before the <code>key</code>.  Otherwise,\n     * return the record that sorts just after.\n     * @return          - the key that was the closest match or null if eof.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.BufferedFSInputStream:toString()" : null,
  "org.apache.hadoop.io.SequenceFile$Writer$BufferSizeOption:<init>(int)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getStatus()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:handleEmptyDstDirectoryOnWindows(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.Path,java.io.File)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:equals(java.lang.Object)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createURL(java.lang.String,java.lang.String,java.lang.String,java.util.Map)" : null,
  "org.apache.hadoop.ipc.Server:setClientBackoffEnabled(boolean)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource,java.lang.Iterable,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,long,boolean)" : null,
  "org.apache.hadoop.net.NodeBase:getNetworkLocation()" : "@return this node's network location",
  "org.apache.hadoop.util.Sets:capacity(int)" : "* Returns a capacity that is sufficient to keep the map from being resized\n   * as long as it grows no larger than expectedSize and the load factor\n   * is ≥ its default (0.75).\n   * The implementation of this method is adapted from Guava version 27.0-jre.",
  "org.apache.hadoop.ipc.DecayRpcScheduler:addServiceUserDecayedCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder)" : null,
  "org.apache.hadoop.security.LdapGroupsMapping:getLdapUrls()" : "* Get URLs of configured LDAP servers.\n   * @return URLs of LDAP servers being used.",
  "org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])" : null,
  "org.apache.hadoop.metrics2.util.SampleQuantiles$SampleItem:toString()" : null,
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor:invokeAny(java.util.Collection,long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.security.SaslInputStream:read(byte[])" : "* Reads up to <code>b.length</code> bytes of data from this input stream into\n   * an array of bytes.\n   * <p>\n   * The <code>read</code> method of <code>InputStream</code> calls the\n   * <code>read</code> method of three arguments with the arguments\n   * <code>b</code>, <code>0</code>, and <code>b.length</code>.\n   * \n   * @param b\n   *          the buffer into which the data is read.\n   * @return the total number of bytes read into the buffer, or <code>-1</code>\n   *         is there is no more data because the end of the stream has been\n   *         reached.\n   * @exception IOException\n   *              if an I/O error occurs.",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getAclStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.DataOutputOutputStream:constructOutputStream(java.io.DataOutput)" : "* Construct an OutputStream from the given DataOutput. If 'out'\n   * is already an OutputStream, simply returns it. Otherwise, wraps\n   * it in an OutputStream.\n   * @param out the DataOutput to wrap\n   * @return an OutputStream instance that outputs to 'out'",
  "org.apache.hadoop.util.JsonSerialization:toString(java.lang.Object)" : "* Convert an instance to a string form for output. This is a robust\n   * operation which will convert any JSON-generating exceptions into\n   * error text.\n   * @param instance non-null instance\n   * @return a JSON string",
  "org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getEnabledConfigKey()" : null,
  "org.apache.hadoop.util.MachineList:<init>(java.lang.String,org.apache.hadoop.util.MachineList$InetAddressFactory)" : null,
  "org.apache.hadoop.http.HttpServer2$Builder:setAuthFilterConfigurationPrefix(java.lang.String)" : null,
  "org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByNameWithSearch(java.lang.String)" : null,
  "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:withAttribute(java.lang.String,java.lang.String)" : "* Add an attribute to the current map.\n     * Replaces any with the existing key.\n     * @param key key to set/update\n     * @param value new value\n     * @return the builder",
  "org.apache.hadoop.fs.FileContext:create(org.apache.hadoop.fs.Path)" : "* Create a {@link FSDataOutputStreamBuilder} for creating or overwriting\n   * a file on indicated path.\n   *\n   * @param f the file path to create builder for.\n   * @return {@link FSDataOutputStreamBuilder} to build a\n   *         {@link FSDataOutputStream}.\n   *\n   * Upon {@link FSDataOutputStreamBuilder#build()} being invoked,\n   * builder parameters will be verified by {@link FileContext} and\n   * {@link AbstractFileSystem#create}. And filesystem states will be modified.\n   *\n   * Client should expect {@link FSDataOutputStreamBuilder#build()} throw the\n   * same exceptions as create(Path, EnumSet, CreateOpts...).\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.fs.FileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)" : "* Set the storage policy for a given file or directory.\n   *\n   * @param src file or directory path.\n   * @param policyName the name of the target storage policy. The list\n   *                   of supported Storage policies can be retrieved\n   *                   via {@link #getAllStoragePolicies}.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.fs.FileSystem:openFile(org.apache.hadoop.fs.PathHandle)" : "* Open a file for reading through a builder API.\n   * Ultimately calls {@link #open(PathHandle, int)} unless a subclass\n   * executes the open command differently.\n   *\n   * If PathHandles are unsupported, this may fail in the\n   * {@code FSDataInputStreamBuilder.build()}  command,\n   * rather than in this {@code openFile()} operation.\n   * @param pathHandle path handle.\n   * @return a FSDataInputStreamBuilder object to build the input stream\n   * @throws IOException if some early checks cause IO failures.\n   * @throws UnsupportedOperationException if support is checked early.",
  "org.apache.hadoop.io.compress.ZStandardCodec:getCompressorType()" : "* Get the type of {@link Compressor} needed by this {@link CompressionCodec}.\n   *\n   * @return the type of compressor needed by this codec.",
  "org.apache.hadoop.metrics2.lib.MethodMetric$1:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)" : null,
  "org.apache.hadoop.security.Credentials:getAllSecretKeys()" : "* Return all the secret key entries in the in-memory map.\n   *\n   * @return Text List.",
  "org.apache.hadoop.fs.RawLocalFileSystem:mkdirs(org.apache.hadoop.fs.Path)" : "* Creates the specified directory hierarchy. Does not\n   * treat existence as an error.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:fsGetter()" : "* Gets file system creator instance.\n   *\n   * @return fs getter.",
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:populateUncompressedBuffer(byte[],int,int,int)" : null,
  "org.apache.hadoop.fs.BufferedFSInputStream:maxReadSizeForVectorReads()" : null,
  "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:isRunning(org.apache.hadoop.util.Daemon)" : null,
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:bytesToChars(byte[])" : null,
  "org.apache.hadoop.service.ServiceStateException:<init>(java.lang.Throwable)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:compareKeys(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.security.CompositeGroupsMapping:addMappingProvider(java.lang.String,java.lang.Class)" : null,
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getAliases()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:<init>(java.io.OutputStream,boolean)" : "* Construct with default buffer size.\n   * @param out output stream to buffer\n   * @param downgradeSyncable should Syncable calls downgrade?",
  "org.apache.hadoop.util.functional.LazyAtomicReference:apply()" : "* Implementation of {@code CallableRaisingIOE.apply()}.\n   * Invoke {@link #eval()}.\n   * @return the value\n   * @throws IOException on any evaluation failure",
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getTempFilePath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)" : "* Create temporary file based on the file path retrieved from local dir allocator\n   * instance. The file is created with .bin suffix. The created file has been granted\n   * posix file permissions available in TEMP_FILE_ATTRS.\n   *\n   * @param conf the configuration.\n   * @param localDirAllocator the local dir allocator instance.\n   * @return path of the file created.\n   * @throws IOException if IO error occurs while local dir allocator tries to retrieve path\n   * from local FS or file creation fails or permission set fails.",
  "org.apache.hadoop.io.WritableName:getClass(java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Return the class for a name.\n   * Default is {@link Class#forName(String)}.\n   *\n   * @param name input name.\n   * @param conf input configuration.\n   * @return class for a name.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createSocket()" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.metrics.RpcMetrics:addDeferredRpcProcessingTime(long)" : null,
  "org.apache.hadoop.fs.FileSystem$Cache$Key:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.shell.Display$Checksum:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.QuotaUsage$Builder:typeConsumed(org.apache.hadoop.fs.StorageType,long)" : null,
  "org.apache.hadoop.ipc.FairCallQueue:remainingCapacity()" : "* Returns maximum remaining capacity. This does not reflect how much you can\n   * ideally fit in this FairCallQueue, as that would depend on the scheduler's\n   * decisions.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader:getMetaBlock(java.lang.String)" : "* Stream access to a meta block.``\n     * \n     * @param name\n     *          The name of the meta block.\n     * @return The input stream.\n     * @throws IOException\n     *           on I/O error.\n     * @throws MetaBlockDoesNotExist\n     *           If the meta block with the name does not exist.",
  "org.apache.hadoop.security.KDiag:printConfOpt(java.lang.String)" : "* Print a configuration option, or {@link #UNSET} if unset.\n   *\n   * @param option option to print",
  "org.apache.hadoop.fs.FilterFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint:getMountedOnPath()" : null,
  "org.apache.hadoop.util.ReflectionUtils:getDeclaredFieldsIncludingInherited(java.lang.Class)" : "* Gets all the declared fields of a class including fields declared in\n   * superclasses.\n   *\n   * @param clazz clazz\n   * @return field List",
  "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],long,long)" : "* Constructor with host, name, offset and length.\n   * @param names names array.\n   * @param hosts host array.\n   * @param offset offset.\n   * @param length length.",
  "org.apache.hadoop.fs.ContentSummary:hashCode()" : null,
  "org.apache.hadoop.io.OutputBuffer:<init>()" : "Constructs a new empty buffer.",
  "org.apache.hadoop.fs.impl.StoreImplementationUtils:hasCapability(java.io.InputStream,java.lang.String)" : "* Probe for an input stream having a capability; returns true\n   * if the stream implements {@link StreamCapabilities} and its\n   * {@code hasCapabilities()} method returns true for the capability.\n   * @param in input stream\n   * @param capability capability to probe for\n   * @return true if the stream declares that it supports the capability.",
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor:invokeAll(java.util.Collection,long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.io.BytesWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.net.NodeBase:getName()" : "@return this node's name",
  "org.apache.hadoop.ipc.Client:getExternalHandler()" : null,
  "org.apache.hadoop.io.Text:encode(java.lang.String,boolean)" : "* Converts the provided String to bytes using the\n   * UTF-8 encoding. If <code>replace</code> is true, then\n   * malformed input is replaced with the\n   * substitution character, which is U+FFFD. Otherwise the\n   * method throws a MalformedInputException.\n   *\n   * @param string input string.\n   * @param replace input replace.\n   * @return ByteBuffer: bytes stores at ByteBuffer.array() \n   *                     and length is ByteBuffer.limit()\n   * @throws CharacterCodingException when a character\n   *                                  encoding or decoding error occurs.",
  "org.apache.hadoop.security.AuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)" : "* Initializes hadoop-auth AuthenticationFilter.\n   * <p>\n   * Propagates to hadoop-auth AuthenticationFilter configuration all Hadoop\n   * configuration properties prefixed with \"hadoop.http.authentication.\"\n   *\n   * @param container The filter container\n   * @param conf Configuration for run-time parameters",
  "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:<init>(java.io.OutputStream,int,boolean)" : "* Construct with custom buffer size.\n   *\n   * @param out output stream to buffer\n   * @param size buffer.\n   * @param downgradeSyncable should Syncable calls downgrade?",
  "org.apache.hadoop.io.retry.MultiException:getExceptions()" : null,
  "org.apache.hadoop.io.MapFile$Merger:open(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.net.NetworkTopology:getNumOfRacks()" : "@return the total number of racks",
  "org.apache.hadoop.fs.LocalFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:isValidName(java.lang.String)" : "* Returns true if the specified string is considered valid in the path part\n   * of a URI by this file system.  The default implementation enforces the rules\n   * of HDFS, but subclasses may override this method to implement specific\n   * validation rules for specific file systems.\n   * \n   * @param src String source filename to check, path part of the URI\n   * @return boolean true if the specified string is considered valid",
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.ha.HAServiceProtocol)" : null,
  "org.apache.hadoop.conf.StorageUnit$1:toKBs(double)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider:noPasswordError()" : "* If a password for the provider is needed, but is not provided, this will\n   * return an error message and instructions for supplying said password to\n   * the provider.\n   * @return An error message and instructions for supplying the password",
  "org.apache.hadoop.util.StringUtils:byteToHexString(byte)" : "* Convert a byte to a hex string.\n   * @see #byteToHexString(byte[])\n   * @see #byteToHexString(byte[], int, int)\n   * @param b byte\n   * @return byte's hex value as a String",
  "org.apache.hadoop.io.erasurecode.ECSchema:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.impl.CombinedFileRange:append(org.apache.hadoop.fs.FileRange)" : "* Add a range to the underlying list; update\n   * the {@link #dataSize} field in the process.\n   * @param range range.",
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:<init>(org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState)" : null,
  "org.apache.hadoop.util.bloom.CountingBloomFilter:or(org.apache.hadoop.util.bloom.Filter)" : null,
  "org.apache.hadoop.fs.XAttrCodec:encodeValue(byte[],org.apache.hadoop.fs.XAttrCodec)" : "* Encode byte[] value to string representation with encoding. \n   * Values encoded as text strings are enclosed in double quotes (\\\"), \n   * while strings encoded as hexadecimal and base64 are prefixed with \n   * 0x and 0s, respectively.\n   * @param value byte[] value\n   * @param encoding encoding.\n   * @return String string representation of value\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.impl.FlagSet:toString()" : null,
  "org.apache.hadoop.fs.DF:getUsed()" : "@return the total used space on the filesystem in bytes.",
  "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)" : "* Construct the preferred type of SequenceFile Writer.\n   * @param fs The configured filesystem. \n   * @param conf The configuration.\n   * @param name The name of the file. \n   * @param keyClass The 'key' type.\n   * @param valClass The 'value' type.\n   * @param compressionType The compression type.\n   * @param codec The compression codec.\n   * @param progress The Progressable object to track progress.\n   * @param metadata The metadata of the file.\n   * @return Returns the handle to the constructed SequenceFile Writer.\n   * @throws IOException raised on errors performing I/O.\n   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}\n   *     instead.",
  "org.apache.hadoop.util.ZKUtil$ZKAuthInfo:getAuth()" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:constructReasonString(long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.ipc.FairCallQueue:iterator()" : "* Iterator is not implemented, as it is not needed.",
  "org.apache.hadoop.util.GenericOptionsParser:processGeneralOptions(org.apache.commons.cli.CommandLine)" : "* Modify configuration according user-specified generic options.\n   *\n   * @param line User-specified generic options",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:pathCapabilities_hasPathCapability(java.lang.Object,org.apache.hadoop.fs.Path,java.lang.String)" : "* Does a path have a given capability?\n   * Calls {@code PathCapabilities#hasPathCapability(Path, String)},\n   * mapping IOExceptions to false.\n   * @param fs filesystem\n   * @param path path to query the capability of.\n   * @param capability non-null, non-empty string to query the path for support.\n   * @return true if the capability is supported\n   * under that part of the FS\n   * false if the method is not loaded or the path lacks the capability.\n   * @throws IllegalArgumentException invalid arguments",
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(java.nio.ByteBuffer[],java.nio.ByteBuffer[])" : "* Encode with inputs and generates outputs.\n   *\n   * Note, for both inputs and outputs, no mixing of on-heap buffers and direct\n   * buffers are allowed.\n   *\n   * If the coder option ALLOW_CHANGE_INPUTS is set true (false by default), the\n   * content of input buffers may change after the call, subject to concrete\n   * implementation. Anyway the positions of input buffers will move forward.\n   *\n   * @param inputs input buffers to read data from. The buffers' remaining will\n   *               be 0 after encoding\n   * @param outputs output buffers to put the encoded data into, ready to read\n   *                after the call\n   * @throws IOException if the encoder is closed.",
  "org.apache.hadoop.security.token.DtFileOperations:printCredentials(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text,java.io.PrintStream)" : "Print out a Credentials object.\n   *  @param creds the Credentials object to be printed out.\n   *  @param alias print only tokens matching alias (null matches all).\n   *  @param out print to this stream.\n   *  @throws IOException failure to unmarshall a token identifier.",
  "org.apache.hadoop.ipc.Server$Connection:close()" : null,
  "org.apache.hadoop.util.GcTimeMonitor:run()" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:getFs()" : null,
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:<init>(java.io.InputStream)" : null,
  "org.apache.hadoop.conf.Configuration:writeXml(java.io.Writer)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)" : "* Requests a delegation token using the configured <code>Authenticator</code>\n   * for authentication.\n   *\n   * @param url the URL to get the delegation token from. Only HTTP/S URLs are\n   * supported.\n   * @param token the authentication token being used for the user where the\n   * Delegation token will be stored.\n   * @param renewer the renewer user.\n   * @return a delegation token.\n   * @throws IOException if an IO error occurred.\n   * @throws AuthenticationException if an authentication exception occurred.",
  "org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache()" : "* Re-Login a user in from the ticket cache.  This\n   * method assumes that login had happened already.\n   * The Subject field of this UserGroupInformation object is updated to have\n   * the new credentials.\n   * @throws IOException raised on errors performing I/O.\n   * @throws KerberosAuthException on a failure",
  "org.apache.hadoop.fs.statistics.MeanStatistic:setSamples(long)" : "* Set the sample count.\n   *\n   * If this is less than zero, it is set to zero.\n   * This stops an ill-formed JSON entry from\n   * breaking deserialization, or get an invalid sample count\n   * into an entry.\n   * @param samples sample count.",
  "org.apache.hadoop.fs.FilterFileSystem:msync()" : null,
  "org.apache.hadoop.fs.permission.AclStatus$Builder:group(java.lang.String)" : "* Sets the file group.\n     *\n     * @param group String file group\n     * @return Builder this builder, for call chaining",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.SecureIOUtils$AlreadyExistsException:<init>(java.lang.Throwable)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:dataSize()" : "* Get the amount of data; if there is no buffer then the size is 0.\n       *\n       * @return the amount of data available to upload.",
  "org.apache.hadoop.fs.audit.CommonAuditContext:removeGlobalContextEntry(java.lang.String)" : "* Remove a global entry.\n   * @param key key to clear.",
  "org.apache.hadoop.util.VersionInfo:_getSrcChecksum()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : "* The specification of this method matches that of\n   * {@link FileContext#deleteSnapshot(Path, String)}.\n   *\n   * @param snapshotDir snapshot dir.\n   * @param snapshotName snapshot name.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.FSInputChecker:<init>(org.apache.hadoop.fs.Path,int,boolean,java.util.zip.Checksum,int,int)" : "Constructor\n   * \n   * @param file The name of the file to be read\n   * @param numOfRetries Number of read retries when ChecksumError occurs\n   * @param sum the type of Checksum engine\n   * @param chunkSize maximun chunk size\n   * @param checksumSize the number byte of each checksum\n   * @param verifyChecksum verify check sum.",
  "org.apache.hadoop.fs.viewfs.ViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)" : null,
  "org.apache.hadoop.security.SecurityUtil:setTokenServiceUseIp(boolean)" : "* For use only by tests and initialization.\n   *\n   * @param flag flag.",
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:setInputFromSavedData()" : "* If a write would exceed the capacity of the direct buffers, it is set\n   * aside to be loaded by this function while the compressed data are\n   * consumed.",
  "org.apache.hadoop.io.file.tfile.BCFile$Magic:size()" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod$1:bind(java.lang.Object)" : "* Returns this method as a BoundMethod for the given receiver.\n     * @param receiver an Object to receive the method invocation\n     * @return a {@link BoundMethod} for this method and the receiver\n     * @throws IllegalStateException if the method is static\n     * @throws IllegalArgumentException if the receiver's class is incompatible",
  "org.apache.hadoop.fs.HarFileSystem$HarMetaData:getArchiveIndexTimestamp()" : null,
  "org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:<init>(java.lang.String,java.lang.String)" : "* Create a SharedFileDescriptorFactory.\n   *\n   * @param prefix    Prefix to add to all file names we use.\n   * @param path      Path to use.",
  "org.apache.hadoop.io.compress.CodecPool:returnCompressor(org.apache.hadoop.io.compress.Compressor)" : "* Return the {@link Compressor} to the pool.\n   * \n   * @param compressor the <code>Compressor</code> to be returned to the pool",
  "org.apache.hadoop.ipc.ProcessingDetails:set(org.apache.hadoop.ipc.ProcessingDetails$Timing,long)" : null,
  "org.apache.hadoop.io.InputBuffer:getPosition()" : "* Returns the current position in the input.\n   * @return the current position in the input.",
  "org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorFactory:<init>()" : null,
  "org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:<init>(org.apache.hadoop.fs.CachingGetSpaceUsed,boolean)" : null,
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:setImpl(org.apache.hadoop.metrics2.MetricsSystem)" : null,
  "org.apache.hadoop.util.GSetByHashMap:size()" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invokeStatic(java.lang.Object[])" : "* Invoke a static method.\n     * @param args arguments.\n     * @return result.\n     * @param <R> type of result.",
  "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:name()" : null,
  "org.apache.hadoop.io.file.tfile.MetaBlockAlreadyExists:<init>(java.lang.String)" : "* Constructor\n   * \n   * @param s\n   *          message.",
  "org.apache.hadoop.fs.QuotaUsage:getTypeQuota(org.apache.hadoop.fs.StorageType)" : "* Return storage type quota.\n   *\n   * @param type storage type.\n   * @return type quota.",
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:reset()" : null,
  "org.apache.hadoop.security.SaslRpcServer:create(org.apache.hadoop.ipc.Server$Connection,java.util.Map,org.apache.hadoop.security.token.SecretManager)" : null,
  "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(java.lang.Class,org.apache.hadoop.metrics2.annotation.Metrics)" : null,
  "org.apache.hadoop.ipc.Client$Connection:addCall(org.apache.hadoop.ipc.Client$Call)" : "* Add a call to this connection's call queue and notify\n     * a listener; synchronized.\n     * Returns false if called during shutdown.\n     * @param call to add\n     * @return true if the call was added.",
  "org.apache.hadoop.crypto.key.kms.ValueQueue:readUnlock(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:addArguments(java.util.Deque,int)" : "* Add a specific number of arguments to this expression. The children are\n   * popped off the head of the expressions.\n   *\n   * @param args\n   *          deque of arguments from which to take the argument\n   * @param count\n   *          number of children to be added",
  "org.apache.hadoop.crypto.key.KeyProviderExtension:invalidateCache(java.lang.String)" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:readBlock()" : "Read the next 'compressed' block",
  "org.apache.hadoop.security.JniBasedUnixGroupsMapping:logError(int,java.lang.String)" : "* Log an error message about a group.  Used from JNI.",
  "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$3:unit()" : null,
  "org.apache.hadoop.security.KDiag:dumpKeytab(java.io.File)" : "* Dump a keytab: list all principals.\n   *\n   * @param keytabFile the keytab file\n   * @throws IOException IO problems",
  "org.apache.hadoop.fs.ContentSummary$Builder:build()" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)" : "* Construct a client-side proxy object that implements the named protocol,\n   * talking to a server at the named address. \n   * @param <T> Generics Type.\n   * @param protocol input protocol.\n   * @param clientVersion input clientVersion.\n   * @param addr input addr.\n   * @param ticket input ticket.\n   * @param conf input configuration.\n   * @param factory input factory.\n   * @param rpcTimeout input rpcTimeout.\n   * @param connectionRetryPolicy input connectionRetryPolicy.\n   * @param fallbackToSimpleAuth input fallbackToSimpleAuth.\n   * @param alignmentContext input alignmentContext.\n   * @return ProtocolProxy.",
  "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : "* Create an FSDataOutputStream at the indicated Path with write-progress\n   * reporting.\n   * @param f the file name to open\n   * @param overwrite if a file with this name already exists, then if true,\n   *   the file will be overwritten, and if false an error will be thrown.\n   * @param bufferSize the size of the buffer to be used.\n   * @param replication required block replication for the file.\n   * @param blockSize the size of the buffer to be used.\n   * @param progress to report progress.\n   * @throws IOException IO failure\n   * @return output stream.",
  "org.apache.hadoop.fs.impl.FileRangeImpl:getOffset()" : null,
  "org.apache.hadoop.fs.store.DataBlocks$DataBlock:getStatistics()" : null,
  "org.apache.hadoop.fs.viewfs.ConfigUtil:setIsNestedMountPointSupported(org.apache.hadoop.conf.Configuration,boolean)" : "* Set the bool value isNestedMountPointSupported in config.\n   * @param conf - from this conf\n   * @param isNestedMountPointSupported - whether nested mount point is supported",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:isValueLengthKnown()" : "* Check whether it is safe to call getValueLength().\n         * \n         * @return true if value length is known before hand. Values less than\n         *         the chunk size will always have their lengths known before\n         *         hand. Values that are written out as a whole (with advertised\n         *         length up-front) will always have their lengths known in\n         *         read.",
  "org.apache.hadoop.io.DoubleWritable:hashCode()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:wrap(org.apache.hadoop.fs.statistics.IOStatistics)" : "* Take an IOStatistics instance and wrap it in a source.\n   * @param statistics statistics.\n   * @return a source which will return the values",
  "org.apache.hadoop.fs.shell.find.Find:getRootExpression()" : "* Return the root expression for this find.\n   * \n   * @return the root expression",
  "org.apache.hadoop.util.Sets:addAll(java.util.Collection,java.util.Iterator)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getUri()" : null,
  "org.apache.hadoop.fs.FileChecksum:equals(java.lang.Object)" : "* Return true if both the algorithms and the values are the same.\n   *\n   * @param other other.\n   * @return if equal true, not false.",
  "org.apache.hadoop.jmx.JMXJsonServlet:writeAttribute(com.fasterxml.jackson.core.JsonGenerator,java.lang.String,java.lang.Object)" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:counter(org.apache.hadoop.metrics2.MetricsInfo,long)" : null,
  "org.apache.hadoop.metrics2.source.JvmMetrics:calculateMaxMemoryUsage(java.lang.management.MemoryUsage)" : null,
  "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroSerializer:close()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:lookupQuietly(java.util.Map,java.lang.String)" : "* Get a reference to the map type providing the\n   * value for a specific key, returning null if it not found.\n   * @param <T> type of map/return type.\n   * @param map map to look up\n   * @param key statistic name\n   * @return the value",
  "org.apache.hadoop.crypto.CryptoInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)" : null,
  "org.apache.hadoop.security.User:equals(java.lang.Object)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Writer$DataBlockRegister:register(long,long,long)" : null,
  "org.apache.hadoop.fs.FileContext:setVerifyChecksum(boolean,org.apache.hadoop.fs.Path)" : "* Set the verify checksum flag for the  file system denoted by the path.\n   * This is only applicable if the \n   * corresponding FileSystem supports checksum. By default doesn't do anything.\n   * @param verifyChecksum verify check sum.\n   * @param f set the verifyChecksum for the Filesystem containing this path\n   *\n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If <code>f</code> does not exist\n   * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n   *           not supported\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server",
  "org.apache.hadoop.metrics2.source.JvmMetrics:initSingleton(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.util.LightWeightGSet:actualArrayLength(int)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.io.SortedMapWritable:equals(java.lang.Object)" : null,
  "org.apache.hadoop.metrics2.sink.GraphiteSink:flush()" : null,
  "org.apache.hadoop.fs.FileContext:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])" : "* Create or overwrite file on indicated path and returns an output stream for\n   * writing into the file.\n   * \n   * @param f the file name to open\n   * @param createFlag gives the semantics of create; see {@link CreateFlag}\n   * @param opts file creation options; see {@link Options.CreateOpts}.\n   *          <ul>\n   *          <li>Progress - to report progress on the operation - default null\n   *          <li>Permission - umask is applied against permission: default is\n   *          FsPermissions:getDefault()\n   * \n   *          <li>CreateParent - create missing parent path; default is to not\n   *          to create parents\n   *          <li>The defaults for the following are SS defaults of the file\n   *          server implementing the target path. Not all parameters make sense\n   *          for all kinds of file system - eg. localFS ignores Blocksize,\n   *          replication, checksum\n   *          <ul>\n   *          <li>BufferSize - buffersize used in FSDataOutputStream\n   *          <li>Blocksize - block size for file blocks\n   *          <li>ReplicationFactor - replication for blocks\n   *          <li>ChecksumParam - Checksum parameters. server default is used\n   *          if not specified.\n   *          </ul>\n   *          </ul>\n   * \n   * @return {@link FSDataOutputStream} for created file\n   * \n   * @throws AccessControlException If access is denied\n   * @throws FileAlreadyExistsException If file <code>f</code> already exists\n   * @throws FileNotFoundException If parent of <code>f</code> does not exist\n   *           and <code>createParent</code> is false\n   * @throws ParentNotDirectoryException If parent of <code>f</code> is not a\n   *           directory.\n   * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n   *           not supported\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws\n   *           undeclared exception to RPC server\n   * \n   * RuntimeExceptions:\n   * @throws InvalidPathException If path <code>f</code> is not valid",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:setDictionary(byte[],int,int)" : null,
  "org.apache.hadoop.fs.permission.FsPermission:getUMask(org.apache.hadoop.conf.Configuration)" : "* Get the user file creation mask (umask)\n   * \n   * {@code UMASK_LABEL} config param has umask value that is either symbolic \n   * or octal.\n   * \n   * Symbolic umask is applied relative to file mode creation mask; \n   * the permission op characters '+' clears the corresponding bit in the mask, \n   * '-' sets bits in the mask.\n   * \n   * Octal umask, the specified bits are set in the file mode creation mask.\n   *\n   * @param conf configuration.\n   * @return FsPermission UMask.",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:checkIoStatisticsContextAvailable()" : "* Require IOStatisticsContext methods to be available.\n   * @throws UnsupportedOperationException if the classes/methods were not found",
  "org.apache.hadoop.io.compress.CodecConstants:<init>()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)" : "* Create a new delegation token identifier\n   *\n   * @param kind token kind\n   * @param owner the effective username of the token owner\n   * @param renewer the username of the renewer\n   * @param realUser the real username of the token owner",
  "org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:toString()" : null,
  "org.apache.hadoop.fs.PathIsDirectoryException:<init>(java.lang.String)" : "@param path for the exception",
  "org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:hasNext()" : null,
  "org.apache.hadoop.crypto.CryptoProtocolVersion:getUnknownValue()" : null,
  "org.apache.hadoop.fs.shell.find.Find:registerExpressions(org.apache.hadoop.fs.shell.find.ExpressionFactory)" : "Register the expressions with the expression factory.",
  "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:<init>(long,org.apache.hadoop.io.retry.RetryPolicy$RetryAction,long,java.lang.Exception)" : null,
  "org.apache.hadoop.fs.FileContext:getFileContext(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Create a FileContext for specified default URI using the specified config.\n   * \n   * @param defaultFsUri defaultFsUri.\n   * @param aConf configrution.\n   * @return new FileContext for specified uri\n   * @throws UnsupportedFileSystemException If the file system with specified is\n   *           not supported\n   * @throws RuntimeException If the file system specified is supported but\n   *         could not be instantiated, or if login fails.",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersionInternal(java.lang.String,byte[])" : null,
  "org.apache.hadoop.util.Shell$ExitCodeException:toString()" : null,
  "org.apache.hadoop.io.LongWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:finish()" : null,
  "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:<init>(java.lang.reflect.Constructor,java.lang.Class)" : null,
  "org.apache.hadoop.util.SysInfoLinux:getCpuUsagePercentage()" : "{@inheritDoc}",
  "org.apache.hadoop.io.MapFile$Merger:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)" : "* Merge multiple MapFiles to one Mapfile.\n     *\n     * @param inMapFiles input inMapFiles.\n     * @param deleteInputs deleteInputs.\n     * @param outMapFile input outMapFile.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.FileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : "* Set permission of a path.\n   * @param p The path\n   * @param permission permission\n   * @throws IOException IO failure",
  "org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path,int)" : "* Append to an existing file (optional operation).\n   * Same as append(f, bufferSize, null).\n   * @param f the existing file to be appended.\n   * @param bufferSize the size of the buffer to be used.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).\n   * @return output stream.",
  "org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:getUnderlyingProxyObject()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getUri()" : null,
  "org.apache.hadoop.log.LogLevel$CLI:parseArguments(java.lang.String[])" : null,
  "org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:<init>(byte[])" : "* Constructor for byteArray upload data block. File and uploadStream\n     * would be null.\n     *\n     * @param byteArray byteArray used to construct BlockUploadData.",
  "org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:gracefulFailover()" : null,
  "org.apache.hadoop.metrics2.source.JvmMetrics:getThreadUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder)" : null,
  "org.apache.hadoop.security.FastSaslClientFactory:<init>(java.util.Map)" : null,
  "org.apache.hadoop.util.ExitUtil$HaltException:toString()" : "* String value does not include exception type, just exit code and message.\n     * @return the exit code and any message",
  "org.apache.hadoop.net.unix.DomainSocket:recvFileInputStreams(java.io.FileInputStream[],byte[],int,int)" : "* Receive some FileDescriptor objects from the process on the other side of\n   * this socket, and wrap them in FileInputStream objects.\n   *\n   * @param streams input stream.\n   * @param buf input buf.\n   * @param offset input offset.\n   * @param length input length.\n   * @return wrap them in FileInputStream objects.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.MD5Hash:getDigester()" : "* Create a thread local MD5 digester.\n   * @return MessageDigest.",
  "org.apache.hadoop.io.SecureIOUtils:openFSDataInputStream(java.io.File,java.lang.String,java.lang.String)" : "* Opens the {@link FSDataInputStream} on the requested file on local file\n   * system, verifying the expected user/group constraints if security is\n   * enabled.\n   * @param file absolute path of the file\n   * @param expectedOwner the expected user owner for the file\n   * @param expectedGroup the expected group owner for the file\n   * @throws IOException if an IO Error occurred or the user/group does not\n   * match if security is enabled\n   * @return FSDataInputStream.",
  "org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File,boolean)" : "* Delete the contents of a directory, not the directory itself.  If\n   * we return false, the directory may be partially-deleted.\n   * If dir is a symlink to a directory, all the contents of the actual\n   * directory pointed to by dir will be deleted.\n   *\n   * @param dir dir.\n   * @param tryGrantPermissions if 'true', try grant +rwx permissions to this\n   * and all the underlying directories before trying to delete their contents.\n   * @return fully delete contents status.",
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:<init>()" : null,
  "org.apache.hadoop.util.Lists:computeArrayListCapacity(int)" : null,
  "org.apache.hadoop.fs.ftp.FTPException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:checkTFileDataIndex()" : "* Lazily loading the TFile index.\n     * \n     * @throws IOException",
  "org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.Path)" : "* Opens an FSDataInputStream at the indicated Path.\n   * @param f the file to open\n   * @throws IOException IO failure\n   * @return input stream.",
  "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:getDelegationToken(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.MetricsInfo,java.lang.Object)" : null,
  "org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:remove()" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:finished()" : null,
  "org.apache.hadoop.net.DNSDomainNameResolver:getAllResolvedHostnameByDomainName(java.lang.String,boolean)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String,byte[])" : null,
  "org.apache.hadoop.io.Text:<init>()" : "* Construct an empty text string.",
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getDelegationToken(java.lang.String)" : null,
  "org.apache.hadoop.security.authorize.ProxyServers:refresh(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:login()" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:isBlockCompressed()" : null,
  "org.apache.hadoop.metrics2.lib.MutableRollingAverages$SumAndCount:getCount()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockData:setState(int,org.apache.hadoop.fs.impl.prefetch.BlockData$State)" : "* Sets the state of the given block to the given value.\n   * @param blockNumber the id of the given block.\n   * @param blockState the target state.\n   * @throws IllegalArgumentException if blockNumber is invalid.",
  "org.apache.hadoop.fs.permission.AclEntry:getName()" : "* Returns the optional ACL entry name.\n   *\n   * @return String ACL entry name, or null if undefined",
  "org.apache.hadoop.ipc.ProtobufRpcEngine:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)" : null,
  "org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,int)" : "* This is a drop-in replacement for \n   * {@link Socket#connect(SocketAddress, int)}.\n   * In the case of normal sockets that don't have associated channels, this \n   * just invokes <code>socket.connect(endpoint, timeout)</code>. If \n   * <code>socket.getChannel()</code> returns a non-null channel,\n   * connect is implemented using Hadoop's selectors. This is done mainly\n   * to avoid Sun's connect implementation from creating thread-local \n   * selectors, since Hadoop does not have control on when these are closed\n   * and could end up taking all the available file descriptors.\n   * \n   * @see java.net.Socket#connect(java.net.SocketAddress, int)\n   * \n   * @param socket socket.\n   * @param address the remote address\n   * @param timeout timeout in milliseconds\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processTokenRemoved(org.apache.curator.framework.recipes.cache.ChildData)" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:getClassLoader()" : "* Override point: get the classloader to use.\n   * @return the classloader for loading a service class.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withDurationTracking(java.lang.String[])" : null,
  "org.apache.hadoop.fs.shell.CommandFactory:getInstance(java.lang.String)" : "* Returns an instance of the class implementing the given command.  The\n   * class must have been registered via\n   * {@link #addClass(Class, String...)}\n   * @param cmd name of the command\n   * @return instance of the requested command",
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:buildChecked()" : "* Returns the first valid implementation as a UnboundMethod or throws a\n     * NoSuchMethodException if there is none.\n     * @return a {@link UnboundMethod} with a valid implementation\n     * @throws NoSuchMethodException if no implementation was found",
  "org.apache.hadoop.ipc.Server:setLogSlowRPC(boolean)" : "* Sets slow RPC flag.\n   * @param logSlowRPCFlag input logSlowRPCFlag.",
  "org.apache.hadoop.util.LimitInputStream:mark(int)" : null,
  "org.apache.hadoop.io.compress.CompressorStream:close()" : null,
  "org.apache.hadoop.http.HttpConfig$Policy:fromString(java.lang.String)" : null,
  "org.apache.hadoop.fs.Options$HandleOpt:path()" : "* Handle is valid iff the referent is unmoved in the namespace.\n     * Equivalent to changed(true), moved(false).\n     * @return Options requiring that the referent exist in the same location,\n     * but its content may have changed.",
  "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxyHosts()" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:initSymbols(java.lang.String)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:fromSummary(java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getMountPoints()" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyCompressor:end()" : "* Closes the compressor and discards any unprocessed input.",
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:readChunk(long,byte[],int,int,byte[])" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:getWaitTime(long)" : null,
  "org.apache.hadoop.log.LogThrottlingHelper:<init>(long,java.lang.String,org.apache.hadoop.util.Timer)" : null,
  "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:get()" : null,
  "org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:read()" : null,
  "org.apache.hadoop.metrics2.util.MetricsCache:update(org.apache.hadoop.metrics2.MetricsRecord)" : "* Update the cache and return the current cache record\n   * @param mr the update record\n   * @return the updated cache record",
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)" : null,
  "org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:getProxy()" : null,
  "org.apache.hadoop.metrics2.MetricsTag:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.FileContext:getLocalFSFileContext()" : "* @return a FileContext for the local file system using the default config.\n   * @throws UnsupportedFileSystemException If the file system for\n   *           {@link FsConstants#LOCAL_FS_URI} is not supported.",
  "org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)" : "* Get an instance of the configured TrashPolicy based on the value\n   * of the configuration parameter fs.trash.classname.\n   *\n   * @param conf the configuration to be used\n   * @param fs the file system to be used\n   * @return an instance of TrashPolicy",
  "org.apache.hadoop.fs.statistics.IOStatisticsSupport:<init>()" : null,
  "org.apache.hadoop.fs.PositionedReadable:readVectored(java.util.List,java.util.function.IntFunction)" : "* Read fully a list of file ranges asynchronously from this file.\n   * The default iterates through the ranges to read each synchronously, but\n   * the intent is that FSDataInputStream subclasses can make more efficient\n   * readers.\n   * As a result of the call, each range will have FileRange.setData(CompletableFuture)\n   * called with a future that when complete will have a ByteBuffer with the\n   * data from the file's range.\n   * <p>\n   *   The position returned by getPos() after readVectored() is undefined.\n   * </p>\n   * <p>\n   *   If a file is changed while the readVectored() operation is in progress, the output is\n   *   undefined. Some ranges may have old data, some may have new and some may have both.\n   * </p>\n   * <p>\n   *   While a readVectored() operation is in progress, normal read api calls may block.\n   * </p>\n   * @param ranges the byte ranges to read\n   * @param allocate the function to allocate ByteBuffer\n   * @throws IOException any IOE.\n   * @throws IllegalArgumentException if the any of ranges are invalid, or they overlap.",
  "org.apache.hadoop.http.HttpServer2:addFilter(java.lang.String,java.lang.String,java.util.Map)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators:haltableRemoteIterator(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.CallableRaisingIOE)" : "* Wrap an iterator with one which adds a continuation probe.\n   * This allows work to exit fast without complicated breakout logic\n   * @param iterator source\n   * @param continueWork predicate which will trigger a fast halt if it returns false.\n   * @param <S> source type.\n   * @return a new iterator",
  "org.apache.hadoop.conf.Configuration:getPattern(java.lang.String,java.util.regex.Pattern)" : "* Get the value of the <code>name</code> property as a <code>Pattern</code>.\n   * If no such property is specified, or if the specified value is not a valid\n   * <code>Pattern</code>, then <code>DefaultValue</code> is returned.\n   * Note that the returned value is NOT trimmed by this method.\n   *\n   * @param name property name\n   * @param defaultValue default value\n   * @return property value as a compiled Pattern, or defaultValue",
  "org.apache.hadoop.io.compress.ZStandardCodec:getLibraryName()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getFileChecksum(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.StringUtils:camelize(java.lang.String)" : "* Convert SOME_STUFF to SomeStuff\n   *\n   * @param s input string\n   * @return camelized string",
  "org.apache.hadoop.fs.QuotaUsage:<init>()" : null,
  "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:checkCalls()" : null,
  "org.apache.hadoop.fs.AvroFSInput:tell()" : null,
  "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)" : "* Sort nodes array by their distances to <i>reader</i>.\n   * <p>\n   * This is the same as {@link NetworkTopology#sortByDistance(Node, Node[],\n   * int)} except with a four-level network topology which contains the\n   * additional network distance of a \"node group\" which is between local and\n   * same rack.\n   *\n   * @param reader    Node where data will be read\n   * @param nodes     Available replicas with the requested data\n   * @param activeLen Number of active nodes at the front of the array",
  "org.apache.hadoop.fs.shell.Command:isRecursive()" : null,
  "org.apache.hadoop.fs.shell.find.FindOptions:setCommandFactory(org.apache.hadoop.fs.shell.CommandFactory)" : "* Set the command factory.\n   *\n   * @param factory {@link CommandFactory}",
  "org.apache.hadoop.fs.viewfs.NflyFSystem:repairAndOpen(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode[],org.apache.hadoop.fs.Path,int)" : "* Iterate all available nodes in the proximity order to attempt repair of all\n   * FileNotFound nodes.\n   *\n   * @param mrNodes work set copy of nodes\n   * @param f path to repair and open\n   * @param bufferSize buffer size for read RPC\n   * @return the closest/most recent replica stream AFTER repair",
  "org.apache.hadoop.util.Progress:get()" : null,
  "org.apache.hadoop.ipc.Server$Listener$Reader:run()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMBeanInfo()" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Object[],java.lang.String)" : "* Validates that the given array is not null and has at least one element.\n   * @param <T> the type of array's elements.\n   * @param array the argument reference to validate.\n   * @param argName the name of the argument being validated.",
  "org.apache.hadoop.fs.DelegateToFileSystem:setVerifyChecksum(boolean)" : null,
  "org.apache.hadoop.fs.Options$HandleOpt:getOpt(java.lang.Class,org.apache.hadoop.fs.Options$HandleOpt[])" : "* Utility method to extract a HandleOpt from the set provided.\n     * @param c Target class\n     * @param opt List of options\n     * @param <T> Type constraint for exact match\n     * @throws IllegalArgumentException If more than one matching type is found.\n     * @return An option assignable from the specified type or null if either\n     * opt is null or a suitable match is not found.",
  "org.apache.hadoop.util.StringUtils:join(java.lang.CharSequence,java.lang.Iterable)" : "* Concatenates strings, using a separator.\n   *\n   * @param separator Separator to join with.\n   * @param strings Strings to join.\n   * @return join string.",
  "org.apache.hadoop.io.nativeio.NativeIO$Windows:createDirectoryWithMode(java.io.File,int)" : "* Create a directory with permissions set to the specified mode.  By setting\n     * permissions at creation time, we avoid issues related to the user lacking\n     * WRITE_DAC rights on subsequent chmod calls.  One example where this can\n     * occur is writing to an SMB share where the user does not have Full Control\n     * rights, and therefore WRITE_DAC is denied.\n     *\n     * @param path directory to create\n     * @param mode permissions of new directory\n     * @throws IOException if there is an I/O error",
  "org.apache.hadoop.security.KDiag:isSimpleAuthentication(org.apache.hadoop.conf.Configuration)" : "* Is the authentication method of this configuration \"simple\"?\n   * @param conf configuration to check\n   * @return true if auth is simple (i.e. not kerberos)",
  "org.apache.hadoop.fs.FSDataInputStream:read(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:finished()" : null,
  "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,byte[])" : "* Writes bytes to a file. This utility method opens the file for writing,\n   * creating the file if it does not exist, or overwrites an existing file. All\n   * bytes in the byte array are written to the file.\n   *\n   * @param fs the file system with which to create the file\n   * @param path the path to the file\n   * @param bytes the byte array with the bytes to write\n   *\n   * @return the file system\n   *\n   * @throws NullPointerException if any of the arguments are {@code null}\n   * @throws IOException if an I/O error occurs creating or writing to the file",
  "org.apache.hadoop.fs.FilterFs:getFileChecksum(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:reset()" : "* Resets everything including the input buffers (user and direct).",
  "org.apache.hadoop.fs.EmptyStorageStatistics:<init>(java.lang.String)" : null,
  "org.apache.hadoop.security.UserGroupInformation$LoginParams:put(org.apache.hadoop.security.UserGroupInformation$LoginParam,java.lang.String)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX:mlock(java.nio.ByteBuffer,long)" : "* Locks the provided direct ByteBuffer into memory, preventing it from\n     * swapping out. After a buffer is locked, future accesses will not incur\n     * a page fault.\n     * \n     * See the mlock(2) man page for more information.\n     * \n     * @throws NativeIOException",
  "org.apache.hadoop.net.SocketInputStream:isOpen()" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String,boolean,boolean)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)" : null,
  "org.apache.hadoop.io.serializer.SerializationFactory:getSerialization(java.lang.Class)" : null,
  "org.apache.hadoop.util.functional.Tuples$Tuple:toString()" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail:shouldRetry(java.lang.Exception,int,int,boolean)" : null,
  "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:keySet()" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:writeFloatArray(java.io.DataOutput)" : null,
  "org.apache.hadoop.util.DirectBufferPool:getBuffer(int)" : "* Allocate a direct buffer of the specified size, in bytes.\n   * If a pooled buffer is available, returns that. Otherwise\n   * allocates a new one.\n   *\n   * @param size size.\n   * @return ByteBuffer.",
  "org.apache.hadoop.fs.viewfs.InodeTree:getMountPoints()" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:warmUpEncryptedKeys(java.lang.String[])" : null,
  "org.apache.hadoop.fs.FilterFs:getFsStatus()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockReader(int)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,org.apache.hadoop.conf.Configuration,java.net.InetAddress)" : "* Authorize the user to access the protocol being used.\n   * \n   * @param user user accessing the service \n   * @param protocol service being accessed\n   * @param conf configuration to use\n   * @param addr InetAddress of the client\n   * @throws AuthorizationException on authorization failure",
  "org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler:<init>(java.nio.channels.AsynchronousFileChannel,java.util.List,java.nio.ByteBuffer[])" : null,
  "org.apache.hadoop.fs.FsShell:printUsage(java.io.PrintStream,java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.kms.ValueQueue:<init>(int,float,long,int,org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller)" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : "* A stream obtained via this call must be closed before using other APIs of\n   * this class or else the invocation will block.",
  "org.apache.hadoop.fs.shell.find.BaseExpression:addArgument(java.lang.String)" : "* Add a single argument to this expression. The argument is popped off the\n   * head of the expressions.\n   *\n   * @param arg\n   *          argument to add to the expression",
  "org.apache.hadoop.net.CachedDNSToSwitchMapping:toString()" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:bufferSize(int)" : "* Create an option with the buffer size for reading the given pathname.\n     * @param value the number of bytes to buffer\n     * @return a new option",
  "org.apache.hadoop.util.JsonSerialization:writeJsonAsBytes(java.lang.Object,java.io.OutputStream)" : "* Write the JSON as bytes, then close the stream.\n   * @param instance instance to write\n   * @param dataOutputStream an output stream that will always be closed\n   * @throws IOException on any failure",
  "org.apache.hadoop.fs.ChecksumFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.service.AbstractService:registerServiceListener(org.apache.hadoop.service.ServiceStateChangeListener)" : null,
  "org.apache.hadoop.fs.FileSystemStorageStatistics:fetch(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData,java.lang.String)" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMaximum(java.lang.String,long)" : null,
  "org.apache.hadoop.ipc.RpcWritable$Buffer:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.SaslPropertiesResolver:getConf()" : null,
  "org.apache.hadoop.fs.permission.PermissionStatus:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.security.alias.JavaKeyStoreProvider:getSchemeName()" : null,
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementMaximum(java.lang.String,long)" : null,
  "org.apache.hadoop.ipc.RPC$Builder:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setMaxDate(long)" : null,
  "org.apache.hadoop.tools.GetGroupsBase:getUgmProtocol()" : "* Get a client of the {@link GetUserMappingsProtocol}.\n   * @return A {@link GetUserMappingsProtocol} client proxy.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.StringUtils:join(char,java.lang.Iterable)" : null,
  "org.apache.hadoop.security.token.DtUtilShell$Import:getUsage()" : null,
  "org.apache.hadoop.net.SocksSocketFactory:createSocket(java.net.InetAddress,int)" : null,
  "org.apache.hadoop.ipc.RetryCache$CacheEntry:hashCode(long)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getDelegationToken(javax.servlet.http.HttpServletRequest)" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:getWorkingDirectory()" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$SortPass$SeqFileComparator:compare(org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable)" : null,
  "org.apache.hadoop.fs.FSInputChecker:needChecksum()" : "* Return true if there is a need for checksum verification.\n   * @return if there is a need for checksum verification true, not false.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$MountPathInfo:<init>(org.apache.hadoop.fs.Path,java.lang.Object)" : null,
  "org.apache.hadoop.util.Options$LongOption:<init>(long)" : null,
  "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroDeserializer:open(java.io.InputStream)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:setPrevious(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)" : "* Returns an authenticated {@link HttpURLConnection}, it uses a Delegation\n   * Token only if the given auth token is an instance of {@link Token} and\n   * it contains a Delegation Token, otherwise use the configured\n   * {@link DelegationTokenAuthenticator} to authenticate the connection.\n   *\n   * @param url the URL to connect to. Only HTTP/S URLs are supported.\n   * @param token the authentication token being used for the user.\n   * @return an authenticated {@link HttpURLConnection}.\n   * @throws IOException if an IO error occurred.\n   * @throws AuthenticationException if an authentication exception occurred.",
  "org.apache.hadoop.fs.FileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])" : "* Concat existing files together.\n   * @param trg the path to the target destination.\n   * @param psrcs the paths to the sources to use for the concatenation.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).",
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:performCoding(java.nio.ByteBuffer[],java.nio.ByteBuffer[])" : null,
  "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:invokeChecked(java.lang.Object,java.lang.Object[])" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.MD5Hash:digest(byte[],int,int)" : "* Construct a hash value for a byte array.\n   * @param data data.\n   * @param start start.\n   * @param len len.\n   * @return MD5Hash.",
  "org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)" : "* Get a protocol proxy that contains a proxy connection to a remote server\n   * and a set of methods that are supported by the server.\n   *\n   * @param <T> Generics Type T.\n   * @param protocol protocol class\n   * @param clientVersion client version\n   * @param addr remote address\n   * @param conf configuration to use\n   * @return the protocol proxy\n   * @throws IOException if the far end through a RemoteException",
  "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce()" : "Invoke the call once without retrying.",
  "org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)" : "* Get a FileSystem instance based on the uri, the passed in\n   * configuration and the user.\n   * @param uri of the filesystem\n   * @param conf the configuration to use\n   * @param user to perform the get as\n   * @return the filesystem instance\n   * @throws IOException failure to load\n   * @throws InterruptedException If the {@code UGI.doAs()} call was\n   * somehow interrupted.",
  "org.apache.hadoop.ipc.Server:getClientBackoffEnable(java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Get from config if client backoff is enabled on that port.",
  "org.apache.hadoop.io.compress.zlib.ZlibFactory:isNativeZlibLoaded(org.apache.hadoop.conf.Configuration)" : "* Check if native-zlib code is loaded &amp; initialized correctly and\n   * can be loaded for this job.\n   * \n   * @param conf configuration\n   * @return <code>true</code> if native-zlib is loaded &amp; initialized\n   *         and can be loaded for this job, else <code>false</code>",
  "org.apache.hadoop.conf.StorageUnit$4:getDefault(double)" : null,
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:close()" : null,
  "org.apache.hadoop.fs.permission.AclEntry:toStringStable()" : "* Returns a string representation guaranteed to be stable across versions to\n   * satisfy backward compatibility requirements, such as for shell command\n   * output or serialization.  The format of this string representation matches\n   * what is expected by the {@link #parseAclSpec(String, boolean)} and\n   * {@link #parseAclEntry(String, boolean)} methods.\n   *\n   * @return stable, backward compatible string representation",
  "org.apache.hadoop.util.CpuTimeTracker:getCpuTrackerUsagePercent()" : "* Return percentage of cpu time spent over the time since last update.\n   * CPU time spent is based on elapsed jiffies multiplied by amount of\n   * time for 1 core. Thus, if you use 2 cores completely you would have spent\n   * twice the actual time between updates and this will return 200%.\n   *\n   * @return Return percentage of cpu usage since last update, {@link\n   * CpuTimeTracker#UNAVAILABLE} if there haven't been 2 updates more than\n   * {@link CpuTimeTracker#minimumTimeInterval} apart",
  "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:<init>(java.io.File,int,long,org.apache.hadoop.fs.store.BlockUploadStatistics)" : null,
  "org.apache.hadoop.conf.Configuration:addResource(java.net.URL)" : "* Add a configuration resource. \n   * \n   * The properties of this resource will override properties of previously \n   * added resources, unless they were marked <a href=\"#Final\">final</a>. \n   * \n   * @param url url of the resource to be added, the local filesystem is \n   *            examined directly to find the resource, without referring to \n   *            the classpath.",
  "org.apache.hadoop.net.SocksSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)" : null,
  "org.apache.hadoop.security.UserGroupInformation$RealUser:getName()" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:getRawSize()" : "* Get the raw size of the block.\n       * \n       * @return the number of uncompressed bytes written through the\n       *         BlockAppender so far.\n       * @throws IOException",
  "org.apache.hadoop.util.JsonSerialization:save(java.io.File,java.lang.Object)" : "* Save to a local file. Any existing file is overwritten unless\n   * the OS blocks that.\n   * @param file file\n   * @param instance instance\n   * @throws IOException IO exception",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setBlockSize(org.apache.hadoop.conf.Configuration,int)" : null,
  "org.apache.hadoop.metrics2.source.JvmMetrics$Singleton:init(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:commit()" : null,
  "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],boolean,int,org.apache.hadoop.fs.Path)" : "* Merges the contents of files passed in Path[]\n     * @param inNames the array of path names\n     * @param deleteInputs true if the input files should be deleted when \n     * unnecessary\n     * @param factor the factor that will be used as the maximum merge fan-in\n     * @param tmpDir the directory to write temporary files into\n     * @return RawKeyValueIteratorMergeQueue\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry$Pair:<init>(int,int)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:getReturnProtoType(java.lang.reflect.Method)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:needsInput()" : null,
  "org.apache.hadoop.fs.FSDataOutputStream$PositionCache:write(byte[],int,int)" : null,
  "org.apache.hadoop.ha.ZKFCRpcServer:gracefulFailover()" : null,
  "org.apache.hadoop.security.token.DelegationTokenIssuer:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)" : "* Given a renewer, add delegation tokens for issuer and it's child issuers\n   * to the <code>Credentials</code> object if it is not already present.\n   *<p>\n   * Note: This method is not intended to be overridden.  Issuers should\n   * implement getCanonicalService and getDelegationToken to ensure\n   * consistent token acquisition behavior.\n   *\n   * @param renewer the user allowed to renew the delegation tokens\n   * @param credentials cache in which to add new delegation tokens\n   * @return list of new delegation tokens\n   * @throws IOException thrown if IOException if an IO error occurs.",
  "org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)" : "* Propagate options to any builder.\n   * {@link FutureIO#propagateOptions(FSBuilder, Configuration, String, boolean)}\n   * @param builder builder to modify\n   * @param conf configuration to read\n   * @param prefix prefix to scan/strip\n   * @param mandatory are the options to be mandatory or optional?",
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:getRemaining()" : "* Returns <code>0</code>.\n   *\n   * @return <code>0</code>.",
  "org.apache.hadoop.crypto.CryptoInputStream:skip(long)" : "Skip n bytes",
  "org.apache.hadoop.conf.StorageUnit$7:toPBs(double)" : null,
  "org.apache.hadoop.conf.Configuration:unset(java.lang.String)" : "* Unset a previously set property.\n   * @param name the property name",
  "org.apache.hadoop.fs.FilterFs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.util.DataChecksum:update(int)" : null,
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:close()" : null,
  "org.apache.hadoop.util.HostsFileReader:readFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:findChecksumRanges(java.util.List,int,int,int)" : "* Find the checksum ranges that correspond to the given data ranges.\n     * @param dataRanges the input data ranges, which are assumed to be sorted\n     *                   and non-overlapping\n     * @return a list of AsyncReaderUtils.CombinedFileRange that correspond to\n     *         the checksum ranges",
  "org.apache.hadoop.security.LdapGroupsMapping:goUpGroupHierarchy(java.util.Set,int,java.util.Set)" : null,
  "org.apache.hadoop.net.NetUtils:getAllStaticResolutions()" : "* This is used to get all the resolutions that were added using\n   * {@link NetUtils#addStaticResolution(String, String)}. The return\n   * value is a List each element of which contains an array of String \n   * of the form String[0]=hostname, String[1]=resolved-hostname\n   * @return the list of resolutions",
  "org.apache.hadoop.util.StringUtils:getStringCollection(java.lang.String,java.lang.String)" : "* Returns a collection of strings.\n   * \n   * @param str\n   *          String to parse\n   * @param delim\n   *          delimiter to separate the values\n   * @return Collection of parsed elements.",
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:preferDirectBuffer()" : "* Tell if direct buffer is preferred or not. It's for callers to\n   * decide how to allocate coding chunk buffers, using DirectByteBuffer or\n   * bytes array. It will return false by default.\n   * @return true if native buffer is preferred for performance consideration,\n   * otherwise false.",
  "org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)" : "* Return time duration in the given time unit. Valid units are encoded in\n   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds\n   * (ms), seconds (s), minutes (m), hours (h), and days (d).\n   *\n   * @param name Property name\n   * @param defaultValue Value returned if no mapping exists.\n   * @param unit Unit to convert the stored property, if it exists.\n   * @throws NumberFormatException If the property stripped of its unit is not\n   *         a number\n   * @return time duration in given time unit",
  "org.apache.hadoop.ipc.Server$RpcKindMapValue:<init>(java.lang.Class,org.apache.hadoop.ipc.RPC$RpcInvoker)" : null,
  "org.apache.hadoop.security.SecurityUtil:getHostFromPrincipal(java.lang.String)" : "* Get the host name from the principal name of format {@literal <}service\n   * {@literal >}/host@realm.\n   * @param principalName principal name of format as described above\n   * @return host name if the the string conforms to the above format, else null",
  "org.apache.hadoop.io.MD5Hash:toString()" : "Returns a string representation of this object.",
  "org.apache.hadoop.net.StandardSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.fs.local.RawLocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* This constructor has the signature needed by\n   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.\n   * \n   * @param theUri which must be that of localFs\n   * @param conf\n   * @throws IOException\n   * @throws URISyntaxException",
  "org.apache.hadoop.conf.Configuration:appendJSONProperty(com.fasterxml.jackson.core.JsonGenerator,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.conf.ConfigRedactor)" : "* Write property and its attributes as json format to given\n   * {@link JsonGenerator}.\n   *\n   * @param jsonGen json writer\n   * @param config configuration\n   * @param name property name\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)" : "* Create a proxy for an interface of implementations of that interface using\n   * the given {@link FailoverProxyProvider} and the same retry policy for each\n   * method in the interface.\n   * \n   * @param iface the interface that the retry will implement\n   * @param proxyProvider provides implementation instances whose methods should be retried\n   * @param retryPolicy the policy for retrying or failing over method call failures\n   * @param <T> T.\n   * @return the retry proxy",
  "org.apache.hadoop.util.VersionInfo:_getProtocVersion()" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:exit(org.apache.hadoop.util.ExitUtil$ExitException)" : "* Exit the JVM using an exception for the exit code and message,\n   * invoking {@link ExitUtil#terminate(ExitUtil.ExitException)}.\n   *\n   * This is the standard way a launched service exits.\n   * An error code of 0 means success -nothing is printed.\n   *\n   * If {@link ExitUtil#disableSystemExit()} has been called, this\n   * method will throw the exception.\n   *\n   * The method <i>may</i> be subclassed for testing\n   * @param ee exit exception\n   * @throws ExitUtil.ExitException if ExitUtil exceptions are disabled",
  "org.apache.hadoop.fs.impl.prefetch.BlockManager:requestPrefetch(int)" : "* Requests optional prefetching of the given block.\n   *\n   * @param blockNumber the id of the block to prefetch.\n   *\n   * @throws IllegalArgumentException if blockNumber is negative.",
  "org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration,boolean)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>()" : null,
  "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:collectThreadLocalStates()" : "* Collects states maintained in {@link ThreadLocal}, if any.",
  "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:<init>(javax.servlet.http.HttpServletRequest)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addLink(java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)" : null,
  "org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String,byte,boolean)" : null,
  "org.apache.hadoop.fs.impl.StoreImplementationUtils:<init>()" : null,
  "org.apache.hadoop.util.Shell:destroyAllShellProcesses()" : "* Static method to destroy all running <code>Shell</code> processes.\n   * Iterates through a map of all currently running <code>Shell</code>\n   * processes and destroys them one by one. This method is thread safe",
  "org.apache.hadoop.ipc.Server$RpcCall:setResponseFields(org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Server$RpcCall$ResponseParams)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider:rollNewVersion(java.lang.String)" : "* Roll a new version of the given key generating the material for it.\n   * <p>\n   * This implementation generates the key material and calls the\n   * {@link #rollNewVersion(String, byte[])} method.\n   *\n   * @param name the basename of the key\n   * @return the name of the new version of the key\n   * @throws IOException              raised on errors performing I/O.\n   * @throws NoSuchAlgorithmException This exception is thrown when a particular\n   *                                  cryptographic algorithm is requested\n   *                                  but is not available in the environment.",
  "org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:setSearchDomains(java.lang.String[])" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:writeObject(java.io.ObjectOutputStream)" : null,
  "org.apache.hadoop.crypto.key.kms.ValueQueue:getNext(java.lang.String)" : "* This removes the value currently at the head of the Queue for the\n   * provided key. Will immediately fire the Queue filler function if key\n   * does not exist.\n   * If Queue exists but all values are drained, It will ask the generator\n   * function to add 1 value to Queue and then drain it.\n   * @param keyName String key name\n   * @return E the next value in the Queue\n   * @throws IOException raised on errors performing I/O.\n   * @throws ExecutionException executionException.",
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(int,int)" : "* Creates a new compressor with the default compression level.\n   * Compressed data will be generated in ZStandard format.\n   * @param level level.\n   * @param bufferSize bufferSize.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:delete(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.fs.shell.Ls:adjustColumnWidths(org.apache.hadoop.fs.shell.PathData[])" : "* Compute column widths and rebuild the format string\n   * @param items to find the max field width for each column",
  "org.apache.hadoop.security.SaslRpcClient:getNegotiatedProperty(java.lang.String)" : null,
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:buffer()" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Convenience method, so that we don't open a new connection when using this\n   * method from within another method. Otherwise every API invocation incurs\n   * the overhead of opening/closing a TCP connection.\n   *\n   * @param channel\n   * @param src\n   * @param dst\n   * @return rename successful?\n   * @throws IOException",
  "org.apache.hadoop.net.InnerNodeImpl:getChildren()" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:getCompressedSize()" : "* Get the compressed size of the block in progress.\n       * \n       * @return the number of compressed bytes written to the underlying FS\n       *         file. The size may be smaller than actual need to compress the\n       *         all data written due to internal buffering inside the\n       *         compressor.\n       * @throws IOException",
  "org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:sourceHasNext()" : null,
  "org.apache.hadoop.fs.shell.Command:setRecursive(boolean)" : null,
  "org.apache.hadoop.conf.Configuration$DeprecationContext:getReverseDeprecatedKeyMap()" : null,
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:getEncryptedKeyVersion()" : "* @return The encrypted encryption key version.",
  "org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPartHandles(java.util.Map)" : "* Utility method to validate partHandles.\n   * @param partHandles handles\n   * @throws IllegalArgumentException if the parts are invalid",
  "org.apache.hadoop.util.HeapSort:sort(org.apache.hadoop.util.IndexedSortable,int,int)" : "* Sort the given range of items using heap sort.\n   * {@inheritDoc}",
  "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:withFileStatus(org.apache.hadoop.fs.FileStatus)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTrackingIdIfEnabled(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:checkScheme(java.net.URI,java.lang.String)" : "* Check that the Uri's scheme matches.\n   *\n   * @param uri name URI of the FS.\n   * @param supportedScheme supported scheme.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:incrementCurrentKeyId()" : "* For subclasses externalizing the storage, for example Zookeeper\n   * based implementations.\n   *\n   * @return currentId.",
  "org.apache.hadoop.ipc.CallerContext$Builder:setSignature(byte[])" : null,
  "org.apache.hadoop.fs.HarFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : "* not implemented.",
  "org.apache.hadoop.ipc.Client$ConnectionId:getConnectionId(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)" : "* Returns a ConnectionId object. \n     * @param addr Remote address for the connection.\n     * @param protocol Protocol for RPC.\n     * @param ticket UGI\n     * @param rpcTimeout timeout\n     * @param conf Configuration object\n     * @return A ConnectionId instance\n     * @throws IOException",
  "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.ipc.Server:getNumInProcessHandler()" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:compress(byte[],int,int)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter:sortAndIterate(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)" : "* Perform a file sort from a set of input files and return an iterator.\n     * @param inFiles the files to be sorted\n     * @param tempDir the directory where temp files are created during sort\n     * @param deleteInput should the input files be deleted as they are read?\n     * @return iterator the RawKeyValueIterator\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.ZStandardCodec:getDecompressionBufferSize(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersions(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto)" : null,
  "org.apache.hadoop.fs.HarFileSystem$LruCache:removeEldestEntry(java.util.Map$Entry)" : null,
  "org.apache.hadoop.fs.VectoredReadUtils:validateRangeRequest(org.apache.hadoop.fs.FileRange)" : "* Validate a single range.\n   * @param range range to validate.\n   * @return the range.\n   * @param <T> range type\n   * @throws IllegalArgumentException the range length is negative or other invalid condition\n   * is met other than the those which raise EOFException or NullPointerException.\n   * @throws EOFException the range offset is negative\n   * @throws NullPointerException if the range is null.",
  "org.apache.hadoop.metrics2.lib.MethodMetric:metricInfo(java.lang.reflect.Method)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider$TimeoutConnConfigurator:<init>(int,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)" : "* Sets the timeout and wraps another connection configurator\n     * @param timeout - will set both connect and read timeouts - in seconds\n     * @param cc - another configurator to wrap - may be null",
  "org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:next()" : null,
  "org.apache.hadoop.util.functional.RemoteIterators:remoteIteratorFromArray(java.lang.Object[])" : "* Create a remote iterator from an array.\n   * @param <T> type\n   * @param array array.\n   * @return a remote iterator",
  "org.apache.hadoop.util.StringUtils:startupShutdownMessage(java.lang.Class,java.lang.String[],org.slf4j.Logger)" : "* Print a log message for starting up and shutting down\n   * @param clazz the class of the server\n   * @param args arguments\n   * @param log the target log object",
  "org.apache.hadoop.util.BlockingThreadPoolExecutorService:newInstance(int,int,long,java.util.concurrent.TimeUnit,java.lang.String)" : "* A thread pool that that blocks clients submitting additional tasks if\n   * there are already {@code activeTasks} running threads and {@code\n   * waitingTasks} tasks waiting in its queue.\n   *\n   * @param activeTasks maximum number of active tasks\n   * @param waitingTasks maximum number of waiting tasks\n   * @param keepAliveTime time until threads are cleaned up in {@code unit}\n   * @param unit time unit\n   * @param prefixName prefix of name for threads\n   * @return BlockingThreadPoolExecutorService.",
  "org.apache.hadoop.security.UserGroupInformation:reset()" : null,
  "org.apache.hadoop.util.Shell:setWorkingDirectory(java.io.File)" : "* Set the working directory.\n   * @param dir The directory where the command will be executed",
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getBlockSize()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:<init>(java.io.DataInput)" : null,
  "org.apache.hadoop.metrics2.MetricsFilter:accepts(org.apache.hadoop.metrics2.MetricsRecord)" : "* Whether to accept the record\n   * @param record  to filter on\n   * @return  true to accept; false otherwise.",
  "org.apache.hadoop.metrics2.MetricsJsonBuilder:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)" : null,
  "org.apache.hadoop.fs.HarFileSystem:getWorkingDirectory()" : "* return the top level archive.",
  "org.apache.hadoop.ha.ActiveStandbyElector:allowSessionReestablishmentForTests()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:startMBeans()" : null,
  "org.apache.hadoop.fs.CachingGetSpaceUsed:getRefreshInterval()" : "* How long in between runs of the background refresh.\n   *\n   * @return refresh interval.",
  "org.apache.hadoop.fs.BatchedRemoteIterator:makeRequestIfNeeded()" : null,
  "org.apache.hadoop.net.SocksSocketFactory:setProxy(java.lang.String)" : "* Set the proxy of this socket factory as described in the string\n   * parameter\n   * \n   * @param proxyStr the proxy address using the format \"host:port\"",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:requireAllMethodsAvailable()" : "* For testing: verify that all methods were found.\n   * @throws UnsupportedOperationException if the method was not found.",
  "org.apache.hadoop.metrics2.impl.MetricCounterInt:visit(org.apache.hadoop.metrics2.MetricsVisitor)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:readUShortLE(byte[],int)" : null,
  "org.apache.hadoop.security.KDiag:verify(boolean,java.lang.String,java.lang.String,java.lang.Object[])" : "* Assert that a condition must hold.\n   *\n   * If not, an exception is raised, or, if {@link #nofail} is set,\n   * an error will be logged and the method return false.\n   *\n   * @param condition condition which must hold\n   * @param category category for exception\n   * @param message string formatting message\n   * @param args any arguments for the formatting\n   * @return true if the verification succeeded, false if it failed but\n   * an exception was not raised.\n   * @throws KerberosDiagsFailure containing the formatted text\n   *         if the condition was met",
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:end()" : null,
  "org.apache.hadoop.ipc.FairCallQueue:drainTo(java.util.Collection)" : null,
  "org.apache.hadoop.net.NetUtils:addMatchingAddrs(java.net.NetworkInterface,org.apache.commons.net.util.SubnetUtils$SubnetInfo,java.util.List)" : "* Add all addresses associated with the given nif in the\n   * given subnet to the given list.",
  "org.apache.hadoop.ha.HAServiceTarget:getTransitionTargetHAStatus()" : null,
  "org.apache.hadoop.fs.shell.find.ExpressionFactory:<init>()" : "* Private constructor to ensure singleton.",
  "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:add(org.apache.hadoop.metrics2.MetricsTag)" : null,
  "org.apache.hadoop.net.NetUtils:getConnectAddress(java.net.InetSocketAddress)" : "* Returns an InetSocketAddress that a client can use to connect to the\n   * given listening address.\n   * \n   * @param addr of a listener\n   * @return socket address that a client can use to connect to the server.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addTimedOperation(java.lang.String,long)" : "* Add a duration to the min/mean/max statistics, using the\n   * given prefix and adding a suffix for each specific value.\n   * <p>\n   * The update is non -atomic, even though each individual statistic\n   * is updated thread-safely. If two threads update the values\n   * simultaneously, at the end of each operation the state will\n   * be correct. It is only during the sequence that the statistics\n   * may be observably inconsistent.\n   * </p>\n   * @param prefix statistic prefix\n   * @param durationMillis duration in milliseconds.",
  "org.apache.hadoop.metrics2.lib.MutableRollingAverages:collectThreadLocalStates()" : "* Collects states maintained in {@link ThreadLocal}, if any.",
  "org.apache.hadoop.fs.CommonPathCapabilities:<init>()" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:checkCreateXorRawEncoder()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.Throwable)" : null,
  "org.apache.hadoop.io.EnumSetWritable:<init>()" : null,
  "org.apache.hadoop.security.alias.CredentialProvider$CredentialEntry:getCredential()" : null,
  "org.apache.hadoop.fs.BatchedRemoteIterator$BatchedListEntries:hasMore()" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:seekToNewSource(long)" : null,
  "org.apache.hadoop.net.DomainNameResolverFactory:<init>()" : null,
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:close()" : null,
  "org.apache.hadoop.http.HttpServer2:getFilterInitializers(org.apache.hadoop.conf.Configuration)" : "Get an array of FilterConfiguration specified in the conf",
  "org.apache.hadoop.fs.permission.FsPermission:getMasked()" : "* Get masked permission if exists.\n   * @return masked.",
  "org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.fs.FileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)" : "* Return the number of bytes that large input files should be optimally\n   * be split into to minimize I/O time.  The given path will be used to\n   * locate the actual filesystem.  The full path does not have to exist.\n   * @param f path of file\n   * @return the default block size for the path's filesystem",
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:checkStream()" : null,
  "org.apache.hadoop.fs.LocatedFileStatus:getBlockLocations()" : "* Get the file's block locations\n   *\n   * In HDFS, the returned BlockLocation will have different formats for\n   * replicated and erasure coded file.\n   * Please refer to\n   * {@link FileSystem#getFileBlockLocations(FileStatus, long, long)}\n   * for more details.\n   *\n   * @return the file's block locations",
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.String,java.lang.String,java.lang.Class[])" : "* Checks for an implementation, first finding the given class by name.\n     * @param className name of a class\n     * @param methodName name of a method (different from constructor)\n     * @param argClasses argument classes for the method\n     * @return this Builder for method chaining",
  "org.apache.hadoop.crypto.key.kms.ValueQueue:writeUnlock(java.lang.String)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* The src file is on the local disk.  Add it to FS at\n   * the given dst name.\n   * delSrc indicates if the source should be removed",
  "org.apache.hadoop.io.compress.snappy.SnappyCompressor:finish()" : "* When called, indicates that compression should end\n   * with the current contents of the input buffer.",
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:multiply(int[],int[])" : "* Compute the multiplication of two polynomials. The index in the array\n   * corresponds to the power of the entry. For example p[0] is the constant\n   * term of the polynomial p.\n   *\n   * @param p input polynomial\n   * @param q input polynomial\n   * @return polynomial represents p*q",
  "org.apache.hadoop.fs.FileEncryptionInfo:<init>(org.apache.hadoop.crypto.CipherSuite,org.apache.hadoop.crypto.CryptoProtocolVersion,byte[],byte[],java.lang.String,java.lang.String)" : "* Create a FileEncryptionInfo.\n   *\n   * @param suite CipherSuite used to encrypt the file\n   * @param edek encrypted data encryption key (EDEK) of the file\n   * @param iv initialization vector (IV) used to encrypt the file\n   * @param keyName name of the key used for the encryption zone\n   * @param ezKeyVersionName name of the KeyVersion used to encrypt the\n   *                         encrypted data encryption key.\n   * @param version version.",
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getInstance(int,int)" : "* Get the object performs Galois field arithmetics.\n   *\n   * @param fieldSize           size of the field\n   * @param primitivePolynomial a primitive polynomial corresponds to the size\n   * @return GaloisField.",
  "org.apache.hadoop.util.Progress:getInternal()" : "Computes progress in this node.",
  "org.apache.hadoop.fs.FileContext:getFileChecksum(org.apache.hadoop.fs.Path)" : "* Get the checksum of a file.\n   *\n   * @param f file path\n   *\n   * @return The file checksum.  The default return value is null,\n   *  which indicates that no checksum algorithm is implemented\n   *  in the corresponding FileSystem.\n   *\n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If <code>f</code> does not exist\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server",
  "org.apache.hadoop.ipc.Server$RpcCall:populateResponseParamsOnError(java.lang.Throwable,org.apache.hadoop.ipc.Server$RpcCall$ResponseParams)" : "* @param t              the {@link java.lang.Throwable} to use to set\n     *                       errorInfo\n     * @param responseParams the {@link ResponseParams} instance to populate",
  "org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.permission.PermissionStatus$2:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.FilterFs:getMyFs()" : null,
  "org.apache.hadoop.util.DataChecksum:writeValue(byte[],int,boolean)" : "* Writes the current checksum to a buffer.\n    * If <i>reset</i> is true, then resets the checksum.\n    *\n    * @param buf buf.\n    * @param offset offset.\n    * @param reset reset.\n    * @return number of bytes written. Will be equal to getChecksumSize();\n    * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.InstrumentedLock:tryLock()" : null,
  "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:shutdown()" : null,
  "org.apache.hadoop.io.compress.CompressorStream:write(byte[],int,int)" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:responses1xx()" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:getCompressionType()" : "* Get the compression type for this file.\n     * @return the compression type",
  "org.apache.hadoop.fs.FilterFileSystem:getCanonicalUri()" : null,
  "org.apache.hadoop.net.SocketInputWrapper:<init>(java.net.Socket,java.io.InputStream)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig)" : "* Returns the proxyuser configuration. All returned properties must start\n   * with <code>proxyuser.</code>'\n   * <p>\n   * Subclasses may override this method if the proxyuser configuration is \n   * read from other place than the filter init parameters.\n   *\n   * @param filterConfig filter configuration object\n   * @return the proxyuser configuration properties.\n   * @throws ServletException thrown if the configuration could not be created.",
  "org.apache.hadoop.fs.viewfs.ViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor$RunnableWithPermitRelease:run()" : null,
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:handleExecutorTimeout(org.apache.hadoop.util.Shell$ShellCommandExecutor,java.lang.String)" : "* Check if the executor had a timeout and logs the event.\n   * @param executor to check\n   * @param user user to log\n   * @return true if timeout has occurred",
  "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getStartPos()" : null,
  "org.apache.hadoop.security.authorize.AccessControlList:addGroup(java.lang.String)" : "* Add group to the names of groups allowed for this service.\n   * \n   * @param group\n   *          The group name",
  "org.apache.hadoop.util.IntrusiveCollection:addAll(java.util.Collection)" : null,
  "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:needsPassword()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<init>(java.lang.String)" : "* Construct the metrics system\n   * @param prefix  for the system",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthenticationFailures()" : null,
  "org.apache.hadoop.io.erasurecode.CodecUtil:createRawCoderFactory(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.FSInputChecker:<init>(org.apache.hadoop.fs.Path,int)" : "Constructor\n   * \n   * @param file The name of the file to be read\n   * @param numOfRetries Number of read retries when ChecksumError occurs",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:checkStream()" : null,
  "org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:getTokenTypes()" : null,
  "org.apache.hadoop.service.launcher.InterruptEscalator$ServiceForcedShutdown:getServiceWasShutdown()" : "* Probe for the service being shutdown.\n     * @return true if the service has been shutdown in the runnable",
  "org.apache.hadoop.io.DataInputBuffer$Buffer:getData()" : null,
  "org.apache.hadoop.security.token.DtFileOperations:fileToPath(java.io.File)" : "Add the service prefix for a local filesystem.",
  "org.apache.hadoop.security.UserGroupInformation:fixKerberosTicketOrder()" : null,
  "org.apache.hadoop.fs.FSBuilder:must(java.lang.String,double)" : "* Set mandatory long option, despite passing in a floating\n   * point value.\n   *\n   * @param key key.\n   * @param value value.\n   * @return generic type B.\n   * @see #must(String, String)",
  "org.apache.hadoop.metrics2.lib.MutableQuantiles:stop()" : null,
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:deleteCredentialEntry(java.lang.String)" : null,
  "org.apache.hadoop.util.JsonSerialization:toJson(java.lang.Object)" : "* Convert an instance to a JSON string.\n   * @param instance instance to convert\n   * @return a JSON string description\n   * @throws JsonProcessingException Json generation problems",
  "org.apache.hadoop.crypto.random.OsSecureRandom:<init>()" : null,
  "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)" : null,
  "org.apache.hadoop.security.authorize.AccessControlList:getUsersString()" : "* Returns comma-separated concatenated single String of the set 'users'\n   *\n   * @return comma separated list of users",
  "org.apache.hadoop.tracing.TraceScope:span()" : null,
  "org.apache.hadoop.security.http.RestCsrfPreventionFilter$ServletFilterHttpInteraction:sendError(int,java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.UserGroupInformation$RealUser:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.FileContext:unsetStoragePolicy(org.apache.hadoop.fs.Path)" : "* Unset the storage policy set for a given file or directory.\n   * @param src file or directory path.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.ha.ActiveStandbyElector:initiateZookeeper(org.apache.zookeeper.client.ZKClientConfig)" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:checkStream()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)" : null,
  "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:chooseEngineServerAlias(java.lang.String,java.security.Principal[],javax.net.ssl.SSLEngine)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(org.apache.hadoop.io.file.tfile.RawComparable)" : "* Compare an entry with a RawComparable object. This is useful when\n         * Entries are stored in a collection, and we want to compare a user\n         * supplied key.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[])" : "* Move the cursor to the first entry whose key is strictly greater than\n       * the input key. Synonymous to upperBound(key, 0, key.length). The entry\n       * returned by the previous entry() call will be invalid.\n       * \n       * @param key\n       *          The input key\n       * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider)" : null,
  "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:<init>(int,java.util.concurrent.RejectedExecutionHandler)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)" : "* Cancels a delegation token from the server end-point. It does not require\n   * being authenticated by the configured <code>Authenticator</code>.\n   *\n   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs\n   * are supported.\n   * @param token the authentication token with the Delegation Token to cancel.\n   * @param dToken abstract delegation token identifier.\n   * @throws IOException if an IO error occurred.",
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,byte[][],byte[][])" : null,
  "org.apache.hadoop.util.CacheableIPList:<init>(org.apache.hadoop.util.FileBasedIPList,long)" : null,
  "org.apache.hadoop.ipc.Server:addTerseExceptions(java.lang.Class[])" : "* Add exception classes for which server won't log stack traces.\n   *\n   * @param exceptionClass exception classes",
  "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:createFile(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.conf.Configuration:addResource(java.net.URL,boolean)" : null,
  "org.apache.hadoop.crypto.CryptoProtocolVersion:setUnknownValue(int)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$BlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:<init>(org.apache.hadoop.fs.LocatedFileStatus,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.ssl.SSLFactory:<init>(org.apache.hadoop.security.ssl.SSLFactory$Mode,org.apache.hadoop.conf.Configuration)" : "* Creates an SSLFactory.\n   *\n   * @param mode SSLFactory mode, client or server.\n   * @param conf Hadoop configuration from where the SSLFactory configuration\n   * will be read.",
  "org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType:get(java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:getBytesWritten()" : "* Get the total number of bytes written.\n     * @return the number of bytes",
  "org.apache.hadoop.fs.ClosedIOException:<init>(java.lang.String,java.lang.String)" : "* Appends the custom error-message to the default error message.\n   * @param path path that encountered the closed resource.\n   * @param message custom error message.",
  "org.apache.hadoop.fs.PathIOException:setOperation(java.lang.String)" : "* Optional operation that will preface the path\n   * @param operation a string",
  "org.apache.hadoop.crypto.CryptoOutputStream:checkStream()" : null,
  "org.apache.hadoop.fs.shell.FsCommand:runAll()" : "@deprecated use {@link Command#run(String...argv)}",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])" : "* Verifies that the given identifier and password are valid and match.\n   * @param identifier Token identifier.\n   * @param password Password in the token.\n   * @throws InvalidToken InvalidToken.",
  "org.apache.hadoop.fs.FileContext:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])" : "* Renames Path src to Path dst\n   * <ul>\n   * <li>Fails if src is a file and dst is a directory.\n   * <li>Fails if src is a directory and dst is a file.\n   * <li>Fails if the parent of dst does not exist or is a file.\n   * </ul>\n   * <p>\n   * If OVERWRITE option is not passed as an argument, rename fails if the dst\n   * already exists.\n   * <p>\n   * If OVERWRITE option is passed as an argument, rename overwrites the dst if\n   * it is a file or an empty directory. Rename fails if dst is a non-empty\n   * directory.\n   * <p>\n   * Note that atomicity of rename is dependent on the file system\n   * implementation. Please refer to the file system documentation for details\n   * <p>\n   * \n   * @param src path to be renamed\n   * @param dst new path after rename\n   * @param options rename options.\n   * \n   * @throws AccessControlException If access is denied\n   * @throws FileAlreadyExistsException If <code>dst</code> already exists and\n   *           <code>options</code> has {@link Options.Rename#OVERWRITE}\n   *           option false.\n   * @throws FileNotFoundException If <code>src</code> does not exist\n   * @throws ParentNotDirectoryException If parent of <code>dst</code> is not a\n   *           directory\n   * @throws UnsupportedFileSystemException If file system for <code>src</code>\n   *           and <code>dst</code> is not supported\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws\n   *           undeclared exception to RPC server",
  "org.apache.hadoop.io.file.tfile.TFile$Reader:begin()" : "* Get the begin location of the TFile.\n     * \n     * @return If TFile is not empty, the location of the first key-value pair.\n     *         Otherwise, it returns end().",
  "org.apache.hadoop.ipc.FairCallQueue:offer(java.lang.Object)" : null,
  "org.apache.hadoop.fs.HarFileSystem:close()" : null,
  "org.apache.hadoop.ipc.CallQueueManager:isClientBackoffEnabled()" : null,
  "org.apache.hadoop.util.FileBasedIPList:isIn(java.lang.String)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:reset()" : "* Reset the thread +.",
  "org.apache.hadoop.fs.FSDataOutputStream$PositionCache:write(int)" : null,
  "org.apache.hadoop.tools.TableListing$Builder:wrapWidth(int)" : "* Set the maximum width of a row in the TableListing. Must have one or\n     * more wrappable fields for this to take effect.\n     *\n     * @param width width.\n     * @return Builder.",
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:performEncodeImpl(java.nio.ByteBuffer[],int[],int,java.nio.ByteBuffer[],int[])" : null,
  "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:counter(org.apache.hadoop.metrics2.MetricsInfo,long)" : null,
  "org.apache.hadoop.util.KMSUtil:toJSON(org.apache.hadoop.crypto.key.KeyProvider$KeyVersion)" : null,
  "org.apache.hadoop.security.ProviderUtils:excludeIncompatibleCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.Class)" : "* There are certain integrations of the credential provider API in\n   * which a recursive dependency between the provider and the hadoop\n   * filesystem abstraction causes a problem. These integration points\n   * need to leverage this utility method to remove problematic provider\n   * types from the existing provider path within the configuration.\n   *\n   * @param config the existing configuration with provider path\n   * @param fileSystemClass the class which providers must be compatible\n   * @return Configuration clone with new provider path\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:maximumFpRemove(int[])" : "* Chooses the bit position that maximizes the number of false positive removed.\n   * @param h The different bit positions.\n   * @return The position that maximizes the number of false positive removed.",
  "org.apache.hadoop.io.MD5Hash:quarterDigest()" : "* Return a 32-bit digest of the MD5.\n   * @return the first 4 bytes of the md5",
  "org.apache.hadoop.fs.ContentSummary$Builder:directoryCount(long)" : null,
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:close()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:close()" : null,
  "org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:hasNext()" : null,
  "org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)" : "* Return time duration in the given time unit. Valid units are encoded in\n   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds\n   * (ms), seconds (s), minutes (m), hours (h), and days (d). If no unit is\n   * provided, the default unit is applied.\n   *\n   * @param name Property name\n   * @param defaultValue Value returned if no mapping exists.\n   * @param defaultUnit Default time unit if no valid suffix is provided.\n   * @param returnUnit The unit used for the returned value.\n   * @throws NumberFormatException If the property stripped of its unit is not\n   *         a number\n   * @return time duration in given time unit",
  "org.apache.hadoop.fs.store.audit.AuditSpan:close()" : "* Close calls {@link #deactivate()}; subclasses may override\n   * but the audit manager's wrapping span will always relay to\n   * {@link #deactivate()} rather\n   * than call this method on the wrapped span.",
  "org.apache.hadoop.util.StringUtils:join(java.lang.CharSequence,java.lang.String[])" : "* Concatenates strings, using a separator.\n   *\n   * @param separator to join with\n   * @param strings to join\n   * @return  the joined string",
  "org.apache.hadoop.ipc.DecayRpcScheduler:newSchedulable(org.apache.hadoop.security.UserGroupInformation)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSDelegationToken:<init>()" : null,
  "org.apache.hadoop.io.SequenceFile$Reader$InputStreamOption:<init>(org.apache.hadoop.fs.FSDataInputStream)" : null,
  "org.apache.hadoop.fs.FilterFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.fs.FsStatus:<init>(long,long,long)" : "* Construct a FsStatus object, using the specified statistics.\n   *\n   * @param capacity capacity.\n   * @param used used.\n   * @param remaining remaining.",
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:buffer()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:parkCursorAtEnd()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:release()" : null,
  "org.apache.hadoop.ha.FailoverController:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceProtocol$RequestSource)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:ensureParentZNode()" : "* Utility function to ensure that the configured base znode exists.\n   * This recursively creates the znode as well as all of its parents.\n   *\n   * @throws IOException raised on errors performing I/O.\n   * @throws InterruptedException interrupted exception.\n   * @throws KeeperException other zookeeper operation errors.",
  "org.apache.hadoop.security.ssl.SSLFactory:getHostnameVerifier(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.ContentSummary:getLength()" : "@return the length",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:clear()" : null,
  "org.apache.hadoop.ipc.Server:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation)" : null,
  "org.apache.hadoop.metrics2.lib.MutableQuantiles:setNumInfo(org.apache.hadoop.metrics2.MetricsInfo)" : "* Set info about the metrics.\n   *\n   * @param pNumInfo info about the metrics.",
  "org.apache.hadoop.security.IngressPortBasedResolver:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.SecurityUtil:isTGSPrincipal(javax.security.auth.kerberos.KerberosPrincipal)" : "* TGS must have the server principal of the form \"krbtgt/FOO@FOO\".\n   * @param principal\n   * @return true or false",
  "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:getBufferSize()" : null,
  "org.apache.hadoop.util.WeakReferenceMap:getEntriesCreatedCount()" : "* Get count of entries created on demand.\n   * @return count of entries created",
  "org.apache.hadoop.io.retry.RetryPolicies:retryForeverWithFixedSleep(long,java.util.concurrent.TimeUnit)" : "* <p>\n   * Keep trying forever with a fixed time between attempts.\n   * </p>\n   *\n   * @param sleepTime sleepTime.\n   * @param timeUnit timeUnit.\n   * @return RetryPolicy.",
  "org.apache.hadoop.security.authorize.PolicyProvider$1:getServices()" : "* Get the {@link Service} definitions from the {@link PolicyProvider}.\n   * @return the {@link Service} definitions",
  "org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)" : "* Return time duration in the given time unit. Valid units are encoded in\n   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds\n   * (ms), seconds (s), minutes (m), hours (h), and days (d).\n   *\n   * @param name Property name\n   * @param vStr The string value with time unit suffix to be converted.\n   * @param unit Unit to convert the stored property, if it exists.\n   * @return time duration in given time unit.",
  "org.apache.hadoop.ipc.Server$ConnectionManager:remove(org.apache.hadoop.ipc.Server$Connection)" : null,
  "org.apache.hadoop.net.NodeBase:<init>(java.lang.String,java.lang.String)" : "Construct a node from its name and its location\n   * @param name this node's name (can be null, must not contain {@link #PATH_SEPARATOR})\n   * @param location this node's location",
  "org.apache.hadoop.fs.ChecksumFileSystem:listStatus(org.apache.hadoop.fs.Path)" : "* List the statuses of the files/directories in the given path if the path is\n   * a directory.\n   *\n   * @param f\n   *          given path\n   * @return the statuses of the files/directories in the given path\n   * @throws IOException if an I/O error occurs.",
  "org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,java.lang.String,boolean)" : null,
  "org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration()" : "* refresh Impersonation rules",
  "org.apache.hadoop.fs.viewfs.InodeTree:hasFallbackLink()" : null,
  "org.apache.hadoop.security.User:setLastLogin(long)" : "* Set the last login time.\n   * @param time the number of milliseconds since the beginning of time",
  "org.apache.hadoop.fs.permission.FsAction:or(org.apache.hadoop.fs.permission.FsAction)" : "* OR operation.\n   * @param that FsAction that.\n   * @return FsAction.",
  "org.apache.hadoop.security.UserGroupInformation:getCredentialsInternal()" : null,
  "org.apache.hadoop.ipc.FairCallQueue:getQueueSizes()" : null,
  "org.apache.hadoop.net.NetUtils:addStaticResolution(java.lang.String,java.lang.String)" : "* Adds a static resolution for host. This can be used for setting up\n   * hostnames with names that are fake to point to a well known host. For e.g.\n   * in some testcases we require to have daemons with different hostnames\n   * running on the same machine. In order to create connections to these\n   * daemons, one can set up mappings from those hostnames to \"localhost\".\n   * {@link NetUtils#getStaticResolution(String)} can be used to query for\n   * the actual hostname. \n   * @param host the hostname or IP use to instantiate the object.\n   * @param resolvedName resolved name.",
  "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:addQueueTime(int,long)" : "* Instrument a Call queue time based on its priority.\n   *\n   * @param priority of the RPC call\n   * @param queueTime of the RPC call in the queue of the priority",
  "org.apache.hadoop.fs.shell.PathData:expandAsGlob(java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Expand the given path as a glob pattern.  Non-existent paths do not\n   * throw an exception because creation commands like touch and mkdir need\n   * to create them.  The \"stat\" field will be null if the path does not\n   * exist.\n   * @param pattern the pattern to expand as a glob\n   * @param conf the hadoop configuration\n   * @return list of {@link PathData} objects.  if the pattern is not a glob,\n   * and does not exist, the list will contain a single PathData with a null\n   * stat \n   * @throws IOException anything else goes wrong...",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartC()" : null,
  "org.apache.hadoop.conf.Configuration:addDefaultResource(java.lang.String)" : "* Add a default resource. Resources are loaded in the order of the resources \n   * added.\n   * @param name file name. File should be present in the classpath.",
  "org.apache.hadoop.conf.ReconfigurableBase:shutdownReconfigurationTask()" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX:posixFadviseIfPossible(java.lang.String,java.io.FileDescriptor,long,long,int)" : "* Call posix_fadvise on the given file descriptor. See the manpage\n     * for this syscall for more information. On systems where this\n     * call is not available, does nothing.\n     *\n     * @throws NativeIOException if there is an error with the syscall",
  "org.apache.hadoop.conf.StorageUnit$7:getLongName()" : null,
  "org.apache.hadoop.util.bloom.CountingBloomFilter:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.Ls:isOrderSize()" : "* Should directory contents be displayed in size order.\n   * @return true size order, false default order",
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:checkClosed()" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:writeByteArray(java.io.DataOutput)" : null,
  "org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.bloom.Filter:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.io.VersionedWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.security.SaslInputStream:close()" : "* Closes this input stream and releases any system resources associated with\n   * the stream.\n   * <p>\n   * The <code>close</code> method of <code>SASLInputStream</code> calls the\n   * <code>close</code> method of its underlying input stream.\n   * \n   * @exception IOException\n   *              if an I/O error occurs.",
  "org.apache.hadoop.ipc.RPC$VersionMismatch:<init>(java.lang.String,long,long)" : "* Create a version mismatch exception\n     * @param interfaceName the name of the protocol mismatch\n     * @param clientVersion the client's version of the protocol\n     * @param serverVersion the server's version of the protocol",
  "org.apache.hadoop.security.KDiag:printSysprop(java.lang.String)" : "* Print a system property, or {@link #UNSET} if unset.\n   * @param property property to print",
  "org.apache.hadoop.net.SocketOutputStream:write(int)" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:checkCreateRSRawDecoder()" : null,
  "org.apache.hadoop.security.SaslPlainServer:<init>(javax.security.auth.callback.CallbackHandler)" : null,
  "org.apache.hadoop.io.wrappedio.WrappedIO:fileSystem_getEnclosingRoot(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Return path of the enclosing root for a given path.\n   * The enclosing root path is a common ancestor that should be used for temp and staging dirs\n   * as well as within encryption zones and other restricted directories.\n   * @param fs filesystem\n   * @param path file path to find the enclosing root path for\n   * @return a path to the enclosing root\n   * @throws IOException early checks like failure to resolve path cause IO failures",
  "org.apache.hadoop.util.CrcUtil:getMonomial(long,int)" : "* Compute x^({@code lengthBytes} * 8) mod {@code mod}, where {@code mod} is\n   * in \"reversed\" (little-endian) format such that {@code mod & 1} represents\n   * x^31 and has an implicit term x^32.\n   *\n   * @param lengthBytes lengthBytes.\n   * @param mod mod.\n   * @return monomial.",
  "org.apache.hadoop.security.token.DtUtilShell:getCommandUsage()" : null,
  "org.apache.hadoop.fs.impl.FSBuilderSupport:<init>(org.apache.hadoop.conf.Configuration)" : "* Constructor.\n   * @param options the configuration options from the builder.",
  "org.apache.hadoop.fs.shell.SetReplication:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.crypto.CryptoOutputStream:freeBuffers()" : "Forcibly free the direct buffers.",
  "org.apache.hadoop.security.ShellBasedIdMapping:loadFullGroupMap()" : null,
  "org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.conf.Configuration)" : "* Create an instance from the given configuration\n   * @param conf configuration",
  "org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(org.slf4j.Logger,java.lang.String,java.lang.Object)" : "* Extract any statistics from the source and log at debug, if\n   * the log is set to log at debug.\n   * No-op if logging is not at debug or the source is null/of\n   * the wrong type/doesn't provide statistics.\n   * @param log log to log to\n   * @param message message for log -this must contain \"{}\" for the\n   * statistics report to actually get logged.\n   * @param source source object",
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getBytesReadDistanceOfFiveOrLarger()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setOwner(org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.io.IntWritable:set(int)" : "* Set the value of this IntWritable.\n   * @param value input value.",
  "org.apache.hadoop.security.SecurityUtil:doAsLoginUserOrFatal(java.security.PrivilegedAction)" : "* Perform the given action as the daemon's login user. If the login\n   * user cannot be determined, this will log a FATAL error and exit\n   * the whole JVM.\n   *\n   * @param action action.\n   * @param <T> generic type T.\n   * @return generic type T.",
  "org.apache.hadoop.fs.impl.prefetch.BlockData:throwIfInvalidOffset(long)" : null,
  "org.apache.hadoop.util.RunJar:useClientClassLoader()" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector:<init>(org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:parseIdentityProvider(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.net.unix.DomainSocket:getOutputStream()" : "* @return                 The socket OutputStream",
  "org.apache.hadoop.io.VIntWritable:compareTo(org.apache.hadoop.io.VIntWritable)" : "Compares two VIntWritables.",
  "org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:getIOStatistics()" : "* Return any IOStatistics offered by the inner stream.\n   * @return inner IOStatistics or null",
  "org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareDataBlock()" : "* Create a Data Block and obtain an output stream for adding data into the\n     * block. There can only be one BlockAppender stream active at any time.\n     * Data Blocks may not be created after the first Meta Blocks. The caller\n     * must call BlockAppender.close() to conclude the block creation.\n     * \n     * @return The BlockAppender stream\n     * @throws IOException",
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:release(int)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String,java.lang.String)" : "* Requests a delegation token using the configured <code>Authenticator</code>\n   * for authentication.\n   *\n   * @param url the URL to get the delegation token from. Only HTTP/S URLs are\n   * supported.\n   * @param token the authentication token being used for the user where the\n   * Delegation token will be stored.\n   * @param renewer the renewer user.\n   * @param doAsUser the user to do as, which will be the token owner.\n   * @throws IOException if an IO error occurred.\n   * @throws AuthenticationException if an authentication exception occurred.\n   * @return abstract delegation token identifier.",
  "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:consume(org.apache.hadoop.metrics2.impl.MetricsBuffer)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[],int,int)" : "* Compare the entry key to another key. Synonymous to compareTo(new\n         * ByteArray(buf, offset, length)\n         * \n         * @param buf\n         *          The key buffer\n         * @param offset\n         *          offset into the key buffer.\n         * @param length\n         *          the length of the key.\n         * @return comparison result between the entry key with the input key.",
  "org.apache.hadoop.util.bloom.Key:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:maximums()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getSinkAdapter(java.lang.String)" : null,
  "org.apache.hadoop.io.DataInputBuffer$Buffer:getPosition()" : null,
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:removeKey(org.apache.hadoop.util.bloom.Key,java.util.List[])" : "* Removes a given key from <i>this</i> filer.\n   * @param k The key to remove.\n   * @param vector The counting vector associated to the key.",
  "org.apache.hadoop.fs.AbstractFileSystem:<init>(java.net.URI,java.lang.String,boolean,int)" : "* Constructor to be called by subclasses.\n   * \n   * @param uri for this file system.\n   * @param supportedScheme the scheme supported by the implementor\n   * @param authorityNeeded if true then theURI must have authority, if false\n   *          then the URI must have null authority.\n   * @param defaultPort default port to use if port is not specified in the URI.\n   * @throws URISyntaxException <code>uri</code> has syntax error",
  "org.apache.hadoop.util.ReflectionUtils:getCacheSize()" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:<init>(org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics,int,org.apache.hadoop.fs.statistics.DurationTrackerFactory)" : "* Constructs an instance of a {@code SingleFilePerBlockCache}.\n   *\n   * @param prefetchingStatistics statistics for this stream.\n   * @param maxBlocksCount max blocks count to be kept in cache at any time.\n   * @param trackerFactory tracker with statistics to update",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMaximum(java.lang.String,long)" : null,
  "org.apache.hadoop.security.KDiag:dump(java.io.File)" : "* Dump any file to standard out.\n   * @param file file to dump\n   * @throws IOException IO problems",
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumAllUnits()" : null,
  "org.apache.hadoop.io.file.tfile.Utils:writeVInt(java.io.DataOutput,int)" : "* Encoding an integer into a variable-length encoding format. Synonymous to\n   * <code>Utils#writeVLong(out, n)</code>.\n   * \n   * @param out\n   *          output stream\n   * @param n\n   *          The integer to be encoded\n   * @throws IOException raised on errors performing I/O.\n   * @see Utils#writeVLong(DataOutput, long)",
  "org.apache.hadoop.http.ProfileServlet:<init>()" : null,
  "org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,int,boolean)" : "* Copies from one stream to another.\n   *\n   * @param in InputStrem to read from\n   * @param out OutputStream to write to\n   * @param buffSize the size of the buffer \n   * @param close whether or not close the InputStream and \n   * OutputStream at the end. The streams are closed in the finally clause.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.curator.ZKCuratorManager:delete(java.lang.String)" : "* Delete a ZNode.\n   * @param path Path of the ZNode.\n   * @return If the znode was deleted.\n   * @throws Exception If it cannot contact ZooKeeper.",
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:getUri()" : null,
  "org.apache.hadoop.ipc.ProtocolProxy:getProxy()" : null,
  "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:getGroupsSet(java.lang.String)" : null,
  "org.apache.hadoop.util.DataChecksum:throwChecksumException(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,java.lang.String,long,int,int)" : null,
  "org.apache.hadoop.ipc.metrics.RpcMetrics:callQueueLength()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsFinishedWithStream()" : null,
  "org.apache.hadoop.fs.DelegateToFileSystem:getHomeDirectory()" : null,
  "org.apache.hadoop.util.ApplicationClassLoader:getResource(java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)" : "* Fully replaces ACL of files and directories, discarding all existing\n   * entries.\n   *\n   * @param path Path to modify\n   * @param aclSpec List describing modifications, which must include entries\n   *   for user, group, and others for compatibility with permission bits.\n   * @throws IOException if an ACL could not be modified\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator:hasNext()" : null,
  "org.apache.hadoop.fs.shell.Count:isHumanReadable()" : "* Should sizes be shown in human readable format rather than bytes?\n   * @return true if human readable format",
  "org.apache.hadoop.fs.shell.TouchCommands$Touchz:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:<init>()" : null,
  "org.apache.hadoop.security.alias.CredentialProvider:needsPassword()" : "* Does this provider require a password? This means that a password is\n   * required for normal operation, and it has not been found through normal\n   * means. If true, the password should be provided by the caller using\n   * setPassword().\n   * @return Whether or not the provider requires a password\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:addTimedOperation(java.lang.String,long)" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:reset()" : null,
  "org.apache.hadoop.security.token.DtFileOperations:removeTokenFromFile(boolean,java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)" : "Remove a token from a file in the local filesystem, matching alias.\n   *  @param cancel cancel token as well as remove from file.\n   *  @param tokenFile a local File object.\n   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output\n   *  @param alias remove only tokens matching alias; null matches all.\n   *  @param conf Configuration object passed along.\n   *  @throws IOException raised on errors performing I/O.\n   *  @throws InterruptedException if the thread is interrupted.",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_available()" : "* Are the bulk delete methods available?\n   * @return true if the methods were found.",
  "org.apache.hadoop.fs.shell.find.FilterExpression:getUsage()" : null,
  "org.apache.hadoop.io.DataOutputBuffer$Buffer:getLength()" : null,
  "org.apache.hadoop.util.Lists:newLinkedList(java.lang.Iterable)" : "* Creates a <i>mutable</i> {@code LinkedList} instance containing the given\n   * elements; a very thin shortcut for creating an empty list then calling\n   * Iterables#addAll.\n   *\n   * <p><b>Performance note:</b> {@link ArrayList} and\n   * {@link java.util.ArrayDeque} consistently\n   * outperform {@code LinkedList} except in certain rare and specific\n   * situations. Unless you have spent a lot of time benchmarking your\n   * specific needs, use one of those instead.</p>\n   *\n   * @param elements elements.\n   * @param <E> Generics Type E.\n   * @return Generics Type E List.",
  "org.apache.hadoop.fs.shell.CommandFactory:getInstance(java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Get an instance of the requested command\n   * @param cmdName name of the command to lookup\n   * @param conf the hadoop configuration\n   * @return the {@link Command} or null if the command is unknown",
  "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsWithMachineLists()" : null,
  "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DefaultDelegationTokenExtension:cancelDelegationToken(org.apache.hadoop.security.token.Token)" : null,
  "org.apache.hadoop.io.EnumSetWritable:getConf()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:getFirstKey()" : "* Get the first key in the TFile.\n     * \n     * @return The first key in the TFile.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ha.ActiveStandbyElector:shouldRetry(org.apache.zookeeper.KeeperException$Code,org.apache.zookeeper.KeeperException$Code)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:reset()" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(java.net.URI,org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.functional.CommonCallableSupplier:maybeAwaitCompletion(java.util.concurrent.CompletableFuture)" : "* Block awaiting completion for any non-null future passed in;\n   * No-op if a null arg was supplied.\n   * @param future future\n   * @throws IOException      if one of the called futures raised an IOE.\n   * @throws RuntimeException if one of the futures raised one.",
  "org.apache.hadoop.fs.FilterFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.FsServerDefaults:getBlockSize()" : null,
  "org.apache.hadoop.fs.QuotaUsage:isTypeConsumedAvailable()" : "* Return true if any storage type consumption information is available.\n   *\n   * @return if any storage type consumption information\n   * is available, not false.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:<init>()" : null,
  "org.apache.hadoop.util.IntrusiveCollection:clear()" : "* Remove all elements.",
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCanonicalServiceName()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:getRecordCount()" : null,
  "org.apache.hadoop.security.KDiag:validateKrb5File()" : "* Locate the {@code krb5.conf} file and dump it.\n   *\n   * No-op on windows.\n   * @throws IOException problems reading the file.",
  "org.apache.hadoop.io.DataInputBuffer:getData()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:end(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation)" : null,
  "org.apache.hadoop.conf.StorageUnit$6:getLongName()" : null,
  "org.apache.hadoop.fs.http.HttpFileSystem:getScheme()" : null,
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeSource(java.lang.String)" : null,
  "org.apache.hadoop.util.dynamic.DynConstructors$Builder:build()" : null,
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor:getWaitingCount()" : "* Get the number of threads waiting to acquire a permit.\n   * @return snapshot of the length of the queue of blocked threads.",
  "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke()" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getMaximumReference(java.lang.String)" : null,
  "org.apache.hadoop.io.IntWritable:toString()" : null,
  "org.apache.hadoop.security.http.CrossOriginFilter:isCrossOrigin(java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:getAllThreadLocalDataSize()" : null,
  "org.apache.hadoop.fs.FileStatus:getModificationTime()" : "* Get the modification time of the file.\n   * @return the modification time of file in milliseconds since January 1, 1970 UTC.",
  "org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:executeFunction(java.util.function.Supplier)" : "* @param f function to run in future on executor pool\n   * @return future\n   * @throws java.util.concurrent.RejectedExecutionException can be thrown\n   * @throws NullPointerException if f param is null",
  "org.apache.hadoop.service.launcher.ServiceLauncher:getUsageMessage()" : "* Get the usage message, ideally dynamically.\n   * @return the usage message",
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:end()" : null,
  "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$1:unit()" : null,
  "org.apache.hadoop.fs.statistics.MeanStatistic:<init>(long,long)" : "* Constructor, with some resilience against invalid sample counts.\n   * If the sample count is 0 or less, the sum is set to 0 and\n   * the sample count to 0.\n   * @param samples sample count.\n   * @param sum sum value",
  "org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration)" : "* Refreshes configuration using the default Proxy user prefix for properties.\n   * @param conf configuration",
  "org.apache.hadoop.http.lib.StaticUserWebFilter:getUsernameFromConf(org.apache.hadoop.conf.Configuration)" : "* Retrieve the static username from the configuration.",
  "org.apache.hadoop.ipc.Server$Listener:getReader()" : null,
  "org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path)" : "* Append to an existing file (optional operation).\n   * Same as\n   * {@code append(f, getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,\n   *     IO_FILE_BUFFER_SIZE_DEFAULT), null)}\n   * @param f the existing file to be appended.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).\n   * @return output stream.",
  "org.apache.hadoop.ipc.ProcessingDetails:get(org.apache.hadoop.ipc.ProcessingDetails$Timing,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.ipc.StandbyException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.security.ssl.SSLFactory:createSSLSocketFactory()" : "* Returns a configured SSLSocketFactory.\n   *\n   * @return the configured SSLSocketFactory.\n   * @throws GeneralSecurityException thrown if the SSLSocketFactory could not\n   * be initialized.\n   * @throws IOException thrown if and IO error occurred while loading\n   * the server keystore.",
  "org.apache.hadoop.security.Credentials:writeTokenStorageToStream(java.io.DataOutputStream)" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompress(byte[],int,int)" : "* Fills specified buffer with uncompressed data. Returns actual number\n   * of bytes of uncompressed data. A return value of 0 indicates that\n   * {@link #needsInput()} should be called in order to determine if more\n   * input data is required.\n   *\n   * @param b   Buffer for the uncompressed data\n   * @param off Start offset of the data\n   * @param len Size of the buffer\n   * @return The actual number of bytes of compressed data.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.ShellBasedIdMapping:updateMaps()" : null,
  "org.apache.hadoop.io.EnumSetWritable:<init>(java.util.EnumSet,java.lang.Class)" : "* Construct a new EnumSetWritable. If the <tt>value</tt> argument is null or\n   * its size is zero, the <tt>elementType</tt> argument must not be null. If\n   * the argument <tt>value</tt>'s size is bigger than zero, the argument\n   * <tt>elementType</tt> is not be used.\n   * \n   * @param value enumSet value.\n   * @param elementType elementType.",
  "org.apache.hadoop.fs.shell.PathData:stringToUri(java.lang.String)" : "Construct a URI from a String with unescaped special characters\n   *  that have non-standard semantics. e.g. /, ?, #. A custom parsing\n   *  is needed to prevent misbehavior.\n   *  @param pathString The input path in string form\n   *  @return URI",
  "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createTrustManagers()" : null,
  "org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : null,
  "org.apache.hadoop.ipc.Server$ExceptionsHandler:addTerseLoggingExceptions(java.lang.Class[])" : "* Add exception classes for which server won't log stack traces.\n     * Optimized for infrequent invocation.\n     * @param exceptionClass exception classes",
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap$EntryImpl:<init>(java.lang.String,java.lang.Object)" : null,
  "org.apache.hadoop.util.GcTimeMonitor:getLatestGcData()" : "* Returns a copy of the most recent data measured by this monitor.\n   * @return a copy of the most recent data measured by this monitor",
  "org.apache.hadoop.metrics2.lib.MutableInverseQuantiles:setQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.text.DecimalFormat)" : "* Sets quantileInfo.\n   *\n   * @param ucName capitalized name of the metric\n   * @param uvName capitalized type of the values\n   * @param desc uncapitalized long-form textual description of the metric\n   * @param lvName uncapitalized type of the values\n   * @param df Number formatter for inverse percentile value",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementCounter(java.lang.String,long)" : null,
  "org.apache.hadoop.ipc.Client$Connection:touch()" : "Update lastActivity with the current time.",
  "org.apache.hadoop.util.GenericOptionsParser:printGenericCommandUsage(java.io.PrintStream)" : "* Print the usage message for generic command-line options supported.\n   * \n   * @param out stream to print the usage message to.",
  "org.apache.hadoop.io.SequenceFile$Reader:getSync()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:getInitialWorkingDirectory()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:resolvePath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.permission.PermissionStatus$2:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)" : "* Constructor.\n   *\n   * @param user user.\n   * @param group group.\n   * @param permission permission.",
  "org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator:hasNext()" : null,
  "org.apache.hadoop.fs.DFCachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder)" : null,
  "org.apache.hadoop.security.token.Token:decodeIdentifier()" : "* Get the token identifier object, or null if it could not be constructed\n   * (because the class could not be loaded, for example).\n   * @return the token identifier, or null if there was no class found for it\n   * @throws IOException failure to unmarshall the data\n   * @throws RuntimeException if the token class could not be instantiated.",
  "org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator:<init>(org.apache.hadoop.io.RawComparator)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:addBlockRegion(org.apache.hadoop.io.file.tfile.BCFile$BlockRegion)" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,boolean)" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:toString()" : null,
  "org.apache.hadoop.util.HttpExceptionUtils:validateResponse(java.net.HttpURLConnection,int)" : "* Validates the status of an <code>HttpURLConnection</code> against an\n   * expected HTTP status code. If the current status code is not the expected\n   * one it throws an exception with a detail message using Server side error\n   * messages if available.\n   * <p>\n   * <b>NOTE:</b> this method will throw the deserialized exception even if not\n   * declared in the <code>throws</code> of the method signature.\n   *\n   * @param conn the <code>HttpURLConnection</code>.\n   * @param expectedStatus the expected HTTP status code.\n   * @throws IOException thrown if the current status code does not match the\n   * expected one.",
  "org.apache.hadoop.io.compress.BZip2Codec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)" : "* Create a {@link CompressionOutputStream} that will write to the given\n   * {@link OutputStream} with the given {@link Compressor}.\n   *\n   * @param out        the location for the final output stream\n   * @param compressor compressor to use\n   * @return a stream the user can write uncompressed data to, to have it \n   *         compressed\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap$EntryImpl:getKey()" : null,
  "org.apache.hadoop.util.Sets:differenceInTreeSets(java.util.Set,java.util.Set)" : "* Returns the difference of two sets as an unmodifiable set.\n   * The returned set contains all elements that are contained by {@code set1}\n   * and not contained by {@code set2}.\n   *\n   * <p>Results are undefined if {@code set1} and {@code set2} are sets based\n   * on different equivalence relations (as {@code HashSet}, {@code TreeSet},\n   * and the keySet of an {@code IdentityHashMap} all are).\n   *\n   * This method is used to find difference for TreeSets. For HashSets,\n   * recommended method is {@link #difference(Set, Set)}.\n   *\n   * @param <E> Generics Type E.\n   * @param set1 set1.\n   * @param set2 set2.\n   * @return a new, empty thread-safe {@code Set}.",
  "org.apache.hadoop.fs.Globber$GlobBuilder:<init>(org.apache.hadoop.fs.FileSystem)" : "* Construct bonded to a filesystem.\n     * @param fs file system.",
  "org.apache.hadoop.metrics2.util.Contracts:<init>()" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.ipc.RpcClientUtil:methodExists(int,long,java.util.Map)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.util.ServletUtil:initHTML(javax.servlet.ServletResponse,java.lang.String)" : "* Initial HTML header.\n   *\n   * @param response response.\n   * @param title title.\n   * @throws IOException raised on errors performing I/O.\n   * @return PrintWriter.",
  "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:merge()" : "This is the single level merge that is called multiple times \n       * depending on the factor size and the number of segments\n       * @return RawKeyValueIterator\n       * @throws IOException",
  "org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.ipc.ProxyCombiner:combine(java.lang.Class,java.lang.Object[])" : "* Combine two or more proxies which together comprise a single proxy\n   * interface. This can be used for a protocol interface which {@code extends}\n   * multiple other protocol interfaces. The returned proxy will implement\n   * all of the methods of the combined proxy interface, delegating calls\n   * to which proxy implements that method. If multiple proxies implement the\n   * same method, the first in the list will be used for delegation.\n   * <p>\n   * This will check that every method on the combined interface is\n   * implemented by at least one of the supplied proxy objects.\n   *\n   * @param combinedProxyInterface The interface of the combined proxy.\n   * @param proxies The proxies which should be used as delegates.\n   * @param <T> The type of the proxy that will be returned.\n   * @return The combined proxy.",
  "org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.alias.CredentialShell$PasswordReader:readPassword(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:finished()" : "* Returns true if the end of the decompressed\n   * data output stream has been reached.\n   *\n   * @return <code>true</code> if the end of the decompressed\n   *         data output stream has been reached.",
  "org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(java.lang.String,java.lang.Object)" : "* Extract any statistics from the source and log to\n   * this class's log at debug, if\n   * the log is set to log at debug.\n   * No-op if logging is not at debug or the source is null/of\n   * the wrong type/doesn't provide statistics.\n   * @param message message for log -this must contain \"{}\" for the\n   * statistics report to actually get logged.\n   * @param source source object",
  "org.apache.hadoop.ipc.Server$Responder:waitPending()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:checkGetBytesArrayBuffer(byte[][],int,int)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)" : "* Partially resolves the path. This is used during symlink resolution in\n   * {@link FSLinkResolver}, and differs from the similarly named method\n   * {@link FileContext#getLinkTarget(Path)}.\n   * @param f the path.\n   * @return target path.\n   * @throws IOException subclass implementations may throw IOException",
  "org.apache.hadoop.util.Lists:newLinkedList()" : "* Creates a <i>mutable</i>, empty {@code LinkedList} instance.\n   *\n   * <p><b>Performance note:</b> {@link ArrayList} and\n   * {@link java.util.ArrayDeque} consistently\n   * outperform {@code LinkedList} except in certain rare and specific\n   * situations. Unless you have\n   * spent a lot of time benchmarking your specific needs, use one of those\n   * instead.</p>\n   *\n   * @param <E> Generics Type E.\n   * @return Generics Type E List.",
  "org.apache.hadoop.util.OperationDuration:humanTime(long)" : "* Convert to a human time of minutes:seconds.millis.\n   * @param time time to humanize.\n   * @return a printable value.",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:create(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:hbMakeCodeLengths(byte[],int[],org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data,int,int)" : null,
  "org.apache.hadoop.util.NativeLibraryChecker:main(java.lang.String[])" : "* A tool to test native library availability.\n   * @param args args.",
  "org.apache.hadoop.http.WebServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : "* Get method is modified to support impersonation and Kerberos\n   * SPNEGO token by forcing client side redirect when accessing\n   * \"/\" (root) of the web application context.",
  "org.apache.hadoop.ha.ZKFailoverController:fatalError(java.lang.String)" : null,
  "org.apache.hadoop.util.ApplicationClassLoader:loadClass(java.lang.String,boolean)" : null,
  "org.apache.hadoop.util.JsonSerialization:writer()" : "* @return an ObjectWriter which pretty-prints its output",
  "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:stop()" : null,
  "org.apache.hadoop.fs.shell.MoveCommands$Rename:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.FsUrlStreamHandler:<init>()" : null,
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:readStreamHeader()" : null,
  "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:getFileLength()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementMinimum(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.ChRootedFileSystem)" : null,
  "org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:getBytesRead()" : "* Returns the total number of uncompressed bytes input so far.\n   *\n   * @return the total (non-negative) number of uncompressed bytes input so far",
  "org.apache.hadoop.fs.FileSystem$Statistics$3:<init>(org.apache.hadoop.fs.FileSystem$Statistics)" : "* Copy constructor.\n     *\n     * @param other    The input Statistics object which is cloned.",
  "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:getDefault()" : null,
  "org.apache.hadoop.io.ArrayWritable:toStrings()" : null,
  "org.apache.hadoop.util.ChunkedArrayList:addChunk(int)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:open(org.apache.hadoop.fs.Path,int)" : null,
  "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getQueueSizes()" : null,
  "org.apache.hadoop.fs.UnionStorageStatistics:reset()" : null,
  "org.apache.hadoop.fs.FilterFs:getInitialWorkingDirectory()" : null,
  "org.apache.hadoop.util.Lists:newArrayListWithCapacity(int)" : "* Creates an {@code ArrayList} instance backed by an array with the\n   * specified initial size;\n   * simply delegates to {@link ArrayList#ArrayList(int)}.\n   *\n   * @param <E> Generics Type E.\n   * @param initialArraySize the exact size of the initial backing array for\n   *     the returned array list\n   *     ({@code ArrayList} documentation calls this value the \"capacity\").\n   * @return a new, empty {@code ArrayList} which is guaranteed not to\n   *     resize itself unless its size reaches {@code initialArraySize + 1}.\n   * @throws IllegalArgumentException if {@code initialArraySize} is negative.",
  "org.apache.hadoop.crypto.key.KeyProvider:noPasswordWarning()" : "* If a password for the provider is needed, but is not provided, this will\n   * return a warning and instructions for supplying said password to the\n   * provider.\n   * @return A warning and instructions for supplying the password",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:<init>(java.lang.String,java.lang.String,int)" : null,
  "org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:offer(java.lang.Object)" : null,
  "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)" : "* Create an FSDataOutputStream at the indicated Path with write-progress\n   * reporting.\n   * Files are overwritten by default.\n   * @param f the file to create\n   * @param progress to report progress\n   * @throws IOException IO failure\n   * @return output stream.",
  "org.apache.hadoop.fs.FilterFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])" : null,
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable,java.lang.Object)" : null,
  "org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : "* Opens an FSDataOutputStream at the indicated Path with write-progress\n   * reporting. Same as create(), except fails if parent directory doesn't\n   * already exist.\n   * @param f the file name to open\n   * @param overwrite if a file with this name already exists, then if true,\n   * the file will be overwritten, and if false an error will be thrown.\n   * @param bufferSize the size of the buffer to be used.\n   * @param replication required block replication for the file.\n   * @param blockSize block size\n   * @param progress the progress reporter\n   * @throws IOException IO failure\n   * @see #setPermission(Path, FsPermission)\n   * @return output stream.",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:compress(byte[],int,int)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader,int)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:startThreads()" : "* should be called before this object is used.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.Server:getTotalRequests()" : null,
  "org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcQueueTime(long)" : "* Add an RPC queue time sample\n   * @param qTime the queue time",
  "org.apache.hadoop.ipc.Server:getAuthMethods(org.apache.hadoop.security.token.SecretManager,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:finished()" : null,
  "org.apache.hadoop.fs.DFCachingGetSpaceUsed:refresh()" : null,
  "org.apache.hadoop.security.Groups:getBackgroundRefreshException()" : null,
  "org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[])" : null,
  "org.apache.hadoop.fs.CompositeCrcFileChecksum:<init>(int,org.apache.hadoop.util.DataChecksum$Type,int)" : "* Create a CompositeCrcFileChecksum.\n   *\n   * @param crc crc.\n   * @param crcType crcType.\n   * @param bytesPerCrc bytesPerCrc.",
  "org.apache.hadoop.metrics2.impl.SinkQueue:waitForData()" : null,
  "org.apache.hadoop.util.GenericOptionsParser:getCommandLine()" : "* Returns the commons-cli <code>CommandLine</code> object \n   * to process the parsed arguments. \n   * \n   * Note: If the object is created with \n   * {@link #GenericOptionsParser(Configuration, String[])}, then returned \n   * object will only contain parsed generic options.\n   * \n   * @return <code>CommandLine</code> representing list of arguments \n   *         parsed against Options descriptor.",
  "org.apache.hadoop.fs.Options$CreateOpts:bytesPerChecksum(short)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:isIOStatisticsThreadLevelEnabled()" : "* Static probe to check if the thread-level IO statistics enabled.\n   *\n   * @return if the thread-level IO statistics enabled.",
  "org.apache.hadoop.fs.FileContext$Util:exists(org.apache.hadoop.fs.Path)" : "* Does the file exist?\n     * Note: Avoid using this method if you already have FileStatus in hand.\n     * Instead reuse the FileStatus \n     * @param f the  file or dir to be checked\n     *\n     * @throws AccessControlException If access is denied\n     * @throws IOException If an I/O error occurred\n     * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n     *           not supported\n     * \n     * Exceptions applicable to file systems accessed over RPC:\n     * @throws RpcClientException If an exception occurred in the RPC client\n     * @throws RpcServerException If an exception occurred in the RPC server\n     * @throws UnexpectedServerException If server implementation throws \n     *           undeclared exception to RPC server\n     * @return if f exists true, not false.",
  "org.apache.hadoop.ha.ZKFailoverController:mainLoop()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor:getAvailablePermits()" : "* Get the number of permits available; guaranteed to be\n   * {@code 0 <= availablePermits <= size}.\n   * @return the number of permits available at the time of invocation.",
  "org.apache.hadoop.net.NetworkTopology:<init>()" : null,
  "org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int)" : null,
  "org.apache.hadoop.ipc.Client:setCallIdAndRetryCount(int,int,java.lang.Object)" : "* Set call id and retry count for the next call.\n   * @param cid input cid.\n   * @param rc input rc.\n   * @param externalHandler input externalHandler.",
  "org.apache.hadoop.fs.permission.FsPermission:toExtendedShort()" : "* Encodes the object to a short.  Unlike {@link #toShort()}, this method may\n   * return values outside the fixed range 00000 - 01777 if extended features\n   * are encoded into this permission, such as the ACL bit.\n   *\n   * @return short extended short representation of this permission",
  "org.apache.hadoop.fs.FSDataInputStream:getPos()" : "* Get the current position in the input stream.\n   *\n   * @return current position in the input stream",
  "org.apache.hadoop.fs.BufferedFSInputStream:getFileDescriptor()" : null,
  "org.apache.hadoop.crypto.CryptoOutputStream:getTmpBuf()" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:getDefaultCompressionAlgorithm()" : null,
  "org.apache.hadoop.net.DNS:getSubinterface(java.lang.String)" : "* @return NetworkInterface for the given subinterface name (eg eth0:0)\n   *    or null if no interface with the given name can be found",
  "org.apache.hadoop.log.LogLevel$CLI:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)" : "* Sample all the mutable metrics and put the snapshot in the builder\n   * @param builder to contain the metrics snapshot\n   * @param all get all the metrics even if the values are not changed.",
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:absolute()" : "* Gets the current absolute position within this file.\n   *\n   * @return the current absolute position within this file.",
  "org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)" : null,
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:createVector()" : "* Creates and initialises the various vectors.",
  "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:<init>(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.FSInputChecker:seek(long)" : null,
  "org.apache.hadoop.ipc.Server$ConnectionManager:close(org.apache.hadoop.ipc.Server$Connection)" : null,
  "org.apache.hadoop.io.IntWritable$Comparator:<init>()" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:truncate(org.apache.hadoop.fs.Path,long)" : null,
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(org.apache.hadoop.util.bloom.Key[])" : "* Adds an array of false positive information to <i>this</i> retouched Bloom filter.\n   * @param keys The array of false positive.",
  "org.apache.hadoop.net.unix.DomainSocketWatcher:<init>(int,java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.KeyProviderExtension:getKeysMetadata(java.lang.String[])" : null,
  "org.apache.hadoop.ha.HAAdmin:getUsageString()" : null,
  "org.apache.hadoop.conf.StorageUnit$7:getSuffixChar()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:listLocatedStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.compress.BZip2Codec:writeHeader(java.io.OutputStream)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:tags()" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$MaybeClose:<init>(java.lang.Object,boolean)" : "* Construct -close the object if it is closeable and close==true.\n     * @param o object to close.\n     * @param close should close?",
  "org.apache.hadoop.conf.ConfigRedactor:redact(java.lang.String,java.lang.String)" : "* Given a key / value pair, decides whether or not to redact and returns\n   * either the original value or text indicating it has been redacted.\n   *\n   * @param key param key.\n   * @param value param value, will return if conditions permit.\n   * @return Original value, or text indicating it has been redacted",
  "org.apache.hadoop.ha.ZKFailoverController:getLastHealthState()" : "* @return the last health state passed to the FC\n   * by the HealthMonitor.",
  "org.apache.hadoop.fs.statistics.DurationTrackerFactory:trackDuration(java.lang.String,long)" : "* Initiate a duration tracking operation by creating/returning\n   * an object whose {@code close()} call will\n   * update the statistics.\n   *\n   * The statistics counter with the key name will be incremented\n   * by the given count.\n   *\n   * The expected use is within a try-with-resources clause.\n   *\n   * The default implementation returns a stub duration tracker.\n   * @param key statistic key prefix\n   * @param count  #of times to increment the matching counter in this\n   * operation.\n   * @return an object to close after an operation completes.",
  "org.apache.hadoop.fs.shell.find.FilterExpression:finish()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$NoMlockCacheManipulator:mlock(java.lang.String,java.nio.ByteBuffer,long)" : null,
  "org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:getProtocolVersion(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.shell.Display$Cat:printToStdout(java.io.InputStream)" : null,
  "org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo:hashCode()" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:needsDictionary()" : null,
  "org.apache.hadoop.util.Options$ProgressableOption:getValue()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.dynamic.BindingUtils:loadInvocation(java.lang.Class,java.lang.Class,java.lang.String,java.lang.Class[])" : "* Get an invocation from the source class, which will be unavailable() if\n   * the class is null or the method isn't found.\n   *\n   * @param <T> return type\n   * @param source source. If null, the method is a no-op.\n   * @param returnType return type class (unused)\n   * @param name method name\n   * @param parameterTypes parameters\n   *\n   * @return the method or \"unavailable\"",
  "org.apache.hadoop.fs.FilterFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:getUri()" : "* Returns a URI whose scheme and authority identify this FileSystem.\n   * \n   * @return the uri of this file system.",
  "org.apache.hadoop.crypto.random.OsSecureRandom:finalize()" : null,
  "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:moveToNextQueue()" : "* Move to the next queue.",
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:create()" : "* Create an FSDataOutputStream at the specified path.\n   *\n   * @return return Generics Type B.",
  "org.apache.hadoop.conf.StorageUnit$1:toGBs(double)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Magic:readAndVerify(java.io.DataInput)" : null,
  "org.apache.hadoop.ipc.CallQueueManager:put(java.lang.Object)" : "* Insert e into the backing queue or block until we can.  If client\n   * backoff is enabled this method behaves like add which throws if\n   * the queue overflows.\n   * If we block and the queue changes on us, we will insert while the\n   * queue is drained.",
  "org.apache.hadoop.service.launcher.ServiceLauncher:extractCommandOptions(org.apache.hadoop.conf.Configuration,java.util.List)" : "* Extract the command options and apply them to the configuration,\n   * building an array of processed arguments to hand down to the service.\n   *\n   * @param conf configuration to update.\n   * @param args main arguments. {@code args[0]}is assumed to be\n   * the service classname and is skipped.\n   * @return the remaining arguments\n   * @throws ExitUtil.ExitException if JVM exiting is disabled.",
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getReplication()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getAllStoragePolicies()" : null,
  "org.apache.hadoop.conf.Configuration:onlyKeyExists(java.lang.String)" : "* Return existence of the <code>name</code> property, but only for\n   * names which have no valid value, usually non-existent or commented\n   * out in XML.\n   *\n   * @param name the property name\n   * @return true if the property <code>name</code> exists without value",
  "org.apache.hadoop.net.NetworkTopology:chooseRandom(org.apache.hadoop.net.InnerNode,org.apache.hadoop.net.Node,java.util.Collection,int,int)" : "* Randomly choose one node under <i>parentNode</i>, considering the exclude\n   * nodes and scope. Should be called with {@link #netlock}'s readlock held.\n   *\n   * @param parentNode        the parent node\n   * @param excludedScopeNode the node corresponding to the exclude scope.\n   * @param excludedNodes     a collection of nodes to be excluded from\n   * @param totalInScopeNodes total number of nodes under parentNode, excluding\n   *                          the excludedScopeNode\n   * @param availableNodes    number of available nodes under parentNode that\n   *                          could be chosen, excluding excludedNodes\n   * @return the chosen node, or null if none can be chosen",
  "org.apache.hadoop.io.WritableName:addName(java.lang.Class,java.lang.String)" : "* Add an alternate name for a class.\n   * @param writableClass input writableClass.\n   * @param name input name.",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setReplication(org.apache.hadoop.fs.Path,short)" : null,
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeObjectName(java.lang.String)" : null,
  "org.apache.hadoop.log.LogLevel:main(java.lang.String[])" : "* A command line implementation\n   * @param args input args.\n   * @throws Exception exception.",
  "org.apache.hadoop.security.token.delegation.DelegationKey:<init>(int,long,byte[])" : null,
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:shutdown()" : "* Shutdown the metrics system",
  "org.apache.hadoop.fs.impl.FlagSet:checkMutable()" : "* Check for mutability before any mutating operation.\n   * @throws IllegalStateException if the set is still mutable",
  "org.apache.hadoop.conf.Configured:<init>()" : "Construct a Configured.",
  "org.apache.hadoop.fs.shell.find.Result:toString()" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:decrypt(org.apache.hadoop.crypto.Decryptor,java.nio.ByteBuffer,java.nio.ByteBuffer,byte)" : "* Do the decryption using inBuffer as input and outBuffer as output.\n   * Upon return, inBuffer is cleared; the decrypted data starts at \n   * outBuffer.position() and ends at outBuffer.limit();",
  "org.apache.hadoop.security.ProviderUtils:nestURIForLocalJavaKeyStoreProvider(java.net.URI)" : "* Mangle given local java keystore file URI to allow use as a\n   * LocalJavaKeyStoreProvider.\n   * @param localFile absolute URI with file scheme and no authority component.\n   *                  i.e. return of File.toURI,\n   *                  e.g. file:///home/larry/creds.jceks\n   * @return URI of the form localjceks://file/home/larry/creds.jceks\n   * @throws IllegalArgumentException if localFile isn't not a file uri or if it\n   *                                  has an authority component.\n   * @throws URISyntaxException if the wrapping process violates RFC 2396",
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:constructRpcRequest(java.lang.reflect.Method,org.apache.hadoop.thirdparty.protobuf.Message)" : null,
  "org.apache.hadoop.util.functional.LazyAutoCloseableReference:<init>(org.apache.hadoop.util.functional.CallableRaisingIOE)" : "* Constructor for this instance.\n   * @param constructor method to invoke to actually construct the inner object.",
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:needsInput()" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree:buildLinkRegexEntry(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.FilterFs:getStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.GenericWritable:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.net.StandardSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:toString()" : null,
  "org.apache.hadoop.util.LightWeightCache:<init>(int,int,long,long)" : "* @param recommendedLength Recommended size of the internal array.\n   * @param sizeLimit the limit of the size of the cache.\n   *            The limit is disabled if it is &lt;= 0.\n   * @param creationExpirationPeriod the time period C &gt; 0 in nanoseconds\n   *            that the creation of an entry is expired if it is added to the\n   *            cache longer than C.\n   * @param accessExpirationPeriod the time period A &gt;= 0 in nanoseconds that\n   *            the access of an entry is expired if it is not accessed\n   *            longer than A.",
  "org.apache.hadoop.fs.FileSystem$Statistics$10:<init>(org.apache.hadoop.fs.FileSystem$Statistics)" : "* Copy constructor.\n     *\n     * @param other    The input Statistics object which is cloned.",
  "org.apache.hadoop.fs.permission.AclEntry$Builder:setScope(org.apache.hadoop.fs.permission.AclEntryScope)" : "* Sets the scope of the ACL entry.  If this method is not called, then the\n     * builder assumes {@link AclEntryScope#ACCESS}.\n     *\n     * @param scope AclEntryScope scope of the ACL entry\n     * @return Builder this builder, for call chaining",
  "org.apache.hadoop.util.JsonSerialization:toBytes(java.lang.Object)" : "* Convert JSON to bytes.\n   * @param instance instance to convert\n   * @return a byte array\n   * @throws IOException IO problems",
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:reencryptEncryptedKeys(java.util.List)" : null,
  "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getName()" : null,
  "org.apache.hadoop.fs.impl.FlagSet:equals(java.lang.Object)" : "* Equality is based on the value of {@link #enumClass} and\n   * {@link #prefix} and the contents of the set, which must match.\n   * <p>\n   * The immutability flag is not considered, nor is the\n   * {@link #namesToValues} map, though as that is generated from\n   * the enumeration and prefix, it is implicitly equal if the prefix\n   * and enumClass fields are equal.\n   * @param o other object\n   * @return true iff the equality condition is met.",
  "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:setConfigurations(java.lang.String,java.lang.String,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getResolvedQualifiedPath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.http.HttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)" : "* List the statuses of the files/directories in the given path if the path is\n   * a directory.\n   *\n   * @param f\n   *          given path\n   * @return the statuses of the files/directories in the given patch\n   * @throws IOException if an I/O error occurs.",
  "org.apache.hadoop.fs.FSInputStream:readFully(long,byte[])" : null,
  "org.apache.hadoop.ipc.Client$Call:getRpcResponse()" : null,
  "org.apache.hadoop.metrics2.lib.MutableRollingAverages:getStats(long)" : "* Retrieve a map of metric name {@literal ->} (aggregate).\n   * Filter out entries that don't have at least minSamples.\n   *\n   * @param minSamples input minSamples.\n   * @return a map of peer DataNode Id to the average latency to that\n   *         node seen over the measurement period.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache$Key:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:updateMountPointFsStatus(org.apache.hadoop.fs.viewfs.ViewFileSystem,java.util.Map,org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint,org.apache.hadoop.fs.Path)" : "* Update FsStatus for the given the mount point.\n   *\n   * @param viewFileSystem\n   * @param mountPointMap\n   * @param mountPoint\n   * @param path",
  "org.apache.hadoop.fs.statistics.IOStatisticsLogging:demandStringifyIOStatistics(org.apache.hadoop.fs.statistics.IOStatistics)" : "* On demand stringifier of an IOStatistics instance.\n   * <p>\n   * Whenever this object's toString() method is called, it evaluates the\n   * statistics.\n   * <p>\n   * This is for use in log statements where for the cost of creation\n   * of this entry is low; it is affordable to use in log statements.\n   * @param statistics statistics to stringify -may be null.\n   * @return an object whose toString() operation returns the current values.",
  "org.apache.hadoop.fs.shell.CommandFactory:<init>(org.apache.hadoop.conf.Configuration)" : "* Factory constructor for commands\n   * @param conf the hadoop configuration",
  "org.apache.hadoop.crypto.key.KeyProvider$Options:setBitLength(int)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:getSegmentDescriptors(int)" : "Return (& remove) the requested number of segment descriptors from the\n       * sorted map.",
  "org.apache.hadoop.io.MultipleIOException:<init>(java.util.List)" : "Constructor is private, use {@link #createIOException(List)}.",
  "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getParameterClasses()" : "The parameter classes.",
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)" : null,
  "org.apache.hadoop.net.NetworkTopology:sortByDistanceUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)" : "* Sort nodes array by network distance to <i>reader</i> with secondary sort.\n   * <p> using network location. This is used when the reader\n   * is not a datanode. Sorting the nodes based on network distance\n   * from the reader reduces network traffic and improves\n   * performance.\n   * </p>\n   *\n   * @param reader    Node where data will be read\n   * @param nodes     Available replicas with the requested data\n   * @param activeLen Number of active nodes at the front of the array",
  "org.apache.hadoop.conf.Configuration:setInt(java.lang.String,int)" : "* Set the value of the <code>name</code> property to an <code>int</code>.\n   * \n   * @param name property name.\n   * @param value <code>int</code> value of the property.",
  "org.apache.hadoop.io.SequenceFile$Sorter:sortPass(boolean)" : null,
  "org.apache.hadoop.io.compress.SnappyCodec:getDecompressorType()" : "* Get the type of {@link Decompressor} needed by this {@link CompressionCodec}.\n   *\n   * @return the type of decompressor needed by this codec.",
  "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:xdr_string(java.lang.String)" : "* Puts a string into the buffer by first writing the size of the string as an\n   * int, followed by the bytes of the string, padded if necessary to a multiple\n   * of 4.\n   * @param s the string to be written to buffer at offset location",
  "org.apache.hadoop.security.SaslRpcClient:isValidAuthType(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.MapFile$Reader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)" : "* Read the next key/value pair in the map into <code>key</code> and\n     * <code>val</code>.  Returns true if such a pair exists and false when at\n     * the end of the map.\n     *\n     * @param key WritableComparable.\n     * @param val Writable.\n     * @return if such a pair exists true,not false.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.BytesWritable:setCapacity(int)" : "* Change the capacity of the backing storage. The data is preserved.\n   *\n   * @param capacity The new capacity in bytes.",
  "org.apache.hadoop.fs.permission.AclUtil:getAclFromPermAndEntries(org.apache.hadoop.fs.permission.FsPermission,java.util.List)" : "* Given permissions and extended ACL entries, returns the full logical ACL.\n   *\n   * @param perm FsPermission containing permissions\n   * @param entries List&lt;AclEntry&gt; containing extended ACL entries\n   * @return List&lt;AclEntry&gt; containing full logical ACL",
  "org.apache.hadoop.util.DiskChecker$DiskErrorException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem:getDefaultBlockSize()" : "* Return the number of bytes that large input files should be optimally\n   * be split into to minimize I/O time.\n   * @deprecated use {@link #getDefaultBlockSize(Path)} instead\n   * @return default block size.",
  "org.apache.hadoop.ipc.ResponseBuffer:<init>(int)" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:decompress(java.nio.ByteBuffer,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.util.IntrusiveCollection$IntrusiveIterator:hasNext()" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.http.HttpServer2$Builder:setAuthFilterConfigurationPrefixes(java.lang.String[])" : null,
  "org.apache.hadoop.conf.Configuration:<init>()" : "A new configuration.",
  "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:isMultiThreadNecessary(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)" : "* Add a link to the config for the default mount table\n   * @param conf - add the link to this conf\n   * @param src - the src path name\n   * @param target - the target URI link",
  "org.apache.hadoop.fs.FilterFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.util.OperationDuration:value()" : "* Get the duration in milliseconds.\n   *\n   * <p>\n   * This will be 0 until a call\n   * to {@link #finished()} has been made.\n   * </p>\n   * @return the currently recorded duration.",
  "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:toString()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:adjustOrder(java.lang.Object[],java.lang.Object[],int[],int[],java.lang.Object[],java.lang.Object[])" : null,
  "org.apache.hadoop.security.alias.UserProvider:flush()" : null,
  "org.apache.hadoop.fs.viewfs.ConfigUtil:getDefaultMountTableName(org.apache.hadoop.conf.Configuration)" : "* Get the name of the default mount table to use. If\n   * {@link Constants#CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE_NAME_KEY} is specified,\n   * it's value is returned. Otherwise,\n   * {@link Constants#CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE} is returned.\n   *\n   * @param conf Configuration to use.\n   * @return the name of the default mount table to use.",
  "org.apache.hadoop.util.ShutdownThreadsHelper:shutdownThread(java.lang.Thread)" : "* @param thread {@link Thread to be shutdown}\n   * @return <tt>true</tt> if the thread is successfully interrupted,\n   * <tt>false</tt> otherwise",
  "org.apache.hadoop.util.concurrent.HadoopExecutors:newSingleThreadExecutor()" : null,
  "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:loadKeyManager(java.nio.file.Path)" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination:processArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.security.ssl.SSLFactory:disableExcludedCiphers(javax.net.ssl.SSLEngine)" : null,
  "org.apache.hadoop.fs.statistics.impl.StubDurationTracker:close()" : null,
  "org.apache.hadoop.fs.GlobPattern:<init>(java.lang.String)" : "* Construct the glob pattern object with a glob pattern string\n   * @param globPattern the glob pattern string",
  "org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:shouldRetry(java.lang.Exception,int,int,boolean)" : null,
  "org.apache.hadoop.fs.DF:getCapacity()" : "@return the capacity of the measured filesystem in bytes.",
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hflush()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextBlockMarker()" : "* Skips bytes in the stream until the start marker of a block is reached\n   * or end of stream is reached. Used for testing purposes to identify the\n   * start offsets of blocks.",
  "org.apache.hadoop.util.XMLUtils:bestEffortSetAttribute(javax.xml.transform.TransformerFactory,java.util.concurrent.atomic.AtomicBoolean,java.lang.String,java.lang.Object)" : "* Set an attribute value on a {@link TransformerFactory}. If the TransformerFactory\n   * does not support the attribute, the method just returns <code>false</code> and\n   * logs the issue at debug level.\n   *\n   * @param transformerFactory to update\n   * @param flag that indicates whether to do the update and the flag can be set to\n   *             <code>false</code> if an update fails\n   * @param name of the attribute to set\n   * @param value to set on the attribute",
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:available()" : null,
  "org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferPool:acquireHelper(int,boolean)" : null,
  "org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:getPermissions(com.jcraft.jsch.ChannelSftp$LsEntry)" : "* Return file permission.\n   *\n   * @param sftpFile\n   * @return file permission",
  "org.apache.hadoop.conf.StorageUnit$1:fromBytes(double)" : null,
  "org.apache.hadoop.metrics2.lib.MethodMetric:newCounter(java.lang.Class)" : null,
  "org.apache.hadoop.fs.FileContext:getStoragePolicy(org.apache.hadoop.fs.Path)" : "* Query the effective storage policy ID for the given file or directory.\n   *\n   * @param path file or directory path.\n   * @return storage policy for give file.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:getAuthMethod()" : null,
  "org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForUserCreds(boolean)" : "* Spawn a thread to do periodic renewals of kerberos credentials. NEVER\n   * directly call this method. This method should only be used for ticket cache\n   * based kerberos credentials.\n   *\n   * @param force - used by tests to forcibly spawn thread",
  "org.apache.hadoop.fs.FileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* The src file is on the local disk.  Add it to filesystem at\n   * the given dst name and the source is kept intact afterwards\n   * @param src path\n   * @param dst path\n   * @throws IOException IO failure",
  "org.apache.hadoop.util.functional.FutureIO:eval(org.apache.hadoop.util.functional.CallableRaisingIOE)" : "* Evaluate a CallableRaisingIOE in the current thread,\n   * converting IOEs to RTEs and propagating.\n   * @param callable callable to invoke\n   * @param <T> Return type.\n   * @return the evaluated result.\n   * @throws UnsupportedOperationException fail fast if unsupported\n   * @throws IllegalArgumentException invalid argument",
  "org.apache.hadoop.fs.impl.AbstractMultipartUploader:abortUploadsUnderPath(org.apache.hadoop.fs.Path)" : "* {@inheritDoc}.\n   * @param path path to abort uploads under.\n   * @return a future to -1.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.http.RestCsrfPreventionFilter$ServletFilterHttpInteraction:<init>(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse,javax.servlet.FilterChain)" : "* Creates a new ServletFilterHttpInteraction.\n     *\n     * @param httpRequest request to process\n     * @param httpResponse response to process\n     * @param chain filter chain to forward to if HTTP interaction is allowed",
  "org.apache.hadoop.util.concurrent.HadoopExecutors:newFixedThreadPool(int,java.util.concurrent.ThreadFactory)" : null,
  "org.apache.hadoop.ha.ZKFailoverController$HealthCallbacks:enteredState(org.apache.hadoop.ha.HealthMonitor$State)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO:ensureInitialized()" : null,
  "org.apache.hadoop.fs.FsShellPermissions$Chown:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:<init>(long,int)" : "* Constructs an instance of {@link FilePosition}.\n   *\n   * @param fileSize size of the associated file.\n   * @param blockSize size of each block within the file.\n   *\n   * @throws IllegalArgumentException if fileSize is negative.\n   * @throws IllegalArgumentException if blockSize is zero or negative.",
  "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:calculateIV(byte[],long,byte[])" : null,
  "org.apache.hadoop.http.HtmlQuoting:unquoteHtmlChars(java.lang.String)" : "* Remove HTML quoting from a string.\n   * @param item the string to unquote\n   * @return the unquoted string",
  "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seekToNewSource(long)" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:getNext()" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getInstance()" : "* Get the single instance of this class.\n   * @return a shared, empty instance.",
  "org.apache.hadoop.fs.ContentSummary:getErasureCodingPolicyHeader()" : null,
  "org.apache.hadoop.fs.DelegateToFileSystem:getFsStatus()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartB()" : null,
  "org.apache.hadoop.fs.FileContext:getFileContext()" : "* Create a FileContext using the default config read from the\n   * $HADOOP_CONFIG/core.xml, Unspecified key-values for config are defaulted\n   * from core-defaults.xml in the release jar.\n   * \n   * @throws UnsupportedFileSystemException If the file system from the default\n   *           configuration is not supported\n   * @return file context.",
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:isIP4Address(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.SplitCompressionInputStream:setStart(long)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(org.apache.hadoop.conf.Configuration)" : "* Convenience Constructor for apps to call directly.\n   * @param conf configuration.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.nativeio.NativeIOException:getErrorCode()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)" : null,
  "org.apache.hadoop.metrics2.impl.MetricGaugeDouble:<init>(org.apache.hadoop.metrics2.MetricsInfo,double)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:toString()" : null,
  "org.apache.hadoop.security.KDiag:validateKinitExecutable()" : "* A cursory look at the {@code kinit} executable.\n   *\n   * If it is an absolute path: it must exist with a size > 0.\n   * If it is just a command, it has to be on the path. There's no check\n   * for that -but the PATH is printed out.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationConsumer(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.ConsumerRaisingIOE)" : "* Given an IOException raising Consumer,\n   * return a new one which wraps the inner and tracks\n   * the duration of the operation, including whether\n   * it passes/fails.\n   * @param factory factory of duration trackers\n   * @param statistic statistic key\n   * @param input input callable.\n   * @param <B> return type.\n   * @return a new consumer which tracks duration and failure.",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_setThreadIOStatisticsContext(java.lang.Object)" : "* Set the IOStatisticsContext for the current thread.\n   * @param statisticsContext IOStatistics context instance for the\n   * current thread. If null, the context is reset.\n   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsDir(java.nio.file.Path,java.lang.String)" : "* Validates that the given path exists and is a directory.\n   * @param path the path to check.\n   * @param argName the name of the argument being validated.",
  "org.apache.hadoop.fs.shell.find.BaseExpression:getPath(org.apache.hadoop.fs.shell.PathData)" : "* Returns the {@link Path} from the {@link PathData} item.\n   *\n   * @param item\n   *          PathData\n   * @return Path\n   *\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.DataOutputOutputStream:write(byte[])" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:<init>(int,int,long,java.util.concurrent.TimeUnit,java.util.concurrent.BlockingQueue,java.util.concurrent.RejectedExecutionHandler)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:forceDecay()" : null,
  "org.apache.hadoop.io.file.tfile.Utils$Version:toString()" : "* Return a string representation of the version.",
  "org.apache.hadoop.net.unix.DomainSocket:toString()" : null,
  "org.apache.hadoop.service.launcher.InterruptEscalator:<init>(org.apache.hadoop.service.launcher.ServiceLauncher,int)" : null,
  "org.apache.hadoop.fs.FileContext:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)" : "* Return blockLocation of the given file for the given offset and len.\n   *  For a nonexistent file or regions, null will be returned.\n   *\n   * This call is most helpful with DFS, where it returns \n   * hostnames of machines that contain the given file.\n   *\n   * In HDFS, if file is three-replicated, the returned array contains\n   * elements like:\n   * <pre>\n   * BlockLocation(offset: 0, length: BLOCK_SIZE,\n   *   hosts: {\"host1:9866\", \"host2:9866, host3:9866\"})\n   * BlockLocation(offset: BLOCK_SIZE, length: BLOCK_SIZE,\n   *   hosts: {\"host2:9866\", \"host3:9866, host4:9866\"})\n   * </pre>\n   *\n   * And if a file is erasure-coded, the returned BlockLocation are logical\n   * block groups.\n   *\n   * Suppose we have a RS_3_2 coded file (3 data units and 2 parity units).\n   * 1. If the file size is less than one stripe size, say 2 * CELL_SIZE, then\n   * there will be one BlockLocation returned, with 0 offset, actual file size\n   * and 4 hosts (2 data blocks and 2 parity blocks) hosting the actual blocks.\n   * 3. If the file size is less than one group size but greater than one\n   * stripe size, then there will be one BlockLocation returned, with 0 offset,\n   * actual file size with 5 hosts (3 data blocks and 2 parity blocks) hosting\n   * the actual blocks.\n   * 4. If the file size is greater than one group size, 3 * BLOCK_SIZE + 123\n   * for example, then the result will be like:\n   * <pre>\n   * BlockLocation(offset: 0, length: 3 * BLOCK_SIZE, hosts: {\"host1:9866\",\n   *   \"host2:9866\",\"host3:9866\",\"host4:9866\",\"host5:9866\"})\n   * BlockLocation(offset: 3 * BLOCK_SIZE, length: 123, hosts: {\"host1:9866\",\n   *   \"host4:9866\", \"host5:9866\"})\n   * </pre>\n   *\n   * @param f - get blocklocations of this file\n   * @param start position (byte offset)\n   * @param len (in bytes)\n   *\n   * @return block locations for given file at specified offset of len\n   *\n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If <code>f</code> does not exist\n   * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n   *           not supported\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server\n   * \n   * RuntimeExceptions:\n   * @throws InvalidPathException If path <code>f</code> is invalid",
  "org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String)" : "* Constructor -increments the counter by 1.\n   * @param iostats statistics to update\n   * @param key prefix of values.",
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateJmxCache()" : null,
  "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:getLogger()" : null,
  "org.apache.hadoop.security.NullGroupsMapping:cacheGroupsRefresh()" : "* Nothing is returned, so nothing is cached.",
  "org.apache.hadoop.fs.sftp.SFTPInputStream:seekInternal()" : null,
  "org.apache.hadoop.io.compress.SnappyCodec:getCompressorType()" : "* Get the type of {@link Compressor} needed by this {@link CompressionCodec}.\n   *\n   * @return the type of compressor needed by this codec.",
  "org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator:remove()" : null,
  "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:hasLogged()" : null,
  "org.apache.hadoop.util.functional.RemoteIterators:toArray(org.apache.hadoop.fs.RemoteIterator,java.lang.Object[])" : "* Build an array from a RemoteIterator.\n   * @param source source iterator\n   * @param a destination array; if too small a new array\n   * of the same type is created\n   * @param <T> type\n   * @return an array of the values.\n   * @throws IOException if the source RemoteIterator raises it.",
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:complete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Constructor.\n   * @param uri base file system\n   * @param conf configuration\n   * @throws IOException",
  "org.apache.hadoop.fs.shell.PathData:toString()" : "* Returns the printable version of the path that is either the path\n   * as given on the commandline, or the full path\n   * @return String of the path",
  "org.apache.hadoop.fs.PathIOException:<init>(java.lang.String)" : "* Constructor a generic I/O error exception\n   *  @param path for the exception",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.NetgroupCache:getNetgroupNames()" : "* Get the list of cached netgroups\n   *\n   * @return list of cached groups",
  "org.apache.hadoop.ipc.Server$Connection:isOnAuxiliaryPort()" : null,
  "org.apache.hadoop.util.StopWatch:reset()" : "* Reset elapsed time to zero and make the state of stopwatch stop.\n   * @return this instance of StopWatch.",
  "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.ipc.Client$ConnectionId:hashCode()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)" : null,
  "org.apache.hadoop.util.CrcUtil:writeInt(byte[],int,int)" : "* Writes big-endian representation of {@code value} into {@code buf}\n   * starting at {@code offset}. buf.length must be greater than or\n   * equal to offset + 4.\n   *\n   * @param buf buf size.\n   * @param offset offset.\n   * @param value value.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult:isInternalDir()" : null,
  "org.apache.hadoop.util.bloom.DynamicBloomFilter:xor(org.apache.hadoop.util.bloom.Filter)" : null,
  "org.apache.hadoop.io.compress.SnappyCodec:createInputStream(java.io.InputStream)" : "* Create a {@link CompressionInputStream} that will read from the given\n   * input stream.\n   *\n   * @param in the stream to read compressed bytes from\n   * @return a stream to read uncompressed bytes from\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.sftp.SFTPInputStream:read()" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping:getGid(java.lang.String)" : null,
  "org.apache.hadoop.log.LogThrottlingHelper$NoLogAction:shouldLog()" : null,
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)" : "* Decrypts an encrypted byte[] key material using the given a key version\n   * name and initialization vector.\n   *\n   * @param encryptedKey contains keyVersionName and IV to decrypt the encrypted \n   * key material\n   * @return a KeyVersion with the decrypted key material, the version name is\n   * 'EK' (For Encryption Key)\n   * @throws IOException thrown if the key material could not be decrypted\n   * @throws GeneralSecurityException thrown if the key material could not be \n   * decrypted because of a cryptographic issue.",
  "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getInputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree:addRegexMountEntry(org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry)" : null,
  "org.apache.hadoop.fs.FileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)" : "* List the statuses of the files/directories in the given path if the path is\n   * a directory.\n   * Return the file's status and block locations If the path is a file.\n   *\n   * If a returned status is a file, it contains the file's block locations.\n   *\n   * @param f is the path\n   *\n   * @return an iterator that traverses statuses of the files/directories\n   *         in the given path\n   *\n   * @throws FileNotFoundException If <code>f</code> does not exist\n   * @throws IOException If an I/O error occurred",
  "org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int,long,long)" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:<init>(java.lang.reflect.Method,java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatusForFallbackLink()" : null,
  "org.apache.hadoop.io.SequenceFile$Metadata:equals(org.apache.hadoop.io.SequenceFile$Metadata)" : null,
  "org.apache.hadoop.fs.FileStatus:setOwner(java.lang.String)" : "* Sets owner.\n   * @param owner if it is null, default value is set",
  "org.apache.hadoop.ipc.Client$Call:setRpcResponse(org.apache.hadoop.io.Writable)" : "Set the return value when there is no error. \n     * Notify the caller the call is done.\n     * \n     * @param rpcResponse return value of the rpc call.",
  "org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:write(java.lang.String)" : null,
  "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:sync()" : "Compress and flush contents to dfs",
  "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:getGroupsSet(java.lang.String)" : null,
  "org.apache.hadoop.fs.HarFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : "* not implemented.",
  "org.apache.hadoop.service.AbstractService:registerGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener)" : "* Register a global listener, which receives notifications\n   * from the state change events of all services in the JVM\n   * @param l listener",
  "org.apache.hadoop.io.BinaryComparable:compareTo(byte[],int,int)" : "* Compare bytes from {#getBytes()} to those provided.\n   *\n   * @param other other.\n   * @param off off.\n   * @param len len.\n   * @return compareBytes.",
  "org.apache.hadoop.fs.FilterFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])" : null,
  "org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)" : "* Authorize the superuser which is doing doAs.\n   *\n   * @param user ugi of the effective or proxy user which contains a real user\n   * @param remoteAddress the inet address of client\n   * @throws AuthorizationException Authorization Exception.",
  "org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials,boolean)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:context()" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getPermission()" : null,
  "org.apache.hadoop.net.unix.DomainSocket$DomainChannel:read(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)" : "* This method is intended to be used for recovering persisted delegation\n   * tokens. Tokens that have an unknown <code>DelegationKey</code> are\n   * marked as expired and automatically cleaned up.\n   * This method must be called before this secret manager is activated (before\n   * startThreads() is called)\n   * @param identifier identifier read from persistent storage\n   * @param renewDate token renew time\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)" : "Create the named map using the named key comparator. \n     * @deprecated Use Writer(Configuration, Path, Option...) instead.\n     * @param conf configuration.\n     * @param fs fs.\n     * @param dirName dirName.\n     * @param comparator comparator.\n     * @param valClass valClass.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.shell.Head:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.permission.PermissionStatus:getPermission()" : "* Return permission.\n   * @return FsPermission.",
  "org.apache.hadoop.fs.XAttrSetFlag:validate(java.lang.String,boolean,java.util.EnumSet)" : null,
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:cacheGroupsAdd(java.util.List)" : "* Adds groups to cache, no need to do that for this provider\n   *\n   * @param groups unused",
  "org.apache.hadoop.util.CrcComposer:update(int,long)" : "* Updates with a single additional CRC which corresponds to an underlying\n   * data size of {@code bytesPerCrc}.\n   *\n   * @param crcB crcB.\n   * @param bytesPerCrc bytesPerCrc.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String)" : "* Unescape commas in the string using the default escape char\n   * @param str a string\n   * @return an unescaped string",
  "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getCurrentKeyId()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferData:stateEqualsOneOf(org.apache.hadoop.fs.impl.prefetch.BufferData$State[])" : null,
  "org.apache.hadoop.fs.GetSpaceUsed$Builder:setInterval(long)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer$BlockSizeOption:<init>(long)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.MergeSort:mergeSort(int[],int[],int,int)" : null,
  "org.apache.hadoop.ipc.RetryCache$CacheEntry:setNext(org.apache.hadoop.util.LightWeightGSet$LinkedElement)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:sort(int)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:init()" : null,
  "org.apache.hadoop.fs.BlockLocation:setOffset(long)" : "* Set the start offset of file associated with this block.\n   * @param offset start offset.",
  "org.apache.hadoop.io.compress.CompressionInputStream:seek(long)" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:setSlope(org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)" : "* @param slope the slope to set",
  "org.apache.hadoop.ipc.DecayRpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.ipc.Client$ConnectionId:getTicket()" : null,
  "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:setHasLogged()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:changeStateToProcessABlock()" : null,
  "org.apache.hadoop.ha.ShellCommandFencer:tryGetPid(java.lang.Process)" : "* Attempt to use evil reflection tricks to determine the\n   * pid of a launched process. This is helpful to ops\n   * if debugging a fencing process that might have gone\n   * wrong. If running on a system or JVM where this doesn't\n   * work, it will simply return null.",
  "org.apache.hadoop.util.Daemon$DaemonFactory:newThread(java.lang.Runnable)" : null,
  "org.apache.hadoop.security.token.DtUtilShell$Remove:getUsage()" : null,
  "org.apache.hadoop.crypto.key.KeyProvider$Options:getDescription()" : null,
  "org.apache.hadoop.conf.StorageUnit$7:fromBytes(double)" : null,
  "org.apache.hadoop.net.SocketOutputStream$Writer:<init>(java.nio.channels.WritableByteChannel,long)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:readUIntLE(byte[],int)" : null,
  "org.apache.hadoop.metrics2.lib.DefaultMetricsFactory:getInstance(java.lang.Class)" : null,
  "org.apache.hadoop.fs.LocalFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.util.concurrent.HadoopExecutors:newSingleThreadScheduledExecutor(java.util.concurrent.ThreadFactory)" : null,
  "org.apache.hadoop.crypto.key.UserProvider:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.DirectoryListingStartAfterNotFoundException:<init>()" : null,
  "org.apache.hadoop.io.MapFile$Writer:keyClass(java.lang.Class)" : null,
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:updateFlushTime(java.util.Date)" : "* Update the {@link #nextFlush} variable to the next flush time. Add\n   * an integer number of flush intervals, preserving the initial random offset.\n   *\n   * @param now the current time",
  "org.apache.hadoop.io.retry.RetryUtils:getMultipleLinearRandomRetry(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String)" : "* Return the MultipleLinearRandomRetry policy specified in the conf,\n   * or null if the feature is disabled.\n   * If the policy is specified in the conf but the policy cannot be parsed,\n   * the default policy is returned.\n   * \n   * Retry policy spec:\n   *   N pairs of sleep-time and number-of-retries \"s1,n1,s2,n2,...\"\n   * \n   * @param conf configuration.\n   * @param retryPolicyEnabledKey     conf property key for enabling retry\n   * @param defaultRetryPolicyEnabled default retryPolicyEnabledKey conf value \n   * @param retryPolicySpecKey        conf property key for retry policy spec\n   * @param defaultRetryPolicySpec    default retryPolicySpecKey conf value\n   * @return the MultipleLinearRandomRetry policy specified in the conf,\n   *         or null if the feature is disabled.",
  "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)" : "* Construct the preferred type of SequenceFile Writer.\n   * @param fs The configured filesystem. \n   * @param conf The configuration.\n   * @param name The name of the file. \n   * @param keyClass The 'key' type.\n   * @param valClass The 'value' type.\n   * @param compressionType The compression type.\n   * @param codec The compression codec.\n   * @param progress The Progressable object to track progress.\n   * @return Returns the handle to the constructed SequenceFile Writer.\n   * @throws IOException raised on errors performing I/O.\n   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}\n   *     instead.",
  "org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:shouldPreserveInput()" : null,
  "org.apache.hadoop.fs.Globber$GlobBuilder:withPathPattern(org.apache.hadoop.fs.Path)" : "* Set the path pattern.\n     * @param pattern pattern to use.\n     * @return the builder",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getRealUser()" : null,
  "org.apache.hadoop.ipc.RefreshResponse:getMessage()" : null,
  "org.apache.hadoop.util.Lists:checkNonnegative(int,java.lang.String)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetBuffer(java.nio.ByteBuffer,int)" : "* Ensure a buffer filled with ZERO bytes from current readable/writable\n   * position.\n   * @param buffer a buffer ready to read / write certain size bytes\n   * @return the buffer itself, with ZERO bytes written, the position and limit\n   *         are not changed after the call",
  "org.apache.hadoop.util.bloom.Key:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateAttrCache(java.lang.Iterable)" : null,
  "org.apache.hadoop.net.NetworkTopology:setRandomSeed(long)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:numberOfBytesTillNextMarker(java.io.InputStream)" : "* Returns the number of bytes between the current stream position\n   * and the immediate next BZip2 block marker.\n   *\n   * @param in\n   *             The InputStream\n   *\n   * @return long Number of bytes between current stream position and the\n   * next BZip2 block start marker.\n * @throws IOException raised on errors performing I/O.\n   *",
  "org.apache.hadoop.conf.StorageUnit$6:toBytes(double)" : null,
  "org.apache.hadoop.fs.FileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : "* This version of the mkdirs method assumes that the permission is absolute.\n   * It has been added to support the FileContext that processes the permission\n   * with umask before calling this method.\n   * This a temporary method added to support the transition from FileSystem\n   * to FileContext for user applications.\n   * @param f path\n   * @param absolutePermission permissions\n   * @return true if the directory was actually created.\n   * @throws IOException IO failure\n   * @see #mkdirs(Path, FsPermission)",
  "org.apache.hadoop.util.HostsFileReader$HostDetails:getExcludedHosts()" : null,
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getRollInterval()" : "* Extract the roll interval from the configuration and return it in\n   * milliseconds.\n   *\n   * @return the roll interval in millis",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.Options$HandleOpt:content()" : "* Handle is valid iff the content of the referent is the same.\n     * Equivalent to changed(false), moved(true).\n     * @return Options requiring that the content of the entity is unchanged,\n     * but it may be at a different location.",
  "org.apache.hadoop.io.SequenceFile$Sorter$SegmentContainer:getSegmentList()" : null,
  "org.apache.hadoop.util.ComparableVersion$StringItem:getType()" : null,
  "org.apache.hadoop.crypto.key.KeyProvider:isTransient()" : "* Indicates whether this provider represents a store\n   * that is intended for transient use - such as the UserProvider\n   * is. These providers are generally used to provide access to\n   * keying material rather than for long term storage.\n   * @return true if transient, false otherwise",
  "org.apache.hadoop.fs.AbstractFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)" : "* The specification of this method matches that of\n   * {@link FileContext#listLocatedStatus(Path)} except that Path f \n   * must be for this file system.\n   *\n   * In HDFS implementation, the BlockLocation of returned LocatedFileStatus\n   * will have different formats for replicated and erasure coded file. Please\n   * refer to {@link FileSystem#getFileBlockLocations(FileStatus, long, long)}\n   * for more details.\n   *\n   * @param f the path.\n   * @throws AccessControlException access control exception.\n   * @throws FileNotFoundException file not found exception.\n   * @throws UnresolvedLinkException unresolved link exception.\n   * @throws IOException raised on errors performing I/O.\n   * @return FileStatus Iterator.",
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:containsValue(java.lang.Object)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : "* The src file is under FS, and the dst is on the local disk.\n   * Copy it from FS control to the local dst name.\n   * If src and dst are directories, the copyCrc parameter\n   * determines whether to copy CRC files.\n   * @param src src path.\n   * @param dst dst path.\n   * @param copyCrc copy csc flag.\n   * @throws IOException if an I/O error occurs.",
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.StringUtils:hasChar(char[],char)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsBufferBuilder:get()" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getCompressionName()" : "* Get the name of the compression algorithm used to compress the block.\n       * \n       * @return name of the compression algorithm.",
  "org.apache.hadoop.security.SaslInputStream:read(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:setHelp(java.lang.String[])" : "* Sets the help text for this {@link Expression} .\n   * @param help help.",
  "org.apache.hadoop.net.NetworkTopology:getFirstHalf(java.lang.String)" : "* @return Divide networklocation string into two parts by last separator, and get\n   * the first part here.\n   * \n   * @param networkLocation input networkLocation.",
  "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsMachineList(java.lang.Class)" : null,
  "org.apache.hadoop.fs.QuotaUsage:toString(boolean)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)" : "* Create a new key generating the material for it.\n   * The given key must not already exist.\n   * <p>\n   * This implementation generates the key material and calls the\n   * {@link #createKey(String, byte[], Options)} method.\n   *\n   * @param name the base name of the key\n   * @param options the options for the new key.\n   * @return the version name of the first version of the key.\n   * @throws IOException raised on errors performing I/O.\n   * @throws NoSuchAlgorithmException no such algorithm exception.",
  "org.apache.hadoop.security.UserGroupInformation:getGroupNames()" : "* Get the group names for this user. {@link #getGroupsSet()} is less\n   * expensive alternative when checking for a contained element.\n   * @return the list of users with the primary group first. If the command\n   *    fails, it returns an empty list.",
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:open(org.apache.hadoop.fs.Path,int)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:metrics()" : null,
  "org.apache.hadoop.fs.BulkDeleteUtils:<init>()" : null,
  "org.apache.hadoop.io.wrappedio.WrappedIO:fileSystem_openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.FileStatus,java.lang.Long,java.util.Map)" : "* OpenFile assistant, easy reflection-based access to\n   * {@link FileSystem#openFile(Path)} and blocks\n   * awaiting the operation completion.\n   * @param fs filesystem\n   * @param path path\n   * @param policy read policy\n   * @param status optional file status\n   * @param length optional file length\n   * @param options nullable map of other options\n   * @return stream of the opened file\n   * @throws UncheckedIOException if an IOE was raised.",
  "org.apache.hadoop.fs.RawLocalFileSystem:exists(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:initialize(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,long,org.apache.hadoop.conf.Configuration,boolean)" : "Common work of the constructors.",
  "org.apache.hadoop.ipc.Server:getMaxIdleTime()" : null,
  "org.apache.hadoop.ipc.Server$Call:setDeferredError(java.lang.Throwable)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<init>()" : "* Construct the system but not initializing (read config etc.) it.",
  "org.apache.hadoop.util.FindClass:<init>()" : "* Empty constructor; passes a new Configuration\n   * object instance to its superclass's constructor",
  "org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,java.lang.Object,byte[],int)" : "* Static method that provides null check for retryCache.\n   * @param cache input cache.\n   * @param payload input payload.\n   * @param clientId client id of this request\n   * @param callId client call id of this request\n   * @return CacheEntryWithPayload.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:merge(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileStatus[])" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:close()" : null,
  "org.apache.hadoop.fs.TrashPolicyDefault:getEmptier()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeyVersions(java.lang.String)" : null,
  "org.apache.hadoop.service.CompositeService$CompositeServiceShutdownHook:<init>(org.apache.hadoop.service.CompositeService)" : null,
  "org.apache.hadoop.fs.HarFileSystem:makeRelative(java.lang.String,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket,long)" : "* Return a {@link SocketInputWrapper} for the socket and set the given\n   * timeout. If the socket does not have an associated channel, then its socket\n   * timeout will be set to the specified value. Otherwise, a\n   * {@link SocketInputStream} will be created which reads with the configured\n   * timeout.\n   * \n   * Any socket created using socket factories returned by {@link #NetUtils},\n   * must use this interface instead of {@link Socket#getInputStream()}.\n   * \n   * In general, this should be called only once on each socket: see the note\n   * in {@link SocketInputWrapper#setTimeout(long)} for more information.\n   *\n   * @see Socket#getChannel()\n   * \n   * @param socket socket.\n   * @param timeout timeout in milliseconds. zero for waiting as\n   *                long as necessary.\n   * @return SocketInputWrapper for reading from the socket.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.service.launcher.ServiceLauncher:exitWithUsageMessage()" : "* Exit with the usage exit code {@link #EXIT_USAGE}\n   * and message {@link #USAGE_MESSAGE}.\n   * @throws ExitUtil.ExitException if exceptions are disabled",
  "org.apache.hadoop.fs.ftp.FTPInputStream:getPos()" : null,
  "org.apache.hadoop.fs.FileUtil:unTar(java.io.InputStream,java.io.File,boolean)" : "* Given a Tar File as input it will untar the file in a the untar directory\n   * passed as the second parameter\n   *\n   * This utility will untar \".tar\" files and \".tar.gz\",\"tgz\" files.\n   *\n   * @param inputStream The tar file as input.\n   * @param untarDir The untar directory where to untar the tar file.\n   * @param gzipped The input stream is gzipped\n   *                TODO Use magic number and PusbackInputStream to identify\n   * @throws IOException an exception occurred\n   * @throws InterruptedException command interrupted\n   * @throws ExecutionException task submit failed",
  "org.apache.hadoop.util.concurrent.ExecutorHelper:logThrowableFromAfterExecute(java.lang.Runnable,java.lang.Throwable)" : null,
  "org.apache.hadoop.util.StringUtils:getStackTrace(java.lang.Thread)" : "* Get stack trace for a given thread.\n   * @param t thread.\n   * @return stack trace string.",
  "org.apache.hadoop.security.UserGroupInformation:getRealAuthenticationMethod()" : "* Get the authentication method from the real user's subject.  If there\n   * is no real user, return the given user's authentication method.\n   * \n   * @return AuthenticationMethod in the subject, null if not present.",
  "org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState)" : null,
  "org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:toString()" : null,
  "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:get(java.lang.Object)" : null,
  "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:getServerAliases(java.lang.String,java.security.Principal[])" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])" : "* Validate outputs decoded from inputs, by decoding an input back from\n   * the outputs and comparing it with the original one.\n   *\n   * For instance, in RS (6, 3), let (d0, d1, d2, d3, d4, d5) be sources\n   * and (p0, p1, p2) be parities, and assume\n   *  inputs = [d0, null (d1), d2, d3, d4, d5, null (p0), p1, null (p2)];\n   *  erasedIndexes = [1, 6];\n   *  outputs = [d1, p0].\n   * Then\n   *  1. Create new inputs, erasedIndexes and outputs for validation so that\n   *     the inputs could contain the decoded outputs, and decode them:\n   *      newInputs = [d1, d2, d3, d4, d5, p0]\n   *      newErasedIndexes = [0]\n   *      newOutputs = [d0']\n   *  2. Compare d0 and d0'. The comparison will fail with high probability\n   *     when the initial outputs are wrong.\n   *\n   * Note that the input buffers' positions must be the ones where data are\n   * read: If the input buffers have been processed by a decoder, the buffers'\n   * positions must be reset before being passed into this method.\n   *\n   * This method does not change outputs and erasedIndexes.\n   *\n   * @param inputs input buffers used for decoding. The buffers' position\n   *               are moved to the end after this method.\n   * @param erasedIndexes indexes of erased units used for decoding\n   * @param outputs decoded output buffers, which are ready to be read after\n   *                the call\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:toString()" : null,
  "org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,org.apache.hadoop.metrics2.MetricsSystem$Callback)" : null,
  "org.apache.hadoop.conf.Configuration:getConfResourceAsReader(java.lang.String)" : "* Get a {@link Reader} attached to the configuration resource with the\n   * given <code>name</code>.\n   * \n   * @param name configuration resource name.\n   * @return a reader attached to the resource.",
  "org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager)" : null,
  "org.apache.hadoop.ipc.Client$Connection$2:run()" : null,
  "org.apache.hadoop.fs.viewfs.ConfigUtil:isNestedMountPointSupported(org.apache.hadoop.conf.Configuration)" : "* Check the bool config whether nested mount point is supported. Default: true\n   * @param conf - from this conf\n   * @return whether nested mount point is supported",
  "org.apache.hadoop.fs.shell.PathData:representsDirectory()" : "* Check if the path represents a directory as determined by the basename\n   * being \".\" or \"..\", or the path ending with a directory separator \n   * @return boolean if this represents a directory",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDtService(java.net.URI)" : null,
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:reset(java.io.DataInputStream)" : null,
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore)" : null,
  "org.apache.hadoop.fs.Options$CreateOpts$ReplicationFactor:getValue()" : null,
  "org.apache.hadoop.fs.QuotaUsage:getStorageTypeHeader(java.util.List)" : "* return the header of with the StorageTypes.\n   *\n   * @param storageTypes storage types.\n   * @return storage header string",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeyVersions(java.lang.String)" : null,
  "org.apache.hadoop.fs.ContentSummary:equals(java.lang.Object)" : null,
  "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterMap()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:getStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.Server$RpcCall:setResponse(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.fs.FileSystem$Cache:getUnique(java.net.URI,org.apache.hadoop.conf.Configuration)" : "The objects inserted into the cache using this method are all unique.",
  "org.apache.hadoop.util.concurrent.HadoopExecutors:shutdown(java.util.concurrent.ExecutorService,org.slf4j.Logger,long,java.util.concurrent.TimeUnit)" : "* Helper routine to shutdown a {@link ExecutorService}. Will wait up to a\n   * certain timeout for the ExecutorService to gracefully shutdown. If the\n   * ExecutorService did not shutdown and there are still tasks unfinished after\n   * the timeout period, the ExecutorService will be notified to forcibly shut\n   * down. Another timeout period will be waited before giving up. So, at most,\n   * a shutdown will be allowed to wait up to twice the timeout value before\n   * giving up.\n   *\n   * @param executorService ExecutorService to shutdown\n   * @param logger Logger\n   * @param timeout the maximum time to wait\n   * @param unit the time unit of the timeout argument",
  "org.apache.hadoop.ipc.RPC$Server:addProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)" : "* Add a protocol to the existing server.\n     * @param rpcKind - input rpcKind\n     * @param protocolClass - the protocol class\n     * @param protocolImpl - the impl of the protocol that will be called\n     * @return the server (for convenience)",
  "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolServerSideTranslatorPB:refreshUserToGroupsMappings(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.security.proto.RefreshUserMappingsProtocolProtos$RefreshUserToGroupsMappingsRequestProto)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:add(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData)" : "* Add another StatisticsData object to this one.",
  "org.apache.hadoop.ipc.RemoteException:valueOf(org.xml.sax.Attributes)" : "* Create RemoteException from attributes.\n   * @param attrs may not be null.\n   * @return RemoteException.",
  "org.apache.hadoop.fs.FileEncryptionInfo:toString()" : null,
  "org.apache.hadoop.fs.impl.WeakRefMetricsSource:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)" : "* If the weak reference is non null, update the metrics.\n   * @param collector to contain the resulting metrics snapshot\n   * @param all if true, return all metrics even if unchanged.",
  "org.apache.hadoop.HadoopIllegalArgumentException:<init>(java.lang.String)" : "* Constructs exception with the specified detail message. \n   * @param message detailed message.",
  "org.apache.hadoop.security.http.CrossOriginFilter:getAllowedHeadersHeader()" : null,
  "org.apache.hadoop.ipc.Server$Responder:doPurge(org.apache.hadoop.ipc.Server$RpcCall,long)" : null,
  "org.apache.hadoop.io.erasurecode.CodecUtil:getRawCoderNames(org.apache.hadoop.conf.Configuration,java.lang.String)" : null,
  "org.apache.hadoop.ipc.ResponseBuffer:setCapacity(int)" : null,
  "org.apache.hadoop.util.concurrent.AsyncGetFuture:callAsyncGet(long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.net.NodeBase:<init>(java.lang.String)" : "Construct a node from its path\n   * @param path \n   *   a concatenation of this node's location, the path separator, and its name",
  "org.apache.hadoop.fs.ContentSummary:toString()" : null,
  "org.apache.hadoop.security.AnnotatedSecurityInfo:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)" : null,
  "org.apache.hadoop.ipc.RPC$Server$ProtoNameVer:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferData:getBlockNumber()" : "* Gets the id of this block.\n   *\n   * @return the id of this block.",
  "org.apache.hadoop.fs.AbstractFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : "* The specification of this method matches that of\n   * {@link FileContext#rename(Path, Path, Options.Rename...)} except that Path\n   * f must be for this file system.\n   *\n   * @param src src.\n   * @param dst dst.\n   * @param overwrite overwrite flag.\n   * @throws AccessControlException access control exception.\n   * @throws FileAlreadyExistsException file already exists exception.\n   * @throws FileNotFoundException file not found exception.\n   * @throws ParentNotDirectoryException parent not directory exception.\n   * @throws UnresolvedLinkException unresolved link exception.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:size()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource,java.lang.Iterable,long,org.apache.hadoop.metrics2.impl.MetricsConfig)" : null,
  "org.apache.hadoop.security.SaslPlainServer:wrap(byte[],int,int)" : null,
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:<init>()" : null,
  "org.apache.hadoop.fs.DelegateToFileSystem:getCanonicalServiceName()" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeys()" : null,
  "org.apache.hadoop.ipc.Server:channelRead(java.nio.channels.ReadableByteChannel,java.nio.ByteBuffer)" : "* This is a wrapper around {@link ReadableByteChannel#read(ByteBuffer)}.\n   * If the amount of data is large, it writes to channel in smaller chunks. \n   * This is to avoid jdk from creating many direct buffers as the size of \n   * ByteBuffer increases. There should not be any performance degredation.\n   * \n   * @see ReadableByteChannel#read(ByteBuffer)",
  "org.apache.hadoop.fs.Options$HandleOpt:changed(boolean)" : "* @param allow If true, resolve references to this entity even if it has\n     *             been modified.\n     * @return Handle option encoding parameter.",
  "org.apache.hadoop.fs.DelegationTokenRenewer:getRenewQueueLength()" : "* For testing purposes.\n   *\n   * @return renew queue length.",
  "org.apache.hadoop.fs.shell.Concat:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:monitorActiveStatus()" : null,
  "org.apache.hadoop.service.AbstractService:getStartTime()" : null,
  "org.apache.hadoop.security.alias.CredentialShell$CheckCommand:validate()" : null,
  "org.apache.hadoop.service.ServiceStateModel:toString()" : "* return the state text as the toString() value\n   * @return the current state's description",
  "org.apache.hadoop.fs.shell.PathData:getStringForChildPath(org.apache.hadoop.fs.Path)" : "* Given a child of this directory, use the directory's path and the child's\n   * basename to construct the string to the child.  This preserves relative\n   * paths since Path will fully qualify.\n   * @param childPath a path contained within this directory\n   * @return String of the path relative to this directory",
  "org.apache.hadoop.fs.shell.find.FindOptions:getErr()" : "* Returns the error stream to be used.\n   *\n   * @return error stream to be used",
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getInputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : "* We have all the data blocks and parity blocks as input blocks for\n   * recovering by default. It's codec specific\n   * @param blockGroup blockGroup.\n   * @return input blocks",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:incrClientBackoffDisconnected()" : "* Client was disconnected due to backoff",
  "org.apache.hadoop.ha.SshFenceByTcpPort:getKeyFiles()" : null,
  "org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClassByName(java.lang.String)" : "* Find the relevant compression codec for the codec's canonical class name\n   * or by codec alias and returns its implemetation class.\n   * <p>\n   * Codec aliases are case insensitive.\n   * <p>\n   * The code alias is the short class name (without the package name).\n   * If the short class name ends with 'Codec', then there are two aliases for\n   * the codec, the complete short class name and the short class name without\n   * the 'Codec' ending. For example for the 'GzipCodec' codec class name the\n   * alias are 'gzip' and 'gzipcodec'.\n   *\n   * @param codecName the canonical class name of the codec\n   * @return the codec class",
  "org.apache.hadoop.fs.impl.WeakReferenceThreadMap:getForCurrentThread()" : "* Get the value for the current thread, creating if needed.\n   * @return an instance.",
  "org.apache.hadoop.fs.shell.Command:setName(java.lang.String)" : "* Define the name of the command.\n   * @param name as invoked",
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader:windowBits()" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:backupToOld(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:getCurrentKey(java.lang.String)" : null,
  "org.apache.hadoop.security.SaslRpcClient:dispose()" : "* Release resources used by wrapped saslClient.\n   * @throws SaslException if authentication or generating response fails,\n   *                       or SASL protocol mixup",
  "org.apache.hadoop.util.Progress:addPhase(java.lang.String)" : "* Adds a named node to the tree.\n   * @param status status.\n   * @return Progress.",
  "org.apache.hadoop.fs.statistics.MeanStatistic:clear()" : "* Set the values to 0.",
  "org.apache.hadoop.ha.HealthMonitor:enterState(org.apache.hadoop.ha.HealthMonitor$State)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:mkdirs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)" : "Get a path from the local FS. Pass size as \n   *  SIZE_UNKNOWN if not known apriori. We\n   *  round-robin over the set of disks (via the configured dirs) and return\n   *  the first complete path which has enough space \n   *  @param pathStr the requested path (this will be created on the first \n   *  available disk)\n   *  @param size the size of the file that is going to be written\n   *  @param conf the Configuration object\n   *  @param checkWrite ensure that the path is writable\n   *  @return the complete path to the file on a local disk\n   *  @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stop()" : null,
  "org.apache.hadoop.util.HostsFileReader:setExcludesFile(java.lang.String)" : null,
  "org.apache.hadoop.io.WritableUtils:isNegativeVInt(byte)" : "* Given the first byte of a vint/vlong, determine the sign\n   * @param value the first byte\n   * @return is the value negative",
  "org.apache.hadoop.io.Text:validateUTF8(byte[])" : "* Check if a byte array contains valid UTF-8.\n   *\n   * @param utf8 byte array\n   * @throws MalformedInputException if the byte array contains invalid UTF-8",
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:findChecksumOffset(long,int)" : null,
  "org.apache.hadoop.fs.http.HttpsFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.impl.FlagSet:createFlagSet(java.lang.Class,java.lang.String,java.util.EnumSet)" : "* Create a FlagSet.\n   * @param enumClass class of enum\n   * @param prefix prefix (with trailing \".\") for path capabilities probe\n   * @param flags flags\n   * @param <E> enum type\n   * @return a mutable FlagSet",
  "org.apache.hadoop.security.authorize.ProxyServers:refresh()" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])" : null,
  "org.apache.hadoop.util.ApplicationClassLoader:<init>(java.lang.String,java.lang.ClassLoader,java.util.List)" : null,
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getChecksumOpt()" : null,
  "org.apache.hadoop.fs.statistics.DurationStatisticSummary:toString()" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.fs.shell.Display:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:getAttrName(java.lang.String)" : null,
  "org.apache.hadoop.conf.Configuration:getFloat(java.lang.String,float)" : "* Get the value of the <code>name</code> property as a <code>float</code>.  \n   * If no such property exists, the provided default value is returned,\n   * or if the specified value is not a valid <code>float</code>,\n   * then an error is thrown.\n   *\n   * @param name property name.\n   * @param defaultValue default value.\n   * @throws NumberFormatException when the value is invalid\n   * @return property value as a <code>float</code>, \n   *         or <code>defaultValue</code>.",
  "org.apache.hadoop.security.alias.CredentialShell:init(java.lang.String[])" : "* Parse the command line arguments and initialize the data.\n   * <pre>\n   * % hadoop credential create alias [-provider providerPath]\n   * % hadoop credential list [-provider providerPath]\n   * % hadoop credential check alias [-provider providerPath]\n   * % hadoop credential delete alias [-provider providerPath] [-f]\n   * </pre>\n   * @param args args.\n   * @return 0 if the argument(s) were recognized, 1 otherwise\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getCurrentKeyId()" : "* For subclasses externalizing the storage, for example Zookeeper\n   * based implementations.\n   *\n   * @return currentId.",
  "org.apache.hadoop.fs.HarFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.wrappedio.WrappedIO:byteBufferPositionedReadable_readFully(java.io.InputStream,long,java.nio.ByteBuffer)" : "* Delegate to {@link ByteBufferPositionedReadable#read(long, ByteBuffer)}.\n   * @param in input stream\n   * @param position position within file\n   * @param buf the ByteBuffer to receive the results of the read operation.\n   * Note: that is the default behaviour of {@link FSDataInputStream#readFully(long, ByteBuffer)}.",
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:equals(java.lang.Object)" : null,
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newObjectName(java.lang.String)" : null,
  "org.apache.hadoop.ipc.Server$Responder:doAsyncWrite(java.nio.channels.SelectionKey)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getRecordNumByLocation(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:release()" : null,
  "org.apache.hadoop.io.FloatWritable$Comparator:compare(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.jmx.JMXJsonServlet:writeObject(com.fasterxml.jackson.core.JsonGenerator,java.lang.Object,java.lang.String)" : null,
  "org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:getCodecName()" : null,
  "org.apache.hadoop.util.StringUtils:hexStringToByte(java.lang.String)" : "* Given a hexstring this will return the byte array corresponding to the\n   * string\n   * @param hex the hex String array\n   * @return a byte array that is a hex string representation of the given\n   *         string. The size of the byte array is therefore hex.length/2",
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getHostname()" : null,
  "org.apache.hadoop.fs.ChecksumFs:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)" : "* Report a checksum error to the file system.\n   * @param f the file name containing the error\n   * @param in the stream open on the file\n   * @param inPos the position of the beginning of the bad data in the file\n   * @param sums the stream open on the checksum file\n   * @param sumsPos the position of the beginning of the bad data in the\n   *         checksum file\n   * @return if retry is necessary",
  "org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult:<init>(org.apache.hadoop.fs.viewfs.InodeTree$ResultKind,java.lang.Object,java.lang.String,org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:cacheGroupsAdd(java.util.List)" : null,
  "org.apache.hadoop.fs.shell.PathData:removeAuthority(java.net.URI)" : null,
  "org.apache.hadoop.fs.BulkDeleteUtils:validatePathIsUnderParent(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Check if a given path is the base path or under the base path.\n   * @param p path to check.\n   * @param basePath base path.\n   * @return true if the given path is the base path or under the base path.",
  "org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:<init>(java.lang.Class,java.lang.Object[])" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.conf.StorageUnit$1:toTBs(double)" : null,
  "org.apache.hadoop.fs.FsStatus:getUsed()" : "* Return the number of bytes used on the file system.\n   * @return used.",
  "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:<init>()" : null,
  "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:withAttributes(java.util.Map)" : "* Add all attributes to the current map.\n     * @param value new value\n     * @return the builder",
  "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat:add(double)" : null,
  "org.apache.hadoop.crypto.OpensslCipher:update(java.nio.ByteBuffer,java.nio.ByteBuffer)" : "* Continues a multiple-part encryption or decryption operation. The data\n   * is encrypted or decrypted, depending on how this cipher was initialized.\n   * <p>\n   * \n   * All <code>input.remaining()</code> bytes starting at \n   * <code>input.position()</code> are processed. The result is stored in\n   * the output buffer.\n   * <p>\n   * \n   * Upon return, the input buffer's position will be equal to its limit;\n   * its limit will not have changed. The output buffer's position will have\n   * advanced by n, when n is the value returned by this method; the output\n   * buffer's limit will not have changed.\n   * <p>\n   * \n   * If <code>output.remaining()</code> bytes are insufficient to hold the\n   * result, a <code>ShortBufferException</code> is thrown.\n   * \n   * @param input the input ByteBuffer\n   * @param output the output ByteBuffer\n   * @return int number of bytes stored in <code>output</code>\n   * @throws ShortBufferException if there is insufficient space in the\n   * output buffer",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)" : "* Returns an authenticated {@link HttpURLConnection}, it uses a Delegation\n   * Token only if the given auth token is an instance of {@link Token} and\n   * it contains a Delegation Token, otherwise use the configured\n   * {@link DelegationTokenAuthenticator} to authenticate the connection.\n   *\n   * @param url the URL to connect to. Only HTTP/S URLs are supported.\n   * @param token the authentication token being used for the user.\n   * @return an authenticated {@link HttpURLConnection}.\n   * @throws IOException if an IO error occurred.\n   * @throws AuthenticationException if an authentication exception occurred.",
  "org.apache.hadoop.fs.store.DataBlocks$DataBlock:flush()" : "* Flush the output.\n     * Only valid in the state {@code Writing}.\n     * In the base class, this is a no-op\n     *\n     * @throws IOException any IO problem.",
  "org.apache.hadoop.ipc.Server:getProtocol()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:<init>(org.apache.hadoop.security.authentication.client.Authenticator)" : null,
  "org.apache.hadoop.fs.shell.find.Find:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parsePositiveInt(java.lang.String[],int,java.lang.String)" : "* Parse the i-th element as an integer.\n     * @return -1 if the parsing fails or the parsed value <= 0;\n     *   otherwise, return the parsed value.",
  "org.apache.hadoop.metrics2.lib.MutableRates:add(java.lang.String,long)" : "* Add a rate sample for a rate metric\n   * @param name of the rate metric\n   * @param elapsed time",
  "org.apache.hadoop.security.token.DtFileOperations:<init>()" : "No public constructor as per checkstyle.",
  "org.apache.hadoop.util.bloom.BloomFilter:add(org.apache.hadoop.util.bloom.Key)" : null,
  "org.apache.hadoop.util.functional.CommonCallableSupplier:<init>(java.util.concurrent.Callable)" : "* Create.\n   * @param call call to invoke.",
  "org.apache.hadoop.io.compress.CompressionCodecFactory:toString()" : "* Print the extension map out as a string.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementMaximum(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.FileContext:getUMask()" : "* \n   * @return the umask of this FileContext",
  "org.apache.hadoop.fs.statistics.MeanStatistic:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:makeAbsolute(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Resolve against given working directory. *\n   * \n   * @param workDir\n   * @param path\n   * @return",
  "org.apache.hadoop.fs.TrashPolicyDefault:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)" : null,
  "org.apache.hadoop.util.VersionInfo:getCompilePlatform()" : "* Returns the OS platform used for the build.\n   * @return the OS platform",
  "org.apache.hadoop.io.file.tfile.Utils$Version:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.audit.CommonAuditContext:createInstance()" : "* Demand invoked to create the instance for this thread.\n   * @return an instance.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:newSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink,org.apache.hadoop.metrics2.impl.MetricsConfig)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:getKind()" : "* Return the delegation token kind\n   * @return returns the delegation token kind",
  "org.apache.hadoop.io.compress.lz4.Lz4Compressor:reinit(org.apache.hadoop.conf.Configuration)" : "* Prepare the compressor to be used in a new stream with settings defined in\n   * the given Configuration\n   *\n   * @param conf Configuration from which new setting are fetched",
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:setWriteLock(java.util.concurrent.locks.Lock)" : null,
  "org.apache.hadoop.metrics2.lib.MutableCounter:info()" : null,
  "org.apache.hadoop.fs.DU:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder)" : null,
  "org.apache.hadoop.fs.permission.AclUtil:<init>()" : "* There is no reason to instantiate this class.",
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:<init>(java.lang.String,java.lang.String,java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_readFullyAvailable(java.io.InputStream)" : "* Probe to see if the input stream is an instance of ByteBufferPositionedReadable.\n   * If the stream is an FSDataInputStream, the wrapped stream is checked.\n   * @param in input stream\n   * @return true if the API is available, the stream implements the interface\n   * (including the innermost wrapped stream) and that it declares the stream capability.\n   * @throws IOException if the operation was attempted and failed.",
  "org.apache.hadoop.http.HttpServer2$QuotingInputFilter:init(javax.servlet.FilterConfig)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree:tryResolveInRegexMountpoint(java.lang.String,boolean)" : "* Walk through all regex mount points to see\n   * whether the path match any regex expressions.\n   *  E.g. link: ^/user/(?&lt;username&gt;\\\\w+) =&gt; s3://$user.apache.com/_${user}\n   *  srcPath: is /user/hadoop/dir1\n   *  resolveLastComponent: true\n   *  then return value is s3://hadoop.apache.com/_hadoop\n   *\n   * @param srcPath srcPath.\n   * @param resolveLastComponent resolveLastComponent.\n   * @return ResolveResult.",
  "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:emitToGangliaHosts()" : "* Sends Ganglia Metrics to the configured hosts\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ha.HAAdmin:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.Groups:isNegativeCacheEnabled()" : null,
  "org.apache.hadoop.log.LogThrottlingHelper:<init>(long)" : "* Create a log helper without any primary recorder.\n   *\n   * @see #LogThrottlingHelper(long, String)\n   * @param minLogPeriodMs input minLogPeriodMs.",
  "org.apache.hadoop.util.CloseableReferenceCount:setClosed()" : "* Mark the status as closed.\n   *\n   * Once the status is closed, it cannot be reopened.\n   *\n   * @return                         The current reference count.\n   * @throws ClosedChannelException  If someone else closes the object\n   *                                 before we do.",
  "org.apache.hadoop.http.ProfileServlet:getMinWidth(javax.servlet.http.HttpServletRequest)" : null,
  "org.apache.hadoop.io.UTF8:getBytes(java.lang.String)" : "* @return Convert a string to a UTF-8 encoded byte array.\n   * @see String#getBytes(String)\n   * @param string input string.",
  "org.apache.hadoop.security.KDiag:fail(java.lang.String,java.lang.String,java.lang.Object[])" : "* Format and raise a failure.\n   *\n   * @param category category for exception\n   * @param message string formatting message\n   * @param args any arguments for the formatting\n   * @throws KerberosDiagsFailure containing the formatted text",
  "org.apache.hadoop.util.RunJar:main(java.lang.String[])" : "Run a Hadoop job jar.  If the main class is not in the jar's manifest,\n   * then it must be provided on the command line.\n   *\n   * @param args args.\n   * @throws Throwable error.",
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:getOutputBlocks()" : null,
  "org.apache.hadoop.service.launcher.ServiceShutdownHook:run()" : "* Shutdown handler.\n   * Query the service hook reference -if it is still valid the \n   * {@link Service#stop()} operation is invoked.",
  "org.apache.hadoop.util.Progress:getParent()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:setVerifyChecksum(boolean)" : null,
  "org.apache.hadoop.io.DataOutputBuffer:writeTo(java.io.OutputStream)" : "* Write to a file stream.\n   * @param out OutputStream.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:logout()" : null,
  "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:runResolveCommand(java.util.List,java.lang.String)" : "* Build and execute the resolution command. The command is\n     * executed in the directory specified by the system property\n     * \"user.dir\" if set; otherwise the current working directory is used.\n     * @param args a list of arguments\n     * @param commandScriptName input commandScriptName.\n     * @return null if the number of arguments is out of range,\n     * or the output of the command.",
  "org.apache.hadoop.io.SequenceFile$Reader:readBuffer(org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.compress.CompressionInputStream)" : "Read a compressed buffer",
  "org.apache.hadoop.security.NetgroupCache:getNetgroups(java.lang.String,java.util.List)" : "* Get netgroups for a given user\n   *\n   * @param user get groups for this user\n   * @param groups put groups into this List",
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:finished()" : null,
  "org.apache.hadoop.net.SocketOutputStream:<init>(java.net.Socket,long)" : "* Same as SocketOutputStream(socket.getChannel(), timeout):<br><br>\n   * \n   * Create a new ouput stream with the given timeout. If the timeout\n   * is zero, it will be treated as infinite timeout. The socket's\n   * channel will be configured to be non-blocking.\n   * \n   * @see SocketOutputStream#SocketOutputStream(WritableByteChannel, long)\n   *  \n   * @param socket should have a channel associated with it.\n   * @param timeout timeout timeout in milliseconds. must not be negative.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:getInputBlocks()" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSource:getIOStatistics()" : "* Return a statistics instance.\n   * <p>\n   * It is not a requirement that the same instance is returned every time.\n   * {@link IOStatisticsSource}.\n   * <p>\n   * If the object implementing this is Closeable, this method\n   * may return null if invoked on a closed object, even if\n   * it returns a valid instance when called earlier.\n   * @return an IOStatistics instance or null",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)" : null,
  "org.apache.hadoop.fs.shell.CommandFactory:registerCommands(java.lang.Class)" : "* Invokes \"static void registerCommands(CommandFactory)\" on the given class.\n   * This method abstracts the contract between the factory and the command\n   * class.  Do not assume that directly invoking registerCommands on the\n   * given class will have the same effect.\n   * @param registrarClass class to allow an opportunity to register",
  "org.apache.hadoop.util.Preconditions:checkArgument(boolean,java.lang.Object)" : "* Ensures the truth of an expression involving one or more parameters to the calling method.\n   *\n   * @param expression a boolean expression\n   * @param errorMessage the exception message to use if the check fails; will be converted to a\n   *     string using {@link String#valueOf(Object)}\n   * @throws IllegalArgumentException if {@code expression} is false",
  "org.apache.hadoop.fs.FileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)" : "* Get all of the xattrs name/value pairs for a file or directory.\n   * Only those xattrs which the logged-in user has permissions to view\n   * are returned.\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to get extended attributes\n   * @param names XAttr names.\n   * @return Map describing the XAttrs of the file or directory\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.fs.FileUtil:list(java.io.File)" : "* A wrapper for {@link File#list()}. This java.io API returns null\n   * when a dir is not a directory or for any I/O error. Instead of having\n   * null check everywhere File#list() is used, we will add utility API\n   * to get around this problem. For the majority of cases where we prefer\n   * an IOException to be thrown.\n   * @param dir directory for which listing should be performed\n   * @return list of file names or empty string list\n   * @exception AccessDeniedException for unreadable directory\n   * @exception IOException for invalid directory or for bad disk",
  "org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)" : "* Construct a client-side proxy object with a ConnectionId.\n   *\n   * @param <T> Generics Type T.\n   * @param protocol input protocol.\n   * @param clientVersion input clientVersion.\n   * @param connId input ConnectionId.\n   * @param conf input Configuration.\n   * @param factory input factory.\n   * @param alignmentContext Alignment context\n   * @throws IOException raised on errors performing I/O.\n   * @return ProtocolProxy.",
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:getRemain()" : "* How many bytes remain in the current chunk?\n     * \n     * @return remaining bytes left in the current chunk.\n     * @throws java.io.IOException",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)" : "* For subclasses externalizing the storage, for example Zookeeper\n   * based implementations.\n   *\n   * @param key DelegationKey.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.shell.Command:run(org.apache.hadoop.fs.shell.PathData)" : "* Execute the command on the input path data. Commands can override to make\n   * use of the resolved filesystem.\n   * @param pathData The input path with resolved filesystem\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.source.JvmMetrics:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)" : null,
  "org.apache.hadoop.http.HttpServer2:hasAdministratorAccess(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : "* Does the user sending the HttpServletRequest has the administrator ACLs? If\n   * it isn't the case, response will be modified to send an error to the user.\n   *\n   * @param servletContext servletContext.\n   * @param request request.\n   * @param response used to send the error response if user does not have admin access.\n   * @return true if admin-authorized, false otherwise\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ha.NodeFencer$FenceMethodWithArg:toString()" : null,
  "org.apache.hadoop.metrics2.lib.MutableRollingAverages:rollOverAvgs()" : "* Iterates over snapshot to capture all Avg metrics into rolling structure\n   * {@link MutableRollingAverages#averages}.",
  "org.apache.hadoop.ha.HAAdmin:help(java.lang.String[])" : null,
  "org.apache.hadoop.util.SysInfoLinux:getNumCores()" : "{@inheritDoc}",
  "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:markBuffers(java.nio.ByteBuffer[])" : null,
  "org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:close()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getLastAccessTime(java.io.File)" : null,
  "org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.metrics2.lib.MutableQuantiles:getEstimator()" : "* Get the quantile estimator.\n   *\n   * @return the quantile estimator",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:get(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.ReadaheadPool:resetInstance()" : null,
  "org.apache.hadoop.ipc.Client:getAsyncRpcResponse()" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:getThreadStatistics()" : "* Get or create the thread-local data associated with the current thread.\n     * @return statistics data.",
  "org.apache.hadoop.security.KDiag:arg(java.lang.String,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.util.PriorityQueue:insert(java.lang.Object)" : "* Adds element to the PriorityQueue in log(size) time if either\n   * the PriorityQueue is not full, or not lessThan(element, top()).\n   * @param element element.\n   * @return true if element is added, false otherwise.",
  "org.apache.hadoop.conf.Configuration:getDouble(java.lang.String,double)" : "* Get the value of the <code>name</code> property as a <code>double</code>.  \n   * If no such property exists, the provided default value is returned,\n   * or if the specified value is not a valid <code>double</code>,\n   * then an error is thrown.\n   *\n   * @param name property name.\n   * @param defaultValue default value.\n   * @throws NumberFormatException when the value is invalid\n   * @return property value as a <code>double</code>, \n   *         or <code>defaultValue</code>.",
  "org.apache.hadoop.crypto.CryptoInputStream:updateDecryptor(org.apache.hadoop.crypto.Decryptor,long,byte[])" : "Calculate the counter and iv, update the decryptor.",
  "org.apache.hadoop.fs.QuotaUsage:getFileAndDirectoryCount()" : "* Return the directory count.\n   *\n   * @return file and directory count.",
  "org.apache.hadoop.security.CompositeGroupsMapping:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:init(java.util.Properties)" : null,
  "org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:createEncoder()" : null,
  "org.apache.hadoop.io.compress.BlockCompressorStream:write(byte[],int,int)" : "* Write the data provided to the compression codec, compressing no more\n   * than the buffer size less the compression overhead as specified during\n   * construction for each block.\n   *\n   * Each block contains the uncompressed length for the block, followed by\n   * one or more length-prefixed blocks of compressed data.",
  "org.apache.hadoop.conf.StorageUnit$4:toBytes(double)" : null,
  "org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean,java.util.List)" : "* Return the string representation of the object in the output format.\n   * For description of the options,\n   * @see #toString(boolean, boolean, boolean, boolean, List)\n   *\n   * @param qOption a flag indicating if quota needs to be printed or not\n   * @param hOption a flag indicating if human readable output if to be used\n   * @param tOption a flag indicating if display quota by storage types\n   * @param types Storage types to display\n   * @return the string representation of the object",
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getPassword()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:hbCreateDecodeTables(int[],int[],int[],char[],int,int,int)" : "* Called by createHuffmanDecodingTables() exclusively.",
  "org.apache.hadoop.net.SocketOutputStream:write(byte[],int,int)" : null,
  "org.apache.hadoop.net.NodeBase:normalize(java.lang.String)" : "Normalize a path by stripping off any trailing {@link #PATH_SEPARATOR}\n   * @param path path to normalize.\n   * @return the normalised path\n   * If <i>path</i>is null or empty {@link #ROOT} is returned\n   * @throws IllegalArgumentException if the first character of a non empty path\n   * is not {@link #PATH_SEPARATOR}",
  "org.apache.hadoop.security.ssl.SSLFactory:destroy()" : "* Releases any resources being used.",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)" : null,
  "org.apache.hadoop.ipc.metrics.RpcMetrics:incrClientBackoff()" : null,
  "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:interceptRemainingPath(org.apache.hadoop.fs.Path)" : "* The interceptRemainingPath will just return the remainingPath passed in.\n   *",
  "org.apache.hadoop.io.UTF8:readString(java.io.DataInput)" : "* @return Read a UTF-8 encoded string.\n   *\n   * @see DataInput#readUTF()\n   * @param in DataInput.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.FileSystem:msync()" : "* Synchronize client metadata state.\n   * <p>\n   * In some FileSystem implementations such as HDFS metadata\n   * synchronization is essential to guarantee consistency of read requests\n   * particularly in HA setting.\n   * @throws IOException If an I/O error occurred.\n   * @throws UnsupportedOperationException if the operation is unsupported.",
  "org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getAppConfigurationEntry(java.lang.String)" : null,
  "org.apache.hadoop.conf.ReconfigurationServlet:printConf(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable)" : "* Print configuration options that can be changed.",
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getLen()" : null,
  "org.apache.hadoop.io.Text:<init>(byte[])" : "* Construct from a byte array.\n   *\n   * @param utf8 input utf8.",
  "org.apache.hadoop.metrics2.lib.MutableInverseQuantiles$InversePercentile:<init>(double)" : null,
  "org.apache.hadoop.fs.sftp.SFTPInputStream:getPos()" : null,
  "org.apache.hadoop.fs.FilterFs:getStatistics()" : null,
  "org.apache.hadoop.fs.shell.FsCommand:run(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.sftp.SFTPConnectionPool:getIdleCount()" : null,
  "org.apache.hadoop.fs.DelegationTokenRenewer:removeRenewAction(org.apache.hadoop.fs.FileSystem)" : "* Remove the associated renew action from the queue\n   *\n   * @param <T> generic type T.\n   * @param fs file system.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.statistics.IOStatisticsSupport:snapshotIOStatistics(org.apache.hadoop.fs.statistics.IOStatistics)" : "* Take a snapshot of the current statistics state.\n   * <p>\n   * This is not an atomic option.\n   * <p>\n   * The instance can be serialized, and its\n   * {@code toString()} method lists all the values.\n   * @param statistics statistics\n   * @return a snapshot of the current values.",
  "org.apache.hadoop.fs.FilterFs:listXAttrs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)" : "* Construct an RPC server.\n     * \n     * @param protocolClass the class of protocol\n     * @param protocolImpl the protocolImpl whose methods will be called\n     * @param conf the configuration to use\n     * @param bindAddress the address to bind on to listen for connection\n     * @param port the port to listen for connections on\n     * @param numHandlers the number of method handler threads to run\n     * @param verbose whether each call should be logged\n     * @param portRangeConfig A config parameter that can be used to restrict\n     * the range of ports used when port is 0 (an ephemeral port)\n     * @param alignmentContext provides server state info on client responses\n     * @param secretManager input secretManager.\n     * @param queueSizePerHandler input queueSizePerHandler.\n     * @param numReaders input numReaders.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.shell.Command:processArgument(org.apache.hadoop.fs.shell.PathData)" : "* Processes a {@link PathData} item, calling\n   * {@link #processPathArgument(PathData)} or\n   * {@link #processNonexistentPath(PathData)} on each item.\n   * @param item {@link PathData} item to process\n   * @throws IOException if anything goes wrong...",
  "org.apache.hadoop.metrics2.util.SampleStat:add(long,double)" : "* Add some sample and a partial sum to the running stat.\n   * Note, min/max is not evaluated using this method.\n   * @param nSamples  number of samples\n   * @param xTotal the partial sum\n   * @return  self",
  "org.apache.hadoop.util.SysInfoWindows:getCpuFrequency()" : "{@inheritDoc}",
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)" : null,
  "org.apache.hadoop.io.DefaultStringifier:toString(java.lang.Object)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:<init>(java.io.File,long,org.apache.hadoop.fs.FileSystem)" : null,
  "org.apache.hadoop.util.StringUtils:byteToHexString(byte[])" : "* Same as byteToHexString(bytes, 0, bytes.length).\n   * @param bytes bytes.\n   * @return byteToHexString.",
  "org.apache.hadoop.fs.Path:isRoot()" : "* Returns true if and only if this path represents the root of a file system.\n   *\n   * @return true if and only if this path represents the root of a file system",
  "org.apache.hadoop.metrics2.lib.MutableCounterLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)" : null,
  "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:<init>(int,java.lang.String,org.apache.hadoop.crypto.CipherSuite,java.lang.String)" : null,
  "org.apache.hadoop.ipc.FairCallQueue:peek()" : "* Peek, like poll, provides no strict consistency.",
  "org.apache.hadoop.fs.permission.PermissionParser:combineModeSegments(char,int,int,boolean)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSEncryptedKeyVersion:<init>(java.lang.String,java.lang.String,byte[],java.lang.String,byte[])" : null,
  "org.apache.hadoop.conf.Configuration:getPasswordFromCredentialProviders(java.lang.String)" : "* Try and resolve the provided element name as a credential provider\n   * alias.\n   * @param name alias of the provisioned credential\n   * @return password or null if not found\n   * @throws IOException when error in fetching password",
  "org.apache.hadoop.metrics2.lib.MethodMetric:isDouble(java.lang.Class)" : null,
  "org.apache.hadoop.fs.GlobalStorageStatistics:get(java.lang.String)" : "* Get the StorageStatistics object with the given name.\n   *\n   * @param name        The storage statistics object name.\n   * @return            The StorageStatistics object with the given name, or\n   *                      null if there is none.",
  "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:blockSort()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:<init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:<init>(org.apache.hadoop.fs.FileSystem$Statistics)" : "* Copy constructor.\n     *\n     * @param other    The input Statistics object which is cloned.",
  "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:description()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:<init>()" : null,
  "org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String,java.lang.String)" : "* Set the <code>value</code> of the <code>name</code> property. If \n   * <code>name</code> is deprecated, it also sets the <code>value</code> to\n   * the keys that replace the deprecated key. Name will be trimmed before put\n   * into configuration.\n   *\n   * @param name property name.\n   * @param value property value.\n   * @param source the place that this configuration value came from \n   * (For debugging).\n   * @throws IllegalArgumentException when the value or name is null.",
  "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:iterator()" : null,
  "org.apache.hadoop.metrics2.lib.MutableQuantiles:add(long)" : null,
  "org.apache.hadoop.ipc.Server:getServerName()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:resolveLink(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.lang.String,java.util.Collection)" : null,
  "org.apache.hadoop.ipc.Server$Connection:sendResponse(org.apache.hadoop.ipc.Server$RpcCall)" : null,
  "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long,boolean)" : null,
  "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolServerSideTranslatorPB:getGroupsForUser(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.tools.proto.GetUserMappingsProtocolProtos$GetGroupsForUserRequestProto)" : null,
  "org.apache.hadoop.fs.FileSystem$Cache:closeAll(org.apache.hadoop.security.UserGroupInformation)" : null,
  "org.apache.hadoop.fs.shell.FsUsage$Du:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder)" : null,
  "org.apache.hadoop.util.JvmPauseMonitor:serviceStop()" : null,
  "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$Writer$Option[])" : null,
  "org.apache.hadoop.fs.FSDataInputStream:minSeekForVectorReads()" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:finished()" : null,
  "org.apache.hadoop.ipc.ExternalCall:<init>(java.security.PrivilegedExceptionAction)" : null,
  "org.apache.hadoop.util.InstrumentedLock:logWaitWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)" : null,
  "org.apache.hadoop.fs.FsShell$Usage:processRawArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.conf.ConfigurationWithLogging:set(java.lang.String,java.lang.String,java.lang.String)" : "* See {@link Configuration#set(String, String, String)}.",
  "org.apache.hadoop.conf.ReconfigurationServlet:getReconfigurable(javax.servlet.http.HttpServletRequest)" : null,
  "org.apache.hadoop.util.IntrusiveCollection:removeAll(java.util.Collection)" : null,
  "org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm:getDecompressor()" : null,
  "org.apache.hadoop.security.User:<init>(java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider:getConf()" : "* Return the provider configuration.\n   * \n   * @return the provider configuration",
  "org.apache.hadoop.security.ProviderUtils:noPasswordError(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.crypto.OpensslCipher:finalize()" : null,
  "org.apache.hadoop.ipc.Client$ConnectionId:getMaxIdleTime()" : null,
  "org.apache.hadoop.ipc.metrics.RpcMetrics:getClientBackoffDisconnected()" : "* Returns the number of disconnected backoffs.\n   * @return long",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies:hasWrappedAccessControlException(java.lang.Exception)" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:launchServiceAndExit(java.util.List)" : "* Launch the service and exit.\n   *\n   * <ol>\n   * <li>Parse the command line.</li> \n   * <li>Build the service configuration from it.</li>\n   * <li>Start the service.</li>\n   * <li>If it is a {@link LaunchableService}: execute it</li>\n   * <li>Otherwise: wait for it to finish.</li>\n   * <li>Exit passing the status code to the {@link #exit(int, String)}\n   * method.</li>\n   * </ol>\n   * @param args arguments to the service. {@code arg[0]} is \n   * assumed to be the service classname.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.ssl.SSLFactory:createSSLEngine()" : "* Returns a configured SSLEngine.\n   *\n   * @return the configured SSLEngine.\n   * @throws GeneralSecurityException thrown if the SSL engine could not\n   * be initialized.\n   * @throws IOException thrown if and IO error occurred while loading\n   * the server keystore.",
  "org.apache.hadoop.io.MapWritable:equals(java.lang.Object)" : null,
  "org.apache.hadoop.security.Groups$TimerToTickerAdapter:<init>(org.apache.hadoop.util.Timer)" : null,
  "org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,boolean)" : "* Set optional boolean parameter for the Builder.\n   * @param key key.\n   * @param value value.\n   * @return generic type B.\n   * @see #opt(String, String)",
  "org.apache.hadoop.fs.shell.SetReplication:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.FilterFs:getServerDefaults()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)" : null,
  "org.apache.hadoop.fs.ChecksumFs:setVerifyChecksum(boolean)" : "* Set whether to verify checksum.",
  "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:startUpload()" : null,
  "org.apache.hadoop.fs.FileSystem:fixRelativePart(org.apache.hadoop.fs.Path)" : "* See {@link FileContext#fixRelativePart}.\n   * @param p the path.\n   * @return relative part.",
  "org.apache.hadoop.fs.shell.FsUsage:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination:preserve(org.apache.hadoop.fs.shell.CommandWithDestination$FileAttribute)" : "* Add file attributes that need to be preserved. This method may be\n   * called multiple times to add attributes.\n   *\n   * @param fileAttribute - Attribute to add, one at a time",
  "org.apache.hadoop.fs.FSOutputSummer:writeChecksumChunks(byte[],int,int)" : "Generate checksums for the given data chunks and output chunks & checksums\n   * to the underlying output stream.",
  "org.apache.hadoop.fs.ftp.FTPInputStream:read()" : null,
  "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createSocket(java.lang.String,int)" : null,
  "org.apache.hadoop.io.MapWritable:keySet()" : null,
  "org.apache.hadoop.conf.Configuration:setFloat(java.lang.String,float)" : "* Set the value of the <code>name</code> property to a <code>float</code>.\n   * \n   * @param name property name.\n   * @param value property value.",
  "org.apache.hadoop.fs.viewfs.FsGetter:getNewInstance(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Gets new file system instance of given uri.\n   * @param uri uri.\n   * @param conf configuration.\n   * @throws IOException raised on errors performing I/O.\n   * @return file system.",
  "org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String,long)" : "* Constructor.\n   * If the supplied count is greater than zero, the counter\n   * of the key name is updated.\n   * @param iostats statistics to update\n   * @param key Key to use as prefix of values.\n   * @param count #of times to increment the matching counter.",
  "org.apache.hadoop.ipc.Server:getQueueClass(java.lang.String,int,org.apache.hadoop.conf.Configuration)" : "* Return class configured by property 'ipc.<port>.callqueue.impl' if it is\n   * present. If the config is not present, default config (without port) is\n   * used to derive class i.e 'ipc.callqueue.impl', and derived class is\n   * returned if class value is present and valid. If default config is also\n   * not present, default class {@link LinkedBlockingQueue} is returned.\n   *\n   * @param namespace Namespace \"ipc\".\n   * @param port Server's listener port.\n   * @param conf Configuration properties.\n   * @return Class returned based on configuration.",
  "org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:setCapacity(int)" : null,
  "org.apache.hadoop.io.compress.CompressionOutputStream:getIOStatistics()" : "* Return any IOStatistics provided by the underlying stream.\n   * @return IO stats from the inner stream.",
  "org.apache.hadoop.net.TableMapping$RawTableMapping:resolve(java.util.List)" : null,
  "org.apache.hadoop.ipc.RpcWritable$Buffer:<init>(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.util.GenericOptionsParser:parseGeneralOptions(org.apache.commons.cli.Options,java.lang.String[])" : "* Parse the user-specified options, get the generic options, and modify\n   * configuration accordingly.\n   *\n   * @param opts Options to use for parsing args.\n   * @param args User-specified arguments\n   * @return true if the parse was successful",
  "org.apache.hadoop.fs.FilterFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)" : null,
  "org.apache.hadoop.fs.FSOutputSummer:flushBuffer(boolean,boolean)" : null,
  "org.apache.hadoop.ha.HAAdmin:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.BBUploadHandle:from(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.fs.FsServerDefaults:getFileBufferSize()" : null,
  "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNumberOfElements(java.util.Collection,int,java.lang.String)" : "* Validates that the given set is not null and has an exact number of items.\n   * @param <T> the type of collection's elements.\n   * @param collection the argument reference to validate.\n   * @param numElements the expected number of elements in the collection.\n   * @param argName the name of the argument being validated.",
  "org.apache.hadoop.tools.CommandShell:printException(java.lang.Exception)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])" : null,
  "org.apache.hadoop.metrics2.util.Quantile:<init>(double,double)" : null,
  "org.apache.hadoop.security.SecurityUtil:getLocalHostName(org.apache.hadoop.conf.Configuration)" : "* Retrieve the name of the current host. Multihomed hosts may restrict the\n   * hostname lookup to a specific interface and nameserver with {@link\n   * org.apache.hadoop.fs.CommonConfigurationKeysPublic#HADOOP_SECURITY_DNS_INTERFACE_KEY}\n   * and {@link org.apache.hadoop.fs.CommonConfigurationKeysPublic#HADOOP_SECURITY_DNS_NAMESERVER_KEY}\n   *\n   * @param conf Configuration object. May be null.\n   * @return\n   * @throws UnknownHostException",
  "org.apache.hadoop.ipc.RpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)" : "* Store a processing time value for an RPC call into this scheduler.\n   *\n   * @param callName The name of the call.\n   * @param schedulable The schedulable representing the incoming call.\n   * @param details The details of processing time.",
  "org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int,long,long)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader,int)" : "* Creates a new compressor using the specified compression level.\n   * Compressed data will be generated in ZLIB format.\n   * \n   * @param level Compression level #CompressionLevel\n   * @param strategy Compression strategy #CompressionStrategy\n   * @param header Compression header #CompressionHeader\n   * @param directBufferSize Size of the direct buffer to be used.",
  "org.apache.hadoop.net.InnerNodeImpl:add(org.apache.hadoop.net.Node)" : null,
  "org.apache.hadoop.util.JsonSerialization:mapReader()" : "* @return an ObjectReader which returns simple Maps.",
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:checkOutputBuffers(byte[][])" : "* Check and ensure the buffers are of the desired length.\n   * @param buffers the buffers to check",
  "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:encrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)" : "* AES-CTR will consume all of the input data. It requires enough space in\n     * the destination buffer to encrypt entire input buffer.",
  "org.apache.hadoop.util.Shell:checkHadoopHomeInner(java.lang.String)" : "*  Validate the accessibility of the Hadoop home directory.\n   *\n   * @return A directory that is expected to be the hadoop home directory\n   * @throws FileNotFoundException if the specified\n   * path is not a reference to a valid directory.",
  "org.apache.hadoop.fs.shell.PathData:getDirectoryContentsIterator()" : "* Returns a RemoteIterator for PathData objects of the items contained in the\n   * given directory.\n   * @return remote iterator of PathData objects for its children\n   * @throws IOException if anything else goes wrong...",
  "org.apache.hadoop.fs.FileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)" : "* Unset the storage policy set for a given file or directory.\n   * @param src file or directory path.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.fs.FSDataInputStream:setReadahead(java.lang.Long)" : null,
  "org.apache.hadoop.util.functional.TaskPool$Builder:suppressExceptions(boolean)" : "* Suppress exceptions from tasks.\n     * RemoteIterator exceptions are not suppressable.\n     * @param suppress new value\n     * @return the builder.",
  "org.apache.hadoop.fs.FileStatus:getBlockSize()" : "* Get the block size of the file.\n   * @return the number of bytes",
  "org.apache.hadoop.io.OutputBuffer:getData()" : "* Returns the current contents of the buffer.\n   *  Data is only valid to {@link #getLength()}.\n   *\n   * @return the current contents of the buffer.",
  "org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:process(org.apache.zookeeper.WatchedEvent)" : null,
  "org.apache.hadoop.fs.shell.FsUsage$Du:getUsagesTable()" : null,
  "org.apache.hadoop.io.DoubleWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:isDirectory()" : null,
  "org.apache.hadoop.fs.FileStatus:compareTo(org.apache.hadoop.fs.FileStatus)" : "* Compare this FileStatus to another FileStatus based on lexicographical\n   * order of path.\n   * @param   o the FileStatus to be compared.\n   * @return  a negative integer, zero, or a positive integer as this object\n   *   is less than, equal to, or greater than the specified object.",
  "org.apache.hadoop.service.ServiceOperations$ServiceListeners:reset()" : "* Reset the listener list",
  "org.apache.hadoop.security.alias.KeyStoreProvider:createPermissions(java.lang.String)" : null,
  "org.apache.hadoop.util.HostsFileReader:refresh(java.io.InputStream,java.io.InputStream)" : null,
  "org.apache.hadoop.util.Time:getUtcTime()" : "* Get the current UTC time in milliseconds.\n   * @return the current UTC time in milliseconds.",
  "org.apache.hadoop.security.SaslRpcClient$WrappedOutputStream:write(byte[],int,int)" : null,
  "org.apache.hadoop.http.HttpServer2$Builder:makeConfigurationChangeMonitor(long,org.eclipse.jetty.util.ssl.SslContextFactory$Server)" : null,
  "org.apache.hadoop.http.HttpRequestLog:getRequestLog(java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.security.UserGroupInformation$UgiMetrics:create()" : null,
  "org.apache.hadoop.net.unix.DomainSocketWatcher:add(org.apache.hadoop.net.unix.DomainSocket,org.apache.hadoop.net.unix.DomainSocketWatcher$Handler)" : "* Add a socket.\n   *\n   * @param sock     The socket to add.  It is an error to re-add a socket that\n   *                   we are already watching.\n   * @param handler  The handler to associate with this socket.  This may be\n   *                   called any time after this function is called.",
  "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:toBuffers(org.apache.hadoop.io.erasurecode.ECChunk[])" : "* Convert an array of this chunks to an array of ByteBuffers\n   * @param chunks chunks to convertToByteArrayState into buffers\n   * @return an array of ByteBuffers",
  "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createPath(org.apache.hadoop.fs.Path,java.lang.String,boolean)" : null,
  "org.apache.hadoop.ipc.DefaultCostProvider:getCost(org.apache.hadoop.ipc.ProcessingDetails)" : "* Returns 1, regardless of the processing details.\n   *\n   * @param details Process details (ignored)\n   * @return 1",
  "org.apache.hadoop.fs.audit.CommonAuditContext:put(java.lang.String,java.lang.String)" : "* Put a context entry.\n   * @param key key\n   * @param value new value., If null, triggers removal.\n   * @return old value or null",
  "org.apache.hadoop.ha.ZKFCRpcServer:stopAndJoin()" : null,
  "org.apache.hadoop.http.HttpServer2:getPort()" : "* Get the port that the server is on\n   * @return the port",
  "org.apache.hadoop.fs.HarFileSystem:getServerDefaults()" : null,
  "org.apache.hadoop.io.file.tfile.Utils$Version:compatibleWith(org.apache.hadoop.io.file.tfile.Utils$Version)" : "* Test compatibility.\n     * \n     * @param other\n     *          The Version object to test compatibility with.\n     * @return true if both versions have the same major version number; false\n     *         otherwise.",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getID()" : "* The ID is always 0.\n   * As the real context implementation counter starts at 1,\n   * we are guaranteed to have unique IDs even between them and\n   * the empty context.\n   * @return 0",
  "org.apache.hadoop.io.ElasticByteBufferPool$Key:compareTo(org.apache.hadoop.io.ElasticByteBufferPool$Key)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.CallableRaisingIOE)" : "* Wrap an iterator with one which adds a continuation probe.\n     * The probe will be called in the {@link #hasNext()} method, before\n     * the source iterator is itself checked and in {@link #next()}\n     * before retrieval.\n     * That is: it may be called multiple times per iteration.\n     * @param source source iterator.\n     * @param continueWork predicate which will trigger a fast halt if it returns false.",
  "org.apache.hadoop.io.SequenceFile$Writer:stream(org.apache.hadoop.fs.FSDataOutputStream)" : null,
  "org.apache.hadoop.fs.LocalDirAllocator:isContextValid(java.lang.String)" : "* Method to check whether a context is valid.\n   * @param contextCfgItemName contextCfgItemName.\n   * @return true/false",
  "org.apache.hadoop.security.authorize.AccessControlList:isAllAllowed()" : null,
  "org.apache.hadoop.fs.BufferedFSInputStream:readFully(long,byte[])" : null,
  "org.apache.hadoop.ipc.FairCallQueue:putQueue(int,org.apache.hadoop.ipc.Schedulable)" : "* Put the element in a queue of a specific priority.\n   * @param priority - queue priority\n   * @param e - element to add",
  "org.apache.hadoop.conf.Configuration$IntegerRanges$RangeNumberIterator:next()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:seek(long)" : "* Set the current byte position in the input file.\n     *\n     * <p>The position passed must be a position returned by {@link\n     * SequenceFile.Writer#getLength()} when writing this file.  To seek to an arbitrary\n     * position, use {@link SequenceFile.Reader#sync(long)}. </p>\n     *\n     * @param position input position.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:containsKmsDt(org.apache.hadoop.security.UserGroupInformation)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer$KeyClassOption:<init>(java.lang.Class)" : null,
  "org.apache.hadoop.util.ComparableVersion$StringItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item)" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String)" : null,
  "org.apache.hadoop.http.HtmlQuoting:quoteHtmlChars(java.lang.String)" : "* Quote the given item to make it html-safe.\n   * @param item the string to quote\n   * @return the quoted string",
  "org.apache.hadoop.security.Credentials:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)" : "* Add a token in the storage (in memory).\n   * @param alias the alias for the key\n   * @param t the token object",
  "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:<init>(java.lang.String,long)" : null,
  "org.apache.hadoop.ipc.ProtobufWrapperLegacy:readFrom(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_fromJsonString(java.lang.String)" : "* Load IOStatisticsSnapshot from a JSON string.\n   * @param json JSON string value.\n   * @return deserialized snapshot.\n   * @throws UncheckedIOException Any IO/jackson exception.\n   * @throws UnsupportedOperationException if the IOStatistics classes were not found",
  "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:destroy()" : "* Releases any resources being used.",
  "org.apache.hadoop.ipc.RemoteException:instantiateException(java.lang.Class)" : null,
  "org.apache.hadoop.fs.FileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)" : "* This version of the mkdirs method assumes that the permission is absolute.\n   * It has been added to support the FileContext that processes the permission\n   * with umask before calling this method.\n   * This a temporary method added to support the transition from FileSystem\n   * to FileContext for user applications.\n   *\n   * @param f the path.\n   * @param absolutePermission permission.\n   * @param createParent create parent.\n   * @throws IOException IO failure.",
  "org.apache.hadoop.util.StringUtils:createStartupShutdownMessage(java.lang.String,java.lang.String,java.lang.String[])" : "* Generate the text for the startup/shutdown message of processes.\n   * @param classname short classname of the class\n   * @param hostname hostname\n   * @param args Command arguments\n   * @return a string to log.",
  "org.apache.hadoop.io.nativeio.NativeIO$Windows:access(java.lang.String,org.apache.hadoop.io.nativeio.NativeIO$Windows$AccessRight)" : "* Checks whether the current process has desired access rights on\n     * the given path.\n     * \n     * Longer term this native function can be substituted with JDK7\n     * function Files#isReadable, isWritable, isExecutable.\n     *\n     * @param path input path\n     * @param desiredAccess ACCESS_READ, ACCESS_WRITE or ACCESS_EXECUTE\n     * @return true if access is allowed\n     * @throws IOException I/O exception on error",
  "org.apache.hadoop.security.token.delegation.DelegationKey:equals(java.lang.Object)" : null,
  "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.http.HttpServer2:bindForSinglePort(org.eclipse.jetty.server.ServerConnector,int)" : "* Bind using single configured port. If findPort is true, we will try to bind\n   * after incrementing port till a free port is found.\n   * @param listener jetty listener.\n   * @param port port which is set in the listener.\n   * @throws Exception",
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:writeObject(java.io.ObjectOutputStream)" : "* Serialize by converting each map to a TreeMap, and saving that\n   * to the stream.\n   * @param s ObjectOutputStream.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.SortedMapWritable:<init>(org.apache.hadoop.io.SortedMapWritable)" : "* Copy constructor.\n   * \n   * @param other the map to copy from",
  "org.apache.hadoop.conf.Configuration:getFinalParameters()" : "* Get the set of parameters marked final.\n   *\n   * @return final parameter set.",
  "org.apache.hadoop.fs.shell.Mkdir:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.crypto.OpensslCipher:<init>(long,int,int,long)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:tryAcquire()" : "* Acquires a resource blocking if one is immediately available. Otherwise returns null.",
  "org.apache.hadoop.fs.shell.CopyCommands$Merge:writeDelimiter(org.apache.hadoop.fs.FSDataOutputStream)" : null,
  "org.apache.hadoop.net.StandardSocketFactory:<init>()" : "* Default empty constructor (for use with the reflection API).",
  "org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:getCodecName()" : null,
  "org.apache.hadoop.util.RunJar:getSystemClasses()" : null,
  "org.apache.hadoop.fs.DelegateToFileSystem:delete(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.service.launcher.ServiceShutdownHook:register(int)" : "* Register the service for shutdown with Hadoop's\n   * {@link ShutdownHookManager}.\n   * @param priority shutdown hook priority",
  "org.apache.hadoop.fs.FileSystem:makeQualified(org.apache.hadoop.fs.Path)" : "* Qualify a path to one which uses this FileSystem and, if relative,\n   * made absolute.\n   * @param path to qualify.\n   * @return this path if it contains a scheme and authority and is absolute, or\n   * a new path that includes a path and authority and is fully qualified\n   * @see Path#makeQualified(URI, Path)\n   * @throws IllegalArgumentException if the path has a schema/URI different\n   * from this FileSystem.",
  "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:getGangliaConfForMetric(java.lang.String)" : "* Lookup GangliaConf from cache. If not found, return default values\n   *\n   * @param metricName metricName.\n   * @return looked up GangliaConf",
  "org.apache.hadoop.security.authorize.AccessControlList:buildACL(java.lang.String[])" : "* Build ACL from the given array of strings.\n   * The strings contain comma separated values.\n   *\n   * @param userGroupStrings build ACL from array of Strings",
  "org.apache.hadoop.util.dynamic.DynConstructors$Ctor:newInstanceChecked(java.lang.Object[])" : null,
  "org.apache.hadoop.util.VersionInfo:main(java.lang.String[])" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addKey(org.apache.hadoop.security.token.delegation.DelegationKey)" : "* Add a previously used master key to cache (when NN restarts), \n   * should be called before activate().\n   *\n   * @param key delegation key.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValueStream()" : "* Stream access to value. The value part of the key-value pair pointed\n         * by the current cursor is not cached and can only be examined once.\n         * Calling any of the following functions more than once without moving\n         * the cursor will result in exception: {@link #getValue(byte[])},\n         * {@link #getValue(byte[], int)}, {@link #getValueStream}.\n         * \n         * @return The input stream for reading the value.\n         * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FSDataInputStream:toString()" : "* String value. Includes the string value of the inner stream\n   * @return the stream",
  "org.apache.hadoop.metrics2.lib.MutableCounterLong:incr(long)" : "* Increment the value by a delta\n   * @param delta of the increment",
  "org.apache.hadoop.fs.FileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : "* Modifies ACL entries of files and directories.  This method can add new ACL\n   * entries or modify the permissions on existing ACL entries.  All existing\n   * ACL entries that are not specified in this call are retained without\n   * changes.  (Modifications are merged into the current ACL.)\n   *\n   * @param path Path to modify\n   * @param aclSpec List&lt;AclEntry&gt; describing modifications\n   * @throws IOException if an ACL could not be modified\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:<init>(java.io.File)" : "* File constructor; input stream and byteArray will be null.\n     *\n     * @param file file to upload",
  "org.apache.hadoop.conf.Configuration:parse(java.io.InputStream,java.lang.String,boolean)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setReplication(org.apache.hadoop.fs.Path,short)" : null,
  "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:createDecryptor()" : null,
  "org.apache.hadoop.fs.FilterFs:msync()" : null,
  "org.apache.hadoop.ipc.ProtocolSignature:getMethods()" : null,
  "org.apache.hadoop.security.SaslRpcClient:getAuthMethod()" : null,
  "org.apache.hadoop.util.PriorityQueue:initialize(int)" : "* Subclass constructors must call this.\n   * @param maxSize max size.",
  "org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,boolean)" : null,
  "org.apache.hadoop.conf.StorageUnit$7:toGBs(double)" : null,
  "org.apache.hadoop.ipc.Server$RpcCall:setDeferredResponse(org.apache.hadoop.io.Writable)" : null,
  "org.apache.hadoop.util.ChunkedArrayList:iterator()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* The main factory method for creating a file system. Get a file system for\n   * the URI's scheme and authority. The scheme of the <code>uri</code>\n   * determines a configuration property name,\n   * <tt>fs.AbstractFileSystem.<i>scheme</i>.impl</tt> whose value names the\n   * AbstractFileSystem class.\n   * \n   * The entire URI and conf is passed to the AbstractFileSystem factory method.\n   * \n   * @param uri for the file system to be created.\n   * @param conf which is passed to the file system impl.\n   * \n   * @return file system for the given URI.\n   * \n   * @throws UnsupportedFileSystemException if the file system for\n   *           <code>uri</code> is not supported.",
  "org.apache.hadoop.util.Lists:newArrayList(java.lang.Object[])" : "* Creates a <i>mutable</i> {@code ArrayList} instance containing the given\n   * elements.\n   *\n   * <p>Note that even when you do need the ability to add or remove,\n   * this method provides only a tiny bit of syntactic sugar for\n   * {@code newArrayList(}\n   * {@link Arrays#asList asList}\n   * {@code (...))}, or for creating an empty list then calling\n   * {@link Collections#addAll}.\n   *\n   * @param <E> Generics Type E.\n   * @param elements elements.\n   * @return ArrayList Generics Type E.",
  "org.apache.hadoop.security.UserGroupInformation:isLoginSuccess()" : "This method checks for a successful Kerberos login\n    * and returns true by default if it is not using Kerberos.\n    *\n    * @return true on successful login",
  "org.apache.hadoop.ha.ZKFailoverController:formatZK(boolean,boolean)" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:getHostName()" : "* @return the hostName",
  "org.apache.hadoop.io.erasurecode.CodecRegistry:getCodecNames()" : "* Get all codec names.\n   * @return a set of all codec names",
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:encodeWithPiggyBacks(java.nio.ByteBuffer[],java.nio.ByteBuffer[][],int,boolean)" : null,
  "org.apache.hadoop.fs.HarFileSystem:decodeString(java.lang.String)" : null,
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor:invokeAny(java.util.Collection)" : null,
  "org.apache.hadoop.ipc.Client$Connection:markClosed(java.io.IOException)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:reinit(org.apache.hadoop.conf.Configuration)" : "* reinit the compressor with the given configuration. It will reset the\n   * compressor's compression level and compression strategy. Different from\n   * <tt>ZlibCompressor</tt>, <tt>BuiltInZlibDeflater</tt> only support three\n   * kind of compression strategy: FILTERED, HUFFMAN_ONLY and DEFAULT_STRATEGY.\n   * It will use DEFAULT_STRATEGY as default if the configured compression\n   * strategy is not supported.",
  "org.apache.hadoop.security.SecurityUtil:validateSslConfiguration(org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)" : null,
  "org.apache.hadoop.io.file.tfile.ByteArray:<init>(byte[],int,int)" : "* Wrap a partial byte array as a RawComparable.\n   * \n   * @param buffer\n   *          the byte array buffer.\n   * @param offset\n   *          the starting offset\n   * @param len\n   *          the length of the consecutive bytes to be wrapped.",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:getMetricsTimeUnit(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider:getKeysMetadata(java.lang.String[])" : "* Get key metadata in bulk.\n   * @param names the names of the keys to get\n   * @throws IOException raised on errors performing I/O.\n   * @return Metadata Array.",
  "org.apache.hadoop.metrics2.impl.MetricsBuffer:<init>(java.lang.Iterable)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer$ValueClassOption:<init>(java.lang.Class)" : null,
  "org.apache.hadoop.ipc.RPC$Builder:setNumHandlers(int)" : "* @return Default: 1.\n     * @param numHandlers input numHandlers.",
  "org.apache.hadoop.fs.FileUtil:isRegularFile(java.io.File)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:<init>()" : null,
  "org.apache.hadoop.fs.FileContext:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : "* Set permission of a path.\n   * @param f the path.\n   * @param permission - the new absolute permission (umask is not applied)\n   *\n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If <code>f</code> does not exist\n   * @throws UnsupportedFileSystemException If file system for <code>f</code>\n   *         is not supported\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:makeMaps()" : null,
  "org.apache.hadoop.security.alias.CredentialShell$PasswordReader:format(java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:equals(java.lang.Object)" : "* @see java.lang.Object#equals(java.lang.Object)",
  "org.apache.hadoop.fs.FileSystem$Statistics$11:<init>(org.apache.hadoop.fs.FileSystem$Statistics)" : "* Copy constructor.\n     *\n     * @param other    The input Statistics object which is cloned.",
  "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:remainingCapacity()" : null,
  "org.apache.hadoop.security.SaslPlainServer:unwrap(byte[],int,int)" : null,
  "org.apache.hadoop.metrics2.sink.StatsDSink:writeMetric(java.lang.String)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:getFsStatus(org.apache.hadoop.fs.Path)" : "* The specification of this method matches that of\n   * {@link FileContext#getFsStatus(Path)} except that Path f must be for this\n   * file system.\n   *\n   * @param f the path.\n   * @throws AccessControlException access control exception.\n   * @throws FileNotFoundException file not found exception.\n   * @throws UnresolvedLinkException unresolved link exception.\n   * @throws IOException raised on errors performing I/O.\n   * @return Fs Status.",
  "org.apache.hadoop.io.SequenceFile$Metadata:set(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:<init>(java.lang.String,java.lang.String)" : "* Create an instance of the launcher.\n   * @param serviceName name of service for text messages\n   * @param serviceClassName classname of the service",
  "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:getNumParityUnits()" : "* The number of parity output units for the coding. A unit can be a byte,\n   * chunk, buffer or even a block.\n   * @return count of parity output units",
  "org.apache.hadoop.fs.shell.find.BaseExpression:getOptions()" : "* Return the options to be used by this expression.\n   * @return options.",
  "org.apache.hadoop.conf.Configuration:writeXml(java.lang.String,java.io.Writer)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$LinkType,java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.file.tfile.TFileDumper:dumpInfo(java.lang.String,java.io.PrintStream,org.apache.hadoop.conf.Configuration)" : "* Dump information about TFile.\n   * \n   * @param file\n   *          Path string of the TFile\n   * @param out\n   *          PrintStream to output the information.\n   * @param conf\n   *          The configuration object.\n   * @throws IOException",
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeMultiAndParity(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int[],int)" : null,
  "org.apache.hadoop.util.DiskChecker:checkDir(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : "* Create the local directory if necessary, check permissions and also ensure\n   * it can be read from and written into.\n   *\n   * @param localFS local filesystem\n   * @param dir directory\n   * @param expected permission\n   * @throws DiskErrorException disk problem.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:resetState()" : null,
  "org.apache.hadoop.fs.FSDataInputStream:readFully(long,java.nio.ByteBuffer)" : "* Delegate to the underlying stream.\n   * @param position position within file\n   * @param buf the ByteBuffer to receive the results of the read operation.\n   * @throws IOException on a failure from the nested stream.\n   * @throws UnsupportedOperationException if the inner stream does not\n   * support this operation.",
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(org.apache.hadoop.fs.PathHandle)" : null,
  "org.apache.hadoop.ipc.UnexpectedServerException:<init>(java.lang.String,java.lang.Throwable)" : "* Constructs exception with the specified detail message and cause.\n   * \n   * @param message message.\n   * @param cause that cause this exception\n   * @param cause the cause (can be retried by the {@link #getCause()} method).\n   *          (A <tt>null</tt> value is permitted, and indicates that the cause\n   *          is nonexistent or unknown.)",
  "org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedOrigins(javax.servlet.FilterConfig)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reinit(org.apache.hadoop.conf.Configuration)" : "* Prepare the compressor to be used in a new stream with settings defined in\n   * the given Configuration. It will reset the compressor's block size and\n   * and work factor.\n   * \n   * @param conf Configuration storing new settings",
  "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[],int,int)" : null,
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getReplication()" : null,
  "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:<init>(java.lang.Object,org.apache.hadoop.metrics2.lib.MutableMetricsFactory)" : null,
  "org.apache.hadoop.net.NetUtils:getLocalInetAddress(java.lang.String)" : "* Checks if {@code host} is a local host name and return {@link InetAddress}\n   * corresponding to that address.\n   * \n   * @param host the specified host\n   * @return a valid local {@link InetAddress} or null\n   * @throws SocketException if an I/O error occurs",
  "org.apache.hadoop.security.KDiag:loginFromKeytab()" : "* Log in from a keytab, dump the UGI, validate it, then try and log in again.\n   *\n   * That second-time login catches JVM/Hadoop compatibility problems.\n   * @throws IOException Keytab loading problems",
  "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:write(int)" : null,
  "org.apache.hadoop.fs.HarFileSystem:appendFile(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry:name()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getGroup()" : null,
  "org.apache.hadoop.io.ObjectWritable:loadClass(org.apache.hadoop.conf.Configuration,java.lang.String)" : "* Find and load the class with given name <tt>className</tt> by first finding\n   * it in the specified <tt>conf</tt>. If the specified <tt>conf</tt> is null,\n   * try load it directly.\n   *\n   * @param conf configuration.\n   * @param className classname.\n   * @return Class.",
  "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:constructRpcRequestHeader(java.lang.reflect.Method)" : null,
  "org.apache.hadoop.security.SaslRpcServer:decodeIdentifier(java.lang.String)" : null,
  "org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[])" : "* Static method to execute a shell command.\n   * Covers most of the simple cases without requiring the user to implement\n   * the <code>Shell</code> interface.\n   * @param env the map of environment key=value\n   * @param cmd shell command to execute.\n   * @return the output of the executed command.\n   * @throws IOException on any problem.",
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[])" : null,
  "org.apache.hadoop.conf.Configuration:getAllPropertiesByTag(java.lang.String)" : "* Get all properties belonging to tag.\n   * @param tag tag\n   * @return Properties with matching tag",
  "org.apache.hadoop.metrics2.impl.MetricCounterLong:visit(org.apache.hadoop.metrics2.MetricsVisitor)" : null,
  "org.apache.hadoop.metrics2.lib.MutableRollingAverages$SumAndCount:<init>(double,long,long)" : "* Constructor for {@link SumAndCount}.\n     *\n     * @param sum sub-sum in sliding windows\n     * @param count sub-total in sliding windows\n     * @param snapshotTimeStamp when is a new SampleStat snapshot.",
  "org.apache.hadoop.io.UTF8$Comparator:<init>()" : null,
  "org.apache.hadoop.io.compress.CodecPool:updateLeaseCount(org.apache.hadoop.thirdparty.com.google.common.cache.LoadingCache,java.lang.Object,int)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry:<init>(int,long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>(long,byte[],java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path)" : "* Return path of the enclosing root for a given path.\n   * The enclosing root path is a common ancestor that should be used for temp and staging dirs\n   * as well as within encryption zones and other restricted directories.\n   *\n   * Call makeQualified on the param path to ensure its part of the correct filesystem.\n   *\n   * @param path file path to find the enclosing root path for\n   * @return a path to the enclosing root\n   * @throws IOException early checks like failure to resolve path cause IO failures",
  "org.apache.hadoop.net.CachedDNSToSwitchMapping:isSingleSwitch()" : "* Delegate the switch topology query to the raw mapping, via\n   * {@link AbstractDNSToSwitchMapping#isMappingSingleSwitch(DNSToSwitchMapping)}\n   * @return true iff the raw mapper is considered single-switch.",
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:setDictionary(byte[],int,int)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX:isPmdkAvailable()" : null,
  "org.apache.hadoop.ipc.Client$Connection:handleConnectionTimeout(int,int,java.io.IOException)" : null,
  "org.apache.hadoop.net.NodeBase:hashCode()" : null,
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:resetReadStats()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:<init>(org.apache.hadoop.fs.viewfs.FsGetter)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy:compressionStrategy()" : null,
  "org.apache.hadoop.util.dynamic.BindingUtils:noop(java.lang.String)" : "* Create a no-op method.\n   *\n   * @param name method name\n   *\n   * @return a no-op method.",
  "org.apache.hadoop.fs.FsShellPermissions$Chmod:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char)" : "* Escape <code>charToEscape</code> in the string \n   * with the escape char <code>escapeChar</code>\n   * \n   * @param str string\n   * @param escapeChar escape char\n   * @param charToEscape the char to be escaped\n   * @return an escaped string",
  "org.apache.hadoop.io.compress.ZStandardCodec:isNativeCodeLoaded()" : null,
  "org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:setCurrentKeyId(int)" : "* For subclasses externalizing the storage, for example Zookeeper\n   * based implementations.\n   *\n   * @param keyId keyId.",
  "org.apache.hadoop.tools.TableListing$Column:setWrapWidth(int)" : null,
  "org.apache.hadoop.conf.Configuration$DeprecationDelta:getNewKeys()" : null,
  "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsAdd(java.util.List)" : "* Add a group to cache, only netgroups are cached\n   *\n   * @param groups list of group names to add to cache",
  "org.apache.hadoop.ipc.Client$Connection:waitForWork()" : null,
  "org.apache.hadoop.io.compress.BZip2Codec:getDecompressorType()" : "* Get the type of {@link Decompressor} needed by this {@link CompressionCodec}.\n   *\n   * @return the type of decompressor needed by this codec.",
  "org.apache.hadoop.fs.permission.FsCreateModes:create(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)" : "* Create from masked and unmasked modes.\n   *\n   * @param masked masked.\n   * @param unmasked unmasked.\n   * @return FsCreateModes.",
  "org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:getIOStatistics()" : null,
  "org.apache.hadoop.util.FileBasedIPList:<init>(java.lang.String)" : null,
  "org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(java.lang.String,long)" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:reset()" : null,
  "org.apache.hadoop.fs.LocalFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileContext:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)" : "* Return the path capabilities of the bonded {@code AbstractFileSystem}.\n   * @param path path to query the capability of.\n   * @param capability string to query the stream support for.\n   * @return true iff the capability is supported under that FS.\n   * @throws IOException path resolution or other IO failure\n   * @throws IllegalArgumentException invalid arguments",
  "org.apache.hadoop.fs.InvalidRequestException:<init>(java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarStatus:getStartIndex()" : null,
  "org.apache.hadoop.fs.store.DataBlocks$DataBlock:getIndex()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordFiltered:timestamp()" : null,
  "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:setCodecOptions(org.apache.hadoop.io.erasurecode.ErasureCodecOptions)" : null,
  "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:isConnected()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:setHandlerAuthMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod)" : null,
  "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String)" : null,
  "org.apache.hadoop.io.WritableFactories:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)" : "* Create a new instance of a class with a defined factory.\n   *\n   * @param c input c.\n   * @param conf input configuration.\n   * @return a new instance of a class with a defined factory.",
  "org.apache.hadoop.fs.sftp.SFTPFileSystem$1:close()" : null,
  "org.apache.hadoop.util.ZKUtil$ZKAuthInfo:<init>(java.lang.String,byte[])" : null,
  "org.apache.hadoop.http.ProfileServlet:getInteger(javax.servlet.http.HttpServletRequest,java.lang.String,java.lang.Integer)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)" : null,
  "org.apache.hadoop.service.AbstractService:enterState(org.apache.hadoop.service.Service$STATE)" : "* Enter a state; record this via {@link #recordLifecycleEvent}\n   * and log at the info level.\n   * @param newState the proposed new state\n   * @return the original state\n   * it wasn't already in that state, and the state model permits state re-entrancy.",
  "org.apache.hadoop.crypto.JceCtrCryptoCodec:close()" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:equals(java.lang.Object)" : null,
  "org.apache.hadoop.io.erasurecode.ECChunk:<init>(java.nio.ByteBuffer)" : "* Wrapping a ByteBuffer\n   * @param buffer buffer to be wrapped by the chunk",
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,int,byte[][],int[],byte[][],int[])" : null,
  "org.apache.hadoop.util.LightWeightCache:get(java.lang.Object)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators:closingRemoteIterator(org.apache.hadoop.fs.RemoteIterator,java.io.Closeable)" : "* This adds an extra close operation alongside the passthrough\n   * to any Closeable.close() method supported by the source iterator.\n   * @param iterator source\n   * @param toClose extra object to close.\n   * @param <S> source type.\n   * @return a new iterator",
  "org.apache.hadoop.io.file.tfile.BCFile$Writer:getDefaultCompressionAlgorithm()" : null,
  "org.apache.hadoop.fs.FileSystem:listFiles(org.apache.hadoop.fs.Path,boolean)" : "* List the statuses and block locations of the files in the given path.\n   * Does not guarantee to return the iterator that traverses statuses\n   * of the files in a sorted order.\n   *\n   * <pre>\n   * If the path is a directory,\n   *   if recursive is false, returns files in the directory;\n   *   if recursive is true, return files in the subtree rooted at the path.\n   * If the path is a file, return the file's status and block locations.\n   * </pre>\n   * @param f is the path\n   * @param recursive if the subdirectories need to be traversed recursively\n   *\n   * @return an iterator that traverses statuses of the files\n   *\n   * @throws FileNotFoundException when the path does not exist;\n   * @throws IOException see specific implementation",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:updateProcessedByteCount(int)" : null,
  "org.apache.hadoop.security.SaslRpcClient:createSaslReply(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState,byte[])" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Compressor:end()" : "* Closes the compressor and discards any unprocessed input.",
  "org.apache.hadoop.fs.LocalDirAllocator:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)" : "*  We search through all the configured dirs for the file's existence\n   *  and return true when we find.\n   *  @param pathStr the requested file (this will be searched)\n   *  @param conf the Configuration object\n   *  @return true if files exist. false otherwise",
  "org.apache.hadoop.io.WritableComparator:<init>()" : null,
  "org.apache.hadoop.util.WeakReferenceMap:resolve(java.lang.ref.WeakReference)" : "* Given a possibly null weak reference, resolve\n   * its value.\n   * @param r reference to resolve\n   * @return the value or null",
  "org.apache.hadoop.util.FindClass:getClass(java.lang.String)" : "* Get a class fromt the configuration\n   * @param name the class name\n   * @return the class\n   * @throws ClassNotFoundException if the class was not found\n   * @throws Error on other classloading problems",
  "org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.conf.Configuration)" : "* Add a configuration resource.\n   *\n   * The properties of this resource will override properties of previously\n   * added resources, unless they were marked <a href=\"#Final\">final</a>.\n   *\n   * @param conf Configuration object from which to load properties",
  "org.apache.hadoop.fs.ftp.FTPFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)" : "This optional operation is not yet supported.",
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:add(org.apache.hadoop.util.bloom.Key)" : null,
  "org.apache.hadoop.fs.FileSystem:addFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)" : "* This method adds a FileSystem instance to the cache so that it can\n   * be retrieved later. It is only for testing.\n   * @param uri the uri to store it under\n   * @param conf the configuration to store it under\n   * @param fs the FileSystem to store\n   * @throws IOException if the current user cannot be determined.",
  "org.apache.hadoop.util.Sets:intersection(java.util.Set,java.util.Set)" : "* Returns the intersection of two sets as an unmodifiable set.\n   * The returned set contains all elements that are contained by both backing\n   * sets.\n   *\n   * <p>Results are undefined if {@code set1} and {@code set2} are sets based\n   * on different equivalence relations (as {@code HashSet}, {@code TreeSet},\n   * and the keySet of an {@code IdentityHashMap} all are).\n   *\n   * @param set1 set1.\n   * @param set2 set2.\n   * @param <E> Generics Type E.\n   * @return a new, empty thread-safe {@code Set}.",
  "org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(byte[],int,int)" : null,
  "org.apache.hadoop.util.functional.FutureIO:unwrapInnerException(java.lang.Throwable)" : "* From the inner cause of an execution exception, extract the inner cause\n   * to an IOException, raising RuntimeExceptions and Errors immediately.\n   * <ol>\n   *   <li> If it is an IOE: Return.</li>\n   *   <li> If it is a {@link UncheckedIOException}: return the cause</li>\n   *   <li> Completion/Execution Exceptions: extract and repeat</li>\n   *   <li> If it is an RTE or Error: throw.</li>\n   *   <li> Any other type: wrap in an IOE</li>\n   * </ol>\n   *\n   * Recursively handles wrapped Execution and Completion Exceptions in\n   * case something very complicated has happened.\n   * @param e exception.\n   * @return an IOException extracted or built from the cause.\n   * @throws RuntimeException if that is the inner cause.\n   * @throws Error if that is the inner cause.",
  "org.apache.hadoop.service.launcher.ServiceLauncher:noteException(org.apache.hadoop.util.ExitUtil$ExitException)" : "* Record that an Exit Exception has been raised.\n   * Save it to {@link #serviceException}, with its exit code in\n   * {@link #serviceExitCode}\n   * @param exitException exception",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.util.VersionInfo:getBranch()" : "* Get the branch on which this originated.\n   * @return The branch name, e.g. \"trunk\" or \"branches/branch-0.20\"",
  "org.apache.hadoop.io.erasurecode.CodecRegistry:getInstance()" : null,
  "org.apache.hadoop.io.ArrayWritable:<init>(java.lang.Class)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:getKey()" : null,
  "org.apache.hadoop.ipc.Server$Connection:initializeAuthContext(int)" : null,
  "org.apache.hadoop.util.StringUtils:formatTimeDiff(long,long)" : "* \n   * Given a finish and start time in long milliseconds, returns a \n   * String in the format Xhrs, Ymins, Z sec, for the time difference between two times. \n   * If finish time comes before start time then negative valeus of X, Y and Z wil return. \n   * \n   * @param finishTime finish time\n   * @param startTime start time\n   * @return a String in the format Xhrs, Ymins, Z sec,\n   *         for the time difference between two times.",
  "org.apache.hadoop.ipc.RpcNoSuchProtocolException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.ipc.IpcException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:cancel()" : null,
  "org.apache.hadoop.util.LimitInputStream:read(byte[],int,int)" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:seek(long)" : null,
  "org.apache.hadoop.metrics2.lib.UniqueNames:uniqueName(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.CommandFactory:addObject(org.apache.hadoop.fs.shell.Command,java.lang.String[])" : "* Register the given object as handling the given list of command\n   * names.  Avoid calling this method and use\n   * {@link #addClass(Class, String...)} whenever possible to avoid\n   * startup overhead from excessive command object instantiations.  This\n   * method is intended only for handling nested non-static classes that\n   * are re-usable.  Namely -help/-usage.\n   * @param cmdObject the object implementing the command names\n   * @param names one or more command names that will invoke this class",
  "org.apache.hadoop.fs.Options$ChecksumOpt:processChecksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt,org.apache.hadoop.fs.Options$ChecksumOpt,int)" : "* A helper method for processing user input and default value to \n     * create a combined checksum option. This is a bit complicated because\n     * bytesPerChecksum is kept for backward compatibility.\n     *\n     * @param defaultOpt Default checksum option\n     * @param userOpt User-specified checksum option. Ignored if null.\n     * @param userBytesPerChecksum User-specified bytesPerChecksum\n     *                Ignored if {@literal <} 0.\n     * @return ChecksumOpt.",
  "org.apache.hadoop.fs.HarFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* copies the file in the har filesystem to a local file.",
  "org.apache.hadoop.security.Credentials:<init>()" : "* Create an empty credentials instance.",
  "org.apache.hadoop.fs.viewfs.InodeTree:buildResolveResultForRegexMountPoint(org.apache.hadoop.fs.viewfs.InodeTree$ResultKind,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)" : "* Build resolve result.\n   * Here's an example\n   * Mountpoint: fs.viewfs.mounttable.mt\n   *     .linkRegex.replaceresolveddstpath:_:-#.^/user/(??&lt;username&gt;\\w+)\n   * Value: /targetTestRoot/$username\n   * Dir path to test:\n   * viewfs://mt/user/hadoop_user1/hadoop_dir1\n   * Expect path: /targetTestRoot/hadoop-user1/hadoop_dir1\n   * resolvedPathStr: /user/hadoop_user1\n   * targetOfResolvedPathStr: /targetTestRoot/hadoop-user1\n   * remainingPath: /hadoop_dir1\n   *\n   * @param resultKind resultKind.\n   * @param resolvedPathStr resolvedPathStr.\n   * @param targetOfResolvedPathStr targetOfResolvedPathStr.\n   * @param remainingPath remainingPath.\n   * @return targetFileSystem or null on exceptions.",
  "org.apache.hadoop.fs.FSDataInputStream:readFully(long,byte[],int,int)" : "* Read bytes from the given position in the stream to the given buffer.\n   * Continues to read until <code>length</code> bytes have been read.\n   *\n   * @param position  position in the input stream to seek\n   * @param buffer    buffer into which data is read\n   * @param offset    offset into the buffer in which data is written\n   * @param length    the number of bytes to read\n   * @throws IOException IO problems\n   * @throws EOFException If the end of stream is reached while reading.\n   *                      If an exception is thrown an undetermined number\n   *                      of bytes in the buffer may have been written.",
  "org.apache.hadoop.ipc.Server$Connection:getAuthorizedUgi(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>(int,boolean)" : "* Creates a new compressor.\n   *\n   * @param directBufferSize size of the direct buffer to be used.\n   * @param useLz4HC use high compression ratio version of lz4, \n   *                 which trades CPU for compression ratio.",
  "org.apache.hadoop.net.NetUtils:getFreeSocketPorts(int)" : "* Return free ports. There is no guarantee they will remain free, so\n   * ports should be used immediately. The number of free ports returned by\n   * this method should match argument {@code numOfPorts}. Num of ports\n   * provided in the argument should not exceed 25.\n   *\n   * @param numOfPorts Number of free ports to acquire.\n   * @return Free ports for binding a local socket.",
  "org.apache.hadoop.io.compress.SplitCompressionInputStream:getAdjustedStart()" : "* After calling createInputStream, the values of start or end\n   * might change.  So this method can be used to get the new value of start.\n   * @return The changed value of start",
  "org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler:<init>()" : null,
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:replication(short)" : "* Set replication factor.",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getCounterReference(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:getName()" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToString(org.apache.hadoop.fs.statistics.IOStatistics)" : "* Convert IOStatistics to a string form.\n   * @param statistics A statistics instance.\n   * @return string value or the empty string if null",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:getRecordIndex()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.SecurityUtil:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)" : "* Look up the TokenInfo for a given protocol. It searches all known\n   * SecurityInfo providers.\n   * @param protocol The protocol class to get the information for.\n   * @param conf Configuration object\n   * @return the TokenInfo or null if it has no KerberosInfo defined",
  "org.apache.hadoop.util.ShutdownHookManager:hasShutdownHook(java.lang.Runnable)" : "* Indicates if a shutdownHook is registered or not.\n   *\n   * @param shutdownHook shutdownHook to check if registered.\n   * @return TRUE/FALSE depending if the shutdownHook is is registered.",
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:<init>(int)" : "* Creates a new compressor.\n   *\n   * @param directBufferSize size of the direct buffer to be used.",
  "org.apache.hadoop.fs.FsShell:printUsage(java.io.PrintStream)" : null,
  "org.apache.hadoop.fs.store.LogExactlyOnce:error(java.lang.String,java.lang.Object[])" : null,
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getNextIdToTry(org.apache.hadoop.fs.Path,int)" : "* Return the next ID suffix to use when creating the log file. This method\n   * will look at the files in the directory, find the one with the highest\n   * ID suffix, and 1 to that suffix, and return it. This approach saves a full\n   * linear probe, which matters in the case where there are a large number of\n   * log files.\n   *\n   * @param initial the base file path\n   * @param lastId the last ID value that was used\n   * @return the next ID to try\n   * @throws IOException thrown if there's an issue querying the files in the\n   * directory",
  "org.apache.hadoop.io.UTF8:fromBytes(byte[])" : "* @return Convert a UTF-8 encoded byte array back into a string.\n   *\n   * @param bytes input bytes.\n   * @throws IOException if the byte array is invalid UTF8",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getUsed()" : "* Return the total size of all files under \"/\", if {@link\n   * Constants#CONFIG_VIEWFS_LINK_MERGE_SLASH} is supported and is a valid\n   * mount point. Else, throw NotInMountpointException.\n   *\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.conf.Configuration:getFile(java.lang.String,java.lang.String)" : "* Get a local file name under a directory named in <i>dirsProp</i> with\n   * the given <i>path</i>.  If <i>dirsProp</i> contains multiple directories,\n   * then one is chosen based on <i>path</i>'s hash code.  If the selected\n   * directory does not exist, an attempt is made to create it.\n   *\n   * @param dirsProp directory in which to locate the file.\n   * @param path file-path.\n   * @return local file under the directory with the given path.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.UserGroupInformation:getUGIFromSubject(javax.security.auth.Subject)" : "* Create a UserGroupInformation from a Subject with Kerberos principal.\n   *\n   * @param subject             The KerberosPrincipal to use in UGI.\n   *                            The creator of subject is responsible for\n   *                            renewing credentials.\n   *\n   * @throws IOException raised on errors performing I/O.\n   * @throws KerberosAuthException if the kerberos login fails\n   * @return UserGroupInformation",
  "org.apache.hadoop.jmx.JMXJsonServlet:listBeans(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.fs.impl.FlagSet:makeImmutable()" : "* Make immutable; no-op if already set.",
  "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:<init>(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)" : null,
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:activeInstance()" : "* Get the statistics instance.\n   * @return the instance to build/return\n   * @throws IllegalStateException if the builder has already been built.",
  "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:initRegistry(java.lang.Object)" : null,
  "org.apache.hadoop.io.EnumSetWritable:toString()" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:getProxyInfo()" : null,
  "org.apache.hadoop.fs.ftp.FTPException:<init>(java.lang.Throwable)" : null,
  "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:setRightAlign(int[])" : "* Change the default left-align of columns\n     * @param indexes of columns to right align",
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:noPasswordError()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path)" : "* Create an FSDataOutputStream at the indicated Path.\n   * Files are overwritten by default.\n   * @param f the file to create\n   * @throws IOException IO failure\n   * @return output stream.",
  "org.apache.hadoop.io.DataOutputBuffer$Buffer:<init>(int)" : null,
  "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:deserializeFromString(java.lang.String)" : "* Create interceptor from config string. The string should be in\n   * replaceresolvedpath:wordToReplace:replaceString\n   * Note that we'll assume there's no ':' in the regex for the moment.\n   *\n   * @return Interceptor instance or null on bad config.",
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:calculateCoding(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:power(int,int)" : "* Compute power n of a field\n   *\n   * @param x input field\n   * @param n power\n   * @return x^n",
  "org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,java.lang.String,java.lang.Object[])" : "* Create the duration text from a {@code String.format()} code call;\n   * log output at info level.\n   * @param log log to write to\n   * @param format format string\n   * @param args list of arguments",
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:isClosed()" : null,
  "org.apache.hadoop.fs.permission.AclUtil:getMinimalAcl(org.apache.hadoop.fs.permission.FsPermission)" : "* Translates the given permission bits to the equivalent minimal ACL.\n   *\n   * @param perm FsPermission to translate\n   * @return List&lt;AclEntry&gt; containing exactly 3 entries representing the\n   *         owner, group and other permissions",
  "org.apache.hadoop.service.launcher.InterruptEscalator:getOwner()" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator)" : null,
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumParityUnits()" : null,
  "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence,java.nio.charset.Charset)" : "* Write a line of text to a file. Characters are encoded into bytes using the\n   * specified charset. This utility method opens the file for writing, creating\n   * the file if it does not exist, or overwrites an existing file.\n   *\n   * @param fs the file system with which to create the file\n   * @param path the path to the file\n   * @param charseq the char sequence to write to the file\n   * @param cs the charset to use for encoding\n   *\n   * @return the file system\n   *\n   * @throws NullPointerException if any of the arguments are {@code null}\n   * @throws IOException if an I/O error occurs creating or writing to the file",
  "org.apache.hadoop.io.CompressedWritable:ensureInflated()" : "Must be called by all methods which access fields to ensure that the data\n   * has been uncompressed.",
  "org.apache.hadoop.util.CpuTimeTracker:updateElapsedJiffies(java.math.BigInteger,long)" : "* Apply delta to accumulators.\n   * @param elapsedJiffies updated jiffies\n   * @param newTime new sample time",
  "org.apache.hadoop.fs.ChecksumFs:getChecksumFileLength(org.apache.hadoop.fs.Path,long)" : "* Return the length of the checksum file given the size of the\n   * actual file.\n   *\n   * @param file the file path.\n   * @param fileSize file size.\n   * @return check sum file length.",
  "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:getGroups(java.lang.String)" : null,
  "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:sourceName(java.lang.String)" : "* Get a source name by given directory name.\n   *\n   * @param dirName directory name\n   * @return the source name",
  "org.apache.hadoop.conf.Configuration:setAllowNullValueProperties(boolean)" : "* Set Configuration to allow keys without values during setup.  Intended\n   * for use during testing.\n   *\n   * @param val If true, will allow Configuration to store keys without values",
  "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getAttributes()" : null,
  "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:getThisBuilder()" : null,
  "org.apache.hadoop.fs.FileChecksum:getChecksumOpt()" : null,
  "org.apache.hadoop.conf.Configuration:isPropertyTag(java.lang.String)" : "* Get Property tag Enum corresponding to given source.\n   *\n   * @param tagStr String representation of Enum\n   * @return true if tagStr is a valid tag",
  "org.apache.hadoop.fs.ZeroCopyUnavailableException:<init>(java.lang.String,java.lang.Exception)" : null,
  "org.apache.hadoop.util.LightWeightGSet:<init>(int)" : "* @param recommended_length Recommended size of the internal array.",
  "org.apache.hadoop.fs.FSDataOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics,long)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies:shouldFailoverOnException(java.lang.Exception)" : null,
  "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:memoryFreed(int)" : null,
  "org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo:equals(java.lang.Object)" : null,
  "org.apache.hadoop.util.CrcComposer:digest()" : "* Returns byte representation of composed CRCs; if no stripeLength was\n   * specified, the digest should be of length equal to exactly one CRC.\n   * Otherwise, the number of CRCs in the returned array is equal to the\n   * total sum bytesPerCrc divided by stripeLength. If the sum of bytesPerCrc\n   * is not a multiple of stripeLength, then the last CRC in the array\n   * corresponds to totalLength % stripeLength underlying data bytes.\n   *\n   * @return byte representation of composed CRCs.",
  "org.apache.hadoop.fs.FileSystem:getNamed(java.lang.String,org.apache.hadoop.conf.Configuration)" : "* @deprecated call {@link #get(URI, Configuration)} instead.\n   *\n   * @param name name.\n   * @param conf configuration.\n   * @return file system.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.fs.shell.PathData:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.FileStatus)" : "* Creates an object to wrap the given parameters as fields.  The string\n   * used to create the path will be recorded since the Path object does not\n   * return exactly the same string used to initialize it.\n   * @param fs the FileSystem\n   * @param pathString a String of the path\n   * @param stat the FileStatus (may be null if the path doesn't exist)",
  "org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,long,long)" : "* Create a instrumented lock instance which logs a warning message\n   * when lock held time is above given threshold.\n   *\n   * @param name the identifier of the lock object\n   * @param logger this class does not have its own logger, will log to the\n   *               given logger instead\n   * @param minLoggingGapMs  the minimum time gap between two log messages,\n   *                         this is to avoid spamming to many logs\n   * @param lockWarningThresholdMs the time threshold to view lock held\n   *                               time as being \"too long\"",
  "org.apache.hadoop.util.ComparableVersion$StringItem:isNull()" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:writeShortArray(java.io.DataOutput)" : null,
  "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:containsHeader(java.lang.String)" : null,
  "org.apache.hadoop.security.CompositeGroupsMapping:getConf()" : null,
  "org.apache.hadoop.io.MapWritable:containsValue(java.lang.Object)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:read(long,byte[],int,int)" : null,
  "org.apache.hadoop.conf.ReconfigurableBase:<init>(org.apache.hadoop.conf.Configuration)" : "* Construct a ReconfigurableBase with the {@link Configuration}\n   * conf.\n   * @param conf configuration.",
  "org.apache.hadoop.fs.FileUtil:execCommand(java.io.File,java.lang.String[])" : null,
  "org.apache.hadoop.ipc.CallQueueManager:offer(java.lang.Object)" : "* Insert e into the backing queue.\n   * Return true if e is queued.\n   * Return false if the queue is full.",
  "org.apache.hadoop.ipc.Server$Connection:processRpcRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)" : "* Process an RPC Request \n     *   - the connection headers and context must have been already read.\n     *   - Based on the rpcKind, decode the rpcRequest.\n     *   - A successfully decoded RpcCall will be deposited in RPC-Q and\n     *     its response will be sent later when the request is processed.\n     * @param header - RPC request header\n     * @param buffer - stream to request payload\n     * @throws RpcServerException - generally due to fatal rpc layer issues\n     *   such as invalid header or deserialization error.  The call queue\n     *   may also throw a fatal or non-fatal exception on overflow.\n     * @throws IOException - fatal internal error that should/could not\n     *   be sent to client.\n     * @throws InterruptedException",
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:writeHeader(byte[],int,int)" : null,
  "org.apache.hadoop.io.Text:readString(java.io.DataInput)" : "* @return Read a UTF8 encoded string from in.\n   * @param in input in.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.shell.CommandWithDestination:setLazyPersist(boolean)" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:error(java.lang.String,java.lang.Throwable)" : "* Report an error. \n   * <p>\n   * This tries to log to {@code LOG.error()}.\n   * <p>\n   * If that log level is disabled disabled the message\n   * is logged to system error along with {@code thrown.toString()}\n   * @param message message for the user\n   * @param thrown the exception thrown",
  "org.apache.hadoop.io.DataInputByteBuffer$Buffer:read(byte[],int,int)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getServerDefaults()" : null,
  "org.apache.hadoop.log.LogThrottlingHelper:<init>(long,java.lang.String)" : "* Create a log helper with a specified primary recorder name; this can be\n   * used in conjunction with {@link #record(String, long, double...)} to set up\n   * primary and dependent recorders. See\n   * {@link #record(String, long, double...)} for more details.\n   *\n   * @param minLogPeriodMs The minimum period with which to log; do not log\n   *                       more frequently than this.\n   * @param primaryRecorderName The name of the primary recorder.",
  "org.apache.hadoop.ha.HAAdmin:help(java.lang.String[],java.util.Map)" : null,
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.fs.shell.Command:processPath(org.apache.hadoop.fs.shell.PathData)" : "* Hook for commands to implement an operation to be applied on each\n   * path for the command.  Note implementation of this method is optional\n   * if earlier methods in the chain handle the operation.\n   * @param item a {@link PathData} object\n   * @throws RuntimeException if invoked but not implemented\n   * @throws IOException if anything else goes wrong in the user-implementation",
  "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:<init>()" : null,
  "org.apache.hadoop.metrics2.lib.MutableRollingAverages$SumAndCount:getSnapshotTimeStamp()" : null,
  "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:setConf(org.apache.hadoop.conf.Configuration)" : "* Sets the configuration for the factory.\n   *\n   * @param conf the configuration for the factory.",
  "org.apache.hadoop.fs.HarFileSystem:<init>()" : "* public construction of harfilesystem",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:updateReportedByteCount(int)" : "* This method is called by the client of this\n   * class in case there are any corrections in\n   * the stream position.  One common example is\n   * when client of this code removes starting BZ\n   * characters from the compressed stream.\n   *\n   * @param count count bytes are added to the reported bytes\n   *",
  "org.apache.hadoop.util.IntrusiveCollection:toArray()" : null,
  "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)" : "* Create the named map for keys of the named class.\n     * @deprecated Use Writer(Configuration, Path, Option...) instead.\n     * @param conf configuration.\n     * @param fs fs.\n     * @param dirName dirName.\n     * @param keyClass keyClass.\n     * @param valClass valClass.\n     * @param compress compress.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.shell.Delete$Rm:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.StorageType:getMovableTypes()" : null,
  "org.apache.hadoop.net.NetworkTopology:getInstance(org.apache.hadoop.conf.Configuration)" : "* Get an instance of NetworkTopology based on the value of the configuration\n   * parameter net.topology.impl.\n   * \n   * @param conf the configuration to be used\n   * @return an instance of NetworkTopology",
  "org.apache.hadoop.security.UserGroupInformation:hashCode()" : "* Return the hash of the subject.",
  "org.apache.hadoop.fs.viewfs.ViewFs:getUriDefaultPort()" : null,
  "org.apache.hadoop.ipc.RpcWritable$Buffer:wrap(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.fs.shell.find.Find:createOptions()" : "Create a new set of find options.",
  "org.apache.hadoop.ipc.Server:getServerRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind)" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)" : "* Write lines of text to a file. Each line is a char sequence and is written\n   * to the file in sequence with each line terminated by the platform's line\n   * separator, as defined by the system property {@code\n   * line.separator}. Characters are encoded into bytes using the specified\n   * charset. This utility method opens the file for writing, creating the file\n   * if it does not exist, or overwrites an existing file.\n   *\n   * @param fileContext the file context with which to create the file\n   * @param path the path to the file\n   * @param lines a Collection to iterate over the char sequences\n   * @param cs the charset to use for encoding\n   *\n   * @return the file context\n   *\n   * @throws NullPointerException if any of the arguments are {@code null}\n   * @throws IOException if an I/O error occurs creating or writing to the file",
  "org.apache.hadoop.fs.FileContext:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : "* Remove an xattr of a file or directory.\n   * The name must be prefixed with the namespace followed by \".\". For example,\n   * \"user.attr\".\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to remove extended attribute\n   * @param name xattr name\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.metrics2.util.SampleStat:<init>()" : "* Construct a new running sample stat",
  "org.apache.hadoop.io.file.tfile.BCFile$Reader:getDataBlock(int)" : "* Stream access to a Data Block.\n     * \n     * @param blockIndex\n     *          0-based data block index.\n     * @return BlockReader input stream for reading the data block.\n     * @throws IOException",
  "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:<init>()" : "Same as this(0, 0, null)",
  "org.apache.hadoop.fs.FileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)" : "Return the {@link QuotaUsage} of a given {@link Path}.\n   * @param f path to use\n   * @return the quota usage\n   * @throws IOException IO failure",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:instance()" : "* Get the singleton instance.\n   * @return the instance",
  "org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int)" : "* Call\n   * {@link #transferToFully(FileChannel, long, int, LongWritable, LongWritable)\n   * }\n   * with null <code>waitForWritableTime</code> and <code>transferToTime</code>.\n   *\n   * @param fileCh input fileCh.\n   * @param position input position.\n   * @param count input count.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.shell.Command:<init>(org.apache.hadoop.conf.Configuration)" : "* Constructor.\n   *\n   * @param conf configuration.",
  "org.apache.hadoop.fs.FSOutputSummer:<init>(org.apache.hadoop.util.DataChecksum)" : null,
  "org.apache.hadoop.ipc.Client$Connection:writeConnectionContext(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.security.SaslRpcServer$AuthMethod)" : null,
  "org.apache.hadoop.fs.FileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Called when we're all done writing to the target.\n   * A local FS will do nothing, because we've written to exactly the\n   * right place.\n   * A remote FS will copy the contents of tmpLocalFile to the correct target at\n   * fsOutputFile.\n   * @param fsOutputFile path of output file\n   * @param tmpLocalFile path to local tmp file\n   * @throws IOException IO failure",
  "org.apache.hadoop.http.HttpServer2Metrics:dispatchedActive()" : null,
  "org.apache.hadoop.util.InvalidChecksumSizeException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.BoundedByteArrayOutputStream:size()" : "* Returns the length of the valid data\n   * currently in the buffer.\n   *\n   * @return the length of the valid data.",
  "org.apache.hadoop.util.PriorityQueue:upHeap()" : null,
  "org.apache.hadoop.metrics2.MetricStringBuilder:setContext(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForField(java.lang.reflect.Field,org.apache.hadoop.metrics2.annotation.Metric)" : "* Override to handle custom mutable metrics for fields\n   * @param field of the metric\n   * @param annotation  of the field\n   * @return a new metric object or null",
  "org.apache.hadoop.fs.GlobExpander$StringWithOffset:<init>(java.lang.String,int)" : null,
  "org.apache.hadoop.fs.FileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Copy a file to the local filesystem, then delete it from the\n   * remote filesystem (if successfully copied).\n   * @param src path src file in the remote filesystem\n   * @param dst path local destination\n   * @throws IOException IO failure",
  "org.apache.hadoop.http.HttpServer2Metrics:requests()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withSampleTracking(java.lang.String[])" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileMeta:<init>(java.lang.String)" : null,
  "org.apache.hadoop.util.LightWeightGSet$Values:clear()" : null,
  "org.apache.hadoop.conf.Configuration:getInts(java.lang.String)" : "* Get the value of the <code>name</code> property as a set of comma-delimited\n   * <code>int</code> values.\n   * \n   * If no such property exists, an empty array is returned.\n   * \n   * @param name property name\n   * @return property value interpreted as an array of comma-delimited\n   *         <code>int</code> values",
  "org.apache.hadoop.io.file.tfile.TFile$Reader:compareKeys(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)" : null,
  "org.apache.hadoop.io.FloatWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.viewfs.FsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Gets file system instance of given uri.\n   *\n   * @param uri uri.\n   * @param conf configuration.\n   * @throws IOException raised on errors performing I/O.\n   * @return FileSystem.",
  "org.apache.hadoop.util.HostsFileReader:updateFileNames(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter:merge(java.util.List,org.apache.hadoop.fs.Path)" : "* Merges the list of segments of type <code>SegmentDescriptor</code>\n     * @param segments the list of SegmentDescriptors\n     * @param tmpDir the directory to write temporary files into\n     * @return RawKeyValueIterator\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.authorize.ProxyUsers:getInstance(org.apache.hadoop.conf.Configuration)" : "* Returns an instance of ImpersonationProvider.\n   * Looks up the configuration to see if there is custom class specified.\n   * @param conf\n   * @return ImpersonationProvider",
  "org.apache.hadoop.net.CachedDNSToSwitchMapping:getCachedHosts(java.util.List)" : "* @param names a list of hostnames to look up (can be be empty)\n   * @return the cached resolution of the list of hostnames/addresses.\n   *  or null if any of the names are not currently in the cache",
  "org.apache.hadoop.ipc.CallQueueManager:poll()" : null,
  "org.apache.hadoop.ha.PowerShellFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)" : null,
  "org.apache.hadoop.ipc.RpcNoSuchMethodException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.security.http.CrossOriginFilter:areHeadersAllowed(java.lang.String)" : null,
  "org.apache.hadoop.util.DurationInfo:toString()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getModificationTime()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:listStatus(org.apache.hadoop.fs.Path)" : "* {@inheritDoc}\n   *\n   * Note: listStatus considers listing from fallbackLink if available. If the\n   * same directory path is present in configured mount path as well as in\n   * fallback fs, then only the fallback path will be listed in the returned\n   * result except for link.\n   *\n   * If any of the the immediate children of the given path f is a symlink(mount\n   * link), the returned FileStatus object of that children would be represented\n   * as a symlink. It will not be resolved to the target path and will not get\n   * the target path FileStatus object. The target path will be available via\n   * getSymlink on that children's FileStatus object. Since it represents as\n   * symlink, isDirectory on that children's FileStatus will return false.\n   * This behavior can be changed by setting an advanced configuration\n   * fs.viewfs.mount.links.as.symlinks to false. In this case, mount points will\n   * be represented as non-symlinks and all the file/directory attributes like\n   * permissions, isDirectory etc will be assigned from it's resolved target\n   * directory/file.\n   *\n   * If you want to get the FileStatus of target path for that children, you may\n   * want to use GetFileStatus API with that children's symlink path. Please see\n   * {@link ViewFs#getFileStatus(Path f)}\n   *\n   * Note: In ViewFs, by default the mount links are represented as symlinks.",
  "org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:compress(byte[],int,int)" : null,
  "org.apache.hadoop.net.SocketIOWithTimeout:close()" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:getChecksumLength(long,int)" : "* Calculated the length of the checksum file in bytes.\n   * @param size the length of the data file in bytes\n   * @param bytesPerSum the number of bytes in a checksum block\n   * @return the number of bytes in the checksum file",
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:end()" : null,
  "org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:close()" : null,
  "org.apache.hadoop.io.file.tfile.TFile:getSupportedCompressionAlgorithms()" : "* Get names of supported compression algorithms. The names are acceptable by\n   * TFile.Writer.\n   * \n   * @return Array of strings, each represents a supported compression\n   *         algorithm. Currently, the following compression algorithms are\n   *         supported.\n   *         <ul>\n   *         <li>\"none\" - No compression.\n   *         <li>\"lzo\" - LZO compression.\n   *         <li>\"gz\" - GZIP compression.\n   *         </ul>",
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Constructor.\n   *\n   * @param fileSystem fileSystem.\n   * @param p path.",
  "org.apache.hadoop.ipc.Server:registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcInvoker)" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getProtocolVersion()" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token)" : null,
  "org.apache.hadoop.net.NetworkTopology:toString()" : "convert a network tree to a string.",
  "org.apache.hadoop.fs.shell.Command:getCommandField(java.lang.String)" : "* Get a public static class field\n   * @param field the field to retrieve\n   * @return String of the field",
  "org.apache.hadoop.fs.FsShell$UnknownCommandException:getMessage()" : null,
  "org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)" : "* Get the value of the <code>name</code> property as an <code>int</code>.\n   *   \n   * If no such property exists, the provided default value is returned,\n   * or if the specified value is not a valid <code>int</code>,\n   * then an error is thrown.\n   * \n   * @param name property name.\n   * @param defaultValue default value.\n   * @throws NumberFormatException when the value is invalid\n   * @return property value as an <code>int</code>, \n   *         or <code>defaultValue</code>.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.ContentSummary:<init>(long,long,long,long,long,long)" : "* Constructor, deprecated by ContentSummary.Builder.\n   *\n   * @param length length.\n   * @param fileCount file count.\n   * @param directoryCount directory count.\n   * @param quota quota.\n   * @param spaceConsumed space consumed.\n   * @param spaceQuota space quota.\n   *",
  "org.apache.hadoop.fs.ContentSummary:getFileCount()" : "@return the file count",
  "org.apache.hadoop.ipc.Client:getRpcResponse(org.apache.hadoop.ipc.Client$Call,org.apache.hadoop.ipc.Client$Connection,long,java.util.concurrent.TimeUnit)" : "@return the rpc response or, in case of timeout, null.",
  "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getRack(java.lang.String)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:getActiveData()" : "* get data set by the active leader\n   * \n   * @return data set by the active instance\n   * @throws ActiveNotFoundException\n   *           when there is no active leader\n   * @throws KeeperException\n   *           other zookeeper operation errors\n   * @throws InterruptedException\n   *           interrupted exception.\n   * @throws IOException\n   *           when ZooKeeper connection could not be established",
  "org.apache.hadoop.io.SequenceFile$UncompressedBytes:getSize()" : null,
  "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameter(java.lang.String)" : "* Unquote the name and quote the value.",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:meanStatistics()" : null,
  "org.apache.hadoop.io.VersionedWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration,boolean)" : "* Copies from one stream to another.\n   *\n   * @param in InputStream to read from\n   * @param out OutputStream to write to\n   * @param conf the Configuration object\n   * @param close whether or not close the InputStream and \n   * OutputStream at the end. The streams are closed in the finally clause.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.LightWeightGSet:<init>()" : null,
  "org.apache.hadoop.io.InputBuffer$Buffer:getPosition()" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:safeDelete(java.lang.String,java.util.List,java.lang.String)" : "* Deletes the path. Checks for existence of path as well.\n   *\n   * @param path Path to be deleted.\n   * @param fencingNodePath fencingNodePath.\n   * @param fencingACL fencingACL.\n   * @throws Exception if any problem occurs while performing deletion.",
  "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:set(float)" : null,
  "org.apache.hadoop.io.UTF8$1:<init>()" : null,
  "org.apache.hadoop.fs.FileContext:listStatus(org.apache.hadoop.fs.Path)" : "* List the statuses of the files/directories in the given path if the path is\n   * a directory.\n   * \n   * @param f is the path\n   *\n   * @return an iterator that traverses statuses of the files/directories \n   *         in the given path\n   *\n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If <code>f</code> does not exist\n   * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n   *           not supported\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server",
  "org.apache.hadoop.fs.shell.Stat:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getUser()" : "* Get the username encoded in the token identifier\n   * \n   * @return the username or owner",
  "org.apache.hadoop.metrics2.lib.UniqueNames$Count:<init>(java.lang.String,int)" : null,
  "org.apache.hadoop.fs.DF:parseOutput()" : null,
  "org.apache.hadoop.fs.FileSystem:getStorageStatistics()" : "* Get the StorageStatistics for this FileSystem object.  These statistics are\n   * per-instance.  They are not shared with any other FileSystem object.\n   *\n   * <p>This is a default method which is intended to be overridden by\n   * subclasses. The default implementation returns an empty storage statistics\n   * object.</p>\n   *\n   * @return    The StorageStatistics for this FileSystem instance.\n   *            Will never be null.",
  "org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesWritten(long)" : "* Increment the bytes written in the statistics.\n     * @param newBytes the additional bytes written",
  "org.apache.hadoop.ipc.CallerContext$Builder:appendIfAbsent(java.lang.String,java.lang.String)" : "* Append new field which contains key and value to the context\n     * if the key(\"key:\") is absent.\n     * @param key the key of field.\n     * @param value the value of field.\n     * @return the builder.",
  "org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:reset()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:<init>(org.apache.hadoop.metrics2.MetricsInfo,long,java.util.List,java.lang.Iterable)" : "* Construct a metrics record\n   * @param info  {@link MetricsInfo} of the record\n   * @param timestamp of the record\n   * @param tags  of the record\n   * @param metrics of the record",
  "org.apache.hadoop.fs.FilterFileSystem:listStatus(org.apache.hadoop.fs.Path)" : "List files in a directory.",
  "org.apache.hadoop.net.DNS:getDefaultIP(java.lang.String)" : "* Returns the first available IP address associated with the provided\n   * network interface or the local host IP if \"default\" is given.\n   *\n   * @param strInterface\n   *            The name of the network interface or subinterface to query\n   *             (e.g. eth0 or eth0:0) or the string \"default\"\n   * @return The IP address in text form, the local host IP is returned\n   *         if the interface name \"default\" is specified\n   * @throws UnknownHostException\n   *             If the given interface is invalid",
  "org.apache.hadoop.fs.CachingGetSpaceUsed:getJitter()" : "* Randomize the refresh interval timing by this amount, the actual interval will be chosen\n   * uniformly between {@code interval-jitter} and {@code interval+jitter}.\n   *\n   * @return between interval-jitter and interval+jitter.",
  "org.apache.hadoop.util.AsyncDiskService:awaitTermination(long)" : "* Wait for the termination of the thread pools.\n   * \n   * @param milliseconds  The number of milliseconds to wait\n   * @return   true if all thread pools are terminated without time limit\n   * @throws InterruptedException if the thread is interrupted.",
  "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:magnitude()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:removeDomain(java.lang.String)" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:decrypt(long,byte[],int,int)" : "* Decrypt length bytes in buffer starting at offset. Output is also put \n   * into buffer starting at offset. It is thread-safe.",
  "org.apache.hadoop.fs.FSInputChecker:fill()" : "* Fills the buffer with a chunk data. \n   * No mark is supported.\n   * This method assumes that all data in the buffer has already been read in,\n   * hence pos > count.",
  "org.apache.hadoop.ipc.Server$Connection:saslReadAndProcess(org.apache.hadoop.ipc.RpcWritable$Buffer)" : null,
  "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:<init>(org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder)" : "* Instantiate.\n   * <p>\n   * All maps/enums passed down are copied into thread safe equivalents.\n   * as their origin is unknown and cannot be guaranteed to\n   * not be shared.\n   * <p>\n   * Context and operationId are expected to be well formed\n   * numeric/hex strings, at least adequate to be\n   * used as individual path elements in a URL.",
  "org.apache.hadoop.fs.AbstractFileSystem:makeQualified(org.apache.hadoop.fs.Path)" : "* Make the path fully qualified to this file system\n   * @param path the path.\n   * @return the qualified path",
  "org.apache.hadoop.ipc.Server:getServiceAuthorizationManager()" : "* Returns a handle to the serviceAuthorizationManager (required in tests)\n   * @return instance of ServiceAuthorizationManager for this server",
  "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:processWaitTimeAndRetryInfo()" : null,
  "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])" : "* Add a LinkMerge to the config for the specified mount table.\n   *\n   * @param conf configuration.\n   * @param mountTableName mountTable.\n   * @param targets targets.",
  "org.apache.hadoop.security.token.Token:isManaged()" : "* Is this token managed so that it can be renewed or cancelled?\n   * @return true, if it can be renewed and cancelled.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.DecayRpcScheduler:parseBackOffByResponseTimeEnabled(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.store.LogExactlyOnce:warn(java.lang.String,java.lang.Object[])" : null,
  "org.apache.hadoop.fs.impl.FileRangeImpl:getLength()" : null,
  "org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:<init>(org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry,java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration)" : "* Returns the configured FileSystem implementation.\n   * @param conf the configuration to use\n   * @return FileSystem.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.util.RunJar:unJar(java.io.InputStream,java.io.File,java.util.regex.Pattern)" : "* Unpack matching files from a jar. Entries inside the jar that do\n   * not match the given pattern will be skipped.\n   *\n   * @param inputStream the jar stream to unpack\n   * @param toDir the destination directory into which to unpack the jar\n   * @param unpackRegex the pattern to match jar entries against\n   *\n   * @throws IOException if an I/O error has occurred or toDir\n   * cannot be created and does not already exist",
  "org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String,java.lang.String)" : "* Guts of the servlet - extracted for easy testing.",
  "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:getRemaining()" : null,
  "org.apache.hadoop.io.erasurecode.CodecRegistry:<init>()" : null,
  "org.apache.hadoop.fs.permission.FsPermission:getDefault()" : "* Get the default permission for directory and symlink.\n   * In previous versions, this default permission was also used to\n   * create files, so files created end up with ugo+x permission.\n   * See HADOOP-9155 for detail. \n   * Two new methods are added to solve this, please use \n   * {@link FsPermission#getDirDefault()} for directory, and use\n   * {@link FsPermission#getFileDefault()} for file.\n   * This method is kept for compatibility.\n   *\n   * @return Default FsPermission.",
  "org.apache.hadoop.io.WritableUtils:readCompressedByteArray(java.io.DataInput)" : null,
  "org.apache.hadoop.net.NetUtils:getIPs(java.lang.String,boolean)" : "* Return an InetAddress for each interface that matches the\n   * given subnet specified using CIDR notation.\n   *\n   * @param subnet subnet specified using CIDR notation\n   * @param returnSubinterfaces\n   *            whether to return IPs associated with subinterfaces\n   * @throws IllegalArgumentException if subnet is invalid\n   * @return ips.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestPrefetch(int)" : "* Requests optional prefetching of the given block.\n   * The block is prefetched only if we can acquire a free buffer.\n   *\n   * @throws IllegalArgumentException if blockNumber is negative.",
  "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.tools.GetUserMappingsProtocol)" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:getConfigurationsToCreate()" : "* Override point: Get a list of configuration classes to create.\n   * @return the array of configs to attempt to create. If any are off the\n   * classpath, that is logged",
  "org.apache.hadoop.fs.ChecksumFileSystem:setWriteChecksum(boolean)" : null,
  "org.apache.hadoop.ipc.Server:addSuppressedLoggingExceptions(java.lang.Class[])" : "* Add exception classes which server won't log at all.\n   *\n   * @param exceptionClass exception classes",
  "org.apache.hadoop.fs.local.RawLocalFs:getServerDefaults()" : null,
  "org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:refreshCallQueue()" : null,
  "org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.String,java.lang.Class[])" : null,
  "org.apache.hadoop.io.ShortWritable:toString()" : "Short values in string format",
  "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:<init>(java.io.InputStream)" : null,
  "org.apache.hadoop.http.JettyUtils:<init>()" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:finish()" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.BooleanWritable$Comparator:<init>()" : null,
  "org.apache.hadoop.metrics2.sink.FileSink:close()" : null,
  "org.apache.hadoop.fs.Globber:unescapePathComponent(java.lang.String)" : "* Convert a path component that contains backslash ecape sequences to a\n   * literal string.  This is necessary when you want to explicitly refer to a\n   * path that contains globber metacharacters.",
  "org.apache.hadoop.util.functional.FunctionRaisingIOE:unchecked(java.lang.Object)" : "* Apply unchecked.\n   * @param t argument\n   * @return the evaluated function\n   * @throws UncheckedIOException IOE raised.",
  "org.apache.hadoop.security.SaslInputStream:available()" : "* Returns the number of bytes that can be read from this input stream without\n   * blocking. The <code>available</code> method of <code>InputStream</code>\n   * returns <code>0</code>. This method <B>should</B> be overridden by\n   * subclasses.\n   * \n   * @return the number of bytes that can be read from this input stream without\n   *         blocking.\n   * @exception IOException\n   *              if an I/O error occurs.",
  "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:getSchemeName()" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:file(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.CachingGetSpaceUsed:<init>(java.io.File,long,long,long)" : "* Keeps track of disk usage.\n   *\n   * @param path        the path to check disk usage in\n   * @param interval    refresh the disk usage at this interval\n   * @param jitter      randomize the refresh interval timing by this amount;\n   *                    the actual interval will be chosen uniformly between\n   *                    {@code interval-jitter} and {@code interval+jitter}\n   * @param initialUsed use this value until next refresh\n   * @throws IOException if we fail to refresh the disk usage",
  "org.apache.hadoop.ipc.ClientId:toBytes(java.lang.String)" : "* @return Convert from clientId string byte[] representation of clientId.\n   * @param id input id.",
  "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForField(java.lang.reflect.Field,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)" : null,
  "org.apache.hadoop.io.compress.Lz4Codec:createCompressor()" : "* Create a new {@link Compressor} for use by this {@link CompressionCodec}.\n   *\n   * @return a new compressor for use by this codec",
  "org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.util.GcTimeMonitor$Builder:build()" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$SupportState:getStateCode()" : null,
  "org.apache.hadoop.fs.VectoredReadUtils:roundUp(long,int)" : "* Calculates the ceiling value of offset based on chunk size.\n   * @param offset file offset.\n   * @param chunkSize file chunk size.\n   * @return ceil value.",
  "org.apache.hadoop.http.HttpServer2:initializeWebServer(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String[])" : null,
  "org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])" : null,
  "org.apache.hadoop.fs.shell.Ls:isOrderTime()" : "* Should directory contents be displayed in mtime order.\n   * @return true mtime order, false default order",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX:getPmdkSupportStateMessage()" : null,
  "org.apache.hadoop.util.ChunkedArrayList:getNumChunks()" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:inflateDirect(java.nio.ByteBuffer,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.ipc.RPC:getProtocolInterfaces(java.lang.Class)" : "* Get all interfaces that the given protocol implements or extends\n   * which are assignable from VersionedProtocol.",
  "org.apache.hadoop.util.IdentityHashStore:put(java.lang.Object,java.lang.Object)" : "* Add a new (key, value) mapping.\n   *\n   * Inserting a new (key, value) never overwrites a previous one.\n   * In other words, you can insert the same key multiple times and it will\n   * lead to multiple entries.\n   *\n   * @param k Generics Type k.\n   * @param v Generics Type v.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMaximumReference(java.lang.String)" : "* Get a reference to the atomic instance providing the\n   * value for a specific maximum. This is useful if\n   * the value is passed around.\n   * @param key statistic name\n   * @return the reference\n   * @throws NullPointerException if there is no entry of that name",
  "org.apache.hadoop.fs.Path:isAbsolute()" : "* Returns true if the path component (i.e. directory) of this URI is\n   * absolute.  This method is a wrapper for {@link #isUriPathAbsolute()}.\n   *\n   * @return whether this URI's path is absolute",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token)" : null,
  "org.apache.hadoop.io.MD5Hash:charToNibble(char)" : null,
  "org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:isEmpty(long)" : "Is the queue empty for more than the given time in millisecond?",
  "org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class)" : "* Add an internal servlet in the server.\n   * Note: This method is to be used for adding servlets that facilitate\n   * internal communication and not for user facing functionality. For\n   * servlets added using this method, filters are not enabled.\n   *\n   * @param name The name of the servlet (can be passed as null)\n   * @param pathSpec The path spec for the servlet\n   * @param clazz The servlet class",
  "org.apache.hadoop.fs.FSDataOutputStream$PositionCache:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics,long)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:setResponse(org.apache.hadoop.thirdparty.protobuf.Message)" : null,
  "org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable)" : "* Uncaught exception handler.\n   * If an error is raised: shutdown\n   * The state of the system is unknown at this point -attempting\n   * a clean shutdown is dangerous. Instead: exit\n   * @param thread thread that failed\n   * @param exception the raised exception",
  "org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler:<init>()" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec:createCompressor()" : null,
  "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getWeight(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)" : null,
  "org.apache.hadoop.metrics2.impl.MetricCounterLong:value()" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:connect()" : "* Connect to the FTP server using configuration parameters *\n   * \n   * @return An FTPClient instance\n   * @throws IOException",
  "org.apache.hadoop.metrics2.sink.StatsDSink:flush()" : null,
  "org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addProcessingTime(java.lang.String,long)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO:renameTo(java.io.File,java.io.File)" : "* A version of renameTo that throws a descriptive exception when it fails.\n   *\n   * @param src                  The source path\n   * @param dst                  The destination path\n   * \n   * @throws NativeIOException   On failure.",
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:prefetch(org.apache.hadoop.fs.impl.prefetch.BufferData,java.time.Instant)" : null,
  "org.apache.hadoop.util.VersionInfo:_getUser()" : null,
  "org.apache.hadoop.ipc.RpcClientUtil$ProtoSigCacheKey:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:truncate(org.apache.hadoop.fs.Path,long)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:close()" : "* Close the reader. The state of the Reader object is undefined after\n     * close. Calling close() for multiple times has no effect.",
  "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:trimIdleSelectors(long)" : "* Closes selectors that are idle for IDLE_TIMEOUT (10 sec). It does not\n     * traverse the whole list, just over the one that have crossed \n     * the timeout.",
  "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getCompressedSize()" : "* Current size of compressed data.\n       * \n       * @return\n       * @throws IOException",
  "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)" : "* Construct the preferred type of SequenceFile Writer.\n   * @param fs The configured filesystem. \n   * @param conf The configuration.\n   * @param name The name of the file. \n   * @param keyClass The 'key' type.\n   * @param valClass The 'value' type.\n   * @param compressionType The compression type.\n   * @param codec The compression codec.\n   * @return Returns the handle to the constructed SequenceFile Writer.\n   * @throws IOException raised on errors performing I/O.\n   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}\n   *     instead.",
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:isNativeZlibLoaded()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateCurrentKey()" : "* Update the current master key \n   * This is called once by startThreads before tokenRemoverThread is created, \n   * and only by tokenRemoverThread afterwards.",
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:<init>(java.io.OutputStream)" : "* Constructs a new <tt>CBZip2OutputStream</tt> with a blocksize of 900k.\n  *\n  * <p>\n  * <b>Attention: </b>The caller is resonsible to write the two BZip2 magic\n  * bytes <tt>\"BZ\"</tt> to the specified stream prior to calling this\n  * constructor.\n  * </p>\n  *\n  * @param out *\n  *            the destination stream.\n  *\n  * @throws IOException\n  *             if an I/O error occurs in the specified stream.\n  * @throws NullPointerException\n  *             if <code>out == null</code>.",
  "org.apache.hadoop.util.ComparableVersion$ListItem:isNull()" : null,
  "org.apache.hadoop.ipc.Server$Call:setClientStateId(long)" : null,
  "org.apache.hadoop.util.Progress:addNewPhase()" : "Adds a new phase. Caller needs to set progress weightage",
  "org.apache.hadoop.ipc.Server:setRpcRequestClass(java.lang.Class)" : null,
  "org.apache.hadoop.ha.HealthMonitor:doHealthChecks()" : null,
  "org.apache.hadoop.fs.Stat:<init>(org.apache.hadoop.fs.Path,long,boolean,org.apache.hadoop.fs.FileSystem)" : null,
  "org.apache.hadoop.metrics2.lib.MutableGaugeInt:decr()" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMinimum(java.lang.String,long)" : null,
  "org.apache.hadoop.util.LambdaUtils:<init>()" : null,
  "org.apache.hadoop.io.ArrayFile$Reader:seek(long)" : "* Positions the reader before its <code>n</code>th value.\n     *\n     * @param n n key.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getAlgorithm()" : "* Get the algorithm from the cipher.\n     * @return the algorithm name",
  "org.apache.hadoop.metrics2.MetricStringBuilder:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)" : null,
  "org.apache.hadoop.ipc.metrics.RpcMetrics:getRpcRequeueCalls()" : "* Returns the number of requeue calls.\n   * @return long",
  "org.apache.hadoop.ha.ActiveStandbyElector:writeBreadCrumbNode(org.apache.zookeeper.data.Stat)" : "* Write the \"ActiveBreadCrumb\" node, indicating that this node may need\n   * to be fenced on failover.\n   * @param oldBreadcrumbStat",
  "org.apache.hadoop.conf.Configuration:dumpConfiguration(org.apache.hadoop.conf.Configuration,java.io.Writer)" : "*  Writes out all properties and their attributes (final and resource) to\n   *  the given {@link Writer}, the format of the output would be,\n   *\n   *  <pre>\n   *  { \"properties\" :\n   *      [ { key : \"key1\",\n   *          value : \"value1\",\n   *          isFinal : \"key1.isFinal\",\n   *          resource : \"key1.resource\" },\n   *        { key : \"key2\",\n   *          value : \"value2\",\n   *          isFinal : \"ke2.isFinal\",\n   *          resource : \"key2.resource\" }\n   *       ]\n   *   }\n   *  </pre>\n   *\n   *  It does not output the properties of the configuration object which\n   *  is loaded from an input stream.\n   *  <p>\n   *\n   * @param config the configuration\n   * @param out the Writer to write to\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.SequenceFile$Reader:nextRawKey(org.apache.hadoop.io.DataOutputBuffer)" : "* Read 'raw' keys.\n     * @param key - The buffer into which the key is read\n     * @return Returns the key length or -1 for end of file\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.crypto.CipherSuite:setUnknownValue(int)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:close()" : "* Finishing reading the block. Release all resources.",
  "org.apache.hadoop.security.token.DtFileOperations:matchAlias(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)" : "Match token service field to alias text.  True if alias is null.",
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getInternal(org.apache.hadoop.fs.impl.prefetch.BufferData)" : null,
  "org.apache.hadoop.io.BooleanWritable:write(java.io.DataOutput)" : "",
  "org.apache.hadoop.io.ArrayPrimitiveWritable:writeDoubleArray(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.net.URI)" : "* Add a link to the config for the specified mount table\n   * @param conf - add the link to this conf\n   * @param mountTableName mountTable.\n   * @param src - the src path name\n   * @param target - the target URI link",
  "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:initialize()" : null,
  "org.apache.hadoop.util.functional.FutureIO:awaitAllFutures(java.util.Collection,java.time.Duration)" : "* Evaluates a collection of futures and returns their results as a list,\n   * but only waits up to the specified timeout for each future to complete.\n   * <p>\n   * This method blocks until all futures in the collection have completed or\n   * the timeout expires, whichever happens first. If any future throws an\n   * exception during its execution, this method extracts and rethrows that exception.\n   * @param collection collection of futures to be evaluated\n   * @param duration timeout duration\n   * @param <T> type of the result.\n   * @return the list of future's result, if all went well.\n   * @throws InterruptedIOException waiting for future completion was interrupted\n   * @throws CancellationException if the future itself was cancelled\n   * @throws IOException if something went wrong\n   * @throws RuntimeException any nested RTE thrown\n   * @throws TimeoutException the future timed out.",
  "org.apache.hadoop.util.AutoCloseableLock:<init>()" : "* Creates an instance of {@code AutoCloseableLock}, initializes\n   * the underlying lock instance with a new {@code ReentrantLock}.",
  "org.apache.hadoop.fs.FileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)" : "* Execute the actual open file operation.\n   *\n   * This is invoked from {@code FSDataInputStreamBuilder.build()}\n   * and from {@link DelegateToFileSystem} and is where\n   * the action of opening the file should begin.\n   *\n   * The base implementation performs a blocking\n   * call to {@link #open(Path, int)} in this call;\n   * the actual outcome is in the returned {@code CompletableFuture}.\n   * This avoids having to create some thread pool, while still\n   * setting up the expectation that the {@code get()} call\n   * is needed to evaluate the result.\n   * @param path path to the file\n   * @param parameters open file parameters from the builder.\n   * @return a future which will evaluate to the opened file.\n   * @throws IOException failure to resolve the link.\n   * @throws IllegalArgumentException unknown mandatory key",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackRemoveToken(org.apache.hadoop.util.functional.InvocationRaisingIOE)" : null,
  "org.apache.hadoop.ipc.RetryCache$CacheEntry:setExpirationTime(long)" : null,
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:cacheGroupsRefresh()" : "* Caches groups, no need to do that for this provider",
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:checkBuffers(byte[][])" : "* Check and ensure the buffers are of the desired length.\n   * @param buffers the buffers to check",
  "org.apache.hadoop.io.CompressedWritable:<init>()" : null,
  "org.apache.hadoop.net.SocksSocketFactory:createSocket(java.lang.String,int)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:keyClass(java.lang.Class)" : null,
  "org.apache.hadoop.security.token.delegation.DelegationKey:readFields(java.io.DataInput)" : "",
  "org.apache.hadoop.io.VIntWritable:<init>()" : null,
  "org.apache.hadoop.io.compress.SplitCompressionInputStream:getAdjustedEnd()" : "* After calling createInputStream, the values of start or end\n   * might change.  So this method can be used to get the new value of end.\n   * @return The changed value of end",
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(java.util.List)" : "* Adds a list of false positive information to <i>this</i> retouched Bloom filter.\n   * @param keys The list of false positive.",
  "org.apache.hadoop.io.TwoDArrayWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:emptyStatistics()" : "* Get the shared instance of the immutable empty statistics\n   * object.\n   * @return an empty statistics object.",
  "org.apache.hadoop.util.JsonSerialization:fromInstance(java.lang.Object)" : "* clone by converting to JSON and back again.\n   * This is much less efficient than any Java clone process.\n   * @param instance instance to duplicate\n   * @return a new instance\n   * @throws IOException IO problems.",
  "org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String)" : "@param path for the exception",
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:get(int,java.nio.ByteBuffer)" : "* Gets the block having the given {@code blockNumber}.\n   *\n   * @throws IllegalArgumentException if buffer is null.",
  "org.apache.hadoop.util.StringUtils:stringToURI(java.lang.String[])" : "* @param str\n   *          The string array to be parsed into an URI array.\n   * @return <tt>null</tt> if str is <tt>null</tt>, else the URI array\n   *         equivalent to str.\n   * @throws IllegalArgumentException\n   *           If any string in str violates RFC&nbsp;2396.",
  "org.apache.hadoop.security.SaslPlainServer$SaslPlainServerFactory:getMechanismNames(java.util.Map)" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getBytesWritten()" : "* Returns the total number of compressed bytes output so far.\n   *\n   * @return the total (non-negative) number of compressed bytes output so far",
  "org.apache.hadoop.fs.permission.ScopedAclEntries:calculatePivotOnDefaultEntries(java.util.List)" : "* Returns the pivot point in the list between the access entries and the\n   * default entries.  This is the index of the first element in the list that\n   * is a default entry.\n   *\n   * @param aclBuilder ArrayList<AclEntry> containing entries to build\n   * @return int pivot point, or -1 if list contains no default entries",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[],int,int)" : "* Move the cursor to the first entry whose key is greater than or equal\n       * to the input key. The entry returned by the previous entry() call will\n       * be invalid.\n       * \n       * @param key\n       *          The input key\n       * @param keyOffset\n       *          offset in the key buffer.\n       * @param keyLen\n       *          key buffer length.\n       * @return true if we find an equal key; false otherwise.\n       * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.crypto.key.KeyProviderExtension:getExtension()" : null,
  "org.apache.hadoop.util.StringUtils:split(java.lang.String,char)" : "* Split a string using the given separator, with no escaping performed.\n   * @param str a string to be split. Note that this may not be null.\n   * @param separator a separator char\n   * @return an array of strings",
  "org.apache.hadoop.fs.ChecksumFileSystem:getRawFileSystem()" : "get the raw file system",
  "org.apache.hadoop.fs.FileContext:resolvePath(org.apache.hadoop.fs.Path)" : "* Resolve the path following any symlinks or mount points\n   * @param f to be resolved\n   * @return fully qualified resolved path\n   * \n   * @throws FileNotFoundException  If <code>f</code> does not exist\n   * @throws AccessControlException if access denied\n   * @throws IOException If an IO Error occurred\n   * @throws UnresolvedLinkException If unresolved link occurred.\n   *\n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws\n   *           undeclared exception to RPC server\n   *\n   * RuntimeExceptions:\n   * @throws InvalidPathException If path <code>f</code> is not valid",
  "org.apache.hadoop.fs.FsShell$Help:processRawArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:addEntry(org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry)" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager)" : "* Construct an RPC server.\n     * @param protocolImpl the instance whose methods will be called\n     * @param conf the configuration to use\n     * @param bindAddress the address to bind on to listen for connection\n     * @param port the port to listen for connections on\n     * @param numHandlers the number of method handler threads to run\n     * @param verbose whether each call should be logged\n     * @param numReaders input numberReaders.\n     * @param queueSizePerHandler input queueSizePerHandler.\n     * @param secretManager input secretManager.\n     * \n     * @deprecated use Server#Server(Class, Object, \n     *      Configuration, String, int, int, int, int, boolean, SecretManager)\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.Text:encode(java.lang.String)" : null,
  "org.apache.hadoop.fs.Options$CreateOpts:donotCreateParent()" : null,
  "org.apache.hadoop.fs.QuotaUsage$Builder:fileAndDirectoryCount(long)" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:validateEntry(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.util.InstrumentedLock:unlock()" : null,
  "org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:getSubjectLock()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:removeAcl(org.apache.hadoop.fs.Path)" : "* Removes all but the base ACL entries of files and directories.  The entries\n   * for user, group, and others are retained for compatibility with permission\n   * bits.\n   *\n   * @param path Path to modify\n   * @throws IOException if an ACL could not be removed",
  "org.apache.hadoop.fs.ContentSummary:<init>(long,long,long)" : "*  Constructor, deprecated by ContentSummary.Builder\n   *  This constructor implicitly set spaceConsumed the same as length.\n   *  spaceConsumed and length must be set explicitly with\n   *  ContentSummary.Builder.\n   *\n   * @param length length.\n   * @param fileCount file count.\n   * @param directoryCount directory count.\n   *",
  "org.apache.hadoop.security.User:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod)" : null,
  "org.apache.hadoop.fs.HarFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:endBlock()" : null,
  "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:ipc(org.apache.hadoop.ipc.internal.ShadedProtobufHelper$IpcCall)" : "* Evaluate a protobuf call, converting any ServiceException to an IOException.\n   * @param call invocation to make\n   * @return the result of the call\n   * @param <T> type of the result\n   * @throws IOException any translated protobuf exception",
  "org.apache.hadoop.util.functional.TaskPool$Builder:setStatisticsContext()" : "* Set the statistics context for this thread.",
  "org.apache.hadoop.net.NodeBase:getParent()" : "@return this node's parent",
  "org.apache.hadoop.io.file.tfile.ByteArray:offset()" : "* @return the offset in the buffer.",
  "org.apache.hadoop.fs.FsShell:displayError(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.security.UserGroupInformation:isKerberosKeyTabLoginRenewalEnabled()" : null,
  "org.apache.hadoop.util.BlockingThreadPoolExecutorService:toString()" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)" : null,
  "org.apache.hadoop.conf.Configuration:getDeprecatedKey(java.lang.String)" : null,
  "org.apache.hadoop.security.WhitelistBasedResolver:getSaslProperties(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPutArguments(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long)" : "* Check all the arguments to the\n   * {@link MultipartUploader#putPart(UploadHandle, int, Path, InputStream, long)}\n   * operation.\n   * @param filePath Target path for upload (as {@link #startUpload(Path)}).\n   * @param inputStream Data for this part. Implementations MUST close this\n   * stream after reading in the data.\n   * @param partNumber Index of the part relative to others.\n   * @param uploadId Identifier from {@link #startUpload(Path)}.\n   * @param lengthInBytes Target length to read from the stream.\n   * @throws IllegalArgumentException invalid argument",
  "org.apache.hadoop.net.NetworkTopology:incrementRacks()" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:doEncode(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][])" : null,
  "org.apache.hadoop.ipc.FairCallQueue:take()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferPool:tryAcquire(int)" : "* Acquires a buffer if one is immediately available. Otherwise returns null.\n   * @param blockNumber the id of the block to try acquire.\n   * @return the acquired block's {@code BufferData} or null.",
  "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:getWarningMessage(java.lang.String,java.lang.String)" : "* Method to provide the warning message. It gives the custom message if\n     * non-null, and default message otherwise.\n     * @param key the associated deprecated key.\n     * @param source the property source.\n     * @return message that is to be logged when a deprecated key is used.",
  "org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab()" : "* Re-Login a user in from a keytab file. Loads a user identity from a keytab\n   * file and logs them in. They become the currently logged-in user. This\n   * method assumes that {@link #loginUserFromKeytab(String, String)} had\n   * happened already.\n   * The Subject field of this UserGroupInformation object is updated to have\n   * the new credentials.\n   * @throws IOException raised on errors performing I/O.\n   * @throws KerberosAuthException on a failure",
  "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:release(org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool$SelectorInfo)" : "* puts selector back at the end of LRU list of free selectos.\n     * Also invokes trimIdleSelectors().\n     * \n     * @param info",
  "org.apache.hadoop.io.DoubleWritable:toString()" : null,
  "org.apache.hadoop.fs.FileSystemStorageStatistics:getScheme()" : null,
  "org.apache.hadoop.http.HttpServer2:addListener(org.eclipse.jetty.server.ServerConnector)" : null,
  "org.apache.hadoop.security.token.Token:getKind()" : "* Get the token kind.\n   * @return the kind of the token",
  "org.apache.hadoop.security.UserGroupInformation:doAs(java.security.PrivilegedAction)" : "* Run the given action as the user.\n   * @param <T> the return type of the run method\n   * @param action the method to execute\n   * @return the value from the run method",
  "org.apache.hadoop.fs.impl.prefetch.BufferPool:toString()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:equals(java.lang.Object)" : null,
  "org.apache.hadoop.conf.Configuration$Parser:handleStartProperty()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:timestamp()" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:seekToNewSource(long)" : null,
  "org.apache.hadoop.conf.Configuration$IntegerRanges:isEmpty()" : "* @return true if there are no values in this range, else false.",
  "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:renew()" : "* Renew or replace the delegation token for this file system.\n     * It can only be called when the action is not in the queue.\n     * @return\n     * @throws IOException",
  "org.apache.hadoop.fs.FilterFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path)" : "List files and its block locations in a directory.",
  "org.apache.hadoop.io.compress.lz4.Lz4Compressor:setDictionary(byte[],int,int)" : "* Does nothing.",
  "org.apache.hadoop.security.alias.CredentialProvider:noPasswordError()" : "* If a password for the provider is needed, but is not provided, this will\n   * return an error message and instructions for supplying said password to\n   * the provider.\n   * @return An error message and instructions for supplying the password",
  "org.apache.hadoop.io.ArrayPrimitiveWritable:readBooleanArray(java.io.DataInput)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:hasNext()" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadFromPath(org.apache.hadoop.fs.Path,char[])" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:allocateBuffer(boolean,int)" : null,
  "org.apache.hadoop.ipc.Client:releaseAsyncCall()" : null,
  "org.apache.hadoop.util.Daemon:<init>()" : "Construct a daemon thread.",
  "org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.ipc.RemoteException)" : null,
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numCached()" : "* Number of caching operations completed.\n   *\n   * @return the number of cached buffers.",
  "org.apache.hadoop.metrics2.MetricsException:<init>(java.lang.String,java.lang.Throwable)" : "* Construct the exception with a message and a cause\n   * @param message for the exception\n   * @param cause of the exception",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : null,
  "org.apache.hadoop.net.NodeBase:<init>(java.lang.String,java.lang.String,org.apache.hadoop.net.Node,int)" : "Construct a node from its name and its location\n   * @param name this node's name (can be null, must not contain {@link #PATH_SEPARATOR})\n   * @param location this node's location \n   * @param parent this node's parent node\n   * @param level this node's level in the tree",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_save(java.io.Serializable,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)" : "* Save IOStatisticsSnapshot to a Hadoop filesystem as a JSON file.\n   * @param snapshot statistics\n   * @param fs filesystem\n   * @param path path\n   * @param overwrite should any existing file be overwritten?\n   * @throws UncheckedIOException Any IO exception.\n   * @throws UnsupportedOperationException if the IOStatistics classes were not found",
  "org.apache.hadoop.fs.shell.SetReplication:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.FileContext:setTimes(org.apache.hadoop.fs.Path,long,long)" : "* Set access time of a file.\n   * @param f The path\n   * @param mtime Set the modification time of this file.\n   *        The number of milliseconds since epoch (Jan 1, 1970). \n   *        A value of -1 means that this call should not set modification time.\n   * @param atime Set the access time of this file.\n   *        The number of milliseconds since Jan 1, 1970. \n   *        A value of -1 means that this call should not set access time.\n   *\n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If <code>f</code> does not exist\n   * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n   *           not supported\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server",
  "org.apache.hadoop.ipc.Server$Call:setDetailedMetricsName(java.lang.String)" : null,
  "org.apache.hadoop.fs.HarFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* not implemented.",
  "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:needsDictionary()" : "* Returns <code>false</code>.\n   *\n   * @return <code>false</code>.",
  "org.apache.hadoop.fs.BlockLocation:<init>(org.apache.hadoop.fs.BlockLocation)" : "* Copy constructor.\n   * @param that blocklocation.",
  "org.apache.hadoop.security.AnnotatedSecurityInfo:getTokenInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.compress.bzip2.CRC:updateCRC(int,int)" : null,
  "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getCoderOptions()" : "* Get a {@link ErasureCoderOptions}.\n   * @return erasure coder options",
  "org.apache.hadoop.fs.FileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)" : "* List a directory.\n   * The returned results include its block location if it is a file\n   * The results are filtered by the given path filter\n   * @param f a path\n   * @param filter a path filter\n   * @return an iterator that traverses statuses of the files/directories\n   *         in the given path\n   * @throws FileNotFoundException if <code>f</code> does not exist\n   * @throws IOException if any I/O error occurred",
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_getCurrent()" : "* Get the context's {@link IOStatisticsContext} which\n   * implements {@link IOStatisticsSource}.\n   * This is either a thread-local value or a global empty context.\n   * @return instance of {@link IOStatisticsContext}.",
  "org.apache.hadoop.fs.permission.AclEntry:hashCode()" : null,
  "org.apache.hadoop.log.LogThrottlingHelper:record(java.lang.String,long,double[])" : "* Record some set of values at the specified time into this helper. This can\n   * be useful to avoid fetching the current time twice if the caller has\n   * already done so for other purposes. This additionally allows the caller to\n   * specify a name for this recorder. When multiple names are used, one is\n   * denoted as the primary recorder. Only recorders named as the primary\n   * will trigger logging; other names not matching the primary can <i>only</i>\n   * be triggered by following the primary. This is used to coordinate multiple\n   * logging points. A primary can be set via the\n   * {@link #LogThrottlingHelper(long, String)} constructor. If no primary\n   * is set in the constructor, then the first recorder name used becomes the\n   * primary.\n   *\n   * If multiple names are used, they maintain entirely different sets of values\n   * and summary information. For example:\n   * <pre>{@code\n   *   // Initialize \"pre\" as the primary recorder name\n   *   LogThrottlingHelper helper = new LogThrottlingHelper(1000, \"pre\");\n   *   LogAction preLog = helper.record(\"pre\", Time.monotonicNow());\n   *   if (preLog.shouldLog()) {\n   *     // ...\n   *   }\n   *   double eventsProcessed = ... // perform some action\n   *   LogAction postLog =\n   *       helper.record(\"post\", Time.monotonicNow(), eventsProcessed);\n   *   if (postLog.shouldLog()) {\n   *     // ...\n   *     // Can use postLog.getStats(0) to access eventsProcessed information\n   *   }\n   * }</pre>\n   * Since \"pre\" is the primary recorder name, logging to \"pre\" will trigger a\n   * log action if enough time has elapsed. This will indicate that \"post\"\n   * should log as well. This ensures that \"post\" is always logged in the same\n   * iteration as \"pre\", yet each one is able to maintain its own summary\n   * information.\n   *\n   * <p>Other behavior is the same as {@link #record(double...)}.\n   *\n   * @param recorderName The name of the recorder. This is used to check if the\n   *                     current recorder is the primary. Other names are\n   *                     arbitrary and are only used to differentiate between\n   *                     distinct recorders.\n   * @param currentTimeMs The current time.\n   * @param values The values to log.\n   * @return The LogAction for the specified recorder.\n   *\n   * @see #record(double...)",
  "org.apache.hadoop.util.KMSUtil:checkNotNull(java.lang.Object,java.lang.String)" : null,
  "org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.conf.Configuration:getAlternativeNames(java.lang.String)" : "* Returns alternative names (non-deprecated keys or previously-set deprecated keys)\n   * for a given non-deprecated key.\n   * If the given key is deprecated, return null.\n   *\n   * @param name property name.\n   * @return alternative names.",
  "org.apache.hadoop.security.token.delegation.DelegationKey:getExpiryDate()" : null,
  "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:<init>(int)" : null,
  "org.apache.hadoop.fs.shell.Tail:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.ipc.Server$Call:getUserGroupInformation()" : null,
  "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String)" : null,
  "org.apache.hadoop.security.UserGroupInformation:isAuthenticationMethodEnabled(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:flush()" : null,
  "org.apache.hadoop.security.token.DtUtilShell$Edit:validate()" : null,
  "org.apache.hadoop.http.HttpServer2:addGlobalFilter(java.lang.String,java.lang.String,java.util.Map)" : null,
  "org.apache.hadoop.util.ExitUtil:terminate(int,java.lang.String)" : "* Terminate the current process. Note that terminate is the *only* method\n   * that should be used to terminate the daemon processes.\n   *\n   * @param status exit code\n   * @param msg message used to create the {@code ExitException}\n   * @throws ExitException if {@link System#exit(int)} is disabled.",
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:compareTo(org.apache.hadoop.fs.FileStatus)" : null,
  "org.apache.hadoop.net.unix.DomainSocketWatcher:remove(org.apache.hadoop.net.unix.DomainSocket)" : "* Remove a socket.  Its handler will be called.\n   *\n   * @param sock     The socket to remove.",
  "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:unpack(org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:setInput(byte[],int,int)" : null,
  "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:remove(java.lang.Object)" : null,
  "org.apache.hadoop.fs.FilterFs:isValidName(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.lib.MutableQuantiles:setEstimator(org.apache.hadoop.metrics2.util.QuantileEstimator)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)" : "* Return a FileStatus representing the given path. If the path refers\n   * to a symlink return a FileStatus representing the link rather than\n   * the object the link refers to.",
  "org.apache.hadoop.io.WritableUtils:readEnum(java.io.DataInput,java.lang.Class)" : "* Read an Enum value from DataInput, Enums are read and written \n   * using String values. \n   * @param <T> Enum type\n   * @param in DataInput to read from \n   * @param enumType Class type of Enum\n   * @return Enum represented by String read from DataInput\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.DoubleWritable:compareTo(org.apache.hadoop.io.DoubleWritable)" : null,
  "org.apache.hadoop.crypto.key.kms.ValueQueue:readLock(java.lang.String)" : null,
  "org.apache.hadoop.conf.StorageUnit$7:toBytes(double)" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyCompressor:finished()" : "* Returns true if the end of the compressed\n   * data output stream has been reached.\n   *\n   * @return <code>true</code> if the end of the compressed\n   *         data output stream has been reached.",
  "org.apache.hadoop.fs.impl.WeakReferenceThreadMap:setForCurrentThread(java.lang.Object)" : "* Set the new value for the current thread.\n   * @param newVal new reference to set for the active thread.\n   * @return the previously set value, possibly null",
  "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:calculateIV(byte[],long,byte[],int)" : null,
  "org.apache.hadoop.ipc.Server:getPriorityLevel(org.apache.hadoop.ipc.Schedulable)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:toString()" : null,
  "org.apache.hadoop.io.MapFile$Reader:comparator(org.apache.hadoop.io.WritableComparator)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:gaussianElimination(int[][])" : "* Perform Gaussian elimination on the given matrix. This matrix has to be a\n   * fat matrix (number of rows &gt; number of columns).\n   *\n   * @param matrix matrix.",
  "org.apache.hadoop.fs.FileStatus:<init>()" : null,
  "org.apache.hadoop.security.token.Token$TrivialRenewer:handleKind(org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String,java.lang.String)" : "* @return Returns the default (first) host name associated by the provided\n   * nameserver with the address bound to the specified network interface.\n   *\n   * @param strInterface\n   *            The name of the network interface to query (e.g. eth0)\n   * @param nameserver\n   *            The DNS host name\n   * @throws UnknownHostException\n   *             If one is encountered while querying the default interface",
  "org.apache.hadoop.fs.FilterFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:<init>(int,java.io.DataInput,org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator)" : "* For reading from file.\n     * \n     * @throws IOException",
  "org.apache.hadoop.fs.BlockLocation:setLength(long)" : "* Set the length of block.\n   * @param length length of block.",
  "org.apache.hadoop.util.LightWeightGSet:put(java.lang.Object)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:setInput(byte[],int,int)" : null,
  "org.apache.hadoop.ipc.ProtocolSignature:getSigFingerprint(java.lang.Class,long)" : "* Return a protocol's signature and finger print from cache\n   * \n   * @param protocol a protocol class\n   * @param serverVersion protocol version\n   * @return its signature and finger print",
  "org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)" : null,
  "org.apache.hadoop.fs.UnionStorageStatistics:getLongStatistics()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:readOnlyMountTable(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.ipc.ExternalCall:doResponse(java.lang.Throwable,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)" : null,
  "org.apache.hadoop.net.NetUtils:getHostPortString(java.net.InetSocketAddress)" : "* Compose a \"host:port\" string from the address.\n   *\n   * @param addr address.\n   * @return hort port string.",
  "org.apache.hadoop.security.authorize.AccessControlList:getAclString()" : "* Returns the access control list as a String that can be used for building a\n   * new instance by sending it to the constructor of {@link AccessControlList}.\n   * @return acl string.",
  "org.apache.hadoop.fs.RawLocalFileSystem:getUri()" : null,
  "org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration)" : "* Get the value of the home dir conf value for default mount table\n   * @param conf - from this conf\n   * @return home dir value, null if variable is not in conf",
  "org.apache.hadoop.util.Options$FSDataOutputStreamOption:<init>(org.apache.hadoop.fs.FSDataOutputStream)" : null,
  "org.apache.hadoop.io.MD5Hash:<init>(java.lang.String)" : "* Constructs an MD5Hash from a hex string.\n   * @param hex input hex.",
  "org.apache.hadoop.util.ChunkedArrayList:<init>(int,int)" : "* @param initialChunkCapacity the capacity of the first chunk to be\n   * allocated\n   * @param maxChunkSize the maximum size of any chunk allocated",
  "org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:release()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:readOnlyMountTable(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.conf.Configuration$ParsedTimeDuration:unitFor(java.lang.String)" : null,
  "org.apache.hadoop.security.Groups:getNegativeCache()" : null,
  "org.apache.hadoop.tools.TableListing:addRow(java.lang.String[])" : "* Add a new row.\n   *\n   * @param row    The row of objects to add-- one per column.",
  "org.apache.hadoop.ipc.CallQueueManager:<init>(java.lang.Class,java.lang.Class,boolean,int,java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.CloseableReferenceCount:isOpen()" : "* Return true if the status is currently open.\n   *\n   * @return                 True if the status is currently open.",
  "org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:setZooKeeperRef(org.apache.zookeeper.ZooKeeper)" : null,
  "org.apache.hadoop.fs.GetSpaceUsed$Builder:setInitialUsed(long)" : null,
  "org.apache.hadoop.fs.shell.find.Find:recursePath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$Writer$Option[])" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:getValidIndexes(java.lang.Object[])" : "* Picking up indexes of valid inputs.\n   * @param inputs decoding input buffers\n   * @param <T>",
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:blockSize(long)" : "* Set block size.",
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:getImpl()" : null,
  "org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.util.PureJavaCrc32C:<init>()" : "Create a new PureJavaCrc32 object.",
  "org.apache.hadoop.fs.impl.prefetch.BufferData:getBufferStr(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:flush()" : null,
  "org.apache.hadoop.fs.shell.Ls:isOrderReverse()" : "* Should directory contents be displayed in reverse order\n   * @return true reverse order, false default order",
  "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:isEmpty()" : null,
  "org.apache.hadoop.fs.ContentSummary:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.io.UTF8:lowSurrogate(int)" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:read()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hasCapability(java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)" : "* Get a scanner that covers a specific key range.\n     * \n     * @param beginKey\n     *          Begin key of the scan (inclusive). If null, scan from the first\n     *          key-value entry of the TFile.\n     * @param endKey\n     *          End key of the scan (exclusive). If null, scan up to the last\n     *          key-value entry of the TFile.\n     * @return The actual coverage of the returned scanner will cover all keys\n     *         greater than or equal to the beginKey and less than the endKey.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfInv(byte)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:getHAZookeeperConnectionState()" : null,
  "org.apache.hadoop.security.alias.KeyStoreProvider:getOutputStreamForKeystore()" : null,
  "org.apache.hadoop.fs.QuotaUsage:equals(java.lang.Object)" : null,
  "org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream)" : null,
  "org.apache.hadoop.util.JvmPauseMonitor$GcTimes:<init>(long,long)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateCounters(java.lang.Long,java.lang.Long)" : "* Aggregate two counters.\n   * @param l left value\n   * @param r right value\n   * @return the aggregate value",
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:needsInput()" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:end()" : null,
  "org.apache.hadoop.http.HttpServer2:defineFilter(org.eclipse.jetty.servlet.ServletContextHandler,java.lang.String,java.lang.String,java.util.Map,java.lang.String[])" : "* Define a filter for a context and set up default url mappings.\n   *\n   * @param ctx ctx.\n   * @param name name.\n   * @param classname classname.\n   * @param parameters parameters.\n   * @param urls urls.",
  "org.apache.hadoop.fs.BlockLocation:getHosts()" : "* Get the list of hosts (hostname) hosting this block.\n   * @return hosts array.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>()" : "* Creates an <code>DelegationTokenAuthenticatedURL</code>.\n   * <p>\n   * An instance of the default {@link DelegationTokenAuthenticator} will be\n   * used.",
  "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:reloadCachedMappings()" : null,
  "org.apache.hadoop.ipc.CallQueueManager:remainingCapacity()" : null,
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:getRemaining()" : null,
  "org.apache.hadoop.fs.FileSystem$DirectoryEntries:getToken()" : null,
  "org.apache.hadoop.security.SaslRpcServer$AuthMethod:valueOf(byte)" : "Return the object represented by the code.",
  "org.apache.hadoop.fs.FilterFileSystem:getInitialWorkingDirectory()" : null,
  "org.apache.hadoop.util.QuickSort:sort(org.apache.hadoop.util.IndexedSortable,int,int)" : "* Sort the given range of items using quick sort.\n   * {@inheritDoc} If the recursion depth falls below {@link #getMaxDepth},\n   * then switch to {@link HeapSort}.",
  "org.apache.hadoop.io.DataInputByteBuffer:<init>(org.apache.hadoop.io.DataInputByteBuffer$Buffer)" : null,
  "org.apache.hadoop.fs.ClusterStorageCapacityExceededException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem:getLength(org.apache.hadoop.fs.Path)" : "* The number of bytes in a file.\n   * @param f the path.\n   * @return the number of bytes; 0 for a directory\n   * @deprecated Use {@link #getFileStatus(Path)} instead.\n   * @throws FileNotFoundException if the path does not resolve\n   * @throws IOException IO failure",
  "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:genCauchyMatrix(byte[],int,int)" : "* Ported from Intel ISA-L library.\n   *\n   * @param k k.\n   * @param a a.\n   * @param m m.",
  "org.apache.hadoop.io.MapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)" : "* Deletes the named map file.\n   * @param fs input fs.\n   * @param name input name.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:close()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:getCanonicalServiceName()" : "* Get a canonical name for this file system.\n   * @return a URI string that uniquely identifies this file system",
  "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:prefetchOperationStarted()" : null,
  "org.apache.hadoop.security.SaslOutputStream:close()" : "* Closes this output stream and releases any system resources associated with\n   * this stream.\n   * \n   * @exception IOException\n   *              if an I/O error occurs.",
  "org.apache.hadoop.ipc.Server:channelWrite(java.nio.channels.WritableByteChannel,java.nio.ByteBuffer)" : "* This is a wrapper around {@link WritableByteChannel#write(ByteBuffer)}.\n   * If the amount of data is large, it writes to channel in smaller chunks. \n   * This is to avoid jdk from creating many direct buffers as the size of \n   * buffer increases. This also minimizes extra copies in NIO layer\n   * as a result of multiple write operations required to write a large \n   * buffer.  \n   *\n   * @see WritableByteChannel#write(ByteBuffer)",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultReplication()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : null,
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider:getAlgorithm(java.lang.String)" : "* Get the algorithm from the cipher.\n   *\n   * @return the algorithm name",
  "org.apache.hadoop.ipc.RemoteException:unwrapRemoteException()" : "* Instantiate and return the exception wrapped up by this remote exception.\n   * \n   * <p> This unwraps any <code>Throwable</code> that has a constructor taking\n   * a <code>String</code> as a parameter.\n   * Otherwise it returns this.\n   * \n   * @return <code>Throwable</code>",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_available()" : "* Are the ByteBufferPositionedReadable methods loaded?\n   * This does not check that a specific stream implements the API;\n   * use {@link #byteBufferPositionedReadable_readFullyAvailable(InputStream)}.\n   * @return true if the hadoop libraries have the method.",
  "org.apache.hadoop.util.LightWeightGSet:contains(java.lang.Object)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider:findProvider(java.util.List,java.lang.String)" : "* Find the provider with the given key.\n   *\n   * @param providerList the list of providers\n   * @param keyName the key name we are looking for.\n   * @return the KeyProvider that has the key\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.Lz4Codec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)" : "* Create a {@link CompressionOutputStream} that will write to the given\n   * {@link OutputStream} with the given {@link Compressor}.\n   *\n   * @param out        the location for the final output stream\n   * @param compressor compressor to use\n   * @return a stream the user can write uncompressed data to have it compressed\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.FSDataInputStream:getFileDescriptor()" : null,
  "org.apache.hadoop.util.Time:formatTime(long)" : "* Convert time in millisecond to human readable format.\n   *\n   * @param millis millisecond.\n   * @return a human readable string for the input time",
  "org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:hasNext()" : null,
  "org.apache.hadoop.http.HttpServer2:isInstrumentationAccessAllowed(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : "* Checks the user has privileges to access to instrumentation servlets.\n   * <p>\n   * If <code>hadoop.security.instrumentation.requires.admin</code> is set to FALSE\n   * (default value) it always returns TRUE.\n   * <p>\n   * If <code>hadoop.security.instrumentation.requires.admin</code> is set to TRUE\n   * it will check that if the current user is in the admin ACLS. If the user is\n   * in the admin ACLs it returns TRUE, otherwise it returns FALSE.\n   *\n   * @param servletContext the servlet context.\n   * @param request the servlet request.\n   * @param response the servlet response.\n   * @return TRUE/FALSE based on the logic decribed above.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.viewfs.ViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.conf.Configuration:getPassword(java.lang.String)" : "* Get the value for a known password configuration element.\n   * In order to enable the elimination of clear text passwords in config,\n   * this method attempts to resolve the property name as an alias through\n   * the CredentialProvider API and conditionally fallsback to config.\n   * @param name property name\n   * @return password\n   * @throws IOException when error in fetching password",
  "org.apache.hadoop.util.Shell:getSetOwnerCommand(java.lang.String)" : "* Return a command to set owner.\n   *\n   * @param owner owner.\n   * @return set owner command.",
  "org.apache.hadoop.conf.Configuration:getValByRegex(java.lang.String)" : "* get keys matching the the regex.\n   * @param regex the regex to match against.\n   * @return {@literal Map<String,String>} with matching keys",
  "org.apache.hadoop.util.HostsFileReader:lazyRefresh(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class,java.util.Map)" : "* Add an internal servlet in the server, with initialization parameters.\n   * Note: This method is to be used for adding servlets that facilitate\n   * internal communication and not for user facing functionality. For\n   * servlets added using this method, filters (except internal Kerberos\n   * filters) are not enabled.\n   *\n   * @param name The name of the servlet (can be passed as null)\n   * @param pathSpec The path spec for the servlet\n   * @param clazz The servlet class\n   * @param params init parameters",
  "org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)" : "* Construct a client-side proxy object that implements the named protocol,\n   * talking to a server at the named address. \n   * @param <T> Generics Type T.\n   * @param protocol input protocol.\n   * @param clientVersion input clientVersion.\n   * @param addr input addr.\n   * @param conf input Configuration.\n   * @param factory input factory.\n   * @throws IOException raised on errors performing I/O.\n   * @return proxy.",
  "org.apache.hadoop.security.UserGroupInformation:setLogin(javax.security.auth.login.LoginContext)" : null,
  "org.apache.hadoop.util.functional.CloseableTaskPoolSubmitter:close()" : "* Shut down the pool.",
  "org.apache.hadoop.security.token.Token:setKind(org.apache.hadoop.io.Text)" : "* Set the token kind. This is only intended to be used by services that\n   * wrap another service's token.\n   * @param newKind newKind.",
  "org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.security.SaslRpcClient:saslEvaluateToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto,boolean)" : "* Evaluate the server provided challenge.  The server must send a token\n   * if it's not done.  If the server is done, the challenge token is\n   * optional because not all mechanisms send a final token for the client to\n   * update its internal state.  The client must also be done after\n   * evaluating the optional token to ensure a malicious server doesn't\n   * prematurely end the negotiation with a phony success.\n   *  \n   * @param saslResponse - client response to challenge\n   * @param serverIsDone - server negotiation state\n   * @throws SaslException - any problems with negotiation",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setVerifyChecksum(boolean)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies:getWrappedRetriableException(java.lang.Exception)" : null,
  "org.apache.hadoop.tracing.Tracer:curThreadTracer()" : null,
  "org.apache.hadoop.security.WhitelistBasedResolver:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:getCipherSuite()" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:setInput(byte[],int,int)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:getComparatorName()" : "* Get the string representation of the comparator.\n     * \n     * @return If the TFile is not sorted by keys, an empty string will be\n     *         returned. Otherwise, the actual comparator string that is\n     *         provided during the TFile creation time will be returned.",
  "org.apache.hadoop.fs.RawLocalFileSystem:open(org.apache.hadoop.fs.PathHandle,int)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Decompressor(org.apache.hadoop.conf.Configuration)" : "* Return the appropriate implementation of the bzip2 decompressor. \n   * \n   * @param conf configuration\n   * @return the appropriate implementation of the bzip2 decompressor.",
  "org.apache.hadoop.security.AccessControlException:<init>()" : "* Default constructor is needed for unwrapping from \n   * {@link org.apache.hadoop.ipc.RemoteException}.",
  "org.apache.hadoop.conf.StorageUnit$7:getShortName()" : null,
  "org.apache.hadoop.fs.FileStatus:equals(java.lang.Object)" : "Compare if this object is equal to another object\n   * @param   o the object to be compared.\n   * @return  true if two file status has the same path name; false if not.",
  "org.apache.hadoop.conf.Configuration:getLong(java.lang.String,long)" : "* Get the value of the <code>name</code> property as a <code>long</code>.  \n   * If no such property exists, the provided default value is returned,\n   * or if the specified value is not a valid <code>long</code>,\n   * then an error is thrown.\n   * \n   * @param name property name.\n   * @param defaultValue default value.\n   * @throws NumberFormatException when the value is invalid\n   * @return property value as a <code>long</code>, \n   *         or <code>defaultValue</code>.",
  "org.apache.hadoop.security.token.Token:addBinaryBuffer(java.lang.StringBuilder,byte[])" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:cleanup()" : "* The default cleanup. Subclasses can override this with a custom\n       * cleanup.\n       * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.SetFile$Reader:seek(org.apache.hadoop.io.WritableComparable)" : null,
  "org.apache.hadoop.fs.impl.OpenFileParameters:withMandatoryKeys(java.util.Set)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isFile()" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:grow()" : null,
  "org.apache.hadoop.util.DurationInfo:close()" : null,
  "org.apache.hadoop.conf.StorageUnit$6:toPBs(double)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getEnclosingRoot(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(long,byte[],int,int)" : null,
  "org.apache.hadoop.util.ShutdownHookManager:clearShutdownHooks()" : "* clear all registered shutdownHooks.",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:truncate(org.apache.hadoop.fs.Path,long)" : null,
  "org.apache.hadoop.util.CacheableIPList:reset()" : "* Reloads the ip list",
  "org.apache.hadoop.util.ExitUtil:addSuppressed(java.lang.Throwable,java.lang.Throwable)" : "* Suppresses if legit and returns the first non-null of the two. Legit means\n   * <code>suppressor</code> if neither <code>null</code> nor <code>suppressed</code>.\n   * @param suppressor <code>Throwable</code> that suppresses <code>suppressed</code>\n   * @param suppressed <code>Throwable</code> that is suppressed by <code>suppressor</code>\n   * @return <code>suppressor</code> if not <code>null</code>, <code>suppressed</code> otherwise",
  "org.apache.hadoop.util.functional.FutureIO:awaitFuture(java.util.concurrent.Future)" : "* Given a future, evaluate it.\n   * <p>\n   * Any exception generated in the future is\n   * extracted and rethrown.\n   * </p>\n   * If this thread is interrupted while waiting for the future to complete,\n   * an {@code InterruptedIOException} is raised.\n   * However, if the future is cancelled, a {@code CancellationException}\n   * is raised in the {code Future.get()} call. This is\n   * passed up as is -so allowing the caller to distinguish between\n   * thread interruption (such as when speculative task execution is aborted)\n   * and future cancellation.\n   * @param future future to evaluate\n   * @param <T> type of the result.\n   * @return the result, if all went well.\n   * @throws InterruptedIOException waiting for future completion was interrupted\n   * @throws CancellationException if the future itself was cancelled\n   * @throws IOException if something went wrong\n   * @throws RuntimeException any nested RTE thrown",
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String,boolean)" : "* Create a mutable rate metric (for throughput measurement).\n   * @param name  of the metric\n   * @param desc  description\n   * @param extended  produce extended stat (stdev/min/max etc.) if true\n   * @return a new mutable rate metric object",
  "org.apache.hadoop.fs.permission.FsPermission:validateObject()" : null,
  "org.apache.hadoop.util.LightWeightGSet:iterator()" : null,
  "org.apache.hadoop.io.BooleanWritable:compareTo(org.apache.hadoop.io.BooleanWritable)" : "",
  "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:<init>(org.apache.hadoop.metrics2.MetricsCollector,org.apache.hadoop.metrics2.MetricsInfo,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,boolean)" : "* @param parent {@link MetricsCollector} using this record builder\n   * @param info metrics information\n   * @param rf\n   * @param mf\n   * @param acceptable",
  "org.apache.hadoop.util.LimitInputStream:skip(long)" : null,
  "org.apache.hadoop.net.SocksSocketFactory:createSocket()" : null,
  "org.apache.hadoop.fs.shell.FsCommand:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : "* Register the command classes used by the fs subcommand\n   * @param factory where to register the class",
  "org.apache.hadoop.fs.impl.prefetch.BlockData:getBlockNumber(long)" : "* Gets the id of the block that contains the given absolute offset.\n   * @param offset the absolute offset to check.\n   * @return the id of the block that contains the given absolute offset.\n   * @throws IllegalArgumentException if offset is invalid.",
  "org.apache.hadoop.fs.FsServerDefaults:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getIssueDate()" : null,
  "org.apache.hadoop.io.SequenceFile$Reader$BufferSizeOption:<init>(int)" : null,
  "org.apache.hadoop.fs.permission.FsPermission:applyUMask(org.apache.hadoop.fs.permission.FsPermission)" : "* Apply a umask to this permission and return a new one.\n   *\n   * The umask is used by create, mkdir, and other Hadoop filesystem operations.\n   * The mode argument for these operations is modified by removing the bits\n   * which are set in the umask.  Thus, the umask limits the permissions which\n   * newly created files and directories get.\n   *\n   * @param umask              The umask to use\n   * \n   * @return                   The effective permission",
  "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetOutputBuffers(java.nio.ByteBuffer[],int)" : "* Initialize the output buffers with ZERO bytes.",
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setReplication(org.apache.hadoop.fs.Path,short)" : null,
  "org.apache.hadoop.fs.store.audit.AuditingFunctions:callableWithinAuditSpan(org.apache.hadoop.fs.store.audit.AuditSpan,java.util.concurrent.Callable)" : "* Given a callable, return a new callable which\n   * activates and deactivates the span around the inner invocation.\n   * @param auditSpan audit span\n   * @param operation operation\n   * @param <T> type of result\n   * @return a new invocation.",
  "org.apache.hadoop.tools.CommandShell:setOut(java.io.PrintStream)" : null,
  "org.apache.hadoop.util.StringUtils:split(java.lang.String,char,char)" : "* Split a string using the given separator\n   * @param str a string that may have escaped separator\n   * @param escapeChar a char that be used to escape the separator\n   * @param separator a separator char\n   * @return an array of strings",
  "org.apache.hadoop.fs.Options$ChecksumOpt:getBytesPerChecksum()" : null,
  "org.apache.hadoop.security.UserGroupInformation:createUserForTesting(java.lang.String,java.lang.String[])" : "* Create a UGI for testing HDFS and MapReduce\n   * @param user the full user principal name\n   * @param userGroups the names of the groups that the user belongs to\n   * @return a fake user for running unit tests",
  "org.apache.hadoop.fs.PartialListing:toString()" : null,
  "org.apache.hadoop.io.compress.ZStandardCodec:getDecompressorType()" : "* Get the type of {@link Decompressor} needed by\n   * this {@link CompressionCodec}.\n   *\n   * @return the type of decompressor needed by this codec.",
  "org.apache.hadoop.metrics2.impl.SinkQueue:<init>(int)" : null,
  "org.apache.hadoop.fs.CompositeCrcFileChecksum:getChecksumOpt()" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:setNext(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry)" : null,
  "org.apache.hadoop.ipc.RpcClientUtil:methodToTraceString(java.lang.reflect.Method)" : "* Convert an RPC method to a string.\n   * The format we want is 'MethodOuterClassShortName#methodName'.\n   *\n   * For example, if the method is:\n   *   org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos.\n   *     ClientNamenodeProtocol.BlockingInterface.getServerDefaults\n   *\n   * the format we want is:\n   *   ClientNamenodeProtocol#getServerDefaults\n   * @param method input method.\n   * @return methodToTraceString.",
  "org.apache.hadoop.util.Options$IntegerOption:<init>(int)" : null,
  "org.apache.hadoop.fs.impl.FileRangeImpl:toString()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState)" : null,
  "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:getCanonicalServiceName()" : null,
  "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[],long,boolean)" : null,
  "org.apache.hadoop.security.alias.CredentialShell$Command:warnIfTransientProvider()" : null,
  "org.apache.hadoop.fs.DelegateToFileSystem:<init>(java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)" : null,
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_save(java.io.Serializable,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)" : "* Save IOStatisticsSnapshot to a Hadoop filesystem as a JSON file.\n   * @param snapshot statistics\n   * @param fs filesystem\n   * @param path path\n   * @param overwrite should any existing file be overwritten?\n   * @throws UncheckedIOException Any IO exception.",
  "org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSources()" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:waitForZKConnectionEvent(int)" : "* Waits for the next event from ZooKeeper to arrive.\n     * \n     * @param connectionTimeoutMs zookeeper connection timeout in milliseconds\n     * @throws KeeperException if the connection attempt times out. This will\n     * be a ZooKeeper ConnectionLoss exception code.\n     * @throws IOException if interrupted while connecting to ZooKeeper",
  "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)" : "* Get a protocol proxy that contains a proxy connection to a remote server\n   * and a set of methods that are supported by the server.\n   *\n   * @param <T> Generics Type T\n   * @param protocol protocol class\n   * @param clientVersion client's version\n   * @param connId client connection identifier\n   * @param conf configuration\n   * @param factory socket factory\n   * @param alignmentContext StateID alignment context\n   * @return the protocol proxy\n   * @throws IOException if the far end through a RemoteException",
  "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord)" : null,
  "org.apache.hadoop.io.compress.CodecPool:getLeaseCount(org.apache.hadoop.thirdparty.com.google.common.cache.LoadingCache,java.lang.Class)" : null,
  "org.apache.hadoop.security.authorize.ProxyServers:isProxyServer(java.lang.String)" : null,
  "org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedHeaders(javax.servlet.FilterConfig)" : null,
  "org.apache.hadoop.io.serializer.WritableSerialization:getDeserializer(java.lang.Class)" : null,
  "org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Path Constructor.\n     * @param fileSystem owner\n     * @param path path to open.",
  "org.apache.hadoop.fs.permission.PermissionStatus:<init>()" : null,
  "org.apache.hadoop.ipc.Client$Connection$PingInputStream:handleTimeout(java.net.SocketTimeoutException,int)" : null,
  "org.apache.hadoop.security.token.Token:encodeWritable(org.apache.hadoop.io.Writable)" : "* Generate a string with the url-quoted base64 encoded serialized form\n   * of the Writable.\n   * @param obj the object to serialize\n   * @return the encoded string\n   * @throws IOException",
  "org.apache.hadoop.security.UserGroupInformation:doAs(java.security.PrivilegedExceptionAction)" : "* Run the given action as the user, potentially throwing an exception.\n   * @param <T> the return type of the run method\n   * @param action the method to execute\n   * @return the value from the run method\n   * @throws IOException if the action throws an IOException\n   * @throws Error if the action throws an Error\n   * @throws RuntimeException if the action throws a RuntimeException\n   * @throws InterruptedException if the action throws an InterruptedException\n   * @throws UndeclaredThrowableException if the action throws something else",
  "org.apache.hadoop.fs.CachingGetSpaceUsed:init()" : null,
  "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:encrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.security.UserGroupInformation:newLoginContext(java.lang.String,javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration)" : null,
  "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:advance()" : null,
  "org.apache.hadoop.conf.Configuration:setBoolean(java.lang.String,boolean)" : "* Set the value of the <code>name</code> property to a <code>boolean</code>.\n   * \n   * @param name property name.\n   * @param value <code>boolean</code> value of the property.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : "* For subclasses externalizing the storage, for example Zookeeper\n   * based implementations\n   *\n   * @param ident ident.\n   * @return DelegationTokenInformation.",
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem:open(org.apache.hadoop.fs.Path,int)" : null,
  "org.apache.hadoop.fs.HarFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.io.InputStream,java.lang.String,java.io.InputStream)" : null,
  "org.apache.hadoop.util.GenericOptionsParser:matchesCurrentDirectory(java.lang.String)" : null,
  "org.apache.hadoop.fs.UnsupportedFileSystemException:<init>(java.lang.String)" : "* Constructs exception with the specified detail message. \n   * @param message exception message.",
  "org.apache.hadoop.util.bloom.Key:<init>()" : "default constructor - use with readFields",
  "org.apache.hadoop.metrics2.impl.SinkQueue:consumeAll(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer)" : "* Consume all the elements, will block if queue is empty\n   * @param consumer  the consumer callback object\n   * @throws InterruptedException",
  "org.apache.hadoop.http.HttpServer2$Builder:loadSSLConfiguration()" : "* Load SSL properties from the SSL configuration.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:syncTokenOwnerStats()" : "* This method syncs token information from currentTokens to tokenOwnerStats.\n   * It is used when the currentTokens is initialized or refreshed. This is\n   * called from a single thread thus no synchronization is needed.",
  "org.apache.hadoop.fs.FileUtil:permissionsFromMode(int)" : "* The permission operation of this method only involves users, user groups, and others.\n   * If SUID is set, only executable permissions are reserved.\n   * @param mode Permissions are represented by numerical values\n   * @return The original permissions for files are stored in collections",
  "org.apache.hadoop.fs.shell.CommandWithDestination:getTargetPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path,int)" : null,
  "org.apache.hadoop.util.StringInterner:weakIntern(java.lang.String)" : "* Interns and returns a reference to the representative instance \n   * for any of a collection of string instances that are equal to each other.\n   * Retains weak reference to the instance, \n   * and so does not prevent it from being garbage-collected.\n   * \n   * @param sample string instance to be interned\n   * @return weak reference to interned string instance",
  "org.apache.hadoop.util.GSetByHashMap:remove(java.lang.Object)" : null,
  "org.apache.hadoop.conf.Configuration:checkForOverride(java.util.Properties,java.lang.String,java.lang.String,java.lang.String)" : "* Print a warning if a property with a given name already exists with a\n   * different value.",
  "org.apache.hadoop.ha.ZKFailoverController:badArg(java.lang.String)" : null,
  "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)" : "Create the named map using the named key comparator.\n     * @param conf configuration.\n     * @param fs filesystem.\n     * @param dirName dirName.\n     * @param comparator comparator.\n     * @param valClass valClass.\n     * @param compress compress.\n     * @throws IOException raised on errors performing I/O.\n     * @deprecated Use Writer(Configuration, Path, Option...) instead.",
  "org.apache.hadoop.fs.TrashPolicyDefault:getTimeFromCheckpoint(java.lang.String)" : null,
  "org.apache.hadoop.conf.ReconfigurationUtil:parseChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint()" : null,
  "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getNodeForNetworkLocation(org.apache.hadoop.net.Node)" : null,
  "org.apache.hadoop.ipc.CallerContext:hashCode()" : null,
  "org.apache.hadoop.io.SecureIOUtils:insecureCreateForWrite(java.io.File,int)" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:nextRawValue(org.apache.hadoop.io.SequenceFile$ValueBytes)" : "* Read 'raw' values.\n     * @param val - The 'raw' value\n     * @return Returns the value length\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:getBytesRead()" : null,
  "org.apache.hadoop.security.token.DelegationTokenIssuer:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)" : "* NEVER call this method directly.\n   *\n   * @param issuer issuer.\n   * @param renewer renewer.\n   * @param credentials cache in which to add new delegation tokens.\n   * @param tokens list of new delegation tokens.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.token.DtUtilShell$Get:getUsage()" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createAuthenticatedURL()" : null,
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:checkEOF()" : "* Check whether we reach the end of the stream.\n     * \n     * @return false if the chunk encoded stream has more data to read (in which\n     *         case available() will be greater than 0); true otherwise.\n     * @throws java.io.IOException\n     *           on I/O errors.",
  "org.apache.hadoop.crypto.CryptoInputStream:setDropBehind(java.lang.Boolean)" : null,
  "org.apache.hadoop.ipc.Client$Call:callComplete()" : "Indicate when the call is complete and the\n     * value or error are available.  Notifies by default.",
  "org.apache.hadoop.fs.UnresolvedLinkException:<init>()" : null,
  "org.apache.hadoop.fs.viewfs.HCFSMountTableConfigLoader:logInvalidFileNameFormat(java.lang.String)" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:createRootDirRecursively(java.lang.String)" : "* Utility function to ensure that the configured base znode exists.\n   * This recursively creates the znode as well as all of its parents.\n   * @param path Path of the znode to create.\n   * @throws Exception If it cannot create the file.",
  "org.apache.hadoop.fs.FilterFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller:<init>(org.apache.hadoop.metrics2.lib.MutableRollingAverages)" : null,
  "org.apache.hadoop.io.compress.DefaultCodec:getCompressorType()" : null,
  "org.apache.hadoop.ipc.ProxyCombiner:<init>()" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:<init>()" : "* Creates a new decompressor with the default buffer size.",
  "org.apache.hadoop.fs.ftp.FTPFileSystem:isParentOf(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Probe for a path being a parent of another\n   * @param parent parent path\n   * @param child possible child path\n   * @return true if the parent's path matches the start of the child's",
  "org.apache.hadoop.ipc.DecayRpcScheduler:getDecayedCallCosts()" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)" : null,
  "org.apache.hadoop.ha.BadFencingConfigurationException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.fs.sftp.SFTPInputStream:seek(long)" : null,
  "org.apache.hadoop.ipc.ExternalCall:get()" : null,
  "org.apache.hadoop.io.retry.AsyncCallHandler:hasSuccessfulCall()" : null,
  "org.apache.hadoop.io.MapFile$Writer:getIndexInterval()" : "* The number of entries that are added before an index entry is added.\n     * @return indexInterval",
  "org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:resetBuffers(java.nio.ByteBuffer[])" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:getCompressionCodec()" : "* Returns the compression codec of data in this file.\n     * @return CompressionCodec.",
  "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.util.CombinedIPWhiteList:<init>(java.lang.String,java.lang.String,long)" : null,
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)" : "* Set the working directory to the given directory.",
  "org.apache.hadoop.io.ElasticByteBufferPool:size(boolean)" : "* Get the size of the buffer pool, for the specified buffer type.\n   *\n   * @param direct Whether the size is returned for direct buffers\n   * @return The size",
  "org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.util.Map)" : "* Constructor with schema name and provided all options. Note the options may\n   * contain additional information for the erasure codec to interpret further.\n   * @param allOptions all schema options",
  "org.apache.hadoop.util.Progress:addPhase(java.lang.String,float)" : "* Adds a named node with a specified progress weightage to the tree.\n   *\n   * @param status status.\n   * @param weightage weightage.\n   * @return Progress.",
  "org.apache.hadoop.io.DataOutputBuffer$Buffer:write(java.io.DataInput,int)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockData:<init>(long,int)" : "* Constructs an instance of {@link BlockData}.\n   * @param fileSize the size of a file.\n   * @param blockSize the file is divided into blocks of this size.\n   * @throws IllegalArgumentException if fileSize is negative.\n   * @throws IllegalArgumentException if blockSize is negative.\n   * @throws IllegalArgumentException if blockSize is zero or negative.",
  "org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)" : "* Create the named file for values of the named class.\n     *\n     * @param conf configuration.\n     * @param fs file system.\n     * @param file file.\n     * @param valClass valClass.\n     * @param compress compress.\n     * @param progress progress.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.CachingGetSpaceUsed:getDirPath()" : "* @return The directory path being monitored.",
  "org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.PathHandle,int)" : "* Open an FSDataInputStream matching the PathHandle instance. The\n   * implementation may encode metadata in PathHandle to address the\n   * resource directly and verify that the resource referenced\n   * satisfies constraints specified at its construciton.\n   * @param fd PathHandle object returned by the FS authority.\n   * @param bufferSize the size of the buffer to use\n   * @throws InvalidPathHandleException If {@link PathHandle} constraints are\n   *                                    not satisfied\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException If not overridden by subclass\n   * @return input stream.",
  "org.apache.hadoop.ipc.RetryCache$CacheEntry:<init>(byte[],int,long)" : null,
  "org.apache.hadoop.fs.PathIOException:getTargetPath()" : "@return Path if the operation involved copying or moving, else null",
  "org.apache.hadoop.crypto.CryptoInputStream:decrypt(java.nio.ByteBuffer,int,int)" : "* Decrypts the given {@link ByteBuffer} in place. {@code length} bytes are\n   * decrypted from {@code buf} starting at {@code start}.\n   * {@code buf.position()} and {@code buf.limit()} are unchanged after this\n   * method returns.\n   *\n   * @see #decrypt(long, ByteBuffer, int, int)",
  "org.apache.hadoop.ipc.Client$Connection:setFallBackToSimpleAuth(java.util.concurrent.atomic.AtomicBoolean)" : null,
  "org.apache.hadoop.io.BooleanWritable:toString()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockManager:get(int)" : "* Gets the block having the given {@code blockNumber}.\n   *\n   * The entire block is read into memory and returned as a {@code BufferData}.\n   * The blocks are treated as a limited resource and must be released when\n   * one is done reading them.\n   *\n   * @param blockNumber the number of the block to be read and returned.\n   * @return {@code BufferData} having data from the given block.\n   *\n   * @throws IOException if there an error reading the given block.\n   * @throws IllegalArgumentException if blockNumber is negative.",
  "org.apache.hadoop.util.IntrusiveCollection:addFirst(org.apache.hadoop.util.IntrusiveCollection$Element)" : "* Add an element to the front of the list.\n   *\n   * @param elem     The new element to add.\n   * @return if addFirst success true, not false.",
  "org.apache.hadoop.fs.shell.Ls:isSorted()" : null,
  "org.apache.hadoop.fs.statistics.MeanStatistic:getSamples()" : "* Get the sample count.\n   * @return the sample count; 0 means empty",
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.io.SequenceFile$Metadata:<init>()" : null,
  "org.apache.hadoop.util.MachineList:getCollection()" : "* returns the contents of the MachineList as a Collection&lt;String&gt; .\n   * This can be used for testing .\n   *\n   * @return contents of the MachineList.",
  "org.apache.hadoop.fs.viewfs.NotInMountpointException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:createTokenInfo(byte[])" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMinimums(java.lang.Long,java.lang.Long)" : "* Aggregate two minimum values.\n   * @param l left\n   * @param r right\n   * @return the new minimum.",
  "org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.lang.String,java.util.Map)" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:getRemaining()" : "* Returns <code>0</code>.\n   *\n   * @return <code>0</code>.",
  "org.apache.hadoop.crypto.random.OsSecureRandom:close()" : null,
  "org.apache.hadoop.io.ObjectWritable:writeObject(java.io.DataOutput,java.lang.Object,java.lang.Class,org.apache.hadoop.conf.Configuration,boolean)" : "* Write a {@link Writable}, {@link String}, primitive type, or an array of\n     * the preceding.  \n     * \n     * @param allowCompactArrays - set true for RPC and internal or intra-cluster\n     * usages.  Set false for inter-cluster, File, and other persisted output \n     * usages, to preserve the ability to interchange files with other clusters \n     * that may not be running the same version of software.  Sometime in ~2013 \n     * we can consider removing this parameter and always using the compact format.\n     *\n     * @param conf configuration.\n     * @param out dataoutput.\n     * @param declaredClass declaredClass.\n     * @param instance instance.\n     * @throws IOException raised on errors performing I/O.\n     *",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getHomeDirectory()" : null,
  "org.apache.hadoop.security.UserGroupInformation$UgiMetrics:addGetGroups(long)" : null,
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.http.HttpsFileSystem:getWorkingDirectory()" : null,
  "org.apache.hadoop.fs.shell.Head:expandArgument(java.lang.String)" : null,
  "org.apache.hadoop.fs.Options$CreateOpts$Progress:<init>(org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.fs.shell.Command:processPathArgument(org.apache.hadoop.fs.shell.PathData)" : "*  This is the last chance to modify an argument before going into the\n   *  (possibly) recursive {@link #processPaths(PathData, PathData...)}\n   *  {@literal ->} {@link #processPath(PathData)} loop.  Ex.  ls and du use\n   *  this to expand out directories.\n   *  @param item a {@link PathData} representing a path which exists\n   *  @throws IOException if anything goes wrong...",
  "org.apache.hadoop.fs.Options$CreateOpts$CreateParent:<init>(boolean)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:getTargetLink()" : "* Get the target of the link. If a merge link then it returned\n     * as \",\" separated URI list.\n     *\n     * @return the path.",
  "org.apache.hadoop.fs.TrashPolicy:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)" : "* Used to setup the trash policy. Must be implemented by all TrashPolicy\n   * implementations. Different from initialize(conf, fs, home), this one does\n   * not assume trash always under /user/$USER due to HDFS encryption zone.\n   * @param conf the configuration to be used\n   * @param fs the filesystem to be used",
  "org.apache.hadoop.fs.FileSystem$Statistics$4:<init>(org.apache.hadoop.fs.FileSystem$Statistics)" : "* Copy constructor.\n     *\n     * @param other    The input Statistics object which is cloned.",
  "org.apache.hadoop.util.HostsFileReader:finishRefresh()" : null,
  "org.apache.hadoop.fs.FsShell:printInstanceUsage(java.io.PrintStream,org.apache.hadoop.fs.shell.Command)" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(long,java.lang.String,long,long)" : "* Validates that the given value is within the given range of values.\n   * @param value the value to check.\n   * @param valueName the name of the argument.\n   * @param minValueInclusive inclusive lower limit for the value.\n   * @param maxValueInclusive inclusive upper limit for the value.",
  "org.apache.hadoop.fs.viewfs.RegexMountPoint:getVarListInString(java.lang.String)" : "* Get $var1 and $var2 style variables in string.\n   *\n   * @param input - the string to be process.\n   * @return",
  "org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:next()" : "* Return the next value.\n     * Will retrieve the next elements if needed.\n     * This is where the mapper takes place.\n     * @return true if there is another data element.\n     * @throws IOException failure in fetch operation or the transformation.\n     * @throws NoSuchElementException no more data",
  "org.apache.hadoop.fs.DelegationTokenRenewer:<init>(java.lang.Class)" : null,
  "org.apache.hadoop.fs.FileUtil:unZip(java.io.InputStream,java.io.File)" : "* Given a stream input it will unzip the it in the unzip directory.\n   * passed as the second parameter\n   * @param inputStream The zip file as input\n   * @param toDir The unzip directory where to unzip the zip file.\n   * @throws IOException an exception occurred",
  "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:toString()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:getUriPath(org.apache.hadoop.fs.Path)" : "* Get the path-part of a pathname. Checks that URI matches this file system\n   * and that the path-part is a valid name.\n   * \n   * @param p path\n   * \n   * @return path-part of the Path p",
  "org.apache.hadoop.fs.shell.find.Find:getExpression(java.lang.String)" : "Gets a named expression from the factory.",
  "org.apache.hadoop.tracing.Span:addTimelineAnnotation(java.lang.String)" : null,
  "org.apache.hadoop.io.WritableComparator:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:hashCode()" : null,
  "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:getDefaultCipherSuites()" : null,
  "org.apache.hadoop.util.Shell:appendScriptExtension(java.io.File,java.lang.String)" : "* Returns a File referencing a script with the given basename, inside the\n   * given parent directory.  The file extension is inferred by platform:\n   * <code>\".cmd\"</code> on Windows, or <code>\".sh\"</code> otherwise.\n   *\n   * @param parent File parent directory\n   * @param basename String script file basename\n   * @return File referencing the script in the directory",
  "org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:readNextRpcPacket()" : null,
  "org.apache.hadoop.tracing.TraceScope:addTimelineAnnotation(java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:revertFromOld(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.crypto.CryptoOutputStream:write(int)" : null,
  "org.apache.hadoop.security.SecurityUtil:getKerberosInfo(java.lang.Class,org.apache.hadoop.conf.Configuration)" : "* Look up the KerberosInfo for a given protocol. It searches all known\n   * SecurityInfo providers.\n   * @param protocol the protocol class to get the information for\n   * @param conf configuration object\n   * @return the KerberosInfo or null if it has no KerberosInfo defined",
  "org.apache.hadoop.fs.ChecksumFs:getSumBufferSize(int,int,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)" : "* Merges the contents of files passed in Path[] using a max factor value\n     * that is already set\n     * @param inNames the array of path names\n     * @param deleteInputs true if the input files should be deleted when \n     * unnecessary\n     * @param tmpDir the directory to write temporary files into\n     * @return RawKeyValueIteratorMergeQueue\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:<init>(int,long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.metrics2.lib.MutableGaugeLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:cleanup()" : "The default cleanup. Subclasses can override this with a custom \n       * cleanup",
  "org.apache.hadoop.ipc.ProtobufHelper:getFixedByteString(org.apache.hadoop.io.Text)" : "* Get the ByteString for frequently used fixed and small set strings.\n   * @param key string\n   * @return the ByteString for frequently used fixed and small set strings.",
  "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:getRegion()" : null,
  "org.apache.hadoop.io.SortedMapWritable:containsValue(java.lang.Object)" : null,
  "org.apache.hadoop.service.ServiceStateModel:isInState(org.apache.hadoop.service.Service$STATE)" : "* Query that the state is in a specific state\n   * @param proposed proposed new state\n   * @return the state",
  "org.apache.hadoop.fs.GlobExpander:expand(java.lang.String)" : "* Expand globs in the given <code>filePattern</code> into a collection of\n   * file patterns so that in the expanded set no file pattern has a slash\n   * character (\"/\") in a curly bracket pair.\n   * <p>\n   * Some examples of how the filePattern is expanded:<br>\n   * <pre>\n   * <b>\n   * filePattern         - Expanded file pattern </b>\n   * {a/b}               - a/b\n   * /}{a/b}             - /}a/b\n   * p{a/b,c/d}s         - pa/bs, pc/ds\n   * {a/b,c/d,{e,f}}     - a/b, c/d, {e,f}\n   * {a/b,c/d}{e,f}      - a/b{e,f}, c/d{e,f}\n   * {a,b}/{b,{c/d,e/f}} - {a,b}/b, {a,b}/c/d, {a,b}/e/f\n   * {a,b}/{c/\\d}        - {a,b}/c/d\n   * </pre>\n   * \n   * @param filePattern file pattern.\n   * @return expanded file patterns\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)" : null,
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:readFrom(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.io.ArrayFile:<init>()" : null,
  "org.apache.hadoop.fs.ContentSummary$Builder:typeConsumed(org.apache.hadoop.fs.StorageType,long)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$CachedUid:<init>(java.lang.String,long)" : null,
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:close()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:rollMasterKey()" : "* Update the current master key for generating delegation tokens \n   * It should be called only by tokenRemoverThread.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.StringUtils:escapeString(java.lang.String)" : "* Escape commas in the string using the default escape char\n   * @param str a string\n   * @return an escaped string",
  "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],org.apache.hadoop.fs.StorageType[],long,long,boolean)" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction:create(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode)" : null,
  "org.apache.hadoop.fs.WindowsGetSpaceUsed:refresh()" : "* Override to hook in DUHelper class.",
  "org.apache.hadoop.util.JsonSerialization:load(java.io.File)" : "* Load from a JSON text file.\n   * @param jsonFile input file\n   * @return the parsed JSON\n   * @throws IOException IO problems\n   * @throws JsonParseException If the input is not well-formatted\n   * @throws JsonMappingException failure to map from the JSON to this class",
  "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:setThreadCount(java.lang.String)" : "* set thread count by option value, if the value less than 1,\n   * use 1 instead.\n   *\n   * @param optValue option value",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)" : "* Renews a delegation token from the server end-point using the\n   * configured <code>Authenticator</code> for authentication.\n   *\n   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are\n   * supported.\n   * @param token the authentication token with the Delegation Token to renew.\n   * @throws IOException if an IO error occurred.\n   * @throws AuthenticationException if an authentication exception occurred.\n   * @return delegation token long value.",
  "org.apache.hadoop.fs.RawLocalFileSystem:createOutputStream(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getCounterReference(java.lang.String)" : null,
  "org.apache.hadoop.fs.impl.FileRangeImpl:setData(java.util.concurrent.CompletableFuture)" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:checkStream()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:initTokenManager(java.util.Properties)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:rewind()" : "* Rewind to the first entry in the scanner. The entry returned by the\n       * previous entry() call will be invalid.\n       * \n       * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.CompressedWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.statistics.impl.StubDurationTrackerFactory:<init>()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:closeChildFileSystems(org.apache.hadoop.fs.FileSystem)" : null,
  "org.apache.hadoop.util.hash.MurmurHash:hash(byte[],int,int)" : null,
  "org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String)" : null,
  "org.apache.hadoop.crypto.OpensslCipher:getInstance(java.lang.String,java.lang.String)" : "* Return an <code>OpensslCipher</code> object that implements the specified\n   * transformation.\n   * \n   * @param transformation the name of the transformation, e.g., \n   * AES/CTR/NoPadding.\n   * @param engineId the openssl engine to use.if not set,\n   * defalut engine will be used.\n   * @return OpensslCipher an <code>OpensslCipher</code> object\n   * @throws NoSuchAlgorithmException if <code>transformation</code> is null, \n   * empty, in an invalid format, or if Openssl doesn't implement the \n   * specified algorithm.\n   * @throws NoSuchPaddingException if <code>transformation</code> contains \n   * a padding scheme that is not available.",
  "org.apache.hadoop.io.SortedMapWritable:values()" : null,
  "org.apache.hadoop.fs.LocalFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.BatchedRemoteIterator:<init>(java.lang.Object)" : null,
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:setAbsolute(long)" : "* If the given {@code pos} lies within the current buffer, updates the current position to\n   * the specified value and returns true; otherwise returns false without changing the position.\n   *\n   * @param pos the absolute position to change the current position to if possible.\n   * @return true if the given current position was updated, false otherwise.",
  "org.apache.hadoop.io.Text:<init>(org.apache.hadoop.io.Text)" : "* Construct from another text.\n   * @param utf8 input utf8.",
  "org.apache.hadoop.util.hash.JenkinsHash:main(java.lang.String[])" : "* Compute the hash of the specified file\n   * @param args name of file to compute hash of.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.conf.StorageUnit$6:getSuffixChar()" : null,
  "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:<init>(java.lang.Object[])" : "* Create a table with headers\n     * @param headers list of headers",
  "org.apache.hadoop.net.unix.DomainSocketWatcher$Entry:getDomainSocket()" : null,
  "org.apache.hadoop.io.DoubleWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.util.WeakReferenceMap:<init>(java.util.function.Function,java.util.function.Consumer)" : "* instantiate.\n   * @param factory supplier of new instances\n   * @param referenceLost optional callback on lost references.",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getAndMoveToFrontDecode0(int)" : null,
  "org.apache.hadoop.util.functional.FutureIO:awaitAllFutures(java.util.Collection)" : "* Evaluates a collection of futures and returns their results as a list.\n   * <p>\n   * This method blocks until all futures in the collection have completed.\n   * If any future throws an exception during its execution, this method\n   * extracts and rethrows that exception.\n   * </p>\n   * @param collection collection of futures to be evaluated\n   * @param <T> type of the result.\n   * @return the list of future's result, if all went well.\n   * @throws InterruptedIOException waiting for future completion was interrupted\n   * @throws CancellationException if the future itself was cancelled\n   * @throws IOException if something went wrong\n   * @throws RuntimeException any nested RTE thrown",
  "org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:skip(long)" : "* Skips over and discards <code>n</code> bytes of data from the\n     * input stream.\n     *\n     *The <code>skip</code> method skips over some smaller number of bytes\n     * when reaching end of file before <code>n</code> bytes have been skipped.\n     * The actual number of bytes skipped is returned.  If <code>n</code> is\n     * negative, no bytes are skipped.\n     *\n     * @param      n   the number of bytes to be skipped.\n     * @return     the actual number of bytes skipped.\n     * @exception  IOException  if an I/O error occurs.\n     *             ChecksumException if the chunk to skip to is corrupted",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getConfiguration(java.lang.String,javax.servlet.FilterConfig)" : "* It delegates to\n   * {@link AuthenticationFilter#getConfiguration(String, FilterConfig)} and\n   * then overrides the {@link AuthenticationHandler} to use if authentication\n   * type is set to <code>simple</code> or <code>kerberos</code> in order to use\n   * the corresponding implementation with delegation token support.\n   *\n   * @param configPrefix parameter not used.\n   * @param filterConfig parameter not used.\n   * @return hadoop-auth de-prefixed configuration for the filter and handler.",
  "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)" : "* Copy files between FileSystems.\n   * @param srcFS src fs.\n   * @param src src.\n   * @param dstFS dst fs.\n   * @param dst dst.\n   * @param deleteSource delete source.\n   * @param conf configuration.\n   * @return if copy success true, not false.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:isBadorWrongPassword(java.io.IOException)" : null,
  "org.apache.hadoop.util.Time:now()" : "* Current system time.  Do not use this to calculate a duration or interval\n   * to sleep, because it will be broken by settimeofday.  Instead, use\n   * monotonicNow.\n   * @return current time in msec.",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:<init>()" : null,
  "org.apache.hadoop.jmx.JMXJsonServlet:extraWrite(java.lang.Object,java.lang.String,com.fasterxml.jackson.core.JsonGenerator)" : null,
  "org.apache.hadoop.net.AbstractDNSToSwitchMapping:getConf()" : null,
  "org.apache.hadoop.ipc.Server$Call:<init>(int,int,org.apache.hadoop.ipc.RPC$RpcKind,byte[])" : null,
  "org.apache.hadoop.conf.Configuration$IntegerRanges:toString()" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:getTag(java.lang.String)" : "* Get a tag by name\n   * @param name  of the tag\n   * @return the tag object",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Compressor(org.apache.hadoop.conf.Configuration)" : "* Return the appropriate implementation of the bzip2 compressor. \n   * \n   * @param conf configuration\n   * @return the appropriate implementation of the bzip2 compressor.",
  "org.apache.hadoop.util.InstrumentedLock:lock()" : null,
  "org.apache.hadoop.conf.Configuration$DeprecationDelta:getCustomMessage()" : null,
  "org.apache.hadoop.conf.Configuration$ParsedTimeDuration$5:unit()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:setSymlink(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ha.ZKFailoverController:becomeStandby()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.ReflectionUtils:clearCache()" : null,
  "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:getOffset()" : null,
  "org.apache.hadoop.crypto.key.KeyProvider:getBaseName(java.lang.String)" : "* Split the versionName in to a base name. Converts \"/aaa/bbb@3\" to\n   * \"/aaa/bbb\".\n   * @param versionName the version name to split\n   * @return the base name of the key\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.Credentials:getAllTokens()" : "* Return all the tokens in the in-memory map.\n   *\n   * @return all the tokens in the in-memory map.",
  "org.apache.hadoop.io.compress.zlib.BuiltInZlibInflater:decompress(byte[],int,int)" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:ctorImpl(java.lang.Class,java.lang.Class[])" : null,
  "org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:available()" : null,
  "org.apache.hadoop.io.retry.MultiException:toString()" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:getSchedulingDecisionSummary()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:startMetricsMBeans()" : null,
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:<init>(long,long)" : "* Create an instance for testing.\n   *\n   * @param flushIntervalMillis the roll interval in millis\n   * @param flushOffsetIntervalMillis the roll offset interval in millis",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:getHomeDirectory()" : null,
  "org.apache.hadoop.tracing.Tracer:newSpan(java.lang.String,org.apache.hadoop.tracing.SpanContext)" : null,
  "org.apache.hadoop.io.DataOutputBuffer:reset()" : "* Resets the buffer to empty.\n   * @return DataOutputBuffer.",
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,int,java.nio.ByteBuffer[],java.nio.ByteBuffer[])" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$INode:getLink()" : "* Return the link if isLink.\n     * @return will return null, for non links.",
  "org.apache.hadoop.fs.BulkDeleteUtils:validateBulkDeletePaths(java.util.Collection,int,org.apache.hadoop.fs.Path)" : "* Preconditions for bulk delete paths.\n   * @param paths paths to delete.\n   * @param pageSize maximum number of paths to delete in a single operation.\n   * @param basePath base path for the delete operation.",
  "org.apache.hadoop.fs.FsShell:printInfo(java.io.PrintStream,java.lang.String,boolean)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies:retryOtherThanRemoteAndSaslException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)" : "* <p>\n   * A retry policy where RemoteException and SaslException are not retried, other individual\n   * exception types can have RetryPolicy overrides, and any other exception type without an\n   * override is not retried.\n   * </p>\n   *\n   * @param defaultPolicy defaultPolicy.\n   * @param exceptionToPolicyMap exceptionToPolicyMap.\n   * @return RetryPolicy.",
  "org.apache.hadoop.metrics2.lib.MutableCounterLong:value()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDuration(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE)" : "* Given an IOException raising callable/lambda expression,\n   * execute it and update the relevant statistic.\n   * @param factory factory of duration trackers\n   * @param statistic statistic key\n   * @param input input callable.\n   * @param <B> return type.\n   * @return the result of the operation.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.SequenceFile$BlockCompressWriter:append(java.lang.Object,java.lang.Object)" : "Append a key/value pair.",
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:monitorHealth(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$MonitorHealthRequestProto)" : null,
  "org.apache.hadoop.io.DataInputBuffer:getPosition()" : "* Returns the current position in the input.\n   *\n   * @return position.",
  "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getAlgorithmName()" : null,
  "org.apache.hadoop.metrics2.util.SampleQuantiles$SampleItem:<init>(long,int,int)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider$Options:toString()" : null,
  "org.apache.hadoop.util.SequentialNumber:equals(java.lang.Object)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)" : "* Renew a delegation token.\n   * @param token the token to renew\n   * @param renewer the full principal name of the user doing the renewal\n   * @return the new expiration time\n   * @throws InvalidToken if the token is invalid\n   * @throws AccessControlException if the user can't renew token",
  "org.apache.hadoop.io.EnumSetWritable:hashCode()" : null,
  "org.apache.hadoop.security.Groups:cacheGroupsAdd(java.util.List)" : "* Add groups to cache\n   *\n   * @param groups list of groups to add to cache",
  "org.apache.hadoop.conf.ConfigurationWithLogging:getFloat(java.lang.String,float)" : "* See {@link Configuration#getFloat(String, float)}.",
  "org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object,java.util.function.Supplier)" : "* Preconditions that the specified argument is not {@code null},\n   * throwing a NPE exception otherwise.\n   *\n   * <p>The message of the exception is {@code msgSupplier.get()}.</p>\n   *\n   * @param <T> the object type\n   * @param obj  the object to check\n   * @param msgSupplier  the {@link Supplier#get()} set the\n   *                 exception message if valid. Otherwise,\n   *                 the message is {@link #VALIDATE_IS_NOT_NULL_EX_MESSAGE}\n   * @return the validated object (never {@code null} for method chaining)\n   * @throws NullPointerException if the object is {@code null}",
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:<init>(org.apache.hadoop.crypto.key.KeyProvider)" : null,
  "org.apache.hadoop.security.token.DtUtilShell$Renew:validate()" : null,
  "org.apache.hadoop.fs.FileContext:getServerDefaults(org.apache.hadoop.fs.Path)" : "* Return a set of server default configuration values based on path.\n   * @param path path to fetch server defaults\n   * @return server default configuration values for path\n   * @throws IOException an I/O error occurred",
  "org.apache.hadoop.io.compress.DefaultCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)" : null,
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:warmUpEncryptedKeys(java.lang.String[])" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.EmptyStorageStatistics:reset()" : null,
  "org.apache.hadoop.security.User:getLastLogin()" : "* Get the time of the last login.\n   * @return the number of milliseconds since the beginning of time.",
  "org.apache.hadoop.security.UserGroupInformation:toString()" : "* Return the username.",
  "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:<init>(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode)" : null,
  "org.apache.hadoop.fs.Trash:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)" : "* Construct a trash can accessor for the FileSystem provided.\n   * @param fs the FileSystem\n   * @param conf a Configuration\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.http.NoCacheFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)" : null,
  "org.apache.hadoop.io.ArrayFile$Reader:key()" : "* Returns the key associated with the most recent call to {@link\n     * #seek(long)}, {@link #next(Writable)}, or {@link\n     * #get(long,Writable)}.\n     *\n     * @return key key.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<init>(java.nio.file.Path,java.util.function.Consumer,java.util.function.Consumer)" : "* See {@link #FileMonitoringTimerTask(List, Consumer, Consumer)}.\n   *\n   * @param filePath The file to monitor.\n   * @param onFileChange What to do when the file changes.\n   * @param onChangeFailure What to do when <code>onFileChange</code>\n   *                        throws an exception.",
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:checkTagName(java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:getOffset()" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:writeBooleanArray(java.io.DataOutput)" : null,
  "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.InetAddress,int)" : null,
  "org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClasses(org.apache.hadoop.conf.Configuration)" : "* Get the list of codecs discovered via a Java ServiceLoader, or\n   * listed in the configuration. Codecs specified in configuration come\n   * later in the returned list, and are considered to override those\n   * from the ServiceLoader.\n   * @param conf the configuration to look in\n   * @return a list of the {@link CompressionCodec} classes",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetBit()" : null,
  "org.apache.hadoop.metrics2.AbstractMetric:equals(java.lang.Object)" : null,
  "org.apache.hadoop.conf.ReconfigurationServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.security.KDiag:validateJAAS(boolean)" : "* Validate any JAAS entry referenced in the {@link #SUN_SECURITY_JAAS_FILE}\n   * property.\n   * @param jaasRequired is JAAS required",
  "org.apache.hadoop.service.AbstractService:serviceInit(org.apache.hadoop.conf.Configuration)" : "* All initialization code needed by a service.\n   *\n   * This method will only ever be called once during the lifecycle of\n   * a specific service instance.\n   *\n   * Implementations do not need to be synchronized as the logic\n   * in {@link #init(Configuration)} prevents re-entrancy.\n   *\n   * The base implementation checks to see if the subclass has created\n   * a new configuration instance, and if so, updates the base class value\n   * @param conf configuration\n   * @throws Exception on a failure -these will be caught,\n   * possibly wrapped, and will trigger a service stop",
  "org.apache.hadoop.ipc.RetryCache:addCacheEntryWithPayload(byte[],int,java.lang.Object)" : null,
  "org.apache.hadoop.service.AbstractService:unregisterServiceListener(org.apache.hadoop.service.ServiceStateChangeListener)" : null,
  "org.apache.hadoop.io.compress.SnappyCodec:createDirectDecompressor()" : "* {@inheritDoc}",
  "org.apache.hadoop.io.file.tfile.CompareUtils:<init>()" : "* Prevent the instantiation of class.",
  "org.apache.hadoop.fs.FileStatus:isErasureCoded()" : "* Tell whether the underlying file or directory is erasure coded or not.\n   *\n   * @return true if the underlying file or directory is erasure coded.",
  "org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)" : "* Opens an FSDataOutputStream at the indicated Path with write-progress\n    * reporting. Same as create(), except fails if parent directory doesn't\n    * already exist.\n    * @param f the file name to open\n    * @param permission file permission\n    * @param flags {@link CreateFlag}s to use for this stream.\n    * @param bufferSize the size of the buffer to be used.\n    * @param replication required block replication for the file.\n    * @param blockSize block size\n    * @param progress the progress reporter\n    * @throws IOException IO failure\n    * @see #setPermission(Path, FsPermission)\n    * @return output stream.",
  "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)" : null,
  "org.apache.hadoop.ipc.Server:stop()" : "Stops the service.  No new calls will be handled after this is called.",
  "org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:setSize(int)" : null,
  "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)" : "* Create an FSDataOutputStream at the indicated Path.\n   * @param f the file name to open\n   * @param overwrite if a file with this name already exists, then if true,\n   *   the file will be overwritten, and if false an error will be thrown.\n   * @param bufferSize the size of the buffer to be used.\n   * @param replication required block replication for the file.\n   * @param blockSize the size of the buffer to be used.\n   * @throws IOException IO failure\n   * @return output stream.",
  "org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)" : "* Sort and merge files containing the named classes.\n     * @param fs input FileSystem.\n     * @param keyClass input keyClass.\n     * @param valClass input valClass.\n     * @param conf input Configuration.",
  "org.apache.hadoop.crypto.CryptoOutputStream:encrypt()" : "* Do the encryption, input is {@link #inBuffer} and output is \n   * {@link #outBuffer}.",
  "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:toLongStatistic(java.util.Map$Entry)" : "* Convert a counter/gauge entry to a long statistics.\n   * @param e entry\n   * @return statistic",
  "org.apache.hadoop.fs.shell.Command:recursePath(org.apache.hadoop.fs.shell.PathData)" : "*  Gets the directory listing for a path and invokes\n   *  {@link #processPaths(PathData, PathData...)}\n   *  @param item {@link PathData} for directory to recurse into\n   *  @throws IOException if anything goes wrong...",
  "org.apache.hadoop.fs.sftp.SFTPInputStream:seekToNewSource(long)" : null,
  "org.apache.hadoop.security.Groups:getGroupsSet(java.lang.String)" : "* Get the group memberships of a given user.\n   * If the user's group is not cached, this method may block.\n   * This provide better performance when user has large group membership via\n   * <br>\n   * 1) avoid {@literal set->list->set} conversion for the caller\n   * UGI/PermissionCheck <br>\n   * 2) fast lookup using contains() via Set instead of List\n   * @param user User's name\n   * @return the group memberships of the user as set\n   * @throws IOException if user does not exist",
  "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:waitForCompletion()" : null,
  "org.apache.hadoop.ipc.Server:getRemotePort()" : "* @return Returns the remote side port when invoked inside an RPC\n   * Returns 0 in case of an error.",
  "org.apache.hadoop.fs.shell.Count:<init>(java.lang.String[],int,org.apache.hadoop.conf.Configuration)" : "Constructor\n   * @deprecated invoke via {@link FsShell}\n   * @param cmd the count command\n   * @param pos the starting index of the arguments\n   * @param conf configuration",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMeanStatistics(org.apache.hadoop.fs.statistics.MeanStatistic,org.apache.hadoop.fs.statistics.MeanStatistic)" : "* Aggregate the mean statistics.\n   * This returns a new instance.\n   * @param l left value\n   * @param r right value\n   * @return aggregate value",
  "org.apache.hadoop.io.MD5Hash$Comparator:<init>()" : null,
  "org.apache.hadoop.fs.ZeroCopyUnavailableException:<init>(java.lang.Exception)" : null,
  "org.apache.hadoop.fs.shell.find.Find:processArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.permission.ChmodParser:applyNewPermission(org.apache.hadoop.fs.FileStatus)" : "* Apply permission against specified file and determine what the\n   * new mode would be\n   * @param file File against which to apply mode\n   * @return File's new mode if applied.",
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:sampleMetrics()" : "* Sample all the sources for a snapshot of metrics/tags\n   * @return  the metrics buffer containing the snapshot",
  "org.apache.hadoop.security.GroupMappingServiceProvider:getGroupsSet(java.lang.String)" : "* Get all various group memberships of a given user.\n   * Returns EMPTY set in case of non-existing user\n   * @param user User's name\n   * @return set of group memberships of user\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:resetState()" : null,
  "org.apache.hadoop.security.SaslPropertiesResolver:getSaslProperties(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.SaslRpcServer$QualityOfProtection)" : "* A util function to retrieve specific additional sasl property from config.\n   * Used by subclasses to read sasl properties used by themselves.\n   * @param conf the configuration\n   * @param configKey the config key to look for\n   * @param defaultQOP the default QOP if the key is missing\n   * @return sasl property associated with the given key",
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,boolean)" : "* Set mandatory boolean option.\n   *\n   * @see #must(String, String)",
  "org.apache.hadoop.fs.GlobPattern:hasWildcard()" : "* @return true if this is a wildcard pattern (with special chars)",
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:entries()" : null,
  "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:write(byte[],int,int)" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:syncLocalCacheWithZk(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : "* This method synchronizes the state of a delegation token information in\n   * local cache with its actual value in Zookeeper.\n   *\n   * @param ident Identifier of the token",
  "org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:isNodeGroup()" : "* Judge if this node represents a node group\n     * \n     * @return true if it has no child or its children are not InnerNodes",
  "org.apache.hadoop.ipc.Client$ConnectionId:getMaxRetriesOnSocketTimeouts()" : "@return max connection retries on socket time outs",
  "org.apache.hadoop.conf.ConfServlet$BadFormatException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.Display$TextRecordInputStream:read()" : null,
  "org.apache.hadoop.fs.FSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)" : null,
  "org.apache.hadoop.security.http.CrossOriginFilter:areOriginsAllowed(java.lang.String)" : null,
  "org.apache.hadoop.util.Options$LongOption:getValue()" : null,
  "org.apache.hadoop.ipc.RPC:getRpcTimeout(org.apache.hadoop.conf.Configuration)" : "* Get the RPC time from configuration;\n   * If not set in the configuration, return the default value.\n   *\n   * @param conf Configuration\n   * @return the RPC timeout (ms)",
  "org.apache.hadoop.fs.audit.CommonAuditContext:<init>()" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.io.IOUtils:fsync(java.io.File)" : "* Ensure that any writes to the given file is written to the storage device\n   * that contains it. This method opens channel on given File and closes it\n   * once the sync is done.<br>\n   * Borrowed from Uwe Schindler in LUCENE-5588\n   * @param fileToSync the file to fsync\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat:snapshotInto(org.apache.hadoop.metrics2.lib.MutableRate)" : null,
  "org.apache.hadoop.io.DataOutputBuffer$Buffer:<init>()" : null,
  "org.apache.hadoop.fs.FSDataOutputStream:toString()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(byte[][],int[],byte[][])" : null,
  "org.apache.hadoop.security.UserGroupInformation:setShouldRenewImmediatelyForTests(boolean)" : "* For the purposes of unit tests, we want to test login\n   * from keytab and don't want to wait until the renew\n   * window (controlled by TICKET_RENEW_WINDOW).\n   * @param immediate true if we should login without waiting for ticket window",
  "org.apache.hadoop.security.CompositeGroupsMapping:cacheGroupsRefresh()" : "* Caches groups, no need to do that for this provider",
  "org.apache.hadoop.ipc.ProtocolProxy:<init>(java.lang.Class,java.lang.Object,boolean)" : "* Constructor\n   * \n   * @param protocol protocol class\n   * @param proxy its proxy\n   * @param supportServerMethodCheck If false proxy will never fetch server\n   *        methods and isMethodSupported will always return true. If true,\n   *        server methods will be fetched for the first call to \n   *        isMethodSupported.",
  "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:getKey()" : "@return Returns the stored rawKey",
  "org.apache.hadoop.fs.shell.TouchCommands$Touch:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read(byte[],int,int)" : null,
  "org.apache.hadoop.util.CrcUtil:compose(int,int,long,int)" : "* compose.\n   *\n   * @param crcA crcA.\n   * @param crcB crcB.\n   * @param lengthB length of content corresponding to {@code crcB}, in bytes.\n   * @param mod mod.\n   * @return compose result.",
  "org.apache.hadoop.conf.Configuration$Resource:getName()" : null,
  "org.apache.hadoop.security.token.DtFileOperations:formatDate(long)" : "Format a long integer type into a date string.",
  "org.apache.hadoop.security.token.Token:getPassword()" : "* Get the token password/secret.\n   * @return the token password/secret",
  "org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:isSingleSwitch()" : "* Declare that the mapper is single-switched if a script was not named\n     * in the configuration.\n     * @return true iff there is no script",
  "org.apache.hadoop.conf.StorageUnit$3:toBytes(double)" : null,
  "org.apache.hadoop.fs.FsUrlStreamHandler:openConnection(java.net.URL)" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:setDropBehind(java.lang.Boolean)" : null,
  "org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)" : null,
  "org.apache.hadoop.security.SaslRpcServer:splitKerberosName(java.lang.String)" : "* Splitting fully qualified Kerberos name into parts.\n   * @param fullName fullName.\n   * @return splitKerberosName.",
  "org.apache.hadoop.io.SequenceFile$Writer:hasCapability(java.lang.String)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)" : "* Get a proxy connection to a remote server.\n   *\n   * @param <T> Generics Type T.\n   * @param protocol protocol class\n   * @param clientVersion client version\n   * @param addr remote address\n   * @param conf configuration to use\n   * @return the proxy\n   * @throws IOException if the far end through a RemoteException",
  "org.apache.hadoop.io.SequenceFile$Sorter:cloneFileAttributes(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)" : "* Clones the attributes (like compression of the input file and creates a \n     * corresponding Writer\n     * @param inputFile the path of the input file whose attributes should be \n     * cloned\n     * @param outputFile the path of the output file \n     * @param prog the Progressable to report status during the file write\n     * @return Writer\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.net.NodeBase:toString()" : "@return this node's path as its string representation",
  "org.apache.hadoop.fs.FSLinkResolver:qualifySymlinkTarget(java.net.URI,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Return a fully-qualified version of the given symlink target if it\n   * has no scheme and authority. Partially and fully-qualified paths\n   * are returned unmodified.\n   * @param pathURI URI of the filesystem of pathWithLink\n   * @param pathWithLink Path that contains the symlink\n   * @param target The symlink's absolute target\n   * @return Fully qualified version of the target.",
  "org.apache.hadoop.crypto.CryptoInputStream:read()" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setCounter(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsLogging$SourceToString:toString()" : null,
  "org.apache.hadoop.fs.LocalFileSystemPathHandle:<init>(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:enterNeutralMode()" : null,
  "org.apache.hadoop.security.SecurityUtil:buildTokenService(java.net.InetSocketAddress)" : "* Construct the service key for a token\n   * @param addr InetSocketAddress of remote connection with a token\n   * @return \"ip:port\" or \"host:port\" depending on the value of\n   *          hadoop.security.token.service.use_ip",
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)" : null,
  "org.apache.hadoop.fs.permission.FsPermission:<init>(short)" : "* Construct by the given mode.\n   * @param mode mode.\n   * @see #toShort()",
  "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)" : "* Construct the preferred type of 'raw' SequenceFile Writer.\n   * @param conf The configuration.\n   * @param out The stream on top which the writer is to be constructed.\n   * @param keyClass The 'key' type.\n   * @param valClass The 'value' type.\n   * @param compressionType The compression type.\n   * @param codec The compression codec.\n   * @param metadata The metadata of the file.\n   * @return Returns the handle to the constructed SequenceFile Writer.\n   * @throws IOException raised on errors performing I/O.\n   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}\n   *     instead.",
  "org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream)" : "* Create a {@link CompressionInputStream} that will read from the given\n   * input stream and return a stream for uncompressed data.\n   *\n   * @param in the stream to read compressed bytes from\n   * @return a stream to read uncompressed bytes from\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.ComparableVersion$IntegerItem:<init>(java.lang.String)" : null,
  "org.apache.hadoop.net.NetworkTopology:interAddNodeWithEmptyRack(org.apache.hadoop.net.Node)" : "* Internal function for update empty rack number\n   * for add or recommission a node.\n   * @param node node to be added; can be null",
  "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:initPiggyBackFullIndexVec(int,int[])" : null,
  "org.apache.hadoop.fs.ContentSummary:toSnapshot(boolean)" : "* Return the string representation of the snapshot counts in the output\n   * format.\n   * @param hOption flag indicating human readable or not\n   * @return String representation of the snapshot counts",
  "org.apache.hadoop.service.launcher.ServiceLauncher:loadConfigurationClasses()" : "* @return This creates all the configurations defined by\n   * {@link #getConfigurationsToCreate()} , ensuring that\n   * the resources have been pushed in.\n   * If one cannot be loaded it is logged and the operation continues\n   * except in the case that the class does load but it isn't actually\n   * a subclass of {@link Configuration}.\n   * @throws ExitUtil.ExitException if a loaded class is of the wrong type",
  "org.apache.hadoop.io.SequenceFile$Reader:stream(org.apache.hadoop.fs.FSDataInputStream)" : "* Create an option to specify the stream with the sequence file.\n     * @param value the stream to read.\n     * @return a new option",
  "org.apache.hadoop.security.http.CrossOriginFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)" : null,
  "org.apache.hadoop.fs.local.RawLocalFs:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String,boolean)" : "* takes input as a comma separated list of files\n   * and verifies if they exist. It defaults for file:///\n   * if the files specified do not have a scheme.\n   * it returns the paths uri converted defaulting to file:///.\n   * So an input of  /home/user/file1,/home/user/file2 would return\n   * file:///home/user/file1,file:///home/user/file2.\n   *\n   * @param files the input files argument\n   * @param expandWildcard whether a wildcard entry is allowed and expanded. If\n   * true, any directory followed by a wildcard is a valid entry and is replaced\n   * with the list of jars in that directory. It is used to support the wildcard\n   * notation in a classpath.\n   * @return a comma-separated list of validated and qualified paths, or null\n   * if the input files argument is null",
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeDirLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.io.SequenceFile$CompressedBytes:reset(java.io.DataInputStream,int)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)" : null,
  "org.apache.hadoop.fs.shell.Mkdir:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:<init>(java.io.InputStream,long,long,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:toString()" : null,
  "org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)" : "* Create a new Path based on the child path resolved against the parent path.\n   *\n   * @param parent the parent path\n   * @param child the child path",
  "org.apache.hadoop.fs.ChecksumFs:getRawFs()" : "* get the raw file system.\n   *\n   * @return abstract file system.",
  "org.apache.hadoop.net.ScriptBasedMappingWithDependency:<init>()" : "* Create an instance with the default configuration.\n   * <p>\n   * Calling {@link #setConf(Configuration)} will trigger a\n   * re-evaluation of the configuration settings and so be used to\n   * set up the mapping script.",
  "org.apache.hadoop.metrics2.lib.MetricsAnnotations:makeSource(java.lang.Object)" : "* Make an metrics source from an annotated object.\n   * @param source  the annotated object.\n   * @return a metrics source",
  "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:numCreated()" : "* Number of items created so far. Mostly for testing purposes.\n   * @return the count.",
  "org.apache.hadoop.io.WritableUtils:readVLong(java.io.DataInput)" : "* Reads a zero-compressed encoded long from input stream and returns it.\n   * @param stream Binary input stream\n   * @throws IOException raised on errors performing I/O.\n   * @return deserialized long from stream.",
  "org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator$1:<init>()" : null,
  "org.apache.hadoop.fs.shell.find.Result:<init>(boolean,boolean)" : null,
  "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:values()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getFallbackFileSystem()" : "* @return Gets the fallback file system configured. Usually, this will be the\n   * default cluster.",
  "org.apache.hadoop.fs.DF:toString()" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)" : null,
  "org.apache.hadoop.util.JsonSerialization:fromBytes(byte[])" : "* Deserialize from a byte array.\n   * @param bytes byte array\n   * @throws IOException IO problems\n   * @throws EOFException not enough data\n   * @return byte array.",
  "org.apache.hadoop.fs.permission.PermissionStatus:getUserName()" : "* Return user name.\n   * @return user name.",
  "org.apache.hadoop.io.MapWritable:put(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)" : null,
  "org.apache.hadoop.security.LdapGroupsMapping:switchBindUser(javax.naming.AuthenticationException)" : "* Switch to the next available user to bind to.\n   * @param e AuthenticationException encountered when contacting LDAP",
  "org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path,boolean)" : "* Append to an existing file (optional operation).\n   * @param f the existing file to be appended.\n   * @param appendToNewBlock whether to append data to a new block\n   * instead of the end of the last partial block\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).\n   * @return output stream.",
  "org.apache.hadoop.net.ScriptBasedMapping:<init>()" : "* Create an instance with the default configuration.\n   * <p>\n   * Calling {@link #setConf(Configuration)} will trigger a\n   * re-evaluation of the configuration settings and so be used to\n   * set up the mapping script.\n   *",
  "org.apache.hadoop.io.MapWritable:clear()" : null,
  "org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path)" : "* Create a snapshot with a default name.\n   *\n   * @param path The directory where snapshots will be taken.\n   * @return the snapshot path.\n   *\n   * @throws IOException If an I/O error occurred\n   *\n   * <p>Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws\n   *           undeclared exception to RPC server",
  "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:createDecryptor()" : null,
  "org.apache.hadoop.ipc.Server$FatalRpcServerException:<init>(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,java.lang.String)" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:reset()" : null,
  "org.apache.hadoop.util.ClassUtil:findContainingResource(java.lang.ClassLoader,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize()" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping:<init>(org.apache.hadoop.conf.Configuration,boolean)" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:getData(java.lang.String)" : "* Get the data in a ZNode.\n   * @param path Path of the ZNode.\n   * @return The data in the ZNode.\n   * @throws Exception If it cannot contact Zookeeper.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:getCurrentIOStatisticsContext()" : "* Get the current thread's IOStatisticsContext instance. If no instance is\n   * present for this thread ID, create one using the factory.\n   * @return instance of IOStatisticsContext.",
  "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithFixedSleep:<init>(int,long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.security.ssl.SSLFactory:readSSLConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.ssl.SSLFactory$Mode)" : null,
  "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:createDecryptor()" : null,
  "org.apache.hadoop.net.NetworkTopology:remove(org.apache.hadoop.net.Node)" : "Remove a node\n   * Update node counter and rack counter if necessary\n   * @param node node to be removed; can be null",
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:<init>(java.lang.String)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:setAclsWithRetries(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibFactory:setCompressionStrategy(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy)" : null,
  "org.apache.hadoop.fs.impl.prefetch.Retryer:<init>(int,int,int)" : "* Initializes a new instance of the {@code Retryer} class.\n   *\n   * @param perRetryDelay per retry delay (in ms).\n   * @param maxDelay maximum amount of delay (in ms) before retry fails.\n   * @param statusUpdateInterval time interval (in ms) at which status update would be made.\n   *\n   * @throws IllegalArgumentException if perRetryDelay is zero or negative.\n   * @throws IllegalArgumentException if maxDelay is less than or equal to perRetryDelay.\n   * @throws IllegalArgumentException if statusUpdateInterval is zero or negative.",
  "org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:long2String(long,java.lang.String,int)" : "* Convert a long integer to a string with traditional binary prefix.\n     * \n     * @param n the value to be converted\n     * @param unit The unit, e.g. \"B\" for bytes.\n     * @param decimalPlaces The number of decimal places.\n     * @return a string with traditional binary prefix.",
  "org.apache.hadoop.fs.shell.Concat:processArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:getHomeDirectory()" : null,
  "org.apache.hadoop.util.bloom.BloomFilter:getVectorSize()" : "* @return size of the the bloomfilter",
  "org.apache.hadoop.util.HostsFileReader:getHosts()" : null,
  "org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroupsSet(java.lang.String)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:valueClass(java.lang.Class)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent)" : "* interface implementation of Zookeeper watch events (connection and node),\n   * proxied by {@link WatcherWithClientRef}.",
  "org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getName(java.lang.reflect.Field)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:generateDelegationToken(org.apache.hadoop.security.token.Token)" : "* Generate a DelegationTokenAuthenticatedURL.Token from the given generic\n   * typed delegation token.\n   *\n   * @param dToken The delegation token.\n   * @return The DelegationTokenAuthenticatedURL.Token, with its delegation\n   *         token set to the delegation token passed in.",
  "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:getClientAliases(java.lang.String,java.security.Principal[])" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:requestTimeMax()" : null,
  "org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)" : null,
  "org.apache.hadoop.ipc.CallQueueManager:throwBackoff()" : null,
  "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:failed()" : null,
  "org.apache.hadoop.io.SequenceFile$RecordCompressWriter:append(java.lang.Object,java.lang.Object)" : "Append a key/value pair.",
  "org.apache.hadoop.ha.HealthCheckFailedException:<init>(java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.ipc.FairCallQueue:poll(long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.net.NetworkTopology:isNodeInScope(org.apache.hadoop.net.Node,java.lang.String)" : "* Checks whether a node belongs to the scope.\n   * @param node  the node to check.\n   * @param scope scope to check.\n   * @return true if node lies within the scope",
  "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getReplication()" : null,
  "org.apache.hadoop.metrics2.impl.MetricGaugeFloat:type()" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeSingle(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int,int,boolean)" : null,
  "org.apache.hadoop.io.WritableUtils:writeCompressedByteArray(java.io.DataOutput,byte[])" : null,
  "org.apache.hadoop.ipc.Client:getAsyncCallCount()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:compareTo(int,long)" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$MakeAccessible:<init>(java.lang.reflect.Method)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:canonicalizeUri(java.net.URI)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters:getPrefetchingStatistics()" : "* @return The prefetching statistics for the stream.",
  "org.apache.hadoop.fs.store.DataBlocks$DataBlock:getState()" : "* Current state.\n     *\n     * @return the current state.",
  "org.apache.hadoop.fs.permission.FsCreateModes:equals(java.lang.Object)" : null,
  "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getFixedByteString(org.apache.hadoop.io.Text)" : "* Get the ByteString for frequently used fixed and small set strings.\n   * @param key Hadoop Writable Text string\n   * @return the ByteString for frequently used fixed and small set strings.",
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)" : "* Removes the existing DelegationKey from the SQL database to\n   * invalidate it.\n   * @param key DelegationKey to remove from the SQL database.",
  "org.apache.hadoop.security.token.Token:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping:getGidAllowingUnknown(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.PathData:suffix(java.lang.String)" : "* Returns a new PathData with the given extension.\n   * @param extension for the suffix\n   * @return PathData\n   * @throws IOException shouldn't happen",
  "org.apache.hadoop.crypto.CipherSuite:getUnknownValue()" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Compressor:getBytesRead()" : "* Return number of bytes given to this compressor since last reset.",
  "org.apache.hadoop.util.StringUtils:popOption(java.lang.String,java.util.List)" : "* From a list of command-line arguments, remove an option.\n   *\n   * @param name  Name of the option to remove.  Example: -foo.\n   * @param args  List of arguments.\n   * @return      true if the option was found and removed; false otherwise.",
  "org.apache.hadoop.fs.DU$DUShell:getExecString()" : null,
  "org.apache.hadoop.metrics2.MetricsTag:value()" : "* Get the value of the tag\n   * @return  the value",
  "org.apache.hadoop.fs.FilterFileSystem:open(org.apache.hadoop.fs.Path,int)" : "* Opens an FSDataInputStream at the indicated Path.\n   * @param f the file name to open\n   * @param bufferSize the size of the buffer to be used.",
  "org.apache.hadoop.ipc.RPC$Builder:setProtocol(java.lang.Class)" : "* @return Mandatory field.\n     * @param protocol input protocol.",
  "org.apache.hadoop.util.InstrumentedLock$SuppressedStats:snapshot()" : "* Captures the current value of the counts into a SuppressedSnapshot object\n     * and resets the values to zero.\n     *\n     * @return SuppressedSnapshot containing the current value of the counters",
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newStat(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)" : "* Create a mutable metric with stats\n   * @param name  of the metric\n   * @param desc  metric description\n   * @param sampleName  of the metric (e.g., \"Ops\")\n   * @param valueName   of the metric (e.g., \"Time\" or \"Latency\")\n   * @param extended    produce extended stat (stdev, min/max etc.) if true.\n   * @return a new mutable stat metric object",
  "org.apache.hadoop.fs.viewfs.ViewFs:getLinkTarget(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumDataUnits()" : null,
  "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:innerClose()" : "* The close operation will delete the destination file if it still\n     * exists.\n     *\n     * @throws IOException IO problems",
  "org.apache.hadoop.fs.shell.Ls:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.FindClass:err(java.lang.String,java.lang.Object[])" : "* print something to stderr\n   * @param s string to print",
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:finished()" : null,
  "org.apache.hadoop.fs.FilterFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.crypto.key.KeyProviderExtension:deleteKey(java.lang.String)" : null,
  "org.apache.hadoop.service.AbstractService:isInState(org.apache.hadoop.service.Service$STATE)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:searchPair(int)" : "* Given the current number of retry, search the corresponding pair.\n     * @return the corresponding pair,\n     *   or null if the current number of retry > maximum number of retry.",
  "org.apache.hadoop.fs.FsServerDefaults$1:<init>()" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:ctorImpl(java.lang.String,java.lang.Class[])" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:onTimerEvent()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token:setDelegationToken(org.apache.hadoop.security.token.Token)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:checkBuffers(java.nio.ByteBuffer[])" : "* Check and ensure the buffers are of the desired length and type, direct\n   * buffers or not.\n   * @param buffers the buffers to check",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:needsDictionary()" : null,
  "org.apache.hadoop.conf.Configuration:size()" : "* Return the number of keys in the configuration.\n   *\n   * @return number of keys in the configuration.",
  "org.apache.hadoop.metrics2.util.MBeans:getMbeanNameName(javax.management.ObjectName)" : null,
  "org.apache.hadoop.util.StopWatch:toString()" : null,
  "org.apache.hadoop.fs.FileRange:createFileRange(long,int,java.lang.Object)" : "* Factory method to create a FileRange object.\n   * @param offset starting offset of the range.\n   * @param length length of the range.\n   * @param reference nullable reference to store in the range.\n   * @return a new instance of FileRangeImpl.",
  "org.apache.hadoop.net.NetUtils:normalizeHostName(java.lang.String)" : "* Given a string representation of a host, return its ip address\n   * in textual presentation.\n   * \n   * @param name a string representation of a host:\n   *             either a textual representation its IP address or its host name\n   * @return its IP address in the string format",
  "org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry:calculateSleepTime(int)" : null,
  "org.apache.hadoop.io.DataInputByteBuffer$Buffer:getData()" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)" : "* Construct an RPC server.\n     * @param protocolClass - the protocol being registered\n     *     can be null for compatibility with old usage (see below for details)\n     * @param protocolImpl the protocol impl that will be called\n     * @param conf the configuration to use\n     * @param bindAddress the address to bind on to listen for connection\n     * @param port the port to listen for connections on\n     * @param numHandlers the number of method handler threads to run\n     * @param verbose whether each call should be logged\n     * @param alignmentContext provides server state info on client responses\n     * @param numReaders input numReaders.\n     * @param portRangeConfig input portRangeConfig.\n     * @param queueSizePerHandler input queueSizePerHandler.\n     * @param secretManager input secretManager.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setWorkFactor(org.apache.hadoop.conf.Configuration,int)" : null,
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:createCollectorPath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.CpuTimeTracker:toString()" : null,
  "org.apache.hadoop.fs.impl.StoreImplementationUtils:objectHasCapability(java.lang.Object,java.lang.String)" : "* Probe for an object having a capability; returns true\n   * if the stream implements {@link StreamCapabilities} and its\n   * {@code hasCapabilities()} method returns true for the capability.\n   * This is a package private method intended to provided a common\n   * implementation for input and output streams.\n   * {@link StreamCapabilities#hasCapability(String)} call is for public use.\n   * @param object object to probe.\n   * @param capability capability to probe for\n   * @return true if the object implements stream capabilities and\n   * declares that it supports the capability.",
  "org.apache.hadoop.fs.Path:<init>(java.lang.String,org.apache.hadoop.fs.Path)" : "* Create a new Path based on the child path resolved against the parent path.\n   *\n   * @param parent the parent path\n   * @param child the child path",
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)" : null,
  "org.apache.hadoop.fs.CompositeCrcFileChecksum:getBytes()" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:visitAll(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsAggregator)" : "* Apply the given aggregator to all StatisticsData objects associated with\n     * this Statistics object.\n     *\n     * For each StatisticsData object, we will call accept on the visitor.\n     * Finally, at the end, we will call aggregate to get the final total.\n     *\n     * @param         visitor to use.\n     * @return        The total.",
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:requestCaching(int)" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination:getRemoteDestination(java.util.LinkedList)" : "*  The last arg is expected to be a remote path, if only one argument is\n   *  given then the destination will be the remote user's directory \n   *  @param args is the list of arguments\n   *  @throws PathIOException if path doesn't exist or matches too many times",
  "org.apache.hadoop.fs.ChecksumFileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.PathIOException:<init>(java.lang.String,java.lang.Throwable)" : "* Appends the text of a Throwable to the default error message\n   * @param path for the exception\n   * @param cause a throwable to extract the error message",
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMinimumFunction(java.lang.String,java.util.function.Function)" : "* add a mapping of a key to a minimum function.\n   * @param key the key\n   * @param eval the evaluator",
  "org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean)" : "Return the string representation of the object in the output format.\n   * For description of the options,\n   * @see #toString(boolean, boolean, boolean, boolean, List)\n   * \n   * @param qOption a flag indicating if quota needs to be printed or not\n   * @param hOption a flag indicating if human readable output if to be used\n   * @return the string representation of the object",
  "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:select(java.nio.channels.SelectableChannel,int,long)" : "* Waits on the channel with the given timeout using one of the \n     * cached selectors. It also removes any cached selectors that are\n     * idle for a few seconds.\n     * \n     * @param channel\n     * @param ops\n     * @param timeout\n     * @return\n     * @throws IOException",
  "org.apache.hadoop.fs.audit.AuditStatisticNames:<init>()" : null,
  "org.apache.hadoop.io.Text:<init>(java.lang.String)" : "* Construct from a string.\n   * @param string input string.",
  "org.apache.hadoop.fs.AbstractFileSystem:methodNotSupported()" : "* Helper method that throws an {@link UnsupportedOperationException} for the\n   * current {@link FileSystem} method being called.",
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getStats()" : null,
  "org.apache.hadoop.security.http.CrossOriginFilter:encodeHeader(java.lang.String)" : null,
  "org.apache.hadoop.io.MapFile:rename(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.String)" : "* Renames an existing map directory.\n   * @param fs fs.\n   * @param oldName oldName.\n   * @param newName newName.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.Shell:isTimedOut()" : "* To check if the passed script to shell command executor timed out or\n   * not.\n   *\n   * @return if the script timed out.",
  "org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)" : "* Create a temp file and a {@link DiskBlock} instance to manage it.\n     *\n     * @param index      block index.\n     * @param limit      limit of the block.\n     * @param statistics statistics to update.\n     * @return the new block.\n     * @throws IOException IO problems",
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:setInitialFlushTime(java.util.Date)" : "* Set the {@link #nextFlush} variable to the initial flush time. The initial\n   * flush will be an integer number of flush intervals past the beginning of\n   * the current hour and will have a random offset added, up to\n   * {@link #rollOffsetIntervalMillis}. The initial flush will be a time in\n   * past that can be used from which to calculate future flush times.\n   *\n   * @param now the current time",
  "org.apache.hadoop.security.JniBasedUnixGroupsMapping:cacheGroupsAdd(java.util.List)" : null,
  "org.apache.hadoop.io.BytesWritable:setSize(int)" : "* Change the size of the buffer. The values in the old range are preserved\n   * and any new values are undefined. The capacity is changed if it is \n   * necessary.\n   * @param size The new number of bytes",
  "org.apache.hadoop.fs.ChecksumFileSystem:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)" : "* Report a checksum error to the file system.\n   * @param f the file name containing the error\n   * @param in the stream open on the file\n   * @param inPos the position of the beginning of the bad data in the file\n   * @param sums the stream open on the checksum file\n   * @param sumsPos the position of the beginning of the bad data in the checksum file\n   * @return if retry is necessary",
  "org.apache.hadoop.security.LdapGroupsMapping:getRelativeDistinguishedName(java.lang.String)" : "* A helper method to get the Relative Distinguished Name (RDN) from\n   * Distinguished name (DN). According to Active Directory documentation,\n   * a group object's RDN is a CN.\n   *\n   * @param distinguishedName A string representing a distinguished name.\n   * @throws NamingException if the DN is malformed.\n   * @return a string which represents the RDN",
  "org.apache.hadoop.fs.AbstractFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path)" : "* Set the source path to satisfy storage policy.\n   * @param path The source path referring to either a directory or a file.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.Text:decode(java.nio.ByteBuffer,boolean)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)" : null,
  "org.apache.hadoop.util.LightWeightResizableGSet:<init>()" : null,
  "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,org.apache.hadoop.tools.TableListing$Justification)" : null,
  "org.apache.hadoop.util.GenericsUtil:getClass(java.lang.Object)" : "* Returns the Class object (of type <code>Class&lt;T&gt;</code>) of the  \n   * argument of type <code>T</code>. \n   * @param <T> The type of the argument\n   * @param t the object to get it class\n   * @return <code>Class&lt;T&gt;</code>",
  "org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:hsync()" : "* If the inner stream is Syncable, flush the buffer and then\n   * invoke the inner stream's hsync() operation.\n   *\n   * Otherwise: throw an exception, unless the stream was constructed with\n   * {@link #downgradeSyncable} set to true, in which case the stream\n   * is just flushed.\n   * @throws IOException IO Problem\n   * @throws UnsupportedOperationException if the inner class is not syncable",
  "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:hashCode()" : null,
  "org.apache.hadoop.net.NetworkTopology:getDatanodesInRack(java.lang.String)" : "* Given a string representation of a rack, return its children\n   * @param loc a path-like string representation of a rack\n   * @return a newly allocated list with all the node's children",
  "org.apache.hadoop.security.UserGroupInformation:hasKerberosCredentials()" : "* checks if logged in using kerberos\n   * @return true if the subject logged via keytab or has a Kerberos TGT",
  "org.apache.hadoop.fs.RawLocalFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : "* Use the command chown to set owner.",
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:seekToNewSource(long)" : null,
  "org.apache.hadoop.io.SequenceFile$Metadata:toString()" : null,
  "org.apache.hadoop.conf.StorageUnit$2:toEBs(double)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine:getAsyncReturnMessage()" : null,
  "org.apache.hadoop.service.AbstractService:start()" : "* {@inheritDoc}\n   * @throws ServiceStateException if the current service state does not permit\n   * this action",
  "org.apache.hadoop.security.KDiag:execute()" : "* Execute diagnostics.\n   * <p>\n   * Things it would be nice if UGI made accessible\n   * <ol>\n   *   <li>A way to enable JAAS debug programatically</li>\n   *   <li>Access to the TGT</li>\n   * </ol>\n   * @return true if security was enabled and all probes were successful\n   * @throws KerberosDiagsFailure explicitly raised failure\n   * @throws Exception other security problems",
  "org.apache.hadoop.security.SaslRpcServer$AuthMethod:read(java.io.DataInput)" : "* Read from in.\n     *\n     * @param in DataInput.\n     * @throws IOException raised on errors performing I/O.\n     * @return AuthMethod.",
  "org.apache.hadoop.io.BloomMapFile$Writer:close()" : null,
  "org.apache.hadoop.crypto.key.KeyProviderExtension:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)" : null,
  "org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallbackAndRemove(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)" : "* Send callback, and if the domain socket was closed as a result of\n   * processing, then also remove the entry for the file descriptor.\n   *\n   * @param caller reason for call\n   * @param entries mapping of file descriptor to entry\n   * @param fdSet set of file descriptors\n   * @param fd file descriptor",
  "org.apache.hadoop.fs.ContentSummary$Builder:typeConsumed(long[])" : null,
  "org.apache.hadoop.io.compress.GzipCodec:createDirectDecompressor()" : null,
  "org.apache.hadoop.fs.permission.AclEntry:toString()" : null,
  "org.apache.hadoop.util.functional.TaskPool:foreach(java.lang.Object[])" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSystem()" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setGauge(java.lang.String,long)" : null,
  "org.apache.hadoop.io.compress.BlockCompressorStream:finish()" : null,
  "org.apache.hadoop.io.LongWritable$Comparator:compare(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(com.jcraft.jsch.ChannelSftp,com.jcraft.jsch.ChannelSftp$LsEntry,org.apache.hadoop.fs.Path)" : "* Convert the file information in LsEntry to a {@link FileStatus} object. *\n   *\n   * @param sftpFile\n   * @param parentPath\n   * @return file status\n   * @throws IOException",
  "org.apache.hadoop.ha.HealthMonitor$MonitorDaemon:run()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumByLocation(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "Used by mergePass to merge the output of the sort\n     * @param inName the name of the input file containing sorted segments\n     * @param indexIn the offsets of the sorted segments\n     * @param tmpDir the relative directory to store intermediate results in\n     * @return RawKeyValueIterator\n     * @throws IOException",
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:reencryptEncryptedKeys(java.util.List)" : "* Batched version of {@link #reencryptEncryptedKey(EncryptedKeyVersion)}.\n   * <p>\n   * For each encrypted key version, re-encrypts an encrypted key version,\n   * using its initialization vector and key material, but with the latest\n   * key version name of its key name. If the latest key version name in the\n   * provider is the same as the one encrypted the passed-in encrypted key\n   * version, the same encrypted key version is returned.\n   * <p>\n   * NOTE: The generated key is not stored by the <code>KeyProvider</code>\n   *\n   * @param  ekvs List containing the EncryptedKeyVersion's\n   * @throws IOException If any EncryptedKeyVersion could not be re-encrypted\n   * @throws GeneralSecurityException If any EncryptedKeyVersion could not be\n   *                            re-encrypted because of a cryptographic issue.",
  "org.apache.hadoop.fs.DelegateToFileSystem:setReplication(org.apache.hadoop.fs.Path,short)" : null,
  "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getNumDataUnits()" : null,
  "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:<init>(org.apache.hadoop.ipc.GenericRefreshProtocol)" : null,
  "org.apache.hadoop.util.bloom.Key:compareTo(org.apache.hadoop.util.bloom.Key)" : null,
  "org.apache.hadoop.fs.store.ByteBufferInputStream:available()" : null,
  "org.apache.hadoop.fs.shell.find.FindOptions:isDepthFirst()" : "* Should directory tree be traversed depth first?\n   *\n   * @return true indicate depth first traversal",
  "org.apache.hadoop.io.WritableUtils:writeCompressedStringArray(java.io.DataOutput,java.lang.String[])" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)" : null,
  "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:addAttribute(java.lang.String,java.lang.String)" : "* Add a query parameter if not null/empty\n   * There's no need to escape here as it is done in the URI\n   * constructor.\n   * @param key query key\n   * @param value query value",
  "org.apache.hadoop.util.functional.LazyAutoCloseableReference:eval()" : "* {@inheritDoc}\n   * @throws IllegalStateException if the reference is closed.",
  "org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type)" : null,
  "org.apache.hadoop.util.IntrusiveCollection:contains(java.lang.Object)" : null,
  "org.apache.hadoop.io.file.tfile.Compression$FinishOnFlushCompressionStream:flush()" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyCompressor:<init>(int)" : "* Creates a new compressor.\n   *\n   * @param directBufferSize size of the direct buffer to be used.",
  "org.apache.hadoop.io.LongWritable$Comparator:<init>()" : null,
  "org.apache.hadoop.ipc.Server:getPort()" : "* Get the port on which the IPC Server is listening for incoming connections.\n   * This could be an ephemeral port too, in which case we return the real\n   * port on which the Server has bound.\n   * @return port on which IPC Server is listening",
  "org.apache.hadoop.util.CrcUtil:<init>()" : "* Hide default constructor for a static utils class.",
  "org.apache.hadoop.fs.Globber:fixRelativePart(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.Text:toString()" : null,
  "org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:<init>(int,java.util.concurrent.ThreadFactory)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenRealOwner(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : "* Return the real owner for a token. If this is a token from a proxy user,\n   * the real/effective user will be returned.\n   *\n   * @param id\n   * @return real owner",
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:error(java.lang.Throwable)" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNegative(long,java.lang.String)" : "* Validates that the given integer argument is not negative.\n   * @param value the argument value to validate\n   * @param argName the name of the argument being validated.",
  "org.apache.hadoop.fs.shell.find.Result:equals(java.lang.Object)" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:getDeclaredComponentType()" : null,
  "org.apache.hadoop.fs.HarFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[],int,int)" : null,
  "org.apache.hadoop.ipc.ExternalCall:isDone()" : null,
  "org.apache.hadoop.io.file.tfile.Utils$Version:size()" : "* Get the size of the serialized Version object.\n     * \n     * @return serialized size of the version object.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:handleChecksumException(org.apache.hadoop.fs.ChecksumException)" : null,
  "org.apache.hadoop.io.compress.Lz4Codec:createOutputStream(java.io.OutputStream)" : "* Create a {@link CompressionOutputStream} that will write to the given\n   * {@link OutputStream}.\n   *\n   * @param out the location for the final output stream\n   * @return a stream the user can write uncompressed data to have it compressed\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.FilterFs:resolvePath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.ProtocolSignature:<init>()" : "* default constructor",
  "org.apache.hadoop.fs.FsShell:run(java.lang.String[])" : "* run",
  "org.apache.hadoop.metrics2.lib.MutableStat:getSnapshotTimeStamp()" : "* @return Return the SampleStat snapshot timestamp.",
  "org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,int)" : "* Copies from one stream to another.\n   * \n   * @param in InputStrem to read from\n   * @param out OutputStream to write to\n   * @param buffSize the size of the buffer.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.TrashPolicyDefault:makeTrashRelativePath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.SequenceFile$UncompressedBytes:writeUncompressedBytes(java.io.DataOutputStream)" : null,
  "org.apache.hadoop.fs.HarFileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* not implemented.",
  "org.apache.hadoop.fs.impl.CombinedFileRange:merge(long,long,org.apache.hadoop.fs.FileRange,int,int)" : "* Merge this input range into the current one, if it is compatible.\n   * It is assumed that otherOffset is greater or equal the current offset,\n   * which typically happens by sorting the input ranges on offset.\n   * @param otherOffset the offset to consider merging\n   * @param otherEnd the end to consider merging\n   * @param other the underlying FileRange to add if we merge\n   * @param minSeek the minimum distance that we'll seek without merging the\n   *                ranges together\n   * @param maxSize the maximum size that we'll merge into a single range\n   * @return true if we have merged the range into this one",
  "org.apache.hadoop.security.authorize.AccessControlList$1:<init>()" : "* This constructor exists primarily for AccessControlList to be Writable.",
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:toString()" : null,
  "org.apache.hadoop.net.AbstractDNSToSwitchMapping:isSingleSwitchByScriptPolicy()" : null,
  "org.apache.hadoop.fs.shell.Command:isPathRecursable(org.apache.hadoop.fs.shell.PathData)" : "* Determines whether a {@link PathData} item is recursable. Default\n   * implementation is to recurse directories but can be overridden to recurse\n   * through symbolic links.\n   *\n   * @param item\n   *          a {@link PathData} object\n   * @return true if the item is recursable, false otherwise\n   * @throws IOException\n   *           if anything goes wrong in the user-implementation",
  "org.apache.hadoop.service.ServiceStateModel:<init>(java.lang.String)" : "* Create the service state model in the {@link Service.STATE#NOTINITED}\n   * state.\n   *\n   * @param name input name.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:<init>()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler:completed(java.lang.Integer,java.lang.Integer)" : null,
  "org.apache.hadoop.util.GenericOptionsParser:<init>(java.lang.String[])" : "* Create an options parser to parse the args.\n   * @param args the command line arguments\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setWorkingDirectory(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.metrics2.lib.MutableGaugeLong:set(long)" : "* Set the value of the metric\n   * @param value to set",
  "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:<init>(java.util.Iterator)" : "* Construct from an interator.\n     * @param source source iterator.",
  "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkRegex(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)" : "* Add a LinkRegex to the config for the specified mount table.\n   * @param conf - get mountable config from this conf\n   * @param mountTableName - the mountable name of the regex config item\n   * @param srcRegex - the src path regex expression that applies to this config\n   * @param targetStr - the string of target path\n   * @param interceptorSettings - the serialized interceptor string to be\n   *                            applied while resolving the mapping",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)" : "* Constructs a new CBZip2InputStream which decompresses bytes read from the\n  * specified stream.\n  *\n  * <p>\n  * Although BZip2 headers are marked with the magic <tt>\"Bz\"</tt> this\n  * constructor expects the next byte in the stream to be the first one after\n  * the magic. Thus callers have to skip the first two bytes. Otherwise this\n  * constructor will throw an exception.\n  * </p>\n  * @param in in.\n  * @param readMode READ_MODE.\n  * @throws IOException\n  *             if the stream content is malformed or an I/O error occurs.\n  * @throws NullPointerException\n  *             if <tt>in == null</tt>",
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:getFsStatus()" : null,
  "org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcLockWaitTime(long)" : null,
  "org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>(org.apache.hadoop.conf.Configuration)" : "* Creates a new compressor, taking settings from the configuration.\n   * @param conf configuration.",
  "org.apache.hadoop.conf.StorageUnit$5:getShortName()" : null,
  "org.apache.hadoop.conf.StorageUnit$7:toMBs(double)" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine:initialize()" : "* Register the rpcRequest deserializer for WritableRpcEngine",
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)" : "* Persists a DelegationKey into the SQL database. The delegation keyId\n   * is expected to be unique and any duplicate key attempts will result\n   * in an IOException.\n   * @param key DelegationKey to persist into the SQL database.",
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:isEmpty()" : null,
  "org.apache.hadoop.fs.HarFileSystem:toFileStatus(org.apache.hadoop.fs.HarFileSystem$HarStatus)" : "* Combine the status stored in the index and the underlying status. \n   * @param h status stored in the index\n   * @return the combined file status\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator:next()" : null,
  "org.apache.hadoop.security.KDiag:run(java.lang.String[])" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX:munmap(java.nio.MappedByteBuffer)" : "* Unmaps the block from memory. See munmap(2).\n     *\n     * There isn't any portable way to unmap a memory region in Java.\n     * So we use the sun.nio method here.\n     * Note that unmapping a memory region could cause crashes if code\n     * continues to reference the unmapped code.  However, if we don't\n     * manually unmap the memory, we are dependent on the finalizer to\n     * do it, and we have no idea when the finalizer will run.\n     *\n     * @param buffer    The buffer to unmap.",
  "org.apache.hadoop.fs.Options$CreateOpts$Perms:<init>(org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getCredentialEntry(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.CodecPool:borrow(java.util.Map,java.lang.Class)" : null,
  "org.apache.hadoop.util.StopWatch:now()" : "* @return current elapsed time in nanosecond.",
  "org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:getPassword(org.apache.hadoop.security.token.TokenIdentifier)" : null,
  "org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:createDecoder()" : null,
  "org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)" : null,
  "org.apache.hadoop.fs.Path:hashCode()" : null,
  "org.apache.hadoop.util.functional.TaskPool$Builder:run(org.apache.hadoop.util.functional.TaskPool$Task)" : "* Execute the task across the data.\n     * @param task task to execute\n     * @param <E> exception which may be raised in execution.\n     * @return true if the operation executed successfully\n     * @throws E any exception raised.\n     * @throws IOException IOExceptions raised by remote iterator or in execution.",
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setGauge(java.lang.String,long)" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:toString()" : null,
  "org.apache.hadoop.util.ChunkedArrayList:clear()" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:<init>()" : "* Creates a new decompressor with the default buffer size.",
  "org.apache.hadoop.util.ConfigurationHelper:<init>()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.net.InnerNodeImpl:<init>(java.lang.String,java.lang.String,org.apache.hadoop.net.InnerNode,int)" : "* Construct an InnerNode\n   * from its name, its network location, its parent, and its level.\n   * @param name input name.\n   * @param location input location.\n   * @param parent input parent.\n   * @param level input level.",
  "org.apache.hadoop.conf.StorageUnit$2:getDefault(double)" : null,
  "org.apache.hadoop.fs.shell.find.FilterExpression:getConf()" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:isFail()" : null,
  "org.apache.hadoop.util.CleanerUtil:newBufferCleaner(java.lang.Class,java.lang.invoke.MethodHandle)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:unsetStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.ChecksumFs:delete(org.apache.hadoop.fs.Path,boolean)" : "* Implement the delete(Path, boolean) in checksum\n   * file system.",
  "org.apache.hadoop.util.JvmPauseMonitor:main(java.lang.String[])" : "* Simple 'main' to facilitate manual testing of the pause monitor.\n   * \n   * This main function just leaks memory into a list. Running this class\n   * with a 1GB heap will very quickly go into \"GC hell\" and result in\n   * log messages about the GC pauses.\n   *\n   * @param args args.\n   * @throws Exception Exception.",
  "org.apache.hadoop.fs.http.HttpsFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.impl.StoreImplementationUtils:isProbeForSyncable(java.lang.String)" : "* Check the probe capability being for {@link StreamCapabilities#HSYNC}\n   * or {@link StreamCapabilities#HFLUSH}\n   * {@code Syncable.hsync()} and {@code Syncable.hflush()} functionality.\n   * @param capability capability string.\n   * @return true if either refers to one of the Syncable operations.",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:stripOutRoot(org.apache.hadoop.fs.Path)" : "* Strip out the root from the path.\n   * @param p - fully qualified path p\n   * @return -  the remaining path  without the beginning /\n   * @throws IOException if the p is not prefixed with root",
  "org.apache.hadoop.fs.QuotaUsage:getQuota()" : "* Return the directory quota.\n   *\n   * @return quota.",
  "org.apache.hadoop.conf.Configuration:getClassByName(java.lang.String)" : "* Load a class by name.\n   * \n   * @param name the class name.\n   * @return the class object.\n   * @throws ClassNotFoundException if the class is not found.",
  "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])" : null,
  "org.apache.hadoop.security.Credentials:writeWritableOutputStream(java.io.DataOutputStream)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,int,int[],byte[][],int[],byte[][],int[])" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_toPrettyString(java.lang.Object)" : "* Convert IOStatistics to a string form, with all the metrics sorted\n   * and empty value stripped.\n   * @param statistics A statistics instance.\n   * @return string value or the empty string if null\n   * @throws UnsupportedOperationException if the IOStatistics classes were not found",
  "org.apache.hadoop.fs.shell.PathData:refreshStatus()" : "* Updates the paths's file status\n   * @return the updated FileStatus\n   * @throws IOException if anything goes wrong...",
  "org.apache.hadoop.fs.permission.AclEntryType:toString()" : null,
  "org.apache.hadoop.ipc.ProcessingDetails:toString()" : null,
  "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:bufferSize(int)" : "* Set the size of the buffer to be used.\n   *\n   * @param bufSize buffer size.\n   * @return FutureDataInputStreamBuilder.",
  "org.apache.hadoop.fs.shell.Command:getReplacementCommand()" : "* The replacement for a deprecated command\n   * @return null if not deprecated, else alternative command",
  "org.apache.hadoop.conf.StorageUnit$3:toMBs(double)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.fs.QuotaUsage$Builder:spaceQuota(long)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getServerDefaults()" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree:getRemainingPath(java.lang.String[],int)" : "* Return remaining path from specified index to the end of the path array.\n   * @param path An array of path components split by slash\n   * @param startIndex the specified start index of the path array\n   * @return remaining path.",
  "org.apache.hadoop.service.launcher.ServiceLauncher:uncaughtException(java.lang.Thread,java.lang.Throwable)" : "* Handler for uncaught exceptions: terminate the service.\n   * @param thread thread\n   * @param exception exception",
  "org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockIndexNear(long)" : "* Find the smallest Block index whose starting offset is greater than or\n     * equal to the specified offset.\n     * \n     * @param offset\n     *          User-specific offset.\n     * @return the index to the data Block if such block exists; or -1\n     *         otherwise.",
  "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:isContextReset()" : null,
  "org.apache.hadoop.ipc.ResponseBuffer:toByteArray()" : null,
  "org.apache.hadoop.ipc.RetryCache$CacheEntry:equals(java.lang.Object)" : null,
  "org.apache.hadoop.util.Shell$1:run()" : "* Check to see if a command needs to be executed and execute if needed.\n   *\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:<init>(org.apache.hadoop.io.SequenceFile$CompressionType)" : null,
  "org.apache.hadoop.crypto.OpensslCipher:init(int,byte[],byte[])" : "* Initialize this cipher with a key and IV.\n   * \n   * @param mode {@link #ENCRYPT_MODE} or {@link #DECRYPT_MODE}\n   * @param key crypto key\n   * @param iv crypto iv",
  "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)" : null,
  "org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:listStatusIterator(org.apache.hadoop.fs.Path)" : "Return a remote iterator for listing in a directory",
  "org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache(boolean)" : null,
  "org.apache.hadoop.net.NetworkTopology:getLeaves(java.lang.String)" : "return leaves in <i>scope</i>\n   * @param scope a path string\n   * @return leaves nodes under specific scope",
  "org.apache.hadoop.fs.FileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)" : "* The base FileSystem implementation generally has no knowledge\n   * of the capabilities of actual implementations.\n   * Unless it has a way to explicitly determine the capabilities,\n   * this method returns false.\n   * {@inheritDoc}",
  "org.apache.hadoop.crypto.key.CachingKeyProvider$CacheExtension:<init>(org.apache.hadoop.crypto.key.KeyProvider,long,long)" : null,
  "org.apache.hadoop.ha.HealthMonitor:isAlive()" : null,
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:throwIfInvalidBuffer()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:initTables(int,int,byte[],int,byte[])" : null,
  "org.apache.hadoop.fs.FileUtil:deleteImpl(java.io.File,boolean)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:isNativeZlibLoaded()" : null,
  "org.apache.hadoop.security.token.Token:decodeWritable(org.apache.hadoop.io.Writable,java.lang.String)" : "* Modify the writable to the value from the newValue.\n   * @param obj the object to read into\n   * @param newValue the string with the url-safe base64 encoded bytes\n   * @throws IOException",
  "org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache$CacheEntry)" : "* This method handles the following conditions:\n   * <ul>\n   * <li>If retry is not to be processed, return null</li>\n   * <li>If there is no cache entry, add a new entry {@code newEntry} and return\n   * it.</li>\n   * <li>If there is an existing entry, wait for its completion. If the\n   * completion state is {@link CacheEntry#FAILED}, the expectation is that the\n   * thread that waited for completion, retries the request. the\n   * {@link CacheEntry} state is set to {@link CacheEntry#INPROGRESS} again.\n   * <li>If the completion state is {@link CacheEntry#SUCCESS}, the entry is\n   * returned so that the thread that waits for it can can return previous\n   * response.</li>\n   * <ul>\n   * \n   * @return {@link CacheEntry}.",
  "org.apache.hadoop.service.launcher.IrqHandler:raise()" : "* Raise the signal.",
  "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration,java.lang.String)" : "* Get the value of the home dir conf value for specified mount table\n   * @param conf - from this conf\n   * @param mountTableName - the mount table\n   * @return home dir value, null if variable is not in conf",
  "org.apache.hadoop.util.Sets:newHashSet()" : "* Creates a <i>mutable</i>, initially empty {@code HashSet} instance.\n   *\n   * <p><b>Note:</b> if mutability is not required, use ImmutableSet#of()\n   * instead. If {@code E} is an {@link Enum} type, use {@link EnumSet#noneOf}\n   * instead. Otherwise, strongly consider using a {@code LinkedHashSet}\n   * instead, at the cost of increased memory footprint, to get\n   * deterministic iteration behavior.</p>\n   *\n   * @param <E> Generics Type E.\n   * @return a new, empty {@code TreeSet}",
  "org.apache.hadoop.io.MapFile$Merger:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.CallQueueManager:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation)" : null,
  "org.apache.hadoop.util.bloom.Key:equals(java.lang.Object)" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:requestTimeStdDev()" : null,
  "org.apache.hadoop.metrics2.source.JvmMetrics:setGcTimeMonitor(org.apache.hadoop.util.GcTimeMonitor)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawKey()" : "* Fills up the rawKey object with the key returned by the Reader.\n       * @return true if there is a key returned; false, otherwise\n       * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:delegationTokenToJSON(org.apache.hadoop.security.token.Token)" : null,
  "org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:<init>(byte[],int,java.lang.Object,long,boolean)" : null,
  "org.apache.hadoop.util.IdentityHashStore:remove(java.lang.Object)" : "* Retrieve a value associated with a given key, and delete the\n   * relevant entry.\n   *\n   * @param k Generics Type k.\n   * @return Generics Type V.",
  "org.apache.hadoop.conf.StorageUnit$5:getSuffixChar()" : null,
  "org.apache.hadoop.util.ShutdownHookManager:removeShutdownHook(java.lang.Runnable)" : "* Removes a shutdownHook.\n   *\n   * @param shutdownHook shutdownHook to remove.\n   * @return TRUE if the shutdownHook was registered and removed,\n   * FALSE otherwise.",
  "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:preferDirectBuffer()" : null,
  "org.apache.hadoop.fs.audit.AuditConstants:<init>()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getWorkingDirectory()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:get(org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable)" : "* Copy the key and value in one shot into BytesWritables. This is\n         * equivalent to getKey(key); getValue(value);\n         * \n         * @param key\n         *          BytesWritable to hold key.\n         * @param value\n         *          BytesWritable to hold value\n         * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:<init>(java.lang.String)" : null,
  "org.apache.hadoop.fs.TrashPolicy:getCurrentTrashDir(org.apache.hadoop.fs.Path)" : "* Get the current trash directory for path specified based on the Trash\n   * Policy\n   * @param path path to be deleted\n   * @return current trash directory for the path to be deleted\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.Server$Connection:setLastContact(long)" : null,
  "org.apache.hadoop.conf.StorageUnit$6:fromBytes(double)" : null,
  "org.apache.hadoop.crypto.key.KeyShell$CreateCommand:getUsage()" : null,
  "org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String)" : null,
  "org.apache.hadoop.ipc.FairCallQueue:add(java.lang.Object)" : null,
  "org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(boolean)" : null,
  "org.apache.hadoop.io.serializer.WritableSerialization:accept(java.lang.Class)" : null,
  "org.apache.hadoop.metrics2.MetricsTag:description()" : null,
  "org.apache.hadoop.fs.FsShell:getHelp()" : null,
  "org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.lang.Object)" : "* Register the MBean using our standard MBeanName format\n   * \"hadoop:service={@literal <serviceName>,name=<nameName>}\"\n   * Where the {@literal <serviceName> and <nameName>} are the supplied\n   * parameters.\n   *\n   * @param serviceName serviceName.\n   * @param nameName nameName.\n   * @param theMbean - the MBean to register\n   * @return the named used to register the MBean",
  "org.apache.hadoop.fs.QuotaUsage:getHeader()" : "Return the header of the output.\n   * @return the header of the output",
  "org.apache.hadoop.io.UTF8:<init>(org.apache.hadoop.io.UTF8)" : "* Construct from a given string.\n   * @param utf8 input utf8.",
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.FilterFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)" : null,
  "org.apache.hadoop.fs.Path:isWindowsAbsolutePath(java.lang.String,boolean)" : "* Determine whether a given path string represents an absolute path on\n   * Windows. e.g. \"C:/a/b\" is an absolute path. \"C:a/b\" is not.\n   *\n   * @param pathString the path string to evaluate\n   * @param slashed true if the given path is prefixed with \"/\"\n   * @return true if the supplied path looks like an absolute path with a Windows\n   * drive-specifier",
  "org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:setResponse(com.google.protobuf.Message)" : null,
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:startUpload(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(java.util.Date)" : null,
  "org.apache.hadoop.security.JniBasedUnixGroupsMapping:cacheGroupsRefresh()" : null,
  "org.apache.hadoop.io.IOUtils:listDirectory(java.io.File,java.io.FilenameFilter)" : "* Return the complete list of files in a directory as strings.<p>\n   *\n   * This is better than File#listDir because it does not ignore IOExceptions.\n   *\n   * @param dir              The directory to list.\n   * @param filter           If non-null, the filter to use when listing\n   *                         this directory.\n   * @return                 The list of files in the directory.\n   *\n   * @throws IOException     On I/O error",
  "org.apache.hadoop.fs.Options$HandleOpt:moved(boolean)" : "* @param allow If true, resolve references to this entity anywhere in\n     *              the namespace.\n     * @return Handle option encoding parameter.",
  "org.apache.hadoop.metrics2.impl.SinkQueue:checkConsumer()" : null,
  "org.apache.hadoop.util.ExitUtil:getFirstExitException()" : "* @return the first {@code ExitException} thrown, null if none thrown yet.",
  "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:refreshSuperUserGroupsConfiguration()" : null,
  "org.apache.hadoop.security.token.DtUtilShell:main(java.lang.String[])" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfo()" : "* Load file permission information (UNIX symbol rwxrwxrwx, sticky bit info).\n     *\n     * To improve peformance, give priority to native stat() call. First try get\n     * permission information by using native JNI call then fall back to use non\n     * native (ProcessBuilder) call in case native lib is not loaded or native\n     * call is not successful",
  "org.apache.hadoop.net.unix.DomainSocket:getAttribute(int)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:checkInputBuffers(byte[][])" : "* Check and ensure the buffers are of the desired length.\n   * @param buffers the buffers to check",
  "org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream)" : null,
  "org.apache.hadoop.util.GenericOptionsParser:buildGeneralOptions(org.apache.commons.cli.Options)" : "* @return Specify properties of each generic option.\n   * <i>Important</i>: as {@link Option} is not thread safe, subclasses\n   * must synchronize use on {@code Option.class}\n   * @param opts input opts.",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:<init>()" : null,
  "org.apache.hadoop.util.Progress:getProgressWeightage(int)" : "* returns progress weightage of the given phase\n   * @param phaseNum the phase number of the phase(child node) for which we need\n   *                 progress weightage\n   * @return returns the progress weightage of the specified phase",
  "org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseReadyBlock(int)" : "* If no blocks were released after calling releaseDoneBlocks() a few times,\n   * we may end up waiting forever. To avoid that situation, we try releasing\n   * a 'ready' block farthest away from the given block.",
  "org.apache.hadoop.crypto.JceCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.FindClass:usage(java.lang.String[])" : "* Print a usage message\n   * @param args the command line arguments\n   * @return an exit code",
  "org.apache.hadoop.util.LineReader:readDefaultLine(org.apache.hadoop.io.Text,int,int)" : "* Read a line terminated by one of CR, LF, or CRLF.",
  "org.apache.hadoop.fs.FileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])" : "* Hook to implement support for {@link PathHandle} operations.\n   * @param stat Referent in the target FileSystem\n   * @param opt Constraints that determine the validity of the\n   *            {@link PathHandle} reference.\n   * @return path handle.",
  "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:setHeader(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.io.erasurecode.CodecUtil:<init>()" : null,
  "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:toString()" : "* @return  the value of the metric",
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:release()" : "* Should be called when release this coder. Good chance to release encoding\n   * or decoding buffers",
  "org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorType:getConfigName()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartA()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:enableIOStatisticsContext()" : "* A method to enable IOStatisticsContext to override if set otherwise in\n   * the configurations for tests.",
  "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:setConfigurationFromURI(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Set configuration from UI.\n   *\n   * @param uri\n   * @param conf\n   * @throws IOException",
  "org.apache.hadoop.util.BlockingThreadPoolExecutorService:getNamedThreadFactory(java.lang.String)" : "* Returns a {@link java.util.concurrent.ThreadFactory} that names each\n   * created thread uniquely,\n   * with a common prefix.\n   *\n   * @param prefix The prefix of every created Thread's name\n   * @return a {@link java.util.concurrent.ThreadFactory} that names threads",
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerGauge(java.lang.String,java.util.concurrent.atomic.AtomicInteger)" : "* Add a gauge statistic to dynamically return the\n   * latest value of the source.\n   * @param key key of this statistic\n   * @param source atomic int gauge\n   * @return the builder.",
  "org.apache.hadoop.fs.shell.FsCommand:getCommandName()" : null,
  "org.apache.hadoop.io.ObjectWritable:set(java.lang.Object)" : "* Reset the instance.\n   * @param instance instance.",
  "org.apache.hadoop.metrics2.util.Quantile:compareTo(org.apache.hadoop.metrics2.util.Quantile)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:getDelegationTokens(java.lang.String)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:entryToString(java.util.Map$Entry)" : "* Convert an entry to the string format used in logging.\n   *\n   * @param entry entry to evaluate\n   * @param <E> entry type\n   * @return formatted string",
  "org.apache.hadoop.util.VersionInfo:getProtocVersion()" : "* Returns the protoc version used for the build.\n   * @return the protoc version",
  "org.apache.hadoop.ipc.CallerContext$Builder:getSignature()" : "* Get the signature.\n     * @return the signature.",
  "org.apache.hadoop.util.IdentityHashStore:get(java.lang.Object)" : "* Retrieve a value associated with a given key.\n   *\n   * @param k Generics Type k.\n   * @return Generics Type V.",
  "org.apache.hadoop.ipc.Server$Call:getPriorityLevel()" : null,
  "org.apache.hadoop.security.Groups:getGroups(java.lang.String)" : "* Get the group memberships of a given user.\n   * If the user's group is not cached, this method may block.\n   * Note this method can be expensive as it involves Set {@literal ->} List\n   * conversion. For user with large group membership\n   * (i.e., {@literal >} 1000 groups), we recommend using getGroupSet\n   * to avoid the conversion and fast membership look up via contains().\n   * @param user User's name\n   * @return the group memberships of the user as list\n   * @throws IOException if user does not exist\n   * @deprecated Use {@link #getGroupsSet(String user)} instead.",
  "org.apache.hadoop.util.bloom.DynamicBloomFilter:add(org.apache.hadoop.util.bloom.Key)" : null,
  "org.apache.hadoop.security.RuleBasedLdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:needsInput()" : "* Returns true if the input data buffer is empty and\n   * {@link #setInput(byte[], int, int)} should be called to\n   * provide more input.\n   *\n   * @return <code>true</code> if the input data buffer is empty and\n   *         {@link #setInput(byte[], int, int)} should be called in\n   *         order to provide more input.",
  "org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>()" : null,
  "org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.shell.CopyCommands$Get:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:close()" : null,
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getData(int)" : null,
  "org.apache.hadoop.ipc.Server:channelIO(java.nio.channels.ReadableByteChannel,java.nio.channels.WritableByteChannel,java.nio.ByteBuffer)" : "* Helper for {@link #channelRead(ReadableByteChannel, ByteBuffer)}\n   * and {@link #channelWrite(WritableByteChannel, ByteBuffer)}. Only\n   * one of readCh or writeCh should be non-null.\n   * \n   * @see #channelRead(ReadableByteChannel, ByteBuffer)\n   * @see #channelWrite(WritableByteChannel, ByteBuffer)",
  "org.apache.hadoop.fs.FileSystem:getServerDefaults()" : "* Return a set of server default configuration values.\n   * @return server default configuration values\n   * @throws IOException IO failure\n   * @deprecated use {@link #getServerDefaults(Path)} instead",
  "org.apache.hadoop.conf.Configuration$DeprecationContext:<init>(org.apache.hadoop.conf.Configuration$DeprecationContext,org.apache.hadoop.conf.Configuration$DeprecationDelta[])" : "* Create a new DeprecationContext by copying a previous DeprecationContext\n     * and adding some deltas.\n     *\n     * @param other   The previous deprecation context to copy, or null to start\n     *                from nothing.\n     * @param deltas  The deltas to apply.",
  "org.apache.hadoop.util.Options$ClassOption:getValue()" : null,
  "org.apache.hadoop.conf.Configuration$Resource:getRestrictParserDefault(java.lang.Object)" : null,
  "org.apache.hadoop.util.ProgramDriver$ProgramDescription:<init>(java.lang.Class,java.lang.String)" : "* Create a description of an example program.\n     * @param mainClass the class with the main for the example program\n     * @param description a string to display to the user in help messages\n     * @throws SecurityException if we can't use reflection\n     * @throws NoSuchMethodException if the class doesn't have a main method",
  "org.apache.hadoop.util.KMSUtil:parseJSONKeyVersion(java.util.Map)" : null,
  "org.apache.hadoop.ipc.Client$Connection:writeConnectionHeader(org.apache.hadoop.ipc.Client$IpcStreams)" : "* Write the connection header - this is sent when connection is established\n     * +----------------------------------+\n     * |  \"hrpc\" 4 bytes                  |      \n     * +----------------------------------+\n     * |  Version (1 byte)                |\n     * +----------------------------------+\n     * |  Service Class (1 byte)          |\n     * +----------------------------------+\n     * |  AuthProtocol (1 byte)           |      \n     * +----------------------------------+",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getQuotaUsage(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.ComparableVersion:<init>(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.impl.MetricGaugeInt:visit(org.apache.hadoop.metrics2.MetricsVisitor)" : null,
  "org.apache.hadoop.fs.FilterFs:makeQualified(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:hashCode()" : null,
  "org.apache.hadoop.metrics2.lib.MutableRollingAverages:replaceScheduledTask(int,long,java.util.concurrent.TimeUnit)" : "* This method is for testing only to replace the scheduledTask.",
  "org.apache.hadoop.metrics2.impl.SinkQueue:front()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:convertToByteArrayState()" : "* Convert to a ByteArrayEncodingState when it's backed by on-heap arrays.",
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addCounterFunction(java.lang.String,java.util.function.Function)" : "* add a mapping of a key to a counter function.\n   * @param key the key\n   * @param eval the evaluator",
  "org.apache.hadoop.fs.FSDataOutputStream$PositionCache:getPos()" : null,
  "org.apache.hadoop.ipc.CallQueueManager:getPriorityLevel(org.apache.hadoop.ipc.Schedulable)" : null,
  "org.apache.hadoop.ipc.CallQueueManager:<init>(java.util.concurrent.BlockingQueue,org.apache.hadoop.ipc.RpcScheduler,boolean,boolean)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.authentication.client.ConnectionConfigurator)" : "* Creates an <code>DelegationTokenAuthenticatedURL</code> using the default\n   * {@link DelegationTokenAuthenticator} class.\n   *\n   * @param connConfigurator a connection configurator.",
  "org.apache.hadoop.io.NullWritable:compareTo(org.apache.hadoop.io.NullWritable)" : null,
  "org.apache.hadoop.util.bloom.CountingBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key)" : null,
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem:getWorkingDirectory()" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:getBytesReadErasureCoded()" : "* Get the total number of bytes read on erasure-coded files.\n     * @return the number of bytes",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:<init>()" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.DataOutputBuffer)" : "@deprecated Call {@link #nextRaw(DataOutputBuffer,SequenceFile.ValueBytes)}.",
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:buildStaticChecked()" : "* Returns the first valid implementation as a StaticMethod or throws a\n     * NoSuchMethodException if there is none.\n     * @return a {@link StaticMethod} with a valid implementation\n     * @throws IllegalStateException if the method is not static\n     * @throws NoSuchMethodException if no implementation was found",
  "org.apache.hadoop.fs.QuotaUsage$Builder:<init>()" : null,
  "org.apache.hadoop.fs.FileSystem:removeFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)" : null,
  "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:addIntHeader(java.lang.String,int)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:getCompressedSize()" : null,
  "org.apache.hadoop.ha.ZKFailoverController:run(java.lang.String[])" : null,
  "org.apache.hadoop.io.SortedMapWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:newInstance(java.lang.Class,java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getOutputStream()" : "* Get the output stream for BlockAppender's consumption.\n       * \n       * @return the output stream suitable for writing block data.",
  "org.apache.hadoop.fs.viewfs.RegexMountPoint:getRegexGroupValueFromMather(java.util.regex.Matcher,java.lang.String)" : "* Get matched capture group value from regex matched string. E.g.\n   * Regex: ^/user/(?<username>\\\\w+), regexGroupNameOrIndexStr: userName\n   * then /user/hadoop should return hadoop while call\n   * getRegexGroupValueFromMather(matcher, usersName)\n   * or getRegexGroupValueFromMather(matcher, 1)\n   *\n   * @param srcMatcher - the matcher to be use\n   * @param regexGroupNameOrIndexStr - the regex group name or index\n   * @return - Null if no matched group named regexGroupNameOrIndexStr found.",
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)" : null,
  "org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String,byte[])" : null,
  "org.apache.hadoop.security.UserGroupInformation:getGroupsSet()" : "* Get the groups names for the user as a Set.\n   * @return the set of users with the primary group first. If the command\n   *     fails, it returns an empty set.",
  "org.apache.hadoop.fs.DelegateToFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.DataChecksum:verifyChunkedSums(java.nio.ByteBuffer,java.nio.ByteBuffer,java.lang.String,long)" : "* Verify that the given checksums match the given data.\n   * \n   * The 'mark' of the ByteBuffer parameters may be modified by this function,.\n   * but the position is maintained.\n   *  \n   * @param data the DirectByteBuffer pointing to the data to verify.\n   * @param checksums the DirectByteBuffer pointing to a series of stored\n   *                  checksums\n   * @param fileName the name of the file being read, for error-reporting\n   * @param basePos the file position to which the start of 'data' corresponds\n   * @throws ChecksumException if the checksums do not match",
  "org.apache.hadoop.io.SecureIOUtils:createForWrite(java.io.File,int)" : "* Open the specified File for write access, ensuring that it does not exist.\n   * @param f the file that we want to create\n   * @param permissions we want to have on the file (if security is enabled)\n   *\n   * @throws AlreadyExistsException if the file already exists\n   * @throws IOException if any other error occurred\n   * @return createForWrite FileOutputStream.",
  "org.apache.hadoop.io.DataInputBuffer:<init>()" : "Constructs a new empty buffer.",
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(byte[],int,int)" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:<init>(org.apache.hadoop.fs.statistics.IOStatistics)" : "* Construct, taking a snapshot of the source statistics data\n   * if the source is non-null.\n   * If the source is null, the empty maps are created\n   * @param source statistics source. Nullable.",
  "org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization:getWriter(java.lang.Class)" : null,
  "org.apache.hadoop.ipc.FairCallQueue:removeNextElement()" : "* Returns an element first non-empty queue equal to the priority returned\n   * by the multiplexer or scans from highest to lowest priority queue.\n   *\n   * Caller must always acquire a semaphore permit before invoking.\n   *\n   * @return the first non-empty queue with less priority, or null if\n   * everything was empty",
  "org.apache.hadoop.util.Shell:setTimedOut()" : "* Declare that the command has timed out.\n   *",
  "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:hasMoreThanOneSourcePaths(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.HarFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:delete(org.apache.hadoop.fs.Path,boolean)" : "* Delete the given path to a file or directory.\n   * @param p the path to delete\n   * @param recursive to delete sub-directories\n   * @return true if the file or directory and all its contents were deleted\n   * @throws IOException if p is non-empty and recursive is false",
  "org.apache.hadoop.security.token.Token:getService()" : "* Get the service on which the token is supposed to be used.\n   * @return the service name",
  "org.apache.hadoop.net.NetworkTopology:getDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)" : "Return the distance between two nodes\n   * It is assumed that the distance from one node to its parent is 1\n   * The distance between two nodes is calculated by summing up their distances\n   * to their closest common ancestor.\n   * @param node1 one node\n   * @param node2 another node\n   * @return the distance between node1 and node2 which is zero if they are the same\n   *  or {@link Integer#MAX_VALUE} if node1 or node2 do not belong to the cluster",
  "org.apache.hadoop.security.alias.CredentialShell$Command:printProviderWritten()" : null,
  "org.apache.hadoop.jmx.JMXJsonServlet:extraCheck(java.lang.Object)" : "* In case you need to modify the logic, how java objects transforms to json,\n   * you can overwrite this method to return true in case special handling\n   * @param value the object what should be judged\n   * @return true, if it needs special transformation",
  "org.apache.hadoop.fs.shell.Delete$Rm:moveToTrash(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators:cleanupRemoteIterator(org.apache.hadoop.fs.RemoteIterator)" : "* Clean up after an iteration.\n   * If the log is at debug, calculate and log the IOStatistics.\n   * If the iterator is closeable, cast and then cleanup the iterator\n   * @param source iterator source\n   * @param <T> type of source",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:writeKey(java.io.OutputStream)" : "* Writing the key to the output stream. This method avoids copying key\n         * buffer from Scanner into user buffer, then writing to the output\n         * stream.\n         * \n         * @param out\n         *          The output stream\n         * @return the length of the key.\n         * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.ipc.RpcWritable$Buffer:readFrom(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.metrics2.AbstractMetric:<init>(org.apache.hadoop.metrics2.MetricsInfo)" : "* Construct the metric\n   * @param info  about the metric",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey)" : null,
  "org.apache.hadoop.fs.FileUtil:getCanonicalPath(java.lang.String,java.io.File)" : "* Gets the canonical path for the given path.\n   *\n   * @param path      The path for which the canonical path needs to be computed.\n   * @param parentDir The parent directory to use if the path is a relative path.\n   * @return The canonical path of the given path.",
  "org.apache.hadoop.ipc.ProtobufRpcEngine$Server:registerForDeferredResponse()" : null,
  "org.apache.hadoop.fs.shell.PathData:compareTo(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_reset()" : "* Reset the context's IOStatistics.\n   * {@code IOStatisticsContext#reset()}\n   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",
  "org.apache.hadoop.util.AutoCloseableLock:isLocked()" : "* A wrapper method that makes a call to {@code isLocked()} of\n   * the underlying {@code ReentrantLock} object.\n   *\n   * Queries if this lock is held by any thread. This method is\n   * designed for use in monitoring of the system state,\n   * not for synchronization control.\n   *\n   * @return {@code true} if any thread holds this lock and\n   *         {@code false} otherwise",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:createIdentifier()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:init(java.lang.String)" : "* Initialized the metrics system with a prefix.\n   * @param prefix  the system will look for configs with the prefix\n   * @return the metrics system object itself",
  "org.apache.hadoop.crypto.CryptoInputStream:setReadahead(java.lang.Long)" : null,
  "org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String)" : "* See {@link Configuration#get(String)}.",
  "org.apache.hadoop.ipc.RpcConstants:<init>()" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersion(java.lang.String)" : null,
  "org.apache.hadoop.util.IntrusiveCollection:add(java.lang.Object)" : "* Add an element to the end of the list.\n   * \n   * @param elem     The new element to add.\n   * @return add result.",
  "org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.fs.Path)" : "* Add a configuration resource. \n   * \n   * The properties of this resource will override properties of previously \n   * added resources, unless they were marked <a href=\"#Final\">final</a>. \n   * \n   * @param file file-path of resource to be added, the local filesystem is\n   *             examined directly to find the resource, without referring to \n   *             the classpath.",
  "org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,byte[])" : "* Writes bytes to a file. This utility method opens the file for writing,\n   * creating the file if it does not exist, or overwrites an existing file. All\n   * bytes in the byte array are written to the file.\n   *\n   * @param fileContext the file context with which to create the file\n   * @param path the path to the file\n   * @param bytes the byte array with the bytes to write\n   *\n   * @return the file context\n   *\n   * @throws NullPointerException if any of the arguments are {@code null}\n   * @throws IOException if an I/O error occurs creating or writing to the file",
  "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:compareAndSet(float,float)" : null,
  "org.apache.hadoop.fs.BufferedFSInputStream:readVectored(java.util.List,java.util.function.IntFunction)" : null,
  "org.apache.hadoop.net.unix.DomainSocket:unreference(boolean)" : null,
  "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:interceptSource(java.lang.String)" : "* Source won't be changed in the interceptor.\n   *\n   * @return source param string passed in.",
  "org.apache.hadoop.util.CpuTimeTracker:<init>(long)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:joinElectionInternal()" : null,
  "org.apache.hadoop.conf.Configuration:getQuietMode()" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,float)" : "* Create a mutable float gauge\n   * @param name  of the metric\n   * @param desc  metric description\n   * @param iVal  initial value\n   * @return a new gauge object",
  "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)" : "* Create the named map using the named key comparator.\n     * @deprecated Use Writer(Configuration, Path, Option...) instead.\n     *\n     * @param conf configuration.\n     * @param fs FileSystem.\n     * @param dirName dirName.\n     * @param comparator comparator.\n     * @param valClass valClass.\n     * @param compress CompressionType.\n     * @param codec codec.\n     * @param progress progress.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.TrashPolicyDefault$Emptier:ceiling(long,long)" : null,
  "org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(int,long,java.lang.Class)" : "* Get a server protocol's signature\n   * \n   * @param clientMethodsHashCode client protocol methods hashcode\n   * @param serverVersion server protocol version\n   * @param protocol protocol\n   * @return the server's protocol signature",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:close()" : null,
  "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:<init>()" : null,
  "org.apache.hadoop.metrics2.lib.MethodMetric:newImpl(org.apache.hadoop.metrics2.annotation.Metric$Type)" : null,
  "org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.PathHandle)" : "* Open an FSDataInputStream matching the PathHandle instance. The\n   * implementation may encode metadata in PathHandle to address the\n   * resource directly and verify that the resource referenced\n   * satisfies constraints specified at its construciton.\n   * @param fd PathHandle object returned by the FS authority.\n   * @throws InvalidPathHandleException If {@link PathHandle} constraints are\n   *                                    not satisfied\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException If {@link #open(PathHandle, int)}\n   *                                       not overridden by subclass\n   * @return input stream.",
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:setInput(byte[],int,int)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getWorkFactor(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.Server:getQueueClass(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getInstance(java.lang.String)" : null,
  "org.apache.hadoop.io.BytesWritable:set(org.apache.hadoop.io.BytesWritable)" : "* Set the BytesWritable to the contents of the given newData.\n   *\n   * @param newData the value to set this BytesWritable to.",
  "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:chooseServerAlias(java.lang.String,java.security.Principal[],java.net.Socket)" : null,
  "org.apache.hadoop.util.Shell$ExitCodeException:<init>(int,java.lang.String)" : null,
  "org.apache.hadoop.io.BytesWritable$Comparator:<init>()" : null,
  "org.apache.hadoop.ipc.Server$Handler:requeueCall(org.apache.hadoop.ipc.Server$Call)" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String)" : "* Validates that the expression (that checks a field is valid) is true.\n   * @param isValid indicates whether the given argument is valid.\n   * @param argName the name of the argument being validated.",
  "org.apache.hadoop.metrics2.lib.MutableRates:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)" : null,
  "org.apache.hadoop.conf.Configuration:setQuietMode(boolean)" : "* Set the quietness-mode. \n   * \n   * In the quiet-mode, error and informational messages might not be logged.\n   * \n   * @param quietmode <code>true</code> to set quiet-mode on, <code>false</code>\n   *              to turn it off.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint:<init>(org.apache.hadoop.fs.Path,java.lang.String[])" : null,
  "org.apache.hadoop.io.DefaultStringifier:fromString(java.lang.String)" : null,
  "org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter,boolean)" : "* File Context constructor for use by {@link GlobBuilder}.\n   * @param fc file context\n   * @param pathPattern path pattern\n   * @param filter optional filter\n   * @param resolveSymlinks should symlinks be resolved.",
  "org.apache.hadoop.util.functional.FunctionalIO:toUncheckedIOExceptionSupplier(org.apache.hadoop.util.functional.CallableRaisingIOE)" : "* Wrap a {@link CallableRaisingIOE} as a {@link Supplier}.\n   * @param call call to wrap\n   * @param <T> type of result\n   * @return a supplier which invokes the call.",
  "org.apache.hadoop.fs.Path:getFileSystem(org.apache.hadoop.conf.Configuration)" : "* Return the FileSystem that owns this Path.\n   *\n   * @param conf the configuration to use when resolving the FileSystem\n   * @return the FileSystem that owns this Path\n   * @throws java.io.IOException thrown if there's an issue resolving the\n   * FileSystem",
  "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:getSource()" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_maximums(java.io.Serializable)" : "* Get the maximums of an IOStatisticsSnapshot.\n   * @param source source of statistics.\n   * @return the map of maximums.",
  "org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStart()" : null,
  "org.apache.hadoop.io.WritableUtils:readVInt(java.io.DataInput)" : "* Reads a zero-compressed encoded integer from input stream and returns it.\n   * @param stream Binary input stream\n   * @throws IOException raised on errors performing I/O.\n   * @return deserialized integer from stream.",
  "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getTotalCallVolume()" : null,
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getFileSystem()" : "* Return the supplied file system for testing or otherwise get a new file\n   * system.\n   *\n   * @return the file system to use\n   * @throws MetricsException thrown if the file system could not be retrieved",
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,java.lang.String)" : "* Set mandatory option to the Builder.\n   *\n   * If the option is not supported or unavailable on the {@link FileSystem},\n   * the client should expect {@link #build()} throws IllegalArgumentException.",
  "org.apache.hadoop.fs.store.ByteBufferInputStream:toString()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)" : "* Cancels a delegation token from the server end-point. It does not require\n   * being authenticated by the configured <code>Authenticator</code>.\n   *\n   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs\n   * are supported.\n   * @param token the authentication token with the Delegation Token to cancel.\n   * @param dToken abstract delegation token identifier.\n   * @param doAsUser the user to do as, which will be the token owner.\n   * @throws IOException if an IO error occurred.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:closeAll()" : null,
  "org.apache.hadoop.log.LogLevel$Servlet:process(org.apache.log4j.Logger,java.lang.String,java.io.PrintWriter)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:upperBound(org.apache.hadoop.io.file.tfile.RawComparable)" : "* @param key\n     *          input key.\n     * @return the ID of the first block that contains key > input key. Or -1\n     *         if no such block exists.",
  "org.apache.hadoop.crypto.key.KeyProvider$Options:getBitLength()" : null,
  "org.apache.hadoop.fs.FileContext:makeQualified(org.apache.hadoop.fs.Path)" : "* Make the path fully qualified if it is isn't. \n   * A Fully-qualified path has scheme and authority specified and an absolute\n   * path.\n   * Use the default file system and working dir in this FileContext to qualify.\n   * @param path the path.\n   * @return qualified path",
  "org.apache.hadoop.fs.shell.Head:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.io.compress.SplitCompressionInputStream:<init>(java.io.InputStream,long,long)" : null,
  "org.apache.hadoop.metrics2.lib.MethodMetric$2:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)" : null,
  "org.apache.hadoop.util.Shell:getQualifiedBin(java.lang.String)" : "*  Fully qualify the path to a binary that should be in a known hadoop\n   *  bin location. This is primarily useful for disambiguating call-outs\n   *  to executable sub-components of Hadoop to avoid clashes with other\n   *  executables that may be in the path.  Caveat:  this call doesn't\n   *  just format the path to the bin directory.  It also checks for file\n   *  existence of the composed path. The output of this call should be\n   *  cached by callers.\n   *\n   * @param executable executable\n   * @return executable file reference\n   * @throws FileNotFoundException if the path does not exist",
  "org.apache.hadoop.security.KDiag:validateHadoopTokenFiles(org.apache.hadoop.conf.Configuration)" : "* Validate that hadoop.token.files (if specified) exist and are valid.\n   * @throws ClassNotFoundException\n   * @throws SecurityException\n   * @throws NoSuchMethodException\n   * @throws KerberosDiagsFailure",
  "org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Object)" : null,
  "org.apache.hadoop.net.unix.DomainSocket:getEffectivePath(java.lang.String,int)" : "* Given a path and a port, compute the effective path by replacing\n   * occurrences of _PORT with the port.  This is mainly to make it \n   * possible to run multiple DataNodes locally for testing purposes.\n   *\n   * @param path            The source path\n   * @param port            Port number to use\n   *\n   * @return                The effective path",
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:hasCapability(java.lang.String)" : null,
  "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)" : null,
  "org.apache.hadoop.fs.ftp.FTPInputStream:<init>(java.io.InputStream,org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.FileSystem$Statistics)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:allowVerboseDump()" : "* Allow to dump verbose info during encoding/decoding.\n   * @return true if it's allowed to do verbose dump, false otherwise.",
  "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:getCompressionAlgorithm()" : null,
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:decompress(byte[],int,int)" : null,
  "org.apache.hadoop.fs.FsStatus:getRemaining()" : "* Return the number of remaining bytes on the file system.\n   * @return remaining.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addTokenForOwnerStats(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : "* Add token stats to the owner to token count mapping.\n   *\n   * @param id token id.",
  "org.apache.hadoop.metrics2.impl.MetricGaugeLong:visit(org.apache.hadoop.metrics2.MetricsVisitor)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getBitLength()" : null,
  "org.apache.hadoop.io.NullWritable:<init>()" : null,
  "org.apache.hadoop.fs.GetSpaceUsed$Builder:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:getType()" : null,
  "org.apache.hadoop.ipc.Server$ConnectionManager:incrUserConnections(java.lang.String)" : null,
  "org.apache.hadoop.fs.LocalDirAllocator:<init>(java.lang.String)" : "* Create an allocator object.\n   * @param contextCfgItemName contextCfgItemName.",
  "org.apache.hadoop.fs.FSOutputSummer:int2byte(int,byte[])" : null,
  "org.apache.hadoop.ipc.Server$Connection:processSaslToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)" : null,
  "org.apache.hadoop.util.functional.TaskPool$Builder:resetStatisticsContext()" : "* Reset the statistics context if it was set earlier.\n     * This unbinds the current thread from any statistics\n     * context.",
  "org.apache.hadoop.http.HttpServer2:addServlet(java.lang.String,java.lang.String,java.lang.Class)" : "* Add a servlet in the server.\n   * @param name The name of the servlet (can be passed as null)\n   * @param pathSpec The path spec for the servlet\n   * @param clazz The servlet class",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:decodeToken(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.net.SocketOutputStream:close()" : null,
  "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:init(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.bzip2.CRC:getFinalCRC()" : null,
  "org.apache.hadoop.fs.HardLink:createHardLinkMult(java.io.File,java.lang.String[],java.io.File)" : "* Creates hardlinks from multiple existing files within one parent\n   * directory, into one target directory.\n   * @param parentDir - directory containing source files\n   * @param fileBaseNames - list of path-less file names, as returned by \n   *                        parentDir.list()\n   * @param linkDir - where the hardlinks should be put. It must already exist.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.VIntWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.io.BoundedByteArrayOutputStream:reset(int)" : "* Reset the limit \n   * @param newlim New Limit",
  "org.apache.hadoop.util.concurrent.AsyncGetFuture:isDone()" : null,
  "org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String,int)" : "* @return Write a UTF8 encoded string with a maximum size to out.\n   *\n   * @param out input out.\n   * @param s input s.\n   * @param maxLength input maxLength.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.FilterFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.fs.HarFileSystem:getPathInHar(org.apache.hadoop.fs.Path)" : "* this method returns the path \n   * inside the har filesystem.\n   * this is relative path inside \n   * the har filesystem.\n   * @param path the fully qualified path in the har filesystem.\n   * @return relative path in the filesystem.",
  "org.apache.hadoop.io.SequenceFile$Writer:hsync()" : null,
  "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.net.URI[])" : "* Add a LinkMerge to the config for the default mount table.\n   *\n   * @param conf configuration.\n   * @param targets targets array.",
  "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:isEmpty()" : "* Does table have any rows \n     * @return boolean",
  "org.apache.hadoop.security.UserGroupInformation:getTokens()" : "* Obtain the collection of tokens associated with this user.\n   * \n   * @return an unmodifiable collection of tokens associated with user",
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)" : null,
  "org.apache.hadoop.io.SortedMapWritable:<init>()" : "default constructor.",
  "org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Get an instance of the configured TrashPolicy based on the value\n   * of the configuration parameter fs.trash.classname.\n   *\n   * @param conf the configuration to be used\n   * @param fs the file system to be used\n   * @param home the home directory\n   * @return an instance of TrashPolicy\n   * @deprecated Use {@link #getInstance(Configuration, FileSystem)} instead.",
  "org.apache.hadoop.security.KDiag:validateSasl(java.lang.String)" : "* Try to load the SASL resolver.\n   * @param saslPropsResolverKey key for the SASL resolver",
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor:delegate()" : null,
  "org.apache.hadoop.security.token.DtUtilShell$Print:execute()" : null,
  "org.apache.hadoop.security.SaslPropertiesResolver:getClientProperties(java.net.InetAddress)" : "* Identify the Sasl Properties to be used for a connection with a server.\n   * @param serverAddress server's address\n   * @return the sasl properties to be used for the connection.",
  "org.apache.hadoop.security.token.delegation.web.ServletUtils:getParameter(javax.servlet.http.HttpServletRequest,java.lang.String)" : "* Extract a query string parameter without triggering http parameters\n   * processing by the servlet container.\n   *\n   * @param request the request\n   * @param name the parameter to get the value.\n   * @return the parameter value, or <code>NULL</code> if the parameter is not\n   * defined.\n   * @throws IOException thrown if there was an error parsing the query string.",
  "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getOwner()" : null,
  "org.apache.hadoop.fs.shell.Command:getListingGroupSize()" : "* While using iterator method for listing for a path, whether to group items\n   * and process as array? If so what is the size of array?\n   * @return size of the grouping array.",
  "org.apache.hadoop.conf.Configuration:setDouble(java.lang.String,double)" : "* Set the value of the <code>name</code> property to a <code>double</code>.\n   * \n   * @param name property name.\n   * @param value property value.",
  "org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:getCodec()" : null,
  "org.apache.hadoop.security.SaslRpcServer:getIdentifier(java.lang.String,org.apache.hadoop.security.token.SecretManager)" : null,
  "org.apache.hadoop.fs.LocalFileSystem:pathToFile(org.apache.hadoop.fs.Path)" : "* Convert a path to a File.\n   * @param path the path.\n   * @return file.",
  "org.apache.hadoop.security.ShellBasedIdMapping:reportDuplicateEntry(java.lang.String,java.lang.Integer,java.lang.String,java.lang.Integer,java.lang.String)" : null,
  "org.apache.hadoop.http.ProfileServlet:setResponseHeader(javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.util.StringUtils:format(java.lang.String,java.lang.Object[])" : "* The same as String.format(Locale.ENGLISH, format, objects).\n   * @param format format.\n   * @param objects objects.\n   * @return format string.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:setAuthHandlerClass(java.util.Properties)" : "* Set AUTH_TYPE property to the name of the corresponding authentication\n   * handler class based on the input properties.\n   * @param props input properties.\n   * @throws ServletException servlet exception.",
  "org.apache.hadoop.metrics2.impl.MetricGaugeLong:value()" : null,
  "org.apache.hadoop.crypto.key.KeyProviderFactory:getProviders(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.http.HttpServer2:toString()" : null,
  "org.apache.hadoop.service.launcher.InterruptEscalator:isSignalAlreadyReceived()" : "* Flag set if a signal has been received.\n   * @return true if there has been one interrupt already.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>()" : "* This is the  constructor with the signature needed by\n   * {@link FileSystem#createFileSystem(URI, Configuration)}\n   *\n   * After this constructor is called initialize() is called.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.RawLocalFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.Text:getBytes()" : "* Returns the raw bytes; however, only data up to {@link #getLength()} is\n   * valid. Please use {@link #copyBytes()} if you\n   * need the returned array to be precisely the length of the data.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockContainsKey(org.apache.hadoop.io.file.tfile.RawComparable,boolean)" : "* if greater is true then returns the beginning location of the block\n     * containing the key strictly greater than input key. if greater is false\n     * then returns the beginning location of the block greater than equal to\n     * the input key\n     * \n     * @param key\n     *          the input key\n     * @param greater\n     *          boolean flag\n     * @return\n     * @throws IOException",
  "org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot:getMaxSuppressedWait()" : null,
  "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:getName()" : null,
  "org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,boolean,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)" : "* Construct the preferred type of SequenceFile Writer.\n   * @param fs The configured filesystem.\n   * @param conf The configuration.\n   * @param name The name of the file.\n   * @param keyClass The 'key' type.\n   * @param valClass The 'value' type.\n   * @param bufferSize buffer size for the underlaying outputstream.\n   * @param replication replication factor for the file.\n   * @param blockSize block size for the file.\n   * @param createParent create parent directory if non-existent\n   * @param compressionType The compression type.\n   * @param codec The compression codec.\n   * @param metadata The metadata of the file.\n   * @return Returns the handle to the constructed SequenceFile Writer.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.http.HttpServer2:addHandlerAtEnd(org.eclipse.jetty.server.Handler)" : "* Add the given handler to the end of the list of handlers.\n   *\n   * @param handler The handler to add",
  "org.apache.hadoop.fs.shell.CommandWithDestination:setDirectWrite(boolean)" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:constructOldPath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:requiredSerializationClasses()" : "* What classes are needed to deserialize this class?\n   * Needed to securely unmarshall this from untrusted sources.\n   * @return a list of required classes to deserialize the data.",
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:<init>(byte[],int,int,long)" : null,
  "org.apache.hadoop.conf.Configuration:asXmlDocument(java.lang.String,org.apache.hadoop.conf.ConfigRedactor)" : "* Return the XML DOM corresponding to this Configuration.",
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCurrentKey(java.lang.String)" : null,
  "org.apache.hadoop.util.bloom.Key:<init>(byte[])" : "* Constructor.\n   * <p>\n   * Builds a key with a default weight.\n   * @param value The byte value of <i>this</i> key.",
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isSymlink()" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(org.apache.hadoop.metrics2.MetricsInfo,int)" : "* Create a mutable integer counter\n   * @param info  metadata of the metric\n   * @param iVal  initial value\n   * @return a new counter object",
  "org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String,java.lang.String,java.lang.String)" : "* Constructor for the helper class to configure the ZooKeeper client connection.\n     * @param zkPrincipal Optional.\n     * @param kerberosPrincipal Optional. Use along with kerberosKeytab.\n     * @param kerberosKeytab Optional. Use along with kerberosPrincipal.",
  "org.apache.hadoop.io.SequenceFile$Reader:start(long)" : "* Create an option to specify the starting byte to read.\n     * @param value the number of bytes to skip over\n     * @return a new option",
  "org.apache.hadoop.fs.viewfs.ChRootedFs:listStatusIterator(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getLen()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByByteRange(long,long)" : "* Get a scanner that covers a portion of TFile based on byte offsets.\n     * \n     * @param offset\n     *          The beginning byte offset in the TFile.\n     * @param length\n     *          The length of the region.\n     * @return The actual coverage of the returned scanner tries to match the\n     *         specified byte-region but always round up to the compression\n     *         block boundaries. It is possible that the returned scanner\n     *         contains zero key-value pairs even if length is positive.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,boolean)" : "* Set optional boolean parameter for the Builder.\n   *\n   * @see #opt(String, String)",
  "org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(org.apache.hadoop.fs.Path,java.util.Date)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.dynamic.DynConstructors$MakeAccessible:<init>(java.lang.reflect.Constructor)" : null,
  "org.apache.hadoop.io.MapFile$Writer$ComparatorOption:getValue()" : null,
  "org.apache.hadoop.io.compress.DecompressorStream:read()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumParityUnits()" : null,
  "org.apache.hadoop.ipc.RefreshResponse:getReturnCode()" : null,
  "org.apache.hadoop.fs.FileContext:delete(org.apache.hadoop.fs.Path,boolean)" : "* Delete a file.\n   * @param f the path to delete.\n   * @param recursive if path is a directory and set to \n   * true, the directory is deleted else throws an exception. In\n   * case of a file the recursive can be set to either true or false.\n   *\n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If <code>f</code> does not exist\n   * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n   *           not supported\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server\n   * \n   * RuntimeExceptions:\n   * @throws InvalidPathException If path <code>f</code> is invalid\n   *\n   * @return if delete success true, not false.",
  "org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean)" : "* Get a protocol proxy that contains a proxy connection to a remote server\n   * and a set of methods that are supported by the server.\n   *\n   * @param <T> Generics Type T.\n   * @param protocol protocol\n   * @param clientVersion client's version\n   * @param addr server address\n   * @param ticket security ticket\n   * @param conf configuration\n   * @param factory socket factory\n   * @param rpcTimeout max time for each rpc; 0 means no timeout\n   * @param connectionRetryPolicy retry policy\n   * @param fallbackToSimpleAuth set to true or false during calls to indicate if\n   *   a secure client falls back to simple auth\n   * @return the proxy\n   * @throws IOException if any error occurs",
  "org.apache.hadoop.util.VersionInfo:getVersion()" : "* Get the Hadoop version.\n   * @return the Hadoop version string, eg. \"0.6.3-dev\"",
  "org.apache.hadoop.fs.viewfs.ChRootedFs:setVerifyChecksum(boolean)" : null,
  "org.apache.hadoop.ha.HealthMonitor:createProxy()" : "* Connect to the service to be monitored. Stubbed out for easier testing.\n   *\n   * @throws IOException raised on errors performing I/O.\n   * @return HAServiceProtocol.",
  "org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : "* Use the command chmod to set permission.",
  "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:getMetricKey(java.lang.String,org.apache.hadoop.metrics2.AbstractMetric,java.util.List)" : null,
  "org.apache.hadoop.io.AbstractMapWritable:getConf()" : "@return the conf",
  "org.apache.hadoop.util.DiskChecker:checkDirInternal(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$BlockFactory:getKeyToBufferDir()" : "* Key to Buffer Directory config for a FS instance.\n     *\n     * @return String containing key to Buffer dir.",
  "org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.HarFileSystem:open(org.apache.hadoop.fs.Path,int)" : "* Returns a har input stream which fakes end of \n   * file. It reads the index files to get the part \n   * file name and the size and start of the file.",
  "org.apache.hadoop.metrics2.impl.MetricCounterInt:type()" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:close()" : "* Close the connection with ZooKeeper.",
  "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],long,long,boolean)" : "* Constructor with host, name, network topology, offset, length \n   * and corrupt flag.\n   * @param names names.\n   * @param hosts hosts.\n   * @param topologyPaths topologyPaths.\n   * @param offset offset.\n   * @param length length.\n   * @param corrupt corrupt.",
  "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:<init>(java.lang.reflect.Method,java.lang.Object[])" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,float)" : "* Create a mutable float gauge\n   * @param info  metadata of the metric\n   * @param iVal  initial value\n   * @return a new gauge object",
  "org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayFactor(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.IdentityHashStore:realloc(int)" : null,
  "org.apache.hadoop.util.DiskChecker$DiskOutOfSpaceException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:getPriorityLevel(org.apache.hadoop.ipc.Schedulable)" : "* Compute the appropriate priority for a schedulable based on past requests.\n   * @param obj the schedulable obj to query and remember\n   * @return the level index which we recommend scheduling in",
  "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:getCurrentIndex()" : "* Gets the current index. Should be accompanied by a call to\n   * advanceIndex at some point.",
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:isCacheSpaceAvailable(long,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)" : "* Determine if the cache space is available on the local FS.\n   *\n   * @param fileSize The size of the file.\n   * @param conf The configuration.\n   * @param localDirAllocator Local dir allocator instance.\n   * @return True if the given file size is less than the available free space on local FS,\n   * False otherwise.",
  "org.apache.hadoop.io.serializer.JavaSerializationComparator:compare(java.lang.Object,java.lang.Object)" : null,
  "org.apache.hadoop.fs.impl.OpenFileParameters:getMandatoryKeys()" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:readRecordLength()" : "* Read and return the next record length, potentially skipping over \n     * a sync block.\n     * @return the length of the next record or -1 if there is no next record\n     * @throws IOException",
  "org.apache.hadoop.service.launcher.ServiceLauncher:toString()" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$SegmentContainer:cleanup()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:truncate(org.apache.hadoop.fs.Path,long)" : null,
  "org.apache.hadoop.fs.FileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Create and initialize a new instance of a FileSystem.\n   * @param uri URI containing the FS schema and FS details\n   * @param conf configuration to use to look for the FS instance declaration\n   * and to pass to the {@link FileSystem#initialize(URI, Configuration)}.\n   * @return the initialized filesystem.\n   * @throws IOException problems loading or initializing the FileSystem",
  "org.apache.hadoop.io.MapWritable:entrySet()" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:getLength()" : "@return Returns the current length of the output file.\n     *\n     * <p>This always returns a synchronized position.  In other words,\n     * immediately after calling {@link SequenceFile.Reader#seek(long)} with a position\n     * returned by this method, {@link SequenceFile.Reader#next(Writable)} may be called.  However\n     * the key may be earlier in the file than key last written when this\n     * method was called (e.g., with block-compression, it may be the first key\n     * in the block that was being written when this method was called).</p>\n     *\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:seek(long)" : null,
  "org.apache.hadoop.fs.SafeMode:setSafeMode(org.apache.hadoop.fs.SafeModeAction)" : "* Enter, leave, or get safe mode.\n   *\n   * @param action One of {@link SafeModeAction} LEAVE, ENTER, GET, FORCE_EXIT.\n   * @throws IOException if set safe mode fails to proceed.\n   * @return true if the action is successfully accepted, otherwise false means rejected.",
  "org.apache.hadoop.io.compress.ZStandardCodec:getBufferSize(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:cloneStatus()" : null,
  "org.apache.hadoop.security.token.Token:encodeToUrlString()" : "* Encode this token as a url safe string.\n   * @return the encoded string\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.StorageType:getTypesSupportingQuota()" : null,
  "org.apache.hadoop.net.SocketInputStream:waitForReadable()" : "* waits for the underlying channel to be ready for reading.\n   * The timeout specified for this stream applies to this wait.\n   * \n   * @throws SocketTimeoutException \n   *         if select on the channel times out.\n   * @throws IOException\n   *         if any other I/O error occurs.",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX:isAvailable()" : "* @return Return true if the JNI-based native IO extensions are available.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:listStatus(org.apache.hadoop.fs.Path)" : "* {@inheritDoc}\n   *\n   * Note: listStatus considers listing from fallbackLink if available. If the\n   * same directory path is present in configured mount path as well as in\n   * fallback fs, then only the fallback path will be listed in the returned\n   * result except for link.\n   *\n   * If any of the the immediate children of the given path f is a symlink(mount\n   * link), the returned FileStatus object of that children would be represented\n   * as a symlink. It will not be resolved to the target path and will not get\n   * the target path FileStatus object. The target path will be available via\n   * getSymlink on that children's FileStatus object. Since it represents as\n   * symlink, isDirectory on that children's FileStatus will return false.\n   * This behavior can be changed by setting an advanced configuration\n   * fs.viewfs.mount.links.as.symlinks to false. In this case, mount points will\n   * be represented as non-symlinks and all the file/directory attributes like\n   * permissions, isDirectory etc will be assigned from it's resolved target\n   * directory/file.\n   *\n   * If you want to get the FileStatus of target path for that children, you may\n   * want to use GetFileStatus API with that children's symlink path. Please see\n   * {@link ViewFileSystem#getFileStatus(Path f)}\n   *\n   * Note: In ViewFileSystem, by default the mount links are represented as\n   * symlinks.",
  "org.apache.hadoop.io.SequenceFile$Writer:writeFileHeader()" : "Write and flush the file header.",
  "org.apache.hadoop.util.bloom.DynamicBloomFilter:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.ha.FailoverController:getGracefulFenceTimeout(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.functional.Tuples:pair(java.lang.Object,java.lang.Object)" : "* Create a 2-tuple.\n   * @param key element 1\n   * @param value element 2\n   * @return a tuple.\n   * @param <K> element 1 type\n   * @param <V> element 2 type",
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getBytesWritten()" : "* Returns the total number of uncompressed bytes output so far.\n   *\n   * @return the total (non-negative) number of uncompressed bytes output so far",
  "org.apache.hadoop.service.launcher.ServiceLauncher:warn(java.lang.String)" : "* Print a warning message.\n   * <p>\n   * This tries to log to the log's warn() operation.\n   * If the log at that level is disabled it logs to system error\n   * @param text warning text",
  "org.apache.hadoop.ha.HealthMonitor:addServiceStateCallback(org.apache.hadoop.ha.HealthMonitor$ServiceStateCallback)" : null,
  "org.apache.hadoop.fs.VectoredReadUtils:validateAndSortRanges(java.util.List,java.util.Optional)" : "* Validate a list of ranges (including overlapping checks) and\n   * return the sorted list.\n   * <p>\n   * Two ranges overlap when the start offset\n   * of second is less than the end offset of first.\n   * End offset is calculated as start offset + length.\n   * @param input input list\n   * @param fileLength file length if known\n   * @return a new sorted list.\n   * @throws IllegalArgumentException if there are overlapping ranges or\n   * a range element is invalid (other than with negative offset)\n   * @throws EOFException if the last range extends beyond the end of the file supplied\n   *                          or a range offset is negative",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:preferDirectBuffer()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:listXAttrs(org.apache.hadoop.fs.Path)" : "* Get all of the xattr names for a file or directory.\n   * Only the xattr names for which the logged-in user has permissions to view\n   * are returned.\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to get extended attributes\n   * @return {@literal Map<String, byte[]>} describing the XAttrs of the file\n   * or directory\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,java.io.Closeable)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getFieldSize()" : "* Return number of elements in the field\n   *\n   * @return number of elements in the field",
  "org.apache.hadoop.security.Groups$GroupCacheLoader:reload(java.lang.String,java.util.Set)" : "* Override the reload method to provide an asynchronous implementation. If\n     * reloadGroupsInBackground is false, then this method defers to the super\n     * implementation, otherwise is arranges for the cache to be updated later",
  "org.apache.hadoop.fs.FsShellPermissions$Chown:parseOwnerGroup(java.lang.String)" : "* Parse the first argument into an owner and group\n     * @param ownerStr string describing new ownership",
  "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getRequestURL()" : "* Quote the url so that users specifying the HOST HTTP header\n       * can't inject attacks.",
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkGreaterOrEqual(long,java.lang.String,long,java.lang.String)" : "* Validates that the first value is greater than or equal to the second value.\n   * @param value1 the first value to check.\n   * @param value1Name the name of the first argument.\n   * @param value2 the second value to check.\n   * @param value2Name the name of the second argument.",
  "org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,byte[],int)" : "* Static method that provides null check for retryCache.\n   * @param cache input Cache.\n   * @param clientId client id of this request\n   * @param callId client call id of this request\n   * @return CacheEntry.",
  "org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendValue(int)" : "* Obtain an output stream for writing a value into TFile. This may only be\n     * called right after a key appending operation (the key append stream must\n     * be closed).\n     * \n     * @param length\n     *          The expected length of the value. If length of the value is not\n     *          known, set length = -1. Otherwise, the application must write\n     *          exactly as many bytes as specified here before calling close on\n     *          the returned output stream. Advertising the value size up-front\n     *          guarantees that the value is encoded in one chunk, and avoids\n     *          intermediate chunk buffering.\n     * @throws IOException raised on errors performing I/O.\n     * @return DataOutputStream.",
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:end()" : "* Resets decompressor and input and output buffers so that a new set of\n   * input data can be processed.",
  "org.apache.hadoop.fs.VectoredReadUtils:sortRanges(java.util.List)" : "* Sort the input ranges by offset; no validation is done.\n   * <p>\n   * This method is used externally and must be retained with\n   * the signature unchanged.\n   * @param input input ranges.\n   * @return a new list of the ranges, sorted by offset.",
  "org.apache.hadoop.fs.FilterFs:truncate(org.apache.hadoop.fs.Path,long)" : null,
  "org.apache.hadoop.fs.shell.TouchCommands$Touchz:processNonexistentPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.FSOutputSummer:flushBuffer()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setGauge(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkLessOrEqual(long,java.lang.String,long,java.lang.String)" : "* Validates that the first value is less than or equal to the second value.\n   * @param value1 the first value to check.\n   * @param value1Name the name of the first argument.\n   * @param value2 the second value to check.\n   * @param value2Name the name of the second argument.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:getNewInstance(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : null,
  "org.apache.hadoop.security.UserGroupInformation:getLoginUser()" : "* Get the currently logged in user.  If no explicit login has occurred,\n   * the user will automatically be logged in with either kerberos credentials\n   * if available, or as the local OS user, based on security settings.\n   * @return the logged in user\n   * @throws IOException if login fails",
  "org.apache.hadoop.fs.GlobalStorageStatistics:reset()" : "* Reset all global storage statistics.",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:resolvePath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)" : null,
  "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:calculateIV(byte[],long,byte[])" : null,
  "org.apache.hadoop.fs.shell.find.Name$Iname:<init>()" : null,
  "org.apache.hadoop.io.OutputBuffer:getLength()" : "* Returns the length of the valid data currently in the buffer.\n   * @return the length of the valid data\n   *          currently in the buffer.",
  "org.apache.hadoop.fs.FilterFs:<init>(org.apache.hadoop.fs.AbstractFileSystem)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Create a decay scheduler.\n   * @param numLevels number of priority levels\n   * @param ns config prefix, so that we can configure multiple schedulers\n   *           in a single instance.\n   * @param conf configuration to use.",
  "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getRevision()" : null,
  "org.apache.hadoop.util.DataChecksum$Java9Crc32CFactory:createChecksum()" : null,
  "org.apache.hadoop.util.GenericsUtil:toArray(java.util.List)" : "* Converts the given <code>List&lt;T&gt;</code> to a an array of \n   * <code>T[]</code>. \n   * @param list the list to convert\n   * @param <T> Generics Type T.\n   * @throws ArrayIndexOutOfBoundsException if the list is empty. \n   * Use {@link #toArray(Class, List)} if the list may be empty.\n   * @return T Array.",
  "org.apache.hadoop.fs.viewfs.ViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:getPermissions(org.apache.commons.net.ftp.FTPFile)" : null,
  "org.apache.hadoop.util.ComparableVersion$IntegerItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item)" : null,
  "org.apache.hadoop.ipc.RPC$Builder:setPort(int)" : "* @return Default: 0.\n     * @param port input port.",
  "org.apache.hadoop.ipc.Client:setAsynchronousMode(boolean)" : "* Set RPC to asynchronous or synchronous mode.\n   *\n   * @param async\n   *          true, RPC will be in asynchronous mode, otherwise false for\n   *          synchronous mode",
  "org.apache.hadoop.fs.permission.FsPermission:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.shell.find.FindOptions:getMinDepth()" : "* Returns the minimum depth for applying expressions.\n   *\n   * @return min depth",
  "org.apache.hadoop.fs.DelegateToFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.fs.http.HttpsFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:create(java.lang.String,java.util.List)" : "* Create a ZNode.\n   * @param path Path of the ZNode.\n   * @param zkAcl ACL for the node.\n   * @return If the ZNode was created.\n   * @throws Exception If it cannot contact Zookeeper.",
  "org.apache.hadoop.fs.FileSystem:isDirectory(org.apache.hadoop.fs.Path)" : "True iff the named path is a directory.\n   * Note: Avoid using this method. Instead reuse the FileStatus\n   * returned by getFileStatus() or listStatus() methods.\n   *\n   * @param f path to check\n   * @throws IOException IO failure\n   * @deprecated Use {@link #getFileStatus(Path)} instead\n   * @return if f is directory true, not false.",
  "org.apache.hadoop.io.Text:getTextLength()" : "* @return Returns the length of this text. The length is equal to the number of\n   * Unicode code units in the text.",
  "org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:encodeData(byte[],java.nio.ByteBuffer[],java.nio.ByteBuffer[])" : "* See above. Try to use the byte[] version when possible.\n   *\n   * @param gfTables gfTables.\n   * @param inputs inputs.\n   * @param outputs outputs.",
  "org.apache.hadoop.fs.impl.prefetch.BlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData)" : "* Releases resources allocated to the given block.\n   *\n   * @param data the {@code BufferData} to release.\n   *\n   * @throws IllegalArgumentException if data is null.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:isRunning()" : "* is secretMgr running\n   * @return true if secret mgr is running",
  "org.apache.hadoop.tracing.Tracer$Builder:build()" : null,
  "org.apache.hadoop.crypto.CryptoOutputStream:hasCapability(java.lang.String)" : null,
  "org.apache.hadoop.ipc.Server$ConnectionManager:add(org.apache.hadoop.ipc.Server$Connection)" : null,
  "org.apache.hadoop.metrics2.MetricStringBuilder:<init>(org.apache.hadoop.metrics2.MetricsCollector,java.lang.String,java.lang.String,java.lang.String)" : "* Build an instance.\n   * @param parent parent collector. Unused in this instance; only used for\n   * the {@link #parent()} method\n   * @param prefix string before each entry\n   * @param separator separator between name and value\n   * @param suffix suffix after each entry",
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:setPath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FilterFs:getFsStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ha.HealthMonitor:join()" : null,
  "org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:clear()" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)" : null,
  "org.apache.hadoop.conf.Configuration:getDeprecatedKeyInfo(java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem:getFileSystemClass(java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Get the FileSystem implementation class of a filesystem.\n   * This triggers a scan and load of all FileSystem implementations listed as\n   * services and discovered via the {@link ServiceLoader}\n   * @param scheme URL scheme of FS\n   * @param conf configuration: can be null, in which case the check for\n   * a filesystem binding declaration in the configuration is skipped.\n   * @return the filesystem\n   * @throws UnsupportedFileSystemException if there was no known implementation\n   *         for the scheme.\n   * @throws IOException if the filesystem could not be loaded",
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:sourceName(java.lang.String,boolean)" : null,
  "org.apache.hadoop.fs.permission.FsPermission:getUnmasked()" : "* Get unmasked permission if exists.\n   * @return unmasked.",
  "org.apache.hadoop.io.WritableComparator:forceInit(java.lang.Class)" : "* Force initialization of the static members.\n   * As of Java 5, referencing a class doesn't force it to initialize. Since\n   * this class requires that the classes be initialized to declare their\n   * comparators, we force that initialization to happen.\n   * @param cls the class to initialize",
  "org.apache.hadoop.metrics2.lib.MutableStat:lastStat()" : "* Return a SampleStat object that supports\n   * calls like StdDev and Mean.\n   * @return SampleStat",
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hsync()" : "* HSync calls sync on fhe file descriptor after a local flush() call.\n     * @throws IOException failure",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getContentSummary(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.Server$ConnectionManager:register(java.nio.channels.SocketChannel,int,boolean)" : null,
  "org.apache.hadoop.crypto.key.KeyProviderExtension:rollNewVersion(java.lang.String,byte[])" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeyVersion(java.lang.String)" : null,
  "org.apache.hadoop.conf.StorageUnit$6:getDefault(double)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:writeJson(java.lang.Object,java.io.OutputStream)" : null,
  "org.apache.hadoop.metrics2.impl.MetricGaugeFloat:value()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:init(javax.servlet.FilterConfig)" : null,
  "org.apache.hadoop.fs.ContentSummary:getHeaderFields()" : "* Returns the names of the fields from the summary header.\n   * \n   * @return names of fields as displayed in the header",
  "org.apache.hadoop.fs.viewfs.ViewFs:setReplication(org.apache.hadoop.fs.Path,short)" : null,
  "org.apache.hadoop.fs.ChecksumFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:decompress(byte[],int,int)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO:getCreateForWriteFileOutputStream(java.io.File,int)" : "* @return Create the specified File for write access, ensuring that it does not exist.\n   * @param f the file that we want to create\n   * @param permissions we want to have on the file (if security is enabled)\n   *\n   * @throws AlreadyExistsException if the file already exists\n   * @throws IOException if any other error occurred",
  "org.apache.hadoop.util.GenericOptionsParser:getRemainingArgs()" : "* Returns an array of Strings containing only application-specific arguments.\n   * \n   * @return array of <code>String</code>s containing the un-parsed arguments\n   * or <strong>empty array</strong> if commandLine was not defined.",
  "org.apache.hadoop.io.VLongWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackInvocation(org.apache.hadoop.util.functional.InvocationRaisingIOE,java.lang.String,org.apache.hadoop.metrics2.lib.MutableRate)" : null,
  "org.apache.hadoop.security.UserGroupInformation$TicketCacheRenewalRunnable:relogin()" : null,
  "org.apache.hadoop.fs.Options$CreateOpts:repFac(short)" : null,
  "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getNodeGroup(java.lang.String)" : "* Given a string representation of a node group for a specific network\n   * location\n   * \n   * @param loc\n   *            a path-like string representation of a network location\n   * @return a node group string",
  "org.apache.hadoop.io.UTF8:skip(java.io.DataInput)" : "* Skips over one UTF8 in the input.\n   * @param in datainput.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ha.ZKFailoverController:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceTarget)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsConfig:toString()" : null,
  "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration)" : "* Create a line reader that reads from the given stream using the\n   * <code>io.file.buffer.size</code> specified in the given\n   * <code>Configuration</code>.\n   * @param in input stream\n   * @param conf configuration\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.ShellBasedIdMapping:loadFullUserMap()" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:deserializeValue(java.lang.Object)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState)" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:readByteArray(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.shell.find.FilterExpression:getHelp()" : null,
  "org.apache.hadoop.security.alias.JavaKeyStoreProvider:getAlgorithm()" : null,
  "org.apache.hadoop.net.NetUtils:isLocalAddress(java.net.InetAddress)" : "* Given an InetAddress, checks to see if the address is a local address, by\n   * comparing the address with all the interfaces on the node.\n   * @param addr address to check if it is local node's address\n   * @return true if the address corresponds to the local node",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getDelegationTokenSeqNum()" : "* For subclasses externalizing the storage, for example Zookeeper\n   * based implementations.\n   *\n   * @return delegationTokenSequenceNumber.",
  "org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[],int,int)" : null,
  "org.apache.hadoop.ipc.Server:isServerFailOverEnabledByQueue()" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine$Invoker:getConnectionId()" : null,
  "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:createKeyProviderDelegationTokenExtension(org.apache.hadoop.crypto.key.KeyProvider)" : "* Creates a <code>KeyProviderDelegationTokenExtension</code> using a given \n   * {@link KeyProvider}.\n   * <p>\n   * If the given <code>KeyProvider</code> implements the \n   * {@link DelegationTokenExtension} interface the <code>KeyProvider</code> \n   * itself will provide the extension functionality, otherwise a default \n   * extension implementation will be used.\n   * \n   * @param keyProvider <code>KeyProvider</code> to use to create the \n   * <code>KeyProviderDelegationTokenExtension</code> extension.\n   * @return a <code>KeyProviderDelegationTokenExtension</code> instance \n   * using the given <code>KeyProvider</code>.",
  "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:write(byte[],int,int)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getPath()" : null,
  "org.apache.hadoop.security.LdapGroupsMapping:getDirContext()" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getCallVolumeSummary()" : null,
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.String,java.lang.String)" : "* Validates that the given string is not null and has non-zero length.\n   * @param arg the argument reference to validate.\n   * @param argName the name of the argument being validated.",
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:close()" : null,
  "org.apache.hadoop.fs.FileSystem:resolvePath(org.apache.hadoop.fs.Path)" : "* Return the fully-qualified path of path, resolving the path\n   * through any symlinks or mount point.\n   * @param p path to be resolved\n   * @return fully qualified path\n   * @throws FileNotFoundException if the path is not present\n   * @throws IOException for any other error",
  "org.apache.hadoop.fs.FilterFileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Returns a local File that the user can write output to.  The caller\n   * provides both the eventual FS target name and the local working\n   * file.  If the FS is local, we write directly into the target.  If\n   * the FS is remote, we write into the tmp local area.",
  "org.apache.hadoop.util.XMLUtils:setOptionalSecureTransformerAttributes(javax.xml.transform.TransformerFactory)" : "* These attributes are recommended for maximum security but some JAXP transformers do\n   * not support them. If at any stage, we fail to set these attributes, then we won't try again\n   * for subsequent transformers.\n   *\n   * @param transformerFactory to update",
  "org.apache.hadoop.fs.FSBuilder:optLong(java.lang.String,long)" : "* Set optional long parameter for the Builder.\n   *\n   * @param key key.\n   * @param value value.\n   * @return generic type B.\n   * @see #opt(String, String)",
  "org.apache.hadoop.util.GSetByHashMap:values()" : null,
  "org.apache.hadoop.fs.FSDataInputStream:readVectored(java.util.List,java.util.function.IntFunction)" : null,
  "org.apache.hadoop.conf.ReconfigurationException:constructMessage(java.lang.String,java.lang.String,java.lang.String)" : "* Construct the exception message.",
  "org.apache.hadoop.fs.StorageStatistics:getScheme()" : "* @return the associated file system scheme if this is scheme specific,\n   * else return null.",
  "org.apache.hadoop.fs.FilterFileSystem:getUsed()" : "Return the total size of all files in the filesystem.",
  "org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(java.lang.String)" : null,
  "org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint()" : null,
  "org.apache.hadoop.io.compress.GzipCodec$GzipZlibDecompressor:<init>()" : null,
  "org.apache.hadoop.net.DNS:getSubinterfaceInetAddrs(java.net.NetworkInterface)" : "* @param nif network interface to get addresses for\n   * @return set containing addresses for each subinterface of nif,\n   *    see below for the rationale for using an ordered set",
  "org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.fs.FileContext:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : "* Get an xattr for a file or directory.\n   * The name must be prefixed with the namespace followed by \".\". For example,\n   * \"user.attr\".\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to get extended attribute\n   * @param name xattr name.\n   * @return byte[] xattr value.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.io.SequenceFile$Reader:length(long)" : "* Create an option to specify the number of bytes to read.\n     * @param value the number of bytes to read\n     * @return a new option",
  "org.apache.hadoop.io.Text$2:<init>()" : "* Construct an empty text string.",
  "org.apache.hadoop.fs.FilterFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.util.JvmPauseMonitor$GcTimes:toString()" : null,
  "org.apache.hadoop.io.compress.BZip2Codec:getCompressorType()" : "* Get the type of {@link Compressor} needed by this {@link CompressionCodec}.\n   *\n   * @return the type of compressor needed by this codec.",
  "org.apache.hadoop.io.retry.RetryPolicies$OtherThanRemoteAndSaslExceptionDependentRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:<init>()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainSort()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getUriPath(org.apache.hadoop.fs.Path)" : "* Make the path Absolute and get the path-part of a pathname.\n   * Checks that URI matches this file system\n   * and that the path-part is a valid name.\n   *\n   * @param p path\n   * @return path-part of the Path p",
  "org.apache.hadoop.ipc.RPC:setProtocolEngine(org.apache.hadoop.conf.Configuration,java.lang.Class,java.lang.Class)" : "* Set a protocol to use a non-default RpcEngine if one\n   * is not specified in the configuration.\n   * @param conf configuration to use\n   * @param protocol the protocol interface\n   * @param engine the RpcEngine impl",
  "org.apache.hadoop.service.ServiceOperations$ServiceListeners:remove(org.apache.hadoop.service.ServiceStateChangeListener)" : "* Remove any registration of a listener from the listener list.\n     * @param l listener\n     * @return true if the listener was found (and then removed)",
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:isChanged()" : null,
  "org.apache.hadoop.ipc.CallQueueManager:addInternal(org.apache.hadoop.ipc.Schedulable,boolean)" : null,
  "org.apache.hadoop.security.KDiag:main(java.lang.String[])" : "* Main entry point.\n   * @param argv args list",
  "org.apache.hadoop.fs.FilterFs:getUriPath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:newSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.impl.MetricsConfig)" : null,
  "org.apache.hadoop.util.OperationDuration:toString()" : "* Return the duration as {@link #humanTime(long)}.\n   * @return a printable duration.",
  "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:getCoderName()" : null,
  "org.apache.hadoop.ipc.Server:updateDeferredMetrics(java.lang.String,long)" : null,
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToObserver(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization:getSchema(org.apache.avro.specific.SpecificRecord)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:getMountPoints()" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getSymlink()" : null,
  "org.apache.hadoop.io.ElasticByteBufferPool:getBufferTree(boolean)" : null,
  "org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[])" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:get(java.lang.String)" : "* Get a metric by name\n   * @param name  of the metric\n   * @return the metric object",
  "org.apache.hadoop.http.HttpServer2Metrics:responses5xx()" : null,
  "org.apache.hadoop.fs.HarFileSystem:decodeFileName(java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:<init>(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.ipc.Server:getCallRetryCount()" : "* @return The current active RPC call's retry count. -1 indicates the retry\n   *         cache is not supported in the client side.",
  "org.apache.hadoop.crypto.CryptoOutputStream:flush()" : "* To flush, we need to encrypt the data in the buffer and write to the \n   * underlying stream, then do the flush.",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:getOperatingSystemPageSize()" : null,
  "org.apache.hadoop.crypto.JceAesCtrCryptoCodec:createEncryptor()" : null,
  "org.apache.hadoop.security.UserGroupInformation:createRemoteUser(java.lang.String,org.apache.hadoop.security.SaslRpcServer$AuthMethod)" : "* Create a user from a login name. It is intended to be used for remote\n   * users in RPC, since it won't have any credentials.\n   * @param user the full user principal name, must not be empty or null\n   * @param authMethod authMethod.\n   * @return the UserGroupInformation for the remote user.",
  "org.apache.hadoop.fs.LocalDirAllocator:<init>(java.lang.String,org.apache.hadoop.util.DiskValidator)" : null,
  "org.apache.hadoop.conf.StorageUnit$1:getLongName()" : null,
  "org.apache.hadoop.fs.permission.AclEntry:getType()" : "* Returns the ACL entry type.\n   *\n   * @return AclEntryType ACL entry type",
  "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:<init>()" : "* Creates a new (pure Java) gzip decompressor.",
  "org.apache.hadoop.fs.shell.Tail:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.security.token.Token:equals(java.lang.Object)" : null,
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)" : null,
  "org.apache.hadoop.service.AbstractService:toString()" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:createConnection()" : null,
  "org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.net.InetAddress)" : "* Convert Kerberos principal name pattern to valid Kerberos principal names.\n   * This method is similar to {@link #getServerPrincipal(String, String)},\n   * except 1) the reverse DNS lookup from addr to hostname is done only when\n   * necessary, 2) param addr can't be null (no default behavior of using local\n   * hostname when addr is null).\n   * \n   * @param principalConfig\n   *          Kerberos principal name pattern to convert\n   * @param addr\n   *          InetAddress of the host used for substitution\n   * @return converted Kerberos principal name\n   * @throws IOException if the client address cannot be determined",
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor:<init>(java.util.concurrent.ExecutorService,int,boolean)" : "* Instantiate without collecting executor aquisition duration information.\n   * @param executorDelegatee Executor to delegate to\n   * @param permitCount number of permits into the queue permitted\n   * @param fair should the semaphore be \"fair\"",
  "org.apache.hadoop.io.ByteBufferPool:release()" : "* Clear the buffer pool thus releasing all the buffers.",
  "org.apache.hadoop.io.SequenceFile$Reader:sync(long)" : "* Seek to the next sync mark past a given position.\n     * @param position position.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getAverageResponseTime()" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_readFully(java.io.InputStream,long,java.nio.ByteBuffer)" : "* Delegate to {@code ByteBufferPositionedReadable#read(long, ByteBuffer)}.\n   * @param in input stream\n   * @param position position within file\n   * @param buf the ByteBuffer to receive the results of the read operation.\n   * @throws UnsupportedOperationException if the input doesn't implement\n   * the interface or, if when invoked, it is raised.\n   * Note: that is the default behaviour of {@code FSDataInputStream#readFully(long, ByteBuffer)}.\n   * @throws IOException if the operation was attempted and failed.",
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:setInput(byte[],int,int)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:appendFile(org.apache.hadoop.fs.Path)" : "* This is overridden to ensure that this class's create() method is\n   * ultimately called.\n   *\n   * {@inheritDoc}",
  "org.apache.hadoop.security.KDiag:println()" : "* Print a new line",
  "org.apache.hadoop.crypto.key.kms.ValueQueue$NamedRunnable:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])" : null,
  "org.apache.hadoop.util.functional.FutureIO:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)" : "* Propagate options to any builder, converting everything with the\n   * prefix to an option where, if there were 2+ dot-separated elements,\n   * it is converted to a schema.\n   * See {@link #propagateOptions(FSBuilder, Configuration, String, boolean)}.\n   * @param builder builder to modify\n   * @param conf configuration to read\n   * @param optionalPrefix prefix for optional settings\n   * @param mandatoryPrefix prefix for mandatory settings\n   * @param <T> type of result\n   * @param <U> type of builder\n   * @return the builder passed in.",
  "org.apache.hadoop.fs.FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : "* The src file is under this filesystem, and the dst is on the local disk.\n   * Copy it from the remote filesystem to the local dst name.\n   * delSrc indicates if the src will be removed\n   * or not. useRawLocalFileSystem indicates whether to use RawLocalFileSystem\n   * as the local file system or not. RawLocalFileSystem is non checksumming,\n   * So, It will not create any crc files at local.\n   *\n   * @param delSrc\n   *          whether to delete the src\n   * @param src\n   *          path\n   * @param dst\n   *          path\n   * @param useRawLocalFileSystem\n   *          whether to use RawLocalFileSystem as local file system or not.\n   *\n   * @throws IOException for any IO error",
  "org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path)" : "* Get all of the xattrs for a file or directory.\n   * Only those xattrs for which the logged-in user has permissions to view\n   * are returned.\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to get extended attributes\n   * @return Map{@literal <}String, byte[]{@literal >} describing the XAttrs\n   * of the file or directory\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.http.HttpServer2Metrics:<init>(org.eclipse.jetty.server.handler.StatisticsHandler,int)" : null,
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor:<init>(java.util.concurrent.ExecutorService,int,boolean,org.apache.hadoop.fs.statistics.DurationTrackerFactory)" : "* Instantiate.\n   * @param executorDelegatee Executor to delegate to\n   * @param permitCount number of permits into the queue permitted\n   * @param fair should the semaphore be \"fair\"\n   * @param trackerFactory duration tracker factory.",
  "org.apache.hadoop.net.unix.DomainSocketWatcher:close()" : "* Close the DomainSocketWatcher and wait for its thread to terminate.\n   *\n   * If there is more than one close, all but the first will be ignored.",
  "org.apache.hadoop.fs.http.HttpFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.fs.statistics.DurationTracker:asDuration()" : "* Get the duration of an operation as a java Duration\n   * instance. If the duration tracker hasn't completed,\n   * or its duration tracking doesn't actually measure duration,\n   * returns Duration.ZERO.\n   * @return a duration, value of ZERO until close().",
  "org.apache.hadoop.util.dynamic.DynConstructors$Builder:impl(java.lang.Class,java.lang.Class[])" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>(int)" : "* Creates a new compressor.\n   *\n   * @param directBufferSize size of the direct buffer to be used.",
  "org.apache.hadoop.fs.FileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])" : "* Renames Path src to Path dst\n   * <ul>\n   *   <li>Fails if src is a file and dst is a directory.</li>\n   *   <li>Fails if src is a directory and dst is a file.</li>\n   *   <li>Fails if the parent of dst does not exist or is a file.</li>\n   * </ul>\n   * <p>\n   * If OVERWRITE option is not passed as an argument, rename fails\n   * if the dst already exists.\n   * </p>\n   * <p>\n   * If OVERWRITE option is passed as an argument, rename overwrites\n   * the dst if it is a file or an empty directory. Rename fails if dst is\n   * a non-empty directory.\n   * </p>\n   * Note that atomicity of rename is dependent on the file system\n   * implementation. Please refer to the file system documentation for\n   * details. This default implementation is non atomic.\n   * <p>\n   * This method is deprecated since it is a temporary method added to\n   * support the transition from FileSystem to FileContext for user\n   * applications.\n   * </p>\n   *\n   * @param src path to be renamed\n   * @param dst new path after rename\n   * @param options rename options.\n   * @throws FileNotFoundException src path does not exist, or the parent\n   * path of dst does not exist.\n   * @throws FileAlreadyExistsException dest path exists and is a file\n   * @throws ParentNotDirectoryException if the parent path of dest is not\n   * a directory\n   * @throws IOException on failure",
  "org.apache.hadoop.ipc.RetryCache:addCacheEntry(byte[],int)" : "* Add a new cache entry into the retry cache. The cache entry consists of \n   * clientId and callId extracted from editlog.\n   *\n   * @param clientId input clientId.\n   * @param callId input callId.",
  "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:<init>(org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolPB)" : null,
  "org.apache.hadoop.util.bloom.Key:hashCode()" : null,
  "org.apache.hadoop.fs.GlobExpander:leftmostOuterCurlyContainingSlash(java.lang.String,int)" : "* Finds the index of the leftmost opening curly bracket containing a\n   * slash character (\"/\") in <code>filePattern</code>.\n   * @param filePattern\n   * @return the index of the leftmost opening curly bracket containing a\n   * slash character (\"/\"), or -1 if there is no such bracket\n   * @throws IOException",
  "org.apache.hadoop.fs.StorageStatistics$LongStatistic:<init>(java.lang.String,long)" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:deserializeKey(java.lang.Object)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:put(java.lang.Runnable)" : null,
  "org.apache.hadoop.fs.ChecksumFs:getChecksumLength(long,int)" : "* Calculated the length of the checksum file in bytes.\n   * @param size the length of the data file in bytes\n   * @param bytesPerSum the number of bytes in a checksum block\n   * @return the number of bytes in the checksum file",
  "org.apache.hadoop.fs.shell.find.FindOptions:setErr(java.io.PrintStream)" : "* Sets the error stream to be used.\n   *\n   * @param err error stream to be used",
  "org.apache.hadoop.fs.HarFileSystem$HarStatus:getPartName()" : null,
  "org.apache.hadoop.net.NodeBase:getPath(org.apache.hadoop.net.Node)" : "* Get the path of a node\n   * @param node a non-null node\n   * @return the path of a node",
  "org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:addReadFileLatency(long)" : "* Add the file read latency to {@link MutableQuantiles} metrics.\n   *\n   * @param readLatency file read latency in microseconds",
  "org.apache.hadoop.io.SortedMapWritable:keySet()" : null,
  "org.apache.hadoop.conf.Configuration$DeprecationDelta:getKey()" : null,
  "org.apache.hadoop.io.SequenceFile$Writer$ReplicationOption:<init>(int)" : null,
  "org.apache.hadoop.util.functional.LazyAtomicReference:<init>(org.apache.hadoop.util.functional.CallableRaisingIOE)" : "* Constructor for this instance.\n   * @param constructor method to invoke to actually construct the inner object.",
  "org.apache.hadoop.fs.viewfs.InodeTree:getRootFallbackLink()" : null,
  "org.apache.hadoop.fs.Path:checkNotSchemeWithRelative()" : "* Test whether this Path uses a scheme and is relative.\n   * Pathnames with scheme and relative path are illegal.",
  "org.apache.hadoop.crypto.key.KeyShell$Command:warnIfTransientProvider()" : null,
  "org.apache.hadoop.fs.QuotaUsage$Builder:typeQuota(org.apache.hadoop.fs.StorageType,long)" : null,
  "org.apache.hadoop.crypto.CipherOption:<init>(org.apache.hadoop.crypto.CipherSuite,byte[],byte[],byte[],byte[])" : null,
  "org.apache.hadoop.fs.CachingGetSpaceUsed:incDfsUsed(long)" : "* Increment the cached value of used space.\n   *\n   * @param value dfs used value.",
  "org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration,java.lang.Class[],java.lang.Object[])" : "Create an object for the given class and initialize it from conf\n   *\n   * @param theClass class of which an object is created\n   * @param conf Configuration\n   * @param argTypes the types of the arguments\n   * @param values the values of the arguments\n   * @param <T> Generics Type.\n   * @return a new object",
  "org.apache.hadoop.ha.HAServiceStatus:isReadyToBecomeActive()" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:release()" : null,
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:locateKeystore()" : "* Open up and initialize the keyStore.\n   *\n   * @throws IOException If there is a problem reading the password file\n   * or a problem reading the keystore.",
  "org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForKeytab()" : "* Spawn a thread to do periodic renewals of kerberos credentials from a\n   * keytab file.",
  "org.apache.hadoop.fs.FileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)" : "* Get the checksum of a file, from the beginning of the file till the\n   * specific length.\n   * @param f The file path\n   * @param length The length of the file range for checksum calculation\n   * @return The file checksum or null if checksums are not supported.\n   * @throws IOException IO failure",
  "org.apache.hadoop.util.Options$StringOption:<init>(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.filter.RegexFilter:compile(java.lang.String)" : null,
  "org.apache.hadoop.io.WritableComparator:compareBytes(byte[],int,int,byte[],int,int)" : "* Lexicographic order of binary data.\n   * @param b1 b1.\n   * @param s1 s1.\n   * @param l1 l1.\n   * @param b2 b2.\n   * @param s2 s2.\n   * @param l2 l2.\n   * @return compare bytes.",
  "org.apache.hadoop.fs.Options$HandleOpt$Location:toString()" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:openFileOnInstance(org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String)" : "* Open a file.\n   * <p>\n   * If the WrappedIO class is found, uses\n   * {@link #fileSystem_openFile(FileSystem, Path, String, FileStatus, Long, Map)} with\n   * {@link #PARQUET_READ_POLICIES} as the list of read policies and passing down\n   * the file status.\n   * <p>\n   * If not, falls back to the classic {@code fs.open(Path)} call.\n   * @param instance dynamic wrapped IO instance.\n   * @param fs filesystem\n   * @param status file status\n   * @param readPolicies read policy to use\n   * @return the input stream\n   * @throws IOException any IO failure.",
  "org.apache.hadoop.ha.ZKFailoverController:doGracefulFailover()" : "* Coordinate a graceful failover. This proceeds in several phases:\n   * 1) Pre-flight checks: ensure that the local node is healthy, and\n   * thus a candidate for failover.\n   * 2a) Determine the current active node. If it is the local node, no\n   * need to failover - return success.\n   * 2b) Get the other nodes\n   * 3a) Ask the other nodes to yield from election for a number of seconds\n   * 3b) Ask the active node to yield from the election for a number of seconds.\n   * 4) Allow the normal election path to run in other threads. Wait until\n   * we either become unhealthy or we see an election attempt recorded by\n   * the normal code path.\n   * 5) Allow the old active to rejoin the election, so a future\n   * failback is possible.",
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:isWithinCurrentBuffer(long)" : "* Determines whether the given absolute position lies within the current buffer.\n   *\n   * @param pos the position to check.\n   * @return true if the given absolute position lies within the current buffer, false otherwise.",
  "org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:cloneAsDirectByteBuffer(byte[],int,int)" : "* Clone an input bytes array as direct ByteBuffer.",
  "org.apache.hadoop.util.Options$ProgressableOption:<init>(org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.ipc.RpcClientException:<init>(java.lang.String)" : "* Constructs exception with the specified detail message.\n   * \n   * @param messages detailed message.",
  "org.apache.hadoop.conf.Configuration:reloadConfiguration()" : "* Reload configuration from previously added resources.\n   *\n   * This method will clear all the configuration read from the added \n   * resources, and final parameters. This will make the resources to \n   * be read again before accessing the values. Values that are added\n   * via set methods will overlay values read from the resources.",
  "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:release()" : null,
  "org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,org.apache.hadoop.conf.Configuration)" : "Get a path from the local FS. This method should be used if the size of \n   *  the file is not known apriori. We go round-robin over the set of disks\n   *  (via the configured dirs) and return the first complete path where\n   *  we could create the parent directory of the passed path. \n   *  @param pathStr the requested path (this will be created on the first \n   *  available disk)\n   *  @param conf the Configuration object\n   *  @return the complete path to the file on a local disk\n   *  @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:<init>(org.apache.hadoop.security.token.SecretManager,org.apache.hadoop.ipc.Server$Connection)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:toString()" : null,
  "org.apache.hadoop.fs.FileUtil:execSetPermission(java.io.File,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:removeAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.Client$IpcStreams:readResponse()" : null,
  "org.apache.hadoop.util.HostsFileReader:readFileToMap(java.lang.String,java.lang.String,java.util.Map)" : null,
  "org.apache.hadoop.io.ByteWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.fs.RawPathHandle:<init>(org.apache.hadoop.fs.PathHandle)" : "* Initialize using a copy of bytes from the serialized handle.\n   * @param handle PathHandle to preserve in serialized form.",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:isPmem(long,long)" : null,
  "org.apache.hadoop.security.SecurityUtil:setConfiguration(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.metrics2.AbstractMetric:info()" : null,
  "org.apache.hadoop.fs.shell.find.Find:addExpression(java.lang.Class)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream$Data:<init>(int)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail:hashCode()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:getFileLinkStatusInternal(org.apache.hadoop.fs.Path,boolean)" : "* Public {@link FileStatus} methods delegate to this function, which in turn\n   * either call the new {@link Stat} based implementation or the deprecated\n   * methods based on platform support.\n   * \n   * @param f Path to stat\n   * @param dereference whether to dereference the final path component if a\n   *          symlink\n   * @return FileStatus of f\n   * @throws IOException",
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferFullyRead()" : "* Determines whether the current buffer has been fully read.\n   *\n   * @return true if the current buffer has been fully read, false otherwise.",
  "org.apache.hadoop.io.compress.SnappyCodec:setConf(org.apache.hadoop.conf.Configuration)" : "* Set the configuration to be used by this object.\n   *\n   * @param conf the configuration object.",
  "org.apache.hadoop.service.CompositeService:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:ignoreSync()" : "disables sync. often invoked for tmp files.",
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:put(int,java.nio.ByteBuffer,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)" : "* Puts the given block in this cache.\n   *\n   * @param blockNumber the block number, used as a key for blocks map.\n   * @param buffer buffer contents of the given block to be added to this cache.\n   * @param conf the configuration.\n   * @param localDirAllocator the local dir allocator instance.\n   * @throws IOException if either local dir allocator fails to allocate file or if IO error\n   * occurs while writing the buffer content to the file.\n   * @throws IllegalArgumentException if buffer is null, or if buffer.limit() is zero or negative.",
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_enabled()" : "* Static probe to check if the thread-level IO statistics enabled.\n   * @return true if the thread-level IO statistics are enabled.",
  "org.apache.hadoop.io.Text:write(java.io.DataOutput)" : "* Serialize. Write this object to out length uses zero-compressed encoding.\n   *\n   * @see Writable#write(DataOutput)",
  "org.apache.hadoop.metrics2.lib.MutableGaugeLong:incr(long)" : "* Increment by delta\n   * @param delta of the increment",
  "org.apache.hadoop.net.ScriptBasedMappingWithDependency:toString()" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:getUniqueIdentityCount()" : null,
  "org.apache.hadoop.security.KerberosAuthException:setUser(java.lang.String)" : null,
  "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getHostKey(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetInt()" : null,
  "org.apache.hadoop.util.ComparableVersion$ListItem:getType()" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:cleanupNewAndOld(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)" : "* Demand create the directory allocator, then create a temporary file.\n     * This does not mark the file for deletion when a process exits.\n     * {@link LocalDirAllocator#createTmpFileForWrite(String, long, Configuration)}.\n     *\n     * @param pathStr prefix for the temporary file.\n     * @param size    the size of the file that is going to be written.\n     * @param conf    the Configuration object.\n     * @return a unique temporary file.\n     * @throws IOException IO problems",
  "org.apache.hadoop.fs.DelegateToFileSystem:getFileChecksum(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.HardLink:getLinkCount(java.io.File)" : "* Retrieves the number of links to the specified file.\n    *\n    * @param fileName file name.\n    * @throws IOException raised on errors performing I/O.\n    * @return link count.",
  "org.apache.hadoop.fs.impl.prefetch.BufferData:updateState(org.apache.hadoop.fs.impl.prefetch.BufferData$State,org.apache.hadoop.fs.impl.prefetch.BufferData$State[])" : "* Updates the current state to the specified value.\n   * Asserts that the current state is as expected.\n   * @param newState the state to transition to.\n   * @param expectedCurrentState the collection of states from which\n   *        transition to {@code newState} is allowed.\n   *\n   * @throws IllegalArgumentException if newState is null.\n   * @throws IllegalArgumentException if expectedCurrentState is null.",
  "org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String)" : "* Obtain an output stream for creating a meta block. This function may not\n     * be called when there is a key append stream or value append stream\n     * active. No more key-value insertion is allowed after a meta data block\n     * has been added to TFile. Data will be compressed using the default\n     * compressor as defined in Writer's constructor.\n     * \n     * @param name\n     *          Name of the meta block.\n     * @return A DataOutputStream that can be used to write Meta Block data.\n     *         Closing the stream would signal the ending of the block.\n     * @throws IOException raised on errors performing I/O.\n     * @throws MetaBlockAlreadyExists\n     *           the Meta Block with the same name already exists.",
  "org.apache.hadoop.security.SecurityUtil:<init>()" : null,
  "org.apache.hadoop.util.SysInfoWindows:getNetworkBytesRead()" : "{@inheritDoc}",
  "org.apache.hadoop.io.erasurecode.ECChunk:<init>(byte[])" : "* Wrapping a bytes array\n   * @param buffer buffer to be wrapped by the chunk",
  "org.apache.hadoop.fs.store.ByteBufferInputStream:hasRemaining()" : "* Check if there is data left.\n   * @return true if there is data remaining in the buffer.",
  "org.apache.hadoop.fs.impl.FileRangeImpl:setLength(int)" : null,
  "org.apache.hadoop.security.alias.CredentialShell:getPasswordReader()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:<init>(java.io.OutputStream,int)" : "* Constructs a new <tt>CBZip2OutputStream</tt> with specified blocksize.\n  *\n  * <p>\n  * <b>Attention: </b>The caller is resonsible to write the two BZip2 magic\n  * bytes <tt>\"BZ\"</tt> to the specified stream prior to calling this\n  * constructor.\n  * </p>\n  *\n  *\n  * @param out\n  *            the destination stream.\n  * @param blockSize\n  *            the blockSize as 100k units.\n  *\n  * @throws IOException\n  *             if an I/O error occurs in the specified stream.\n  * @throws IllegalArgumentException\n  *             if {@code (blockSize < 1) || (blockSize > 9)}\n  * @throws NullPointerException\n  *             if {@code out == null}.\n  *\n  * @see #MIN_BLOCKSIZE\n  * @see #MAX_BLOCKSIZE",
  "org.apache.hadoop.util.dynamic.BindingUtils:loadClass(java.lang.String)" : "* Load a class by name.\n   * @param className classname\n   * @return the class or null if it could not be loaded.",
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeAcl(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)" : "* Filter files/directories in the given list of paths using user-supplied\n   * path filter.\n   * <p>\n   * Does not guarantee to return the List of files/directories status in a\n   * sorted order.\n   *\n   * @param files\n   *          a list of paths\n   * @param filter\n   *          the user-supplied path filter\n   * @return a list of statuses for the files under the given paths after\n   *         applying the filter\n   * @throws FileNotFoundException when the path does not exist\n   * @throws IOException see specific implementation",
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:getPathHandle(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)" : "* Convenience method, so that we don't open a new connection when using this\n   * method from within another method. Otherwise every API invocation incurs\n   * the overhead of opening/closing a TCP connection.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.RawComparable,boolean)" : null,
  "org.apache.hadoop.io.MultipleIOException:createIOException(java.util.List)" : "* A convenient method to create an {@link IOException}.\n   * @param exceptions IOException List.\n   * @return IOException.",
  "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:close()" : "closes the underlying reader",
  "org.apache.hadoop.util.Progress:toString()" : null,
  "org.apache.hadoop.fs.HarFileSystem:initializeMetadataCache(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.Options$CreateOpts:bufferSize(int)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:parseThresholds(java.lang.String,org.apache.hadoop.conf.Configuration,int)" : null,
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:overwrite(boolean)" : "* Set to true to overwrite the existing file.\n   * Set it to false, an exception will be thrown when calling {@link #build()}\n   * if the file exists.\n   *\n   * @param overwrite overrite.\n   * @return Generics Type B.",
  "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:available()" : null,
  "org.apache.hadoop.fs.DelegateToFileSystem:getDefaultPortIfDefined(org.apache.hadoop.fs.FileSystem)" : "* Returns the default port if the file system defines one.\n   * {@link FileSystem#getDefaultPort()} returns 0 to indicate the default port\n   * is undefined.  However, the logic that consumes this value expects to\n   * receive -1 to indicate the port is undefined, which agrees with the\n   * contract of {@link URI#getPort()}.\n   *\n   * @param theFsImpl file system to check for default port\n   * @return default port, or -1 if default port is undefined",
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.String,java.lang.Class[])" : "* Checks for an implementation, first finding the given class by name.\n     * <p>\n     * The name passed to the constructor is the method name used.\n     * @param className name of a class\n     * @param argClasses argument classes for the method\n     * @return this Builder for method chaining",
  "org.apache.hadoop.fs.store.LogExactlyOnce:debug(java.lang.String,java.lang.Object[])" : "* Log at DEBUG if nothing has been logged yet.\n   * @param format format string\n   * @param args arguments",
  "org.apache.hadoop.io.EnumSetWritable:equals(java.lang.Object)" : "* Returns true if <code>o</code> is an EnumSetWritable with the same value,\n   * or both are null.",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_aggregate(java.io.Serializable,java.lang.Object)" : "* Aggregate an existing {@code IOStatisticsSnapshot} with\n   * the supplied statistics.\n   * @param snapshot snapshot to update\n   * @param statistics IOStatistics to add\n   * @return true if the snapshot was updated.\n   * @throws IllegalArgumentException if the {@code statistics} argument is not\n   * null but not an instance of IOStatistics, or if  {@code snapshot} is invalid.\n   * @throws UnsupportedOperationException if the IOStatistics classes were not found",
  "org.apache.hadoop.util.ProgramDriver:driver(java.lang.String[])" : "* API compatible with Hadoop 1.x.\n   *\n   * @param argv argv.\n   * @throws Throwable Anything thrown\n   *                   by the example program's main",
  "org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:close()" : null,
  "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:<init>(java.lang.String,java.lang.String,byte[])" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState)" : null,
  "org.apache.hadoop.security.SaslPropertiesResolver:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.ShellBasedIdMapping:getGroupName(int,java.lang.String)" : null,
  "org.apache.hadoop.util.ProtoUtil:makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto,int,int,byte[])" : null,
  "org.apache.hadoop.fs.shell.Command:setCommandFactory(org.apache.hadoop.fs.shell.CommandFactory)" : "* sets the command factory for later use.\n   * @param factory factory.",
  "org.apache.hadoop.util.JvmPauseMonitor:getNumGcWarnThresholdExceeded()" : null,
  "org.apache.hadoop.metrics2.util.Metrics2Util$TopN:offer(org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair)" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:getChildren(java.lang.String)" : "* Get children of a ZNode.\n   * @param path Path of the ZNode.\n   * @return The list of children.\n   * @throws Exception If it cannot contact Zookeeper.",
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:countDots(java.lang.String)" : "* Counts the number of dots \".\" in a string.\n         *\n         * @param s string to count dots from\n         * @return number of dots",
  "org.apache.hadoop.fs.RawLocalFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.conf.Configuration:hasWarnedDeprecation(java.lang.String)" : "* Returns whether or not a deprecated name has been warned. If the name is not\n   * deprecated then always return false\n   * @param name proprties.\n   * @return true if name is a warned deprecation.",
  "org.apache.hadoop.fs.shell.find.Result:isDescend()" : "* Should further directories be descended.\n   * @return if is pass true,not false.",
  "org.apache.hadoop.util.SysInfoLinux:safeParseLong(java.lang.String)" : "*\n   * Wrapper for Long.parseLong() that returns zero if the value is\n   * invalid. Under some circumstances, swapFree in /proc/meminfo can\n   * go negative, reported as a very large decimal value.",
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFsStatus()" : null,
  "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class,java.lang.String)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:<init>()" : null,
  "org.apache.hadoop.io.MapFile$Reader:getKeyClass()" : "* Returns the class of keys in this file.\n     *\n     * @return keyClass.",
  "org.apache.hadoop.net.ScriptBasedMapping:setConf(org.apache.hadoop.conf.Configuration)" : "* {@inheritDoc}.\n   * <p>\n   * This will get called in the superclass constructor, so a check is needed\n   * to ensure that the raw mapping is defined before trying to relaying a null\n   * configuration.\n   * </p>\n   * @param conf input Configuration.",
  "org.apache.hadoop.util.PureJavaCrc32:reset()" : null,
  "org.apache.hadoop.io.MapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)" : "* Append a key/value pair to the map.  The key must be greater or equal\n     * to the previous key added to the map.\n     *\n     * @param key key.\n     * @param val value.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainQSort3(org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data,int,int,int)" : "* Method \"mainQSort3\", file \"blocksort.c\", BZip2 1.0.2",
  "org.apache.hadoop.fs.FileSystem:getBlockSize(org.apache.hadoop.fs.Path)" : "* Get the block size for a particular file.\n   * @param f the filename\n   * @return the number of bytes in a block\n   * @deprecated Use {@link #getFileStatus(Path)} instead\n   * @throws FileNotFoundException if the path is not present\n   * @throws IOException IO failure",
  "org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsSourceToString(java.lang.Object)" : "* Extract the statistics from a source object -or \"\"\n   * if it is not an instance of {@link IOStatistics},\n   * {@link IOStatisticsSource} or the retrieved\n   * statistics are null.\n   * <p>\n   * Exceptions are caught and downgraded to debug logging.\n   * @param source source of statistics.\n   * @return a string for logging.",
  "org.apache.hadoop.io.file.tfile.TFile$Writer:close()" : "* Close the Writer. Resources will be released regardless of the exceptions\n     * being thrown. Future close calls will have no effect.\n     * \n     * The underlying FSDataOutputStream is not closed.",
  "org.apache.hadoop.io.SequenceFile$Writer:metadata(org.apache.hadoop.io.SequenceFile$Metadata)" : null,
  "org.apache.hadoop.metrics2.lib.MutableRate:<init>(java.lang.String,java.lang.String,boolean)" : null,
  "org.apache.hadoop.fs.TrashPolicyDefault:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* @deprecated Use {@link #initialize(Configuration, FileSystem)} instead.",
  "org.apache.hadoop.service.launcher.IrqHandler$InterruptData:<init>(java.lang.String,int)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:makeAbsolute(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.crypto.key.CachingKeyProvider:getKeyVersion(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:<init>()" : null,
  "org.apache.hadoop.fs.http.HttpFileSystem:getWorkingDirectory()" : null,
  "org.apache.hadoop.util.CrcUtil:toMultiCrcString(byte[])" : "* For use with debug statements; verifies bytes.length on creation,\n   * expecting it to be divisible by CRC byte size, and returns a list of\n   * hex formatted values.\n   *\n   * @param bytes bytes.\n   * @throws IOException raised on errors performing I/O.\n   * @return a list of hex formatted values.",
  "org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String)" : "* Add config variable for homedir for default mount table\n   * @param conf - add to this conf\n   * @param homedir - the home dir path starting with slash",
  "org.apache.hadoop.io.DataInputByteBuffer$Buffer:reset(java.nio.ByteBuffer[])" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:getIOStatistics()" : null,
  "org.apache.hadoop.conf.StorageUnit$2:toPBs(double)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:equals(java.lang.Object)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider$Metadata:toString()" : null,
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.util.ServletUtil:getRawPath(javax.servlet.http.HttpServletRequest,java.lang.String)" : "* Parse the path component from the given request and return w/o decoding.\n   * @param request Http request to parse\n   * @param servletName the name of servlet that precedes the path\n   * @return path component, null if the default charset is not supported",
  "org.apache.hadoop.fs.shell.CommandWithDestination:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.util.StopWatch:stop()" : "* Stop elapsed time and make the state of stopwatch stop.\n   * @return this instance of StopWatch.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader:isSorted()" : "* Is the TFile sorted?\n     * \n     * @return true if TFile is sorted.",
  "org.apache.hadoop.util.LightWeightGSet:computeCapacity(long,double,java.lang.String)" : null,
  "org.apache.hadoop.fs.HardLink$HardLinkCGUnix:setLinkCountCmdTemplate(java.lang.String[])" : null,
  "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)" : "* Create an FSDataOutputStream at the indicated Path with write-progress\n   * reporting.\n   * @param f the file name to open\n   * @param permission file permission\n   * @param flags {@link CreateFlag}s to use for this stream.\n   * @param bufferSize the size of the buffer to be used.\n   * @param replication required block replication for the file.\n   * @param blockSize block size\n   * @param progress the progress reporter\n   * @throws IOException IO failure\n   * @see #setPermission(Path, FsPermission)\n   * @return output stream.",
  "org.apache.hadoop.service.CompositeService:getServices()" : "* Get a cloned list of services\n   * @return a list of child services at the time of invocation -\n   * added services will not be picked up.",
  "org.apache.hadoop.tools.CommandShell:getErr()" : null,
  "org.apache.hadoop.util.SequentialNumber:hashCode()" : null,
  "org.apache.hadoop.metrics2.impl.SinkQueue:size()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.util.ComparableVersion$ListItem:toString()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:createSocket()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:supportsSymlinks()" : "* Returns true if the file system supports symlinks, false otherwise.\n   * @return true if filesystem supports symlinks",
  "org.apache.hadoop.io.ArrayPrimitiveWritable:readDoubleArray(java.io.DataInput)" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer:compareTo(byte[],int,int,byte[],int,int)" : "* Lexicographically compare two arrays.\n       *\n       * @param buffer1 left operand\n       * @param buffer2 right operand\n       * @param offset1 Where to start comparing in the left buffer\n       * @param offset2 Where to start comparing in the right buffer\n       * @param length1 How much to compare from the left buffer\n       * @param length2 How much to compare from the right buffer\n       * @return 0 if equal, < 0 if left is less than right, etc.",
  "org.apache.hadoop.conf.StorageUnit$5:getLongName()" : null,
  "org.apache.hadoop.ipc.Client:setConnectTimeout(org.apache.hadoop.conf.Configuration,int)" : "* set the connection timeout value in configuration\n   * \n   * @param conf Configuration\n   * @param timeout the socket connect timeout value",
  "org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.metrics2.util.SampleStat:variance()" : "* @return  the variance of the samples",
  "org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:createEncoder()" : null,
  "org.apache.hadoop.ipc.Client$Connection:setupIOstreams(java.util.concurrent.atomic.AtomicBoolean)" : "Connect to the server and set up the I/O streams. It then sends\n     * a header to the server and starts\n     * the connection thread that waits for responses.",
  "org.apache.hadoop.crypto.CryptoInputStream:decrypt(long,java.nio.ByteBuffer,int,int)" : "* Decrypts the given {@link ByteBuffer} in place. {@code length} bytes are\n   * decrypted from {@code buf} starting at {@code start}.\n   * {@code buf.position()} and {@code buf.limit()} are unchanged after this\n   * method returns. This method is thread-safe.\n   *\n   * <p>\n   *   This method decrypts the input buf chunk-by-chunk and writes the\n   *   decrypted output back into the input buf. It uses two local buffers\n   *   taken from the {@link #bufferPool} to assist in this process: one is\n   *   designated as the input buffer and it stores a single chunk of the\n   *   given buf, the other is designated as the output buffer, which stores\n   *   the output of decrypting the input buffer. Both buffers are of size\n   *   {@link #bufferSize}.\n   * </p>\n   *\n   * <p>\n   *   Decryption is done by using a {@link Decryptor} and the\n   *   {@link #decrypt(Decryptor, ByteBuffer, ByteBuffer, byte)} method. Once\n   *   the decrypted data is written into the output buffer, is is copied back\n   *   into buf. Both buffers are returned back into the pool once the entire\n   *   buf is decrypted.\n   * </p>\n   *\n   * @param filePosition the current position of the file being read\n   * @param buf the {@link ByteBuffer} to decrypt\n   * @param length the number of bytes in {@code buf} to decrypt\n   * @param start the position in {@code buf} to start decrypting data from",
  "org.apache.hadoop.ipc.Server:getHandlers()" : null,
  "org.apache.hadoop.crypto.JceCtrCryptoCodec:setProvider(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write0(int)" : null,
  "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.ipc.ResponseBuffer:getFramedBuffer()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createFile(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:reportCRCError()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:listLocatedStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:writeCharArray(java.io.DataOutput)" : null,
  "org.apache.hadoop.ipc.Client:getTimeout(org.apache.hadoop.conf.Configuration)" : "* The time after which a RPC will timeout.\n   * If ping is not enabled (via ipc.client.ping), then the timeout value is the \n   * same as the pingInterval.\n   * If ping is enabled, then there is no timeout value.\n   * \n   * @param conf Configuration\n   * @return the timeout period in milliseconds. -1 if no timeout value is set\n   * @deprecated use {@link #getRpcTimeout(Configuration)} instead",
  "org.apache.hadoop.fs.DF:<init>(java.io.File,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.Server$Call:toString()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:size()" : null,
  "org.apache.hadoop.io.BooleanWritable:get()" : "* Returns the value of the BooleanWritable.\n   * @return the value of the BooleanWritable.",
  "org.apache.hadoop.service.ServiceStateException:<init>(java.lang.String,java.lang.Throwable)" : "* Instantiate with a message and cause; if the cause has an exit code\n   * then it is used, otherwise the generic\n   * {@link LauncherExitCodes#EXIT_SERVICE_LIFECYCLE_EXCEPTION} exit code\n   * is used.\n   * @param message exception message\n   * @param cause optional inner cause",
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:allowChangeInputs()" : "* Allow change into input buffers or not while perform encoding/decoding.\n   * @return true if it's allowed to change inputs, false otherwise",
  "org.apache.hadoop.metrics2.util.SampleQuantiles:getSampleCount()" : "* Returns the number of samples kept by the estimator\n   * \n   * @return count current number of samples",
  "org.apache.hadoop.ipc.Server$Connection:shouldClose()" : null,
  "org.apache.hadoop.fs.store.EtagChecksum:getAlgorithmName()" : null,
  "org.apache.hadoop.ipc.Server$Connection:processOneRpc(java.nio.ByteBuffer)" : "* Process one RPC Request from buffer read from socket stream \n     *  - decode rpc in a rpc-Call\n     *  - handle out-of-band RPC requests such as the initial connectionContext\n     *  - A successfully decoded RpcCall will be deposited in RPC-Q and\n     *    its response will be sent later when the request is processed.\n     * \n     * Prior to this call the connectionHeader (\"hrpc...\") has been handled and\n     * if SASL then SASL has been established and the buf we are passed\n     * has been unwrapped from SASL.\n     * \n     * @param bb - contains the RPC request header and the rpc request\n     * @throws IOException - internal error that should not be returned to\n     *         client, typically failure to respond to client\n     * @throws InterruptedException",
  "org.apache.hadoop.ha.SshFenceByTcpPort$Args:<init>(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:inMiniClusterMode()" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:requestsActiveMax()" : null,
  "org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)" : "* Get crypto codec for specified algorithm/mode/padding.\n   * \n   * @param conf\n   *          the configuration\n   * @param cipherSuite\n   *          algorithm/mode/padding\n   * @return CryptoCodec the codec object. Null value will be returned if no\n   *         crypto codec classes with cipher suite configured.",
  "org.apache.hadoop.fs.FileSystem:getTrashRoot(org.apache.hadoop.fs.Path)" : "* Get the root directory of Trash for current user when the path specified\n   * is deleted.\n   *\n   * @param path the trash root of the path to be determined.\n   * @return the default implementation returns {@code /user/$USER/.Trash}",
  "org.apache.hadoop.net.DNS:reverseDns(java.net.InetAddress,java.lang.String)" : "* Returns the hostname associated with the specified IP address by the\n   * provided nameserver.\n   *\n   * Loopback addresses \n   * @param hostIp The address to reverse lookup\n   * @param ns The host name of a reachable DNS server\n   * @return The host name associated with the provided IP\n   * @throws NamingException If a NamingException is encountered",
  "org.apache.hadoop.fs.sftp.SFTPConnectionPool:returnToPool(com.jcraft.jsch.ChannelSftp)" : "Add the channel into pool.\n   * @param channel",
  "org.apache.hadoop.net.NetUtils:getSocketFactoryFromProperty(org.apache.hadoop.conf.Configuration,java.lang.String)" : "* Get the socket factory corresponding to the given proxy URI. If the\n   * given proxy URI corresponds to an absence of configuration parameter,\n   * returns null. If the URI is malformed raises an exception.\n   *\n   * @param conf configuration.\n   * @param propValue the property which is the class name of the\n   *        SocketFactory to instantiate; assumed non null and non empty.\n   * @return a socket factory as defined in the property value.",
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:setPath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getConfig()" : null,
  "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:getKeyStoreType()" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatistics:meanStatistics()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)" : "* Cancel a token by removing it from cache.\n   *\n   * @param token token.\n   * @param canceller canceller.\n   * @return Identifier of the canceled token\n   * @throws InvalidToken for invalid token\n   * @throws AccessControlException if the user isn't allowed to cancel",
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:substitute(byte[][],int[],int,byte[],int,int)" : "* A \"bulk\" version of the substitute.\n   * Tends to be 2X faster than the \"int\" substitute in a loop.\n   *\n   * @param p input polynomial\n   * @param offsets input offset.\n   * @param len input len.\n   * @param q store the return result\n   * @param offset input offset.\n   * @param x input field",
  "org.apache.hadoop.fs.permission.FsCreateModes:applyUMask(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)" : "* Create from unmasked mode and umask.\n   *\n   * @param mode mode.\n   * @param umask umask.\n   * @return If the mode is already\n   * an FsCreateModes object, return it.",
  "org.apache.hadoop.ipc.DecayRpcScheduler:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover:run()" : null,
  "org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>()" : "* Zero-args constructor for the serialization.",
  "org.apache.hadoop.fs.FilterFileSystem:setWriteChecksum(boolean)" : null,
  "org.apache.hadoop.util.ComparableVersion$ListItem:normalize()" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:setInput(byte[],int,int)" : null,
  "org.apache.hadoop.fs.FileContext:checkDependencies(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:addDecayedCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder)" : null,
  "org.apache.hadoop.net.StandardSocketFactory:createSocket()" : null,
  "org.apache.hadoop.fs.FileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : "* See {@link FileContext#createSymlink(Path, Path, boolean)}.\n   *\n   * @param target target path.\n   * @param link link.\n   * @param createParent create parent.\n   * @throws AccessControlException if access is denied.\n   * @throws FileAlreadyExistsException when the path does not exist.\n   * @throws FileNotFoundException when the path does not exist.\n   * @throws ParentNotDirectoryException if the parent path of dest is not\n   *                                     a directory.\n   * @throws UnsupportedFileSystemException if there was no known implementation\n   *                                        for the scheme.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.WeightedTimeCostProvider:init(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getNonNegative(java.lang.String,int)" : "* Return the property value if it's non-negative and throw an exception if\n   * it's not.\n   *\n   * @param key the property key\n   * @param defaultValue the default value",
  "org.apache.hadoop.security.alias.JavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.Credentials:removeSecretKey(org.apache.hadoop.io.Text)" : "* Remove the key for a given alias.\n   * @param alias the alias for the key",
  "org.apache.hadoop.log.LogLevel$CLI:parseSetLevelArgs(java.lang.String[],int)" : null,
  "org.apache.hadoop.security.authorize.AuthorizationException:printStackTrace(java.io.PrintStream)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:numAvailable()" : "* Number of items available to be acquired. Mostly for testing purposes.\n   * @return the number available.",
  "org.apache.hadoop.io.MD5Hash$Comparator:compare(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:<init>()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.tracing.TraceScope:detach()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$DataBlock:hasData()" : "* Predicate to check if there is data in the block.\n     *\n     * @return true if there is",
  "org.apache.hadoop.io.nativeio.NativeIO:copyFileUnbuffered(java.io.File,java.io.File)" : "* Unbuffered file copy from src to dst without tainting OS buffer cache\n   *\n   * In POSIX platform:\n   * It uses FileChannel#transferTo() which internally attempts\n   * unbuffered IO on OS with native sendfile64() support and falls back to\n   * buffered IO otherwise.\n   *\n   * It minimizes the number of FileChannel#transferTo call by passing the the\n   * src file size directly instead of a smaller size as the 3rd parameter.\n   * This saves the number of sendfile64() system call when native sendfile64()\n   * is supported. In the two fall back cases where sendfile is not supported,\n   * FileChannle#transferTo already has its own batching of size 8 MB and 8 KB,\n   * respectively.\n   *\n   * In Windows Platform:\n   * It uses its own native wrapper of CopyFileEx with COPY_FILE_NO_BUFFERING\n   * flag, which is supported on Windows Server 2008 and above.\n   *\n   * Ideally, we should use FileChannel#transferTo() across both POSIX and Windows\n   * platform. Unfortunately, the wrapper(Java_sun_nio_ch_FileChannelImpl_transferTo0)\n   * used by FileChannel#transferTo for unbuffered IO is not implemented on Windows.\n   * Based on OpenJDK 6/7/8 source code, Java_sun_nio_ch_FileChannelImpl_transferTo0\n   * on Windows simply returns IOS_UNSUPPORTED.\n   *\n   * Note: This simple native wrapper does minimal parameter checking before copy and\n   * consistency check (e.g., size) after copy.\n   * It is recommended to use wrapper function like\n   * the Storage#nativeCopyFileUnbuffered() function in hadoop-hdfs with pre/post copy\n   * checks.\n   *\n   * @param src                  The source path\n   * @param dst                  The destination path\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.BlockLocation:setHosts(java.lang.String[])" : "* Set the hosts hosting this block.\n   * @param hosts hosts array.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.ha.ZKFailoverController:doFence(org.apache.hadoop.ha.HAServiceTarget)" : null,
  "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:size()" : null,
  "org.apache.hadoop.io.LongWritable:set(long)" : "* Set the value of this LongWritable.\n   * @param value value.",
  "org.apache.hadoop.util.PriorityQueue:clear()" : "Removes all entries from the PriorityQueue.",
  "org.apache.hadoop.io.VersionMismatchException:toString()" : "Returns a string representation of this object.",
  "org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)" : "* Create a MD5FileChecksum.\n   *\n   * @param bytesPerCRC bytesPerCRC.\n   * @param crcPerBlock crcPerBlock.\n   * @param md5 md5.",
  "org.apache.hadoop.fs.FSInputChecker:verifySums(byte[],int,int)" : null,
  "org.apache.hadoop.fs.AvroFSInput:close()" : null,
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:ratioRemove(int[])" : "* Chooses the bit position that minimizes the number of false negative generated while maximizing.\n   * the number of false positive removed.\n   * @param h The different bit positions.\n   * @return The position that minimizes the number of false negative generated while maximizing.",
  "org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletionIgnoringExceptions(java.util.concurrent.CompletableFuture)" : "* Wait for a single of future to complete, ignoring exceptions raised.\n   * @param future future to wait for.\n   * @param <T> Generics Type T.",
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:computeRatio()" : "* Computes the ratio A/FP.",
  "org.apache.hadoop.fs.shell.Command:displayError(java.lang.String)" : "* Display an error string prefaced with the command name.  Also increments\n   * the error count for the command which will result in a non-zero exit\n   * code.\n   * @param message error message to display",
  "org.apache.hadoop.metrics2.AbstractMetric:name()" : null,
  "org.apache.hadoop.conf.ConfigurationWithLogging:getLong(java.lang.String,long)" : "* See {@link Configuration#getLong(String, long)}.",
  "org.apache.hadoop.fs.Path:hasWindowsDrive(java.lang.String)" : null,
  "org.apache.hadoop.util.SysInfoLinux:getNetworkBytesWritten()" : "{@inheritDoc}",
  "org.apache.hadoop.security.FastSaslServerFactory:<init>(java.util.Map)" : null,
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerMinimum(java.lang.String,java.util.concurrent.atomic.AtomicInteger)" : "* Add a minimum statistic to dynamically return the\n   * latest value of the source.\n   * @param key key of this statistic\n   * @param source atomic int minimum\n   * @return the builder.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)" : "* Cancels a delegation token from the server end-point. It does not require\n   * being authenticated by the configured <code>Authenticator</code>.\n   *\n   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs\n   * are supported.\n   * @param token the authentication token with the Delegation Token to cancel.\n   * @throws IOException if an IO error occurred.",
  "org.apache.hadoop.ha.ZKFailoverController:printUsage()" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:createTransaction(java.util.List,java.lang.String)" : null,
  "org.apache.hadoop.security.http.XFrameOptionsFilter$XFrameOptionsResponseWrapper:setIntHeader(java.lang.String,int)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(org.apache.hadoop.io.BytesWritable)" : "* Copy the key into BytesWritable. The input BytesWritable will be\n         * automatically resized to the actual key size.\n         * \n         * @param key\n         *          BytesWritable to hold the key.\n         * @throws IOException raised on errors performing I/O.\n         * @return the key into BytesWritable.",
  "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:setRandom(java.util.Random)" : null,
  "org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:createFilterConfig(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner:run()" : null,
  "org.apache.hadoop.util.ProcessUtils:getPid()" : null,
  "org.apache.hadoop.io.Text:decode(byte[])" : "* @return Converts the provided byte array to a String using the\n   * UTF-8 encoding. If the input is malformed,\n   * replace by a default value.\n   *\n   * @param utf8 input utf8.\n   * @throws CharacterCodingException when a character\n   *                                  encoding or decoding error occurs.",
  "org.apache.hadoop.fs.shell.find.Find:addStop(org.apache.hadoop.fs.shell.PathData)" : "Add the {@link PathData} item to the stop set.",
  "org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration)" : "* Get crypto codec for algorithm/mode/padding in config value\n   * hadoop.security.crypto.cipher.suite\n   * \n   * @param conf\n   *          the configuration\n   * @return CryptoCodec the codec object Null value will be returned if no\n   *         crypto codec classes with cipher suite configured.",
  "org.apache.hadoop.fs.FilterFs:open(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FsShell:printInstanceHelp(java.io.PrintStream,org.apache.hadoop.fs.shell.Command)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:dataSize()" : "* Get the amount of data; if there is no buffer then the size is 0.\n     *\n     * @return the amount of data available to upload.",
  "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyManagers()" : null,
  "org.apache.hadoop.util.dynamic.BindingUtils:extractIOEs(java.util.function.Supplier)" : "* Invoke the supplier, catching any {@code UncheckedIOException} raised,\n   * extracting the inner IOException and rethrowing it.\n   * @param call call to invoke\n   * @return result\n   * @param <T> type of result\n   * @throws IOException if the call raised an IOException wrapped by an UncheckedIOException.",
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey)" : null,
  "org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer:open(java.io.OutputStream)" : null,
  "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:<init>(int,int,long,java.util.concurrent.TimeUnit,java.util.concurrent.BlockingQueue)" : null,
  "org.apache.hadoop.io.BytesWritable:equals(java.lang.Object)" : "* Are the two byte sequences equal?",
  "org.apache.hadoop.util.JvmPauseMonitor:<init>()" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.shell.PathData,boolean)" : null,
  "org.apache.hadoop.util.bloom.Key:getWeight()" : "@return Returns the weight associated to <i>this</i> key.",
  "org.apache.hadoop.fs.impl.FlagSet:toConfigurationString()" : "* Convert to a string which can be then set in a configuration.\n   * This is effectively a marshalled form of the flags.\n   * @return a comma separated list of flag names.",
  "org.apache.hadoop.fs.FileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])" : "* Given an opaque iteration token, return the next batch of entries in a\n   * directory. This is a private API not meant for use by end users.\n   * <p>\n   * This method should be overridden by FileSystem subclasses that want to\n   * use the generic {@link FileSystem#listStatusIterator(Path)} implementation.\n   * @param f Path to list\n   * @param token opaque iteration token returned by previous call, or null\n   *              if this is the first call.\n   * @return directory entries.\n   * @throws FileNotFoundException when the path does not exist.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.metrics2.impl.SinkQueue:clearConsumerLock()" : null,
  "org.apache.hadoop.http.ProfileServlet:getEvent(javax.servlet.http.HttpServletRequest)" : null,
  "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:getNumAllUnits()" : "* The number of all the involved units in the coding.\n   * @return count of all the data units and parity units",
  "org.apache.hadoop.io.ArrayPrimitiveWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:listStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.conf.StorageUnit:toString()" : null,
  "org.apache.hadoop.ipc.Server$Connection:getLastContact()" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(java.util.Optional,java.util.Optional)" : "* Constructor with both optional path and path handle.\n   * Either or both argument may be empty, but it is an error for\n   * both to be defined.\n   * @param optionalPath a path or empty\n   * @param optionalPathHandle a path handle/empty\n   * @throws IllegalArgumentException if both parameters are set.",
  "org.apache.hadoop.util.ExitUtil$HaltException:<init>(int,java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.fs.FileUtil:setPermission(java.io.File,org.apache.hadoop.fs.permission.FsPermission)" : "* Set permissions to the required value. Uses the java primitives instead\n   * of forking if group == other.\n   * @param f the file to change\n   * @param permission the new permissions\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getCurrentKeyId()" : "* Obtains the value of the last delegation key id.\n   * @return Last delegation key id.",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:decompress(byte[],int,int)" : null,
  "org.apache.hadoop.service.AbstractService:recordLifecycleEvent()" : "* Add a state change event to the lifecycle history",
  "org.apache.hadoop.util.RunJar:run(java.lang.String[])" : null,
  "org.apache.hadoop.fs.FsServerDefaults:getChecksumType()" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:grow(int[],int)" : null,
  "org.apache.hadoop.fs.shell.Command:getName()" : "* The name of the command.  Will first try to use the assigned name\n   * else fallback to the command's preferred name\n   * @return name of the command",
  "org.apache.hadoop.fs.FsServerDefaults:<init>()" : null,
  "org.apache.hadoop.util.functional.FunctionalIO:uncheckIOExceptions(org.apache.hadoop.util.functional.CallableRaisingIOE)" : "* Invoke any operation, wrapping IOExceptions with\n   * {@code UncheckedIOException}.\n   * @param call callable\n   * @param <T> type of result\n   * @return result\n   * @throws UncheckedIOException if an IOE was raised.",
  "org.apache.hadoop.tracing.TraceScope:close()" : null,
  "org.apache.hadoop.util.MachineList:includes(java.lang.String)" : "* Accepts an ip address and return true if ipAddress is in the list.\n   * {@link #includes(InetAddress)} should be preferred\n   * to avoid possibly re-resolving the ip address.\n   *\n   * @param ipAddress ipAddress.\n   * @return true if ipAddress is part of the list",
  "org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.io.compress.DefaultCodec:createDirectDecompressor()" : "* {@inheritDoc}",
  "org.apache.hadoop.io.ObjectWritable:writeObject(java.io.DataOutput,java.lang.Object,java.lang.Class,org.apache.hadoop.conf.Configuration)" : "* Write a {@link Writable}, {@link String}, primitive type, or an array of\n   * the preceding.\n   *\n   * @param out DataOutput.\n   * @param instance instance.\n   * @param conf Configuration.\n   * @param declaredClass declaredClass.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.alias.CredentialProvider:noPasswordWarning()" : "* If a password for the provider is needed, but is not provided, this will\n   * return a warning and instructions for supplying said password to the\n   * provider.\n   * @return A warning and instructions for supplying the password",
  "org.apache.hadoop.fs.FileSystem$DirectoryEntries:getEntries()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Writer:finishDataBlock(boolean)" : "* Close the current data block if necessary.\n     * \n     * @param bForceFinish\n     *          Force the closure regardless of the block size.\n     * @throws IOException",
  "org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int)" : "* Returns a proxy to connect to the target HA service for health monitoring.\n   * If {@link #getHealthMonitorAddress()} is implemented to return a non-null\n   * address, then this proxy will connect to that address.  Otherwise, the\n   * returned proxy defaults to using {@link #getAddress()}, which means this\n   * method's behavior is identical to {@link #getProxy(Configuration, int)}.\n   *\n   * @param conf configuration.\n   * @param timeoutMs timeout in milliseconds\n   * @return a proxy to connect to the target HA service for health monitoring\n   * @throws IOException if there is an error",
  "org.apache.hadoop.ipc.RetryCache:unlock()" : null,
  "org.apache.hadoop.fs.protocolPB.PBHelper:<init>()" : null,
  "org.apache.hadoop.fs.shell.Delete$Rmr:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.metrics2.lib.MutableGaugeInt:incr(int)" : "* Increment by delta\n   * @param delta of the increment",
  "org.apache.hadoop.io.OutputBuffer$Buffer:getData()" : null,
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],int[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder)" : "* The constructor with all the necessary info.\n   * @param inputBlocks inputBlocks.\n   * @param erasedIndexes the indexes of erased blocks in inputBlocks array\n   * @param outputBlocks outputBlocks.\n   * @param rawDecoder rawDecoder.",
  "org.apache.hadoop.util.ComparableVersion$StringItem:<init>(java.lang.String,boolean)" : null,
  "org.apache.hadoop.security.token.Token$TrivialRenewer:getKind()" : null,
  "org.apache.hadoop.fs.UnionStorageStatistics:getLong(java.lang.String)" : null,
  "org.apache.hadoop.fs.impl.WrappedIOException:<init>(java.io.IOException)" : "* Construct from a non-null IOException.\n   * @param cause inner cause\n   * @throws NullPointerException if the cause is null.",
  "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationSerializer:close()" : null,
  "org.apache.hadoop.ipc.RemoteException:<init>(java.lang.String,java.lang.String)" : "* @param className wrapped exception, may be null\n   * @param msg may be null",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getAclStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.Text:readString(java.io.DataInput,int)" : "* @return Read a UTF8 encoded string with a maximum size.\n   * @param in input datainput.\n   * @param maxLength input maxLength.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.shell.Test:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:executorAcquired(java.time.Duration)" : null,
  "org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.net.unix.DomainSocket:accept()" : "* Accept a new UNIX domain connection.\n   *\n   * This method can only be used on sockets that were bound with bind().\n   *\n   * @return                The new connection.\n   * @throws IOException    If there was an I/O error performing the accept--\n   *                        such as the socket being closed from under us.\n   *                        Particularly when the accept is timed out, it throws\n   *                        SocketTimeoutException.",
  "org.apache.hadoop.io.LongWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : "* Authenticates a request looking for the <code>delegation</code>\n   * query-string parameter and verifying it is a valid token. If there is not\n   * <code>delegation</code> query-string parameter, it delegates the\n   * authentication to the {@link KerberosAuthenticationHandler} unless it is\n   * disabled.\n   *\n   * @param request the HTTP client request.\n   * @param response the HTTP client response.\n   * @return the authentication token for the authenticated request.\n   * @throws IOException thrown if an IO error occurred.\n   * @throws AuthenticationException thrown if the authentication failed.",
  "org.apache.hadoop.util.CombinedIPList:<init>(java.lang.String,java.lang.String,long)" : null,
  "org.apache.hadoop.ipc.AsyncCallLimitExceededException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.tracing.TraceUtils:byteStringToSpanContext(org.apache.hadoop.thirdparty.protobuf.ByteString)" : null,
  "org.apache.hadoop.security.LdapGroupsMapping:doGetGroups(java.lang.String,int)" : "* Perform LDAP queries to get group names of a user.\n   *\n   * Perform the first LDAP query to get the user object using the user's name.\n   * If one-query is enabled, retrieve the group names from the user object.\n   * If one-query is disabled, or if it failed, perform the second query to\n   * get the groups.\n   *\n   * @param user user name\n   * @return a list of group names for the user. If the user can not be found,\n   * return an empty string array.\n   * @throws NamingException if unable to get group names",
  "org.apache.hadoop.crypto.key.KeyShell:init(java.lang.String[])" : "* Parse the command line arguments and initialize the data.\n   * <pre>\n   * % hadoop key create keyName [-size size] [-cipher algorithm]\n   *    [-provider providerPath]\n   * % hadoop key roll keyName [-provider providerPath]\n   * % hadoop key list [-provider providerPath]\n   * % hadoop key delete keyName [-provider providerPath] [-i]\n   * % hadoop key invalidateCache keyName [-provider providerPath]\n   * </pre>\n   * @param args Command line arguments.\n   * @return 0 on success, 1 on failure.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller:run()" : null,
  "org.apache.hadoop.util.Progress:setStatus(java.lang.String)" : null,
  "org.apache.hadoop.util.OperationDuration:getDurationString()" : "* Return the duration as {@link #humanTime(long)}.\n   * @return a printable duration.",
  "org.apache.hadoop.fs.BBPartHandle:equals(java.lang.Object)" : null,
  "org.apache.hadoop.ha.ZKFailoverController:getCurrentActive()" : "* @return an {@link HAServiceTarget} for the current active node\n   * in the cluster, or null if no node is active.\n   * @throws IOException if a ZK-related issue occurs\n   * @throws InterruptedException if thread is interrupted",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartC()" : null,
  "org.apache.hadoop.io.nativeio.NativeIOException:getErrno()" : null,
  "org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)" : "* Add config variable for homedir the specified mount table\n   * @param conf - add to this conf\n   * @param homedir - the home dir path starting with slash\n   * @param mountTableName - the mount table.",
  "org.apache.hadoop.util.DataChecksum:update(byte[],int,int)" : null,
  "org.apache.hadoop.fs.Trash:checkpoint()" : "* Create a trash checkpoint.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.Client:nextCallId()" : "* Returns the next valid sequential call ID by incrementing an atomic counter\n   * and masking off the sign bit.  Valid call IDs are non-negative integers in\n   * the range [ 0, 2^31 - 1 ].  Negative numbers are reserved for special\n   * purposes.  The values can overflow back to 0 and be reused.  Note that prior\n   * versions of the client did not mask off the sign bit, so a server may still\n   * see a negative call ID if it receives connections from an old client.\n   * \n   * @return next call ID",
  "org.apache.hadoop.crypto.UnsupportedCodecException:<init>(java.lang.Throwable)" : "* Constructs a new exception with the specified cause.\n   * \n   * @param cause the cause",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults()" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.Class,java.lang.Class[])" : "* Checks for a method implementation.\n     * <p>\n     * The name passed to the constructor is the method name used.\n     * @param targetClass the class to check for an implementation\n     * @param argClasses argument classes for the method\n     * @return this Builder for method chaining",
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:start()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:setDelegationTokenSeqNum(int)" : "* For subclasses externalizing the storage, for example Zookeeper\n   * based implementations.\n   *\n   * @param seqNum seqNum.",
  "org.apache.hadoop.ipc.Client$Call:<init>(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable)" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getAggregator()" : null,
  "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:initPiggyBackIndexWithoutPBVec(int,int)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:isSupportSparseMetrics()" : "* @return whether sparse metrics are supported",
  "org.apache.hadoop.fs.viewfs.ViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.ipc.CallQueueManager:drainTo(java.util.Collection,int)" : null,
  "org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:getRequiredNumDataBlocks()" : "* Get required data blocks count in a BlockGroup.\n   * @return count of required data blocks",
  "org.apache.hadoop.io.file.tfile.Utils:readString(java.io.DataInput)" : "* Read a String as a VInt n, followed by n Bytes in Text format.\n   * \n   * @param in\n   *          The input stream.\n   * @return The string\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.conf.Configuration:reloadExistingConfigurations()" : "* Reload existing configuration instances.",
  "org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)" : "* Get the value of the <code>name</code> property as a <code>boolean</code>.  \n   * If no such property is specified, or if the specified value is not a valid\n   * <code>boolean</code>, then <code>defaultValue</code> is returned.\n   * \n   * @param name property name.\n   * @param defaultValue default value.\n   * @return property value as a <code>boolean</code>, \n   *         or <code>defaultValue</code>.",
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processKeyAddOrUpdate(byte[])" : null,
  "org.apache.hadoop.io.BoundedByteArrayOutputStream:getBuffer()" : "* Returns the underlying buffer.\n   *  Data is only valid to {@link #size()}.\n   * @return the underlying buffer.",
  "org.apache.hadoop.security.alias.KeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferData:setPrefetch(java.util.concurrent.Future)" : "* Indicates that a prefetch operation is in progress.\n   *\n   * @param actionFuture the {@code Future} of a prefetch action.\n   *\n   * @throws IllegalArgumentException if actionFuture is null.",
  "org.apache.hadoop.conf.StorageUnit$2:toBytes(double)" : null,
  "org.apache.hadoop.fs.Options$HandleOpt:resolve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Options$HandleOpt[])" : "* Utility function for mapping {@link FileSystem#getPathHandle} to a\n     * fixed set of handle options.\n     * @param fs Target filesystem\n     * @param opt Options to bind in partially evaluated function\n     * @return Function reference with options fixed",
  "org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:flushBuffer()" : null,
  "org.apache.hadoop.fs.permission.PermissionStatus:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)" : "* Constructor.\n   *\n   * @param user user.\n   * @param group group.\n   * @param permission permission.",
  "org.apache.hadoop.io.ArrayPrimitiveWritable:<init>()" : "* Construct an empty instance, for use during Writable read",
  "org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:getPrimitivePower(int,int)" : null,
  "org.apache.hadoop.io.DataOutputOutputStream:write(byte[],int,int)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMeanStatisticSample(java.lang.String,long)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:<init>(java.lang.Object)" : "* Instantiate.\n     * @param singleton single value...may be null",
  "org.apache.hadoop.util.bloom.BloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key)" : null,
  "org.apache.hadoop.net.SocketOutputStream:write(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.util.FindClass:loadResource(java.lang.String)" : "* Load a resource\n   * @param name resource name\n   * @return the status code",
  "org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>()" : null,
  "org.apache.hadoop.fs.WindowsGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder)" : null,
  "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Get all of the paths that currently exist in the working directories.\n     * @param pathStr the path underneath the roots\n     * @param conf the configuration to look up the roots in\n     * @return all of the paths that exist under any of the roots\n     * @throws IOException",
  "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationSerializer:open(java.io.OutputStream)" : null,
  "org.apache.hadoop.metrics2.util.MBeans:unregister(javax.management.ObjectName)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:truncate(org.apache.hadoop.fs.Path,long)" : "* The specification of this method matches that of\n   * {@link FileContext#truncate(Path, long)} except that Path f must be for\n   * this file system.\n   *\n   * @param f the path.\n   * @param newLength new length.\n   * @throws AccessControlException access control exception.\n   * @throws FileNotFoundException file not found exception.\n   * @throws UnresolvedLinkException unresolved link exception.\n   * @throws IOException raised on errors performing I/O.\n   * @return if successfully truncate success true, not false.",
  "org.apache.hadoop.io.file.tfile.TFile$Writer:initDataBlock()" : "* Check if we need to start a new data block.\n     * \n     * @throws IOException",
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:<init>(org.apache.hadoop.crypto.key.KeyProvider,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$CryptoExtension)" : "* This constructor is to be used by sub classes that provide\n   * delegating/proxying functionality to the {@link KeyProviderCryptoExtension}\n   *\n   * @param keyProvider key provider.\n   * @param extension crypto extension.",
  "org.apache.hadoop.fs.BlockLocation:<init>()" : "* Default Constructor.",
  "org.apache.hadoop.io.compress.CompressorStream:compress()" : null,
  "org.apache.hadoop.io.ArrayFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Construct an array reader for the named file.\n     * @param fs FileSystem.\n     * @param file file.\n     * @param conf configuration.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.hash.JenkinsHash:rot(long,int)" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:reencryptEncryptedKeys(java.util.List)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:getCoderName()" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:readFully(long,java.nio.ByteBuffer)" : "* Positioned readFully using {@link ByteBuffer}s. This method is thread-safe.",
  "org.apache.hadoop.io.DataOutputBuffer$Buffer:setCount(int)" : "* Set the count for the current buf.\n     * @param newCount the new count to set\n     * @return the original count",
  "org.apache.hadoop.util.StringUtils:equalsIgnoreCase(java.lang.String,java.lang.String)" : "* Compare strings locale-freely by using String#equalsIgnoreCase.\n   *\n   * @param s1  Non-null string to be converted\n   * @param s2  string to be converted\n   * @return     the str, converted to uppercase.",
  "org.apache.hadoop.ha.HAAdmin:addTransitionToActiveCliOpts(org.apache.commons.cli.Options)" : "* Add CLI options which are specific to the transitionToActive command and\n   * no others.",
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_means(java.io.Serializable)" : "* Get the means of an IOStatisticsSnapshot.\n   * Each value in the map is the (sample, sum) tuple of the values;\n   * the mean is then calculated by dividing sum/sample wherever sample count is non-zero.\n   * @param source source of statistics.\n   * @return a map of mean key to (sample, sum) tuples.",
  "org.apache.hadoop.service.AbstractService:setConfig(org.apache.hadoop.conf.Configuration)" : "* Set the configuration for this service.\n   * This method is called during {@link #init(Configuration)}\n   * and should only be needed if for some reason a service implementation\n   * needs to override that initial setting -for example replacing\n   * it with a new subclass of {@link Configuration}\n   * @param conf new configuration.",
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:setDictionary(byte[],int,int)" : null,
  "org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.String[])" : "* Initialize the registry with all rate names passed in.\n   * This is an alternative to the above init function since this metric\n   * can be used more than just for rpc name.\n   * @param names the array of all rate names",
  "org.apache.hadoop.fs.ContentSummary$Builder:quota(long)" : null,
  "org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)" : "* Construct an IPC client whose values are of the given {@link Writable}\n   * class.\n   *\n   * @param valueClass input valueClass.\n   * @param conf input configuration.\n   * @param factory input factory.",
  "org.apache.hadoop.util.ShutdownHookManager:addShutdownHook(java.lang.Runnable,int)" : "* Adds a shutdownHook with a priority, the higher the priority\n   * the earlier will run. ShutdownHooks with same priority run\n   * in a non-deterministic order.\n   *\n   * @param shutdownHook shutdownHook <code>Runnable</code>\n   * @param priority priority of the shutdownHook.",
  "org.apache.hadoop.service.CompositeService:stop(int,boolean)" : "* Stop the services in reverse order\n   *\n   * @param numOfServicesStarted index from where the stop should work\n   * @param stopOnlyStartedServices flag to say \"only start services that are\n   * started, not those that are NOTINITED or INITED.\n   * @throws RuntimeException the first exception raised during the\n   * stop process -<i>after all services are stopped</i>",
  "org.apache.hadoop.io.file.tfile.Utils:<init>()" : "* Prevent the instantiation of Utils.",
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write(int)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:refreshQueueSizeGauge()" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:deleteKey(java.lang.String)" : null,
  "org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.LongWritable)" : "* Transfers data from FileChannel using \n   * {@link FileChannel#transferTo(long, long, WritableByteChannel)}.\n   * Updates <code>waitForWritableTime</code> and <code>transferToTime</code>\n   * with the time spent blocked on the network and the time spent transferring\n   * data from disk to network respectively.\n   * \n   * Similar to readFully(), this waits till requested amount of \n   * data is transfered.\n   * \n   * @param fileCh FileChannel to transfer data from.\n   * @param position position within the channel where the transfer begins\n   * @param count number of bytes to transfer.\n   * @param waitForWritableTime nanoseconds spent waiting for the socket \n   *        to become writable\n   * @param transferToTime nanoseconds spent transferring data\n   * \n   * @throws EOFException \n   *         If end of input file is reached before requested number of \n   *         bytes are transfered.\n   *\n   * @throws SocketTimeoutException \n   *         If this channel blocks transfer longer than timeout for \n   *         this stream.\n   *          \n   * @throws IOException Includes any exception thrown by \n   *         {@link FileChannel#transferTo(long, long, WritableByteChannel)}.",
  "org.apache.hadoop.conf.ConfigurationWithLogging:getBoolean(java.lang.String,boolean)" : "* See {@link Configuration#getBoolean(String, boolean)}.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.FindClass:out(java.lang.String,java.lang.Object[])" : "* print something to stdout\n   * @param s string to print",
  "org.apache.hadoop.security.alias.CredentialShell:promptForCredential()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)" : null,
  "org.apache.hadoop.io.SortedMapWritable:entrySet()" : null,
  "org.apache.hadoop.log.LogThrottlingHelper$NoLogAction:getCount()" : null,
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:isSupported()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getTokenManager()" : null,
  "org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean,org.apache.hadoop.fs.BlockLocation[])" : "* Constructor.\n   *\n   * @param length a file's length\n   * @param isdir if the path is a directory\n   * @param block_replication the file's replication factor\n   * @param blocksize a file's block size\n   * @param modification_time a file's modification time\n   * @param access_time a file's access time\n   * @param permission a file's permission\n   * @param owner a file's owner\n   * @param group a file's group\n   * @param symlink symlink if the path is a symbolic link\n   * @param path the path's qualified name\n   * @param hasAcl entity has associated ACLs\n   * @param isEncrypted entity is encrypted\n   * @param isErasureCoded entity is erasure coded\n   * @param locations a file's block locations",
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)" : "* Construct an RPC server.\n     *\n     * @param protocolClass the class of protocol\n     * @param protocolImpl the protocolImpl whose methods will be called\n     * @param conf the configuration to use\n     * @param bindAddress the address to bind on to listen for connection\n     * @param port the port to listen for connections on\n     * @param numHandlers the number of method handler threads to run\n     * @param numReaders number of read threads\n     * @param queueSizePerHandler the size of the queue contained\n     *                            in each Handler\n     * @param verbose whether each call should be logged\n     * @param secretManager the server-side secret manager for each token type\n     * @param portRangeConfig A config parameter that can be used to restrict\n     * the range of ports used when port is 0 (an ephemeral port)\n     * @param alignmentContext provides server state info on client responses\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.RefreshRegistry:unregisterAll(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.GzipCodec:getCompressorType()" : null,
  "org.apache.hadoop.io.compress.BlockCompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)" : "* Create a {@link BlockCompressorStream} with given output-stream and \n   * compressor.\n   * Use default of 512 as bufferSize and compressionOverhead of \n   * (1% of bufferSize + 12 bytes) =  18 bytes (zlib algorithm).\n   * \n   * @param out stream\n   * @param compressor compressor to be used",
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getChecksumFilePos(long)" : null,
  "org.apache.hadoop.fs.FileUtil:makeShellPath(java.io.File,boolean)" : "* Convert a os-native filename to a path that works for the shell.\n   * @param file The filename to convert\n   * @param makeCanonicalPath\n   *          Whether to make canonical path for the file passed\n   * @return The unix pathname\n   * @throws IOException on windows, there can be problems with the subprocess",
  "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getUgi()" : null,
  "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:init(int)" : "* Initialize the metrics for JMX with priority levels.\n   * @param numLevels input numLevels.",
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:next()" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getPathHandle()" : "* Get the PathHandle: only valid if constructed with a PathHandle.\n   * @return the PathHandle\n   * @throws NoSuchElementException if the field is empty.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumNear(long)" : "* Get the RecordNum for the first key-value pair in a compressed block\n     * whose byte offset in the TFile is greater than or equal to the specified\n     * offset.\n     * \n     * @param offset\n     *          the user supplied offset.\n     * @return the RecordNum to the corresponding entry. If no such entry\n     *         exists, it returns the total entry count.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>(int,int,int)" : "* Creates a new compressor using the specified block size.\n   * Compressed data will be generated in bzip2 format.\n   * \n   * @param blockSize The block size to be used for compression.  This is\n   *        an integer from 1 through 9, which is multiplied by 100,000 to \n   *        obtain the actual block size in bytes.\n   * @param workFactor This parameter is a threshold that determines when a \n   *        fallback algorithm is used for pathological data.  It ranges from\n   *        0 to 250.\n   * @param directBufferSize Size of the direct buffer to be used.",
  "org.apache.hadoop.fs.FSDataInputStream:read(long,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.crypto.random.OpensslSecureRandom:nextBytes(byte[])" : "* Generates a user-specified number of random bytes.\n   * It's thread-safe.\n   * \n   * @param bytes the array to be filled in with random bytes.",
  "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheHit()" : null,
  "org.apache.hadoop.conf.Configuration:addResource(java.lang.String)" : "* Add a configuration resource. \n   * \n   * The properties of this resource will override properties of previously \n   * added resources, unless they were marked <a href=\"#Final\">final</a>. \n   * \n   * @param name resource to be added, the classpath is examined for a file \n   *             with that name.",
  "org.apache.hadoop.fs.shell.Ls:isHideNonPrintable()" : null,
  "org.apache.hadoop.fs.Options$ChecksumOpt:processChecksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt,org.apache.hadoop.fs.Options$ChecksumOpt)" : "* A helper method for processing user input and default value to \n     * create a combined checksum option. \n     *\n     * @param defaultOpt Default checksum option\n     * @param userOpt User-specified checksum option\n     *\n     * @return ChecksumOpt.",
  "org.apache.hadoop.util.AsyncDiskService:<init>(java.lang.String[])" : "* Create a AsyncDiskServices with a set of volumes (specified by their\n   * root directories).\n   * \n   * The AsyncDiskServices uses one ThreadPool per volume to do the async\n   * disk operations.\n   * \n   * @param volumes The roots of the file system volumes.",
  "org.apache.hadoop.net.SocksSocketFactory:<init>(java.net.Proxy)" : "* Constructor with a supplied Proxy\n   * \n   * @param proxy the proxy to use to create sockets",
  "org.apache.hadoop.fs.AbstractFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)" : "* Retrieve the storage policy for a given file or directory.\n   *\n   * @param src file or directory path.\n   * @return storage policy for give file.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.lib.MutableQuantiles:setQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.text.DecimalFormat)" : "* Sets quantileInfo.\n   *\n   * @param ucName capitalized name of the metric\n   * @param uvName capitalized type of the values\n   * @param desc uncapitalized long-form textual description of the metric\n   * @param lvName uncapitalized type of the values\n   * @param pDecimalFormat Number formatter for percentile value",
  "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithProportionalSleep:calculateSleepTime(int)" : null,
  "org.apache.hadoop.io.erasurecode.ECBlockGroup:getDataBlocks()" : "* Get data blocks\n   * @return data blocks",
  "org.apache.hadoop.util.Shell:run()" : "* Check to see if a command needs to be executed and execute if needed.\n   *\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttribute(java.lang.String)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:hasNext()" : null,
  "org.apache.hadoop.fs.FileSystem:exists(org.apache.hadoop.fs.Path)" : "Check if a path exists.\n   *\n   * It is highly discouraged to call this method back to back with other\n   * {@link #getFileStatus(Path)} calls, as this will involve multiple redundant\n   * RPC calls in HDFS.\n   *\n   * @param f source path\n   * @return true if the path exists\n   * @throws IOException IO failure",
  "org.apache.hadoop.io.BytesWritable:getBytes()" : "* Get the data backing the BytesWritable. Please use {@link #copyBytes()}\n   * if you need the returned array to be precisely the length of the data.\n   * @return The data is only valid between 0 and getLength() - 1.",
  "org.apache.hadoop.io.compress.DefaultCodec:getDefaultExtension()" : null,
  "org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getPackages()" : null,
  "org.apache.hadoop.ipc.Server:logSlowRpcCalls(java.lang.String,org.apache.hadoop.ipc.Server$Call,org.apache.hadoop.ipc.ProcessingDetails)" : "* Logs a Slow RPC Request.\n   *\n   * @param methodName - RPC Request method name\n   * @param details - Processing Detail.\n   *\n   * If a request took significant more time than other requests,\n   * and its processing time is at least `logSlowRPCThresholdMs` we consider that as a slow RPC.\n   *\n   * The definition rules for calculating whether the current request took too much time\n   * compared to other requests are as follows:\n   * 3 is a magic number that comes from 3 sigma deviation.\n   * A very simple explanation can be found by searching for 68-95-99.7 rule.\n   * We flag an RPC as slow RPC if and only if it falls above 99.7% of requests.\n   * We start this logic only once we have enough sample size.",
  "org.apache.hadoop.util.ZKUtil$ZKAuthInfo:getScheme()" : null,
  "org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:login()" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine:ensureInitialized()" : "* Initialize this class if it isn't already.",
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:toString()" : null,
  "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:getKeyStoreType()" : null,
  "org.apache.hadoop.fs.shell.CopyCommands$Merge:processArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:decompress(byte[],int,int)" : null,
  "org.apache.hadoop.fs.QuotaUsage:getTypesQuotaUsage(boolean,java.util.List)" : null,
  "org.apache.hadoop.io.IOUtils:writeFully(java.nio.channels.WritableByteChannel,java.nio.ByteBuffer)" : "* Write a ByteBuffer to a WritableByteChannel, handling short writes.\n   * \n   * @param bc               The WritableByteChannel to write to\n   * @param buf              The input buffer\n   * @throws IOException     On I/O error",
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeMBeanName(javax.management.ObjectName)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.conf.StorageUnit$2:toGBs(double)" : null,
  "org.apache.hadoop.fs.StorageType:getConf(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.StorageType,java.lang.String)" : "* Get the configured values for different StorageType.\n   * @param conf - absolute or fully qualified path\n   * @param t - the StorageType\n   * @param name - the sub-name of key\n   * @return the file system of the path",
  "org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.util.Map)" : null,
  "org.apache.hadoop.security.token.DtUtilShell$Get:isGenericUrl()" : null,
  "org.apache.hadoop.util.functional.TaskPool:foreach(org.apache.hadoop.fs.RemoteIterator)" : "* Create a task builder for the remote iterator.\n   * @param items item source.\n   * @param <I> type of result.\n   * @return builder.",
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:get(java.lang.Object)" : null,
  "org.apache.hadoop.ha.ShellCommandFencer:setConfAsEnvVars(java.util.Map)" : "* Set the environment of the subprocess to be the Configuration,\n   * with '.'s replaced by '_'s.",
  "org.apache.hadoop.io.WritableUtils:readVIntInRange(java.io.DataInput,int,int)" : "* Reads an integer from the input stream and returns it.\n   *\n   * This function validates that the integer is between [lower, upper],\n   * inclusive.\n   *\n   * @param stream Binary input stream\n   * @param lower input lower.\n   * @param upper input upper.\n   * @throws IOException raised on errors performing I/O.\n   * @return deserialized integer from stream.",
  "org.apache.hadoop.io.MapWritable:<init>()" : "Default constructor.",
  "org.apache.hadoop.fs.LocalFileSystem:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)" : "* Moves files to a bad file directory on the same device, so that their\n   * storage will not be reused.",
  "org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(java.lang.Object)" : "* @return Get the 'value' corresponding to the last read 'key'.\n     * @param val : The 'value' to be read.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.Trash:isEnabled()" : "* Returns whether the trash is enabled for this filesystem.\n   *\n   * @return return if isEnabled true,not false.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getMetrics()" : null,
  "org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])" : null,
  "org.apache.hadoop.ipc.Schedulable:getCallerContext()" : "* This is overridden only in {@link Server.Call}.\n   * The CallerContext field will be used to carry information\n   * about the user in cases where UGI proves insufficient.\n   * Any other classes that might try to use this method,\n   * will get an UnsupportedOperationException.\n   *\n   * @return an instance of CallerContext if method\n   * is overridden else get an UnsupportedOperationException",
  "org.apache.hadoop.ipc.ResponseBuffer:reset()" : null,
  "org.apache.hadoop.util.functional.FutureIO:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)" : "* Propagate options to any builder, converting everything with the\n   * prefix to an option where, if there were 2+ dot-separated elements,\n   * it is converted to a schema.\n   * <pre>\n   *   fs.example.s3a.option becomes \"s3a.option\"\n   *   fs.example.fs.io.policy becomes \"fs.io.policy\"\n   *   fs.example.something becomes \"something\"\n   * </pre>\n   * @param builder builder to modify\n   * @param conf configuration to read\n   * @param prefix prefix to scan/strip\n   * @param mandatory are the options to be mandatory or optional?",
  "org.apache.hadoop.ipc.Server$Call:getProtocol()" : null,
  "org.apache.hadoop.fs.DF:getFilesystem()" : "* @return a string indicating which filesystem volume we're checking.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.conf.StorageUnit$3:toKBs(double)" : null,
  "org.apache.hadoop.ipc.RpcClientUtil:isMethodSupported(java.lang.Object,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcKind,long,java.lang.String)" : "* Returns whether the given method is supported or not.\n   * The protocol signatures are fetched and cached. The connection id for the\n   * proxy provided is re-used.\n   * @param rpcProxy Proxy which provides an existing connection id.\n   * @param protocol Protocol for which the method check is required.\n   * @param rpcKind The RpcKind for which the method check is required.\n   * @param version The version at the client.\n   * @param methodName Name of the method.\n   * @return true if the method is supported, false otherwise.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getGaugeReference(java.lang.String)" : null,
  "org.apache.hadoop.net.unix.DomainSocket:bindAndListen(java.lang.String)" : "* Create a new DomainSocket listening on the given path.\n   *\n   * @param path         The path to bind and listen on.\n   * @return             The new DomainSocket.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.FileSystemLinkResolver:resolve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Attempt calling overridden {@link #doCall(Path)} method with\n   * specified {@link FileSystem} and {@link Path}. If the call fails with an\n   * UnresolvedLinkException, it will try to resolve the path and retry the call\n   * by calling {@link #next(FileSystem, Path)}.\n   * @param filesys FileSystem with which to try call\n   * @param path Path with which to try call\n   * @return Generic type determined by implementation\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingSampleCount()" : "* Returns the number of samples that we have seen so far.\n   * @return long",
  "org.apache.hadoop.metrics2.MetricsException:<init>(java.lang.String)" : "* Construct the exception with a message\n   * @param message for the exception",
  "org.apache.hadoop.http.HttpServer2Metrics:requestTimeMean()" : null,
  "org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:getProtocolSignature(java.lang.String,long,int)" : null,
  "org.apache.hadoop.ipc.ProtobufHelper:getByteString(byte[])" : "* Get the byte string of a non-null byte array.\n   * If the array is 0 bytes long, return a singleton to reduce object allocation.\n   * @param bytes bytes to convert.\n   * @return a value",
  "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:execShellGetUserForNetgroup(java.lang.String)" : "* Calls shell to get users for a netgroup by calling getent\n   * netgroup, this is a low level function that just returns string\n   * that \n   *\n   * @param netgroup get users for this netgroup\n   * @return string of users for a given netgroup in getent netgroups format\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ha.ZKFailoverController:becomeActive()" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyCompressor:reset()" : "* Resets compressor so that a new set of input data can be processed.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incAtomicLong(java.util.concurrent.atomic.AtomicLong,long)" : "* increment an atomic long and return its value;\n   * null long is no-op returning 0.\n   * @param aLong atomic long; may be null\n   * param increment amount to increment; negative for a decrement\n   * @return final value or 0 if the long is null",
  "org.apache.hadoop.crypto.random.OsSecureRandom:getConf()" : null,
  "org.apache.hadoop.ipc.Server$ConnectionManager:isFull()" : null,
  "org.apache.hadoop.security.token.DtFileOperations:getTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)" : "Fetch a token from a service and save to file in the local filesystem.\n   *  @param tokenFile a local File object to hold the output.\n   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output\n   *  @param alias overwrite service field of fetched token with this text.\n   *  @param service use a DtFetcher implementation matching this service text.\n   *  @param url pass this URL to fetcher after stripping any http/s prefix.\n   *  @param renewer pass this renewer to the fetcher.\n   *  @param conf Configuration object passed along.\n   *  @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.impl.prefetch.BlockData:getRelativeOffset(int,long)" : "* Gets the relative offset corresponding to the given block and the absolute offset.\n   * @param blockNumber the id of the given block.\n   * @param offset absolute offset in the file.\n   * @return the relative offset corresponding to the given block and the absolute offset.\n   * @throws IllegalArgumentException if either blockNumber or offset is invalid.",
  "org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.Path)" : "* Call {@link #mkdirs(Path, FsPermission)} with default permission.\n   * @param f path\n   * @return true if the directory was created\n   * @throws IOException IO failure",
  "org.apache.hadoop.util.ShutdownHookManager$HookEntry:hashCode()" : null,
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)" : "* Re-encrypts an encrypted key version, using its initialization vector\n   * and key material, but with the latest key version name of its key name\n   * in the key provider.\n   * <p>\n   * If the latest key version name in the provider is the\n   * same as the one encrypted the passed-in encrypted key version, the same\n   * encrypted key version is returned.\n   * <p>\n   * NOTE: The generated key is not stored by the <code>KeyProvider</code>\n   *\n   * @param  ekv The EncryptedKeyVersion containing keyVersionName and IV.\n   * @return     The re-encrypted EncryptedKeyVersion.\n   * @throws IOException If the key material could not be re-encrypted\n   * @throws GeneralSecurityException If the key material could not be\n   *                            re-encrypted because of a cryptographic issue.",
  "org.apache.hadoop.metrics2.impl.MetricsBuffer$Entry:records()" : null,
  "org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:hasCapability(java.lang.String)" : "* If the inner stream supports {@link StreamCapabilities},\n   * forward the probe to it.\n   * Otherwise: return false.\n   *\n   * @param capability string to query the stream support for.\n   * @return true if a capability is known to be supported.",
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:entrySet()" : "* Creating the entry set forces an evaluation of the functions.\n   *\n   * This is not a snapshot, so if the evaluators actually return\n   * references to mutable objects (e.g. a MeanStatistic instance)\n   * then that value may still change.\n   *\n   * The evaluation may be parallelized.\n   * @return an evaluated set of values",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:getGroup()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileChecksum(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.UTF8:getBytes()" : "@return The raw bytes.",
  "org.apache.hadoop.fs.LocatedFileStatus:equals(java.lang.Object)" : "Compare if this object is equal to another object\n   * @param   o the object to be compared.\n   * @return  true if two file status has the same path name; false if not.",
  "org.apache.hadoop.io.MultipleIOException$Builder:isEmpty()" : "* @return whether any exception was added.",
  "org.apache.hadoop.ipc.DecayRpcScheduler:shouldBackOff(org.apache.hadoop.ipc.Schedulable)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:asDuration()" : "* @return the global duration",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackStoreToken(org.apache.hadoop.util.functional.InvocationRaisingIOE)" : null,
  "org.apache.hadoop.util.concurrent.HadoopExecutors:newCachedThreadPool(java.util.concurrent.ThreadFactory)" : null,
  "org.apache.hadoop.util.CloseableReferenceCount:unreferenceCheckClosed()" : "* Decrement the reference count, checking to make sure that the\n   * CloseableReferenceCount is not closed.\n   *\n   * @throws AsynchronousCloseException  If the status is closed.",
  "org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo:<init>(org.apache.hadoop.ha.HAServiceProtocol$RequestSource)" : null,
  "org.apache.hadoop.ipc.Server:start()" : "Starts the service.  Must be called before any calls will be handled.",
  "org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:<init>(org.apache.hadoop.crypto.key.KeyProvider,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension)" : null,
  "org.apache.hadoop.security.Credentials:getTokenMap()" : "* Returns an unmodifiable version of the full map of aliases to Tokens.\n   *\n   * @return TokenMap.",
  "org.apache.hadoop.fs.store.DataBlocks$DataBlock:write(byte[],int,int)" : "* Write a series of bytes from the buffer, from the offset.\n     * Returns the number of bytes written.\n     * Only valid in the state {@code Writing}.\n     * Base class verifies the state but does no writing.\n     *\n     * @param buffer buffer.\n     * @param offset offset.\n     * @param length length of write.\n     * @return number of bytes written.\n     * @throws IOException trouble",
  "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration,byte[])" : "* Create a line reader that reads from the given stream using the\n   * <code>io.file.buffer.size</code> specified in the given\n   * <code>Configuration</code>, and using a custom delimiter of array of\n   * bytes.\n   * @param in input stream\n   * @param conf configuration\n   * @param recordDelimiterBytes The delimiter\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.shell.find.Name:addArguments(java.util.Deque)" : null,
  "org.apache.hadoop.net.NetUtils:getCanonicalUri(java.net.URI,int)" : "* Resolve the uri's hostname and add the default port if not in the uri\n   * @param uri to resolve\n   * @param defaultPort if none is given\n   * @return URI",
  "org.apache.hadoop.http.HttpServer2:createWebAppContext(org.apache.hadoop.http.HttpServer2$Builder,org.apache.hadoop.security.authorize.AccessControlList,java.lang.String)" : null,
  "org.apache.hadoop.util.Preconditions:checkState(boolean,java.lang.String,java.lang.Object[])" : "* Ensures the truth of an expression involving the state of the calling instance\n   * without involving any parameters to the calling method.\n   *\n   * <p>The message of the exception is {@code String.format(f, m)}.</p>\n   *\n   * @param expression a boolean expression\n   * @param errorMsg  the {@link String#format(String, Object...)}\n   *                 exception message if valid. Otherwise,\n   *                 the message is {@link #CHECK_STATE_EX_MESSAGE}\n   * @param errorMsgArgs the optional values for the formatted exception message.\n   * @throws IllegalStateException if {@code expression} is false",
  "org.apache.hadoop.io.compress.BlockDecompressorStream:getCompressedData()" : null,
  "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getVersions()" : null,
  "org.apache.hadoop.net.SocketInputStream:<init>(java.net.Socket,long)" : "* Same as SocketInputStream(socket.getChannel(), timeout): <br><br>\n   * \n   * Create a new input stream with the given timeout. If the timeout\n   * is zero, it will be treated as infinite timeout. The socket's\n   * channel will be configured to be non-blocking.\n   * \n   * @see SocketInputStream#SocketInputStream(ReadableByteChannel, long)\n   *  \n   * @param socket should have a channel associated with it.\n   * @param timeout timeout timeout in milliseconds. must not be negative.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class)" : "* Construct for a {@link WritableComparable} implementation.\n   * @param keyClass WritableComparable Class.",
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttributes(javax.management.AttributeList)" : null,
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:append()" : "* Append to an existing file (optional operation).",
  "org.apache.hadoop.metrics2.lib.MutableRollingAverages:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)" : null,
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:<init>()" : "* Create an empty instance.  Required for reflection.",
  "org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set)" : null,
  "org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:string2long(java.lang.String)" : "* Convert a string to long.\n     * The input string is first be trimmed\n     * and then it is parsed with traditional binary prefix.\n     *\n     * For example,\n     * \"-1230k\" will be converted to -1230 * 1024 = -1259520;\n     * \"891g\" will be converted to 891 * 1024^3 = 956703965184;\n     *\n     * @param s input string\n     * @return a long value represented by the input string.",
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferStartOffset()" : "* Gets the start of the current block's absolute offset.\n   *\n   * @return the start of the current block's absolute offset.",
  "org.apache.hadoop.util.JsonSerialization:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileStatus)" : "* Load from a Hadoop filesystem.\n   * If a file status is supplied, it's passed in to the openFile()\n   * call so that FS implementations can optimize their opening.\n   * @param fs filesystem\n   * @param path path\n   * @param status status of the file to open.\n   * @return a loaded object\n   * @throws PathIOException JSON parse problem\n   * @throws EOFException file status references an empty file\n   * @throws IOException IO problems",
  "org.apache.hadoop.jmx.JMXJsonServletNaNFiltered:extraCheck(java.lang.Object)" : null,
  "org.apache.hadoop.metrics2.lib.MutableCounterInt:incr(int)" : "* Increment the value by a delta\n   * @param delta of the increment",
  "org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])" : "* Construct a uncompressed writer from a set of options.\n     * @param conf the configuration to use\n     * @param opts the options used when creating the writer\n     * @throws IOException if it fails",
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:makeAbsolute(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Resolve against given working directory.\n   *\n   * @param workDir\n   * @param path\n   * @return absolute path",
  "org.apache.hadoop.fs.Path:compareTo(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable:run()" : null,
  "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:compareTo(java.util.concurrent.Delayed)" : null,
  "org.apache.hadoop.io.TwoDArrayWritable:toArray()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatusForFallbackLink()" : null,
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToActive(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto)" : null,
  "org.apache.hadoop.util.ReflectionUtils:getClass(java.lang.Object)" : "* Return the correctly-typed {@link Class} of the given object.\n   *  \n   * @param o object whose correctly-typed <code>Class</code> is to be obtained\n   * @param <T> Generics Type T.\n   * @return the correctly typed <code>Class</code> of the given object.",
  "org.apache.hadoop.io.DataOutputBuffer:<init>(org.apache.hadoop.io.DataOutputBuffer$Buffer)" : null,
  "org.apache.hadoop.metrics2.lib.MethodMetric:newTag(java.lang.Class)" : null,
  "org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:getAllStatistics()" : null,
  "org.apache.hadoop.fs.local.RawLocalFs:getServerDefaults(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:toString()" : null,
  "org.apache.hadoop.crypto.key.KeyProvider:buildVersionName(java.lang.String,int)" : "* Build a version string from a basename and version number. Converts\n   * \"/aaa/bbb\" and 3 to \"/aaa/bbb@3\".\n   * @param name the basename of the key\n   * @param version the version of the key\n   * @return the versionName of the key.",
  "org.apache.hadoop.ipc.CallQueueManager:drainTo(java.util.Collection)" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:getDatagramSocket()" : "* Used only by unit tests\n   * @return the datagramSocket for this sink",
  "org.apache.hadoop.ipc.Client:getPingInterval(org.apache.hadoop.conf.Configuration)" : "* Get the ping interval from configuration;\n   * If not set in the configuration, return the default value.\n   * \n   * @param conf Configuration\n   * @return the ping interval",
  "org.apache.hadoop.ipc.Server$Connection:buildSaslResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslState,byte[])" : null,
  "org.apache.hadoop.util.functional.TaskPool:throwOne(java.util.Collection)" : "* Throw one exception, adding the others as suppressed\n   * exceptions attached to the one thrown.\n   * This method never completes normally.\n   * @param exceptions collection of exceptions\n   * @param <E> class of exceptions\n   * @throws E an extracted exception.",
  "org.apache.hadoop.crypto.key.kms.KMSDelegationToken$KMSDelegationTokenIdentifier:<init>()" : null,
  "org.apache.hadoop.ipc.RefreshResponse:toString()" : null,
  "org.apache.hadoop.fs.shell.Stat:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.io.BytesWritable:getSize()" : "* Get the current size of the buffer.\n   * @deprecated Use {@link #getLength()} instead.\n   * @return current size of the buffer.",
  "org.apache.hadoop.ipc.Server:getNumDroppedConnections()" : "* The number of RPC connections dropped due to\n   * too many connections.\n   * @return the number of dropped rpc connections",
  "org.apache.hadoop.util.bloom.Filter:<init>()" : null,
  "org.apache.hadoop.ha.HealthMonitor:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceTarget)" : null,
  "org.apache.hadoop.io.compress.DefaultCodec:getDecompressorType()" : null,
  "org.apache.hadoop.util.bloom.CountingBloomFilter:toString()" : null,
  "org.apache.hadoop.ipc.Server$ExceptionsHandler:isTerseLog(java.lang.Class)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.ipc.ProtobufHelper:getRemoteException(org.apache.hadoop.thirdparty.protobuf.ServiceException)" : "* Return the IOException thrown by the remote server wrapped in\n   * ServiceException as cause.\n   * @param se ServiceException that wraps IO exception thrown by the server\n   * @return Exception wrapped in ServiceException or\n   *         a new IOException that wraps the unexpected ServiceException.",
  "org.apache.hadoop.fs.permission.FsPermission:hashCode()" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:getStringData(java.lang.String,org.apache.zookeeper.data.Stat)" : "* Get the data in a ZNode.\n   * @param path Path of the ZNode.\n   * @param stat Output statistics of the ZNode.\n   * @return The data in the ZNode.\n   * @throws Exception If it cannot contact Zookeeper.",
  "org.apache.hadoop.io.SequenceFile$Reader:init(boolean)" : "* Initialize the {@link Reader}\n     * @param tmpReader <code>true</code> if we are constructing a temporary\n     *                  reader {@link SequenceFile.Sorter.cloneFileAttributes}, \n     *                  and hence do not initialize every component; \n     *                  <code>false</code> otherwise.\n     * @throws IOException",
  "org.apache.hadoop.ipc.DecayRpcScheduler:parseServiceUserNames(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec:getDecompressorType()" : null,
  "org.apache.hadoop.util.Shell:fileNotFoundException(java.lang.String,java.lang.Exception)" : "* Create a {@code FileNotFoundException} with the inner nested cause set\n   * to the given exception. Compensates for the fact that FNFE doesn't\n   * have an initializer that takes an exception.\n   * @param text error text\n   * @param ex inner exception\n   * @return a new exception to throw.",
  "org.apache.hadoop.io.SequenceFile$Writer:append(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)" : "* Append a key/value pair.\n     * @param key input Writable key.\n     * @param val input Writable val.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.Text:set(org.apache.hadoop.io.Text)" : "* Copy a text.\n   * @param other other.",
  "org.apache.hadoop.io.SortedMapWritable:tailMap(java.lang.Object)" : null,
  "org.apache.hadoop.crypto.key.KeyShell:prettifyException(java.lang.Exception)" : null,
  "org.apache.hadoop.fs.ContentSummary$Builder:spaceQuota(long)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:start()" : null,
  "org.apache.hadoop.ipc.Client$ConnectionId:isEqual(java.lang.Object,java.lang.Object)" : null,
  "org.apache.hadoop.ipc.DefaultRpcScheduler:getPriorityLevel(org.apache.hadoop.ipc.Schedulable)" : null,
  "org.apache.hadoop.io.compress.CompressionCodecFactory:main(java.lang.String[])" : "* A little test program.\n   * @param args arguments.\n   * @throws Exception exception.",
  "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:add(org.apache.hadoop.net.Node)" : "Add a leaf node\n   * Update node counter &amp; rack counter if necessary\n   * @param node node to be added; can be null\n   * @exception IllegalArgumentException if add a node to a leave \n   *                                     or node to be added is not a leaf",
  "org.apache.hadoop.util.Shell:getGetPermissionCommand()" : "* Return a command to get permission information.\n   *\n   * @return permission command.",
  "org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:<init>(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterClosedState()" : "* Enter the closed state.\n     *\n     * @return true if the class was in any other state, implying that\n     * the subclass should do its close operations.",
  "org.apache.hadoop.security.UserGroupInformation:getBestUGI(java.lang.String,java.lang.String)" : "* Find the most appropriate UserGroupInformation to use\n   *\n   * @param ticketCachePath    The Kerberos ticket cache path, or NULL\n   *                           if none is specfied\n   * @param user               The user name, or NULL if none is specified.\n   *\n   * @return                   The most appropriate UserGroupInformation\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.permission.AclStatus$Builder:addEntries(java.lang.Iterable)" : "* Adds a list of ACL entries.\n     *\n     * @param entries AclEntry entries to add\n     * @return Builder this builder, for call chaining",
  "org.apache.hadoop.security.LdapGroupsMapping:cacheGroupsAdd(java.util.List)" : "* Adds groups to cache, no need to do that for this provider\n   *\n   * @param groups unused",
  "org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String)" : "* Create an InetSocketAddress from the given target string and\n   * default port. If the string cannot be parsed correctly, the\n   * <code>configName</code> parameter is used as part of the\n   * exception message, allowing the user to better diagnose\n   * the misconfiguration.\n   *\n   * @param target a string of either \"host\" or \"host:port\"\n   * @param defaultPort the default port if <code>target</code> does not\n   *                    include a port number\n   * @param configName the name of the configuration from which\n   *                   <code>target</code> was loaded. This is used in the\n   *                   exception message in the case that parsing fails.\n   * @return socket addr.",
  "org.apache.hadoop.security.SaslRpcServer:init(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:finish()" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:end()" : "* Resets decompressor and input and output buffers so that a new set of\n   * input data can be processed.",
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getUri()" : null,
  "org.apache.hadoop.util.concurrent.HadoopExecutors:newSingleThreadScheduledExecutor()" : null,
  "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:protoFromToken(org.apache.hadoop.security.token.Token)" : "* Create a {@code TokenProto} instance\n   * from a hadoop token.\n   * This builds and caches the fields\n   * (identifier, password, kind, service) but not\n   * renewer or any payload.\n   * @param tok token\n   * @return a marshallable protobuf class.",
  "org.apache.hadoop.security.UserGroupInformation:createProxyUser(java.lang.String,org.apache.hadoop.security.UserGroupInformation)" : "* Create a proxy user using username of the effective user and the ugi of the\n   * real user.\n   * @param user user.\n   * @param realUser realUser.\n   * @return proxyUser ugi",
  "org.apache.hadoop.crypto.key.KeyProviderExtension:getKeys()" : null,
  "org.apache.hadoop.fs.shell.Ls:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.http.HttpServer2:setContextAttributes(org.eclipse.jetty.servlet.ServletContextHandler,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:startUpload()" : null,
  "org.apache.hadoop.ipc.Server:getPurgeIntervalNanos()" : null,
  "org.apache.hadoop.io.ArrayWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.util.GenericsUtil:isLog4jLogger(java.lang.Class)" : "* Determine whether the log of <code>clazz</code> is Log4j implementation.\n   * @param clazz a class to be determined\n   * @return true if the log of <code>clazz</code> is Log4j implementation.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:snapshotMap(java.util.Map,java.util.function.Function)" : "* Take a snapshot of a supplied map, using the copy function\n   * to replicate the source values.\n   * @param source source map\n   * @param copyFn function to copy the value\n   * @param <E> type of values.\n   * @return a concurrent hash map referencing the same values.",
  "org.apache.hadoop.fs.FileSystem:newInstanceLocal(org.apache.hadoop.conf.Configuration)" : "* Get a unique local FileSystem object.\n   * @param conf the configuration to configure the FileSystem with\n   * @return a new LocalFileSystem object.\n   * @throws IOException FS creation or initialization failure.",
  "org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[])" : null,
  "org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:gracefulFailover(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.ZKFCProtocolProtos$GracefulFailoverRequestProto)" : null,
  "org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:setSchema(org.apache.hadoop.io.erasurecode.ECSchema)" : "* Set EC schema.\n   * @param schema schema.",
  "org.apache.hadoop.ha.HealthMonitor:start()" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:stripRoot()" : null,
  "org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],boolean)" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:finished()" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:size()" : "* Gets the number of blocks in this cache.",
  "org.apache.hadoop.fs.DelegateToFileSystem:getLinkTarget(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:putAll(java.util.Map)" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[])" : "* Constructor given input blocks and output blocks.\n   *\n   * @param inputBlocks inputBlocks.\n   * @param outputBlocks outputBlocks.",
  "org.apache.hadoop.conf.Configuration$IntegerRanges:convertToInt(java.lang.String,int)" : "* Convert a string to an int treating empty strings as the default value.\n     * @param value the string value\n     * @param defaultValue the value for if the string is empty\n     * @return the desired integer",
  "org.apache.hadoop.io.erasurecode.CodecUtil:createDecoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)" : "* Create decoder corresponding to given codec.\n   * @param options Erasure codec options\n   * @param conf configuration.\n   * @return erasure decoder",
  "org.apache.hadoop.io.DataInputBuffer:reset(byte[],int,int)" : "* Resets the data that the buffer reads.\n   *\n   * @param input input.\n   * @param start start.\n   * @param length length.",
  "org.apache.hadoop.fs.permission.FsPermission:toShort()" : "* Encode the object to a short.\n   * @return object to a short.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader:end()" : "* Get the end location of the TFile.\n     * \n     * @return The location right after the last key-value pair in TFile.",
  "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:allowChangeInputs()" : "* Allow changing input buffer content (not positions). Maybe better\n   * performance if not allowed.\n   * @return true if allowing input content to be changed, false otherwise",
  "org.apache.hadoop.security.authorize.AccessControlList:getGroups()" : "* Get the names of user groups allowed for this service.\n   * @return the set of group names. the set must not be modified.",
  "org.apache.hadoop.util.Shell$ShellCommandExecutor:execute()" : "* Execute the shell command.\n     * @throws IOException if the command fails, or if the command is\n     * not well constructed.",
  "org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.InputStream)" : null,
  "org.apache.hadoop.util.ExitUtil$ExitException:<init>(int,java.lang.String)" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyCompressor:reinit(org.apache.hadoop.conf.Configuration)" : "* Prepare the compressor to be used in a new stream with settings defined in\n   * the given Configuration\n   *\n   * @param conf Configuration from which new setting are fetched",
  "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getCrcType()" : "* returns the CRC type.\n   * @return data check sum type.",
  "org.apache.hadoop.fs.FileUtil:maybeIgnoreMissingDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.FileNotFoundException)" : "* Method to call after a FNFE has been raised on a treewalk, so as to\n   * decide whether to throw the exception (default), or, if the FS\n   * supports inconsistent directory listings, to log and ignore it.\n   * If this returns then the caller should ignore the failure and continue.\n   * @param fs filesystem\n   * @param path path\n   * @param e exception caught\n   * @throws FileNotFoundException the exception passed in, if rethrown.",
  "org.apache.hadoop.fs.CachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder)" : "* This is the constructor used by the builder.\n   * All overriding classes should implement this.\n   *\n   * @param builder builder.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.DirectoryListingStartAfterNotFoundException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartB()" : null,
  "org.apache.hadoop.ipc.Server:closeConnection(org.apache.hadoop.ipc.Server$Connection)" : null,
  "org.apache.hadoop.fs.ChecksumFs:isChecksumFile(org.apache.hadoop.fs.Path)" : "* Return true iff file is a checksum file name.\n   *\n   * @param file the file path.\n   * @return if is checksum file true,not false.",
  "org.apache.hadoop.io.SequenceFile$Reader:getValueClassName()" : "@return Returns the name of the value class.",
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.Class,java.lang.Class[])" : "* Checks for a method implementation.\n     * <p>\n     * The name passed to the constructor is the method name used.\n     * @param targetClass the class to check for an implementation\n     * @param argClasses argument classes for the method\n     * @return this Builder for method chaining",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX:getName(org.apache.hadoop.io.nativeio.NativeIO$POSIX$IdCache,int)" : null,
  "org.apache.hadoop.fs.shell.FsUsage$Du:setHumanReadable(boolean)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:checkAndCopyBytesToLocal(int)" : null,
  "org.apache.hadoop.io.compress.BlockDecompressorStream:resetState()" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$RetryForever:shouldRetry(java.lang.Exception,int,int,boolean)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:sync()" : "* create a sync point.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.dynamic.BindingUtils:loadStaticMethod(java.lang.Class,java.lang.Class,java.lang.String,java.lang.Class[])" : "* Load a static method from the source class, which will be a noop() if\n   * the class is null or the method isn't found.\n   * If the class and method are not found, then an {@code IllegalStateException}\n   * is raised on the basis that this means that the binding class is broken,\n   * rather than missing/out of date.\n   *\n   * @param <T> return type\n   * @param source source. If null, the method is a no-op.\n   * @param returnType return type class (unused)\n   * @param name method name\n   * @param parameterTypes parameters\n   *\n   * @return the method or a no-op.\n   * @throws IllegalStateException if the method is not static.",
  "org.apache.hadoop.metrics2.MetricsJsonBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)" : null,
  "org.apache.hadoop.io.WritableUtils:writeCompressedString(java.io.DataOutput,java.lang.String)" : null,
  "org.apache.hadoop.fs.ClusterStorageCapacityExceededException:<init>()" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:writeToNew(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor:execute(java.lang.Runnable)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:compareTo(java.lang.Object)" : null,
  "org.apache.hadoop.fs.FSDataInputStream:maxReadSizeForVectorReads()" : null,
  "org.apache.hadoop.ipc.ProtocolSignature:getFingerprint(java.lang.reflect.Method)" : "* Calculate a method's hash code considering its method\n   * name, returning type, and its parameter types\n   * \n   * @param method a method\n   * @return its hash code",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKeyLength()" : "* Get the length of the key.\n         * \n         * @return the length of the key.",
  "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:<init>(int,int,boolean,boolean)" : null,
  "org.apache.hadoop.io.ArrayWritable:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:returnBuffer(java.nio.ByteBuffer)" : "Return direct buffer to pool",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getMaxDate()" : null,
  "org.apache.hadoop.util.ShutdownHookManager:getShutdownTimeout(org.apache.hadoop.conf.Configuration)" : "* Get the shutdown timeout in seconds, from the supplied\n   * configuration.\n   * @param conf configuration to use.\n   * @return a timeout, always greater than or equal to {@link #TIMEOUT_MINIMUM}",
  "org.apache.hadoop.metrics2.impl.MetricsConfig:getPlugin(java.lang.String)" : null,
  "org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path,int)" : "* Opens an FSDataInputStream at the indicated Path.\n   * \n   * @param f the file name to open\n   * @param bufferSize the size of the buffer to be used.\n   * \n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If file <code>f</code> does not exist\n   * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n   *           not supported\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server\n   * @return output stream.",
  "org.apache.hadoop.fs.impl.prefetch.Retryer:continueRetry()" : "* Returns true if retrying should continue, false otherwise.\n   *\n   * @return true if the caller should retry, false otherwise.",
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:needsInput()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination:setOverwrite(boolean)" : "* \n   * This method is used to enable the force(-f)  option while copying the files.\n   * \n   * @param flag true/false",
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues3(int,int)" : null,
  "org.apache.hadoop.security.token.DtUtilShell$Remove:validate()" : null,
  "org.apache.hadoop.util.StringUtils:split(java.lang.String)" : "* Split a string using the default separator\n   * @param str a string that may have escaped separator\n   * @return an array of strings",
  "org.apache.hadoop.util.concurrent.HadoopExecutors:newScheduledThreadPool(int,java.util.concurrent.ThreadFactory)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:equals(java.lang.Object)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibFactory:getCompressionLevel(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.WritableUtils:readStringSafely(java.io.DataInput,int)" : "* Read a string, but check it for sanity. The format consists of a vint\n   * followed by the given number of bytes.\n   * @param in the stream to read from\n   * @param maxLength the largest acceptable length of the encoded string\n   * @return the bytes as a string\n   * @throws IOException if reading from the DataInput fails\n   * @throws IllegalArgumentException if the encoded byte size for string \n             is negative or larger than maxSize. Only the vint is read.",
  "org.apache.hadoop.fs.shell.CommandFormat$DuplicatedOptionException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.fs.FSDataInputStream:readFully(long,byte[])" : "* See {@link #readFully(long, byte[], int, int)}.",
  "org.apache.hadoop.ipc.WeightedTimeCostProvider:getCost(org.apache.hadoop.ipc.ProcessingDetails)" : "* Calculates a weighted sum of the times stored on the provided processing\n   * details to be used as the cost in {@link DecayRpcScheduler}.\n   *\n   * @param details Processing details\n   * @return The weighted sum of the times. The returned unit is the same\n   *         as the default unit used by the provided processing details.",
  "org.apache.hadoop.ipc.Client:toString()" : null,
  "org.apache.hadoop.metrics2.lib.MutableQuantiles:getInterval()" : "* Get the rollover interval (in seconds) of the estimator.\n   *\n   * @return  intervalSecs of the estimator.",
  "org.apache.hadoop.io.erasurecode.CodecRegistry:getCoderNames(java.lang.String)" : "* Get all coder names of the given codec.\n   * @param codecName the name of codec\n   * @return an array of all coder names, null if not exist",
  "org.apache.hadoop.conf.StorageUnit$6:toEBs(double)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:progressable(org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread:<init>(org.apache.hadoop.conf.ReconfigurableBase)" : null,
  "org.apache.hadoop.util.SemaphoredDelegatingExecutor:invokeAll(java.util.Collection)" : null,
  "org.apache.hadoop.ipc.Server:getRemoteIp()" : "* @return Returns the remote side ip address when invoked inside an RPC\n   *  Returns null in case of an error.",
  "org.apache.hadoop.fs.shell.find.BaseExpression:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.metrics2.sink.FileSink:flush()" : null,
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$2:check(java.lang.String[],java.lang.String[],java.lang.String[])" : "* Checks to see if the supplied hostname matches any of the supplied CNs\n     * or \"DNS\" Subject-Alts.  Most implementations only look at the first CN,\n     * and ignore any additional CNs.  Most implementations do look at all of\n     * the \"DNS\" Subject-Alts. The CNs or Subject-Alts may contain wildcards\n     * according to RFC 2818.\n     *\n     * @param cns         CN fields, in order, as extracted from the X.509\n     *                    certificate.\n     * @param subjectAlts Subject-Alt fields of type 2 (\"DNS\"), as extracted\n     *                    from the X.509 certificate.\n     * @param hosts       The array of hostnames to verify.\n     * @throws SSLException If verification failed.",
  "org.apache.hadoop.net.NetworkTopology:getNodeForNetworkLocation(org.apache.hadoop.net.Node)" : "* Return a reference to the node given its string representation.\n   * Default implementation delegates to {@link #getNode(String)}.\n   * \n   * <p>To be overridden in subclasses for specific NetworkTopology \n   * implementations, as alternative to overriding the full {@link #add(Node)}\n   *  method.\n   * \n   * @param node The string representation of this node's network location is\n   * used to retrieve a Node object. \n   * @return a reference to the node; null if the node is not in the tree\n   * \n   * @see #add(Node)\n   * @see #getNode(String)",
  "org.apache.hadoop.crypto.key.KeyProvider$Metadata:getCreated()" : null,
  "org.apache.hadoop.ha.SshFenceByTcpPort$Args:parseConfiggedPort(java.lang.String)" : null,
  "org.apache.hadoop.ha.StreamPumper:start()" : null,
  "org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])" : null,
  "org.apache.hadoop.ipc.Server$Connection:getEstablishedQOP()" : null,
  "org.apache.hadoop.net.InnerNodeImpl:isParent(org.apache.hadoop.net.Node)" : "Judge if this node is the parent of node <i>n</i>.\n   *\n   * @param n a node\n   * @return true if this node is the parent of <i>n</i>",
  "org.apache.hadoop.io.compress.lz4.Lz4Compressor:getBytesWritten()" : "* Return number of bytes consumed by callers of compress since last reset.",
  "org.apache.hadoop.io.DataInputByteBuffer:getPosition()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.util.AutoCloseableLock:release()" : "* A wrapper method that makes a call to {@code unlock()} of the\n   * underlying {@code ReentrantLock} object.\n   *\n   * Attempts to release the lock.\n   *\n   * If the current thread holds the lock, decrements the hold\n   * count. If the hold count reaches zero, the lock is released.\n   *\n   * If the current thread does not hold the lock, then\n   * {@link IllegalMonitorStateException} is thrown.",
  "org.apache.hadoop.util.LightWeightResizableGSet:<init>(int,float)" : null,
  "org.apache.hadoop.io.MapFile$Merger:mergePass()" : "* Merge all input files to output map file.<br>\n     * 1. Read first key/value from all input files to keys/values array. <br>\n     * 2. Select the least key and corresponding value. <br>\n     * 3. Write the selected key and value to output file. <br>\n     * 4. Replace the already written key/value in keys/values arrays with the\n     * next key/value from the selected input <br>\n     * 5. Repeat step 2-4 till all keys are read. <br>",
  "org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Create a new Path based on the child path resolved against the parent path.\n   *\n   * @param parent the parent path\n   * @param child the child path",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:incrementDelegationTokenSeqNum()" : "* For subclasses externalizing the storage, for example Zookeeper\n   * based implementations.\n   *\n   * @return delegationTokenSequenceNumber.",
  "org.apache.hadoop.util.SysInfoWindows:getNumVCoresUsed()" : "{@inheritDoc}",
  "org.apache.hadoop.conf.Configuration$ParsedTimeDuration:unitFor(java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceHasNext()" : "* Check for the source having a next element.\n     * If it does not, this object's close() method\n     * is called and false returned\n     * @return true if there is a new value\n     * @throws IOException failure to retrieve next value",
  "org.apache.hadoop.io.compress.SnappyCodec:getDefaultExtension()" : "* Get the default filename extension for this kind of compression.\n   *\n   * @return <code>.snappy</code>.",
  "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:cancel()" : null,
  "org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)" : "Create an object for the given class and initialize it from conf\n   * \n   * @param theClass class of which an object is created\n   * @param conf Configuration\n   * @param <T> Generics Type T.\n   * @return a new object",
  "org.apache.hadoop.fs.FileContext:truncate(org.apache.hadoop.fs.Path,long)" : "* Truncate the file in the indicated path to the indicated size.\n   * <ul>\n   * <li>Fails if path is a directory.\n   * <li>Fails if path does not exist.\n   * <li>Fails if path is not closed.\n   * <li>Fails if new size is greater than current size.\n   * </ul>\n   * @param f The path to the file to be truncated\n   * @param newLength The size the file is to be truncated to\n   *\n   * @return <code>true</code> if the file has been truncated to the desired\n   * <code>newLength</code> and is immediately available to be reused for\n   * write operations such as <code>append</code>, or\n   * <code>false</code> if a background process of adjusting the length of\n   * the last block has been started, and clients should wait for it to\n   * complete before proceeding with further file updates.\n   *\n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If file <code>f</code> does not exist\n   * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n   *           not supported\n   * @throws IOException If an I/O error occurred\n   *\n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws\n   *           undeclared exception to RPC server",
  "org.apache.hadoop.io.BytesWritable:copyBytes()" : "* Get a copy of the bytes that is exactly the length of the data.\n   * See {@link #getBytes()} for faster access to the underlying array.\n   *\n   * @return copyBytes.",
  "org.apache.hadoop.fs.FutureDataInputStreamBuilder:build()" : null,
  "org.apache.hadoop.fs.permission.FsCreateModes:toString()" : null,
  "org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthorizationSuccesses()" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:addServiceUserRawCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder)" : null,
  "org.apache.hadoop.util.LightWeightCache:updateRecommendedLength(int,int)" : null,
  "org.apache.hadoop.fs.impl.OpenFileParameters:withBufferSize(int)" : null,
  "org.apache.hadoop.fs.sftp.SFTPConnectionPool$ConnectionInfo:getHost()" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:parseCommandArgs(org.apache.hadoop.conf.Configuration,java.util.List)" : "* Parse the command arguments, extracting the service class as the last\n   * element of the list (after extracting all the rest).\n   *\n   * The field {@link #commandOptions} field must already have been set.\n   * @param conf configuration to use\n   * @param args command line argument list\n   * @return the remaining arguments\n   * @throws ServiceLaunchException if processing of arguments failed",
  "org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:getReason()" : null,
  "org.apache.hadoop.util.Shell$ShellCommandExecutor:close()" : null,
  "org.apache.hadoop.security.ssl.SSLFactory:init()" : "* Initializes the factory.\n   *\n   * @throws  GeneralSecurityException thrown if an SSL initialization error\n   * happened.\n   * @throws IOException thrown if an IO error happened while reading the SSL\n   * configuration.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getDelegationKey(int)" : "* For subclasses externalizing the storage, for example Zookeeper\n   * based implementations.\n   *\n   * @param keyId keyId.\n   * @return DelegationKey.",
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:end()" : null,
  "org.apache.hadoop.util.IdentityHashStore:<init>(int)" : null,
  "org.apache.hadoop.net.NetworkTopology:decommissionNode(org.apache.hadoop.net.Node)" : "* Update empty rack number when remove a node like decommission.\n   * @param node node to be added; can be null",
  "org.apache.hadoop.fs.statistics.IOStatisticsLogging$StatisticsToString:<init>(org.apache.hadoop.fs.statistics.IOStatistics)" : "* Constructor.\n     * @param statistics statistics",
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:getTargetFileSystemForClose()" : null,
  "org.apache.hadoop.io.IOUtils:readFully(java.io.InputStream,byte[],int,int)" : "* Reads len bytes in a loop.\n   *\n   * @param in InputStream to read from\n   * @param buf The buffer to fill\n   * @param off offset from the buffer\n   * @param len the length of bytes to read\n   * @throws IOException if it could not read requested number of bytes \n   * for any reason (including EOF)",
  "org.apache.hadoop.security.KDiag:dumpTokens(org.apache.hadoop.security.UserGroupInformation)" : "* Dump all tokens of a UGI.\n   * @param ugi UGI to examine",
  "org.apache.hadoop.io.compress.CodecPool:payback(java.util.Map,java.lang.Object)" : null,
  "org.apache.hadoop.fs.FileUtil:compareFs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileSystem)" : null,
  "org.apache.hadoop.security.Groups:parseStaticMapping(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:tags()" : null,
  "org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:<init>()" : "Same as this(0, 0, null)",
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:convertToByteBufferState()" : "* Convert to a ByteBufferEncodingState when it's backed by on-heap arrays.",
  "org.apache.hadoop.log.LogLevel$CLI:sendLogLevelRequest()" : "* Send HTTP/HTTPS request to the daemon.\n     * @throws HadoopIllegalArgumentException if arguments are invalid.\n     * @throws Exception if unable to connect",
  "org.apache.hadoop.ipc.Server$Call:getTimestampNanos()" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:getWriteOps()" : "* Get the number of file system write operations such as create, append\n     * rename etc.\n     * @return number of write operations",
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:reinit(org.apache.hadoop.conf.Configuration)" : "* Prepare the compressor to be used in a new stream with settings defined in\n   * the given Configuration. It will reset the compressor's compression level\n   * and compression strategy.\n   * \n   * @param conf Configuration storing new settings",
  "org.apache.hadoop.io.file.tfile.Compression:getSupportedAlgorithms()" : null,
  "org.apache.hadoop.http.ProfileServlet$Event:fromInternalName(java.lang.String)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry$Pair:toString()" : null,
  "org.apache.hadoop.util.Shell:getEnvironment(java.lang.String)" : "* Get an environment variable.\n   * @param env the environment var\n   * @return the value or null if it was unset.",
  "org.apache.hadoop.io.SequenceFile$Writer:filesystem(org.apache.hadoop.fs.FileSystem)" : "* @deprecated only used for backwards-compatibility in the createWriter methods\n     * that take FileSystem.",
  "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:getWarningMessage(java.lang.String)" : null,
  "org.apache.hadoop.util.StringUtils:getTrimmedStrings(java.lang.String)" : "* Splits a comma or newline separated value <code>String</code>, trimming\n   * leading and trailing whitespace on each value.\n   *\n   * @param str a comma or newline separated <code>String</code> with values,\n   *            may be null\n   * @return an array of <code>String</code> values, empty array if null String\n   *         input",
  "org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:exists(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)" : "* Convenience method, so that we don't open a new connection when using this\n   * method from within another method. Otherwise every API invocation incurs\n   * the overhead of opening/closing a TCP connection.\n   * @throws IOException on IO problems other than FileNotFoundException",
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:requestPrefetch(int)" : null,
  "org.apache.hadoop.service.ServiceStateException:<init>(java.lang.String)" : "* Instantiate\n   * @param message error message",
  "org.apache.hadoop.util.ThreadUtil:sleepAtLeastIgnoreInterrupts(long)" : "* Cause the current thread to sleep as close as possible to the provided\n   * number of milliseconds. This method will log and ignore any\n   * {@link InterruptedException} encountered.\n   * \n   * @param millis the number of milliseconds for the current thread to sleep",
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:minimums()" : null,
  "org.apache.hadoop.conf.Configuration:addResourceObject(org.apache.hadoop.conf.Configuration$Resource)" : null,
  "org.apache.hadoop.fs.shell.find.FindOptions:setFollowLink(boolean)" : "* Sets flag indicating whether symbolic links should be followed.\n   *\n   * @param followLink true indicates follow links",
  "org.apache.hadoop.fs.FileSystem:getFileChecksum(org.apache.hadoop.fs.Path)" : "* Get the checksum of a file, if the FS supports checksums.\n   *\n   * @param f The file path\n   * @return The file checksum.  The default return value is null,\n   *  which indicates that no checksum algorithm is implemented\n   *  in the corresponding FileSystem.\n   * @throws IOException IO failure",
  "org.apache.hadoop.fs.shell.find.Find:setRootExpression(org.apache.hadoop.fs.shell.find.Expression)" : "* Set the root expression for this find.\n   * \n   * @param expression",
  "org.apache.hadoop.fs.FilterFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)" : null,
  "org.apache.hadoop.util.WeakReferenceMap:lookup(java.lang.Object)" : "* look up the value, returning the possibly empty weak reference\n   * to a value, or null if no value was found.\n   * @param key key to look up\n   * @return null if there is no entry, a weak reference if found",
  "org.apache.hadoop.io.SequenceFile$Reader$StartOption:<init>(long)" : null,
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getBlockSize()" : null,
  "org.apache.hadoop.security.SecurityUtil:buildDTServiceName(java.net.URI,int)" : "* create the service name for a Delegation token\n   * @param uri of the service\n   * @param defPort is used if the uri lacks a port\n   * @return the token service, or null if no authority\n   * @see #buildTokenService(InetSocketAddress)",
  "org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.io.erasurecode.ECBlock:<init>(boolean,boolean)" : "* A constructor specifying isParity and isErased.\n   * @param isParity is a parity block\n   * @param isErased is erased or not",
  "org.apache.hadoop.fs.Globber$GlobBuilder:withPathFiltern(org.apache.hadoop.fs.PathFilter)" : "* Set the path filter.\n     * @param pathFilter filter\n     * @return the builder",
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:getType()" : "* @return the type of a visited metric",
  "org.apache.hadoop.fs.FilterFileSystem:getFileStatus(org.apache.hadoop.fs.Path)" : "* Get file status.",
  "org.apache.hadoop.conf.Configuration:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)" : "* Set the socket address a client can use to connect for the\n   * <code>name</code> property as a <code>host:port</code>.  The wildcard\n   * address is replaced with the local host's address. If the host and address\n   * properties are configured the host component of the address will be combined\n   * with the port component of the addr to generate the address.  This is to allow\n   * optional control over which host name is used in multi-home bind-host\n   * cases where a host can have multiple names\n   * @param hostProperty the bind-host configuration name\n   * @param addressProperty the service address configuration name\n   * @param defaultAddressValue the service default address configuration value\n   * @param addr InetSocketAddress of the service listener\n   * @return InetSocketAddress for clients to connect",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:numDroppedConnections()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockData:getStartOffset(int)" : "* Gets the start offset of the given block.\n   * @param blockNumber the id of the given block.\n   * @return the start offset of the given block.\n   * @throws IllegalArgumentException if blockNumber is invalid.",
  "org.apache.hadoop.util.ShutdownHookManager:shutdownExecutor(org.apache.hadoop.conf.Configuration)" : "* Shutdown the executor thread itself.\n   * @param conf the configuration containing the shutdown timeout setting.",
  "org.apache.hadoop.fs.shell.Command:processOptions(java.util.LinkedList)" : "* Must be implemented by commands to process the command line flags and\n   * check the bounds of the remaining arguments.  If an\n   * IllegalArgumentException is thrown, the FsShell object will print the\n   * short usage of the command.\n   * @param args the command line arguments\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:toString()" : null,
  "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroDeserializer:deserialize(java.lang.Object)" : null,
  "org.apache.hadoop.ipc.ProcessingDetails:add(org.apache.hadoop.ipc.ProcessingDetails$Timing,long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.io.compress.CompressionInputStream:close()" : null,
  "org.apache.hadoop.security.UserGroupInformation:createRemoteUser(java.lang.String)" : "* Create a user from a login name. It is intended to be used for remote\n   * users in RPC, since it won't have any credentials.\n   * @param user the full user principal name, must not be empty or null\n   * @return the UserGroupInformation for the remote user.",
  "org.apache.hadoop.security.token.Token:<init>()" : "* Default constructor.",
  "org.apache.hadoop.fs.FilterFileSystem:<init>(org.apache.hadoop.fs.FileSystem)" : null,
  "org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:getAndAdvanceCurrentIndex()" : "* Use the mux by getting and advancing index.",
  "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:maximums()" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:hasCapability(java.lang.String)" : null,
  "org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[],long)" : null,
  "org.apache.hadoop.conf.Configuration:clear()" : "* Clears all keys from the configuration.",
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:needsDictionary()" : null,
  "org.apache.hadoop.ipc.Server$Connection:disposeSasl()" : null,
  "org.apache.hadoop.fs.CompositeCrcFileChecksum:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.util.GcTimeMonitor:calculateGCTimePercentageWithinObservedInterval()" : null,
  "org.apache.hadoop.fs.HarFileSystem:getStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])" : null,
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerCounter(java.lang.String,java.util.concurrent.atomic.AtomicInteger)" : "* Add a counter statistic to dynamically return the\n   * latest value of the source.\n   * @param key key of this statistic\n   * @param source atomic int counter\n   * @return the builder.",
  "org.apache.hadoop.net.NetworkTopology:getWeightUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)" : "* Returns an integer weight which specifies how far away <i>node</i> is\n   * from <i>reader</i>. A lower value signifies that a node is closer.\n   * It uses network location to calculate the weight\n   *\n   * @param reader Node where data will be read\n   * @param node Replica of data\n   * @return weight",
  "org.apache.hadoop.security.token.delegation.web.HttpUserGroupInformation:get()" : "* Returns the remote {@link UserGroupInformation} in context for the current\n   * HTTP request, taking into account proxy user requests.\n   *\n   * @return the remote {@link UserGroupInformation}, <code>NULL</code> if none.",
  "org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:makeBlockGroup(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[])" : "* Calculating and organizing BlockGroup, to be called by ECManager\n   * @param dataBlocks Data blocks to compute parity blocks against\n   * @param parityBlocks To be computed parity blocks\n   * @return ECBlockGroup.",
  "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState)" : null,
  "org.apache.hadoop.security.Credentials:getSecretKeyMap()" : "* Returns an unmodifiable version of the full map of aliases to secret keys.\n   *\n   * @return SecretKeyMap.",
  "org.apache.hadoop.fs.viewfs.InodeTree:getRootLink()" : null,
  "org.apache.hadoop.io.MapFile$Reader$ComparatorOption:getValue()" : null,
  "org.apache.hadoop.ipc.Server$Call:getProcessingDetails()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfVectMulInit(byte,byte[],int)" : "* Ported from Intel ISA-L library.\n   *\n   * Calculates const table gftbl in GF(2^8) from single input A\n   * gftbl(A) = {A{00}, A{01}, A{02}, ... , A{0f} }, {A{00}, A{10}, A{20},\n   * ... , A{f0} } -- from ISA-L implementation.\n   *\n   * @param c c.\n   * @param tbl tbl.\n   * @param offset offset.",
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:performDecodeImpl(java.nio.ByteBuffer[],int[],int,int[],java.nio.ByteBuffer[],int[])" : null,
  "org.apache.hadoop.security.token.Token:renew(org.apache.hadoop.conf.Configuration)" : "* Renew this delegation token.\n   * @param conf configuration.\n   * @return the new expiration time\n   * @throws IOException raised on errors performing I/O.\n   * @throws InterruptedException if the thread is interrupted.",
  "org.apache.hadoop.util.MachineList:<init>(java.util.Collection,org.apache.hadoop.util.MachineList$InetAddressFactory)" : "* Accepts a collection of ip/cidr/host addresses\n   * \n   * @param hostEntries hostEntries.\n   * @param addressFactory addressFactory to convert host to InetAddress",
  "org.apache.hadoop.ipc.ResponseBuffer:<init>()" : null,
  "org.apache.hadoop.conf.StorageUnit$3:fromBytes(double)" : null,
  "org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File)" : "* Unpack a jar file into a directory.\n   *\n   * This version unpacks all files inside the jar regardless of filename.\n   *\n   * @param jarFile the .jar file to unpack\n   * @param toDir the destination directory into which to unpack the jar\n   *\n   * @throws IOException if an I/O error has occurred or toDir\n   * cannot be created and does not already exist",
  "org.apache.hadoop.fs.FileSystem$DirListingIterator:hasNext()" : null,
  "org.apache.hadoop.service.launcher.IrqHandler:bind()" : "* Bind to the interrupt handler.\n   * @throws IllegalArgumentException if the exception could not be set",
  "org.apache.hadoop.fs.StorageStatistics:<init>(java.lang.String)" : null,
  "org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler:<init>(org.apache.hadoop.security.token.Token)" : null,
  "org.apache.hadoop.fs.ContentSummary:formatSize(long,boolean)" : "* Formats a size to be human readable or in bytes.\n   * @param size value to be formatted\n   * @param humanReadable flag indicating human readable or not\n   * @return String representation of the size",
  "org.apache.hadoop.util.WeakReferenceMap:put(java.lang.Object,java.lang.Object)" : "* Put a value under the key.\n   * A null value can be put, though on a get() call\n   * a new entry is generated\n   *\n   * @param key key\n   * @param value value\n   * @return any old non-null reference.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:invokeTrackingDuration(org.apache.hadoop.fs.statistics.DurationTracker,org.apache.hadoop.util.functional.CallableRaisingIOE)" : "* Given an IOException raising callable/lambda expression,\n   * execute it, updating the tracker on success/failure.\n   * @param tracker duration tracker.\n   * @param input input callable.\n   * @param <B> return type.\n   * @return the result of the invocation\n   * @throws IOException on failure.",
  "org.apache.hadoop.io.SequenceFile$Sorter:mergePass(org.apache.hadoop.fs.Path)" : "sort calls this to generate the final merged output",
  "org.apache.hadoop.http.HttpServer2:addJerseyResourcePackage(java.lang.String,java.lang.String)" : "* Add a Jersey resource package.\n   * @param packageName The Java package name containing the Jersey resource.\n   * @param pathSpec The path spec for the servlet",
  "org.apache.hadoop.io.ShortWritable:compareTo(org.apache.hadoop.io.ShortWritable)" : "Compares two ShortWritable.",
  "org.apache.hadoop.util.Shell:getOSType()" : null,
  "org.apache.hadoop.io.erasurecode.ErasureCoderOptions:getNumDataUnits()" : "* The number of data input units for the coding. A unit can be a byte,\n   * chunk or buffer or even a block.\n   * @return count of data input units",
  "org.apache.hadoop.metrics2.impl.SinkQueue:enqueue(java.lang.Object)" : null,
  "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:toString()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,org.apache.hadoop.io.file.tfile.TFile$Reader$Location,org.apache.hadoop.io.file.tfile.TFile$Reader$Location)" : "* Constructor\n       * \n       * @param reader\n       *          The TFile reader object.\n       * @param begin\n       *          Begin location of the scan.\n       * @param end\n       *          End location of the scan.\n       * @throws IOException",
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:reinit(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.net.AbstractDNSToSwitchMapping:<init>(org.apache.hadoop.conf.Configuration)" : "* Create an instance, caching the configuration file.\n   * This constructor does not call {@link #setConf(Configuration)}; if\n   * a subclass extracts information in that method, it must call it explicitly.\n   * @param conf the configuration",
  "org.apache.hadoop.util.SignalLogger:register(org.slf4j.Logger)" : "* Register some signal handlers.\n   *\n   * @param log The log4j logfile to use in the signal handlers.",
  "org.apache.hadoop.fs.GlobExpander:expandLeftmost(org.apache.hadoop.fs.GlobExpander$StringWithOffset)" : "* Expand the leftmost outer curly bracket pair containing a\n   * slash character (\"/\") in <code>filePattern</code>.\n   * @param filePatternWithOffset\n   * @return expanded file patterns\n   * @throws IOException",
  "org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:getAlgorithm()" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:open(org.apache.hadoop.fs.Path,int)" : null,
  "org.apache.hadoop.fs.FileSystemStorageStatistics:isTracked(java.lang.String)" : "* Return true if a statistic is being tracked.\n   *\n   * @return         True only if the statistic is being tracked.",
  "org.apache.hadoop.fs.QuotaUsage$Builder:typeConsumed(long[])" : null,
  "org.apache.hadoop.ipc.Server$Connection:doSaslReply(org.apache.hadoop.thirdparty.protobuf.Message)" : null,
  "org.apache.hadoop.io.DefaultStringifier:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)" : null,
  "org.apache.hadoop.ipc.Client$IpcStreams:setOutputStream(java.io.OutputStream)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getBytesWritten()" : "* Returns the total number of uncompressed bytes output so far.\n   *\n   * @return the total (non-negative) number of uncompressed bytes output so far",
  "org.apache.hadoop.security.UserGroupInformation:print()" : null,
  "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],long,long,boolean)" : "* Constructor with host, name, offset, length and corrupt flag.\n   * @param names names.\n   * @param hosts hosts.\n   * @param offset offset.\n   * @param length length.\n   * @param corrupt corrupt.",
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:build()" : "* Build the IOStatistics instance.\n   * @return an instance.\n   * @throws IllegalStateException if the builder has already been built.",
  "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:<init>(java.lang.String,org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion)" : null,
  "org.apache.hadoop.fs.statistics.impl.SourceWrappedStatistics:<init>(org.apache.hadoop.fs.statistics.IOStatistics)" : "* Constructor.\n   * @param source source of statistics.",
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:addToCacheAndRelease(org.apache.hadoop.fs.impl.prefetch.BufferData,java.util.concurrent.Future,java.time.Instant)" : null,
  "org.apache.hadoop.fs.FileUtil:createLocalTempFile(java.io.File,java.lang.String,boolean)" : "* Create a tmp file for a base file.\n   * @param basefile the base file of the tmp\n   * @param prefix file name prefix of tmp\n   * @param isDeleteOnExit if true, the tmp will be deleted when the VM exits\n   * @return a newly created tmp file\n   * @exception IOException If a tmp file cannot created\n   * @see java.io.File#createTempFile(String, String, File)\n   * @see java.io.File#deleteOnExit()",
  "org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration)" : "* Construct &amp; cache an IPC client with the default SocketFactory\n   * and default valueClass if no cached client exists. \n   * \n   * @param conf Configuration\n   * @return an IPC client",
  "org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : null,
  "org.apache.hadoop.ipc.ObserverRetryOnActiveException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.ipc.ProtocolSignature:resetCache()" : null,
  "org.apache.hadoop.io.IOUtils:closeStreams(java.io.Closeable[])" : "* Closes the streams ignoring {@link Throwable}.\n   * Must only be called in cleaning up from exception handlers.\n   *\n   * @param streams the Streams to close",
  "org.apache.hadoop.io.Text:set(java.lang.String)" : "* Set to contain the contents of a string.\n   *\n   * @param string input string.",
  "org.apache.hadoop.util.CacheableIPList:isIn(java.lang.String)" : null,
  "org.apache.hadoop.security.KDiag:getAndSet(java.lang.String)" : "* Set the System property to true; return the old value for caching.\n   *\n   * @param sysprop property\n   * @return the previous value",
  "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:reset()" : null,
  "org.apache.hadoop.io.UTF8:equals(java.lang.Object)" : "Returns true iff <code>o</code> is a UTF8 with the same contents.",
  "org.apache.hadoop.ipc.RPC$Server:initProtocolMetaInfo(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.shell.find.Find:parseExpression(java.util.Deque)" : "* Parse a list of arguments to to extract the {@link Expression} elements.\n   * The input Deque will be modified to remove the used elements.\n   * \n   * @param args arguments to be parsed\n   * @return list of {@link Expression} elements applicable to this command\n   * @throws IOException if list can not be parsed",
  "org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:decrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)" : "*  AES-CTR will consume all of the input data. It requires enough space in\n     * the destination buffer to decrypt entire input buffer.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:fromStorageStatistics(org.apache.hadoop.fs.StorageStatistics)" : "* Create  IOStatistics from a storage statistics instance.\n   *\n   * This will be updated as the storage statistics change.\n   * @param storageStatistics source data.\n   * @return an IO statistics source.",
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getChecksumOpt()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:isValidName(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:setInputFromSavedData()" : null,
  "org.apache.hadoop.io.file.tfile.Utils:lowerBound(java.util.List,java.lang.Object,java.util.Comparator)" : "* Lower bound binary search. Find the index to the first element in the list\n   * that compares greater than or equal to key.\n   * \n   * @param <T>\n   *          Type of the input key.\n   * @param list\n   *          The list\n   * @param key\n   *          The input key.\n   * @param cmp\n   *          Comparator for the key.\n   * @return The index to the desired element if it exists; or list.size()\n   *         otherwise.",
  "org.apache.hadoop.fs.BatchedRemoteIterator:makeRequest()" : null,
  "org.apache.hadoop.metrics2.lib.MutableGaugeFloat:value()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:getAclStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:getKeyClass()" : "@return Returns the class of keys in this file.",
  "org.apache.hadoop.io.SequenceFile$Writer:appendIfExists(boolean)" : null,
  "org.apache.hadoop.ipc.CallerContext$Builder:getContext()" : "* Get the context.\n     * For example, the context is \"key1:value1,key2:value2\".\n     * @return the valid context or null.",
  "org.apache.hadoop.metrics2.impl.MetricCounterInt:value()" : null,
  "org.apache.hadoop.fs.store.ByteBufferInputStream:verifyOpen()" : "* Verify that the stream is open.\n   * @throws IOException if the stream is closed",
  "org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration,java.lang.String)" : "* Refreshes configuration using the specified Proxy user prefix for\n   * properties.\n   *\n   * @param conf configuration\n   * @param proxyUserPrefix proxy user configuration prefix",
  "org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX:chmod(java.lang.String,int)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:setClientTokenProvider(org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension$DelegationTokenExtension)" : null,
  "org.apache.hadoop.util.ChunkedArrayList:<init>()" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:setUsage(java.lang.String[])" : "* Sets the usage text for this {@link Expression} .\n   * @param usage usage array.",
  "org.apache.hadoop.tracing.Span:getContext()" : null,
  "org.apache.hadoop.util.Sets:newHashSet(java.util.Iterator)" : "* Creates a <i>mutable</i> {@code HashSet} instance containing the given\n   * elements. A very thin convenience for creating an empty set and then\n   * calling Iterators#addAll.\n   *\n   * <p><b>Note:</b> if mutability is not required and the elements are\n   * non-null, use ImmutableSet#copyOf(Iterator) instead.</p>\n   *\n   * <p><b>Note:</b> if {@code E} is an {@link Enum} type, you should create\n   * an {@link EnumSet} instead.</p>\n   *\n   * <p>Overall, this method is not very useful and will likely be deprecated\n   * in the future.</p>\n   *\n   * @param <E> Generics Type E.\n   * @param elements elements.\n   * @return a new, empty thread-safe {@code Set}.",
  "org.apache.hadoop.security.Groups$GroupCacheLoader:fetchGroupSet(java.lang.String)" : "* Queries impl for groups belonging to the user.\n     * This could involve I/O and take awhile.",
  "org.apache.hadoop.fs.permission.FsPermission:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:setTimeout(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.conf.Configuration)" : "* Set the FTPClient's timeout based on configuration.\n   * FS_FTP_TIMEOUT is set as timeout (defaults to DEFAULT_TIMEOUT).",
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.security.cert.X509Certificate)" : null,
  "org.apache.hadoop.ipc.Server$Connection:readAndProcess()" : "* This method reads in a non-blocking fashion from the channel: \n     * this method is called repeatedly when data is present in the channel; \n     * when it has enough data to process one rpc it processes that rpc.\n     * \n     * On the first pass, it processes the connectionHeader, \n     * connectionContext (an outOfBand RPC) and at most one RPC request that \n     * follows that. On future passes it will process at most one RPC request.\n     *  \n     * Quirky things: dataLengthBuffer (4 bytes) is used to read \"hrpc\" OR \n     * rpc request length.\n     *    \n     * @return -1 in case of error, else num bytes read so far\n     * @throws IOException - internal error that should not be returned to\n     *         client, typically failure to respond to client\n     * @throws InterruptedException - if the thread is interrupted.",
  "org.apache.hadoop.util.functional.RemoteIterators:foreach(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.ConsumerRaisingIOE)" : "* Apply an operation to all values of a RemoteIterator.\n   *\n   * If the iterator is an IOStatisticsSource returning a non-null\n   * set of statistics, <i>and</i> this classes log is set to DEBUG,\n   * then the statistics of the operation are evaluated and logged at\n   * debug.\n   * <p>\n   * The number of entries processed is returned, as it is useful to\n   * know this, especially during tests or when reporting values\n   * to users.\n   * </p>\n   * This does not close the iterator afterwards.\n   * @param source iterator source\n   * @param consumer consumer of the values.\n   * @return the number of elements processed\n   * @param <T> type of source\n   * @throws IOException if the source RemoteIterator or the consumer raise one.",
  "org.apache.hadoop.ipc.RpcWritable$WritableWrapper:writeTo(org.apache.hadoop.ipc.ResponseBuffer)" : null,
  "org.apache.hadoop.security.UserGroupInformation$TestingGroups:<init>(org.apache.hadoop.security.Groups)" : null,
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:create()" : "* Create an FSDataOutputStream at the specified path.",
  "org.apache.hadoop.net.ScriptBasedMappingWithDependency:getDependency(java.lang.String)" : "* Get dependencies in the topology for a given host\n   * @param name - host name for which we are getting dependency\n   * @return a list of hosts dependent on the provided host name",
  "org.apache.hadoop.tools.CommandShell:setSubCommand(org.apache.hadoop.tools.CommandShell$SubCommand)" : null,
  "org.apache.hadoop.net.NodeBase:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(byte[])" : null,
  "org.apache.hadoop.io.serializer.SerializationFactory:add(org.apache.hadoop.conf.Configuration,java.lang.String)" : null,
  "org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:capacity()" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(byte[],int,int)" : null,
  "org.apache.hadoop.fs.FileSystem:getDefaultReplication(org.apache.hadoop.fs.Path)" : "* Get the default replication for a path.\n   * The given path will be used to locate the actual FileSystem to query.\n   * The full path does not have to exist.\n   * @param path of the file\n   * @return default replication for the path's filesystem",
  "org.apache.hadoop.security.SaslRpcServer:encodePassword(byte[])" : null,
  "org.apache.hadoop.fs.permission.AclStatus:getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry,org.apache.hadoop.fs.permission.FsPermission)" : "* Get the effective permission for the AclEntry. <br>\n   * Recommended to use this API ONLY if client communicates with the old\n   * NameNode, needs to pass the Permission for the path to get effective\n   * permission, else use {@link AclStatus#getEffectivePermission(AclEntry)}.\n   * @param entry AclEntry to get the effective action\n   * @param permArg Permission for the path. However if the client is NOT\n   *          communicating with old namenode, then this argument will not have\n   *          any preference.\n   * @return Returns the effective permission for the entry.\n   * @throws IllegalArgumentException If the client communicating with old\n   *           namenode and permission is not passed as an argument.",
  "org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration)" : "* Get a {@link Compressor} for the given {@link CompressionCodec} from the \n   * pool or a new one.\n   *\n   * @param codec the <code>CompressionCodec</code> for which to get the \n   *              <code>Compressor</code>\n   * @param conf the <code>Configuration</code> object which contains confs for creating or reinit the compressor\n   * @return <code>Compressor</code> for the given \n   *         <code>CompressionCodec</code> from the pool or a new one",
  "org.apache.hadoop.ipc.ProtocolSignature:getFingerprint(int[])" : "* Get the hash code of an array of hashcodes\n   * Hashcodes are sorted before hashcode is calculated.\n   * So the returned value is irrelevant of the hashcode order in the array.\n   * \n   * @param methods an array of methods\n   * @return the hash code",
  "org.apache.hadoop.metrics2.util.Quantile:toString()" : null,
  "org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object)" : null,
  "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:getQueueName(int)" : "* @return Returns the rate name inside the metric.\n   * @param priority input priority.",
  "org.apache.hadoop.ipc.Server$RpcCall:isOpen()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getGroup()" : null,
  "org.apache.hadoop.util.dynamic.DynConstructors$Builder:<init>()" : null,
  "org.apache.hadoop.fs.ChecksumFs:setReplication(org.apache.hadoop.fs.Path,short)" : "* Set replication for an existing file.\n   * Implement the abstract <tt>setReplication</tt> of <tt>FileSystem</tt>\n   * @param src file name\n   * @param replication new replication\n   * @throws IOException if an I/O error occurs.\n   * @return true if successful;\n   *         false if file does not exist or is a directory",
  "org.apache.hadoop.fs.impl.prefetch.Validate:checkGreater(long,java.lang.String,long,java.lang.String)" : "* Validates that the first value is greater than the second value.\n   * @param value1 the first value to check.\n   * @param value1Name the name of the first argument.\n   * @param value2 the second value to check.\n   * @param value2Name the name of the second argument.",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_enabled()" : "* Probe to check if the thread-level IO statistics enabled.\n   * If the relevant classes and methods were not found, returns false\n   * @return true if the IOStatisticsContext API was found\n   * and is enabled.",
  "org.apache.hadoop.io.BinaryComparable:equals(java.lang.Object)" : "* Return true if bytes from {#getBytes()} match.",
  "org.apache.hadoop.io.SequenceFile$Metadata:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$1:<init>(org.apache.hadoop.fs.FileSystem$Statistics)" : "* Copy constructor.\n     *\n     * @param other    The input Statistics object which is cloned.",
  "org.apache.hadoop.net.SocksSocketFactory:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.impl.FutureIOSupport:raiseInnerCause(java.util.concurrent.CompletionException)" : "* Extract the cause of a completion failure and rethrow it if an IOE\n   * or RTE.\n   * See {@link FutureIO#raiseInnerCause(CompletionException)}.\n   * @param e exception.\n   * @param <T> type of return value.\n   * @return nothing, ever.\n   * @throws IOException either the inner IOException, or a wrapper around\n   * any non-Runtime-Exception\n   * @throws RuntimeException if that is the inner cause.",
  "org.apache.hadoop.io.MapFile$Writer:setIndexInterval(org.apache.hadoop.conf.Configuration,int)" : "* Sets the index interval and stores it in conf.\n     * @see #getIndexInterval()\n     *\n     * @param conf configuration.\n     * @param interval interval.",
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSinks()" : null,
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm:returnDecompressor(org.apache.hadoop.io.compress.Decompressor)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)" : "* Returns an authenticated {@link HttpURLConnection}. If the Delegation\n   * Token is present, it will be used taking precedence over the configured\n   * <code>Authenticator</code>. If the <code>doAs</code> parameter is not NULL,\n   * the request will be done on behalf of the specified <code>doAs</code> user.\n   *\n   * @param url the URL to connect to. Only HTTP/S URLs are supported.\n   * @param token the authentication token being used for the user.\n   * @param doAs user to do the the request on behalf of, if NULL the request is\n   * as self.\n   * @return an authenticated {@link HttpURLConnection}.\n   * @throws IOException if an IO error occurred.\n   * @throws AuthenticationException if an authentication exception occurred.",
  "org.apache.hadoop.fs.impl.FileRangeImpl:getReference()" : null,
  "org.apache.hadoop.util.functional.LazyAtomicReference:getReference()" : "* Get the reference.\n   * Subclasses working with this need to be careful working with this.\n   * @return the reference.",
  "org.apache.hadoop.util.Shell:bashQuote(java.lang.String)" : "* Quote the given arg so that bash will interpret it as a single value.\n   * Note that this quotes it for one level of bash, if you are passing it\n   * into a badly written shell script, you need to fix your shell script.\n   * @param arg the argument to quote\n   * @return the quoted string",
  "org.apache.hadoop.fs.ContentSummary$Builder:fileCount(long)" : null,
  "org.apache.hadoop.ipc.Server$Call:doResponse(java.lang.Throwable,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs:readOnlyMountTable(java.lang.String,org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.service.launcher.IrqHandler:getSignalCount()" : "* Get the count of how many times a signal has been raised.\n   * @return the count of signals",
  "org.apache.hadoop.service.launcher.ServiceLauncher$MinimalGenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])" : null,
  "org.apache.hadoop.ha.ShellCommandFencer:checkArgs(java.lang.String)" : null,
  "org.apache.hadoop.fs.FileContext:openFile(org.apache.hadoop.fs.Path)" : "* Open a file for reading through a builder API.\n   * Ultimately calls {@link #open(Path, int)} unless a subclass\n   * executes the open command differently.\n   *\n   * The semantics of this call are therefore the same as that of\n   * {@link #open(Path, int)} with one special point: it is in\n   * {@code FSDataInputStreamBuilder.build()} in which the open operation\n   * takes place -it is there where all preconditions to the operation\n   * are checked.\n   * @param path file path\n   * @return a FSDataInputStreamBuilder object to build the input stream\n   * @throws IOException if some early checks cause IO failures.\n   * @throws UnsupportedOperationException if support is checked early.",
  "org.apache.hadoop.fs.ChecksumFs:open(org.apache.hadoop.fs.Path,int)" : "* Opens an FSDataInputStream at the indicated Path.\n   * @param f the file name to open\n   * @param bufferSize the size of the buffer to be used.",
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:gauges()" : null,
  "org.apache.hadoop.net.DNS:getIPsAsInetAddressList(java.lang.String,boolean)" : "* Returns all the IPs associated with the provided interface, if any, as\n   * a list of InetAddress objects.\n   *\n   * @param strInterface\n   *            The name of the network interface or sub-interface to query\n   *            (eg eth0 or eth0:0) or the string \"default\"\n   * @param returnSubinterfaces\n   *            Whether to return IPs associated with subinterfaces of\n   *            the given interface\n   * @return A list of all the IPs associated with the provided\n   *         interface. The local host IP is returned if the interface\n   *         name \"default\" is specified or there is an I/O error looking\n   *         for the given interface.\n   * @throws UnknownHostException\n   *             If the given interface is invalid\n   *",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:addMinimumSample(java.lang.String,long)" : null,
  "org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[])" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:unregisterSource(java.lang.String)" : null,
  "org.apache.hadoop.util.JvmPauseMonitor:serviceStart()" : null,
  "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:close()" : null,
  "org.apache.hadoop.http.HttpServer2$Builder:setEnabledProtocols(org.eclipse.jetty.util.ssl.SslContextFactory)" : null,
  "org.apache.hadoop.io.Text:decode(byte[],int,int,boolean)" : "* @return Converts the provided byte array to a String using the\n   * UTF-8 encoding. If <code>replace</code> is true, then\n   * malformed input is replaced with the\n   * substitution character, which is U+FFFD. Otherwise the\n   * method throws a MalformedInputException.\n   *\n   * @param utf8 input utf8.\n   * @param start input start.\n   * @param length input length.\n   * @param replace input replace.\n   * @throws CharacterCodingException when a character\n   *                                  encoding or decoding error occurs.",
  "org.apache.hadoop.fs.Options$HandleOpt:exact()" : "* Handle is valid iff the referent is neither moved nor changed.\n     * Equivalent to changed(false), moved(false).\n     * @return Options requiring that the content and location of the entity\n     * be unchanged between calls.",
  "org.apache.hadoop.fs.FileStatus:attributes(boolean,boolean,boolean,boolean)" : "* Convert boolean attributes to a set of flags.\n   * @param acl   See {@link AttrFlags#HAS_ACL}.\n   * @param crypt See {@link AttrFlags#HAS_CRYPT}.\n   * @param ec    See {@link AttrFlags#HAS_EC}.\n   * @param sn    See {@link AttrFlags#SNAPSHOT_ENABLED}.\n   * @return converted set of flags.",
  "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:loadFrom(java.nio.file.Path)" : null,
  "org.apache.hadoop.security.LdapGroupsMapping:getPasswordFromCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:extractQueryParameters(java.lang.String)" : "* Split up the string. Uses httpClient: make sure it is on the classpath.\n   * Any query param with a name but no value, e.g ?something is\n   * returned in the map with an empty string as the value.\n   * @param header URI to parse\n   * @return a map of parameters.\n   * @throws URISyntaxException failure to build URI from header.",
  "org.apache.hadoop.security.SecurityUtil:getAuthenticationMethod(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.metrics2.util.Contracts:checkArg(int,boolean,java.lang.Object)" : "* Check an argument for false conditions\n   * @param arg the argument to check\n   * @param expression  the boolean expression for the condition\n   * @param msg the error message if {@code expression} is false\n   * @return the argument for convenience",
  "org.apache.hadoop.http.HttpServer2:setAttribute(java.lang.String,java.lang.Object)" : "* Set a value in the webapp context. These values are available to the jsp\n   * pages as \"application.getAttribute(name)\".\n   * @param name The name of the attribute\n   * @param value The value of the attribute",
  "org.apache.hadoop.security.authorize.ImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)" : "* Authorize the superuser which is doing doAs.\n   * {@link #authorize(UserGroupInformation, InetAddress)} should\n   *             be preferred to avoid possibly re-resolving the ip address.\n   * @param user ugi of the effective or proxy user which contains a real user.\n   * @param remoteAddress the ip address of client.\n   * @throws AuthorizationException Authorization Exception.",
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:mustDouble(java.lang.String,double)" : "* Set optional double parameter for the Builder.\n   *\n   * @see #opt(String, String)",
  "org.apache.hadoop.ipc.DefaultCostProvider:init(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])" : null,
  "org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Creates an instance of the requested {@link Expression} class.\n   *\n   * @param expressionClassname\n   *          name of the {@link Expression} class to be instantiated\n   * @param conf\n   *          the Hadoop configuration\n   * @return a new instance of the requested {@link Expression} class",
  "org.apache.hadoop.metrics2.MetricsJsonBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)" : null,
  "org.apache.hadoop.security.SaslRpcClient:sendSaslMessage(java.io.OutputStream,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:getComponentType()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.fs.PathIsNotEmptyDirectoryException:<init>(java.lang.String)" : "@param path for the exception",
  "org.apache.hadoop.util.bloom.BloomFilter:getNBytes()" : null,
  "org.apache.hadoop.fs.AvroFSInput:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)" : "Construct given a {@link FileContext} and a {@link Path}.\n   * @param fc filecontext.\n   * @param p the path.\n   * @throws IOException If an I/O error occurred.\n   *",
  "org.apache.hadoop.fs.ChecksumFileSystem:getApproxChkSumLength(long)" : null,
  "org.apache.hadoop.security.HadoopKerberosName:main(java.lang.String[])" : null,
  "org.apache.hadoop.util.SysInfoWindows:getPhysicalMemorySize()" : "{@inheritDoc}",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream$Data:initTT(int)" : "* Initializes the {@link #tt} array.\n    *\n    * This method is called when the required length of the array is known.\n    * I don't initialize it at construction time to avoid unnecessary\n    * memory allocation when compressing small files.",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:setGauge(java.lang.String,long)" : null,
  "org.apache.hadoop.net.SocketIOWithTimeout:timeoutExceptionString(java.nio.channels.SelectableChannel,long,int)" : null,
  "org.apache.hadoop.util.bloom.CountingBloomFilter:and(org.apache.hadoop.util.bloom.Filter)" : null,
  "org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : null,
  "org.apache.hadoop.net.unix.DomainSocket:<init>(java.lang.String,int)" : null,
  "org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet)" : "* Creates a new Nfly instance.\n   *\n   * @param uris the list of uris in the mount point\n   * @param conf configuration object\n   * @param minReplication minimum copies to commit a write op\n   * @param nflyFlags modes such readMostRecent\n   * @throws IOException",
  "org.apache.hadoop.conf.ConfigurationWithLogging:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Called when we're all done writing to the target.  A local FS will\n   * do nothing, because we've written to exactly the right place.  A remote\n   * FS will copy the contents of tmpLocalFile to the correct target at\n   * fsOutputFile.",
  "org.apache.hadoop.http.HttpServer2$XFrameOption:toString()" : null,
  "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getCrcTypeFromAlgorithmName(java.lang.String)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:remainder(byte[][],int[],int,int[])" : "* The \"bulk\" version of the remainder.\n   * Warning: This function will modify the \"dividend\" inputs.\n   *\n   * @param dividend dividend.\n   * @param offsets offsets.\n   * @param len len.\n   * @param divisor divisor.",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getMyFs()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:findFirstValidInput(java.lang.Object[])" : "* Find the valid input from all the inputs.\n   * @param inputs input buffers to look for valid input\n   * @return the first valid input",
  "org.apache.hadoop.util.Shell:getExitCode()" : "get the exit code.\n   * @return the exit code of the process",
  "org.apache.hadoop.fs.ByteBufferUtil:fallbackRead(java.io.InputStream,org.apache.hadoop.io.ByteBufferPool,int)" : "* Perform a fallback read.\n   *\n   * @param stream input stream.\n   * @param bufferPool bufferPool.\n   * @param maxLength maxLength.\n   * @throws IOException raised on errors performing I/O.\n   * @return byte buffer.",
  "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,short)" : "* Create an FSDataOutputStream at the indicated Path.\n   * Files are overwritten by default.\n   * @param f the file to create\n   * @param replication the replication factor\n   * @throws IOException IO failure\n   * @return output stream1",
  "org.apache.hadoop.util.SysInfoWindows:<init>()" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:checkIoStatisticsAvailable()" : "* Require a IOStatistics to be available.\n   * @throws UnsupportedOperationException if the method was not found.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:entry()" : "* Get an entry to access the key and value.\n       * \n       * @return The Entry object to access the key and value.\n       * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.SequenceFile$Writer:getCompressionCodec()" : "@return Returns the compression codec of data in this file.",
  "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:toString()" : null,
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:bufferSize(int)" : "* Set the size of the buffer to be used.",
  "org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:hashCode()" : "Override hashcode to avoid findbugs warnings",
  "org.apache.hadoop.util.functional.LazyAutoCloseableReference:close()" : "* Close the reference value if it is non-null.\n   * Sets the reference to null afterwards, even on\n   * a failure.\n   * @throws Exception failure to close.",
  "org.apache.hadoop.fs.permission.FsAction:not()" : "* NOT operation.\n   * @return FsAction.",
  "org.apache.hadoop.fs.viewfs.RegexMountPoint:replaceRegexCaptureGroupInPath(java.lang.String,java.util.regex.Matcher,java.lang.String,java.util.Set)" : "* Use capture group named regexGroupNameOrIndexStr in mather to replace\n   * parsedDestPath.\n   * E.g. link: ^/user/(?<username>\\\\w+) => s3://$user.apache.com/_${user}\n   * srcMatcher is from /user/hadoop.\n   * Then the params will be like following.\n   * parsedDestPath: s3://$user.apache.com/_${user},\n   * regexGroupNameOrIndexStr: user\n   * groupRepresentationStrSetInDest: {user:$user; user:${user}}\n   * return value will be s3://hadoop.apache.com/_hadoop\n   * @param parsedDestPath\n   * @param srcMatcher\n   * @param regexGroupNameOrIndexStr\n   * @param groupRepresentationStrSetInDest\n   * @return return parsedDestPath while ${var},$var replaced or\n   * parsedDestPath nothing found.",
  "org.apache.hadoop.fs.viewfs.ChRootedFs:truncate(org.apache.hadoop.fs.Path,long)" : null,
  "org.apache.hadoop.util.ShutdownHookManager:getShutdownHooksInOrder()" : "* Returns the list of shutdownHooks in order of execution,\n   * Highest priority first.\n   *\n   * @return the list of shutdownHooks in order of execution.",
  "org.apache.hadoop.io.FloatWritable:<init>()" : null,
  "org.apache.hadoop.ipc.Client:getSocketFactory()" : "Return the socket factory of this client\n   *\n   * @return this client's socket factory",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)" : "* Constructor\n       * \n       * @param reader\n       *          The TFile reader object.\n       * @param beginKey\n       *          Begin key of the scan. If null, scan from the first\n       *          &lt;K, V&gt; entry of the TFile.\n       * @param endKey\n       *          End key of the scan. If null, scan up to the last &lt;K, V&gt;\n       *          entry of the TFile.\n       * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:getPrivateKey(java.lang.String)" : null,
  "org.apache.hadoop.http.AdminAuthorizedServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.util.SysInfoWindows:reset()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec$StubDecompressor:end()" : null,
  "org.apache.hadoop.io.compress.CodecPool:getDecompressor(org.apache.hadoop.io.compress.CompressionCodec)" : "* Get a {@link Decompressor} for the given {@link CompressionCodec} from the\n   * pool or a new one.\n   *  \n   * @param codec the <code>CompressionCodec</code> for which to get the \n   *              <code>Decompressor</code>\n   * @return <code>Decompressor</code> for the given \n   *         <code>CompressionCodec</code> the pool or a new one",
  "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getScheme()" : null,
  "org.apache.hadoop.io.file.tfile.ByteArray:size()" : "* @return the size of the byte array.",
  "org.apache.hadoop.util.Sets:newTreeSet(java.lang.Iterable)" : "* Creates a <i>mutable</i> {@code TreeSet} instance containing the given\n   * elements sorted by their natural ordering.\n   *\n   * <p><b>Note:</b> if mutability is not required, use\n   * ImmutableSortedSet#copyOf(Iterable) instead.\n   *\n   * <p><b>Note:</b> If {@code elements} is a {@code SortedSet} with an\n   * explicit comparator, this method has different behavior than\n   * {@link TreeSet#TreeSet(SortedSet)}, which returns a {@code TreeSet}\n   * with that comparator.\n   *\n   * <p><b>Note for Java 7 and later:</b> this method is now unnecessary and\n   * should be treated as deprecated. Instead, use the {@code TreeSet}\n   * constructor directly, taking advantage of the new\n   * <a href=\"http://goo.gl/iz2Wi\">\"diamond\" syntax</a>.\n   *\n   * <p>This method is just a small convenience for creating an empty set and\n   * then calling Iterables#addAll. This method is not very useful and will\n   * likely be deprecated in the future.\n   *\n   * @param <E> Generics Type E.\n   * @param elements the elements that the set should contain\n   * @return a new {@code TreeSet} containing those elements (minus duplicates)",
  "org.apache.hadoop.io.SequenceFile$Writer:flush()" : null,
  "org.apache.hadoop.fs.FileStatus:hashCode()" : "* Returns a hash code value for the object, which is defined as\n   * the hash code of the path name.\n   *\n   * @return  a hash code value for the path name.",
  "org.apache.hadoop.io.erasurecode.CodecRegistry:updateCoders(java.lang.Iterable)" : "* Update coderMap and coderNameMap with iterable type of coder factories.\n   * @param coderFactories",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getChildFileSystems()" : null,
  "org.apache.hadoop.util.Shell:<init>()" : "* Create an instance with no minimum interval between runs; stderr is\n   * not merged with stdout.",
  "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:acquireHelper(boolean)" : null,
  "org.apache.hadoop.net.SocketInputWrapper:setTimeout(long)" : "* Set the timeout for reads from this stream.\n   * \n   * Note: the behavior here can differ subtly depending on whether the\n   * underlying socket has an associated Channel. In particular, if there is no\n   * channel, then this call will affect the socket timeout for <em>all</em>\n   * readers of this socket. If there is a channel, then this call will affect\n   * the timeout only for <em>this</em> stream. As such, it is recommended to\n   * only create one {@link SocketInputWrapper} instance per socket.\n   * \n   * @param timeoutMs\n   *          the new timeout, 0 for no timeout\n   * @throws SocketException\n   *           if the timeout cannot be set",
  "org.apache.hadoop.ipc.Server:getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto)" : null,
  "org.apache.hadoop.ipc.RefreshResponse:getSenderName()" : null,
  "org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:take()" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:hasNext()" : "* Trigger a fetch if an entry is needed.\n     * @return true if there was already an entry return,\n     * or there was not but one could then be retrieved.set\n     * @throws IOException failure in fetch operation",
  "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:registerMetrics2Source(java.lang.String)" : null,
  "org.apache.hadoop.fs.permission.FsPermission:<init>()" : null,
  "org.apache.hadoop.crypto.key.KeyProviderExtension:rollNewVersion(java.lang.String)" : null,
  "org.apache.hadoop.ipc.Server$Responder:doRespond(org.apache.hadoop.ipc.Server$RpcCall)" : null,
  "org.apache.hadoop.util.IntrusiveCollection$IntrusiveIterator:next()" : null,
  "org.apache.hadoop.ha.ShellCommandFencer:abbreviate(java.lang.String,int)" : "* Abbreviate a string by putting '...' in the middle of it,\n   * in an attempt to keep logs from getting too messy.\n   * @param cmd the string to abbreviate\n   * @param len maximum length to abbreviate to\n   * @return abbreviated string",
  "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:isOnSameNodeGroup(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)" : "* Check if two nodes are on the same node group (hypervisor) The\n   * assumption here is: each nodes are leaf nodes.\n   * \n   * @param node1\n   *            one node (can be null)\n   * @param node2\n   *            another node (can be null)\n   * @return true if node1 and node2 are on the same node group; false\n   *         otherwise\n   * @exception IllegalArgumentException\n   *                when either node1 or node2 is null, or node1 or node2 do\n   *                not belong to the cluster",
  "org.apache.hadoop.util.Shell:<init>(long,boolean)" : "* Create a shell instance which can be re-executed when the {@link #run()}\n   * method is invoked with a given elapsed time between calls.\n   *\n   * @param interval the minimum duration in milliseconds to wait before\n   *        re-executing the command. If set to 0, there is no minimum.\n   * @param redirectErrorStream should the error stream be merged with\n   *        the normal output stream?",
  "org.apache.hadoop.io.compress.CodecPool:getLeasedDecompressorsCount(org.apache.hadoop.io.compress.CompressionCodec)" : "* Return the number of leased {@link Decompressor}s for this\n   * {@link CompressionCodec}.\n   *\n   * @param codec codec.\n   * @return the number of leased",
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:reset()" : null,
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:setInstance(org.apache.hadoop.metrics2.MetricsSystem)" : null,
  "org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRandomRead(java.io.File,java.lang.String,java.lang.String,java.lang.String)" : "* @return Same as openForRandomRead except that it will run even if security is off.\n   * This is used by unit tests.\n   *\n   * @param f input f.\n   * @param mode input mode.\n   * @param expectedOwner input expectedOwner.\n   * @param expectedGroup input expectedGroup.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.permission.FsAction:and(org.apache.hadoop.fs.permission.FsAction)" : "* AND operation.\n   * @param that FsAction that.\n   * @return FsAction.",
  "org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:pageSize()" : null,
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping$PartialGroupNameException:<init>(java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.util.CrcComposer:newStripedCrcComposer(org.apache.hadoop.util.DataChecksum$Type,long,long)" : "* Returns a CrcComposer which will collapse CRCs for every combined\n   * underlying data size which aligns with the specified stripe boundary. For\n   * example, if \"update\" is called with 20 CRCs and bytesPerCrc == 5, and\n   * stripeLength == 10, then every two (10 / 5) consecutive CRCs will be\n   * combined with each other, yielding a list of 10 CRC \"stripes\" in the\n   * final digest, each corresponding to 10 underlying data bytes. Using\n   * a stripeLength greater than the total underlying data size is equivalent\n   * to using a non-striped CrcComposer.\n   *\n   * @param type type.\n   * @param bytesPerCrcHint bytesPerCrcHint.\n   * @param stripeLength stripeLength.\n   * @return a CrcComposer which will collapse CRCs for every combined.\n   * underlying data size which aligns with the specified stripe boundary.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.shell.Command:getDescription()" : "* The long usage suitable for help output\n   * @return text of the usage",
  "org.apache.hadoop.fs.FSOutputSummer:resetChecksumBufSize()" : null,
  "org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:get(java.nio.channels.SelectableChannel)" : "* Takes one selector from end of LRU list of free selectors.\n     * If there are no selectors awailable, it creates a new selector.\n     * Also invokes trimIdleSelectors(). \n     * \n     * @param channel\n     * @return \n     * @throws IOException",
  "org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:beforeExecute(java.lang.Thread,java.lang.Runnable)" : null,
  "org.apache.hadoop.conf.Configuration:addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[])" : "* Adds a set of deprecated keys to the global deprecations.\n   *\n   * This method is lockless.  It works by means of creating a new\n   * DeprecationContext based on the old one, and then atomically swapping in\n   * the new context.  If someone else updated the context in between us reading\n   * the old context and swapping in the new one, we try again until we win the\n   * race.\n   *\n   * @param deltas   The deprecations to add.",
  "org.apache.hadoop.net.NetworkTopology:interRemoveNodeWithEmptyRack(org.apache.hadoop.net.Node)" : "* Internal function for update empty rack number\n   * for remove or decommission a node.\n   * @param node node to be removed; can be null",
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_aggregate(java.io.Serializable,java.lang.Object)" : "* Aggregate an existing {@link IOStatisticsSnapshot} with\n   * the supplied statistics.\n   * @param snapshot snapshot to update\n   * @param statistics IOStatistics to add\n   * @return true if the snapshot was updated.\n   * @throws IllegalArgumentException if the {@code statistics} argument is not\n   * null but not an instance of IOStatistics, or if  {@code snapshot} is invalid.",
  "org.apache.hadoop.fs.store.DataBlocks$DataBlock:<init>(long,org.apache.hadoop.fs.store.BlockUploadStatistics)" : null,
  "org.apache.hadoop.fs.Options$CreateOpts:getOpt(java.lang.Class,org.apache.hadoop.fs.Options$CreateOpts[])" : "* Get an option of desired type\n     * @param clazz is the desired class of the opt\n     * @param opts - not null - at least one opt must be passed\n     * @return an opt from one of the opts of type theClass.\n     *   returns null if there isn't any",
  "org.apache.hadoop.fs.viewfs.RegexMountPoint:getVarInDestPathMap()" : null,
  "org.apache.hadoop.ipc.Server:logException(org.slf4j.Logger,java.lang.Throwable,org.apache.hadoop.ipc.Server$Call)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:add(org.apache.hadoop.metrics2.AbstractMetric)" : null,
  "org.apache.hadoop.util.dynamic.DynConstructors$Builder:impl(java.lang.String,java.lang.Class[])" : null,
  "org.apache.hadoop.fs.QuotaUsage:isTypeQuotaSet()" : "* Return true if any storage type quota has been set.\n   *\n   * @return if any storage type quota has been set true, not false.\n   *",
  "org.apache.hadoop.conf.Configuration:appendXMLProperty(org.w3c.dom.Document,org.w3c.dom.Element,java.lang.String,org.apache.hadoop.conf.ConfigRedactor)" : "*  Append a property with its attributes to a given {#link Document}\n   *  if the property is found in configuration.\n   *\n   * @param doc\n   * @param conf\n   * @param propertyName",
  "org.apache.hadoop.io.compress.DecompressorStream:read(byte[],int,int)" : null,
  "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)" : null,
  "org.apache.hadoop.io.WritableComparator:compare(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.WritableComparable)" : "* Compare two WritableComparables.\n   *\n   * The default implementation uses the natural ordering, calling {@link\n   * Comparable#compareTo(Object)}.\n   * @param a the first object to be compared.\n   * @param b the second object to be compared.\n   * @return compare result.",
  "org.apache.hadoop.util.Shell:checkWindowsCommandLineLength(java.lang.String[])" : "* Checks if a given command (String[]) fits in the Windows maximum command\n   * line length Note that the input is expected to already include space\n   * delimiters, no extra count will be added for delimiters.\n   *\n   * @param commands command parts, including any space delimiters\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.shell.FsUsage$TableBuilder:printToStream(java.io.PrintStream)" : "* Render the table to a stream.\n     * @param out PrintStream for output",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:createKeyProvider(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:equals(java.lang.Object)" : null,
  "org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:noPasswordError()" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:<init>(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.ElasticByteBufferPool$Key:equals(java.lang.Object)" : null,
  "org.apache.hadoop.util.bloom.CountingBloomFilter:buckets2words(int)" : "returns the number of 64 bit words it would take to hold vectorSize buckets",
  "org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:xdr_int(int)" : "* Puts an integer into the buffer as 4 bytes, big-endian.\n   * @param i i.",
  "org.apache.hadoop.fs.viewfs.RegexMountPoint:getRemainingPathStr(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.util.SysInfoLinux:getVirtualMemorySize()" : "{@inheritDoc}",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.tracing.Tracer:newScope(java.lang.String)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.DoubleWritable:set(double)" : null,
  "org.apache.hadoop.util.InstrumentedReadLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long,org.apache.hadoop.util.Timer)" : null,
  "org.apache.hadoop.service.AbstractService:unregisterGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener)" : "* unregister a global listener.\n   * @param l listener to unregister\n   * @return true if the listener was found (and then deleted)",
  "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:clearAccessed()" : null,
  "org.apache.hadoop.util.ExitUtil:halt(int)" : "* Forcibly terminates the currently running Java virtual machine.\n   * @param status status code\n   * @throws HaltException if {@link Runtime#halt(int)} is disabled.",
  "org.apache.hadoop.io.BoundedByteArrayOutputStream:reset()" : "Reset the buffer",
  "org.apache.hadoop.fs.permission.AclStatus:toString()" : null,
  "org.apache.hadoop.fs.DF:verifyExitCode()" : null,
  "org.apache.hadoop.io.BloomMapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.Count:getAndCheckStorageTypes(java.lang.String)" : null,
  "org.apache.hadoop.fs.FileSystem:getAdditionalTokenIssuers()" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:<init>(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink,java.lang.String,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,int,int,int,float,int)" : null,
  "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:getLength()" : null,
  "org.apache.hadoop.util.LightWeightGSet$SetIterator:remove()" : null,
  "org.apache.hadoop.service.launcher.AbstractLaunchableService:execute()" : "* {@inheritDoc}\n   * <p>\n   * The action is to signal success by returning the exit code 0.",
  "org.apache.hadoop.ipc.WritableRpcEngine:getClient(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(org.apache.hadoop.io.BytesWritable)" : "* Copy the value into BytesWritable. The input BytesWritable will be\n         * automatically resized to the actual value size. The implementation\n         * directly uses the buffer inside BytesWritable for storing the value.\n         * The call does not require the value length to be known.\n         * \n         * @param value value.\n         * @throws IOException raised on errors performing I/O.\n         * @return long value.",
  "org.apache.hadoop.io.compress.CompressionInputStream:seekToNewSource(long)" : "* This method is current not supported.\n   *\n   * @throws UnsupportedOperationException Unsupported Operation Exception.",
  "org.apache.hadoop.ipc.CallerContext:setCurrent(org.apache.hadoop.ipc.CallerContext)" : null,
  "org.apache.hadoop.conf.Configuration:setClass(java.lang.String,java.lang.Class,java.lang.Class)" : "* Set the value of the <code>name</code> property to the name of a \n   * <code>theClass</code> implementing the given interface <code>xface</code>.\n   * \n   * An exception is thrown if <code>theClass</code> does not implement the \n   * interface <code>xface</code>. \n   * \n   * @param name property name.\n   * @param theClass property value.\n   * @param xface the interface implemented by the named class.",
  "org.apache.hadoop.fs.GetSpaceUsed$Builder:getKlass()" : null,
  "org.apache.hadoop.ipc.Server:refreshServiceAcl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)" : "* Refresh the service authorization ACL for the service handled by this server.\n   *\n   * @param conf input Configuration.\n   * @param provider input PolicyProvider.",
  "org.apache.hadoop.util.CrcComposer:<init>(int,int,long,long)" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyCompressor:needsInput()" : "* Returns true if the input data buffer is empty and\n   * #setInput() should be called to provide more input.\n   *\n   * @return <code>true</code> if the input data buffer is empty and\n   *         #setInput() should be called in order to provide more input.",
  "org.apache.hadoop.fs.DUHelper:calculateFolderSize(java.lang.String)" : null,
  "org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,int)" : "* Set optional int parameter for the Builder.\n   *\n   * @param key key.\n   * @param value value.\n   * @return generic type B.\n   * @see #opt(String, String)",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:setCounter(java.lang.String,long)" : null,
  "org.apache.hadoop.io.UTF8:getLength()" : "@return The number of bytes in the encoded string.",
  "org.apache.hadoop.fs.FileUtil:getJarsInDirectory(java.lang.String,boolean)" : "* Returns all jars that are in the directory. It is useful in expanding a\n   * wildcard path to return all jars from the directory to use in a classpath.\n   *\n   * @param path the path to the directory. The path may include the wildcard.\n   * @param useLocal use local.\n   * @return the list of jars as URLs, or an empty list if there are no jars, or\n   * the directory does not exist",
  "org.apache.hadoop.crypto.key.KeyProvider$Metadata:serialize()" : "* Serialize the metadata to a set of bytes.\n     * @return the serialized bytes\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)" : null,
  "org.apache.hadoop.util.concurrent.AsyncGetFuture:get(long,java.util.concurrent.TimeUnit)" : null,
  "org.apache.hadoop.fs.ContentSummary:<init>()" : "Constructor deprecated by ContentSummary.Builder",
  "org.apache.hadoop.util.DiskChecker$DefaultFileIoProvider:get(java.io.File)" : "* See {@link FileOutputStream#FileOutputStream(File)}.",
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(byte[],int,int)" : "*",
  "org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String,java.lang.String,boolean)" : "* Returns the default (first) host name associated by the provided\n   * nameserver with the address bound to the specified network interface\n   * \n   * @param strInterface\n   *            The name of the network interface to query (e.g. eth0)\n   * @param nameserver\n   *            The DNS host name\n   * @param tryfallbackResolution\n   *            Input tryfallbackResolution.\n   * @return The default host names associated with IPs bound to the network\n   *         interface\n   * @throws UnknownHostException\n   *             If one is encountered while querying the default interface",
  "org.apache.hadoop.fs.FsShell:close()" : "*  Performs any necessary cleanup\n   * @throws IOException upon error",
  "org.apache.hadoop.fs.DelegateToFileSystem:open(org.apache.hadoop.fs.Path,int)" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:snapshot()" : "* Create a new empty snapshot.\n   * A new one is always created for isolation.\n   *\n   * @return a statistics snapshot",
  "org.apache.hadoop.fs.shell.CommandFactory:getNames()" : "* Gets all of the registered commands\n   * @return a sorted list of command names",
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:createGroupIDExecutor(java.lang.String)" : "* Create a ShellCommandExecutor object for fetch a user's group id list.\n   *\n   * @param userName the user's name\n   * @return a ShellCommandExecutor object",
  "org.apache.hadoop.fs.statistics.MeanStatistic:<init>()" : "* Create an empty statistic.",
  "org.apache.hadoop.util.hash.Hash:getInstance(int)" : "* Get a singleton instance of hash function of a given type.\n   * @param type predefined hash type\n   * @return hash function instance, or null if type is invalid",
  "org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:getPasswordCharArray(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.CompressionOutputStream:flush()" : null,
  "org.apache.hadoop.metrics2.lib.MutableCounter:<init>(org.apache.hadoop.metrics2.MetricsInfo)" : null,
  "org.apache.hadoop.fs.Path:suffix(java.lang.String)" : "* Adds a suffix to the final name in the path.\n   *\n   * @param suffix the suffix to add\n   * @return a new path with the suffix added",
  "org.apache.hadoop.util.ProgramDriver:addClass(java.lang.String,java.lang.Class,java.lang.String)" : "* This is the method that adds the classed to the repository.\n   * @param name The name of the string you want the class instance to be called with\n   * @param mainClass The class that you want to add to the repository\n   * @param description The description of the class\n   * @throws NoSuchMethodException when a particular method cannot be found.\n   * @throws SecurityException security manager to indicate a security violation.",
  "org.apache.hadoop.util.functional.RemoteIterators:mappingRemoteIterator(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)" : "* Create an iterator from an iterator and a transformation function.\n   * @param <S> source type\n   * @param <T> result type\n   * @param iterator source\n   * @param mapper transformation\n   * @return a remote iterator",
  "org.apache.hadoop.io.file.tfile.Compression$FinishOnFlushCompressionStream:write(byte[],int,int)" : null,
  "org.apache.hadoop.io.GenericWritable:set(org.apache.hadoop.io.Writable)" : "* Set the instance that is wrapped.\n   * \n   * @param obj input obj.",
  "org.apache.hadoop.http.HttpServer2$QuotingInputFilter:inferMimeType(javax.servlet.ServletRequest)" : "* Infer the mime type for the response based on the extension of the request\n     * URI. Returns null if unknown.",
  "org.apache.hadoop.util.BasicDiskValidator:checkStatus(java.io.File)" : null,
  "org.apache.hadoop.fs.CachingGetSpaceUsed:setUsed(long)" : "* Reset the current used data amount. This should be called\n   * when the cached value is re-computed.\n   *\n   * @param usedValue new value that should be the disk usage.",
  "org.apache.hadoop.ipc.RPC:getProtocolName(java.lang.Class)" : "* Get the protocol name.\n   *  If the protocol class has a ProtocolAnnotation, then get the protocol\n   *  name from the annotation; otherwise the class name is the protocol name.\n   *\n   * @param protocol input protocol.\n   * @return protocol name.",
  "org.apache.hadoop.ipc.RpcException:<init>(java.lang.String)" : "* Constructs exception with the specified detail message.\n   * \n   * @param messages detailed message.",
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:setInputFromSavedData()" : null,
  "org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String,java.lang.String[])" : "* Get the comma delimited values of the <code>name</code> property as \n   * an array of <code>String</code>s, trimmed of the leading and trailing whitespace.\n   * If no such property is specified then default value is returned.\n   * \n   * @param name property name.\n   * @param defaultValue The default value\n   * @return property value as an array of trimmed <code>String</code>s, \n   *         or default value.",
  "org.apache.hadoop.fs.FileSystem$Cache:<init>(org.apache.hadoop.conf.Configuration)" : "* Instantiate. The configuration is used to read the\n     * count of permits issued for concurrent creation\n     * of filesystem instances.\n     * @param conf configuration",
  "org.apache.hadoop.util.DiskChecker$DiskErrorException:<init>(java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMaximums(java.lang.Long,java.lang.Long)" : "* Aggregate two maximum values.\n   * @param l left\n   * @param r right\n   * @return the new minimum.",
  "org.apache.hadoop.security.UserGroupInformation$RealUser:hashCode()" : null,
  "org.apache.hadoop.conf.Configuration:substituteCommonVariables(java.lang.String)" : "* Provides a public wrapper over substituteVars in order to avoid compatibility issues.\n   * See HADOOP-18021 for further details.\n   *\n   * @param expr the literal value of a config key\n   * @return null if expr is null, otherwise the value resulting from expanding\n   * expr using the algorithm above.\n   * @throws IllegalArgumentException when more than\n   * {@link Configuration#MAX_SUBST} replacements are required",
  "org.apache.hadoop.fs.FileStatus:isDir()" : "* Old interface, instead use the explicit {@link FileStatus#isFile()},\n   * {@link FileStatus#isDirectory()}, and {@link FileStatus#isSymlink()}\n   * @return true if this is a directory.\n   * @deprecated Use {@link FileStatus#isFile()},\n   * {@link FileStatus#isDirectory()}, and {@link FileStatus#isSymlink()}\n   * instead.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKeyBuffer()" : null,
  "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:init(java.util.Properties)" : null,
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroups(java.lang.String)" : "* Returns list of groups for a user\n   *\n   * @param userName get groups for this user\n   * @return list of groups for a given user",
  "org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:getRequiredNumParityBlocks()" : "* Get required parity blocks count in a BlockGroup.\n   * @return count of required parity blocks",
  "org.apache.hadoop.io.SequenceFile$UncompressedBytes:writeCompressedBytes(java.io.DataOutputStream)" : null,
  "org.apache.hadoop.util.KMSUtil:checkNotEmpty(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.ipc.Server:getListenerAddress()" : "* Return the socket (ip+port) on which the RPC server is listening to.\n   * @return the socket (ip+port) on which the RPC server is listening to.",
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)" : null,
  "org.apache.hadoop.fs.FileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* Initialize a FileSystem.\n   *\n   * Called after the new FileSystem instance is constructed, and before it\n   * is ready for use.\n   *\n   * FileSystem implementations overriding this method MUST forward it to\n   * their superclass, though the order in which it is done, and whether\n   * to alter the configuration before the invocation are options of the\n   * subclass.\n   * @param name a URI whose authority section names the host, port, etc.\n   *   for this FileSystem\n   * @param conf the configuration\n   * @throws IOException on any failure to initialize this instance.\n   * @throws IllegalArgumentException if the URI is considered invalid.",
  "org.apache.hadoop.util.ProtoUtil:convert(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto)" : null,
  "org.apache.hadoop.service.AbstractService:putBlocker(java.lang.String,java.lang.String)" : "* Put a blocker to the blocker map -replacing any\n   * with the same name.\n   * @param name blocker name\n   * @param details any specifics on the block. This must be non-null.",
  "org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getFixedByteString(java.lang.String)" : "* Get the ByteString for frequently used fixed and small set strings.\n   * @param key string\n   * @return ByteString for frequently used fixed and small set strings.",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementCounter(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.util.LightWeightGSet:convert(org.apache.hadoop.util.LightWeightGSet$LinkedElement)" : null,
  "org.apache.hadoop.io.compress.DecompressorStream:decompress(byte[],int,int)" : null,
  "org.apache.hadoop.security.http.RestCsrfPreventionFilter:isBrowser(java.lang.String)" : "* This method interrogates the User-Agent String and returns whether it\n   * refers to a browser.  If its not a browser, then the requirement for the\n   * CSRF header will not be enforced; if it is a browser, the requirement will\n   * be enforced.\n   * <p>\n   * A User-Agent String is considered to be a browser if it matches\n   * any of the regex patterns from browser-useragent-regex; the default\n   * behavior is to consider everything a browser that matches the following:\n   * \"^Mozilla.*,^Opera.*\".  Subclasses can optionally override\n   * this method to use different behavior.\n   *\n   * @param userAgent The User-Agent String, or null if there isn't one\n   * @return true if the User-Agent String refers to a browser, false if not",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:toString()" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:<init>(java.lang.Object)" : "* Wrap an existing array of primitives\n   * @param value - array of primitives",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:deleteKey(java.lang.String)" : null,
  "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream)" : "* Create a line reader that reads from the given stream using the\n   * default buffer-size (64k).\n   * @param in The input stream",
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttribute(javax.management.Attribute)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getDelegationKey(int)" : "* Obtains the DelegationKey from the SQL database.\n   * @param keyId KeyId of the DelegationKey to obtain.\n   * @return DelegationKey that matches the given keyId or null\n   *         if it doesn't exist in the database.",
  "org.apache.hadoop.ipc.Client$Call:setAlignmentContext(org.apache.hadoop.ipc.AlignmentContext)" : "* Set an AlignmentContext for the call to update when call is done.\n     *\n     * @param ac alignment context to update.",
  "org.apache.hadoop.ipc.Server$ConnectionManager:getDroppedConnections()" : null,
  "org.apache.hadoop.http.HttpServer2$Builder:getPasswordString(org.apache.hadoop.conf.Configuration,java.lang.String)" : "* A wrapper of {@link Configuration#getPassword(String)}. It returns\n     * <code>String</code> instead of <code>char[]</code>.\n     *\n     * @param conf the configuration\n     * @param name the property name\n     * @return the password string or null",
  "org.apache.hadoop.service.AbstractService:getBlockers()" : null,
  "org.apache.hadoop.net.SocketIOWithTimeout:<init>(java.nio.channels.SelectableChannel,long)" : null,
  "org.apache.hadoop.util.bloom.Filter:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:decompress(byte[],int,int)" : null,
  "org.apache.hadoop.util.CrcUtil:toSingleCrcString(byte[])" : "* For use with debug statements; verifies bytes.length on creation,\n   * expecting it to represent exactly one CRC, and returns a hex\n   * formatted value.\n   *\n   * @param bytes bytes.\n   * @throws IOException raised on errors performing I/O.\n   * @return a list of hex formatted values.",
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read()" : null,
  "org.apache.hadoop.fs.shell.find.Find:getOptions()" : "Returns the current find options, creating them if necessary.",
  "org.apache.hadoop.http.HttpServer2$QuotingInputFilter:initHttpHeaderMap()" : null,
  "org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,long,long,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)" : "* Creates CompressionInputStream to be used to read off uncompressed data\n   * in one of the two reading modes. i.e. Continuous or Blocked reading modes\n   *\n   * @param seekableIn The InputStream\n   * @param start The start offset into the compressed stream\n   * @param end The end offset into the compressed stream\n   * @param readMode Controls whether progress is reported continuously or\n   *                 only at block boundaries.\n   *\n   * @return CompressionInputStream for BZip2 aligned at block boundaries",
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:invalidate()" : "* Marks the current position as invalid.",
  "org.apache.hadoop.util.FindClass:<init>(org.apache.hadoop.conf.Configuration)" : "* Create a class with a specified configuration\n   * @param conf configuration",
  "org.apache.hadoop.util.functional.LazyAtomicReference:lazyAtomicReferenceFromSupplier(java.util.function.Supplier)" : "* Create from a supplier.\n   * This is not a constructor to avoid ambiguity when a lambda-expression is\n   * passed in.\n   * @param supplier supplier implementation.\n   * @return a lazy reference.\n   * @param <T> type of reference",
  "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:addRecord(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.PathData:checkIfSchemeInferredFromPath(java.lang.String)" : "* Validates the given Windows path.\n   * @param pathString a String of the path supplied by the user.\n   * @return true if the URI scheme was not present in the pathString but\n   * inferred; false, otherwise.\n   * @throws IOException if anything goes wrong",
  "org.apache.hadoop.fs.FileStatus:setGroup(java.lang.String)" : "* Sets group.\n   * @param group if it is null, default value is set",
  "org.apache.hadoop.http.ProfilerDisabledServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read()" : null,
  "org.apache.hadoop.io.UTF8:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.crypto.CryptoProtocolVersion:getVersion()" : null,
  "org.apache.hadoop.fs.permission.ScopedAclEntries:getAccessEntries()" : "* Returns access entries.\n   *\n   * @return List&lt;AclEntry&gt; containing just access entries, or an empty\n   * list if there are no access entries",
  "org.apache.hadoop.conf.Configuration:readTagFromConfig(java.lang.String,java.lang.String,java.lang.String,java.lang.String[])" : "* Read the values passed as tags and store them in a\n   * map for later retrieval.\n   * @param attributeValue\n   * @param confName\n   * @param confValue\n   * @param confSource",
  "org.apache.hadoop.io.SequenceFile$Sorter$SortPass:setProgressable(org.apache.hadoop.util.Progressable)" : "set the progressable object in order to report progress",
  "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getPermission()" : null,
  "org.apache.hadoop.fs.DUHelper:getFileSize(java.io.File)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:<init>(org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator)" : "* For writing to file.",
  "org.apache.hadoop.util.ExitUtil:terminate(int)" : "* Like {@link #terminate(int, Throwable)} without a message.\n   *\n   * @param status exit code\n   * @throws ExitException if {@link System#exit(int)} is disabled.",
  "org.apache.hadoop.metrics2.util.SampleStat:stddev()" : "* @return  the standard deviation of the samples",
  "org.apache.hadoop.net.AbstractDNSToSwitchMapping:dumpTopology()" : "* Generate a string listing the switch mapping implementation,\n   * the mapping for every known node and the number of nodes and\n   * unique switches known about -each entry to a separate line.\n   * @return a string that can be presented to the ops team or used in\n   * debug messages.",
  "org.apache.hadoop.ipc.ProtobufRpcEngine2:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)" : null,
  "org.apache.hadoop.fs.shell.CommandFormat:<init>(int,int,java.lang.String[])" : "* Simple parsing of command line arguments\n   * @param min minimum arguments required\n   * @param max maximum arguments permitted\n   * @param possibleOpt list of the allowed switches",
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reset()" : null,
  "org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:handle(javax.security.auth.callback.Callback[])" : null,
  "org.apache.hadoop.metrics2.util.MBeans:getMBeanName(java.lang.String,java.lang.String,java.util.Map)" : null,
  "org.apache.hadoop.fs.FileContext:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : "* Delete a snapshot of a directory.\n   *\n   * @param path The directory that the to-be-deleted snapshot belongs to\n   * @param snapshotName The name of the snapshot\n   *\n   * @throws IOException If an I/O error occurred\n   *\n   * <p>Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws\n   *           undeclared exception to RPC server",
  "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:serializeToString()" : null,
  "org.apache.hadoop.util.ExitUtil$HaltException:<init>(int,java.lang.String)" : null,
  "org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,byte[])" : "* Create a line reader that reads from the given stream using the\n   * default buffer-size, and using a custom delimiter of array of\n   * bytes.\n   * @param in The input stream\n   * @param recordDelimiterBytes The delimiter",
  "org.apache.hadoop.io.MapFile$Writer:valueClass(java.lang.Class)" : null,
  "org.apache.hadoop.fs.Globber:createGlobber(org.apache.hadoop.fs.FileContext)" : "* Create a builder for a Globber, bonded to the specific file\n   * context.\n   * @param fileContext file context.\n   * @return the builder to finish configuring.",
  "org.apache.hadoop.fs.FileChecksum:hashCode()" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getModificationTime()" : null,
  "org.apache.hadoop.fs.shell.Truncate:processArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.ipc.RPC$Builder:setVerbose(boolean)" : "* @return Default: false.\n     * @param verbose input verbose.",
  "org.apache.hadoop.util.JvmPauseMonitor:getNumGcInfoThresholdExceeded()" : null,
  "org.apache.hadoop.io.SequenceFile$CompressedBytes:writeUncompressedBytes(java.io.DataOutputStream)" : null,
  "org.apache.hadoop.fs.FileSystem:listStatus(java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)" : "* Filter files/directories in the given path using the user-supplied path\n   * filter. Results are added to the given array <code>results</code>.\n   * @throws FileNotFoundException when the path does not exist\n   * @throws IOException see specific implementation",
  "org.apache.hadoop.io.MD5Hash:set(org.apache.hadoop.io.MD5Hash)" : "* Copy the contents of another instance into this instance.\n   * @param that that.",
  "org.apache.hadoop.fs.statistics.MeanStatistic:setSum(long)" : "* Set the sum.\n   * @param sum new sum",
  "org.apache.hadoop.fs.impl.FutureIOSupport:<init>()" : null,
  "org.apache.hadoop.security.alias.CredentialShell$ListCommand:validate()" : null,
  "org.apache.hadoop.security.User:getName()" : "* Get the full name of the user.",
  "org.apache.hadoop.io.WritableComparator:getKeyClass()" : "* Returns the WritableComparable implementation class.\n   * @return WritableComparable.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.io.compress.ZStandardCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)" : "* Create a {@link CompressionInputStream} that will read from the given\n   * {@link InputStream} with the given {@link Decompressor}.\n   *\n   * @param in           the stream to read compressed bytes from\n   * @param decompressor decompressor to use\n   * @return a stream to read uncompressed bytes from\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.erasurecode.ErasureCodecOptions:getSchema()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:release()" : null,
  "org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:<init>()" : "* Basic exception handler -logs simple exceptions, then continues.",
  "org.apache.hadoop.security.token.DtUtilShell$Edit:execute()" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaMetricVisitor:gauge(org.apache.hadoop.metrics2.MetricsInfo,double)" : null,
  "org.apache.hadoop.fs.FilterFs:setVerifyChecksum(boolean)" : null,
  "org.apache.hadoop.ipc.Client:isAsynchronousMode()" : "* Check if RPC is in asynchronous mode or not.\n   *\n   * @return true, if RPC is in asynchronous mode, otherwise false for\n   *          synchronous mode.",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX:getStat(java.lang.String)" : "* Return the file stat for a file path.\n     *\n     * @param path  file path\n     * @return  the file stat\n     * @throws IOException  thrown if there is an IO error while obtaining the\n     * file stat",
  "org.apache.hadoop.io.ArrayPrimitiveWritable:writeLongArray(java.io.DataOutput)" : null,
  "org.apache.hadoop.io.ArrayPrimitiveWritable:checkArray(java.lang.Object)" : null,
  "org.apache.hadoop.security.authorize.AccessControlList:getGroupsString()" : "* Returns comma-separated concatenated single String of the set 'groups'\n   *\n   * @return comma separated list of groups",
  "org.apache.hadoop.net.unix.DomainSocket:shutdown()" : "* Call shutdown(SHUT_RDWR) on the UNIX domain socket.\n   *\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.PassthroughCodec:getCompressorType()" : null,
  "org.apache.hadoop.util.Shell$ShellTimeoutTimerTask:run()" : null,
  "org.apache.hadoop.ipc.Client$Connection$RpcRequestSender:run()" : null,
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:<init>(java.io.InputStream)" : null,
  "org.apache.hadoop.io.UTF8:set(java.lang.String)" : "* Set to contain the contents of a string.\n   * @param string input string.",
  "org.apache.hadoop.security.token.DtFileOperations:matchService(org.apache.hadoop.security.token.DtFetcher,org.apache.hadoop.io.Text,java.lang.String)" : "Match fetcher's service name to the service text and/or url prefix.",
  "org.apache.hadoop.fs.FileSystem:getInitialWorkingDirectory()" : "* Note: with the new FileContext class, getWorkingDirectory()\n   * will be removed.\n   * The working directory is implemented in FileContext.\n   *\n   * Some FileSystems like LocalFileSystem have an initial workingDir\n   * that we use as the starting workingDir. For other file systems\n   * like HDFS there is no built in notion of an initial workingDir.\n   *\n   * @return if there is built in notion of workingDir then it\n   * is returned; else a null is returned.",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX:assertCodeLoaded()" : null,
  "org.apache.hadoop.fs.FileStatus:isFile()" : "* Is this a file?\n   * @return true if this is a file",
  "org.apache.hadoop.ipc.RPC:<init>()" : null,
  "org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)" : "Get a path from the local FS for reading. We search through all the\n     *  configured dirs for the file's existence and return the complete\n     *  path to the file when we find one",
  "org.apache.hadoop.ipc.Server:isServerFailOverEnabled()" : null,
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getDelegationTokenSeqNum()" : "* Obtains the value of the last reserved sequence number.\n   * @return Last reserved sequence number.",
  "org.apache.hadoop.ipc.ProtocolSignature$ProtocolSigFingerprint:<init>(org.apache.hadoop.ipc.ProtocolSignature,int)" : null,
  "org.apache.hadoop.fs.HarFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ipc.Server$Listener:setIsAuxiliary()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockData:isLastBlock(int)" : "* Indicates whether the given block is the last block in the associated file.\n   * @param blockNumber the id of the desired block.\n   * @return true if the given block is the last block in the associated file, false otherwise.\n   * @throws IllegalArgumentException if blockNumber is invalid.",
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:fileSystem_openFile_available()" : "* Is the {@link #fileSystem_openFile(FileSystem, Path, String, FileStatus, Long, Map)}\n   * method available.\n   * @return true if the optimized open file method can be invoked.",
  "org.apache.hadoop.security.KDiag:error(java.lang.String,java.lang.String,java.lang.Object[])" : "* Print a message as an error\n   * @param category error category\n   * @param message format string\n   * @param args list of arguments",
  "org.apache.hadoop.io.nativeio.NativeIO$Windows$AccessRight:accessRight()" : null,
  "org.apache.hadoop.ipc.Server:updateMetrics(org.apache.hadoop.ipc.Server$Call,long,boolean)" : null,
  "org.apache.hadoop.util.DataChecksum:reset()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:augmentURL(java.net.URL,java.util.Map)" : null,
  "org.apache.hadoop.metrics2.lib.MutableStat:add(long,long)" : "* Add a number of samples and their sum to the running stat\n   *\n   * Note that although use of this method will preserve accurate mean values,\n   * large values for numSamples may result in inaccurate variance values due\n   * to the use of a single step of the Welford method for variance calculation.\n   * @param numSamples  number of samples\n   * @param sum of the samples",
  "org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationSerializer:serialize(java.io.Serializable)" : null,
  "org.apache.hadoop.crypto.key.CachingKeyProvider:invalidateCache(java.lang.String)" : null,
  "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:<init>()" : null,
  "org.apache.hadoop.io.IntWritable:<init>()" : null,
  "org.apache.hadoop.security.alias.CredentialShell$Command:doHelp()" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:close()" : "* Signaling the end of write to the block. The block register will be\n       * called for registering the finished block.",
  "org.apache.hadoop.ipc.RefreshResponse:successResponse()" : "* Convenience method to create a response for successful refreshes.\n   * @return void response",
  "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:toString()" : null,
  "org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],long,long,boolean)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:<init>(java.lang.Class,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)" : "* This constructor takes a connectionId, instead of creating a new one.\n     *\n     * @param protocol input protocol.\n     * @param connId input connId.\n     * @param conf input Configuration.\n     * @param factory input factory.\n     * @param alignmentContext Alignment context",
  "org.apache.hadoop.crypto.JceCtrCryptoCodec:getConf()" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData:getReadOps()" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:hashCode()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:trackDuration(java.lang.String,long)" : "* If the store is tracking the given key, return the\n   * duration tracker for it. If not tracked, return the\n   * stub tracker.\n   * @param key statistic key prefix\n   * @param count  #of times to increment the matching counter in this\n   * operation.\n   * @return a tracker.",
  "org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.tracing.Tracer:activateSpan(org.apache.hadoop.tracing.Span)" : null,
  "org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)" : "* Constructor with PathHandle.\n   * @param fileSystem owner FS.\n   * @param pathHandle path handle",
  "org.apache.hadoop.security.SecurityUtil:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Reader:getMetaBlock(java.lang.String)" : "* Stream access to a Meta Block.\n     * \n     * @param name\n     *          meta block name\n     * @return BlockReader input stream for reading the meta block.\n     * @throws IOException\n     * @throws MetaBlockDoesNotExist\n     *           The Meta Block with the given name does not exist.",
  "org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:withEvaluated(java.lang.String,java.util.function.Supplier)" : "* Add an evaluated attribute to the current map.\n     * Replaces any with the existing key.\n     * Set evaluated methods.\n     * @param key key\n     * @param value new value\n     * @return the builder",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:getCodecName()" : null,
  "org.apache.hadoop.security.SaslPlainServer:throwIfNotComplete()" : null,
  "org.apache.hadoop.util.ShutdownThreadsHelper:shutdownExecutorService(java.util.concurrent.ExecutorService)" : "* shutdownExecutorService.\n   *\n   * @param service {@link ExecutorService to be shutdown}\n   * @return <tt>true</tt> if the service is terminated,\n   * <tt>false</tt> otherwise\n   * @throws InterruptedException if the thread is interrupted.",
  "org.apache.hadoop.fs.shell.PathData:checkIfExists(org.apache.hadoop.fs.shell.PathData$FileTypeRequirement)" : "* Ensure that the file exists and if it is or is not a directory\n   * @param typeRequirement Set it to the desired requirement.\n   * @throws PathIOException if file doesn't exist or the type does not match\n   * what was specified in typeRequirement.",
  "org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:setDelegate(org.apache.hadoop.ipc.FairCallQueue)" : null,
  "org.apache.hadoop.crypto.UnsupportedCodecException:<init>(java.lang.String,java.lang.Throwable)" : "* Constructs a new exception with the specified detail message and\n   * cause.\n   * \n   * @param message the detail message\n   * @param cause the cause",
  "org.apache.hadoop.fs.Trash:getCurrentTrashDir()" : "* get the current working directory.\n   *\n   * @throws IOException on raised on errors performing I/O.\n   * @return Trash Dir.",
  "org.apache.hadoop.fs.shell.find.FilterExpression:setOptions(org.apache.hadoop.fs.shell.find.FindOptions)" : null,
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>()" : "Default constructor - use with readFields",
  "org.apache.hadoop.fs.shell.CommandFormat$IllegalNumberOfArgumentsException:<init>(int,int)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree:createLink(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$LinkType,java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.ipc.Server$ExceptionsHandler:isSuppressedLog(java.lang.Class)" : null,
  "org.apache.hadoop.metrics2.util.MetricsCache$Record:getMetric(java.lang.String)" : "* Lookup a metric value\n     * @param key name of the metric\n     * @return the metric value",
  "org.apache.hadoop.fs.shell.Command:getUsage()" : "* The short usage suitable for the synopsis\n   * @return \"name options\"",
  "org.apache.hadoop.util.LightWeightGSet:remove(int,java.lang.Object)" : "* Remove the element corresponding to the key,\n   * given key.hashCode() == index.\n   *\n   * @param key key.\n   * @param index index.\n   * @return If such element exists, return it.\n   *         Otherwise, return null.",
  "org.apache.hadoop.fs.FileSystem$Statistics:reset()" : "* Resets all statistics to 0.\n     *\n     * In order to reset, we add up all the thread-local statistics data, and\n     * set rootData to the negative of that.\n     *\n     * This may seem like a counterintuitive way to reset the statistics.  Why\n     * can't we just zero out all the thread-local data?  Well, thread-local\n     * data can only be modified by the thread that owns it.  If we tried to\n     * modify the thread-local data from this thread, our modification might get\n     * interleaved with a read-modify-write operation done by the thread that\n     * owns the data.  That would result in our update getting lost.\n     *\n     * The approach used here avoids this problem because it only ever reads\n     * (not writes) the thread-local data.  Both reads and writes to rootData\n     * are done under the lock, so we're free to modify rootData from any thread\n     * that holds the lock.",
  "org.apache.hadoop.util.PrintJarMainClass:main(java.lang.String[])" : "* @param args args.",
  "org.apache.hadoop.io.compress.CompressionOutputStream:setTrackedCompressor(org.apache.hadoop.io.compress.Compressor)" : null,
  "org.apache.hadoop.http.HttpServer2:join()" : null,
  "org.apache.hadoop.ipc.CallQueueManager:isServerFailOverEnabledByQueue()" : null,
  "org.apache.hadoop.security.Credentials:getToken(org.apache.hadoop.io.Text)" : "* Returns the Token object for the alias.\n   * @param alias the alias for the Token\n   * @return token for this alias",
  "org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler:handle(org.apache.hadoop.net.unix.DomainSocket)" : null,
  "org.apache.hadoop.io.compress.Lz4Codec:getConf()" : "* Return the configuration used by this object.\n   *\n   * @return the configuration object used by this objec.",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:getTag(java.lang.String)" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:init(org.apache.commons.configuration2.SubsetConfiguration)" : null,
  "org.apache.hadoop.ipc.RPC$Server$ProtoNameVer:hashCode()" : null,
  "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:<init>(org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolPB)" : null,
  "org.apache.hadoop.conf.Configuration:logDeprecation(java.lang.String)" : null,
  "org.apache.hadoop.fs.HarFileSystem:getDefaultReplication()" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,int)" : "* Create a mutable integer gauge\n   * @param name  of the metric\n   * @param desc  metric description\n   * @param iVal  initial value\n   * @return a new gauge object",
  "org.apache.hadoop.util.Shell:getSetPermissionCommand(java.lang.String,boolean,java.lang.String)" : "* Return a command to set permission for specific file.\n   *\n   * @param perm String permission to set\n   * @param recursive boolean true to apply to all sub-directories recursively\n   * @param file String file to set\n   * @return String[] containing command and arguments",
  "org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolSignature(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto)" : null,
  "org.apache.hadoop.fs.shell.CopyCommands$Put:processArguments(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.statistics.DurationStatisticSummary:<init>(java.lang.String,boolean,long,long,long,org.apache.hadoop.fs.statistics.MeanStatistic)" : "* Constructor.\n   * @param key Statistic key.\n   * @param success Are these success or failure statistics.\n   * @param count Count of operation invocations.\n   * @param max Max duration; -1 if unknown.\n   * @param min Min duration; -1 if unknown.\n   * @param mean Mean duration -may be null. (will be cloned)",
  "org.apache.hadoop.util.OperationDuration:<init>()" : "* Instantiate.\n   * The start time and finished time are both set\n   * to the current clock time.",
  "org.apache.hadoop.fs.FileSystem$Cache:get(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:writeStreamToFile(java.io.InputStream,org.apache.hadoop.fs.shell.PathData,boolean,boolean)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterState(org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState,org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState)" : "* Atomically enter a state, verifying current state.\n     *\n     * @param current current state. null means \"no check\"\n     * @param next    next state\n     * @throws IllegalStateException if the current state is not as expected",
  "org.apache.hadoop.ipc.Client$ConnectionId:getProtocol()" : null,
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:finish()" : null,
  "org.apache.hadoop.security.SaslOutputStream:<init>(java.io.OutputStream,javax.security.sasl.SaslClient)" : "* Constructs a SASLOutputStream from an OutputStream and a SaslClient <br>\n   * Note: if the specified OutputStream or SaslClient is null, a\n   * NullPointerException may be thrown later when they are used.\n   * \n   * @param outStream\n   *          the OutputStream to be processed\n   * @param saslClient\n   *          an initialized SaslClient object",
  "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:flush()" : "* Flush operation will flush to disk.\n     *\n     * @throws IOException IOE raised on FileOutputStream",
  "org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:convertToByteBufferState()" : "* Convert to a ByteBufferDecodingState when it's backed by on-heap arrays.",
  "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:reset()" : null,
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addTimedOperation(java.lang.String,java.time.Duration)" : null,
  "org.apache.hadoop.net.NetUtils:createSocketAddrForHost(java.lang.String,int)" : "* Create a socket address with the given host and port.  The hostname\n   * might be replaced with another host that was set via\n   * {@link #addStaticResolution(String, String)}.  The value of\n   * hadoop.security.token.service.use_ip will determine whether the\n   * standard java host resolver is used, or if the fully qualified resolver\n   * is used.\n   * @param host the hostname or IP use to instantiate the object\n   * @param port the port number\n   * @return InetSocketAddress",
  "org.apache.hadoop.service.launcher.AbstractLaunchableService:<init>(java.lang.String)" : "* Construct an instance with the given name.\n   *\n   * @param name input name.",
  "org.apache.hadoop.fs.FilterFs:getAllStoragePolicies()" : null,
  "org.apache.hadoop.fs.FileContext:listLocatedStatus(org.apache.hadoop.fs.Path)" : "* List the statuses of the files/directories in the given path if the path is\n   * a directory. \n   * Return the file's status and block locations If the path is a file.\n   * \n   * If a returned status is a file, it contains the file's block locations.\n   *\n   * @param f is the path\n   *\n   * @return an iterator that traverses statuses of the files/directories \n   *         in the given path\n   * If any IO exception (for example the input directory gets deleted while\n   * listing is being executed), next() or hasNext() of the returned iterator\n   * may throw a RuntimeException with the io exception as the cause.\n   *\n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If <code>f</code> does not exist\n   * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n   *           not supported\n   * @throws IOException If an I/O error occurred\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server",
  "org.apache.hadoop.ipc.CallQueueManager:createScheduler(java.lang.Class,int,java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.DataInputBuffer:getLength()" : "* Returns the index one greater than the last valid character in the input\n   * stream buffer.\n   *\n   * @return length.",
  "org.apache.hadoop.util.InstrumentedLock:newCondition()" : null,
  "org.apache.hadoop.io.file.tfile.TFileDumper$Align:format(long,int,org.apache.hadoop.io.file.tfile.TFileDumper$Align)" : null,
  "org.apache.hadoop.fs.permission.AclEntry$Builder:setType(org.apache.hadoop.fs.permission.AclEntryType)" : "* Sets the ACL entry type.\n     *\n     * @param type AclEntryType ACL entry type\n     * @return Builder this builder, for call chaining",
  "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getGroup()" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:destroy()" : null,
  "org.apache.hadoop.fs.impl.FlagSet:<init>(java.lang.Class,java.lang.String,java.util.EnumSet)" : "* Create a FlagSet.\n   * @param enumClass class of enum\n   * @param prefix prefix (with trailing \".\") for path capabilities probe\n   * @param flags flags. A copy of these are made.",
  "org.apache.hadoop.fs.FsShellPermissions$Chmod:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.ipc.RetryCache:skipRetryCache(byte[],int)" : null,
  "org.apache.hadoop.ipc.Server:shutdownMetricsUpdaterExecutor()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getProcessedByteCount()" : "* This method reports the processed bytes so far. Please note that this\n  * statistic is only updated on block boundaries and only when the stream is\n  * initiated in BYBLOCK mode.\n  * @return ProcessedByteCount.",
  "org.apache.hadoop.metrics2.impl.MetricsConfig:toString(org.apache.commons.configuration2.Configuration)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:setVerifyChecksum(boolean)" : "* Set whether to verify checksum.",
  "org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:release()" : null,
  "org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:remainingCapacity()" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,long)" : null,
  "org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Writer:<init>(org.apache.hadoop.fs.FSDataOutputStream,java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Constructor\n     * \n     * @param fout\n     *          FS output stream.\n     * @param compressionName\n     *          Name of the compression algorithm, which will be used for all\n     *          data blocks.\n     * @throws IOException\n     * @see Compression#getSupportedAlgorithms",
  "org.apache.hadoop.service.ServiceStateModel:ensureCurrentState(org.apache.hadoop.service.Service$STATE)" : "* Verify that that a service is in a given state.\n   * @param expectedState the desired state\n   * @throws ServiceStateException if the service state is different from\n   * the desired state",
  "org.apache.hadoop.fs.impl.OpenFileParameters:withOptionalKeys(java.util.Set)" : null,
  "org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)" : "* Add a LinkMergeSlash to the config for the specified mount table.\n   *\n   * @param conf configuration.\n   * @param mountTableName mountTable.\n   * @param target target.",
  "org.apache.hadoop.io.DataInputBuffer$Buffer:<init>()" : null,
  "org.apache.hadoop.io.erasurecode.ErasureCodeConstants:<init>()" : null,
  "org.apache.hadoop.ha.HAAdmin:transitionToStandby(org.apache.commons.cli.CommandLine)" : null,
  "org.apache.hadoop.io.file.tfile.Utils:upperBound(java.util.List,java.lang.Object,java.util.Comparator)" : "* Upper bound binary search. Find the index to the first element in the list\n   * that compares greater than the input key.\n   * \n   * @param <T>\n   *          Type of the input key.\n   * @param list\n   *          The list\n   * @param key\n   *          The input key.\n   * @param cmp\n   *          Comparator for the key.\n   * @return The index to the desired element if it exists; or list.size()\n   *         otherwise.",
  "org.apache.hadoop.util.curator.ZKCuratorManager$SafeTransaction:delete(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.DecompressorStream:checkStream()" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addDir(java.lang.String,org.apache.hadoop.security.UserGroupInformation)" : null,
  "org.apache.hadoop.util.ComparableVersion$ListItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item)" : null,
  "org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.security.token.DtUtilShell$Get:validate()" : null,
  "org.apache.hadoop.metrics2.lib.MutableRollingAverages:close()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getAccessTime()" : null,
  "org.apache.hadoop.security.authorize.ServiceAuthorizationManager:getProtocolsBlockedMachineList(java.lang.Class)" : null,
  "org.apache.hadoop.fs.BufferedFSInputStream:skip(long)" : null,
  "org.apache.hadoop.fs.FSOutputSummer:getBufferedDataSize()" : "* Return the number of valid bytes currently in the buffer.\n   *\n   * @return buffer data size.",
  "org.apache.hadoop.io.nativeio.NativeIO:link(java.io.File,java.io.File)" : "* Creates a hardlink \"dst\" that points to \"src\".\n   *\n   * This is deprecated since JDK7 NIO can create hardlinks via the\n   * {@link java.nio.file.Files} API.\n   *\n   * @param src source file\n   * @param dst hardlink location\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:hashCode()" : null,
  "org.apache.hadoop.util.XMLUtils:newSecureTransformerFactory()" : "* This method should be used if you need a {@link TransformerFactory}. Use this method\n   * instead of {@link TransformerFactory#newInstance()}. The factory that is returned has\n   * secure configuration enabled.\n   *\n   * @return a {@link TransformerFactory} with secure configuration enabled\n   * @throws TransformerConfigurationException if the {@code JAXP} transformer does not\n   * support the secure configuration",
  "org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)" : "* Cancels a token by removing it from the SQL database. This will\n   * call the corresponding method in {@link AbstractDelegationTokenSecretManager}\n   * to perform validation and remove the token from the cache.\n   * @return Identifier of the canceled token",
  "org.apache.hadoop.security.authorize.Service:getServiceKey()" : "* Get the configuration key for the service.\n   * @return the configuration key for the service",
  "org.apache.hadoop.conf.Configuration$IntegerRanges:<init>(java.lang.String)" : null,
  "org.apache.hadoop.util.WeakReferenceMap:remove(java.lang.Object)" : "* Remove any value under the key.\n   * @param key key\n   * @return any old non-null reference.",
  "org.apache.hadoop.io.AbstractMapWritable:getClass(byte)" : "* the Class class for the specified id.\n   * @param id id.\n   * @return the Class class for the specified id.",
  "org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:validate()" : null,
  "org.apache.hadoop.metrics2.util.SampleStat:numSamples()" : "* @return  the total number of samples",
  "org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:<init>(org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolPB)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:close()" : null,
  "org.apache.hadoop.fs.sftp.SFTPConnectionPool:disconnect(com.jcraft.jsch.ChannelSftp)" : null,
  "org.apache.hadoop.util.functional.FutureIO:cancelAllFuturesAndAwaitCompletion(java.util.Collection,boolean,java.time.Duration)" : "* Cancels a collection of futures and awaits the specified duration for their completion.\n   * <p>\n   * This method blocks until all futures in the collection have completed or\n   * the timeout expires, whichever happens first.\n   * All exceptions thrown by the futures are ignored. as is any TimeoutException.\n   * @param collection collection of futures to be evaluated\n   * @param interruptIfRunning should the cancel interrupt any active futures?\n   * @param duration total timeout duration\n   * @param <T> type of the result.\n   * @return all futures which completed successfully.",
  "org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:interceptResolvedDestPathStr(java.lang.String)" : "* Intercept resolved path, e.g.\n   * Mount point /^(\\\\w+)/, ${1}.hadoop.net\n   * If incoming path is /user1/home/tmp/job1,\n   * then the resolved path str will be user1.\n   *\n   * @return intercepted string",
  "org.apache.hadoop.util.InstrumentedWriteLock:unlock()" : null,
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm$3:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getUsed(org.apache.hadoop.fs.Path)" : "Return the total size of all files from a specified path.",
  "org.apache.hadoop.util.Shell:execCommand(java.lang.String[])" : "* Static method to execute a shell command.\n   * Covers most of the simple cases without requiring the user to implement\n   * the <code>Shell</code> interface.\n   * @param cmd shell command to execute.\n   * @return the output of the executed command.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:accept(java.lang.Class)" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:dispatched()" : null,
  "org.apache.hadoop.ipc.Client$IpcStreams:setInputStream(java.io.InputStream)" : null,
  "org.apache.hadoop.io.WritableComparator:compare(java.lang.Object,java.lang.Object)" : "* Compare two Object.\n   *\n   * @param a the first object to be compared.\n   * @param b the second object to be compared.\n   * @return compare result.",
  "org.apache.hadoop.security.KDiag:<init>()" : null,
  "org.apache.hadoop.util.IdentityHashStore:visitAll(org.apache.hadoop.util.IdentityHashStore$Visitor)" : "* Visit all key, value pairs in the IdentityHashStore.\n   *\n   * @param visitor visitor.",
  "org.apache.hadoop.io.file.tfile.TFile$Writer$KeyRegister:close()" : null,
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:formatTokenId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier)" : null,
  "org.apache.hadoop.ha.ZKFailoverController:scheduleRecheck(long)" : "* Schedule a call to {@link #recheckElectability()} in the future.",
  "org.apache.hadoop.io.WritableUtils:skipFully(java.io.DataInput,int)" : "* Skip <i>len</i> number of bytes in input stream<i>in</i>\n   * @param in input stream\n   * @param len number of bytes to skip\n   * @throws IOException when skipped less number of bytes",
  "org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)" : "* Send callback and return whether or not the domain socket was closed as a\n   * result of processing.\n   *\n   * @param caller reason for call\n   * @param entries mapping of file descriptor to entry\n   * @param fdSet set of file descriptors\n   * @param fd file descriptor\n   * @return true if the domain socket was closed as a result of processing",
  "org.apache.hadoop.util.bloom.Key:<init>(byte[],double)" : "* Constructor.\n   * <p>\n   * Builds a key with a specified weight.\n   * @param value The value of <i>this</i> key.\n   * @param weight The weight associated to <i>this</i> key.",
  "org.apache.hadoop.io.erasurecode.CodecRegistry:getCoderByName(java.lang.String,java.lang.String)" : "* Get a specific coder factory defined by codec name and coder name.\n   * @param codecName name of the codec\n   * @param coderName name of the coder\n   * @return the specific coder, null if not exist",
  "org.apache.hadoop.metrics2.impl.SinkQueue:consume(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer)" : "* Consume one element, will block if queue is empty\n   * Only one consumer at a time is allowed\n   * @param consumer  the consumer callback object",
  "org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable,boolean)" : "* Append to an existing file (optional operation).\n   * This function is used for being overridden by some FileSystem like DistributedFileSystem\n   * @param f the existing file to be appended.\n   * @param bufferSize the size of the buffer to be used.\n   * @param progress for reporting progress if it is not null.\n   * @param appendToNewBlock whether to append data to a new block\n   * instead of the end of the last partial block\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default).\n   * @return output stream.",
  "org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:acquire()" : "* Acquires a resource blocking if necessary until one becomes available.",
  "org.apache.hadoop.ipc.Server:getAuxiliaryPortEstablishedQOP()" : "* Returns the SASL qop for the current call, if the current call is\n   * set, and the SASL negotiation is done. Otherwise return null\n   * Note this only returns established QOP for auxiliary port, and\n   * returns null for primary (non-auxiliary) port.\n   *\n   * Also note that CurCall is thread local object. So in fact, different\n   * handler threads will process different CurCall object.\n   *\n   * Also, only return for RPC calls, not supported for other protocols.\n   * @return the QOP of the current connection.",
  "org.apache.hadoop.util.bloom.BloomFilter:or(org.apache.hadoop.util.bloom.Filter)" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockManager:getBlockData()" : "* Gets block data information.\n   *\n   * @return instance of {@code BlockData}.",
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:createLogFile(org.apache.hadoop.fs.Path)" : "* Create a new log file and return the {@link FSDataOutputStream}. If a\n   * file with the specified path already exists, add a suffix, starting with 1\n   * and try again. Keep incrementing the suffix until a nonexistent target\n   * path is found.\n   *\n   * Once the file is open, update {@link #currentFSOutStream},\n   * {@link #currentOutStream}, and {@#link #currentFilePath} are set\n   * appropriately.\n   *\n   * @param initial the target path\n   * @throws IOException thrown if the call to see if the exists fails",
  "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : null,
  "org.apache.hadoop.conf.Configuration:dumpDeprecatedKeys()" : null,
  "org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:calculateCoding(org.apache.hadoop.io.erasurecode.ECBlockGroup)" : null,
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:relative()" : "* Gets the current position within this file relative to the start of the associated buffer.\n   *\n   * @return the current position within this file relative to the start of the associated buffer.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:set(int,long)" : null,
  "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)" : "* Create an {@link FSDataOutputStream} at the indicated Path\n   * with write-progress reporting.\n   *\n   * The frequency of callbacks is implementation-specific; it may be \"none\".\n   * @param f the path of the file to open\n   * @param overwrite if a file with this name already exists, then if true,\n   *   the file will be overwritten, and if false an error will be thrown.\n   * @param bufferSize the size of the buffer to be used.\n   * @param progress to report progress.\n   * @throws IOException IO failure\n   * @return output stream.",
  "org.apache.hadoop.security.UserGroupInformation:getGroups()" : "* Get the group names for this user. {@link #getGroupsSet()} is less\n   * expensive alternative when checking for a contained element.\n   * @return the list of users with the primary group first. If the command\n   *    fails, it returns an empty list.\n   * @deprecated Use {@link #getGroupsSet()} instead.",
  "org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])" : null,
  "org.apache.hadoop.util.LightWeightResizableGSet:get(java.lang.Object)" : null,
  "org.apache.hadoop.io.MapFile$Reader:close()" : "* Close the map.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.GzipCodec:getDecompressorType()" : null,
  "org.apache.hadoop.security.alias.CredentialShell$ListCommand:execute()" : null,
  "org.apache.hadoop.fs.FsShell:printHelp(java.io.PrintStream,java.lang.String)" : null,
  "org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream)" : "* Allow derived classes to directly set the underlying stream.\n   * \n   * @param in Underlying input stream.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerPutPart(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long)" : null,
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaConf:getDmax()" : "* @return the dmax",
  "org.apache.hadoop.security.ssl.ReloadingX509TrustManager:checkServerTrusted(java.security.cert.X509Certificate[],java.lang.String)" : null,
  "org.apache.hadoop.security.token.DtFileOperations:doFormattedWrite(java.io.File,java.lang.String,org.apache.hadoop.security.Credentials,org.apache.hadoop.conf.Configuration)" : "Write out a Credentials object as a local file.\n   *  @param f a local File object.\n   *  @param format a string equal to FORMAT_PB or FORMAT_JAVA.\n   *  @param creds the Credentials object to be written out.\n   *  @param conf a Configuration object passed along.\n   *  @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ha.ActiveStandbyElector:deleteWithRetries(java.lang.String,int)" : null,
  "org.apache.hadoop.util.JsonSerialization:fromResource(java.lang.String)" : "* Convert from a JSON file.\n   * @param resource input file\n   * @return the parsed JSON\n   * @throws IOException IO problems\n   * @throws JsonParseException If the input is not well-formatted\n   * @throws JsonMappingException failure to map from the JSON to this class",
  "org.apache.hadoop.fs.shell.Delete:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.metrics2.util.SampleStat$MinMax:reset()" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:becomeStandby()" : null,
  "org.apache.hadoop.ipc.Client$Connection:shouldAuthenticateOverKrb()" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$Windows:createFileOutputStreamWithMode(java.io.File,boolean,int)" : "* Create a file for write with permissions set to the specified mode.  By\n     * setting permissions at creation time, we avoid issues related to the user\n     * lacking WRITE_DAC rights on subsequent chmod calls.  One example where\n     * this can occur is writing to an SMB share where the user does not have\n     * Full Control rights, and therefore WRITE_DAC is denied.\n     *\n     * This method mimics the semantics implemented by the JDK in\n     * {@link java.io.FileOutputStream}.  The file is opened for truncate or\n     * append, the sharing mode allows other readers and writers, and paths\n     * longer than MAX_PATH are supported.  (See io_util_md.c in the JDK.)\n     *\n     * @param path file to create\n     * @param append if true, then open file for append\n     * @param mode permissions of new directory\n     * @return FileOutputStream of opened file\n     * @throws IOException if there is an I/O error",
  "org.apache.hadoop.fs.FileSystem:getName()" : "* @return uri to string.\n   * @deprecated call {@link #getUri()} instead.",
  "org.apache.hadoop.security.SaslRpcServer$SaslGssCallbackHandler:handle(javax.security.auth.callback.Callback[])" : null,
  "org.apache.hadoop.http.ProfileServlet:getOutput(javax.servlet.http.HttpServletRequest)" : null,
  "org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile(boolean)" : "* Read /proc/meminfo, parse and compute memory information.\n   * @param readAgain if false, read only on the first time",
  "org.apache.hadoop.fs.VectoredReadUtils:readRangeFrom(org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.FileRange,java.util.function.IntFunction)" : "* Synchronously reads a range from the stream dealing with the combinations\n   * of ByteBuffers buffers and PositionedReadable streams.\n   * @param stream the stream to read from\n   * @param range the range to read\n   * @param allocate the function to allocate ByteBuffers\n   * @return the CompletableFuture that contains the read data or an exception.\n   * @throws IllegalArgumentException the range is invalid other than by offset or being null.\n   * @throws EOFException the range offset is negative\n   * @throws NullPointerException if the range is null.",
  "org.apache.hadoop.security.token.SecretManager:retriableRetrievePassword(org.apache.hadoop.security.token.TokenIdentifier)" : "* The same functionality with {@link #retrievePassword}, except that this \n   * method can throw a {@link RetriableException} or a {@link StandbyException}\n   * to indicate that client can retry/failover the same operation because of \n   * temporary issue on the server side.\n   * \n   * @param identifier the identifier to validate\n   * @return the password to use\n   * @throws InvalidToken the token was invalid\n   * @throws StandbyException the server is in standby state, the client can\n   *         try other servers\n   * @throws RetriableException the token was invalid, and the server thinks \n   *         this may be a temporary issue and suggests the client to retry\n   * @throws IOException to allow future exceptions to be added without breaking\n   *         compatibility",
  "org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)" : "* Propagate options to any builder.\n   * {@link FutureIO#propagateOptions(FSBuilder, Configuration, String, String)}\n   * @param builder builder to modify\n   * @param conf configuration to read\n   * @param optionalPrefix prefix for optional settings\n   * @param mandatoryPrefix prefix for mandatory settings\n   * @param <T> type of result\n   * @param <U> type of builder\n   * @return the builder passed in.",
  "org.apache.hadoop.ha.ZKFailoverController:verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:clear()" : null,
  "org.apache.hadoop.conf.StorageUnit$7:toTBs(double)" : null,
  "org.apache.hadoop.io.MapFile$Reader:open(org.apache.hadoop.fs.Path,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])" : null,
  "org.apache.hadoop.io.LongWritable:<init>(long)" : null,
  "org.apache.hadoop.util.Preconditions:checkState(boolean,java.lang.Object)" : "* Ensures the truth of an expression involving the state of the calling instance\n   * without involving any parameters to the calling method.\n   *\n   * @param expression a boolean expression\n   * @param errorMessage the exception message to use if the check fails; will be converted to a\n   *     string using {@link String#valueOf(Object)}\n   * @throws IllegalStateException if {@code expression} is false",
  "org.apache.hadoop.util.bloom.RetouchedBloomFilter:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.util.HttpExceptionUtils:getOneLineMessage(java.lang.Throwable)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:complete()" : null,
  "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getConf()" : null,
  "org.apache.hadoop.fs.HarFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)" : null,
  "org.apache.hadoop.net.NetworkTopology:contains(org.apache.hadoop.net.Node)" : "Check if the tree contains node <i>node</i>\n   * \n   * @param node a node\n   * @return true if <i>node</i> is already in the tree; false otherwise",
  "org.apache.hadoop.util.SysInfoLinux:readProcNetInfoFile()" : "* Read /proc/net/dev file, parse and calculate amount\n   * of bytes read and written through the network.",
  "org.apache.hadoop.fs.HarFileSystem:getHarAuth(java.net.URI)" : "* Create a har specific auth \n   * har-underlyingfs:port\n   * @param underLyingUri the uri of underlying\n   * filesystem\n   * @return har specific auth",
  "org.apache.hadoop.ipc.Server:getClientId()" : "* @return Returns the clientId from the current RPC request.",
  "org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping:isNonEmpty()" : null,
  "org.apache.hadoop.fs.FilterFs:listCorruptFileBlocks(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:substitute(java.nio.ByteBuffer[],int,java.nio.ByteBuffer,int)" : "* A \"bulk\" version of the substitute, using ByteBuffer.\n   * Tends to be 2X faster than the \"int\" substitute in a loop.\n   *\n   * @param p input polynomial\n   * @param q store the return result\n   * @param x input field\n   * @param len input len.",
  "org.apache.hadoop.ipc.CallQueueManager:take()" : "* Retrieve an E from the backing queue or block until we can.\n   * Guaranteed to return an element from the current queue.",
  "org.apache.hadoop.fs.AbstractFileSystem:getAclStatus(org.apache.hadoop.fs.Path)" : "* Gets the ACLs of files and directories.\n   *\n   * @param path Path to get\n   * @return RemoteIterator{@literal <AclStatus>} which returns each AclStatus\n   * @throws IOException if an ACL could not be read",
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompressDirectBuf()" : null,
  "org.apache.hadoop.util.Waitable:await()" : null,
  "org.apache.hadoop.fs.shell.CopyCommands$Put:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:toString()" : null,
  "org.apache.hadoop.fs.FSDataOutputStream:hsync()" : null,
  "org.apache.hadoop.fs.shell.find.BaseExpression:addChildren(java.util.Deque,int)" : "* Add a specific number of children to this expression. The children are\n   * popped off the head of the expressions.\n   *\n   * @param exprs\n   *          deque of expressions from which to take the children\n   * @param count\n   *          number of children to be added",
  "org.apache.hadoop.fs.FsUrlConnection:getInputStream()" : null,
  "org.apache.hadoop.fs.shell.PathData:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String)" : "* Looks up the file status for a path.  If the path\n   * doesn't exist, then the status will be null\n   * @param fs the FileSystem for the path\n   * @param pathString a string for a path \n   * @throws IOException if anything goes wrong",
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:decompress(byte[],int,int)" : null,
  "org.apache.hadoop.fs.shell.Command:getDepth()" : null,
  "org.apache.hadoop.util.StringUtils:join(char,java.lang.String[])" : null,
  "org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:isStatic()" : "* @return whether the method is a static method",
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:instance()" : "* @return the metrics system object",
  "org.apache.hadoop.fs.FilterFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:end()" : null,
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getFS()" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)" : "* The constructor with all the necessary info.\n   *\n   * @param inputBlocks inputBlocks.\n   * @param outputBlocks outputBlocks.\n   * @param rsRawEncoder  underlying RS encoder for hitchhiker encoding\n   * @param xorRawEncoder underlying XOR encoder for hitchhiker encoding",
  "org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:size()" : null,
  "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:createEncryptor()" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_create(java.lang.Object)" : "* Create a new {@code IOStatisticsSnapshot} instance.\n   * @param source optional source statistics\n   * @return an IOStatisticsSnapshot.\n   * @throws ClassCastException if the {@code source} is not valid.\n   * @throws UnsupportedOperationException if the IOStatistics classes were not found",
  "org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:<init>(java.lang.String[],java.lang.String)" : null,
  "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:finished()" : null,
  "org.apache.hadoop.util.StringUtils:toLowerCase(java.lang.String)" : "* Converts all of the characters in this String to lower case with\n   * Locale.ENGLISH.\n   *\n   * @param str  string to be converted\n   * @return     the str, converted to lowercase.",
  "org.apache.hadoop.io.compress.DecompressorStream:markSupported()" : null,
  "org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.ftp.FtpFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : "* This constructor has the signature needed by\n   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.\n   * \n   * @param theUri which must be that of localFs\n   * @param conf\n   * @throws IOException\n   * @throws URISyntaxException",
  "org.apache.hadoop.fs.Path:isUriPathAbsolute()" : "* Returns true if the path component (i.e. directory) of this URI is\n   * absolute.\n   *\n   * @return whether this URI's path is absolute",
  "org.apache.hadoop.security.Credentials:getSecretKey(org.apache.hadoop.io.Text)" : "* Returns the key bytes for the alias.\n   * @param alias the alias for the key\n   * @return key for this alias",
  "org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String,java.lang.String)" : "* Adds the deprecated key to the global deprecation map.\n   * It does not override any existing entries in the deprecation map.\n   * This is to be used only by the developers in order to add deprecation of\n   * keys, and attempts to call this method after loading resources once,\n   * would lead to <tt>UnsupportedOperationException</tt>\n   * \n   * If you have multiple deprecation entries to add, it is more efficient to\n   * use #addDeprecations(DeprecationDelta[] deltas) instead.\n   *\n   * @param key to be deprecated\n   * @param newKey key that take up the values of deprecated key\n   * @param customMessage deprecation message",
  "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:<init>(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.net.NetUtils:normalizeHostNames(java.util.Collection)" : "* Given a collection of string representation of hosts, return a list of\n   * corresponding IP addresses in the textual representation.\n   * \n   * @param names a collection of string representations of hosts\n   * @return a list of corresponding IP addresses in the string format\n   * @see #normalizeHostName(String)",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:atEnd()" : "* Is cursor at the end location?\n       * \n       * @return true if the cursor is at the end location.",
  "org.apache.hadoop.crypto.CryptoInputStream:getDecryptor()" : "Get decryptor from pool",
  "org.apache.hadoop.fs.GlobPattern:compiled()" : "* @return the compiled pattern",
  "org.apache.hadoop.io.WritableUtils:readString(java.io.DataInput)" : null,
  "org.apache.hadoop.util.HostsFileReader:getExcludedHosts()" : null,
  "org.apache.hadoop.metrics2.source.JvmMetrics:getThreadUsageFromGroup(org.apache.hadoop.metrics2.MetricsRecordBuilder)" : null,
  "org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:close()" : null,
  "org.apache.hadoop.net.NetworkTopologyWithNodeGroup:remove(org.apache.hadoop.net.Node)" : "Remove a node\n   * Update node counter and rack counter if necessary\n   * @param node node to be removed; can be null",
  "org.apache.hadoop.io.erasurecode.rawcoder.InvalidDecodingException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.fs.QuotaUsage:getTypeConsumed(org.apache.hadoop.fs.StorageType)" : "* Return storage type consumed.\n   *\n   * @param type storage type.\n   * @return type consumed.",
  "org.apache.hadoop.io.serializer.JavaSerialization:accept(java.lang.Class)" : null,
  "org.apache.hadoop.ipc.CallerContext:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:getInitialWorkingDirectory()" : "* Some file systems like LocalFileSystem have an initial workingDir\n   * that is used as the starting workingDir. For other file systems\n   * like HDFS there is no built in notion of an initial workingDir.\n   * \n   * @return the initial workingDir if the file system has such a notion\n   *         otherwise return a null.",
  "org.apache.hadoop.crypto.CryptoInputStream:freeBuffers()" : "Forcibly free the direct buffers.",
  "org.apache.hadoop.metrics2.impl.MetricsRecordImpl:name()" : null,
  "org.apache.hadoop.util.AsyncDiskService:execute(java.lang.String,java.lang.Runnable)" : "* Execute the task sometime in the future, using ThreadPools.\n   *\n   * @param root root.\n   * @param task task.",
  "org.apache.hadoop.fs.Globber:listStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:convert(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)" : null,
  "org.apache.hadoop.util.JvmPauseMonitor:getTotalGcExtraSleepTime()" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:writeTo(org.apache.hadoop.ipc.ResponseBuffer)" : null,
  "org.apache.hadoop.io.Text:readWithKnownLength(java.io.DataInput,int)" : "* Read a Text object whose length is already known.\n   * This allows creating Text from a stream which uses a different serialization\n   * format.\n   *\n   * @param in input in.\n   * @param len input len.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:alterCipherList(java.lang.String[])" : null,
  "org.apache.hadoop.util.Shell:getWaitingThread()" : "get the thread that is waiting on this instance of <code>Shell</code>.\n   * @return the thread that ran runCommand() that spawned this shell\n   * or null if no thread is waiting for this shell to complete",
  "org.apache.hadoop.conf.ReconfigurationException:<init>(java.lang.String,java.lang.String,java.lang.String)" : "* Create a new instance of {@link ReconfigurationException}.\n   * @param property property name.\n   * @param newVal new value.\n   * @param oldVal old value.",
  "org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String[],java.lang.String)" : "* Adds the deprecated key to the global deprecation map.\n   * It does not override any existing entries in the deprecation map.\n   * This is to be used only by the developers in order to add deprecation of\n   * keys, and attempts to call this method after loading resources once,\n   * would lead to <tt>UnsupportedOperationException</tt>\n   * \n   * If a key is deprecated in favor of multiple keys, they are all treated as \n   * aliases of each other, and setting any one of them resets all the others \n   * to the new value.\n   *\n   * If you have multiple deprecation entries to add, it is more efficient to\n   * use #addDeprecations(DeprecationDelta[] deltas) instead.\n   * \n   * @param key to be deprecated\n   * @param newKeys list of keys that take up the values of deprecated key\n   * @param customMessage depcrication message\n   * @deprecated use {@link #addDeprecation(String key, String newKey,\n      String customMessage)} instead",
  "org.apache.hadoop.security.ShellBasedIdMapping:isExpired()" : null,
  "org.apache.hadoop.io.IntWritable:compareTo(org.apache.hadoop.io.IntWritable)" : "Compares two IntWritables.",
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem$HttpDataInputStream:readFully(long,byte[],int,int)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:setDataWithRetries(java.lang.String,byte[],int)" : null,
  "org.apache.hadoop.util.ProgramDriver$ProgramDescription:invoke(java.lang.String[])" : "* Invoke the example application with the given arguments\n     * @param args the arguments for the application\n     * @throws Throwable The exception thrown by the invoked method",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createSecretKey(byte[])" : "* Convert the byte[] to a secret key\n   * @param key the byte[] to create the secret key from\n   * @return the secret key",
  "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String)" : "* Get unix groups (parent) and netgroups for given user\n   *\n   * @param user get groups and netgroups for this user\n   * @return groups and netgroups for user",
  "org.apache.hadoop.ipc.Server$Connection:processConnectionContext(org.apache.hadoop.ipc.RpcWritable$Buffer)" : "Reads the connection context following the connection header\n     * @throws RpcServerException - if the header cannot be\n     *         deserialized, or the user is not authorized",
  "org.apache.hadoop.net.unix.DomainSocket:close()" : "* Close the Socket.",
  "org.apache.hadoop.fs.FileUtil:setReadable(java.io.File,boolean)" : "* Platform independent implementation for {@link File#setReadable(boolean)}\n   * File#setReadable does not work as expected on Windows.\n   * @param f input file\n   * @param readable readable.\n   * @return true on success, false otherwise",
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:<init>()" : null,
  "org.apache.hadoop.security.SecurityUtil:doAsCurrentUser(java.security.PrivilegedExceptionAction)" : "* Perform the given action as the daemon's current user. If an\n   * InterruptedException is thrown, it is converted to an IOException.\n   *\n   * @param action the action to perform\n   * @param <T> generic type T.\n   * @return the result of the action\n   * @throws IOException in the event of error",
  "org.apache.hadoop.tools.TableListing$Column:getMaxWidth()" : null,
  "org.apache.hadoop.fs.HarFileSystem:makeQualified(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.EmptyStorageStatistics:getLongStatistics()" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getFileLinkStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_reset()" : "* Reset the context's IOStatistics.\n   * {@link IOStatisticsContext#reset()}",
  "org.apache.hadoop.util.LineReader:getIOStatistics()" : "* Return any IOStatistics provided by the source.\n   * @return IO stats from the input stream.",
  "org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:checkEmpty()" : null,
  "org.apache.hadoop.fs.impl.CombinedFileRange:toString()" : null,
  "org.apache.hadoop.ha.SshFenceByTcpPort:execCommand(com.jcraft.jsch.Session,java.lang.String)" : "* Execute a command through the ssh session, pumping its\n   * stderr and stdout to our own logs.",
  "org.apache.hadoop.io.BytesWritable:getLength()" : "* Get the current size of the buffer.",
  "org.apache.hadoop.fs.audit.CommonAuditContext:containsKey(java.lang.String)" : "* Does the context contain a specific key?\n   * @param key key\n   * @return true if it is in the context.",
  "org.apache.hadoop.fs.shell.find.Name:apply(org.apache.hadoop.fs.shell.PathData,int)" : null,
  "org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)" : null,
  "org.apache.hadoop.fs.FileSystemStorageStatistics:getLongStatistics()" : null,
  "org.apache.hadoop.security.SaslInputStream:read()" : "* Reads the next byte of data from this input stream. The value byte is\n   * returned as an <code>int</code> in the range <code>0</code> to\n   * <code>255</code>. If no byte is available because the end of the stream has\n   * been reached, the value <code>-1</code> is returned. This method blocks\n   * until input data is available, the end of the stream is detected, or an\n   * exception is thrown.\n   * <p>\n   * \n   * @return the next byte of data, or <code>-1</code> if the end of the stream\n   *         is reached.\n   * @exception IOException\n   *              if an I/O error occurs.",
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getRemaining()" : "* Returns the number of bytes remaining in the input buffers; normally\n   * called when finished() is true to determine amount of post-gzip-stream\n   * data.\n   *\n   * @return the total (non-negative) number of unprocessed bytes in input",
  "org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:decrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)" : "* Get all of the xattrs for a file or directory.\n   * Only those xattrs for which the logged-in user has permissions to view\n   * are returned.\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to get extended attributes\n   * @param names XAttr names.\n   * @return Map{@literal <}String, byte[]{@literal >} describing the XAttrs\n   * of the file or directory\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:getMemlockLimit()" : null,
  "org.apache.hadoop.fs.http.HttpsFileSystem:delete(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path,int)" : null,
  "org.apache.hadoop.security.KDiag:validateKeyLength()" : null,
  "org.apache.hadoop.util.ClassUtil:findContainingJar(java.lang.Class)" : "* Find a jar that contains a class of the same name, if any.\n   * It will return a jar file, even if that is not the first thing\n   * on the class path that has a class with the same name.\n   * \n   * @param clazz the class to find.\n   * @return a jar file that contains the class, or null.",
  "org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processWaitTimeAndRetryInfo()" : "* It first processes the wait time, if there is any,\n     * and then invokes {@link #processRetryInfo()}.\n     *\n     * If the wait time is positive, it either sleeps for synchronous calls\n     * or immediately returns for asynchronous calls.\n     *\n     * @return {@link CallReturn#RETRY} if the retryInfo is processed;\n     *         otherwise, return {@link CallReturn#WAIT_RETRY}.",
  "org.apache.hadoop.service.CompositeService:serviceStop()" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:getDefaultThresholds(int)" : "* Generate default thresholds if user did not specify. Strategy is\n   * to halve each time, since queue usage tends to be exponential.\n   * So if numLevels is 4, we would generate: double[]{0.125, 0.25, 0.5}\n   * which specifies the boundaries between each queue's usage.\n   * @param numLevels number of levels to compute for\n   * @return array of boundaries of length numLevels - 1",
  "org.apache.hadoop.fs.FilterFileSystem:makeQualified(org.apache.hadoop.fs.Path)" : "Make sure that a path specifies a FileSystem.",
  "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:toString()" : null,
  "org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:finished()" : null,
  "org.apache.hadoop.fs.FileContext:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)" : "* Checks if the user can access a path.  The mode specifies which access\n   * checks to perform.  If the requested permissions are granted, then the\n   * method returns normally.  If access is denied, then the method throws an\n   * {@link AccessControlException}.\n   * <p>\n   * The default implementation of this method calls {@link #getFileStatus(Path)}\n   * and checks the returned permissions against the requested permissions.\n   * Note that the getFileStatus call will be subject to authorization checks.\n   * Typically, this requires search (execute) permissions on each directory in\n   * the path's prefix, but this is implementation-defined.  Any file system\n   * that provides a richer authorization model (such as ACLs) may override the\n   * default implementation so that it checks against that model instead.\n   * <p>\n   * In general, applications should avoid using this method, due to the risk of\n   * time-of-check/time-of-use race conditions.  The permissions on a file may\n   * change immediately after the access call returns.  Most applications should\n   * prefer running specific file system actions as the desired user represented\n   * by a {@link UserGroupInformation}.\n   *\n   * @param path Path to check\n   * @param mode type of access to check\n   * @throws AccessControlException if access is denied\n   * @throws FileNotFoundException if the path does not exist\n   * @throws UnsupportedFileSystemException if file system for <code>path</code>\n   *   is not supported\n   * @throws IOException see specific implementation\n   * \n   * Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws \n   *           undeclared exception to RPC server",
  "org.apache.hadoop.fs.HarFileSystem$HarMetaData:addPartFileStatuses(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.ThreadUtil:joinUninterruptibly(java.lang.Thread)" : "* Join a thread as uninterruptible.\n   * The call continues to block until the result is available even when the\n   * caller thread is interrupted.\n   * The method will log any {@link InterruptedException} then will re-interrupt\n   * the thread.\n   *\n   * @param toJoin the thread to Join on.",
  "org.apache.hadoop.http.IsActiveServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)" : "* Check whether this instance is the Active one.\n   * @param req HTTP request\n   * @param resp HTTP response to write to",
  "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)" : null,
  "org.apache.hadoop.util.Shell:getMemlockLimit(java.lang.Long)" : "* Static method to return the memory lock limit for datanode.\n   * @param ulimit max value at which memory locked should be capped.\n   * @return long value specifying the memory lock limit.",
  "org.apache.hadoop.crypto.CryptoInputStream:seekToNewSource(long)" : null,
  "org.apache.hadoop.ipc.Client:getConnectionIds()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.MD5Hash:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:stopMBeans()" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry:toString()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)" : "* Set an xattr of a file or directory.\n   * The name must be prefixed with the namespace followed by \".\". For example,\n   * \"user.attr\".\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to modify\n   * @param name xattr name.\n   * @param value xattr value.\n   * @param flag xattr set flag\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.log.LogThrottlingHelper:reset()" : null,
  "org.apache.hadoop.conf.StorageUnit$3:toTBs(double)" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler:getFailoverCount()" : null,
  "org.apache.hadoop.ipc.RemoteException:getErrorCode()" : "* @return may be null if the code was newer than our protobuf definitions or none was given.",
  "org.apache.hadoop.util.WeakReferenceMap:size()" : "* Map size.\n   * @return the current map size.",
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String)" : "* Requests a delegation token using the configured <code>Authenticator</code>\n   * for authentication.\n   *\n   * @param url the URL to get the delegation token from. Only HTTP/S URLs are\n   * supported.\n   * @param token the authentication token being used for the user where the\n   * Delegation token will be stored.\n   * @param renewer the renewer user.\n   * @throws IOException if an IO error occurred.\n   * @throws AuthenticationException if an authentication exception occurred.\n   * @return abstract delegation token identifier.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:createTracker(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String)" : "* Create the tracker. If the factory is null, a stub\n   * tracker is returned.\n   * @param factory tracker factory\n   * @param statistic statistic to track\n   * @return a duration tracker.",
  "org.apache.hadoop.util.DataChecksum:getChecksumHeaderSize()" : null,
  "org.apache.hadoop.fs.QuotaUsage$Builder:quota(long)" : null,
  "org.apache.hadoop.fs.PathIOException:getPath()" : "@return Path that generated the exception",
  "org.apache.hadoop.util.StringUtils:byteToHexString(byte[],int,int)" : "* Given an array of bytes it will convert the bytes to a hex string\n   * representation of the bytes\n   * @param bytes bytes.\n   * @param start start index, inclusively\n   * @param end end index, exclusively\n   * @return hex string representation of the byte array",
  "org.apache.hadoop.util.DataChecksum:writeHeader(java.io.DataOutputStream)" : "* Writes the checksum header to the output stream <i>out</i>.\n   *\n   * @param out output stream.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.util.HostsFileReader$HostDetails:<init>(java.lang.String,java.util.Set,java.lang.String,java.util.Map)" : null,
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:requireIOStatisticsSnapshot(java.io.Serializable)" : "* Require the parameter to be an instance of {@link IOStatisticsSnapshot}.\n   * @param snapshot object to validate\n   * @return cast value\n   * @throws IllegalArgumentException if the supplied class is not a snapshot",
  "org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getUsersForNetgroup(java.lang.String)" : "* Gets users for a netgroup\n   *\n   * @param netgroup return users for this netgroup\n   * @return list of users for a given netgroup\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.ipc.RetryCache$CacheEntry:getExpirationTime()" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:enterNeutralMode()" : null,
  "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:getThreadCount()" : null,
  "org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processOptions(java.util.LinkedList)" : null,
  "org.apache.hadoop.security.SaslPlainServer:getAuthorizationID()" : null,
  "org.apache.hadoop.security.SaslRpcClient:getInputStream(java.io.InputStream)" : "* Get SASL wrapped InputStream if SASL QoP requires unwrapping,\n   * otherwise return original stream.  Can be called only after\n   * saslConnect() has been called.\n   * \n   * @param in - InputStream used to make the connection\n   * @return InputStream that may be using SASL unwrap\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.TrashPolicyDefault$Emptier:run()" : null,
  "org.apache.hadoop.ipc.RpcWritable$Buffer:getByteBuffer()" : null,
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadIncompleteFlush(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* The KeyStore might have gone down during a flush, In which case either the\n   * _NEW or _OLD files might exists. This method tries to load the KeyStore\n   * from one of these intermediate files.\n   * @param oldPath the _OLD file created during flush\n   * @param newPath the _NEW file created during flush\n   * @return The permissions of the loaded file\n   * @throws IOException\n   * @throws NoSuchAlgorithmException\n   * @throws CertificateException",
  "org.apache.hadoop.io.retry.RetryInvocationHandler:newCall(java.lang.reflect.Method,java.lang.Object[],boolean,int)" : null,
  "org.apache.hadoop.io.compress.DecompressorStream:reset()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.io.UTF8:toString()" : "Convert to a String.",
  "org.apache.hadoop.crypto.OpensslCtrCryptoCodec:generateSecureRandom(byte[])" : null,
  "org.apache.hadoop.fs.impl.FileRangeImpl:setOffset(long)" : null,
  "org.apache.hadoop.security.authorize.AuthorizationException:<init>(java.lang.Throwable)" : "* Constructs a new exception with the specified cause and a detail\n   * message of <tt>(cause==null ? null : cause.toString())</tt> (which\n   * typically contains the class and detail message of <tt>cause</tt>).\n   * @param  cause the cause (which is saved for later retrieval by the\n   *         {@link #getCause()} method).  (A <tt>null</tt> value is\n   *         permitted, and indicates that the cause is nonexistent or\n   *         unknown.)",
  "org.apache.hadoop.ipc.Server$Call:<init>()" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:<init>(org.apache.hadoop.fs.FileSystem)" : null,
  "org.apache.hadoop.fs.FileSystem:getContentSummary(org.apache.hadoop.fs.Path)" : "Return the {@link ContentSummary} of a given {@link Path}.\n   * @param f path to use\n   * @throws FileNotFoundException if the path does not resolve\n   * @throws IOException IO failure\n   * @return content summary.",
  "org.apache.hadoop.security.UserGroupInformation:getRefreshTime(javax.security.auth.kerberos.KerberosTicket)" : null,
  "org.apache.hadoop.fs.FileUtil:unTarUsingJava(java.io.InputStream,java.io.File,boolean)" : null,
  "org.apache.hadoop.metrics2.MetricStringBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)" : null,
  "org.apache.hadoop.ipc.Server$ExceptionsHandler:addSuppressedLoggingExceptions(java.lang.Class[])" : "* Add exception classes which server won't log at all.\n     * Optimized for infrequent invocation.\n     * @param exceptionClass exception classes",
  "org.apache.hadoop.util.GenericsUtil:isLog4jLogger(java.lang.String)" : "* Determine whether the log of the given logger is of Log4J implementation.\n   *\n   * @param logger the logger name, usually class name as string.\n   * @return true if the logger uses Log4J implementation.",
  "org.apache.hadoop.security.LdapGroupsMapping:resolveCustomGroupFilterArgs(javax.naming.directory.SearchResult)" : null,
  "org.apache.hadoop.ipc.RetryCache$CacheEntry:hashCode()" : null,
  "org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File,java.util.regex.Pattern)" : "* Unpack matching files from a jar. Entries inside the jar that do\n   * not match the given pattern will be skipped.\n   *\n   * @param jarFile the .jar file to unpack\n   * @param toDir the destination directory into which to unpack the jar\n   * @param unpackRegex the pattern to match jar entries against\n   *\n   * @throws IOException if an I/O error has occurred or toDir\n   * cannot be created and does not already exist",
  "org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot:<init>(long,long)" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:blocks()" : "* Gets the blocks in this cache.",
  "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:configureSocket(java.net.Socket)" : null,
  "org.apache.hadoop.util.KMSUtil:toJSON(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)" : null,
  "org.apache.hadoop.ipc.Server:setAlignmentContext(org.apache.hadoop.ipc.AlignmentContext)" : "* Set alignment context to pass state info thru RPC.\n   *\n   * @param alignmentContext alignment state context",
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)" : null,
  "org.apache.hadoop.fs.ftp.FtpConfigKeys:getServerDefaults()" : null,
  "org.apache.hadoop.security.User:setLogin(javax.security.auth.login.LoginContext)" : "* Set the login object\n   * @param login",
  "org.apache.hadoop.io.SequenceFile$Metadata:<init>(java.util.TreeMap)" : null,
  "org.apache.hadoop.util.Progress:startNextPhase()" : "Called during execution to move to the next phase at this level in the\n   * tree.",
  "org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision,long,java.lang.String)" : null,
  "org.apache.hadoop.security.UserGroupInformation:getKeytab()" : null,
  "org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,java.util.List,org.apache.hadoop.ipc.RemoteException)" : null,
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$PrefetchTask:get()" : null,
  "org.apache.hadoop.io.erasurecode.ECBlockGroup:getErasedCount()" : "* Get erased blocks count\n   * @return erased count of blocks",
  "org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])" : "* Runs the given <code>Tool</code> by {@link Tool#run(String[])}, after \n   * parsing with the given generic arguments. Uses the given \n   * <code>Configuration</code>, or builds one if null.\n   * \n   * Sets the <code>Tool</code>'s configuration with the possibly modified \n   * version of the <code>conf</code>.  \n   * \n   * @param conf <code>Configuration</code> for the <code>Tool</code>.\n   * @param tool <code>Tool</code> to run.\n   * @param args command-line arguments to the tool.\n   * @return exit code of the {@link Tool#run(String[])} method.\n   * @throws Exception Exception.",
  "org.apache.hadoop.util.CacheableIPList:updateCacheExpiryTime()" : null,
  "org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:close()" : null,
  "org.apache.hadoop.fs.Options$CreateOpts:progress(org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.ipc.CallQueueManager:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)" : null,
  "org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:<init>(java.lang.String,int)" : null,
  "org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopMetricsMBeans()" : null,
  "org.apache.hadoop.fs.BufferedFSInputStream:seekToNewSource(long)" : null,
  "org.apache.hadoop.util.OperationDuration:time()" : "* Evaluate the system time.\n   * @return the current clock time.",
  "org.apache.hadoop.io.OutputBuffer$Buffer:reset()" : null,
  "org.apache.hadoop.fs.FsShell$UnknownCommandException:<init>()" : null,
  "org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:failover(long,java.lang.reflect.Method,int)" : null,
  "org.apache.hadoop.conf.ConfigRedactor:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:create(java.lang.String)" : "* Create a ZNode.\n   * @param path Path of the ZNode.\n   * @return If the ZNode was created.\n   * @throws Exception If it cannot contact Zookeeper.",
  "org.apache.hadoop.fs.FileStatus:isSymlink()" : "* Is this a symbolic link?\n   * @return true if this is a symbolic link",
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:isValid()" : "* Determines if the current position is valid.\n   *\n   * @return true if the current position is valid, false otherwise.",
  "org.apache.hadoop.fs.DelegateToFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileSystem:canonicalizeUri(java.net.URI)" : "* Canonicalize the given URI.\n   *\n   * This is implementation-dependent, and may for example consist of\n   * canonicalizing the hostname using DNS and adding the default\n   * port if not specified.\n   *\n   * The default implementation simply fills in the default port if\n   * not specified and if {@link #getDefaultPort()} returns a\n   * default port.\n   *\n   * @param uri url.\n   * @return URI\n   * @see NetUtils#getCanonicalUri(URI, int)",
  "org.apache.hadoop.util.ConfTest$1:<init>()" : null,
  "org.apache.hadoop.io.DataInputBuffer$Buffer:getLength()" : null,
  "org.apache.hadoop.security.UserGroupInformation:hasSufficientTimeElapsed(long)" : null,
  "org.apache.hadoop.util.StringUtils:getStringCollection(java.lang.String)" : "* Returns a collection of strings.\n   * @param str comma separated string values\n   * @return an <code>ArrayList</code> of string values",
  "org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.net.StandardSocketFactory:createSocket(java.net.InetAddress,int)" : null,
  "org.apache.hadoop.net.NetworkTopology:recommissionNode(org.apache.hadoop.net.Node)" : "* Update empty rack number when add a node like recommission.\n   * @param node node to be added; can be null",
  "org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:finish()" : null,
  "org.apache.hadoop.io.ReadaheadPool:getInstance()" : "* @return Return the singleton instance for the current process.",
  "org.apache.hadoop.util.ConfigurationHelper:mapEnumNamesToValues(java.lang.String,java.lang.Class)" : "* Given an enum class, build a map of lower case names to values.\n   * @param prefix prefix (with trailing \".\") for path capabilities probe\n   * @param enumClass class of enum\n   * @param <E> enum type\n   * @return a mutable map of lower case names to enum values\n   * @throws IllegalArgumentException if there are two entries which differ only by case.",
  "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getSchedulingDecisionSummary()" : null,
  "org.apache.hadoop.fs.ParentNotDirectoryException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.fs.FilterFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.io.MapFile$Reader:finalKey(org.apache.hadoop.io.WritableComparable)" : "* Reads the final key from the file.\n     *\n     * @param key key to read into\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:invoke(java.lang.String,java.lang.Object[],java.lang.String[])" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem:getRack(java.lang.String)" : "* Utility to replace null with DEFAULT_RACK.\n   *\n   * @param rackString rack value, can be null\n   * @return non-null rack string",
  "org.apache.hadoop.fs.http.HttpsFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.tools.TableListing:toString()" : null,
  "org.apache.hadoop.fs.FileSystem:cacheSize()" : null,
  "org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parseCommaSeparatedString(java.lang.String)" : "* Parse the given string as a MultipleLinearRandomRetry object.\n     * The format of the string is \"t_1, n_1, t_2, n_2, ...\",\n     * where t_i and n_i are the i-th pair of sleep time and number of retries.\n     * Note that the white spaces in the string are ignored.\n     *\n     * @param s input string.\n     * @return the parsed object, or null if the parsing fails.",
  "org.apache.hadoop.fs.Options$HandleOpt:resolve(java.util.function.BiFunction,org.apache.hadoop.fs.Options$HandleOpt[])" : "* Utility function for partial evaluation of {@link FileStatus}\n     * instances to a fixed set of handle options.\n     * @param fsr Function reference\n     * @param opt Options to associate with {@link FileStatus} instances to\n     *            produce {@link PathHandle} instances.\n     * @return Function reference with options fixed",
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:isLocalhost(java.lang.String)" : null,
  "org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:<init>(java.lang.String,java.io.FileDescriptor,long,long)" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:getCounter(long)" : null,
  "org.apache.hadoop.fs.viewfs.RegexMountPoint:initializeInterceptors()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:listStatus(org.apache.hadoop.fs.Path)" : "* {@inheritDoc}\n   *\n   * (<b>Note</b>: Returned list is not sorted in any given order,\n   * due to reliance on Java's {@link File#list()} API.)",
  "org.apache.hadoop.fs.Path:getPathWithoutSchemeAndAuthority(org.apache.hadoop.fs.Path)" : "* Return a version of the given Path without the scheme information.\n   *\n   * @param path the source Path\n   * @return a copy of this Path without the scheme information",
  "org.apache.hadoop.net.NetworkTopology:getNumOfLeaves()" : "@return the total number of leaf nodes",
  "org.apache.hadoop.metrics2.impl.MetricsBufferBuilder:add(java.lang.String,java.lang.Iterable)" : null,
  "org.apache.hadoop.fs.CreateFlag:validateForAppend(java.util.EnumSet)" : "* Validate the CreateFlag for the append operation. The flag must contain\n   * APPEND, and cannot contain OVERWRITE.\n   *\n   * @param flag enum set flag.",
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumDataUnits()" : null,
  "org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int,int)" : null,
  "org.apache.hadoop.fs.RawPathHandle:readObjectNoData()" : null,
  "org.apache.hadoop.metrics2.MetricStringBuilder:parent()" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.util.FindClass:dumpResource(java.lang.String)" : "* Dump a resource to out\n   * @param name resource name\n   * @return the status code",
  "org.apache.hadoop.fs.audit.CommonAuditContext:get(java.lang.String)" : "* Get a context entry.\n   * @param key key\n   * @return value or null",
  "org.apache.hadoop.crypto.random.OsSecureRandom:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:available()" : "* Just forward to the fis.",
  "org.apache.hadoop.fs.viewfs.InodeTree:<init>(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI,boolean)" : "* Create Inode Tree from the specified mount-table specified in Config.\n   *\n   * @param config the mount table keys are prefixed with\n   *               FsConstants.CONFIG_VIEWFS_PREFIX.\n   * @param viewName the name of the mount table\n   *                 if null use defaultMT name.\n   * @param theUri heUri.\n   * @param initingUriAsFallbackOnNoMounts initingUriAsFallbackOnNoMounts.\n   * @throws UnsupportedFileSystemException file system for <code>uri</code> is\n   *                                        not found.\n   * @throws URISyntaxException if the URI does not have an authority\n   *                            it is badly formed.\n   * @throws FileAlreadyExistsException there is a file at the path specified\n   *                                    or is discovered on one of its ancestors.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.authorize.AccessControlList:isUserAllowed(org.apache.hadoop.security.UserGroupInformation)" : null,
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getPermission()" : null,
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getBufferSize()" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:needsDictionary()" : null,
  "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:equals(java.lang.Object)" : null,
  "org.apache.hadoop.net.unix.DomainSocket:isOpen()" : "* Return true if the file descriptor is currently open.\n   *\n   * @return                 True if the file descriptor is currently open.",
  "org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:getCodecName()" : null,
  "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:getProcessingName(int)" : "* @return Returns the rate name inside the metric.\n   * @param priority input priority.",
  "org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:setDictionary(byte[],int,int)" : null,
  "org.apache.hadoop.fs.impl.prefetch.FilePosition:data()" : null,
  "org.apache.hadoop.fs.AbstractFileSystem:printStatistics()" : "* Prints statistics for all file systems.",
  "org.apache.hadoop.fs.shell.find.Print:apply(org.apache.hadoop.fs.shell.PathData,int)" : null,
  "org.apache.hadoop.security.UserGroupInformation$RealUser:getRealUser()" : null,
  "org.apache.hadoop.util.XMLUtils:transform(java.io.InputStream,java.io.InputStream,java.io.Writer)" : "* Transform input xml given a stylesheet.\n   * \n   * @param styleSheet the style-sheet\n   * @param xml input xml data\n   * @param out output\n   * @throws TransformerConfigurationException synopsis signals a problem\n   *         creating a transformer object.\n   * @throws TransformerException this is used for throwing processor\n   *          exceptions before the processing has started.",
  "org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:next()" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2:registerProtocolEngine()" : null,
  "org.apache.hadoop.fs.HarFileSystem:fixBlockLocations(org.apache.hadoop.fs.BlockLocation[],long,long,long)" : "* Fix offset and length of block locations.\n   * Note that this method modifies the original array.\n   * @param locations block locations of har part file\n   * @param start the start of the desired range in the contained file\n   * @param len the length of the desired range\n   * @param fileOffsetInHar the offset of the desired file in the har part file\n   * @return block locations with fixed offset and length",
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getInstance()" : "* Get the object performs Galois field arithmetic with default setting.\n   * @return GaloisField.",
  "org.apache.hadoop.ha.HealthMonitor:shutdown()" : null,
  "org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File)" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:skip(long)" : null,
  "org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getServerName()" : "* Quote the server name so that users specifying the HOST HTTP header\n       * can't inject attacks.",
  "org.apache.hadoop.fs.BatchedRemoteIterator$BatchedListEntries:get(int)" : null,
  "org.apache.hadoop.security.alias.JavaKeyStoreProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.QuickSort:fix(org.apache.hadoop.util.IndexedSortable,int,int)" : null,
  "org.apache.hadoop.fs.FileSystem:delete(org.apache.hadoop.fs.Path)" : "* Delete a file/directory.\n   * @param f the path.\n   * @throws IOException IO failure.\n   * @return if delete success true, not false.\n   * @deprecated Use {@link #delete(Path, boolean)} instead.",
  "org.apache.hadoop.ha.ZKFailoverController:doRun(java.lang.String[])" : null,
  "org.apache.hadoop.security.authorize.AccessControlList:addUser(java.lang.String)" : "* Add user to the names of users allowed for this service.\n   * \n   * @param user\n   *          The user name",
  "org.apache.hadoop.security.UserGroupInformation:unprotectedRelogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean)" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_counters(java.io.Serializable)" : "* Get the counters of an IOStatisticsSnapshot.\n   * @param source source of statistics.\n   * @return the map of counters.",
  "org.apache.hadoop.fs.FileContext:getAllStatistics()" : "* @return Map of uri and statistics for each filesystem instantiated. The uri\n   *         consists of scheme and authority for the filesystem.",
  "org.apache.hadoop.net.DNS:getIPs(java.lang.String)" : "* @return Like {@link DNS#getIPs(String, boolean)}, but returns all\n   * IPs associated with the given interface and its subinterfaces.\n   *\n   * @param strInterface input strInterface.\n   * @throws UnknownHostException\n   * If no IP address for the local host could be found.",
  "org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable)" : "* Positions the reader at the named key, or if none such exists, at the\n     * first entry after the named key.\n     *\n     * @return  0   - exact match found\n     *          < 0 - positioned at next record\n     *          1   - no more records in file",
  "org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getPermission()" : null,
  "org.apache.hadoop.conf.StorageSize:getValue()" : null,
  "org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.fs.ChecksumFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)" : null,
  "org.apache.hadoop.io.compress.SnappyCodec:createOutputStream(java.io.OutputStream)" : "* Create a {@link CompressionOutputStream} that will write to the given\n   * {@link OutputStream}.\n   *\n   * @param out the location for the final output stream\n   * @return a stream the user can write uncompressed data to have it compressed\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.tracing.SpanContext:<init>()" : null,
  "org.apache.hadoop.fs.MultipartUploaderBuilder:build()" : "* Create the FSDataOutputStream to write on the file system.\n   *\n   * @throws IllegalArgumentException if the parameters are not valid.\n   * @throws IOException on errors when file system creates or appends the file.\n   * @return S Generics Type.",
  "org.apache.hadoop.fs.statistics.IOStatisticsContext:setThreadIOStatisticsContext(org.apache.hadoop.fs.statistics.IOStatisticsContext)" : "* Set the IOStatisticsContext for the current thread.\n   * @param statisticsContext IOStatistics context instance for the\n   * current thread. If null, the context is reset.",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthenticationSuccesses()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BufferPool:numCreated()" : null,
  "org.apache.hadoop.io.DataInputByteBuffer$Buffer:getLength()" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:<init>(java.io.DataInput)" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:getOptions()" : "* Get the mutable option configuration.\n   * @return the option configuration.",
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeysMetadata(java.lang.String[])" : null,
  "org.apache.hadoop.fs.shell.Tail:expandArgument(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.find.Find:getExpression(java.lang.Class)" : "Gets an instance of an expression from the factory.",
  "org.apache.hadoop.ha.HAAdmin:run(java.lang.String[])" : null,
  "org.apache.hadoop.ha.FailoverController:tryGracefulFence(org.apache.hadoop.ha.HAServiceTarget)" : "* Try to get the HA state of the node at the given address. This\n   * function is guaranteed to be \"quick\" -- ie it has a short timeout\n   * and no retries. Its only purpose is to avoid fencing a node that\n   * has already restarted.",
  "org.apache.hadoop.util.functional.Tuples$Tuple:equals(java.lang.Object)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsInfoImpl:toString()" : null,
  "org.apache.hadoop.metrics2.lib.MutableRates:init(java.lang.Class)" : "* Initialize the registry with all the methods in a protocol\n   * so they all show up in the first snapshot.\n   * Convenient for JMX implementations.\n   * @param protocol the protocol class",
  "org.apache.hadoop.io.InputBuffer:<init>(org.apache.hadoop.io.InputBuffer$Buffer)" : null,
  "org.apache.hadoop.net.StandardSocketFactory:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.FileSystem:resolveLink(org.apache.hadoop.fs.Path)" : "* See {@link AbstractFileSystem#getLinkTarget(Path)}.\n   * @param f the path.\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).\n   * @throws IOException IO failure.\n   * @return the path.",
  "org.apache.hadoop.util.HostsFileReader:refreshInternal(java.lang.String,java.lang.String,boolean)" : null,
  "org.apache.hadoop.metrics2.util.MetricsCache$Record:toString()" : null,
  "org.apache.hadoop.security.UserGroupInformation:logUserInfo(org.slf4j.Logger,java.lang.String,org.apache.hadoop.security.UserGroupInformation)" : "* Log current UGI and token information into specified log.\n   * @param ugi - UGI\n   * @param log log.\n   * @param caption caption.",
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getBytesRead()" : "* <p>Returns the total number of uncompressed bytes input so far.</p>\n   *\n   * @return the total (non-negative) number of uncompressed bytes input so far",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileSystem$Statistics:getBytesReadByDistance(int)" : "* In the common network topology setup, distance value should be an even\n     * number such as 0, 2, 4, 6. To make it more general, we group distance\n     * by {1, 2}, {3, 4} and {5 and beyond} for accounting. So if the caller\n     * ask for bytes read for distance 2, the function will return the value\n     * for group {1, 2}.\n     * @param distance the network distance\n     * @return the total number of bytes read by the network distance",
  "org.apache.hadoop.tracing.Span:close()" : null,
  "org.apache.hadoop.util.StringUtils:wrap(java.lang.String,int,java.lang.String,boolean)" : "* Same as WordUtils#wrap in commons-lang 2.6. Unlike commons-lang3, leading\n   * spaces on the first line are NOT stripped.\n   *\n   * @param str  the String to be word wrapped, may be null\n   * @param wrapLength  the column to wrap the words at, less than 1 is treated\n   *                   as 1\n   * @param newLineStr  the string to insert for a new line,\n   *  <code>null</code> uses the system property line separator\n   * @param wrapLongWords  true if long words (such as URLs) should be wrapped\n   * @return a line with newlines inserted, <code>null</code> if null input",
  "org.apache.hadoop.fs.permission.AclEntry$Builder:build()" : "* Builds a new AclEntry populated with the set properties.\n     *\n     * @return AclEntry new AclEntry",
  "org.apache.hadoop.io.compress.CompressorStream:resetState()" : null,
  "org.apache.hadoop.util.InstrumentedReadLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long)" : null,
  "org.apache.hadoop.security.CompositeGroupsMapping:getGroupsSet(java.lang.String)" : null,
  "org.apache.hadoop.fs.PositionedReadable:maxReadSizeForVectorReads()" : "* What is the largest size that we should group ranges together as?\n   * @return the number of bytes to read at once",
  "org.apache.hadoop.fs.FilterFs:getFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawValue(org.apache.hadoop.io.SequenceFile$ValueBytes)" : "* Fills up the passed rawValue with the value corresponding to the key\n       * read earlier.\n       * @param rawValue input ValueBytes rawValue.\n       * @return the length of the value\n       * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.BatchedRemoteIterator:hasNext()" : null,
  "org.apache.hadoop.ipc.Server$Connection:unwrapPacketAndProcessRpcs(byte[])" : "* Process a wrapped RPC Request - unwrap the SASL packet and process\n     * each embedded RPC request \n     * @param inBuf - SASL wrapped request of one or more RPCs\n     * @throws IOException - SASL packet cannot be unwrapped\n     * @throws InterruptedException",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupBlock()" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:getServiceName()" : "* Get the service name via {@link Service#getName()}.\n   *\n   * If the service is not instantiated, the classname is returned instead.\n   * @return the service name",
  "org.apache.hadoop.fs.viewfs.ViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:prepareDecoding(java.lang.Object[],int[])" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:setDictionary(byte[],int,int)" : null,
  "org.apache.hadoop.util.Sets:newHashSet(java.lang.Iterable)" : "* Creates a <i>mutable</i> {@code HashSet} instance containing the given\n   * elements. A very thin convenience for creating an empty set then calling\n   * {@link Collection#addAll} or Iterables#addAll.\n   *\n   * <p><b>Note:</b> if mutability is not required and the elements are\n   * non-null, use ImmutableSet#copyOf(Iterable) instead. (Or, change\n   * {@code elements} to be a FluentIterable and call {@code elements.toSet()}.)</p>\n   *\n   * <p><b>Note:</b> if {@code E} is an {@link Enum} type, use\n   * newEnumSet(Iterable, Class) instead.</p>\n   *\n   * @param <E> Generics Type E.\n   * @param elements the elements that the set should contain.\n   * @return a new, empty thread-safe {@code Set}.",
  "org.apache.hadoop.ipc.Client:getRetryCount()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:getRead(int)" : null,
  "org.apache.hadoop.security.NullGroupsMapping:getGroups(java.lang.String)" : "* Returns an empty list.\n   * @param user ignored\n   * @return an empty list",
  "org.apache.hadoop.metrics2.filter.AbstractPatternFilter:init(org.apache.commons.configuration2.SubsetConfiguration)" : null,
  "org.apache.hadoop.util.QuickSort:sortInternal(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable,int)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.io.BytesWritable:getCapacity()" : "* Get the capacity, which is the maximum size that could handled without\n   * resizing the backing storage.\n   *\n   * @return The number of bytes",
  "org.apache.hadoop.log.LogLevel$CLI:parseGetLevelArgs(java.lang.String[],int)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators:remoteIteratorFromIterable(java.lang.Iterable)" : "* Create a remote iterator from a java.util.Iterable -e.g. a list\n   * or other collection.\n   * @param <T> type\n   * @param iterable iterable.\n   * @return a remote iterator",
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:resetKeyStoreState(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.util.bloom.CountingBloomFilter:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Throwable)" : null,
  "org.apache.hadoop.fs.DelegateToFileSystem:supportsSymlinks()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.fs.FsShell:getTrash()" : null,
  "org.apache.hadoop.ipc.Client$IpcStreams:setSaslClient(org.apache.hadoop.security.SaslRpcClient)" : null,
  "org.apache.hadoop.conf.ReconfigurationUtil:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File)" : "* Delete a directory and all its contents.  If\n   * we return false, the directory may be partially-deleted.\n   * (1) If dir is symlink to a file, the symlink is deleted. The file pointed\n   *     to by the symlink is not deleted.\n   * (2) If dir is symlink to a directory, symlink is deleted. The directory\n   *     pointed to by symlink is not deleted.\n   * (3) If dir is a normal file, it is deleted.\n   * (4) If dir is a normal directory, then dir and all its contents recursively\n   *     are deleted.\n   * @param dir dir.\n   * @return fully delete status.",
  "org.apache.hadoop.fs.local.LocalConfigKeys:getServerDefaults()" : null,
  "org.apache.hadoop.fs.FileSystem:getStoragePolicy(org.apache.hadoop.fs.Path)" : "* Query the effective storage policy ID for the given file or directory.\n   *\n   * @param src file or directory path.\n   * @return storage policy for give file.\n   * @throws IOException IO failure\n   * @throws UnsupportedOperationException if the operation is unsupported\n   *         (default outcome).",
  "org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)" : "* Create an FSDataOutputStream at the indicated Path with a custom\n   * checksum option.\n   * @param f the file name to open\n   * @param permission file permission\n   * @param flags {@link CreateFlag}s to use for this stream.\n   * @param bufferSize the size of the buffer to be used.\n   * @param replication required block replication for the file.\n   * @param blockSize block size\n   * @param progress the progress reporter\n   * @param checksumOpt checksum parameter. If null, the values\n   *        found in conf will be used.\n   * @throws IOException IO failure\n   * @see #setPermission(Path, FsPermission)\n   * @return output stream.",
  "org.apache.hadoop.service.Service$STATE:getValue()" : "* Get the integer value of a state\n     * @return the numeric value of the state",
  "org.apache.hadoop.fs.HarFileSystem:checkPath(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.security.alias.LocalKeyStoreProvider:getOutputStreamForKeystore()" : null,
  "org.apache.hadoop.util.functional.TaskPool$Builder:<init>(java.lang.Iterable)" : "* Create the builder.\n     * @param items items to process",
  "org.apache.hadoop.fs.FilterFileSystem:setVerifyChecksum(boolean)" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$Reader:createReader(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion)" : null,
  "org.apache.hadoop.util.LightWeightResizableGSet:put(java.lang.Object)" : null,
  "org.apache.hadoop.security.alias.LocalKeyStoreProvider:modeToPosixFilePermission(int)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addTimedOperation(java.lang.String,long)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:parseBackOffResponseTimeThreshold(java.lang.String,org.apache.hadoop.conf.Configuration,int)" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler:getCallVolumeSummary()" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String,boolean)" : "* Add a tag to the metrics\n   * @param info  metadata of the tag\n   * @param value of the tag\n   * @param override existing tag if true\n   * @return the registry (for keep adding tags etc.)",
  "org.apache.hadoop.security.AccessControlException:<init>(java.lang.Throwable)" : "* Constructs a new exception with the specified cause and a detail\n   * message of <tt>(cause==null ? null : cause.toString())</tt> (which\n   * typically contains the class and detail message of <tt>cause</tt>).\n   * @param  cause the cause (which is saved for later retrieval by the\n   *         {@link #getCause()} method).  (A <tt>null</tt> value is\n   *         permitted, and indicates that the cause is nonexistent or\n   *         unknown.)",
  "org.apache.hadoop.fs.permission.PermissionStatus$1:<init>()" : null,
  "org.apache.hadoop.fs.FSInputChecker:getPos()" : null,
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$CachePutTask:get()" : null,
  "org.apache.hadoop.net.SocketIOWithTimeout:doIO(java.nio.ByteBuffer,int)" : "* Performs one IO and returns number of bytes read or written.\n   * It waits up to the specified timeout. If the channel is \n   * not read before the timeout, SocketTimeoutException is thrown.\n   * \n   * @param buf buffer for IO\n   * @param ops Selection Ops used for waiting. Suggested values: \n   *        SelectionKey.OP_READ while reading and SelectionKey.OP_WRITE while\n   *        writing. \n   *        \n   * @return number of bytes read or written. negative implies end of stream.\n   * @throws IOException",
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getFirstKey()" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:release()" : "* Should be called when release this coder. Good chance to release encoding\n   * or decoding buffers",
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeyVersion(java.lang.String)" : null,
  "org.apache.hadoop.ipc.Server$Connection:toString()" : null,
  "org.apache.hadoop.io.ShortWritable:<init>(short)" : null,
  "org.apache.hadoop.fs.FileSystemStorageStatistics:<init>(java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsLogging:mapToSortedString(java.lang.StringBuilder,java.lang.String,java.util.Map,java.util.function.Predicate)" : "* Given a map, produce a string with all the values, sorted.\n   * Needs to create a treemap and insert all the entries.\n   * @param sb string buffer to append to\n   * @param type type (for output)\n   * @param map map to evaluate\n   * @param <E> type of values of the map",
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation:getTimestamp()" : null,
  "org.apache.hadoop.io.compress.BlockCompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int,int)" : "* Create a {@link BlockCompressorStream}.\n   * \n   * @param out stream\n   * @param compressor compressor to be used\n   * @param bufferSize size of buffer\n   * @param compressionOverhead maximum 'overhead' of the compression \n   *                            algorithm with given bufferSize",
  "org.apache.hadoop.io.compress.bzip2.CRC:updateCRC(int)" : null,
  "org.apache.hadoop.io.file.tfile.TFileDumper$Align:calculateWidth(java.lang.String,long)" : null,
  "org.apache.hadoop.io.serializer.JavaSerialization:getSerializer(java.lang.Class)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:checkAndSkipBytes(int)" : null,
  "org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)" : "* Create the named map for keys of the named class.\n     * @deprecated Use Writer(Configuration, Path, Option...) instead.\n     *\n     * @param conf configuration.\n     * @param fs fs.\n     * @param dirName dirName.\n     * @param keyClass keyClass.\n     * @param valClass valClass.\n     * @param compress compress.\n     * @param progress progress.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.HarFileSystem:truncate(org.apache.hadoop.fs.Path,long)" : "* Not implemented.",
  "org.apache.hadoop.fs.AbstractFileSystem:checkPath(org.apache.hadoop.fs.Path)" : "* Check that a Path belongs to this FileSystem.\n   * \n   * If the path is fully qualified URI, then its scheme and authority\n   * matches that of this file system. Otherwise the path must be \n   * slash-relative name.\n   * @param path the path.\n   * @throws InvalidPathException if the path is invalid",
  "org.apache.hadoop.io.SequenceFile$Metadata:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfInvocation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)" : "* Given an IOException raising callable/lambda expression,\n   * execute it and update the relevant statistic.\n   * @param factory factory of duration trackers\n   * @param statistic statistic key\n   * @param input input callable.\n   * @throws IOException IO failure.",
  "org.apache.hadoop.io.retry.AsyncCallHandler:newAsyncCall(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler)" : null,
  "org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.ipc.Server$RpcCall:getHostInetAddress()" : null,
  "org.apache.hadoop.security.http.CrossOriginFilter:initializeMaxAge(javax.servlet.FilterConfig)" : null,
  "org.apache.hadoop.fs.local.LocalFs:<init>(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.ExitUtil$ExitException:<init>(int,java.lang.Throwable)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$CachePutTask:<init>(org.apache.hadoop.fs.impl.prefetch.BufferData,java.util.concurrent.Future,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager,java.time.Instant)" : null,
  "org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:printAclEntriesForSingleScope(org.apache.hadoop.fs.permission.AclStatus,org.apache.hadoop.fs.permission.FsPermission,java.util.List)" : "* Prints all the ACL entries in a single scope.\n     * @param aclStatus AclStatus for the path\n     * @param fsPerm FsPermission for the path\n     * @param entries List<AclEntry> containing ACL entries of file",
  "org.apache.hadoop.io.erasurecode.CodecUtil:createEncoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)" : "* Create encoder corresponding to given codec.\n   * @param options Erasure codec options\n   * @param conf configuration.\n   * @return erasure encoder",
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:hasCapability(java.lang.String)" : "* Probe the inner stream for a capability.\n     * Syncable operations are rejected before being passed down.\n     * @param capability string to query the stream support for.\n     * @return true if a capability is known to be supported.",
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:cleanupAllTmpFiles()" : null,
  "org.apache.hadoop.fs.FsShell:printHelp(java.io.PrintStream)" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:cleanBufferPool()" : "Clean direct buffer pool",
  "org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:getFallbackLink()" : null,
  "org.apache.hadoop.net.InnerNodeImpl:isRack()" : "Judge if this node represents a rack.\n   * @return true if it has no child or its children are not InnerNodes",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:toString()" : null,
  "org.apache.hadoop.fs.shell.PathData:equals(java.lang.Object)" : null,
  "org.apache.hadoop.fs.BufferedFSInputStream:getPos()" : null,
  "org.apache.hadoop.ipc.Server:join()" : "* Wait for the server to be stopped.\n   * Does not wait for all subthreads to finish.\n   *  See {@link #stop()}.\n   * @throws InterruptedException if the thread is interrupted.",
  "org.apache.hadoop.fs.impl.OpenFileParameters:getBufferSize()" : null,
  "org.apache.hadoop.security.UserGroupInformation:relogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean)" : null,
  "org.apache.hadoop.util.CrcUtil:readInt(byte[],int)" : "* Reads 4-byte big-endian int value from {@code buf} starting at\n   * {@code offset}. buf.length must be greater than or equal to offset + 4.\n   *\n   * @param offset offset.\n   * @param buf buf.\n   * @return int.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.GetSpaceUsed$Builder:getJitter()" : null,
  "org.apache.hadoop.util.ShutdownThreadsHelper:shutdownThread(java.lang.Thread,long)" : "* @param thread {@link Thread to be shutdown}\n   * @param timeoutInMilliSeconds time to wait for thread to join after being\n   *                              interrupted\n   * @return <tt>true</tt> if the thread is successfully interrupted,\n   * <tt>false</tt> otherwise",
  "org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])" : "* Set an xattr of a file or directory.\n   * The name must be prefixed with the namespace followed by \".\". For example,\n   * \"user.attr\".\n   * <p>\n   * Refer to the HDFS extended attributes user documentation for details.\n   *\n   * @param path Path to modify\n   * @param name xattr name.\n   * @param value xattr value.\n   * @throws IOException If an I/O error occurred.",
  "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:getPassFactor(int,int)" : null,
  "org.apache.hadoop.security.NullGroupsMapping:getGroupsSet(java.lang.String)" : "* Get all various group memberships of a given user.\n   * Returns EMPTY set in case of non-existing user\n   *\n   * @param user User's name\n   * @return set of group memberships of user\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.erasurecode.coder.util.HHUtil:<init>()" : null,
  "org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : "* Convenience method, so that we don't open a new connection when using this\n   * method from within another method. Otherwise every API invocation incurs\n   * the overhead of opening/closing a TCP connection.",
  "org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,boolean)" : null,
  "org.apache.hadoop.fs.FSDataInputStream:<init>(java.io.InputStream)" : null,
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(java.lang.String,java.lang.String,java.lang.String)" : "* Add a tag to the metrics\n   * @param name  of the tag\n   * @param description of the tag\n   * @param value of the tag\n   * @return the registry (for keep adding tags)",
  "org.apache.hadoop.fs.shell.Command:processPathInternal(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)" : null,
  "org.apache.hadoop.conf.Configuration:<init>(org.apache.hadoop.conf.Configuration)" : "* A new configuration with the same settings cloned from another.\n   * \n   * @param other the configuration from which to clone settings.",
  "org.apache.hadoop.io.MapWritable:toString()" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:incrementMaximum(java.lang.String,long)" : null,
  "org.apache.hadoop.security.Groups$GroupCacheLoader:load(java.lang.String)" : "* This method will block if a cache entry doesn't exist, and\n     * any subsequent requests for the same user will wait on this\n     * request to return. If a user already exists in the cache,\n     * and when the key expires, the first call to reload the key\n     * will block, but subsequent requests will return the old\n     * value until the blocking thread returns.\n     * If reloadGroupsInBackground is true, then the thread that\n     * needs to refresh an expired key will not block either. Instead\n     * it will return the old cache value and schedule a background\n     * refresh\n     * @param user key of cache\n     * @return List of groups belonging to user\n     * @throws IOException to prevent caching negative entries",
  "org.apache.hadoop.io.ArrayPrimitiveWritable:isDeclaredComponentType(java.lang.Class)" : null,
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newMBeanName(java.lang.String)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumCountWithFixedSleep(int,long,java.util.concurrent.TimeUnit)" : "* <p>\n   * Keep trying a limited number of times, waiting a fixed time between attempts,\n   * and then fail by re-throwing the exception.\n   * </p>\n   *\n   * @param maxRetries maxRetries.\n   * @param sleepTime sleepTime.\n   * @param timeUnit timeUnit.\n   * @return RetryPolicy.",
  "org.apache.hadoop.metrics2.sink.GraphiteSink:close()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:add(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation)" : null,
  "org.apache.hadoop.security.token.delegation.DelegationKey:getKeyId()" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:constructRpcRequest(java.lang.reflect.Method,com.google.protobuf.Message)" : null,
  "org.apache.hadoop.fs.shell.Ls:processPathArgument(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.util.DataChecksum:getChecksumSize()" : "* the size for a checksum.\n   * @return the size for a checksum.",
  "org.apache.hadoop.io.erasurecode.ECSchema:toString()" : "* Make a meaningful string representation for log output.\n   * @return string representation",
  "org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator:<init>()" : null,
  "org.apache.hadoop.fs.FileSystem:getFSofPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.RunJar:ensureDirectory(java.io.File)" : "* Ensure the existence of a given directory.\n   *\n   * @param dir Directory to check\n   *\n   * @throws IOException if it cannot be created and does not already exist",
  "org.apache.hadoop.fs.Options$CreateOpts:perms(org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.ha.HAServiceTarget:addFencingParameters(java.util.Map)" : "* Hook to allow subclasses to add any parameters they would like to\n   * expose to fencing implementations/scripts. Fencing methods are free\n   * to use this map as they see fit -- notably, the shell script\n   * implementation takes each entry, prepends 'target_', substitutes\n   * '_' for '.', and adds it to the environment of the script.\n   *\n   * Subclass implementations should be sure to delegate to the superclass\n   * implementation as well as adding their own keys.\n   *\n   * @param ret map which can be mutated to pass parameters to the fencer",
  "org.apache.hadoop.fs.FileUtil:readLink(java.io.File)" : "* Returns the target of the given symlink. Returns the empty string if\n   * the given path does not refer to a symlink or there is an error\n   * accessing the symlink.\n   * @param f File representing the symbolic link.\n   * @return The target of the symbolic link, empty string on error or if not\n   *         a symlink.",
  "org.apache.hadoop.fs.ContentSummary:getSnapshotFileCount()" : null,
  "org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer)" : "* Sort nodes array by network distance to <i>reader</i> with secondary sort.\n   * <p>\n   * In a three-level topology, a node can be either local, on the same rack,\n   * or on a different rack from the reader. Sorting the nodes based on network\n   * distance from the reader reduces network traffic and improves\n   * performance.\n   * </p>\n   * As an additional twist, we also randomize the nodes at each network\n   * distance. This helps with load balancing when there is data skew.\n   *\n   * @param reader    Node where data will be read\n   * @param nodes     Available replicas with the requested data\n   * @param activeLen Number of active nodes at the front of the array\n   * @param secondarySort a secondary sorting strategy which can inject into\n   *     that point from outside to help sort the same distance.\n   * @param <T> Generics Type T",
  "org.apache.hadoop.security.ShellBasedIdMapping:checkAndUpdateMaps()" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:<init>(org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode)" : null,
  "org.apache.hadoop.io.DataInputByteBuffer$Buffer:read()" : null,
  "org.apache.hadoop.fs.FileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)" : "* Set access time of a file.\n   * @param p The path\n   * @param mtime Set the modification time of this file.\n   *              The number of milliseconds since Jan 1, 1970.\n   *              A value of -1 means that this call should not set modification time.\n   * @param atime Set the access time of this file.\n   *              The number of milliseconds since Jan 1, 1970.\n   *              A value of -1 means that this call should not set access time.\n   * @throws IOException IO failure",
  "org.apache.hadoop.util.bloom.CountingBloomFilter:<init>()" : "Default constructor - use with readFields",
  "org.apache.hadoop.io.compress.Lz4Codec:createInputStream(java.io.InputStream)" : "* Create a {@link CompressionInputStream} that will read from the given\n   * input stream.\n   *\n   * @param in the stream to read compressed bytes from\n   * @return a stream to read uncompressed bytes from\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$4:check(java.lang.String[],java.lang.String[],java.lang.String[])" : "* Checks to see if the supplied hostname matches any of the supplied CNs\n     * or \"DNS\" Subject-Alts.  Most implementations only look at the first CN,\n     * and ignore any additional CNs.  Most implementations do look at all of\n     * the \"DNS\" Subject-Alts. The CNs or Subject-Alts may contain wildcards\n     * according to RFC 2818.\n     *\n     * @param cns         CN fields, in order, as extracted from the X.509\n     *                    certificate.\n     * @param subjectAlts Subject-Alt fields of type 2 (\"DNS\"), as extracted\n     *                    from the X.509 certificate.\n     * @param hosts       The array of hostnames to verify.\n     * @throws SSLException If verification failed.",
  "org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getGroup()" : null,
  "org.apache.hadoop.service.AbstractService:getServiceState()" : null,
  "org.apache.hadoop.conf.Configured:setConf(org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:checkCreateXorRawEncoder()" : null,
  "org.apache.hadoop.fs.FileUtil:setWritable(java.io.File,boolean)" : "* Platform independent implementation for {@link File#setWritable(boolean)}\n   * File#setWritable does not work as expected on Windows.\n   * @param f input file\n   * @param writable writable.\n   * @return true on success, false otherwise",
  "org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,java.lang.String)" : "* interface implementation of Zookeeper callback for create",
  "org.apache.hadoop.io.VIntWritable:set(int)" : "* Set the value of this VIntWritable.\n   * @param value input value.",
  "org.apache.hadoop.io.serializer.avro.AvroSerialization$AvroSerializer:serialize(java.lang.Object)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getModificationTime()" : null,
  "org.apache.hadoop.io.compress.ZStandardCodec:checkNativeCodeLoaded()" : null,
  "org.apache.hadoop.io.MD5Hash:digest(java.lang.String)" : "* Construct a hash value for a String.\n   * @param string string.\n   * @return MD5Hash.",
  "org.apache.hadoop.fs.FilterFileSystem:getHomeDirectory()" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$CallInfo:<init>(org.apache.hadoop.ipc.RPC$Server,java.lang.String)" : null,
  "org.apache.hadoop.metrics2.source.JvmMetricsInfo:description()" : null,
  "org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies:calculateExponentialTime(long,int)" : null,
  "org.apache.hadoop.metrics2.lib.MutableCounterInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(byte[],int)" : "* Copy value into user-supplied buffer. User supplied buffer must be\n         * large enough to hold the whole value (starting from the offset). The\n         * value part of the key-value pair pointed by the current cursor is not\n         * cached and can only be examined once. Calling any of the following\n         * functions more than once without moving the cursor will result in\n         * exception: {@link #getValue(byte[])}, {@link #getValue(byte[], int)},\n         * {@link #getValueStream}.\n         *\n         * @param buf buf.\n         * @param offset offset.\n         * @return the length of the value. Does not require\n         *         isValueLengthKnown() to be true.\n         * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMaximumFunction(java.lang.String,java.util.function.Function)" : "* add a mapping of a key to a maximum function.\n   * @param key the key\n   * @param eval the evaluator",
  "org.apache.hadoop.util.HostsFileReader:getHostDetails()" : "* Retrieve an atomic view of the included and excluded hosts.\n   *\n   * @return the included and excluded hosts",
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:buildChecked(java.lang.Object)" : "* Returns the first valid implementation as a BoundMethod or throws a\n     * NoSuchMethodException if there is none.\n     * @param receiver an Object to receive the method invocation\n     * @return a {@link BoundMethod} with a valid implementation and receiver\n     * @throws IllegalStateException if the method is static\n     * @throws IllegalArgumentException if the receiver's class is incompatible\n     * @throws NoSuchMethodException if no implementation was found",
  "org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeSSLContext(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode)" : null,
  "org.apache.hadoop.util.Shell:getHadoopHomeDir()" : "* Get the Hadoop home directory. If it is invalid,\n   * throw an exception.\n   * @return a path referring to hadoop home.\n   * @throws FileNotFoundException if the directory doesn't exist.",
  "org.apache.hadoop.fs.FilterFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* Renames Path src to Path dst.  Can take place on local fs\n   * or remote DFS.",
  "org.apache.hadoop.fs.FSInputChecker:available()" : null,
  "org.apache.hadoop.fs.FilterFs:getDelegationTokens(java.lang.String)" : null,
  "org.apache.hadoop.io.DataOutputBuffer:<init>(int)" : null,
  "org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File)" : "* Delete the contents of a directory, not the directory itself.  If\n   * we return false, the directory may be partially-deleted.\n   * If dir is a symlink to a directory, all the contents of the actual\n   * directory pointed to by dir will be deleted.\n   *\n   * @param dir dir.\n   * @return fullyDeleteContents Status.",
  "org.apache.hadoop.fs.statistics.MeanStatistic:getSum()" : "* Get the sum of samples.\n   * @return the sum",
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:getGaugeReference(java.lang.String)" : null,
  "org.apache.hadoop.fs.FileStatus:getAccessTime()" : "* Get the access time of the file.\n   * @return the access time of file in milliseconds since January 1, 1970 UTC.",
  "org.apache.hadoop.net.CachedDNSToSwitchMapping:resolve(java.util.List)" : null,
  "org.apache.hadoop.fs.permission.UmaskParser:getUMask()" : "* To be used for file/directory creation only. Symbolic umask is applied\n   * relative to file mode creation mask; the permission op characters '+'\n   * results in clearing the corresponding bit in the mask, '-' results in bits\n   * for indicated permission to be set in the mask.\n   * \n   * For octal umask, the specified bits are set in the file mode creation mask.\n   * \n   * @return umask",
  "org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)" : "* The method sends metrics to Ganglia servers. The method has been taken from\n   * org.apache.hadoop.metrics.ganglia.GangliaContext30 with minimal changes in\n   * order to keep it in sync.\n   * @param groupName The group name of the metric\n   * @param name The metric name\n   * @param type The type of the metric\n   * @param value The value of the metric\n   * @param gConf The GangliaConf for this metric\n   * @param gSlope The slope for this metric\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:close()" : null,
  "org.apache.hadoop.util.GenericOptionsParser:preProcessForWindows(java.lang.String[])" : "* Windows powershell and cmd can parse key=value themselves, because\n   * /pkey=value is same as /pkey value under windows. However this is not\n   * compatible with how we get arbitrary key values in -Dkey=value format.\n   * Under windows -D key=value or -Dkey=value might be passed as\n   * [-Dkey, value] or [-D key, value]. This method does undo these and\n   * return a modified args list by manually changing [-D, key, value]\n   * into [-D, key=value]\n   *\n   * @param args command line arguments\n   * @return fixed command line arguments that GnuParser can parse",
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:executeHeaderState()" : "* Parse the gzip header (assuming we're in the appropriate state).\n   * In order to deal with degenerate cases (e.g., user buffer is one byte\n   * long), we copy (some) header bytes to another buffer.  (Filename,\n   * comment, and extra-field bytes are simply skipped.)</p>\n   *\n   * See http://www.ietf.org/rfc/rfc1952.txt for the gzip spec.  Note that\n   * no version of gzip to date (at least through 1.4.0, 2010-01-20) supports\n   * the FHCRC header-CRC16 flagbit; instead, the implementation treats it\n   * as a multi-file continuation flag (which it also doesn't support). :-(\n   * Sun's JDK v6 (1.6) supports the header CRC, however, and so do we.",
  "org.apache.hadoop.fs.FilterFileSystem:truncate(org.apache.hadoop.fs.Path,long)" : null,
  "org.apache.hadoop.conf.StorageUnit$7:toKBs(double)" : null,
  "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createKeyManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,long)" : "* Implements logic of initializing the KeyManagers with the options\n   * to reload keystores.\n   * @param mode client or server\n   * @param keystoreType The keystore type.\n   * @param storesReloadInterval The interval to check if the keystore certificates\n   *                             file has changed.",
  "org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getRenewer()" : null,
  "org.apache.hadoop.io.file.tfile.BCFile$DataIndex:<init>(java.lang.String)" : null,
  "org.apache.hadoop.util.GcTimeMonitor:<init>(long,long,int,org.apache.hadoop.util.GcTimeMonitor$GcTimeAlertHandler)" : "* Create an instance of GCTimeMonitor. Once it's started, it will stay alive\n   * and monitor GC time percentage until shutdown() is called. If you don't\n   * put a limit on the number of GCTimeMonitor instances that you create, and\n   * alertHandler != null, you should necessarily call shutdown() once the given\n   * instance is not needed. Otherwise, you may create a memory leak, because\n   * each running GCTimeMonitor will keep its alertHandler object in memory,\n   * which in turn may reference and keep in memory many more other objects.\n   *\n   * @param observationWindowMs the interval over which the percentage\n   *   of GC time should be calculated. A practical value would be somewhere\n   *   between 30 sec and several minutes.\n   * @param sleepIntervalMs how frequently this thread should wake up to check\n   *   GC timings. This is also a frequency with which alertHandler will be\n   *   invoked if GC time percentage exceeds the specified limit. A practical\n   *   value would likely be 500..1000 ms.\n   * @param maxGcTimePercentage A GC time percentage limit (0..100) within\n   *   observationWindowMs. Once this is exceeded, alertHandler will be\n   *   invoked every sleepIntervalMs milliseconds until GC time percentage\n   *   falls below this limit.\n   * @param alertHandler a single method in this interface is invoked when GC\n   *   time percentage exceeds the specified limit.",
  "org.apache.hadoop.ha.ActiveStandbyElector:reJoinElection(int)" : null,
  "org.apache.hadoop.util.curator.ZKCuratorManager:getZKAuths(org.apache.hadoop.conf.Configuration)" : "* Utility method to fetch ZK auth info from the configuration.\n   *\n   * @param conf configuration.\n   * @throws java.io.IOException if the Zookeeper ACLs configuration file\n   * cannot be read\n   * @throws ZKUtil.BadAuthFormatException if the auth format is invalid\n   * @return ZKAuthInfo List.",
  "org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.io.retry.RetryPolicy,long)" : "* Get a protocol proxy that contains a proxy connection to a remote server\n   * and a set of methods that are supported by the server.\n   *\n   * @param <T> Generics Type.\n   * @param protocol protocol class\n   * @param clientVersion client version\n   * @param addr remote address\n   * @param conf configuration to use\n   * @param rpcTimeout timeout for each RPC\n   * @param connectionRetryPolicy input connectionRetryPolicy.\n   * @param timeout time in milliseconds before giving up\n   * @return the proxy\n   * @throws IOException if the far end through a RemoteException.",
  "org.apache.hadoop.fs.FileSystem$Statistics$8:<init>(org.apache.hadoop.fs.FileSystem$Statistics)" : "* Copy constructor.\n     *\n     * @param other    The input Statistics object which is cloned.",
  "org.apache.hadoop.ha.ActiveStandbyElector:isSuccess(org.apache.zookeeper.KeeperException$Code)" : null,
  "org.apache.hadoop.security.Credentials:readTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)" : "* Convenience method for reading a token storage file and loading its Tokens.\n   * @param filename filename.\n   * @param conf configuration.\n   * @throws IOException  raised on errors performing I/O.\n   * @return Credentials.",
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkIfPropertyExists(java.lang.String)" : "* Throw a {@link MetricsException} if the given property is not set.\n   *\n   * @param key the key to validate",
  "org.apache.hadoop.fs.AbstractFileSystem:clearStatistics()" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination:getLocalDestination(java.util.LinkedList)" : "*  The last arg is expected to be a local path, if only one argument is\n   *  given then the destination will be the current directory \n   *  @param args is the list of arguments\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_minimums(java.io.Serializable)" : "* Get the minimums of an IOStatisticsSnapshot.\n   * @param source source of statistics.\n   * @return the map of minimums.",
  "org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:isSupported()" : null,
  "org.apache.hadoop.crypto.key.CachingKeyProvider:getMetadata(java.lang.String)" : null,
  "org.apache.hadoop.io.retry.LossyRetryInvocationHandler:<init>(int,org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)" : null,
  "org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget)" : null,
  "org.apache.hadoop.fs.FileAlreadyExistsException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter:init(javax.servlet.FilterConfig)" : null,
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numReadErrors()" : "* Number of errors encountered when reading.\n   *\n   * @return the number of errors encountered when reading.",
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:writeFile(java.nio.file.Path,java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem:createFile(org.apache.hadoop.fs.Path)" : "* This is overridden to ensure that this class's create() method is\n   * ultimately called.\n   *\n   * {@inheritDoc}",
  "org.apache.hadoop.metrics2.lib.MutableGauge:<init>(org.apache.hadoop.metrics2.MetricsInfo)" : null,
  "org.apache.hadoop.security.SecurityUtil:doAsLoginUser(java.security.PrivilegedExceptionAction)" : "* Perform the given action as the daemon's login user. If an\n   * InterruptedException is thrown, it is converted to an IOException.\n   *\n   * @param action the action to perform\n   * @param <T> Generics Type T.\n   * @return the result of the action\n   * @throws IOException in the event of error",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:incrRequeueCalls()" : "* Increments the Requeue Calls counter.",
  "org.apache.hadoop.fs.VectoredReadUtils:readVectored(org.apache.hadoop.fs.PositionedReadable,java.util.List,java.util.function.IntFunction)" : "* This is the default implementation which iterates through the ranges\n   * to read each synchronously, but the intent is that subclasses\n   * can make more efficient readers.\n   * The data or exceptions are pushed into {@link FileRange#getData()}.\n   * @param stream the stream to read the data from\n   * @param ranges the byte ranges to read\n   * @param allocate the byte buffer allocation\n   * @throws IllegalArgumentException if there are overlapping ranges or a range is invalid\n   * @throws EOFException the range offset is negative",
  "org.apache.hadoop.ipc.Server$Listener$Reader:addConnection(org.apache.hadoop.ipc.Server$Connection)" : "* Updating the readSelector while it's being used is not thread-safe,\n       * so the connection must be queued.  The reader will drain the queue\n       * and update its readSelector before performing the next select",
  "org.apache.hadoop.util.KMSUtil:<init>()" : null,
  "org.apache.hadoop.ha.ShellCommandFencer:parseArgs(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState,java.lang.String)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:close()" : null,
  "org.apache.hadoop.util.LineReader:fillBuffer(java.io.InputStream,byte[],boolean)" : null,
  "org.apache.hadoop.fs.FSOutputSummer:createWriteTraceScope()" : null,
  "org.apache.hadoop.fs.viewfs.RegexMountPoint:initialize()" : "* Initialize regex mount point.\n   *\n   * @throws IOException",
  "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:fenceOldActive(byte[])" : null,
  "org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.AbstractMetric)" : null,
  "org.apache.hadoop.fs.FileUtil:makeShellPath(java.lang.String)" : "* Convert a os-native filename to a path that works for the shell.\n   * @param filename The filename to convert\n   * @return The unix pathname\n   * @throws IOException on windows, there can be problems with the subprocess",
  "org.apache.hadoop.crypto.CryptoInputStream:resetStreamOffset(long)" : "* Reset the underlying stream offset; clear {@link #inBuffer} and \n   * {@link #outBuffer}. This Typically happens during {@link #seek(long)} \n   * or {@link #skip(long)}.",
  "org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>(java.lang.Class,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)" : "* This constructor takes a connectionId, instead of creating a new one.\n     * @param protocol input protocol.\n     * @param connId input connId.\n     * @param conf input Configuration.\n     * @param factory input factory.\n     * @param alignmentContext Alignment context",
  "org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxyGroups()" : null,
  "org.apache.hadoop.crypto.CryptoInputStream:read(long,byte[],int,int)" : "Positioned read. It is thread-safe",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.io.IOUtils:closeSocket(java.net.Socket)" : "* Closes the socket ignoring {@link IOException}\n   *\n   * @param sock the Socket to close",
  "org.apache.hadoop.ha.ActiveStandbyElector:zkDoWithRetries(org.apache.hadoop.ha.ActiveStandbyElector$ZKAction)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:getPos()" : null,
  "org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsIDForUserCommand(java.lang.String)" : "* Returns just the shell command to be used to fetch a user's group IDs list.\n   * This is mainly separate to make some tests easier.\n   * @param userName The username that needs to be passed into the command built\n   * @return An appropriate shell command with arguments",
  "org.apache.hadoop.io.retry.RetryPolicies$RemoteExceptionDependentRetry:shouldRetry(java.lang.Exception,int,int,boolean)" : null,
  "org.apache.hadoop.io.compress.ZStandardCodec:createCompressor()" : "* Create a new {@link Compressor} for use by this {@link CompressionCodec}.\n   *\n   * @return a new compressor for use by this codec",
  "org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)" : null,
  "org.apache.hadoop.util.DiskChecker:checkDirInternal(java.io.File)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion:<init>(long,long,boolean)" : null,
  "org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:end()" : null,
  "org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:newAttrInfo(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider$Options:setCipher(java.lang.String)" : null,
  "org.apache.hadoop.security.Credentials:<init>(org.apache.hadoop.security.Credentials)" : "* Create a copy of the given credentials.\n   * @param credentials to copy",
  "org.apache.hadoop.io.EnumSetWritable:iterator()" : null,
  "org.apache.hadoop.io.SequenceFile$Reader:nextRaw(org.apache.hadoop.io.DataOutputBuffer,org.apache.hadoop.io.SequenceFile$ValueBytes)" : "* Read 'raw' records.\n     * @param key - The buffer into which the key is read\n     * @param val - The 'raw' value\n     * @return Returns the total record length or -1 for end of file\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.lib.MetricsRegistry:newStat(java.lang.String,java.lang.String,java.lang.String,java.lang.String)" : "* Create a mutable metric with stats\n   * @param name  of the metric\n   * @param desc  metric description\n   * @param sampleName  of the metric (e.g., \"Ops\")\n   * @param valueName   of the metric (e.g., \"Time\" or \"Latency\")\n   * @return a new mutable metric object",
  "org.apache.hadoop.io.MD5Hash:digest(byte[])" : "* Construct a hash value for a byte array.\n   * @param data data.\n   * @return MD5Hash.",
  "org.apache.hadoop.ipc.RPC:getSuperInterfaces(java.lang.Class[])" : "* Get all superInterfaces that extend VersionedProtocol\n   * @param childInterfaces\n   * @return the super interfaces that extend VersionedProtocol",
  "org.apache.hadoop.crypto.key.KeyProvider:generateKey(int,java.lang.String)" : "* Generates a key material.\n   *\n   * @param size length of the key.\n   * @param algorithm algorithm to use for generating the key.\n   * @return the generated key.\n   * @throws NoSuchAlgorithmException no such algorithm exception.",
  "org.apache.hadoop.io.MD5Hash:digest(org.apache.hadoop.io.UTF8)" : "* Construct a hash value for a String.\n   * @param utf8 utf8.\n   * @return MD5Hash.",
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:minimums()" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsW(int,int)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:construct(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader,int)" : null,
  "org.apache.hadoop.io.MD5Hash:equals(java.lang.Object)" : "Returns true iff <code>o</code> is an MD5Hash whose digest contains the\n   * same values.",
  "org.apache.hadoop.security.UserGroupInformation:getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation)" : "* Returns the authentication method of a ugi. If the authentication method is\n   * PROXY, returns the authentication method of the real user.\n   * \n   * @param ugi ugi.\n   * @return AuthenticationMethod",
  "org.apache.hadoop.util.SequentialNumber:<init>(long)" : "* Create a new instance with the given initial value.\n   * @param initialValue initialValue.",
  "org.apache.hadoop.security.http.RestCsrfPreventionFilter:parseMethodsToIgnore(java.lang.String)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:<init>(org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto,org.apache.hadoop.thirdparty.protobuf.Message)" : null,
  "org.apache.hadoop.tracing.TraceScope:reattach()" : null,
  "org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:toString()" : null,
  "org.apache.hadoop.io.DataOutputBuffer:<init>()" : "Constructs a new empty buffer.",
  "org.apache.hadoop.fs.FilterFileSystem:delete(org.apache.hadoop.fs.Path,boolean)" : "Delete a file",
  "org.apache.hadoop.ipc.Server:getRemoteAddress()" : "@return Returns remote address as a string when invoked inside an RPC.\n   *  Returns null in case of an error.",
  "org.apache.hadoop.service.launcher.InterruptEscalator:register(java.lang.String)" : "* Register an interrupt handler.\n   * @param signalName signal name\n   * @throws IllegalArgumentException if the registration failed",
  "org.apache.hadoop.io.ArrayPrimitiveWritable:writeIntArray(java.io.DataOutput)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:preferDirectBuffer()" : null,
  "org.apache.hadoop.io.compress.PassthroughCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:hbAssignCodes(int[],byte[],int,int,int)" : null,
  "org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroups(java.lang.String)" : "* Returns list of groups for a user.\n     * This calls {@link LdapGroupsMapping}'s getGroups and applies the\n     * configured rules on group names before returning.\n     *\n     * @param user get groups for this user\n     * @return list of groups for a given user",
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:meanStatistics()" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:close()" : null,
  "org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:write(byte[],int,int)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:invalidateCache(java.lang.String)" : null,
  "org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:counters()" : null,
  "org.apache.hadoop.util.PriorityQueue:size()" : "* Returns the number of elements currently stored in the PriorityQueue.\n   *\n   * @return size.",
  "org.apache.hadoop.ipc.Server:getNumOpenConnections()" : "* The number of open RPC conections\n   * @return the number of open rpc connections",
  "org.apache.hadoop.fs.VectoredReadUtils:readNonByteBufferPositionedReadable(org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer)" : "* Read into a direct tor indirect buffer using {@code PositionedReadable.readFully()}.\n   * @param stream stream\n   * @param range file range\n   * @param buffer destination buffer\n   * @throws IOException IO problems.",
  "org.apache.hadoop.fs.GlobFilter:<init>(java.lang.String)" : "* Creates a glob filter with the specified file pattern.\n   *\n   * @param filePattern the file pattern.\n   * @throws IOException thrown if the file pattern is incorrect.",
  "org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int,int,long,long)" : null,
  "org.apache.hadoop.fs.audit.CommonAuditContext:currentThreadID()" : "* A thread ID which is unique for this process and shared across all\n   * S3A clients on the same thread, even those using different FS instances.\n   * @return a thread ID for reporting.",
  "org.apache.hadoop.security.FastSaslServerFactory:createSaslServer(java.lang.String,java.lang.String,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler)" : null,
  "org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingMean()" : "* Returns mean of RPC Processing Times.\n   * @return double",
  "org.apache.hadoop.security.token.DtUtilShell$Get:execute()" : null,
  "org.apache.hadoop.net.SocketInputStream:read(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceNext()" : "* Get the next source value.\n     * This calls {@link #sourceHasNext()} first to verify\n     * that there is data.\n     * @return the next value\n     * @throws IOException failure\n     * @throws NoSuchElementException no more data",
  "org.apache.hadoop.fs.AbstractFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path)" : "* Removes all default ACL entries from files and directories.\n   *\n   * @param path Path to modify\n   * @throws IOException if an ACL could not be modified",
  "org.apache.hadoop.fs.FSDataInputStream:releaseBuffer(java.nio.ByteBuffer)" : null,
  "org.apache.hadoop.ha.HAAdmin:checkHealth(org.apache.commons.cli.CommandLine)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getRawFileSystem()" : "* Get the raw file system \n   * @return FileSystem being filtered",
  "org.apache.hadoop.fs.viewfs.InodeTree:resolve(java.lang.String,boolean)" : "* Resolve the pathname p relative to root InodeDir.\n   * @param p - input path\n   * @param resolveLastComponent resolveLastComponent.\n   * @return ResolveResult which allows further resolution of the remaining path\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.AbstractFileSystem:resolvePath(org.apache.hadoop.fs.Path)" : "* Return the fully-qualified path of path f resolving the path\n   * through any internal symlinks or mount point\n   * @param p path to be resolved\n   * @return fully qualified path \n   * @throws FileNotFoundException when file not find throw.\n   * @throws AccessControlException when accees control error throw.\n   * @throws IOException raised on errors performing I/O.\n   * @throws UnresolvedLinkException if symbolic link on path cannot be\n   * resolved internally",
  "org.apache.hadoop.util.BlockingThreadPoolExecutorService:getActiveCount()" : "* Get the actual number of active threads.\n   * @return the active thread count",
  "org.apache.hadoop.fs.shell.find.Result:isPass()" : "* Should processing continue.\n   * @return if is pass true,not false.",
  "org.apache.hadoop.fs.FilterFs:supportsSymlinks()" : null,
  "org.apache.hadoop.util.hash.JenkinsHash:hash(byte[],int,int)" : "* taken from  hashlittle() -- hash a variable-length key into a 32-bit value\n   * \n   * @param key the key (the unaligned variable-length array of bytes)\n   * @param nbytes number of bytes to include in hash\n   * @param initval can be any integer value\n   * @return a 32-bit value.  Every bit of the key affects every bit of the\n   * return value.  Two keys differing by one or two bits will have totally\n   * different hash values.\n   * \n   * <p>The best hash table sizes are powers of 2.  There is no need to do mod\n   * a prime (mod is sooo slow!).  If you need less than 32 bits, use a bitmask.\n   * For example, if you need only 10 bits, do\n   * <code>h = (h &amp; hashmask(10));</code>\n   * In which case, the hash table should have hashsize(10) elements.\n   * \n   * <p>If you are hashing n strings byte[][] k, do it like this:\n   * for (int i = 0, h = 0; i &lt; n; ++i) h = hash( k[i], h);\n   * \n   * <p>By Bob Jenkins, 2006.  bob_jenkins@burtleburtle.net.  You may use this\n   * code any way you wish, private, educational, or commercial.  It's free.\n   * \n   * <p>Use for hash table lookup, or anything where one collision in 2^^32 is\n   * acceptable.  Do NOT use for cryptographic purposes.",
  "org.apache.hadoop.metrics2.lib.MutableGaugeInt:incr()" : null,
  "org.apache.hadoop.fs.FsShell:getFS()" : null,
  "org.apache.hadoop.fs.HarFileSystem$Store:<init>(long,long)" : null,
  "org.apache.hadoop.ipc.Client$ConnectionId:toString()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:getSummary(java.lang.StringBuilder)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:fullPath(org.apache.hadoop.fs.Path)" : "* \n   * @param path\n   * @return return full path including the chroot",
  "org.apache.hadoop.ipc.WritableRpcEngine$Invocation:getMethodName()" : "The name of the method invoked.",
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:offset()" : null,
  "org.apache.hadoop.io.compress.BZip2Codec:createCompressor()" : "* Create a new {@link Compressor} for use by this {@link CompressionCodec}.\n   *\n   * @return a new compressor for use by this codec",
  "org.apache.hadoop.io.compress.zstd.ZStandardCompressor:setDictionary(byte[],int,int)" : null,
  "org.apache.hadoop.io.erasurecode.ErasureCodecOptions:<init>(org.apache.hadoop.io.erasurecode.ECSchema)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.shell.find.FindOptions:setOut(java.io.PrintStream)" : "* Sets the output stream to be used.\n   *\n   * @param out output stream to be used",
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:tagName(java.lang.String,int)" : null,
  "org.apache.hadoop.io.MapWritable:values()" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree$INode:isLink()" : null,
  "org.apache.hadoop.ha.HAServiceStatus:getState()" : null,
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:entryToString(java.lang.String,java.lang.Object)" : "* Convert entry values to the string format used in logging.\n   *\n   * @param <E> type of values.\n   * @param name statistic name\n   * @param value stat value\n   * @return formatted string",
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion:getAddress()" : null,
  "org.apache.hadoop.fs.shell.find.Name:<init>(boolean)" : "* Construct a Name {@link Expression} with a specified case sensitivity.\n   *\n   * @param caseSensitive if true the comparisons are case sensitive.",
  "org.apache.hadoop.conf.Configuration:getLocalPath(java.lang.String,java.lang.String)" : "* Get a local file under a directory named by <i>dirsProp</i> with\n   * the given <i>path</i>.  If <i>dirsProp</i> contains multiple directories,\n   * then one is chosen based on <i>path</i>'s hash code.  If the selected\n   * directory does not exist, an attempt is made to create it.\n   *\n   * @param dirsProp directory in which to locate the file.\n   * @param path file-path.\n   * @return local file under the directory with the given path.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:execute()" : null,
  "org.apache.hadoop.util.Sets:symmetricDifference(java.util.Set,java.util.Set)" : "* Returns the symmetric difference of two sets as an unmodifiable set.\n   * The returned set contains all elements that are contained in either\n   * {@code set1} or {@code set2} but not in both. The iteration order of the\n   * returned set is undefined.\n   *\n   * <p>Results are undefined if {@code set1} and {@code set2} are sets based\n   * on different equivalence relations (as {@code HashSet}, {@code TreeSet},\n   * and the keySet of an {@code IdentityHashMap} all are).\n   *\n   * @param set1 set1.\n   * @param set2 set2.\n   * @param <E> Generics Type E.\n   * @return a new, empty thread-safe {@code Set}.",
  "org.apache.hadoop.util.JvmPauseMonitor:getGcTimes()" : null,
  "org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getUniqueIdentityCount()" : null,
  "org.apache.hadoop.fs.FileContext:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)" : "* Creates a symbolic link to an existing file. An exception is thrown if \n   * the symlink exits, the user does not have permission to create symlink,\n   * or the underlying file system does not support symlinks.\n   * \n   * Symlink permissions are ignored, access to a symlink is determined by\n   * the permissions of the symlink target.\n   * \n   * Symlinks in paths leading up to the final path component are resolved \n   * transparently. If the final path component refers to a symlink some \n   * functions operate on the symlink itself, these are:\n   * - delete(f) and deleteOnExit(f) - Deletes the symlink.\n   * - rename(src, dst) - If src refers to a symlink, the symlink is \n   *   renamed. If dst refers to a symlink, the symlink is over-written.\n   * - getLinkTarget(f) - Returns the target of the symlink. \n   * - getFileLinkStatus(f) - Returns a FileStatus object describing\n   *   the symlink.\n   * Some functions, create() and mkdir(), expect the final path component\n   * does not exist. If they are given a path that refers to a symlink that \n   * does exist they behave as if the path referred to an existing file or \n   * directory. All other functions fully resolve, ie follow, the symlink. \n   * These are: open, setReplication, setOwner, setTimes, setWorkingDirectory,\n   * setPermission, getFileChecksum, setVerifyChecksum, getFileBlockLocations,\n   * getFsStatus, getFileStatus, exists, and listStatus.\n   * \n   * Symlink targets are stored as given to createSymlink, assuming the \n   * underlying file system is capable of storing a fully qualified URI.\n   * Dangling symlinks are permitted. FileContext supports four types of \n   * symlink targets, and resolves them as follows\n   * <pre>\n   * Given a path referring to a symlink of form:\n   * \n   *   {@literal <---}X{@literal --->}\n   *   fs://host/A/B/link \n   *   {@literal <-----}Y{@literal ----->}\n   * \n   * In this path X is the scheme and authority that identify the file system,\n   * and Y is the path leading up to the final path component \"link\". If Y is\n   * a symlink  itself then let Y' be the target of Y and X' be the scheme and\n   * authority of Y'. Symlink targets may:\n   * \n   * 1. Fully qualified URIs\n   * \n   * fs://hostX/A/B/file  Resolved according to the target file system.\n   * \n   * 2. Partially qualified URIs (eg scheme but no host)\n   * \n   * fs:///A/B/file  Resolved according to the target file system. Eg resolving\n   *                 a symlink to hdfs:///A results in an exception because\n   *                 HDFS URIs must be fully qualified, while a symlink to \n   *                 file:///A will not since Hadoop's local file systems \n   *                 require partially qualified URIs.\n   * \n   * 3. Relative paths\n   * \n   * path  Resolves to [Y'][path]. Eg if Y resolves to hdfs://host/A and path \n   *       is \"../B/file\" then [Y'][path] is hdfs://host/B/file\n   * \n   * 4. Absolute paths\n   * \n   * path  Resolves to [X'][path]. Eg if Y resolves hdfs://host/A/B and path\n   *       is \"/file\" then [X][path] is hdfs://host/file\n   * </pre>\n   * \n   * @param target the target of the symbolic link\n   * @param link the path to be created that points to target\n   * @param createParent if true then missing parent dirs are created if \n   *                     false then parent must exist\n   *\n   *\n   * @throws AccessControlException If access is denied\n   * @throws FileAlreadyExistsException If file <code>link</code> already exists\n   * @throws FileNotFoundException If <code>target</code> does not exist\n   * @throws ParentNotDirectoryException If parent of <code>link</code> is not a\n   *           directory.\n   * @throws UnsupportedFileSystemException If file system for \n   *           <code>target</code> or <code>link</code> is not supported\n   * @throws IOException If an I/O error occurred",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:checkPathIsSlash(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.permission.FsPermission:getUserAction()" : "* @return Return user {@link FsAction}.",
  "org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:getCoderName()" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsContext:enabled()" : "* Static probe to check if the thread-level IO statistics enabled.\n   *\n   * @return if the thread-level IO statistics enabled.",
  "org.apache.hadoop.ha.HAServiceTarget:getFencingParameters()" : null,
  "org.apache.hadoop.util.Waitable:<init>(java.util.concurrent.locks.Condition)" : null,
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyDecompressor:setDictionary(byte[],int,int)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.DecodingState:checkParameters(java.lang.Object[],int[],java.lang.Object[])" : "* Check and validate decoding parameters, throw exception accordingly. The\n   * checking assumes it's a MDS code. Other code  can override this.\n   * @param inputs input buffers to check\n   * @param erasedIndexes indexes of erased units in the inputs array\n   * @param outputs output buffers to check",
  "org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:createEncryptor()" : null,
  "org.apache.hadoop.fs.FilterFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)" : null,
  "org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumErasedBlocks(org.apache.hadoop.io.erasurecode.ECBlock[])" : "* Find out how many blocks are erased.\n   * @param inputBlocks all the input blocks\n   * @return number of erased blocks",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttrs(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:<init>(org.apache.hadoop.fs.FileSystem)" : null,
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getBlockSize()" : null,
  "org.apache.hadoop.fs.shell.CommandWithDestination:setWriteChecksum(boolean)" : null,
  "org.apache.hadoop.io.wrappedio.WrappedIO:bulkDelete_delete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Collection)" : "* Delete a list of files/objects.\n   * <ul>\n   *   <li>Files must be under the path provided in {@code base}.</li>\n   *   <li>The size of the list must be equal to or less than the page size.</li>\n   *   <li>Directories are not supported; the outcome of attempting to delete\n   *       directories is undefined (ignored; undetected, listed as failures...).</li>\n   *   <li>The operation is not atomic.</li>\n   *   <li>The operation is treated as idempotent: network failures may\n   *        trigger resubmission of the request -any new objects created under a\n   *        path in the list may then be deleted.</li>\n   *    <li>There is no guarantee that any parent directories exist after this call.\n   *    </li>\n   * </ul>\n   * @param fs filesystem\n   * @param base path to delete under.\n   * @param paths list of paths which must be absolute and under the base path.\n   * @return a list of all the paths which couldn't be deleted for a reason other\n   *          than \"not found\" and any associated error message.\n   * @throws UnsupportedOperationException bulk delete under that path is not supported.\n   * @throws UncheckedIOException if an IOE was raised.\n   * @throws IllegalArgumentException if a path argument is invalid.",
  "org.apache.hadoop.ipc.Server$Connection:getMessage(org.apache.hadoop.thirdparty.protobuf.Message,org.apache.hadoop.ipc.RpcWritable$Buffer)" : "* Decode the a protobuf from the given input stream \n     * @return Message - decoded protobuf\n     * @throws RpcServerException - deserialization failed",
  "org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:rename(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getAllStoragePolicies()" : null,
  "org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:cacheGroupsRefresh()" : null,
  "org.apache.hadoop.io.compress.DecompressorStream:resetState()" : null,
  "org.apache.hadoop.fs.FileContext:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)" : "* Modifies ACL entries of files and directories.  This method can add new ACL\n   * entries or modify the permissions on existing ACL entries.  All existing\n   * ACL entries that are not specified in this call are retained without\n   * changes.  (Modifications are merged into the current ACL.)\n   *\n   * @param path Path to modify\n   * @param aclSpec List{@literal <}AclEntry{@literal >} describing\n   * modifications\n   * @throws IOException if an ACL could not be modified",
  "org.apache.hadoop.fs.HarFileSystem$HarStatus:getName()" : null,
  "org.apache.hadoop.tracing.Tracer:<init>(java.lang.String)" : null,
  "org.apache.hadoop.util.InstrumentedWriteLock:startLockTiming()" : "* Starts timing for the instrumented write lock.",
  "org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration)" : "* Copies from one stream to another. <strong>closes the input and output streams \n   * at the end</strong>.\n   *\n   * @param in InputStrem to read from\n   * @param out OutputStream to write to\n   * @param conf the Configuration object.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.HarFileSystem:fileStatusesInIndex(org.apache.hadoop.fs.HarFileSystem$HarStatus,java.util.List)" : "* Get filestatuses of all the children of a given directory. This just reads\n   * through index file and reads line by line to get all statuses for children\n   * of a directory. Its a brute force way of getting all such filestatuses\n   * \n   * @param parent\n   *          the parent path directory\n   * @param statuses\n   *          the list to add the children filestatuses to",
  "org.apache.hadoop.fs.store.DataBlocks$DiskBlock:dataSize()" : null,
  "org.apache.hadoop.fs.viewfs.NflyFSystem:listStatus(org.apache.hadoop.fs.Path)" : "* Returns the closest non-failing destination's result.\n   *\n   * @param f given path\n   * @return array of file statuses according to nfly modes\n   * @throws FileNotFoundException\n   * @throws IOException",
  "org.apache.hadoop.io.MD5Hash:digest(java.io.InputStream)" : "* Construct a hash value for the content from the InputStream.\n   * @param in input stream.\n   * @return MD5Hash.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeSourceName(java.lang.String)" : null,
  "org.apache.hadoop.io.BloomMapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)" : "* Fast version of the\n     * {@link MapFile.Reader#get(WritableComparable, Writable)} method. First\n     * it checks the Bloom filter for the existence of the key, and only if\n     * present it performs the real get operation. This yields significant\n     * performance improvements for get operations on sparsely populated files.",
  "org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)" : null,
  "org.apache.hadoop.fs.HarFileSystem:getHarVersion()" : null,
  "org.apache.hadoop.fs.BufferedFSInputStream:minSeekForVectorReads()" : null,
  "org.apache.hadoop.metrics2.util.Contracts:checkArg(float,boolean,java.lang.Object)" : "* Check an argument for false conditions\n   * @param arg the argument to check\n   * @param expression  the boolean expression for the condition\n   * @param msg the error message if {@code expression} is false\n   * @return the argument for convenience",
  "org.apache.hadoop.io.file.tfile.Chunk:<init>()" : "* Prevent the instantiation of class.",
  "org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)" : "* The src file is on the local disk.  Add it to the filesystem at\n   * the given dst name.\n   * delSrc indicates if the source should be removed\n   * @param delSrc whether to delete the src\n   * @param overwrite whether to overwrite an existing file\n   * @param src path\n   * @param dst path\n   * @throws IOException IO failure",
  "org.apache.hadoop.util.ApplicationClassLoader:isSystemClass(java.lang.String,java.util.List)" : "* Checks if a class should be included as a system class.\n   *\n   * A class is a system class if and only if it matches one of the positive\n   * patterns and none of the negative ones.\n   *\n   * @param name the class name to check\n   * @param systemClasses a list of system class configurations.\n   * @return true if the class is a system class",
  "org.apache.hadoop.fs.viewfs.ChRootedFs:getFileChecksum(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileContext:setWorkingDirectory(org.apache.hadoop.fs.Path)" : "* Set the working directory for wd-relative names (such a \"foo/bar\"). Working\n   * directory feature is provided by simply prefixing relative names with the\n   * working dir. Note this is different from Unix where the wd is actually set\n   * to the inode. Hence setWorkingDir does not follow symlinks etc. This works\n   * better in a distributed environment that has multiple independent roots.\n   * {@link #getWorkingDirectory()} should return what setWorkingDir() set.\n   * \n   * @param newWDir new working directory\n   * @throws IOException \n   * <br>\n   *           NewWdir can be one of:\n   *           <ul>\n   *           <li>relative path: \"foo/bar\";</li>\n   *           <li>absolute without scheme: \"/foo/bar\"</li>\n   *           <li>fully qualified with scheme: \"xx://auth/foo/bar\"</li>\n   *           </ul>\n   * <br>\n   *           Illegal WDs:\n   *           <ul>\n   *           <li>relative with scheme: \"xx:foo/bar\"</li>\n   *           <li>non existent directory</li>\n   *           </ul>",
  "org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.InputStream,java.io.File,boolean)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,java.lang.String)" : "* Add a configuration resource. \n   * \n   * The properties of this resource will override properties of previously \n   * added resources, unless they were marked <a href=\"#Final\">final</a>. \n   * \n   * @param in InputStream to deserialize the object from.\n   * @param name the name of the resource because InputStream.toString is not\n   * very descriptive some times.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileStatus(org.apache.hadoop.fs.Path)" : "* {@inheritDoc}\n   *\n   * If the given path is a symlink(mount link), the path will be resolved to a\n   * target path and it will get the resolved path's FileStatus object. It will\n   * not be represented as a symlink and isDirectory API returns true if the\n   * resolved path is a directory, false otherwise.",
  "org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:source()" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:shuffle(org.apache.hadoop.crypto.key.kms.KMSClientProvider[])" : null,
  "org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsStore:setMaximum(java.lang.String,long)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.security.token.DtUtilShell$Renew:getUsage()" : null,
  "org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:close()" : null,
  "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:<init>(int)" : null,
  "org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:multiply(int,int)" : "* Compute the multiplication of two fields\n   *\n   * @param x input field\n   * @param y input field\n   * @return result of multiplication",
  "org.apache.hadoop.fs.ContentSummary$Builder:length(long)" : null,
  "org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:shutdown()" : "* Shutdown the instrumentation process.",
  "org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpointsImmediately()" : null,
  "org.apache.hadoop.fs.FsServerDefaults:getReplication()" : null,
  "org.apache.hadoop.net.NetUtils:getDefaultSocketFactory(org.apache.hadoop.conf.Configuration)" : "* Get the default socket factory as specified by the configuration\n   * parameter <tt>hadoop.rpc.socket.factory.default</tt>\n   * \n   * @param conf the configuration\n   * @return the default socket factory as specified in the configuration or\n   *         the JVM default socket factory if the configuration does not\n   *         contain a default socket factory property.",
  "org.apache.hadoop.ipc.CallQueueManager:queueIsReallyEmpty(java.util.concurrent.BlockingQueue)" : "* Checks if queue is empty by checking at CHECKPOINT_NUM points with\n   * CHECKPOINT_INTERVAL_MS interval.\n   * This doesn't mean the queue might not fill up at some point later, but\n   * it should decrease the probability that we lose a call this way.",
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.String,java.lang.Class[])" : "* Checks for an implementation, first finding the given class by name.\n     * <p>\n     * The name passed to the constructor is the method name used.\n     * @param className name of a class\n     * @param argClasses argument classes for the method\n     * @return this Builder for method chaining",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)" : null,
  "org.apache.hadoop.fs.RawLocalFileSystem:open(org.apache.hadoop.fs.Path,int)" : null,
  "org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.apache.commons.logging.Log,java.lang.String,long)" : "* Log the current thread stacks at INFO level.\n   * @param log the logger that logs the stack trace\n   * @param title a descriptive title for the call stacks\n   * @param minInterval the minimum time from the last\n   * @deprecated to be removed with 3.4.0. Use {@link #logThreadInfo(Logger, String, long)} instead.",
  "org.apache.hadoop.fs.shell.find.Find$1:<init>()" : "Default constructor for the Find command.",
  "org.apache.hadoop.io.ByteWritable:compareTo(org.apache.hadoop.io.ByteWritable)" : "Compares two ByteWritables.",
  "org.apache.hadoop.crypto.JceCtrCryptoCodec:calculateIV(byte[],long,byte[],int)" : null,
  "org.apache.hadoop.io.compress.snappy.SnappyCompressor:setInput(byte[],int,int)" : "* Sets input data for compression.\n   * This should be called whenever #needsInput() returns\n   * <code>true</code> indicating that more input data is required.\n   *\n   * @param b   Input data\n   * @param off Start offset\n   * @param len Length",
  "org.apache.hadoop.conf.ReconfigurationServlet:printFooter(java.io.PrintWriter)" : null,
  "org.apache.hadoop.fs.shell.find.Find:postProcessPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.io.file.tfile.Utils:upperBound(java.util.List,java.lang.Object)" : "* Upper bound binary search. Find the index to the first element in the list\n   * that compares greater than the input key.\n   * \n   * @param <T>\n   *          Type of the input key.\n   * @param list\n   *          The list\n   * @param key\n   *          The input key.\n   * @return The index to the desired element if it exists; or list.size()\n   *         otherwise.",
  "org.apache.hadoop.crypto.key.JavaKeyStoreProvider:noPasswordWarning()" : null,
  "org.apache.hadoop.fs.shell.Command:displayWarning(java.lang.String)" : "* Display an warning string prefaced with the command name.\n   * @param message warning message to display",
  "org.apache.hadoop.fs.FilterFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getBytesRead()" : "* Returns the total number of compressed bytes input so far.\n   *\n   * @return the total (non-negative) number of compressed bytes input so far",
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:bufferSize(int)" : "* Set the size of the buffer to be used.\n   *\n   * @param bufSize buffer size.\n   * @return Generics Type B.",
  "org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator:compare(byte[],int,int,byte[],int,int)" : null,
  "org.apache.hadoop.fs.shell.Stat:registerCommands(org.apache.hadoop.fs.shell.CommandFactory)" : null,
  "org.apache.hadoop.metrics2.lib.MethodMetric:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Location:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader$Location)" : null,
  "org.apache.hadoop.io.WritableComparator:readVLong(byte[],int)" : "* Reads a zero-compressed encoded long from a byte array and returns it.\n   * @param bytes byte array with decode long\n   * @param start starting index\n   * @throws IOException raised on errors performing I/O.\n   * @return deserialized long",
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementMinimum(java.lang.String,long)" : null,
  "org.apache.hadoop.net.NetUtils:bindToLocalAddress(java.net.InetAddress,boolean)" : "* Return an @{@link InetAddress} to bind to. If bindWildCardAddress is true\n   * than returns null.\n   *\n   * @param localAddr local addr.\n   * @param bindWildCardAddress bind wildcard address.\n   * @return InetAddress",
  "org.apache.hadoop.fs.shell.find.And:apply(org.apache.hadoop.fs.shell.PathData,int)" : "* Applies child expressions to the {@link PathData} item. If all pass then\n   * returns {@link Result#PASS} else returns the result of the first\n   * non-passing expression.",
  "org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:createCuratorClient(org.apache.hadoop.conf.Configuration,java.lang.String)" : null,
  "org.apache.hadoop.net.SocketInputStream:<init>(java.net.Socket)" : "* Same as SocketInputStream(socket.getChannel(), socket.getSoTimeout())\n   * :<br><br>\n   * \n   * Create a new input stream with the given timeout. If the timeout\n   * is zero, it will be treated as infinite timeout. The socket's\n   * channel will be configured to be non-blocking.\n   * @see SocketInputStream#SocketInputStream(ReadableByteChannel, long)\n   *  \n   * @param socket should have a channel associated with it.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.crypto.key.KeyProviderExtension:flush()" : null,
  "org.apache.hadoop.fs.FileStatus:compareTo(java.lang.Object)" : "* Compare this FileStatus to another FileStatus based on lexicographical\n   * order of path.\n   * This method was added back by HADOOP-14683 to keep binary compatibility.\n   *\n   * @param   o the FileStatus to be compared.\n   * @return  a negative integer, zero, or a positive integer as this object\n   *   is less than, equal to, or greater than the specified object.\n   * @throws ClassCastException if the specified object is not FileStatus",
  "org.apache.hadoop.util.ConfTest:listFiles(java.io.File)" : null,
  "org.apache.hadoop.conf.Configuration$IntegerRanges:getRangeStart()" : "* Get range start for the first integer range.\n     * @return range start.",
  "org.apache.hadoop.security.alias.UserProvider:<init>()" : null,
  "org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator:compare(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)" : null,
  "org.apache.hadoop.io.retry.RetryPolicies:retryByException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)" : "* <p>\n   * Set a default policy with some explicit handlers for specific exceptions.\n   * </p>\n   *\n   * @param exceptionToPolicyMap exceptionToPolicyMap.\n   * @param defaultPolicy defaultPolicy.\n   * @return RetryPolicy.",
  "org.apache.hadoop.fs.FsShellPermissions$Chown:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withMutableCounter(java.lang.String,org.apache.hadoop.metrics2.lib.MutableCounterLong)" : "* Build a dynamic counter statistic from a\n   * {@link MutableCounterLong}.\n   * @param key key of this statistic\n   * @param source mutable long counter\n   * @return the builder.",
  "org.apache.hadoop.io.MapFile:<init>()" : null,
  "org.apache.hadoop.io.compress.lz4.Lz4Decompressor:reset()" : null,
  "org.apache.hadoop.security.SaslInputStream:read(byte[],int,int)" : "* Reads up to <code>len</code> bytes of data from this input stream into an\n   * array of bytes. This method blocks until some input is available. If the\n   * first argument is <code>null,</code> up to <code>len</code> bytes are read\n   * and discarded.\n   * \n   * @param b\n   *          the buffer into which the data is read.\n   * @param off\n   *          the start offset of the data.\n   * @param len\n   *          the maximum number of bytes read.\n   * @return the total number of bytes read into the buffer, or <code>-1</code>\n   *         if there is no more data because the end of the stream has been\n   *         reached.\n   * @exception IOException\n   *              if an I/O error occurs.",
  "org.apache.hadoop.http.HttpServer2Metrics:statsOnMs()" : null,
  "org.apache.hadoop.metrics2.util.SampleStat:add(double)" : "* Add a sample the running stat.\n   * @param x the sample number\n   * @return  self",
  "org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheHit()" : "* One cache hit event",
  "org.apache.hadoop.fs.permission.PermissionParser:applyNormalPattern(java.lang.String,java.util.regex.Matcher)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider$Metadata:<init>(byte[])" : "* Deserialize a new metadata object from a set of bytes.\n     * @param bytes the serialized metadata\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.security.UserGroupInformation$UgiMetrics:reattach()" : null,
  "org.apache.hadoop.crypto.key.UserProvider:getKeyVersions(java.lang.String)" : null,
  "org.apache.hadoop.fs.Options$CreateOpts:checksumParam(org.apache.hadoop.fs.Options$ChecksumOpt)" : null,
  "org.apache.hadoop.util.Shell$ExitCodeException:getExitCode()" : null,
  "org.apache.hadoop.ipc.CallerContext:<init>(org.apache.hadoop.ipc.CallerContext$Builder)" : null,
  "org.apache.hadoop.fs.FileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)" : "* Checks if the user can access a path.  The mode specifies which access\n   * checks to perform.  If the requested permissions are granted, then the\n   * method returns normally.  If access is denied, then the method throws an\n   * {@link AccessControlException}.\n   * <p>\n   * The default implementation calls {@link #getFileStatus(Path)}\n   * and checks the returned permissions against the requested permissions.\n   *\n   * Note that the {@link #getFileStatus(Path)} call will be subject to\n   * authorization checks.\n   * Typically, this requires search (execute) permissions on each directory in\n   * the path's prefix, but this is implementation-defined.  Any file system\n   * that provides a richer authorization model (such as ACLs) may override the\n   * default implementation so that it checks against that model instead.\n   * <p>\n   * In general, applications should avoid using this method, due to the risk of\n   * time-of-check/time-of-use race conditions.  The permissions on a file may\n   * change immediately after the access call returns.  Most applications should\n   * prefer running specific file system actions as the desired user represented\n   * by a {@link UserGroupInformation}.\n   *\n   * @param path Path to check\n   * @param mode type of access to check\n   * @throws AccessControlException if access is denied\n   * @throws FileNotFoundException if the path does not exist\n   * @throws IOException see specific implementation",
  "org.apache.hadoop.metrics2.sink.RollingFileSystemSink:findCurrentDirectory(java.util.Date)" : "* Use the given time to determine the current directory. The current\n   * directory will be based on the {@link #rollIntervalMinutes}.\n   *\n   * @param now the current time\n   * @return the current directory",
  "org.apache.hadoop.net.NetworkTopology:getLastHalf(java.lang.String)" : "* @return Divide networklocation string into two parts by last separator, and get\n   * the second part here.\n   * \n   * @param networkLocation input networkLocation.",
  "org.apache.hadoop.service.AbstractService:init(org.apache.hadoop.conf.Configuration)" : "* {@inheritDoc}\n   * This invokes {@link #serviceInit}\n   * @param conf the configuration of the service. This must not be null\n   * @throws ServiceStateException if the configuration was null,\n   * the state change not permitted, or something else went wrong",
  "org.apache.hadoop.ipc.Server:getProtocolClass(java.lang.String,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.FilterFileSystem:getConf()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:write(java.io.DataOutput)" : null,
  "org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getChunkPosition(long)" : null,
  "org.apache.hadoop.io.compress.bzip2.BZip2DummyCompressor:reset()" : null,
  "org.apache.hadoop.fs.HardLink:createHardLink(java.io.File,java.io.File)" : "* Creates a hardlink.\n   * @param file - existing source file\n   * @param linkName - desired target link file\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.FileSystem:closeAllForUGI(org.apache.hadoop.security.UserGroupInformation)" : "* Close all cached FileSystem instances for a given UGI.\n   * Be sure those filesystems are not used anymore.\n   * @param ugi user group info to close\n   * @throws IOException a problem arose closing one or more filesystem.",
  "org.apache.hadoop.ipc.CallQueueManager:offer(org.apache.hadoop.ipc.Schedulable)" : "* Insert e into the backing queue.\n   * Return true if e is queued.\n   * Return false if the queue is full.",
  "org.apache.hadoop.util.ExitUtil:halt(org.apache.hadoop.util.ExitUtil$HaltException)" : "* Halts the JVM if halt is enabled, rethrow provided exception or any raised error otherwise.\n   * If halt is disabled, this method throws either the exception argument if no\n   * error arise, the first error if at least one arise, suppressing <code>he</code>.\n   * If halt is enabled, all throwables are caught, even errors.\n   *\n   * @param he the exception containing the status code, message and any stack\n   * trace.\n   * @throws HaltException if {@link Runtime#halt(int)} is disabled and not suppressed by an Error\n   * @throws Error if {@link Runtime#halt(int)} is disabled and one Error arise, suppressing\n   * anyuthing else, even <code>he</code>",
  "org.apache.hadoop.security.Credentials:writeProto(java.io.DataOutput)" : "* Write contents of this instance as CredentialsProto message to DataOutput.\n   * @param out\n   * @throws IOException",
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStatus(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)" : "* Create a snapshot.\n   *\n   * @param path The directory where snapshots will be taken.\n   * @param snapshotName The name of the snapshot\n   * @return the snapshot path.\n   *\n   * @throws IOException If an I/O error occurred\n   *\n   * <p>Exceptions applicable to file systems accessed over RPC:\n   * @throws RpcClientException If an exception occurred in the RPC client\n   * @throws RpcServerException If an exception occurred in the RPC server\n   * @throws UnexpectedServerException If server implementation throws\n   *           undeclared exception to RPC server",
  "org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.Class,java.lang.String,java.lang.Class[])" : "* Checks for a method implementation.\n     * @param targetClass the class to check for an implementation\n     * @param methodName name of a method (different from constructor)\n     * @param argClasses argument classes for the method\n     * @return this Builder for method chaining",
  "org.apache.hadoop.net.NetUtils:createURI(java.lang.String,boolean,java.lang.String,boolean)" : null,
  "org.apache.hadoop.util.LightWeightCache:evictExpiredEntries()" : "Evict expired entries.",
  "org.apache.hadoop.util.ReflectionUtils$CopyInCopyOutBuffer:moveData()" : "* Move the data from the output buffer to the input buffer.",
  "org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.net.DNSToSwitchMapping)" : "* Create an instance from the given raw mapping\n   * @param rawMap raw DNSTOSwithMapping",
  "org.apache.hadoop.util.LightWeightCache:evict()" : null,
  "org.apache.hadoop.security.KDiag:title(java.lang.String,java.lang.Object[])" : "* Print a title entry.\n   *\n   * @param format format string\n   * @param args any arguments",
  "org.apache.hadoop.fs.Globber$GlobBuilder:<init>(org.apache.hadoop.fs.FileContext)" : "* Construct bonded to a file context.\n     * @param fc file context.",
  "org.apache.hadoop.net.SocketIOWithTimeout:checkChannelValidity(java.lang.Object)" : "* Utility function to check if channel is ok.\n   * Mainly to throw IOException instead of runtime exception\n   * in case of mismatch. This mismatch can occur for many runtime\n   * reasons.",
  "org.apache.hadoop.io.ShortWritable$Comparator:<init>()" : null,
  "org.apache.hadoop.fs.FileStatus:getPath()" : null,
  "org.apache.hadoop.fs.FSInputChecker:readFully(java.io.InputStream,byte[],int,int)" : "* A utility function that tries to read up to <code>len</code> bytes from\n   * <code>stm</code>\n   * \n   * @param stm    an input stream\n   * @param buf    destination buffer\n   * @param offset offset at which to store data\n   * @param len    number of bytes to read\n   * @return actual number of bytes read\n   * @throws IOException if there is any IO error",
  "org.apache.hadoop.io.compress.PassthroughCodec:createOutputStream(java.io.OutputStream)" : null,
  "org.apache.hadoop.fs.FSDataInputStream:hasCapability(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:recvDecodingTables()" : null,
  "org.apache.hadoop.io.SequenceFile$Writer:checkAndWriteSync()" : null,
  "org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)" : "* The src files are on the local disk.  Add it to FS at\n   * the given dst name.\n   * delSrc indicates if the source should be removed",
  "org.apache.hadoop.security.token.DtUtilShell$Edit:getUsage()" : null,
  "org.apache.hadoop.ipc.Server$Listener:getSelector()" : null,
  "org.apache.hadoop.util.GSetByHashMap:<init>(int,float)" : null,
  "org.apache.hadoop.io.compress.GzipCodec:createCompressor()" : null,
  "org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getCacheFilePath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)" : "* Return temporary file created based on the file path retrieved from local dir allocator.\n   *\n   * @param conf The configuration object.\n   * @param localDirAllocator Local dir allocator instance.\n   * @return Path of the temporary file created.\n   * @throws IOException if IO error occurs while local dir allocator tries to retrieve path\n   * from local FS or file creation fails or permission set fails.",
  "org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:writeStreamHeader()" : null,
  "org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:ioStatisticsAvailable()" : "* Are the core IOStatistics methods and classes available.\n   * @return true if the relevant methods are loaded.",
  "org.apache.hadoop.net.unix.DomainSocket:socketpair()" : "* Create a pair of UNIX domain sockets which are connected to each other\n   * by calling socketpair(2).\n   *\n   * @return                An array of two UNIX domain sockets connected to\n   *                        each other.\n   * @throws IOException    on error.",
  "org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.fs.Path,boolean)" : null,
  "org.apache.hadoop.fs.viewfs.InodeTree:checkMntEntryKeyEqualsTarget(java.lang.String,java.lang.String)" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(org.apache.hadoop.io.file.tfile.RawComparable,boolean)" : "* Advance cursor in block until we find a key that is greater than or\n       * equal to the input key.\n       * \n       * @param key\n       *          Key to compare.\n       * @param greater\n       *          advance until we find a key greater than the input key.\n       * @return true if we find a equal key.\n       * @throws IOException",
  "org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.tracing.TraceScope:getSpan()" : null,
  "org.apache.hadoop.tracing.Tracer$Builder:conf(org.apache.hadoop.tracing.TraceConfiguration)" : null,
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:drain(java.lang.String)" : null,
  "org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressor(org.apache.hadoop.conf.Configuration)" : "* Return the appropriate implementation of the zlib decompressor. \n   * \n   * @param conf configuration\n   * @return the appropriate implementation of the zlib decompressor.",
  "org.apache.hadoop.io.NullWritable:readFields(java.io.DataInput)" : null,
  "org.apache.hadoop.io.erasurecode.codec.ErasureCodec:setCoderOptions(org.apache.hadoop.io.erasurecode.ErasureCoderOptions)" : null,
  "org.apache.hadoop.util.ShutdownHookManager:isShutdownInProgress()" : "* Indicates if shutdown is in progress or not.\n   * \n   * @return TRUE if the shutdown is in progress, otherwise FALSE.",
  "org.apache.hadoop.util.functional.RemoteIterators:<init>()" : null,
  "org.apache.hadoop.security.KerberosAuthException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.lang.String[],java.lang.String[],boolean,boolean)" : null,
  "org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:needsInput()" : null,
  "org.apache.hadoop.fs.FileContext:fixRelativePart(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[])" : "* See {@link #listStatus(Path[], PathFilter)}\n     *\n     * @param files files.\n     * @throws AccessControlException If access is denied.\n     * @throws FileNotFoundException If <code>files</code> does not exist.\n     * @throws IOException If an I/O error occurred.\n     * @return file status array.",
  "org.apache.hadoop.util.ToolRunner:printGenericCommandUsage(java.io.PrintStream)" : "* Prints generic command-line argurments and usage information.\n   * \n   *  @param out stream to write usage information to.",
  "org.apache.hadoop.util.functional.Tuples$Tuple:hashCode()" : null,
  "org.apache.hadoop.fs.FileStatus:isDirectory()" : "* Is this a directory?\n   * @return true if this is a directory",
  "org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[])" : "* Create the duration text from a {@code String.format()} code call\n   * and log either at info or debug.\n   * @param log log to write to\n   * @param logAtInfo should the log be at info, rather than debug\n   * @param format format string\n   * @param args list of arguments",
  "org.apache.hadoop.crypto.key.kms.ValueQueue:initializeQueuesForKeys(java.lang.String[])" : "* Initializes the Value Queues for the provided keys by calling the\n   * fill Method with \"numInitValues\" values\n   * @param keyNames Array of key Names\n   * @throws ExecutionException executionException.",
  "org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration)" : "Construct a Configured.\n   * @param conf the Configuration object.",
  "org.apache.hadoop.util.AsyncDiskService:shutdownNow()" : "* Shut down all ThreadPools immediately.\n   *\n   * @return Runnable List.",
  "org.apache.hadoop.fs.FileUtil:canWrite(java.io.File)" : "* Platform independent implementation for {@link File#canWrite()}\n   * @param f input file\n   * @return On Unix, same as {@link File#canWrite()}\n   *         On Windows, true if process has write access on the path",
  "org.apache.hadoop.util.Waitable:provide(java.lang.Object)" : null,
  "org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:nextIdx()" : null,
  "org.apache.hadoop.ipc.Server$Responder:doRunLoop()" : null,
  "org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)" : "* Create the named file for values of the named class.\n     *\n     * @param conf configuration.\n     * @param fs file system.\n     * @param file file.\n     * @param valClass valClass.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.service.AbstractService:serviceStart()" : "* Actions called during the INITED to STARTED transition.\n   *\n   * This method will only ever be called once during the lifecycle of\n   * a specific service instance.\n   *\n   * Implementations do not need to be synchronized as the logic\n   * in {@link #start()} prevents re-entrancy.\n   *\n   * @throws Exception if needed -these will be caught,\n   * wrapped, and trigger a service stop",
  "org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getReader(java.lang.Class)" : null,
  "org.apache.hadoop.util.Options:prependOptions(java.lang.Object[],java.lang.Object[])" : "* Prepend some new options to the old options\n   * @param <T> the type of options\n   * @param oldOpts the old options\n   * @param newOpts the new options\n   * @return a new array of options",
  "org.apache.hadoop.io.compress.zlib.ZlibCompressor:getBytesWritten()" : "* Returns the total number of compressed bytes output so far.\n   *\n   * @return the total (non-negative) number of compressed bytes output so far",
  "org.apache.hadoop.conf.StorageSize:checkState(boolean,java.lang.String)" : null,
  "org.apache.hadoop.conf.StorageUnit$3:getLongName()" : null,
  "org.apache.hadoop.fs.permission.AclStatus:<init>(java.lang.String,java.lang.String,boolean,java.lang.Iterable,org.apache.hadoop.fs.permission.FsPermission)" : "* Private constructor.\n   *\n   * @param file Path file associated to this ACL\n   * @param owner String file owner\n   * @param group String file group\n   * @param stickyBit the sticky bit\n   * @param entries the ACL entries\n   * @param permission permission of the path",
  "org.apache.hadoop.io.file.tfile.TFile:main(java.lang.String[])" : "* Dumping the TFile information.\n   * \n   * @param args\n   *          A list of TFile paths.",
  "org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:writeMetrics(java.io.Writer)" : null,
  "org.apache.hadoop.crypto.key.KeyProvider$KeyVersion:getVersionName()" : null,
  "org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:rejectUnknownMandatoryKeys(java.util.Set,java.util.Collection,java.lang.String)" : "* Reject a configuration if one or more mandatory keys are\n   * not in the set of mandatory keys.\n   * The first invalid key raises the exception; the order of the\n   * scan and hence the specific key raising the exception is undefined.\n   * @param mandatory the set of mandatory keys\n   * @param knownKeys a possibly empty collection of known keys\n   * @param extraErrorText extra error text to include.\n   * @throws IllegalArgumentException if any key is unknown.",
  "org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.Throwable)" : "* Create an exception with the specific exit code.\n   * @param exitCode exit code\n   * @param cause cause of the exception",
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:getBufferSize()" : null,
  "org.apache.hadoop.io.Text:utf8Length(java.lang.String)" : "* For the given string, returns the number of UTF-8 bytes\n   * required to encode the string.\n   * @param string text to encode\n   * @return number of UTF-8 bytes required to encode",
  "org.apache.hadoop.fs.viewfs.ChRootedFs:setReplication(org.apache.hadoop.fs.Path,short)" : null,
  "org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:put(org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor)" : null,
  "org.apache.hadoop.crypto.OpensslCipher:isSupported(org.apache.hadoop.crypto.CipherSuite)" : null,
  "org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getRawFileSystem(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)" : "* This is an admin only API to give access to its child raw file system, if\n   * the path is link. If the given path is an internal directory(path is from\n   * mount paths tree), it will initialize the file system of given path uri\n   * directly. If path cannot be resolved to any internal directory or link, it\n   * will throw NotInMountpointException. Please note, this API will not return\n   * chrooted file system. Instead, this API will get actual raw file system\n   * instances.\n   *\n   * @param path - fs uri path\n   * @param conf - configuration\n   * @throws IOException raised on errors performing I/O.\n   * @return file system.",
  "org.apache.hadoop.ipc.ProcessingDetails:getReturnStatus()" : null,
  "org.apache.hadoop.fs.shell.PrintableString:<init>(java.lang.String)" : null,
  "org.apache.hadoop.fs.shell.Delete$Rmdir:processPath(org.apache.hadoop.fs.shell.PathData)" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:asyncRequests()" : null,
  "org.apache.hadoop.fs.HarFileSystem:delete(org.apache.hadoop.fs.Path,boolean)" : "* Not implemented.",
  "org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingStdDev()" : null,
  "org.apache.hadoop.util.ZKUtil$BadAuthFormatException:<init>(java.lang.String)" : null,
  "org.apache.hadoop.fs.permission.FsPermission:getOtherAction()" : "* @return Return other {@link FsAction}.",
  "org.apache.hadoop.ipc.CallQueueManager:add(java.lang.Object)" : null,
  "org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.io.compress.ZStandardCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)" : "* Create a {@link CompressionOutputStream} that will write to the given\n   * {@link OutputStream} with the given {@link Compressor}.\n   *\n   * @param out        the location for the final output stream\n   * @param compressor compressor to use\n   * @return a stream the user can write uncompressed data to have compressed\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.impl.FlagSet:hashCode()" : "* Hash code is based on the flags.\n   * @return a hash code.",
  "org.apache.hadoop.service.AbstractService:getLifecycleHistory()" : null,
  "org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks:notifyFatalError(java.lang.String)" : null,
  "org.apache.hadoop.conf.StorageUnit$5:toTBs(double)" : null,
  "org.apache.hadoop.crypto.CryptoProtocolVersion:supports(org.apache.hadoop.crypto.CryptoProtocolVersion)" : "* Returns if a given protocol version is supported.\n   *\n   * @param version version number\n   * @return true if the version is supported, else false",
  "org.apache.hadoop.io.erasurecode.coder.HHErasureCodingStep:getSubPacketSize()" : null,
  "org.apache.hadoop.fs.FileContext:getFileLinkStatus(org.apache.hadoop.fs.Path)" : "* Return a file status object that represents the path. If the path \n   * refers to a symlink then the FileStatus of the symlink is returned.\n   * The behavior is equivalent to #getFileStatus() if the underlying\n   * file system does not support symbolic links.\n   * @param  f The path we want information from.\n   * @return A FileStatus object\n   * \n   * @throws AccessControlException If access is denied\n   * @throws FileNotFoundException If <code>f</code> does not exist\n   * @throws UnsupportedFileSystemException If file system for <code>f</code> is\n   *           not supported\n   * @throws IOException If an I/O error occurred",
  "org.apache.hadoop.io.ObjectWritable$NullInstance:<init>()" : null,
  "org.apache.hadoop.security.alias.CredentialShell$Command:getCredentialProvider()" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:serializer()" : "* Get a JSON serializer for this class.\n   * @return a serializer.",
  "org.apache.hadoop.fs.shell.FsUsage:formatSize(long)" : null,
  "org.apache.hadoop.http.HttpServer2:start()" : "* Start the server. Does not wait for the server to start.\n   *\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.conf.StorageUnit$5:toBytes(double)" : null,
  "org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.util.Collection)" : "* Randomly choose one node from <i>scope</i>.\n   *\n   * If scope starts with ~, choose one from the all nodes except for the\n   * ones in <i>scope</i>; otherwise, choose one from <i>scope</i>.\n   * If excludedNodes is given, choose a node that's not in excludedNodes.\n   *\n   * @param scope range of nodes from which a node will be chosen\n   * @param excludedNodes nodes to be excluded from\n   * @return the chosen node",
  "org.apache.hadoop.fs.Path:<init>(java.lang.String)" : "* Construct a path from a String.  Path strings are URIs, but with\n   * unescaped elements and some additional normalization.\n   *\n   * @param pathString the path string",
  "org.apache.hadoop.ipc.RpcServerException:getRpcErrorCodeProto()" : "* @return get the detailed rpc status corresponding to this exception.",
  "org.apache.hadoop.log.LogThrottlingHelper$LoggingAction:getStats(int)" : null,
  "org.apache.hadoop.util.StopWatch:close()" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.util.VersionInfo:_getRevision()" : null,
  "org.apache.hadoop.http.HttpServer2Metrics:responses4xx()" : null,
  "org.apache.hadoop.io.OutputBuffer:<init>(org.apache.hadoop.io.OutputBuffer$Buffer)" : null,
  "org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)" : "* Copy from src to dst, optionally deleting src and overwriting dst.\n     * @param src src.\n     * @param dst dst.\n     * @param deleteSource - delete src if true\n     * @param overwrite  overwrite dst if true; throw IOException if dst exists\n     *         and overwrite is false.\n     *\n     * @return true if copy is successful\n     *\n     * @throws AccessControlException If access is denied\n     * @throws FileAlreadyExistsException If <code>dst</code> already exists\n     * @throws FileNotFoundException If <code>src</code> does not exist\n     * @throws ParentNotDirectoryException If parent of <code>dst</code> is not\n     *           a directory\n     * @throws UnsupportedFileSystemException If file system for \n     *         <code>src</code> or <code>dst</code> is not supported\n     * @throws IOException If an I/O error occurred\n     * \n     * Exceptions applicable to file systems accessed over RPC:\n     * @throws RpcClientException If an exception occurred in the RPC client\n     * @throws RpcServerException If an exception occurred in the RPC server\n     * @throws UnexpectedServerException If server implementation throws \n     *           undeclared exception to RPC server\n     * \n     * RuntimeExceptions:\n     * @throws InvalidPathException If path <code>dst</code> is invalid",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:writeValue(java.io.OutputStream)" : "* Writing the value to the output stream. This method avoids copying\n         * value data from Scanner into user buffer, then writing to the output\n         * stream. It does not require the value length to be known.\n         * \n         * @param out\n         *          The output stream\n         * @return the length of the value\n         * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class)" : null,
  "org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:createEncoder()" : null,
  "org.apache.hadoop.fs.sftp.SFTPInputStream:<init>(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem$Statistics)" : null,
  "org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.commons.logging.Log,org.apache.hadoop.service.Service)" : "* Stop a service; if it is null do nothing. Exceptions are caught and\n   * logged at warn level. (but not Throwables). This operation is intended to\n   * be used in cleanup operations\n   *\n   * @param log the log to warn at\n   * @param service a service; may be null\n   * @return any exception that was caught; null if none was.\n   * @deprecated to be removed with 3.4.0. Use {@link #stopQuietly(Logger, Service)} instead.",
  "org.apache.hadoop.io.EnumSetWritable:<init>(java.util.EnumSet)" : "* Construct a new EnumSetWritable. Argument <tt>value</tt> should not be null\n   * or empty.\n   * \n   * @param value enumSet value.",
  "org.apache.hadoop.fs.GlobPattern:set(java.lang.String)" : "* Set and compile a glob pattern\n   * @param glob  the glob pattern string",
  "org.apache.hadoop.io.retry.RetryPolicies$ExceptionDependentRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)" : null,
  "org.apache.hadoop.conf.Configuration:setRestrictSystemProps(boolean)" : null,
  "org.apache.hadoop.fs.http.AbstractHttpFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.fs.ftp.FTPFileSystem:getUri()" : null,
  "org.apache.hadoop.io.file.tfile.TFile$Writer:<init>(org.apache.hadoop.fs.FSDataOutputStream,int,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)" : "* Constructor\n     * \n     * @param fsdos\n     *          output stream for writing. Must be at position 0.\n     * @param minBlockSize\n     *          Minimum compressed block size in bytes. A compression block will\n     *          not be closed until it reaches this size except for the last\n     *          block.\n     * @param compressName\n     *          Name of the compression algorithm. Must be one of the strings\n     *          returned by {@link TFile#getSupportedCompressionAlgorithms()}.\n     * @param comparator\n     *          Leave comparator as null or empty string if TFile is not sorted.\n     *          Otherwise, provide the string name for the comparison algorithm\n     *          for keys. Two kinds of comparators are supported.\n     *          <ul>\n     *          <li>Algorithmic comparator: binary comparators that is language\n     *          independent. Currently, only \"memcmp\" is supported.\n     *          <li>Language-specific comparator: binary comparators that can\n     *          only be constructed in specific language. For Java, the syntax\n     *          is \"jclass:\", followed by the class name of the RawComparator.\n     *          Currently, we only support RawComparators that can be\n     *          constructed through the default constructor (with no\n     *          parameters). Parameterized RawComparators such as\n     *          {@link WritableComparator} or\n     *          {@link JavaSerializationComparator} may not be directly used.\n     *          One should write a wrapper class that inherits from such classes\n     *          and use its default constructor to perform proper\n     *          initialization.\n     *          </ul>\n     * @param conf\n     *          The configuration object.\n     * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getGaugeReference(java.lang.String)" : "* Get a reference to the atomic instance providing the\n   * value for a specific gauge. This is useful if\n   * the value is passed around.\n   * @param key statistic name\n   * @return the reference\n   * @throws NullPointerException if there is no entry of that name",
  "org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[],long)" : null,
  "org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:build()" : null,
  "org.apache.hadoop.conf.Configuration:getCredentialEntry(org.apache.hadoop.security.alias.CredentialProvider,java.lang.String)" : "* Get the credential entry by name from a credential provider.\n   *\n   * Handle key deprecation.\n   *\n   * @param provider a credential provider\n   * @param name alias of the credential\n   * @return the credential entry or null if not found",
  "org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:<init>(org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolPB)" : null,
  "org.apache.hadoop.fs.statistics.IOStatisticsLogging:demandStringifyIOStatisticsSource(org.apache.hadoop.fs.statistics.IOStatisticsSource)" : "* On demand stringifier of an IOStatisticsSource instance.\n   * <p>\n   * Whenever this object's toString() method is called, it evaluates the\n   * statistics.\n   * <p>\n   * This is designed to affordable to use in log statements.\n   * @param source source of statistics -may be null.\n   * @return an object whose toString() operation returns the current values.",
  "org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMeanStatistic(java.lang.String)" : null,
  "org.apache.hadoop.ha.ActiveStandbyElector:terminateConnection()" : null,
  "org.apache.hadoop.fs.permission.PermissionStatus:read(java.io.DataInput)" : "* Create and initialize a {@link PermissionStatus} from {@link DataInput}.\n   * @param in data input.\n   * @throws IOException raised on errors performing I/O.\n   * @return PermissionStatus.",
  "org.apache.hadoop.fs.impl.prefetch.BufferData:setCaching(java.util.concurrent.Future)" : "* Indicates that a caching operation is in progress.\n   *\n   * @param actionFuture the {@code Future} of a caching action.\n   *\n   * @throws IllegalArgumentException if actionFuture is null.",
  "org.apache.hadoop.ha.ZKFailoverController:cedeRemoteActive(org.apache.hadoop.ha.HAServiceTarget,int)" : "* Ask the remote zkfc to cede its active status and wait for the specified\n   * timeout before attempting to claim leader status.\n   * @param remote node to ask\n   * @param timeout amount of time to cede\n   * @return the {@link ZKFCProtocol} used to talk to the ndoe\n   * @throws IOException",
  "org.apache.hadoop.fs.impl.prefetch.BlockData:getState(int)" : "* Gets the state of the given block.\n   * @param blockNumber the id of the given block.\n   * @return the state of the given block.\n   * @throws IllegalArgumentException if blockNumber is invalid.",
  "org.apache.hadoop.fs.impl.prefetch.BufferPool:getAll()" : "* Gets a list of all blocks in this pool.\n   * @return a list of all blocks in this pool.",
  "org.apache.hadoop.util.DataChecksum:newDataChecksum(byte[],int)" : "* Creates a DataChecksum from HEADER_LEN bytes from arr[offset].\n   *\n   * @param bytes bytes.\n   * @param offset offset.\n   * @return DataChecksum of the type in the array or null in case of an error.\n   * @throws IOException raised on errors performing I/O.",
  "org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:readLength()" : "* Reading the length of next chunk.\n     * \n     * @throws java.io.IOException\n     *           when no more data is available.",
  "org.apache.hadoop.security.token.Token$TrivialRenewer:cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)" : null,
  "org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:close()" : null,
  "org.apache.hadoop.util.AutoCloseableLock:tryLock()" : "* A wrapper method that makes a call to {@code tryLock()} of\n   * the underlying {@code Lock} object.\n   *\n   * If the lock is not held by another thread, acquires the lock, set the\n   * hold count to one and returns {@code true}.\n   *\n   * If the current thread already holds the lock, the increment the hold\n   * count by one and returns {@code true}.\n   *\n   * If the lock is held by another thread then the method returns\n   * immediately with {@code false}.\n   *\n   * @return {@code true} if the lock was free and was acquired by the\n   *          current thread, or the lock was already held by the current\n   *          thread; and {@code false} otherwise.",
  "org.apache.hadoop.ipc.Client$Connection$PingInputStream:read(byte[],int,int)" : "Read bytes into a buffer starting from offset <code>off</code>\n       * Send a ping if timeout on read. Retries if no failure is detected\n       * until a byte is read.\n       * \n       * @return the total number of bytes read; -1 if the connection is closed.",
  "org.apache.hadoop.metrics2.lib.MutableStat:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)" : "* Construct a snapshot stat metric with extended stat off by default\n   * @param name        of the metric\n   * @param description of the metric\n   * @param sampleName  of the metric (e.g. \"Ops\")\n   * @param valueName   of the metric (e.g. \"Time\", \"Latency\")",
  "org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:parent()" : null,
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults(org.apache.hadoop.fs.Path)" : null,
  "org.apache.hadoop.fs.FSDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)" : "* Constructor.\n   *\n   * @param fileSystem file system.\n   * @param p the path.",
  "org.apache.hadoop.io.BloomMapFile:byteArrayForBloomKey(org.apache.hadoop.io.DataOutputBuffer)" : null,
  "org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:setThreadPoolQueueSize(java.lang.String)" : "* set thread pool queue size by option value, if the value less than 1,\n   * use DEFAULT_QUEUE_SIZE instead.\n   *\n   * @param optValue option value",
  "org.apache.hadoop.security.token.delegation.DelegationKey:<init>(int,long,javax.crypto.SecretKey)" : null,
  "org.apache.hadoop.service.launcher.ServiceLauncher:verifyConfigurationFilesExist(java.lang.String[])" : "* Verify that all the specified filenames exist.\n   * @param filenames a list of files\n   * @throws ServiceLaunchException if a file is not found",
  "org.apache.hadoop.ha.HAAdmin:isOtherTargetNodeActive(java.lang.String,boolean)" : "* Checks whether other target node is active or not\n   * @param targetNodeToActivate\n   * @return true if other target node is active or some other exception \n   * occurred and forceActive was set otherwise false\n   * @throws IOException",
  "org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object)" : "* <p>Preconditions that the specified argument is not {@code null},\n   * throwing a NPE exception otherwise.\n   *\n   * <p>The message of the exception is\n   * &quot;The validated object is null&quot;.</p>\n   *\n   * @param <T> the object type\n   * @param obj  the object to check\n   * @return the validated object\n   * @throws NullPointerException if the object is {@code null}\n   * @see #checkNotNull(Object, Object)",
  "org.apache.hadoop.io.SequenceFile$UncompressedBytes:reset(java.io.DataInputStream,int)" : null,
  "org.apache.hadoop.metrics2.util.SampleQuantiles:compress()" : "* Try to remove extraneous items from the set of sampled items. This checks\n   * if an item is unnecessary based on the desired error bounds, and merges it\n   * with the adjacent item if it is.",
  "org.apache.hadoop.util.KMSUtil:parseJSONMetadata(java.util.Map)" : null,
  "org.apache.hadoop.fs.store.DataBlocks$DataBlockByteArrayOutputStream:<init>(int)" : null,
  "org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorFactory:create(java.lang.String)" : "* interceptorSettingsString string should be like ${type}:${string},\n   * e.g. replaceresolveddstpath:word1,word2.\n   *\n   * @param interceptorSettingsString\n   * @return Return interceptor based on setting or null on bad/unknown config.",
  "org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,long,long)" : "* Constructor\n       * \n       * @param reader\n       *          The TFile reader object.\n       * @param offBegin\n       *          Begin byte-offset of the scan.\n       * @param offEnd\n       *          End byte-offset of the scan.\n       * @throws IOException\n       * \n       *           The offsets will be rounded to the beginning of a compressed\n       *           block whose offset is greater than or equal to the specified\n       *           offset.",
  "org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:build()" : "* Perform the open operation.\n     *\n     * @return a future to the input stream.\n     * @throws IOException early failure to open\n     * @throws UnsupportedOperationException if the specific operation\n     * is not supported.\n     * @throws IllegalArgumentException if the parameters are not valid.",
  "org.apache.hadoop.fs.viewfs.ViewFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)" : null,
  "org.apache.hadoop.util.hash.Hash:parseHashType(java.lang.String)" : "* This utility method converts String representation of hash function name\n   * to a symbolic constant. Currently two function types are supported,\n   * \"jenkins\" and \"murmur\".\n   * @param name hash function name\n   * @return one of the predefined constants",
  "org.apache.hadoop.fs.viewfs.NflyFSystem:open(org.apache.hadoop.fs.Path,int)" : "* Category: READ.\n   *\n   * @param f the file name to open\n   * @param bufferSize the size of the buffer to be used.\n   * @return input stream according to nfly flags (closest, most recent)\n   * @throws IOException\n   * @throws FileNotFoundException iff all destinations generate this exception",
  "org.apache.hadoop.fs.impl.FutureIOSupport:eval(org.apache.hadoop.util.functional.CallableRaisingIOE)" : "* Evaluate a CallableRaisingIOE in the current thread,\n   * converting IOEs to RTEs and propagating.\n   * See {@link FutureIO#eval(CallableRaisingIOE)}.\n   *\n   * @param callable callable to invoke\n   * @param <T> Return type.\n   * @return the evaluated result.\n   * @throws UnsupportedOperationException fail fast if unsupported\n   * @throws IllegalArgumentException invalid argument",
  "org.apache.hadoop.fs.DelegateToFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)" : null,
  "org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:iterator()" : null,
  "org.apache.hadoop.ha.ZKFailoverController$ServiceStateCallBacks:reportServiceStatus(org.apache.hadoop.ha.HAServiceStatus)" : null,
  "org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat:getOwner()" : null,
  "org.apache.hadoop.security.UGIExceptionMessages:<init>()" : null,
  "org.apache.hadoop.fs.impl.prefetch.BlockOperations:setDebug(boolean)" : null,
  "org.apache.hadoop.metrics2.lib.MutableQuantiles:<init>()" : null,
  "org.apache.hadoop.util.AutoCloseableLock:newCondition()" : "* See {@link ReentrantLock#newCondition()}.\n   * @return the Condition object",
  "org.apache.hadoop.ipc.RefreshRegistry:register(java.lang.String,org.apache.hadoop.ipc.RefreshHandler)" : "* Registers an object as a handler for a given identity.\n   * Note: will prevent handler from being GC'd, object should unregister itself\n   *  when done\n   * @param identifier a unique identifier for this resource,\n   *                   such as org.apache.hadoop.blacklist\n   * @param handler the object to register",
  "org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry:getSettings()" : null,
  "org.apache.hadoop.fs.impl.WeakRefMetricsSource:getSource()" : "* Get the source, will be null if the reference has been GC'd\n   * @return the source reference",
  "org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:checkGetDirectBuffer(java.nio.ByteBuffer[],int,int)" : null
}