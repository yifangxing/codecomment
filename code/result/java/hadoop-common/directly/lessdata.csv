file_path,Name,full_name,Start Line,End Line,Comment,Pre_Comment,child Name,domain,inner_method,node_level
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileRange.java,createFileRange,"org.apache.hadoop.fs.FileRange:createFileRange(long,int)",73,75,"/**
* Creates a FileRange object with specified offset and length.
* @param offset file offset in bytes
* @param length file length in bytes
*/","* Factory method to create a FileRange object.
   * @param offset starting offset of the range.
   * @param length length of the range.
   * @return a new instance of FileRangeImpl.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileRange.java,createFileRange,"org.apache.hadoop.fs.FileRange:createFileRange(long,int,java.lang.Object)",84,86,"/**
* Creates a new FileRange object with the specified offset, length and reference.
* @param offset starting position in bytes
* @param length file size in bytes
* @param reference arbitrary user-provided data (e.g. metadata)","* Factory method to create a FileRange object.
   * @param offset starting offset of the range.
   * @param length length of the range.
   * @param reference nullable reference to store in the range.
   * @return a new instance of FileRangeImpl.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,validateRangeRequest,org.apache.hadoop.fs.VectoredReadUtils:validateRangeRequest(org.apache.hadoop.fs.FileRange),65,75,"/**
* Validates a FileRange object.
* @param range FileRange to validate
* @return validated FileRange object or throws exception if invalid
*/","* Validate a single range.
   * @param range range to validate.
   * @return the range.
   * @param <T> range type
   * @throws IllegalArgumentException the range length is negative or other invalid condition
   * is met other than the those which raise EOFException or NullPointerException.
   * @throws EOFException the range offset is negative
   * @throws NullPointerException if the range is null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNull,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNull(java.lang.Object,java.lang.String)",45,47,"/**
* Ensures the provided object is not null.
* @param obj the object to validate
* @param argName name of the argument for error message
*/","* Validates that the given reference argument is not null.
   * @param obj the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPositiveInteger,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPositiveInteger(long,java.lang.String)",54,56,"/**
* Validates that a given long value represents a positive integer.
* @param value the value to validate
* @param argName name of the argument being validated
*/","* Validates that the given integer argument is not zero or negative.
   * @param value the argument value to validate
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNegative,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNegative(long,java.lang.String)",63,65,"/**
* Ensures the provided long value is non-negative.
* @param value the value to validate
* @param argName name of the argument being validated
*/","* Validates that the given integer argument is not negative.
   * @param value the argument value to validate
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkRequired,"org.apache.hadoop.fs.impl.prefetch.Validate:checkRequired(boolean,java.lang.String)",72,74,"/**
* Checks if a value is present and throws an exception if not.
* @param isPresent true if the value is present, false otherwise
* @param argName name of the argument being checked
*/","* Validates that the expression (that checks a required field is present) is true.
   * @param isPresent indicates whether the given argument is present.
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkValid,"org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String)",81,83,"/**
* Asserts whether a value is valid.
* @param isValid true if valid, false otherwise
* @param argName name of the argument being validated
*/","* Validates that the expression (that checks a field is valid) is true.
   * @param isValid indicates whether the given argument is valid.
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkValid,"org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String,java.lang.String)",91,96,"/**
* Validates argument value against a set of allowed values.
* @param isValid flag indicating whether the value is valid
* @param argName name of the argument being validated
* @param validValues comma-separated list of valid values
*/","* Validates that the expression (that checks a field is valid) is true.
   * @param isValid indicates whether the given argument is valid.
   * @param argName the name of the argument being validated.
   * @param validValues the list of values that are allowed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkValuesEqual,"org.apache.hadoop.fs.impl.prefetch.Validate:checkValuesEqual(long,java.lang.String,long,java.lang.String)",201,213,"/**
* Verifies that two values are equal, throwing an exception if not.
* @param value1 the first value to compare
* @param value1Name name of the first value
* @param value2 the second value to compare
* @param value2Name name of the second value
*/","* Validates that the given two values are equal.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkIntegerMultiple,"org.apache.hadoop.fs.impl.prefetch.Validate:checkIntegerMultiple(long,java.lang.String,long,java.lang.String)",222,234,"/**
* Validates if the given values are integer multiples of each other.
* @param value1 numeric value to check
* @param value1Name name of the first value
* @param value2 numeric value against which to check
* @param value2Name name of the second value
*/","* Validates that the first value is an integer multiple of the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkGreater,"org.apache.hadoop.fs.impl.prefetch.Validate:checkGreater(long,java.lang.String,long,java.lang.String)",243,255,"/**
* Checks if the first value is greater than the second.
* @param value1 long value to compare
* @param value1Name name of the first value
* @param value2 long value to compare against
* @param value2Name name of the second value
*/","* Validates that the first value is greater than the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkGreaterOrEqual,"org.apache.hadoop.fs.impl.prefetch.Validate:checkGreaterOrEqual(long,java.lang.String,long,java.lang.String)",264,276,"/**
* Validates if the first value is greater than or equal to the second.
* @param value1 long value to validate
* @param value1Name name of the first value
* @param value2 long value for comparison
* @param value2Name name of the second value
*/","* Validates that the first value is greater than or equal to the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkLessOrEqual,"org.apache.hadoop.fs.impl.prefetch.Validate:checkLessOrEqual(long,java.lang.String,long,java.lang.String)",285,297,"/**
* Validates that one long value is less than or equal to another.
* @param value1 the first value
* @param value1Name name of the first value
* @param value2 the second value
* @param value2Name name of the second value
*/","* Validates that the first value is less than or equal to the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkWithinRange,"org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(long,java.lang.String,long,long)",306,318,"/**
* Validates if a given value is within a specified inclusive range.
* @param value the value to check
* @param valueName name of the value being validated
* @param minValueInclusive minimum allowed value (inclusive)
* @param maxValueInclusive maximum allowed value (inclusive) 
*/","* Validates that the given value is within the given range of values.
   * @param value the value to check.
   * @param valueName the name of the argument.
   * @param minValueInclusive inclusive lower limit for the value.
   * @param maxValueInclusive inclusive upper limit for the value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkWithinRange,"org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(double,java.lang.String,double,double)",327,339,"/**
* Validates if a numeric value falls within a specified inclusive range.
* @param value numeric value to check
* @param valueName descriptive name of the value
* @param minValueInclusive minimum allowed value (inclusive)
* @param maxValueInclusive maximum allowed value (inclusive)
*/","* Validates that the given value is within the given range of values.
   * @param value the value to check.
   * @param valueName the name of the argument.
   * @param minValueInclusive inclusive lower limit for the value.
   * @param maxValueInclusive inclusive upper limit for the value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotEmpty(int,java.lang.String)",393,398,"/**
* Verifies that an integer array has at least one element.
* @param arraySize size of the array
* @param argName name of the argument being validated
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BulkDeleteUtils.java,validateBulkDeletePaths,"org.apache.hadoop.fs.BulkDeleteUtils:validateBulkDeletePaths(java.util.Collection,int,org.apache.hadoop.fs.Path)",39,48,"/**
* Validates a collection of bulk delete paths.
* @param paths collection of file system paths to validate
* @param pageSize maximum number of paths allowed per page
* @param basePath base directory for validation
*/","* Preconditions for bulk delete paths.
   * @param paths paths to delete.
   * @param pageSize maximum number of paths to delete in a single operation.
   * @param basePath base path for the delete operation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:<init>(java.io.File),177,182,"/**
* Initializes upload data from a local file.
* @param file the file to be uploaded
*/","* File constructor; input stream and byteArray will be null.
     *
     * @param file file to upload",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,requireIOStatisticsSnapshot,org.apache.hadoop.io.wrappedio.WrappedStatistics:requireIOStatisticsSnapshot(java.io.Serializable),352,356,"/**
* Casts a serializable object to an IOStatisticsSnapshot.
* @param snapshot serialized IOStatisticsSnapshot object
*/","* Require the parameter to be an instance of {@link IOStatisticsSnapshot}.
   * @param snapshot object to validate
   * @return cast value
   * @throws IllegalArgumentException if the supplied class is not a snapshot",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputWrapper.java,<init>,"org.apache.hadoop.net.SocketInputWrapper:<init>(java.net.Socket,java.io.InputStream)",43,52,"/**
* Initializes the SocketInputWrapper with a socket and input stream.
* @param s the underlying socket
* @param is the input stream to read from; must be a SocketInputStream if the socket has a channel
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfigurationHelper.java,mapEnumNamesToValues,"org.apache.hadoop.util.ConfigurationHelper:mapEnumNamesToValues(java.lang.String,java.lang.Class)",109,124,"/**
* Maps enum names (in lower case) to their corresponding Enum values with a specified prefix.
* @param prefix string prefix to append to each enum name
* @param enumClass Class of the Enum type to map
* @return Map of lowercase enum names to their original Enum values or null if empty
*/","* Given an enum class, build a map of lower case names to values.
   * @param prefix prefix (with trailing ""."") for path capabilities probe
   * @param enumClass class of enum
   * @param <E> enum type
   * @return a mutable map of lower case names to enum values
   * @throws IllegalArgumentException if there are two entries which differ only by case.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,sortRanges,org.apache.hadoop.fs.VectoredReadUtils:sortRanges(java.util.List),358,361,"/**
* Sorts and returns an array of file ranges from a list.
* @param input list of file ranges to be sorted
*/","* Sort the input ranges by offset; no validation is done.
   * <p>
   * This method is used externally and must be retained with
   * the signature unchanged.
   * @param input input ranges.
   * @return a new list of the ranges, sorted by offset.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,perms,org.apache.hadoop.fs.Options$CreateOpts:perms(org.apache.hadoop.fs.permission.FsPermission),65,67,"/**
 * Creates a Perms instance from an FsPermission.
 * @param perm permission to wrap in a Perms object
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,seekInternal,org.apache.hadoop.fs.sftp.SFTPInputStream:seekInternal(),79,96,"/**
* Seeks the internal stream to the specified position.
* @throws IOException if an error occurs while seeking
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,checkNotClosed,org.apache.hadoop.fs.sftp.SFTPInputStream:checkNotClosed(),135,141,"/**
* Verifies stream is not closed before proceeding.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,isParentOf,"org.apache.hadoop.fs.ftp.FTPFileSystem:isParentOf(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",648,657,"/**
* Checks if a path is a subdirectory of another.
* @param parent the potential parent directory
* @param child the directory to check
* @return true if child is a subdirectory of parent, false otherwise
*/","* Probe for a path being a parent of another
   * @param parent parent path
   * @param child possible child path
   * @return true if the parent's path matches the start of the child's",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,isSameFS,"org.apache.hadoop.fs.FileContext:isSameFS(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2306,2312,"/**
* Checks if two file system paths have the same scheme but different authorities.
* @param qualPath1 first path to compare
* @param qualPath2 second path to compare
* @return true if paths are in the same scheme but different hosts, false otherwise
*/","* Are qualSrc and qualDst of the same file system?
   * @param qualPath1 - fully qualified path
   * @param qualPath2 - fully qualified path
   * @return is same fs true,not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,deleteOnExit,org.apache.hadoop.fs.FileSystem:deleteOnExit(org.apache.hadoop.fs.Path),1804,1812,"/**
* Removes file on exit if it exists.
* @param f Path to the file
* @return true if file was scheduled for deletion, false otherwise
*/","* Mark a path to be deleted when its FileSystem is closed.
   * When the JVM shuts down cleanly, all cached FileSystem objects will be
   * closed automatically. These the marked paths will be deleted as a result.
   *
   * If a FileSystem instance is not cached, i.e. has been created with
   * {@link #createFileSystem(URI, Configuration)}, then the paths will
   * be deleted in when {@link #close()} is called on that instance.
   *
   * The path must exist in the filesystem at the time of the method call;
   * it does not have to exist at the time of JVM shutdown.
   *
   * Notes
   * <ol>
   *   <li>Clean shutdown of the JVM cannot be guaranteed.</li>
   *   <li>The time to shut down a FileSystem will depends on the number of
   *   files to delete. For filesystems where the cost of checking
   *   for the existence of a file/directory and the actual delete operation
   *   (for example: object stores) is high, the time to shutdown the JVM can be
   *   significantly extended by over-use of this feature.</li>
   *   <li>Connectivity problems with a remote filesystem may delay shutdown
   *   further, and may cause the files to not be deleted.</li>
   * </ol>
   * @param f the path to delete.
   * @return  true if deleteOnExit is successful, otherwise false.
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,processDeleteOnExit,org.apache.hadoop.fs.FileSystem:processDeleteOnExit(),1833,1848,"/**
* Removes files marked for deletion on exit.
* @throws IOException if unable to delete file
*/","* Delete all paths that were marked as delete-on-exit. This recursively
   * deletes all files and directories in the specified paths.
   *
   * The time to process this operation is {@code O(paths)}, with the actual
   * time dependent on the time for existence and deletion operations to
   * complete, successfully or not.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,keystoreExists,org.apache.hadoop.security.alias.KeyStoreProvider:keystoreExists(),58,61,"/**
* Checks if a keystore file exists at the specified path.
* @return true if keystore file is found, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,compareTo,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:compareTo(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode),140,151,"/**
* Compares the MRNflyNode based on its status modification time.
* @param other MRNflyNode to compare with
* @return -1 if this node has a newer status, 0 if both have null or same status, 1 otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getModificationTime,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getModificationTime(),504,507,"/**
* Returns the modification time of the underlying status. 
* @return Modification time in milliseconds since epoch.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getModificationTime,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getModificationTime(),80,83,"/**
* Retrieves file system modification time.
*@return last modified timestamp in milliseconds
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,decodeFileName,org.apache.hadoop.fs.HarFileSystem:decodeFileName(java.lang.String),261,268,"/**
* Decodes file name based on version-specific rules.
* @param fname original file name string
* @return decoded file name or original string if not applicable
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,compareTo,org.apache.hadoop.fs.shell.PathData:compareTo(org.apache.hadoop.fs.shell.PathData),593,596,"/**
* Compares this PathData with another by comparing their paths.
* @param o PathData object to compare against
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,isChecksumFile,org.apache.hadoop.fs.ChecksumFs:isChecksumFile(org.apache.hadoop.fs.Path),98,101,"/**
* Checks if a file has a CRC checksum.
* @param file Path object representing the file to check
*/","* Return true iff file is a checksum file name.
   *
   * @param file the file path.
   * @return if is checksum file true,not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,isChecksumFile,org.apache.hadoop.fs.ChecksumFileSystem:isChecksumFile(org.apache.hadoop.fs.Path),130,133,"/**
* Checks if the given file has a CRC checksum.
* @param file Path to the file to check
* @return true if the file ends with "".crc"", false otherwise
*/","* Return true if file is a checksum file name.
   *
   * @param file the file path.
   * @return if file is a checksum file true, not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,fixBlockLocations,"org.apache.hadoop.fs.HarFileSystem:fixBlockLocations(org.apache.hadoop.fs.BlockLocation[],long,long,long)",423,457,"/**
* Adjusts BlockLocation array based on desired file range.
* @param locations input array of block locations
* @param start start offset of desired range
* @param len length of desired range
* @param fileOffsetInHar offset of desired file in HAR
* @return modified BlockLocation array","* Fix offset and length of block locations.
   * Note that this method modifies the original array.
   * @param locations block locations of har part file
   * @param start the start of the desired range in the contained file
   * @param len the length of the desired range
   * @param fileOffsetInHar the offset of the desired file in the har part file
   * @return block locations with fixed offset and length",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,compareTo,org.apache.hadoop.fs.FileStatus:compareTo(org.apache.hadoop.fs.FileStatus),411,413,"/**
* Compares this FileStatus with another based on file path.
* @param o another FileStatus object to compare with
*/","* Compare this FileStatus to another FileStatus based on lexicographical
   * order of path.
   * @param   o the FileStatus to be compared.
   * @return  a negative integer, zero, or a positive integer as this object
   *   is less than, equal to, or greater than the specified object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,stat2Paths,org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[]),114,122,"/**
* Converts array of file statuses to array of paths.
* @param stats array of FileStatus objects
* @return Path[] or null if input is null
*/","* convert an array of FileStatus to an array of Path
   *
   * @param stats
   *          an array of FileStatus objects
   * @return an array of paths corresponding to the input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,addPartFileStatuses,org.apache.hadoop.fs.HarFileSystem$HarMetaData:addPartFileStatuses(org.apache.hadoop.fs.Path),1148,1152,"/**
* Adds file statuses to a map based on the provided directory path.
* @param path directory path to scan for file statuses
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,merge,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:merge(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileStatus[])",1228,1242,"/**
* Merges two arrays of file statuses, removing duplicates based on path name.
* @param toStatuses array of file statuses to include
* @param fromStatuses array of file statuses to potentially add
* @return merged array of unique file statuses
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getPath,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getPath(),529,532,"/**
* Returns the path associated with this status.
* @return Path object representing the status location
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,merge,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:merge(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileStatus[])",1622,1636,"/**
* Merges two arrays of file statuses, removing duplicates by path name.
* @param toStatuses array of initial file statuses
* @param fromStatuses additional file statuses to merge
* @return merged array of unique file statuses or null if empty
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,resolveIntermediate,org.apache.hadoop.fs.FileContext:resolveIntermediate(org.apache.hadoop.fs.Path),2353,2361,"/**
* Resolves intermediate file system links.
* @param f path to resolve
*/","* Resolves all symbolic links in the specified path leading up 
   * to, but not including the final path component.
   * @param f path to resolve
   * @return the new path object.
   * @throws IOException If an I/O error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getReplication,org.apache.hadoop.fs.FileSystem:getReplication(org.apache.hadoop.fs.Path),1603,1606,"/**
* Retrieves replication factor from file status.
* @param src path to file or directory
* @return replication factor (short value)
*/","* Get the replication factor.
   *
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @param src file name
   * @return file replication
   * @throws FileNotFoundException if the path does not resolve.
   * @throws IOException an IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getReplication,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getReplication(),499,502,"/**
* Retrieves replication status from underlying data.
* @return Replication level as a short value.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getReplication,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getReplication(),75,78,"/**
* Retrieves file system replication level.
* @return Replication level (short value) representing file system configuration.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBlockSize,org.apache.hadoop.fs.FileSystem:getBlockSize(org.apache.hadoop.fs.Path),2742,2745,"/**
* Retrieves file block size from file status.
* @param f Path to file
* @return File block size in bytes or 0 if unavailable
*/","* Get the block size for a particular file.
   * @param f the filename
   * @return the number of bytes in a block
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @throws FileNotFoundException if the path is not present
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getBlockSize,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getBlockSize(),494,497,"/**
* Retrieves block size from associated real status.
* @return Block size in bytes or 0 if not available.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getBlockSize,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getBlockSize(),70,73,"/**
* Returns file system block size.
* @return Block size in bytes.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getAccessTime,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getAccessTime(),509,512,"/**
* Returns the access time of the underlying status.
* @return timestamp in milliseconds representing when access was last granted
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getAccessTime,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getAccessTime(),85,88,"/**
* Returns the last access time of the file system.
* @return Last access time in milliseconds since the epoch
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getPermission,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getPermission(),514,517,"/**
* Retrieves file system permission based on underlying status.
* @return FsPermission object representing current permissions
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getPermission,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getPermission(),90,93,"/**
* Retrieves file system permission.
* @return FsPermission object representing the current permission
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,stashOriginalFilePermissions,org.apache.hadoop.security.alias.KeyStoreProvider:stashOriginalFilePermissions(),73,79,"/**
* Saves original file permissions for potential future use.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,isPermissionLoaded,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:isPermissionLoaded(),927,929,"/**
 * Checks if permission is loaded based on owner presence. 
 * @return true if owner is not empty, false otherwise 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getOwner,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getOwner(),519,522,"/**
* Retrieves the owner of the status update.
* @return the owner's username or null if not set
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getOwner,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getOwner(),95,98,"/**
* Retrieves filesystem owner name.
* @return owner name as string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getGroup,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getGroup(),524,527,"/**
* Retrieves group status from underlying data source.
* @return Group status string or null if not available
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getGroup,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getGroup(),100,103,"/**
 * Retrieves the group associated with the current file system.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,msync,org.apache.hadoop.fs.HarFileSystem:msync(),661,664,"/**
 * Synchronizes file system data to stable storage.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,msync,org.apache.hadoop.fs.FilterFileSystem:msync(),465,468,"/**
 * Flushes and synchronizes file buffer with storage.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.HarFileSystem:getDefaultReplication(),1294,1298,"/**
* Returns the default replication factor from the underlying file system.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultReplication,org.apache.hadoop.fs.FileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),2785,2787,"/**
* Returns default replication value (assuming same as global default).
*/","* Get the default replication for a path.
   * The given path will be used to locate the actual FileSystem to query.
   * The full path does not have to exist.
   * @param path of the file
   * @return default replication for the path's filesystem",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.FilterFileSystem:getDefaultReplication(),431,434,"/**
* Retrieves default replication setting.
* @return Default replication value (short)",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,cleanUp,org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReference:cleanUp(),4148,4159,"/**
* Removes thread-local statistics and folds them into root data.
*/","* Performs clean-up action when the associated thread is garbage
       * collected.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlConnection.java,<init>,"org.apache.hadoop.fs.FsUrlConnection:<init>(org.apache.hadoop.conf.Configuration,java.net.URL)",48,53,"/**
* Initializes a new FsUrlConnection with the specified configuration and URL.
* @param conf Hadoop Configuration object
* @param url File system URL to connect to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,validatePositionedReadArgs,"org.apache.hadoop.fs.FSInputStream:validatePositionedReadArgs(long,byte[],int,int)",102,116,"/**
* Validates positioned read arguments to prevent errors.
* @param position file position
* @param buffer data buffer
* @param offset buffer offset
* @param length requested bytes
*/","* Validation code, available for use in subclasses.
   * @param position position: if negative an EOF exception is raised
   * @param buffer destination buffer
   * @param offset offset within the buffer
   * @param length length of bytes to read
   * @throws EOFException if the position is negative
   * @throws IndexOutOfBoundsException if there isn't space for the amount of
   * data requested.
   * @throws IllegalArgumentException other arguments are invalid.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkUploadId,org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkUploadId(byte[]),80,85,"/**
* Validates non-empty byte array for upload ID.
* @param uploadId byte array containing the upload identifier
*/","* Utility method to validate uploadIDs.
   * @param uploadId Upload ID
   * @throws IllegalArgumentException invalid ID",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkPartHandles,org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPartHandles(java.util.Map),92,100,"/**
* Validates uploaded parts by checking for empty uploads and invalid indices.
*@param partHandles Map of part handles to their indices
*/","* Utility method to validate partHandles.
   * @param partHandles handles
   * @throws IllegalArgumentException if the parts are invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/PathCapabilitiesSupport.java,validatePathCapabilityArgs,"org.apache.hadoop.fs.impl.PathCapabilitiesSupport:validatePathCapabilityArgs(org.apache.hadoop.fs.Path,java.lang.String)",42,49,"/**
* Converts capability to lowercase and validates input arguments.
* @param path file system path
* @param capability capability string (non-empty, non-null)
* @return capability in lowercase form
*/","* Validate the arguments to
   * {@link PathCapabilities#hasPathCapability(Path, String)}.
   * @param path path to query the capability of.
   * @param capability non-null, non-empty string to query the path for support.
   * @return the string to use in a switch statement.
   * @throws IllegalArgumentException if a an argument is invalid.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,<init>,"org.apache.hadoop.service.launcher.InterruptEscalator:<init>(org.apache.hadoop.service.launcher.ServiceLauncher,int)",74,78,"/**
* Initializes interrupt escalator with service launcher and shutdown time.
* @param owner ServiceLauncher instance to which interrupts will be escalated
* @param shutdownTimeMillis time in milliseconds after which the escalator will shut down
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/IrqHandler.java,<init>,"org.apache.hadoop.service.launcher.IrqHandler:<init>(java.lang.String,org.apache.hadoop.service.launcher.IrqHandler$Interrupted)",78,83,"/**
* Initializes an IRQ handler with the given name and callback.
* @param name unique identifier for this IRQ handler
* @param handler Interrupted callback to be executed on IRQ occurrence
*/","* Create an IRQ handler bound to the specific interrupt.
   * @param name signal name
   * @param handler handler",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,partition,"org.apache.hadoop.util.Lists:partition(java.util.List,int)",269,284,"/**
* Partitions a list into smaller sub-lists of specified page size.
* @param originalList input list to partition
* @param pageSize number of elements per sub-list
* @return List of sub-lists or empty list if invalid input
*/","* Returns consecutive sub-lists of a list, each of the same size
   * (the final list may be smaller).
   * @param originalList original big list.
   * @param pageSize desired size of each sublist ( last one
   *                 may be smaller)
   * @param <T> Generics Type.
   * @return a list of sub lists.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,<init>,"org.apache.hadoop.util.JsonSerialization:<init>(java.lang.Class,boolean,boolean)",105,113,"/**
* Configures JSON serialization for the given class type.
* @param classType class type to serialize
* @param failOnUnknownProperties whether to fail on unknown properties
* @param pretty whether to use indented output
*/","* Create an instance bound to a specific type.
   * @param classType class to marshall
   * @param failOnUnknownProperties fail if an unknown property is encountered.
   * @param pretty generate pretty (indented) output?",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,fetch,"org.apache.hadoop.fs.FileSystemStorageStatistics:fetch(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData,java.lang.String)",86,116,"/**
* Fetches a specific statistic from the provided StatisticsData.
* @param data statistics data to query
* @param key name of the statistic to fetch (e.g. ""bytesRead"")
* @return Long value of the requested statistic or null if unknown
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,<init>,"org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.statistics.IOStatistics)",47,54,"/**
* Initializes storage statistics from I/O statistics.
* @param name identifier
* @param scheme storage scheme
* @param ioStatistics input/output statistics data
*/","* Instantiate.
   * @param name storage statistics name.
   * @param scheme FS scheme; may be null.
   * @param ioStatistics IOStatistics source.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/EmptyStorageStatistics.java,<init>,org.apache.hadoop.fs.EmptyStorageStatistics:<init>(java.lang.String),28,30,"/**
 * Initializes empty storage statistics with specified name.
 * @param name unique identifier of the storage resource
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/UnionStorageStatistics.java,<init>,"org.apache.hadoop.fs.UnionStorageStatistics:<init>(java.lang.String,org.apache.hadoop.fs.StorageStatistics[])",79,92,"/**
* Constructs UnionStorageStatistics object with given name and array of StorageStatistics.
* @param name unique identifier for this union storage
* @param stats array of individual storage statistics, cannot be null or contain null elements
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,getScheme,org.apache.hadoop.fs.FileSystemStorageStatistics:getScheme(),127,130,"/**
* Retrieves the scheme from the underlying statistics.
* @return Scheme string (e.g., HTTP or HTTPS)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatistics,org.apache.hadoop.fs.FileSystem:getStatistics(),4560,4567,"/**
* Retrieves a map of existing statistics from the internal table.
* @return A synchronized map of scheme-to-statistics entries
*/","* Get the Map of Statistics object indexed by URI Scheme.
   * @return a Map having a key as URI scheme and value as Statistics object
   * @deprecated use {@link #getGlobalStorageStatistics()}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFs:listXAttrs(org.apache.hadoop.fs.Path),846,851,"/**
* Lists extended attributes for the given file or directory.
* @param path Path to the entity
* @return List of attribute names, or empty list if none exist
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listXAttrs,org.apache.hadoop.fs.FilterFs:listXAttrs(org.apache.hadoop.fs.Path),387,390,"/**
* Lists extended attributes of the given file system object.
* @param path Path to the file system object
* @return List of attribute names or empty list if none exist
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",1275,1284,"/**
* Creates a new FSDataOutputStream.
* @param f file path
* @param permission file permissions
* @param flags creation flags
* @param bufferSize buffer size
* @param replication replication factor
* @param blockSize block size
* @param progress progress monitor (optional)
*/","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * @param f the file name to open
   * @param permission file permission
   * @param flags {@link CreateFlag}s to use for this stream.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize block size
   * @param progress the progress reporter
   * @throws IOException IO failure
   * @see #setPermission(Path, FsPermission)
   * @return output stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,create,"org.apache.hadoop.fs.FilterFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",201,212,"/**
* Wraps the FSDataOutputStream creation method.
* @param f file path
* @return FSDataOutputStream object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createNonRecursive,"org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1456,1463,"/**
* Creates a non-recursive FSDataOutputStream for the given file path.
* @param f file path to create output stream for
* @param permission file permissions
* @param overwrite whether to overwrite existing files (default: false)
*/","* Opens an FSDataOutputStream at the indicated Path with write-progress
   * reporting. Same as create(), except fails if parent directory doesn't
   * already exist.
   * @param f the file name to open
   * @param permission file permission
   * @param overwrite if a file with this name already exists, then if true,
   * the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize block size
   * @param progress the progress reporter
   * @throws IOException IO failure
   * @see #setPermission(Path, FsPermission)
   * @return output stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.FilterFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",222,229,"/**
* Creates a non-recursive file output stream.
* @param f the file path
* @param permission the file permissions
* @param flags create flags
* @param bufferSize buffer size
* @param replication replication factor
* @param blockSize block size
* @param progress progress monitor
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathAccessDeniedException.java,<init>,org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String),24,26,"/**
* Constructs an exception indicating permission denied access to the specified file/directory path.
* @param path the file/directory path that caused the access denial
*/",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String),26,28,"/**
* Constructs an exception with a specific error message for a denied file system operation.
* @param path the path to which access was denied
*/",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,"org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.String)",34,36,"/**
* Constructs a custom exception for unauthorized directory access.
* @param path restricted file system path
* @param error descriptive error message
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String),26,28,"/**
 * Constructs a PathNotFoundException with the specified path and message.
 */",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,"org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.String)",34,36,"/**
* Constructs a custom exception for a missing file system path.
* @param path the requested file path
* @param error a detailed error message
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathExistsException.java,<init>,org.apache.hadoop.fs.PathExistsException:<init>(java.lang.String),26,28,"/**
 * Constructs a PathExistsException with the given file path.
 * @param path the existing file path
 */",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathExistsException.java,<init>,"org.apache.hadoop.fs.PathExistsException:<init>(java.lang.String,java.lang.String)",30,32,"/**
 * Constructs a custom exception for a non-existent file system path.
 * @param path the non-existent file system path
 * @param error an optional error message to accompany the exception
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,<init>,org.apache.hadoop.fs.PathIOException:<init>(java.lang.String),43,45,"/**
* Constructs an IOException with the specified file system path.
* @param path the file system path causing the exception
*/","* Constructor a generic I/O error exception
   *  @param path for the exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ClosedIOException.java,<init>,"org.apache.hadoop.fs.ClosedIOException:<init>(java.lang.String,java.lang.String)",36,38,"/**
 * Constructs a new ClosedIOException instance with specified file path and error message.
 */)","* Appends the custom error-message to the default error message.
   * @param path path that encountered the closed resource.
   * @param message custom error message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,getThisBuilder,org.apache.hadoop.fs.FSDataOutputStreamBuilder:getThisBuilder(),102,102,"/**
 * Returns this builder instance. 
 */",* Return the concrete implementation of the builder instance.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.permission.FsPermission),43,47,"/**
* Converts FsPermission object to FsPermissionProto.
* @param p FsPermission object
* @return FsPermissionProto object or throws IOException on conversion error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,checkReturnValue,"org.apache.hadoop.fs.FileUtil:checkReturnValue(boolean,java.io.File,org.apache.hadoop.fs.permission.FsPermission)",1510,1518,"/**
* Verifies the return value of setting file permissions.
* @param rv true if successful, false otherwise
* @param p File object for which permissions were set
* @param permission target FsPermission to be applied
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,write,org.apache.hadoop.fs.permission.FsPermission:write(java.io.DataOutput),179,183,"/**
* Writes user ID as short value to output stream.
* @throws IOException if I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,toExtendedShort,org.apache.hadoop.fs.permission.FsPermission:toExtendedShort(),240,243,"/**
* Converts to Extended Short using legacy conversion.
* @deprecated Use toShort() instead.","* Encodes the object to a short.  Unlike {@link #toShort()}, this method may
   * return values outside the fixed range 00000 - 01777 if extended features
   * are encoded into this permission, such as the ACL bit.
   *
   * @return short extended short representation of this permission",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,toOctal,org.apache.hadoop.fs.permission.FsPermission:toOctal(),251,255,"/**
* Converts the short integer to its equivalent octal representation.
* @return Octal value as a short integer
*/","* Returns the FsPermission in an octal format.
   *
   * @return short Unlike {@link #toShort()} which provides a binary
   * representation, this method returns the standard octal style permission.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,hashCode,org.apache.hadoop.fs.permission.FsPermission:hashCode(),269,270,"/**
 * Calculates and returns hash code as a short integer value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringInterner.java,internStringsInArray,org.apache.hadoop.util.StringInterner:internStringsInArray(java.lang.String[]),81,86,"/**
* Interns all strings in the given array using a weak intern mechanism.
* @param strings input array of strings
* @return modified string array with interned values
*/","* Interns all the strings in the given array in place,
   * returning the same array.
   *
   * @param strings strings.
   * @return internStringsInArray.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleStartProperty,org.apache.hadoop.conf.Configuration$Parser:handleStartProperty(),3267,3294,"/**
* Initializes configuration properties from start tag attributes.
* @param reader XML attribute reader
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,isDir,org.apache.hadoop.fs.FileStatus:isDir(),240,243,"/**
* Checks if current file path points to a directory.
* @return true if it's a directory, false otherwise
*/","* Old interface, instead use the explicit {@link FileStatus#isFile()},
   * {@link FileStatus#isDirectory()}, and {@link FileStatus#isSymlink()}
   * @return true if this is a directory.
   * @deprecated Use {@link FileStatus#isFile()},
   * {@link FileStatus#isDirectory()}, and {@link FileStatus#isSymlink()}
   * instead.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,isDirectory,org.apache.hadoop.fs.ChecksumFs:isDirectory(org.apache.hadoop.fs.Path),446,453,"/**
* Checks if the specified file is a directory.
* @param f Path to the file
* @return true if the file is a directory, false otherwise
*/","True iff the named path is a directory.
   * Note: Avoid using this method. Instead reuse the FileStatus 
   * returned by getFileStatus() or listStatus() methods.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processPath,org.apache.hadoop.fs.shell.CopyCommands$Merge:processPath(org.apache.hadoop.fs.shell.PathData),133,143,"/**
 * Recursively processes a directory or file path, adding files to the result set.
 * @param src PathData object containing the directory/file information
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,isPathRecursable,org.apache.hadoop.fs.shell.Command:isPathRecursable(org.apache.hadoop.fs.shell.PathData),418,420,"/**
* Checks if a path is directory and therefore recursable.
* @param item PathData object containing file information
*/","* Determines whether a {@link PathData} item is recursable. Default
   * implementation is to recurse directories but can be overridden to recurse
   * through symbolic links.
   *
   * @param item
   *          a {@link PathData} object
   * @return true if the item is recursable, false otherwise
   * @throws IOException
   *           if anything goes wrong in the user-implementation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,getAclEntries,org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:getAclEntries(org.apache.hadoop.fs.shell.PathData),283,289,"/**
* Retrieves ACL entries for the given PathData item.
* @param item PathData object containing file/directory metadata
*/","* Returns the ACL entries to use in the API call for the given path.  For a
     * recursive operation, returns all specified ACL entries if the item is a
     * directory or just the access ACL entries if the item is a file.  For a
     * non-recursive operation, returns all specified ACL entries.
     *
     * @param item PathData path to check
     * @return List<AclEntry> ACL entries to use in the API call",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processPathArgument,org.apache.hadoop.fs.shell.FsUsage$Du:processPathArgument(org.apache.hadoop.fs.shell.PathData),209,217,"/**
* Processes a path argument, recursing into directories unless in summary mode.
* @param item PathData object containing file/directory information
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,isDirectory,org.apache.hadoop.fs.FileSystem:isDirectory(org.apache.hadoop.fs.Path),1877,1884,"/**
* Checks if a file exists and is a directory.
* @param f Path to the file
* @return true if the file exists and is a directory, false otherwise
*/","True iff the named path is a directory.
   * Note: Avoid using this method. Instead reuse the FileStatus
   * returned by getFileStatus() or listStatus() methods.
   *
   * @param f path to check
   * @throws IOException IO failure
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @return if f is directory true, not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,isDirectory,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isDirectory(),484,487,"/**
 * Checks if this file system object represents a directory.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,isDirectory,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isDirectory(),60,63,"/**
* Checks whether the current file system object represents a directory.
* @return true if it's a directory, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ChmodParser.java,applyNewPermission,org.apache.hadoop.fs.permission.ChmodParser:applyNewPermission(org.apache.hadoop.fs.FileStatus),48,54,"/**
* Applies new permissions to a file status.
* @param file the file status
* @return updated short permission value
*/","* Apply permission against specified file and determine what the
   * new mode would be
   * @param file File against which to apply mode
   * @return File's new mode if applied.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,isFile,org.apache.hadoop.fs.FileStatus:isFile(),220,222,"/**
* Checks if the file system entry is a regular file.
* @return true if it's a file, false otherwise
*/","* Is this a file?
   * @return true if this is a file",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,getSymlink,org.apache.hadoop.fs.FileStatus:getSymlink(),393,398,"/**
* Retrieves the symbolic link associated with this path.
* @return The symbolic link, or null if it's not a symbolic link
*/","* @return The contents of the symbolic link.
   *
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,isSymlink,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isSymlink(),489,492,"/**
* Checks if this file is a symbolic link.
* @return true if it's a symlink, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,isSymlink,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isSymlink(),65,68,"/**
* Checks if the file system entry is a symbolic link.
* @return true if it's a symlink, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getFileLength,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:getFileLength(),270,275,"/**
* Retrieves the length of a file.
* @return the file length in bytes or -1 for unknown files
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getFileLength,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getFileLength(),330,335,"/**
* Retrieves the length of the file in bytes.
* @return the length of the file, or -1 if unknown
*/","* Calculate length of file if not already cached.
     * @return file length.
     * @throws IOException any IOE.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,totalPartsLen,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:totalPartsLen(java.util.List),168,174,"/**
* Calculates the total length of all parts in the given list.
* @param partHandles list of Path objects representing file parts
* @return total length in bytes or -1 if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getLength,org.apache.hadoop.fs.FileSystem:getLength(org.apache.hadoop.fs.Path),1912,1915,"/**
* Retrieves the length of a file in bytes.
* @param f Path to the file
* @return Length of the file or -1 if not found
*/","* The number of bytes in a file.
   * @param f the path.
   * @return the number of bytes; 0 for a directory
   * @deprecated Use {@link #getFileStatus(Path)} instead.
   * @throws FileNotFoundException if the path does not resolve
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getLen,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getLen(),474,477,"/**
* Retrieves the length of the underlying status.
* @return Length as a long integer value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getLen,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getLen(),50,53,"/**
* Retrieves the file length from the underlying file system.
* @return File length in bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,<init>,"org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String,byte)",82,91,"/**
* Initializes FsServerDefaults object with specified parameters.
* @param blockSize block size
* @param bytesPerChecksum checksum bytes per packet
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFs:getStoragePolicy(org.apache.hadoop.fs.Path),418,422,"/**
* Retrieves block storage policy associated with the given file path.
* @param src file path to fetch policy for
* @return BlockStoragePolicySpi object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs:getStoragePolicy(org.apache.hadoop.fs.Path),914,919,"/**
* Retrieves the storage policy for a given file system path.
* @param src file system path
* @return BlockStoragePolicySpi instance or null if not found
*/","* Retrieve the storage policy for a given file or directory.
   *
   * @param src file or directory path.
   * @return storage policy for give file.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getStoragePolicy,org.apache.hadoop.fs.FilterFs:getStoragePolicy(org.apache.hadoop.fs.Path),432,436,"/**
* Retrieves storage policy for a given file system path.
* @param src file system path to retrieve policy for
* @return BlockStoragePolicySpi object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",816,822,"/**
* Sets an extended attribute on the specified file.
* @param path   file to set XAttr for
* @param name   XAttr name
* @param value  XAttr value
* @param flag   flags for setting XAttr
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,setXAttr,"org.apache.hadoop.fs.AbstractFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",1358,1362,"/**
* Sets extended attribute with specified name and value.
* @param path file system path
* @param name extended attribute name
* @param value byte array representing the attribute value
*/","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setXAttr,"org.apache.hadoop.fs.FilterFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",365,369,"/**
* Sets extended attribute on a file or directory.
* @param path file system path
* @param name attribute name
* @param value attribute value as byte array
* @param flag XAttrSetFlag enumeration specifying the operation mode
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,blockSize,org.apache.hadoop.fs.Options$CreateOpts:blockSize(long),46,48,"/**
* Creates a new BlockSize instance with the specified byte size.
* @param bs byte size to initialize the block size with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,bufferSize,org.apache.hadoop.fs.Options$CreateOpts:bufferSize(int),49,51,"/**
* Creates BufferSize instance from given size value.
* @param bs size value to initialize BufferSize with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,repFac,org.apache.hadoop.fs.Options$CreateOpts:repFac(short),52,54,"/**
* Creates a ReplicationFactor instance from a short value.
* @param rf replication factor value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,bytesPerChecksum,org.apache.hadoop.fs.Options$CreateOpts:bytesPerChecksum(short),55,57,"/**
* Creates BytesPerChecksum instance from given CRC value.
* @param crc 16-bit checksum value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,checksumParam,org.apache.hadoop.fs.Options$CreateOpts:checksumParam(org.apache.hadoop.fs.Options$ChecksumOpt),58,61,"/**
* Creates a ChecksumParam object from a given ChecksumOpt.
* @param csumOpt Checksum options
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,progress,org.apache.hadoop.fs.Options$CreateOpts:progress(org.apache.hadoop.util.Progressable),62,64,"/**
* Creates a new Progress instance from a Progressable.
* @param prog object to be converted into Progress
* @return newly created Progress object or null if invalid input
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,createParent,org.apache.hadoop.fs.Options$CreateOpts:createParent(),68,70,"/**
* Creates a new instance of CreateParent with enabled flag set to true.
* @return CreateParent object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,donotCreateParent,org.apache.hadoop.fs.Options$CreateOpts:donotCreateParent(),71,73,"/**
 * Creates a CreateParent instance with auto-create disabled.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathAccessDeniedException.java,<init>,"org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",32,36,"/**
* Constructs a PathAccessDeniedException with specified error message and cause.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,"org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",38,42,"/**
* Constructs a custom exception with specified file system path and error message.
* @param path affected file system path
* @param error detailed error description
* @param cause underlying cause of the issue (optional)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,"org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",38,42,"/**
* Constructs a PathNotFoundException with specified error and cause.
* @param path affected file or directory path
* @param error brief description of the exception
* @param cause underlying exception (optional)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,<init>,"org.apache.hadoop.fs.PathIOException:<init>(java.lang.String,java.lang.Throwable)",52,54,"/**
* Creates an IOException with file system related error.
* @param path file system path
* @param cause underlying exception cause
*/","* Appends the text of a Throwable to the default error message
   * @param path for the exception
   * @param cause a throwable to extract the error message",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFs:removeAcl(org.apache.hadoop.fs.Path),794,800,"/**
* Removes ACL from the specified file or directory.
* @param path Filesystem path of the item to remove ACL from
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeAcl,org.apache.hadoop.fs.FilterFs:removeAcl(org.apache.hadoop.fs.Path),344,347,"/**
 * Removes Access Control List (ACL) for the specified file system path.
 * @param path file system path to remove ACL from
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AvroFSInput.java,seek,org.apache.hadoop.fs.AvroFSInput:seek(long),75,78,"/**
 * Seeks to a specific position in the underlying stream.
 * @param p position to seek to (in bytes)
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long,long,int)",924,937,"/**
* Initializes a HarFsInputStream from a file, seeking to a specified offset and length.
* @param fs FileSystem instance
* @param path Path to the file
* @param start Offset to seek to (in bytes)
* @param length Length of the data to read (non-negative)
* @param bufferSize Buffer size for reading
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,skip,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:skip(long),1006,1022,"/**
* Skips up to n bytes from current position.
* @param n number of bytes to skip
* @return actual number of bytes skipped (clamped to remaining size)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,seek,org.apache.hadoop.io.SequenceFile$Reader:seek(long),2818,2824,"/**
* Seeks to a specific position in the input stream.
* @param position new seek position
*/","* Set the current byte position in the input file.
     *
     * <p>The position passed must be a position returned by {@link
     * SequenceFile.Writer#getLength()} when writing this file.  To seek to an arbitrary
     * position, use {@link SequenceFile.Reader#sync(long)}. </p>
     *
     * @param position input position.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.java,read,"org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[],int,int)",87,106,"/**
* Reads up to len bytes from the stream into b starting at offset off.
* @param b buffer to read into
* @param off starting offset in buffer
* @param len maximum number of bytes to read
* @return actual number of bytes read, or -1 on end-of-stream
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AvroFSInput.java,tell,org.apache.hadoop.fs.AvroFSInput:tell(),80,83,"/**
* Returns the current position in the input stream.
* @throws IOException if an I/O error occurs while accessing the stream
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,available,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:available(),939,946,"/**
* Returns the number of bytes that can be read from this stream without blocking.
* @return Number of available bytes, capped at Integer.MAX_VALUE to avoid overflow.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readRecordLength,org.apache.hadoop.io.SequenceFile$Reader:readRecordLength(),2538,2558,"/**
* Reads record length from input stream.
* @throws IOException if file is corrupt or at end of stream
*/","* Read and return the next record length, potentially skipping over 
     * a sync block.
     * @return the length of the next record or -1 if there is no next record
     * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getPosition,org.apache.hadoop.io.SequenceFile$Reader:getPosition(),2873,2875,"/**
* Retrieves the current position from the input stream.
* @throws IOException if an I/O error occurs
*/","* @return Return the current byte position in the input file.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setOwner,"org.apache.hadoop.fs.DelegateToFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",214,219,"/**
* Sets owner and group of a file or directory.
* @param f path to the file/directory
* @param username new user owner
* @param groupname new group owner
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processPath,org.apache.hadoop.fs.FsShellPermissions$Chown:processPath(org.apache.hadoop.fs.shell.PathData),173,190,"/**
* Updates file ownership if specified, otherwise leaves unchanged.
*@param item PathData object containing file metadata
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setOwner,"org.apache.hadoop.fs.FilterFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",533,537,"/**
* Sets owner of a file or directory.
* @param p Path to the file/directory
* @param username new owner's username
* @param groupname new owner's group name
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,registerExpression,org.apache.hadoop.fs.shell.find.ExpressionFactory:registerExpression(java.lang.Class),59,69,"/**
* Registers an instance of the given expression class.
* @param expressionClass type of expression to register
*/","* Invokes ""static void registerExpression(FindExpressionFactory)"" on the
   * given class. This method abstracts the contract between the factory and the
   * expression class. Do not assume that directly invoking registerExpression
   * on the given class will have the same effect.
   *
   * @param expressionClass
   *          class to allow an opportunity to register",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,registerCommands,org.apache.hadoop.fs.shell.CommandFactory:registerCommands(java.lang.Class),64,72,"/**
* Registers commands using a provided registrar class.
* @param registrarClass class containing registerCommands method
*/","* Invokes ""static void registerCommands(CommandFactory)"" on the given class.
   * This method abstracts the contract between the factory and the command
   * class.  Do not assume that directly invoking registerCommands on the
   * given class will have the same effect.
   * @param registrarClass class to allow an opportunity to register",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doResponse,"org.apache.hadoop.ipc.Server$RpcCall:doResponse(java.lang.Throwable,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)",1308,1328,"/**
* Handles RPC response with optional Throwable exception.
* @param t Throwable exception
* @param status Rpc status (optional)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileLinkStatus,org.apache.hadoop.fs.viewfs.ViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path),436,443,"/**
* Retrieves the file status for a given path.
* @param f Path to query
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getUri,org.apache.hadoop.fs.FilterFs:getUri(),179,182,"/**
* Returns the file system's URI.
* @return URI of the file system
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,setSymlink,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:setSymlink(org.apache.hadoop.fs.Path),544,547,"/**
* Sets symlink of the underlying file system object.
* @param p Path to the new symlink
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,read,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(),968,972,"/**
* Reads a single byte from the underlying stream.
* @return The byte value or -1 if end-of-stream is reached
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,read,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(byte[]),978,982,"/**
* Reads bytes from underlying stream into byte array.
* @param b target byte array to fill
* @return number of bytes read or -1 on end-of-stream
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,seek,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:seek(long),1029,1034,"/**
* Seeks to a specific position in the underlying stream.
* @param pos target position (in bytes)
* @throws IOException if an I/O error occurs during seeking
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,read,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(long,byte[],int,int)",1058,1071,"/**
* Reads data from underlying stream to provided buffer.
* @param pos relative position in file
* @param b destination byte array
* @param offset starting index in byte array
* @param length number of bytes to read
* @return actual number of bytes read or -1 for end-of-stream
*/",* implementing position readable.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,readFully,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:readFully(long,byte[],int,int)",1076,1087,"/**
* Reads fully into a byte array from the current position.
* @param pos current file position
* @param b target byte array
* @param offset starting index in the byte array
* @param length number of bytes to read
*/",* position readable again.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,setReadahead,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:setReadahead(java.lang.Long),1089,1092,"/**
* Sets read-ahead value on underlying stream.
* @param readahead new read-ahead value (null to reset)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,setDropBehind,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:setDropBehind(java.lang.Boolean),1094,1097,"/**
* Sets whether to write data without waiting for previous writes to complete.
* @param dropBehind flag indicating whether to drop behind or not
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,getCurrentTrashDir,org.apache.hadoop.fs.Trash:getCurrentTrashDir(org.apache.hadoop.fs.Path),198,200,"/**
* Retrieves the current trash directory based on the provided path.
* @param path file system path
* @return Path object to current trash directory or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,completed,"org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler:completed(java.lang.Integer,java.lang.Integer)",365,383,"/**
* Handles completed reads by processing the remaining buffer.
* @param result read result (-1 indicates EOF)
* @param r file ID associated with the read operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobExpander.java,expandLeftmost,org.apache.hadoop.fs.GlobExpander:expandLeftmost(org.apache.hadoop.fs.GlobExpander$StringWithOffset),86,146,"/**
* Expands the leftmost pattern in a file glob pattern with offset.
* @param filePatternWithOffset file glob pattern and its offset
* @return List of expanded patterns or null if not found
*/","* Expand the leftmost outer curly bracket pair containing a
   * slash character (""/"") in <code>filePattern</code>.
   * @param filePatternWithOffset
   * @return expanded file patterns
   * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatusBatch,"org.apache.hadoop.fs.FileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])",2060,2068,"/**
* Returns directory status as a single batch, ignoring provided token.
* @param f file path to retrieve listing for
* @param token ignored, not used in this implementation
*/","* Given an opaque iteration token, return the next batch of entries in a
   * directory. This is a private API not meant for use by end users.
   * <p>
   * This method should be overridden by FileSystem subclasses that want to
   * use the generic {@link FileSystem#listStatusIterator(Path)} implementation.
   * @param f Path to list
   * @param token opaque iteration token returned by previous call, or null
   *              if this is the first call.
   * @return directory entries.
   * @throws FileNotFoundException when the path does not exist.
   * @throws IOException If an I/O error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/XAttrCodec.java,encodeValue,"org.apache.hadoop.fs.XAttrCodec:encodeValue(byte[],org.apache.hadoop.fs.XAttrCodec)",109,119,"/**
* Encodes a byte array into a string using the specified codec.
* @param value input byte array
* @param encoding encoding scheme (HEX, BASE64, or STRING)
*/","* Encode byte[] value to string representation with encoding. 
   * Values encoded as text strings are enclosed in double quotes (\""), 
   * while strings encoded as hexadecimal and base64 are prefixed with 
   * 0x and 0s, respectively.
   * @param value byte[] value
   * @param encoding encoding.
   * @return String string representation of value
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,"org.apache.hadoop.fs.FileSystem:listStatus(java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2076,2085,"/**
* Filters file statuses from a directory and adds them to the result list.
* @param results list of filtered FileStatus objects
* @param f directory path
* @param filter PathFilter instance for filtering files
*/","* Filter files/directories in the given path using the user-supplied path
   * filter. Results are added to the given array <code>results</code>.
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsTag.java,<init>,"org.apache.hadoop.metrics2.MetricsTag:<init>(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",43,46,"/**
* Constructs a MetricsTag object from provided tag information and value.
* @param info tag metadata
* @param value the actual metric value as a string
*/","* Construct the tag with name, description and value
   * @param info  of the tag
   * @param value of the tag",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounter.java,<init>,org.apache.hadoop.metrics2.lib.MutableCounter:<init>(org.apache.hadoop.metrics2.MetricsInfo),35,37,"/**
 * Initializes a new MutableCounter instance with the provided metrics information. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGauge.java,<init>,org.apache.hadoop.metrics2.lib.MutableGauge:<init>(org.apache.hadoop.metrics2.MetricsInfo),35,37,"/**
* Initializes a new MutableGauge instance with the provided MetricsInfo. 
* @param info metrics information, must not be null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,<init>,org.apache.hadoop.metrics2.lib.MutableRates:<init>(org.apache.hadoop.metrics2.lib.MetricsRegistry),48,50,"/**
* Initializes MutableRates with a metrics registry.
* @param registry MetricsRegistry instance to track rates
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsInfoImpl.java,<init>,"org.apache.hadoop.metrics2.lib.MetricsInfoImpl:<init>(java.lang.String,java.lang.String)",34,37,"/**
* Initializes metrics info with given name and description.
* @param name unique identifier
* @param description brief description of the metric",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/AbstractMetric.java,<init>,org.apache.hadoop.metrics2.AbstractMetric:<init>(org.apache.hadoop.metrics2.MetricsInfo),41,43,"/**
 * Initializes an AbstractMetric instance with the given metrics information.
 * @param info MetricsInfo object containing metric data
 */","* Construct the metric
   * @param info  about the metric",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String,java.lang.String)",389,403,"/**
* Retrieves a delegation token from the specified authenticator.
* @param url URL to authenticate with
* @param token Existing authentication token (optional)
* @param renewer User or service to delegate to
* @param doAsUser User running the request
* @return Delegation token, or null on failure
*/","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @param doAsUser the user to do as, which will be the token owner.
   * @return a delegation token.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",433,446,"/**
* Renews a delegation token.
* @param url URL of the service
* @param token Token to renew
* @param doAsUser User to impersonate during renewal
* @return New delegationToken value or -1 on failure
*/","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",472,484,"/**
* Cancels a Kerberos delegation token.
* @param url URL of the service
* @param token Token to be cancelled
* @param doAsUser User running the cancellation operation
*/","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,<init>,"org.apache.hadoop.crypto.key.kms.ValueQueue:<init>(int,float,long,int,org.apache.hadoop.crypto.key.kms.ValueQueue$SyncGenerationPolicy,org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller)",222,260,"/**
* Initializes a ValueQueue with specified parameters.
* @param numValues total number of values
* @param lowWatermark minimum value threshold (0 < lowWatermark <= 1)
* @param expiry cache expiration time (in milliseconds)
* @param numFillerThreads number of filler threads to use
* @param policy synchronization generation policy
* @param refiller queue refiller object
*/","* Constructor takes the following tunable configuration parameters
   * @param numValues The number of values cached in the Queue for a
   *    particular key.
   * @param lowWatermark The ratio of (number of current entries/numValues)
   *    below which the <code>fillQueueForKey()</code> funciton will be
   *    invoked to fill the Queue.
   * @param expiry Expiry time after which the Key and associated Queue are
   *    evicted from the cache.
   * @param numFillerThreads Number of threads to use for the filler thread
   * @param policy The SyncGenerationPolicy to use when client
   *    calls ""getAtMost""
   * @param refiller implementation of the QueueRefiller",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Preconditions.java,checkNotNull,org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object),68,70,"/**
* Returns the provided object, throwing exception if it's null.
* @param obj Object to be checked
*/","* <p>Preconditions that the specified argument is not {@code null},
   * throwing a NPE exception otherwise.
   *
   * <p>The message of the exception is
   * &quot;The validated object is null&quot;.</p>
   *
   * @param <T> the object type
   * @param obj  the object to check
   * @return the validated object
   * @throws NullPointerException if the object is {@code null}
   * @see #checkNotNull(Object, Object)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFs:getAclStatus(org.apache.hadoop.fs.Path),809,814,"/**
* Retrieves ACL status for a given file system path.
* @param path the file system path to query
* @return AclStatus object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getAclStatus,org.apache.hadoop.fs.FilterFs:getAclStatus(org.apache.hadoop.fs.Path),354,357,"/**
* Retrieves ACL status for a given file or directory.
* @param path file system path to query
* @return AclStatus object representing the access control list status
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobalStorageStatistics.java,put,"org.apache.hadoop.fs.GlobalStorageStatistics:put(java.lang.String,org.apache.hadoop.fs.GlobalStorageStatistics$StorageStatisticsProvider)",73,93,"/**
* Retrieves or creates StorageStatistics by name, using the provider to generate it if not found.
* @param name unique identifier
* @param provider StorageStatisticsProvider instance
* @return non-null StorageStatistics object
*/","* Create or return the StorageStatistics object with the given name.
   *
   * @param name        The storage statistics object name.
   * @param provider    An object which can create a new StorageStatistics
   *                      object if needed.
   * @return            The StorageStatistics object with the given name.
   * @throws RuntimeException  If the StorageStatisticsProvider provides a null
   *                           object or a new StorageStatistics object with the
   *                           wrong name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobalStorageStatistics.java,next,org.apache.hadoop.fs.GlobalStorageStatistics$StorageIterator:next(),127,139,"/**
* Retrieves the next available storage statistics.
* @return StorageStatistics object or null if exhausted
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,clearStatistics,org.apache.hadoop.fs.FileSystem:clearStatistics(),4610,4612,"/**
* Resets global storage statistics.
*/",* Reset all statistics for all file systems.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,<init>,org.apache.hadoop.fs.FsShell$UnknownCommandException:<init>(),410,410,"/**
* Initializes an UnknownCommandException with no command name.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,close,org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:close(),258,266,"/**
* Closes the block upload data instance and performs necessary cleanups.
* @throws IOException if an I/O error occurs during cleanup
*/","* Close: closes any upload stream and byteArray provided in the
     * constructor.
     *
     * @throws IOException inherited exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,copyFileUnbuffered,"org.apache.hadoop.io.nativeio.NativeIO:copyFileUnbuffered(java.io.File,java.io.File)",1138,1161,"/**
* Copies a file from source to destination without buffering.
* @param src source file
* @param dst destination file
*/","* Unbuffered file copy from src to dst without tainting OS buffer cache
   *
   * In POSIX platform:
   * It uses FileChannel#transferTo() which internally attempts
   * unbuffered IO on OS with native sendfile64() support and falls back to
   * buffered IO otherwise.
   *
   * It minimizes the number of FileChannel#transferTo call by passing the the
   * src file size directly instead of a smaller size as the 3rd parameter.
   * This saves the number of sendfile64() system call when native sendfile64()
   * is supported. In the two fall back cases where sendfile is not supported,
   * FileChannle#transferTo already has its own batching of size 8 MB and 8 KB,
   * respectively.
   *
   * In Windows Platform:
   * It uses its own native wrapper of CopyFileEx with COPY_FILE_NO_BUFFERING
   * flag, which is supported on Windows Server 2008 and above.
   *
   * Ideally, we should use FileChannel#transferTo() across both POSIX and Windows
   * platform. Unfortunately, the wrapper(Java_sun_nio_ch_FileChannelImpl_transferTo0)
   * used by FileChannel#transferTo for unbuffered IO is not implemented on Windows.
   * Based on OpenJDK 6/7/8 source code, Java_sun_nio_ch_FileChannelImpl_transferTo0
   * on Windows simply returns IOS_UNSUPPORTED.
   *
   * Note: This simple native wrapper does minimal parameter checking before copy and
   * consistency check (e.g., size) after copy.
   * It is recommended to use wrapper function like
   * the Storage#nativeCopyFileUnbuffered() function in hadoop-hdfs with pre/post copy
   * checks.
   *
   * @param src                  The source path
   * @param dst                  The destination path
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,closeStream,org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable),276,280,"/**
* Closes a given Closeable stream and performs cleanup.
* @param stream stream to be closed
*/","* Closes the stream ignoring {@link Throwable}.
   * Must only be called in cleaning up from exception handlers.
   *
   * @param stream the Stream to close",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,closeStreams,org.apache.hadoop.io.IOUtils:closeStreams(java.io.Closeable[]),288,292,"/**
* Closes multiple streams with logging.
* @param streams array of Closeable streams to be closed
*/","* Closes the streams ignoring {@link Throwable}.
   * Must only be called in cleaning up from exception handlers.
   *
   * @param streams the Streams to close",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,stop,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:stop(),207,218,"/**
* Terminates the process by interrupting and joining the sink thread.
* Closes any Closeable resources held by the sink.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,close,org.apache.hadoop.crypto.OpensslCtrCryptoCodec:close(),111,117,"/**
* Closes the random number generator, releasing any underlying resources.
* @throws IOException if an I/O error occurs during cleanup
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,close,org.apache.hadoop.crypto.random.OsSecureRandom:close(),120,126,"/**
* Closes the underlying stream and releases resources.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,diskIoCheckWithoutNativeIo,org.apache.hadoop.util.DiskChecker:diskIoCheckWithoutNativeIo(java.io.File),283,302,"/**
* Deletes a file and performs disk I/O checks without native I/O.
* @param file the file to delete and check
*/","* Try to perform some disk IO by writing to the given file
   * without using Native IO.
   *
   * @param file
   * @throws IOException if there was a non-retriable error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hflush,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hflush(),501,504,"/**
* Flushes buffered output and ensures all data is written to underlying stream.
* This implementation simply calls the standard flush() method. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hsync,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hsync(),510,514,"/**
* Synchronizes and flushes file output stream.
*/","* HSync calls sync on fhe file descriptor after a local flush() call.
     * @throws IOException failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,close,org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:close(),95,105,"/**
* Closes the object and records timed operation statistics.
* @param none
*/","* Set the finished time and then update the statistics.
   * If the operation failed then the key + .failures counter will be
   * incremented by one.
   * The operation min/mean/max values will be updated with the duration;
   * on a failure these will all be the .failures metrics.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,skip,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:skip(long),274,283,"/**
* Skips 'n' bytes in the input stream.
* @param n number of bytes to skip
* @return number of skipped bytes or -1 if invalid operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,write,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(byte[],int,int)",479,488,"/**
* Writes data to the file stream.
* @param b byte array to write, starting at offset off for length len
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,write,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(int),490,499,"/**
* Writes a byte to the file stream and updates write statistics.
* @throws IOException if an I/O operation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hasCapability,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hasCapability(java.lang.String),516,527,"/**
* Checks if the given capability is supported.
* @param capability unique capability identifier
* @return true if supported, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,<init>,"org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,java.util.List,org.apache.hadoop.ipc.RemoteException)",52,58,"/**
* Initializes a PartialListing object with the given path, listing data, or an associated exception.
* @param listedPath directory path
* @param partialListing list of items (or null if exception is provided)
* @param exception remote exception (or null if listing data is provided) 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,setCount,org.apache.hadoop.io.DataOutputBuffer$Buffer:setCount(int),78,83,"/**
* Updates and returns the previous count value, ensuring new count is within valid bounds.
* @param newCount new value for the count, must be between 0 and buffer length
* @return old count value before update","* Set the count for the current buf.
     * @param newCount the new count to set
     * @return the original count",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,"org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Object,java.lang.Throwable,org.apache.hadoop.io.retry.CallReturn$State)",60,65,"/**
* Initializes a CallReturn object with return value, exception, and state.
* @param r the return value or null
* @param t the thrown exception or null
* @param s the execution state
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getConnectorAddress,org.apache.hadoop.http.HttpServer2:getConnectorAddress(int),1330,1342,"/**
* Retrieves the address of a server connector by index.
* @param index unique index of the connector (0-based)
* @return InetSocketAddress object or null if invalid or closed","* Get the address that corresponds to a particular connector.
   *
   * @param index index.
   * @return the corresponding address for the connector, or null if there's no
   *         such connector or the connector is not bounded or was closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec:calculateIV(byte[],long,byte[],int)",63,80,"/**
* Updates IV value based on initial IV, counter, and block size.
* @param initIV initial IV bytes
* @param counter counter value
* @param iv updated IV bytes
* @param blockSize size of each IV block
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.JceCtrCryptoCodec:calculateIV(byte[],long,byte[],int)",55,72,"/**
* Updates IV with initIV and counter values.
* @param initIV initialization vector, blockSize bytes long
* @param counter 64-bit value to incorporate into IV
* @param iv updated IV buffer, blockSize bytes long
* @param blockSize size of IV in bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,<init>,"org.apache.hadoop.util.GcTimeMonitor:<init>(long,long,int,org.apache.hadoop.util.GcTimeMonitor$GcTimeAlertHandler)",125,151,"/**
* Initializes the GcTimeMonitor with specified observation window, sleep interval,
* and maximum GC time percentage. Sets up internal buffers and daemon thread.
* @param observationWindowMs observation window in milliseconds
* @param sleepIntervalMs sleep interval in milliseconds
* @param maxGcTimePercentage maximum GC time as a percentage of total time
* @param alertHandler handler for GC time alerts
*/","* Create an instance of GCTimeMonitor. Once it's started, it will stay alive
   * and monitor GC time percentage until shutdown() is called. If you don't
   * put a limit on the number of GCTimeMonitor instances that you create, and
   * alertHandler != null, you should necessarily call shutdown() once the given
   * instance is not needed. Otherwise, you may create a memory leak, because
   * each running GCTimeMonitor will keep its alertHandler object in memory,
   * which in turn may reference and keep in memory many more other objects.
   *
   * @param observationWindowMs the interval over which the percentage
   *   of GC time should be calculated. A practical value would be somewhere
   *   between 30 sec and several minutes.
   * @param sleepIntervalMs how frequently this thread should wake up to check
   *   GC timings. This is also a frequency with which alertHandler will be
   *   invoked if GC time percentage exceeds the specified limit. A practical
   *   value would likely be 500..1000 ms.
   * @param maxGcTimePercentage A GC time percentage limit (0..100) within
   *   observationWindowMs. Once this is exceeded, alertHandler will be
   *   invoked every sleepIntervalMs milliseconds until GC time percentage
   *   falls below this limit.
   * @param alertHandler a single method in this interface is invoked when GC
   *   time percentage exceeds the specified limit.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ServletUtil.java,getRawPath,"org.apache.hadoop.util.ServletUtil:getRawPath(javax.servlet.http.HttpServletRequest,java.lang.String)",107,110,"/**
* Extracts raw path from HTTP request.
* @param request HTTP request object
* @param servletName name of the servlet to match
* @return raw path as a string or null if invalid request
*/","* Parse the path component from the given request and return w/o decoding.
   * @param request Http request to parse
   * @param servletName the name of servlet that precedes the path
   * @return path component, null if the default charset is not supported",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,getMovableTypes,org.apache.hadoop.fs.StorageType:getMovableTypes(),78,80,"/**
 * Returns list of movable storage types.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,getTypesSupportingQuota,org.apache.hadoop.fs.StorageType:getTypesSupportingQuota(),82,84,"/**
* Returns list of storage types supporting quota.
* @return List of StorageType objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,parseStorageType,org.apache.hadoop.fs.StorageType:parseStorageType(java.lang.String),90,92,"/**
* Parses string representation of storage type to enum value.
* @param s string containing storage type (e.g. ""HDD"", ""SSD"")
* @return StorageType enum value or null if invalid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,initMode,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initMode(),631,638,"/**
* Determines the initialization mode based on system property and environment variable.
* @return InitMode enum value (NORMAL, DEVELOPMENT, etc.)",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",838,844,"/**
* Retrieves extended attributes for the given file or directory.
* @param path file system path
* @param names list of attribute names to retrieve
* @return map of attribute names to their corresponding byte values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getXAttrs,"org.apache.hadoop.fs.FilterFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",381,385,"/**
* Retrieves extended attributes by name from the file system.
* @param path file system path to query
* @param names list of attribute names to fetch
* @return map of attribute names to their corresponding byte arrays or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,unbuffer,org.apache.hadoop.fs.FSDataInputStream:unbuffer(),237,240,"/**
 * Unbuffers input stream according to policy.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,equals,org.apache.hadoop.fs.FileStatus:equals(java.lang.Object),435,445,"/**
* Compares two FileStatus objects for equality based on their paths.
* @param o the object to compare with
* @return true if both objects have the same path, false otherwise
*/","Compare if this object is equal to another object
   * @param   o the object to be compared.
   * @return  true if two file status has the same path name; false if not.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,equals,org.apache.hadoop.fs.shell.PathData:equals(java.lang.Object),598,603,"/**
* Compares this PathData object with another Object for equality.
* Two PathData objects are considered equal if their 'path' fields match.
* @param o the Object to compare with
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,hashCode,org.apache.hadoop.fs.FileStatus:hashCode(),453,456,"/**
* Returns hash code based on file path.
* @return Hash code of file path
*/","* Returns a hash code value for the object, which is defined as
   * the hash code of the path name.
   *
   * @return  a hash code value for the path name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,hashCode,org.apache.hadoop.fs.shell.PathData:hashCode(),605,608,"/**
* Returns hash code based on the underlying path's hash code.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,setPath,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:setPath(org.apache.hadoop.fs.Path),534,537,"/**
 * Sets the path of the real status.
 * @param p Path object to be assigned
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,calculateFolderSize,org.apache.hadoop.fs.DUHelper:calculateFolderSize(java.lang.String),38,43,"/**
* Calculates total size of a given file system folder.
* @param folder path to the folder
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,check,org.apache.hadoop.fs.DUHelper:check(java.lang.String),45,53,"/**
* Calculates and returns used space, file count, and usage percentage for a given folder.
* @param folder path to the folder
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",853,858,"/**
* Removes an extended attribute from a file or directory.
* @param path file system path
* @param name name of the attribute to remove
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeXAttr,"org.apache.hadoop.fs.FilterFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",392,395,"/**
* Removes extended attribute by name from specified file path.
* @param path file system path to operate on
* @param name name of the extended attribute to remove
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,setSamplesAndSum,"org.apache.hadoop.fs.statistics.MeanStatistic:setSamplesAndSum(long,long)",157,161,"/**
* Sets samples count and sum in a thread-safe manner.
* @param sampleCount new number of samples
* @param newSum updated sum value
*/","* Set the sum and samples.
   * Synchronized.
   * @param sampleCount new sample count.
   * @param newSum new sum",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,add,org.apache.hadoop.fs.statistics.MeanStatistic:add(org.apache.hadoop.fs.statistics.MeanStatistic),212,230,"/**
* Merges two MeanStatistic objects into one.
* @param other MeanStatistic object to merge
* @return updated MeanStatistic object with combined data
*/","* Add another MeanStatistic.
   * @param other other value
   * @return mean statistic.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,equals,org.apache.hadoop.fs.statistics.MeanStatistic:equals(java.lang.Object),254,269,"/**
* Compares this MeanStatistic object with another for equality.
* @param o Object to compare with
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,toString,org.apache.hadoop.fs.statistics.MeanStatistic:toString(),285,289,"/**
* Returns a human-readable string representation of this data set.
* @return formatted string with sample count, total sum, and mean value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,mapToString,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:mapToString(java.lang.StringBuilder,java.lang.String,java.util.Map,java.lang.String)",133,149,"/**
* Converts a map to a string representation.
* @param sb StringBuilder to append the result
* @param type Type prefix for the map (e.g., ""user"")
* @param map Map of key-value pairs
* @param separator String delimiter between entries
*/","* Given a map, add its entryset to the string.
   * The entries are only sorted if the source entryset
   * iterator is sorted, such as from a TreeMap.
   * @param sb string buffer to append to
   * @param type type (for output)
   * @param map map to evaluate
   * @param separator separator
   * @param <E> type of values of the map",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,entryToString,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:entryToString(java.util.Map$Entry),136,139,"/**
* Converts a map entry to a string representation.
* @param entry map entry containing key and value
*/","* Convert an entry to the string format used in logging.
   *
   * @param entry entry to evaluate
   * @param <E> entry type
   * @return formatted string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationTrackerFactory.java,trackDuration,"org.apache.hadoop.fs.statistics.DurationTrackerFactory:trackDuration(java.lang.String,long)",48,50,"/**
* Tracks duration with specified key and count.
* @param key unique identifier for tracked duration
* @param count time value to be tracked in milliseconds
*/","* Initiate a duration tracking operation by creating/returning
   * an object whose {@code close()} call will
   * update the statistics.
   *
   * The statistics counter with the key name will be incremented
   * by the given count.
   *
   * The expected use is within a try-with-resources clause.
   *
   * The default implementation returns a stub duration tracker.
   * @param key statistic key prefix
   * @param count  #of times to increment the matching counter in this
   * operation.
   * @return an object to close after an operation completes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/EmptyPrefetchingStatistics.java,prefetchOperationStarted,org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:prefetchOperationStarted(),45,48,"/**
 * Returns a pre-configured duration tracker instance.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,getLong,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:getLong(java.lang.String),97,104,"/**
* Retrieves a long value from counters or gauges by key.
* @param key unique identifier for the value
* @return Long value or null if not found in either counter or gauge
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,isTracked,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:isTracked(java.lang.String),106,110,"/**
* Checks if a metric (key) is tracked by either counter or gauge.
* @param key unique metric identifier
* @return true if tracked, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,toLongStatistic,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:toLongStatistic(java.util.Map$Entry),85,87,"/**
* Converts a map entry of user ID and count to a LongStatistic object.
* @param e map entry containing user ID and count
*/","* Convert a counter/gauge entry to a long statistics.
   * @param e entry
   * @return statistic",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,next,org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:next(),70,78,"/**
* Advances to the next statistic in sequence and returns its value.
* @return LongStatistic object containing key-value pair or null if exhausted
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EvaluatingStatisticsMap.java,<init>,org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:<init>(),51,53,"/**
* Initializes statistics map with passthrough function.
* @param passthroughFn function to be passed through in statistics evaluation
*/",* Construct with the copy function being simple passthrough.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addCounterFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addCounterFunction(java.lang.String,java.util.function.Function)",91,93,"/**
* Adds a custom counter function with the specified key and evaluation logic.
* @param key unique identifier for the counter
* @param eval evaluation function that returns a Long value
*/","* add a mapping of a key to a counter function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addGaugeFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addGaugeFunction(java.lang.String,java.util.function.Function)",100,102,"/**
* Adds a gauge function with the specified key and evaluation expression.
* @param key unique identifier of the gauge function
* @param eval lambda expression that evaluates to a long value
*/","* add a mapping of a key to a gauge function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addMinimumFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMinimumFunction(java.lang.String,java.util.function.Function)",109,111,"/**
* Adds a custom evaluation function with a given key.
* @param key unique identifier
* @param eval Function to evaluate as a long value
*/","* add a mapping of a key to a minimum function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addMaximumFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMaximumFunction(java.lang.String,java.util.function.Function)",118,120,"/**
* Adds a function to evaluate maximum value based on input string.
* @param key unique identifier for the function
* @param eval function that takes input string and returns maximum long value
*/","* add a mapping of a key to a maximum function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addMeanStatisticFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMeanStatisticFunction(java.lang.String,java.util.function.Function)",127,130,"/**
* Adds a custom function to calculate mean statistic.
* @param key unique identifier for the function
* @param eval evaluation function that returns MeanStatistic object
*/","* add a mapping of a key to a meanStatistic function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,wrap,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:wrap(org.apache.hadoop.fs.statistics.IOStatistics),116,118,"/**
* Wraps given IOStatistics into a SourceWrappedStatistics object.
* @param statistics IOStatistics instance to be wrapped
*/","* Take an IOStatistics instance and wrap it in a source.
   * @param statistics statistics.
   * @return a source which will return the values",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EmptyIOStatisticsContextImpl.java,getAggregator,org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getAggregator(),49,52,"/**
 * Returns an empty aggregator instance.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,emptyStatisticsStore,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:emptyStatisticsStore(),107,109,"/**
* Creates an empty statistics store instance.
* @return EmptyIOStatisticsStore singleton instance
*/","* Get the shared instance of the immutable empty statistics
   * store.
   * @return an empty statistics object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EmptyIOStatisticsContextImpl.java,getIOStatistics,org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getIOStatistics(),54,57,"/**
* Returns empty IO statistics.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,emptyStatistics,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:emptyStatistics(),98,100,"/**
 * Returns an instance of EmptyIOStatistics.
 */","* Get the shared instance of the immutable empty statistics
   * object.
   * @return an empty statistics object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setCounter,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setCounter(java.lang.String,long)",172,176,"/**
* Sets atomic long value for given counter key.
* @param key unique counter identifier
* @param value new value for the counter
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setMaximum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMaximum(java.lang.String,long)",199,202,"/**
* Sets the maximum atomic long value in the map.
* @param key unique identifier of the maximum value
* @param value new maximum atomic long value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setMinimum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMinimum(java.lang.String,long)",209,212,"/**
* Sets minimum atomic long value in map by key.
* @param key unique identifier for minimum value
* @param value new minimum value to be stored
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setGauge,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setGauge(java.lang.String,long)",235,238,"/**
* Sets gauge metric by key and value.
* @param key unique metric identifier
* @param value metric value to be stored
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementCounter,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementCounter(java.lang.String,long)",178,197,"/**
* Increments a named counter atomically.
* @param key unique counter identifier
* @param value increment amount (may be negative)
* @return updated counter value or old value if invalid input
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementMaximum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementMaximum(java.lang.String,long)",204,207,"/**
* Atomically increments maximum value for given key.
* @param key unique identifier for maximum value
* @param value new value to update maximum with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementMinimum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementMinimum(java.lang.String,long)",214,217,"/**
* Increments atomic minimum value by specified delta.
* @param key unique map key
* @param value delta to add to minimum value
* @return updated minimum value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementGauge,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementGauge(java.lang.String,long)",240,243,"/**
* Increments an atomic long gauge by a specified value.
* @param key unique identifier for the gauge
* @param value amount to increment the gauge by
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addMinimumSample,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMinimumSample(java.lang.String,long)",219,225,"/**
* Updates the minimum sample value for a given key.
* @param key unique key identifier
* @param value new sample value to consider for minimum update
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addMaximumSample,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMaximumSample(java.lang.String,long)",227,233,"/**
* Updates or sets the maximum sample for a given key.
* @param key unique identifier
* @param value new maximum sample value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addMeanStatisticSample,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMeanStatisticSample(java.lang.String,long)",253,259,"/**
* Adds a sample to the mean statistic for the specified key.
* @param key unique identifier
* @param value new sample value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getCounterReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getCounterReference(java.lang.String),372,375,"/**
 * Retrieves atomic counter reference by specified key.
 * @param key unique identifier for counter
 */","* Get a reference to the atomic instance providing the
   * value for a specific counter. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getMaximumReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMaximumReference(java.lang.String),385,388,"/**
* Retrieves maximum reference value associated with the given key.
* @param key unique identifier for the maximum reference value
*/","* Get a reference to the atomic instance providing the
   * value for a specific maximum. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getMinimumReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMinimumReference(java.lang.String),398,401,"/**
* Retrieves minimum reference value associated with given key.
* @param key unique identifier or key
* @return AtomicLong containing minimum reference value or null if not found
*/","* Get a reference to the atomic instance providing the
   * value for a specific minimum. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getGaugeReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getGaugeReference(java.lang.String),411,414,"/**
* Retrieves gauge reference value by key from the gauge map.
* @param key unique identifier of the gauge reference
*/","* Get a reference to the atomic instance providing the
   * value for a specific gauge. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getMeanStatistic,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMeanStatistic(java.lang.String),422,425,"/**
 * Retrieves mean statistic value by its unique identifier. 
 * @param key unique key of mean statistic to fetch
 */","* Get a mean statistic.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/PairedDurationTrackerFactory.java,asDuration,org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:asDuration(),87,90,"/**
* Returns the duration representation of this time period.
* @return Duration object representing the same time interval
*/",* @return the global duration,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,counters,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:counters(),55,58,"/**
 * Returns counters for the wrapped object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,gauges,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:gauges(),79,82,"/**
* Returns a map of gauge metrics.
* @return A map with metric names as keys and values as counts.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,minimums,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:minimums(),84,87,"/**
* Retrieves minimum values from wrapped data.
* @return map of minimum values keyed by field name
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,maximums,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:maximums(),89,92,"/**
* Retrieves maximum values from wrapped data source.
* @return Map of maximum values keyed by field name.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,meanStatistics,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:meanStatistics(),94,97,"/**
* Returns statistics with means for wrapped data.
* @return map of statistic names to mean values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,setWrapped,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:setWrapped(org.apache.hadoop.fs.statistics.IOStatistics),73,77,"/**
* Sets the wrapped IO statistics.
* @param wrapped new IO statistics object
*/","* Set the wrapped statistics.
   * Will fail if the field is already set.
   * @param wrapped new value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,activeInstance,org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:activeInstance(),63,66,"/**
* Returns the currently active instance of DynamicIOStatistics.
* @return The active instance or null if not yet built
*/","* Get the statistics instance.
   * @return the instance to build/return
   * @throws IllegalStateException if the builder has already been built.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,checkMutable,org.apache.hadoop.fs.impl.FlagSet:checkMutable(),125,128,"/**
 * Verifies that FlagSet is mutable.
 * @throws IllegalStateException if FlagSet is immutable
 */","* Check for mutability before any mutating operation.
   * @throws IllegalStateException if the set is still mutable",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,toByteArray,org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:toByteArray(),235,250,"/**
* Converts block data to byte array. Reads from file or stream if necessary.
* @throws IOException on read errors
*/","* Convert to a byte array.
     * If the data is stored in a file, it will be read and returned.
     * If the data was passed in via an input stream (which happens if the
     * data is stored in a bytebuffer) then it will be converted to a byte
     * array -which will then be cached for any subsequent use.
     *
     * @return byte[] after converting the uploadBlock.
     * @throws IOException throw if an exception is caught while reading
     *                     File/InputStream or closing InputStream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/IrqHandler.java,bind,org.apache.hadoop.service.launcher.IrqHandler:bind(),89,100,"/**
* Binds a signal handler to the specified signal.
* @throws IllegalArgumentException if binding fails
*/","* Bind to the interrupt handler.
   * @throws IllegalArgumentException if the exception could not be set",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CloseableReferenceCount.java,unreference,org.apache.hadoop.util.CloseableReferenceCount:unreference(),65,70,"/**
* Unreferences an object, decrementing its reference count.
* @return true if the object is now closed, false otherwise
*/","* Decrement the reference count.
   *
   * @return          True if the object is closed and has no outstanding
   *                  references.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,run,org.apache.hadoop.ha.HealthMonitor$MonitorDaemon:run(),285,296,"/**
* Runs the application loop until interrupted or stopped.
* Performs health checks and connection establishment within each iteration. 
* @throws InterruptedException if thread is interrupted while running
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,setZooKeeperRef,org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:setZooKeeperRef(org.apache.zookeeper.ZooKeeper),1226,1231,"/**
* Sets the ZooKeeper reference, ensuring it's set only once.
* @param zk ZooKeeper instance to be referenced
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,snapshotMap,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:snapshotMap(java.util.Map,java.util.function.Function)",214,221,"/**
* Creates a serialized snapshot of the given map.
* @param source source map to be copied
* @param copyFn function to serialize each value
* @return a new map containing the serialized values
*/","* Take a snapshot of a supplied map, using the copy function
   * to replicate the source values.
   * @param source source map
   * @param copyFn function to copy the value
   * @param <E> type of values.
   * @return a concurrent hash map referencing the same values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,trackDuration,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDuration(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE)",445,450,"/**
* Tracks duration of an operation using the specified factory and input.
* @param factory Duration tracker factory
* @param statistic Name of the statistic to track
* @return Result object containing tracked duration or null if failed
*/","* Given an IOException raising callable/lambda expression,
   * execute it and update the relevant statistic.
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @param <B> return type.
   * @return the result of the operation.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,pairedTrackerFactory,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:pairedTrackerFactory(org.apache.hadoop.fs.statistics.DurationTrackerFactory,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",687,691,"/**
* Creates a factory for paired duration trackers from two individual factories.
* @param first first tracker factory
* @param second second tracker factory
*/","* Create a DurationTrackerFactory which aggregates the tracking
   * of two other factories.
   * @param first first tracker factory
   * @param second second tracker factory
   * @return a factory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterLong.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableCounterLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",60,66,"/**
* Records metrics snapshot based on 'all' flag or changes since last snapshot.
* @param builder MetricsRecordBuilder to add counter data
* @param all true for full snapshot, false for incremental update
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,getCacheHit,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheHit(),82,84,"/**
* Retrieves the current cache hit value.
* @return Cache hit count as a long integer",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,getCacheCleared,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheCleared(),86,88,"/**
* Retrieves the value of cleared cache.
* @return Number of cache entries cleared.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,getCacheUpdated,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheUpdated(),90,92,"/**
 * Retrieves the current cache update timestamp.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getClientBackoffDisconnected,org.apache.hadoop.ipc.metrics.RpcMetrics:getClientBackoffDisconnected(),358,360,"/**
* Retrieves the client backoff value for disconnections.
* @return The current backoff value in milliseconds.","* Returns the number of disconnected backoffs.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getRpcSlowCalls,org.apache.hadoop.ipc.metrics.RpcMetrics:getRpcSlowCalls(),420,422,"/**
* Retrieves the count of RPC slow calls.
* @return number of RPC calls exceeding performance threshold
*/","* Returns the number of slow calls.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getRpcRequeueCalls,org.apache.hadoop.ipc.metrics.RpcMetrics:getRpcRequeueCalls(),428,431,"/**
* Retrieves the count of RPC requeue calls.
* @return current RPC requeue call count
*/","* Returns the number of requeue calls.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,counters,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:counters(),48,51,"/**
* Retrieves inner statistics counters.
* @return Map of statistic counters with key-value pairs.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,gauges,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:gauges(),53,56,"/**
* Returns a map of gauge statistics from inner system.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,minimums,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:minimums(),58,61,"/**
* Returns inner statistics minimum values.
* @return map of minimum values by key (e.g. count, sum)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,maximums,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:maximums(),63,66,"/**
* Returns the maximum values of inner statistics.
* @return map with key-value pairs of maximums
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,meanStatistics,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:meanStatistics(),68,71,"/**
* Calculates mean statistics for this object.
* @return Map of mean statistics with keys as statistic names
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,aggregate,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:aggregate(org.apache.hadoop.fs.statistics.IOStatistics),73,76,"/**
 * Aggregates given IOStatistics with inner statistics.
 * @param statistics optional IOStatistics to aggregate (null if none)
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementCounter,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementCounter(java.lang.String,long)",78,81,"/**
* Increments counter by specified value for given key.
* @param key unique identifier of counter to update
* @param value new value to add to counter
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setCounter,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setCounter(java.lang.String,long)",83,86,"/**
* Sets counter value by key.
* @param key unique identifier for counter
* @param value new counter value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setGauge,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setGauge(java.lang.String,long)",88,91,"/**
* Sets gauge metric by key.
* @param key unique metric identifier
* @param value gauge value to set
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementGauge,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementGauge(java.lang.String,long)",93,96,"/**
* Increments a gauge with the specified key and value.
* @param key unique identifier for the gauge
* @param value new value to set in the gauge
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setMaximum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMaximum(java.lang.String,long)",98,101,"/**
* Sets maximum value for specified metric.
* @param key unique metric identifier
* @param value maximum value to store
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementMaximum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementMaximum(java.lang.String,long)",103,106,"/**
* Increments maximum value in inner statistics by specified amount.
* @param key unique identifier for statistic
* @param value value to add to maximum
* @return updated maximum value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setMinimum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMinimum(java.lang.String,long)",108,112,"/**
* Sets minimum statistics value for the specified key.
* @param key unique key identifier
* @param value minimum value to be recorded
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementMinimum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementMinimum(java.lang.String,long)",114,118,"/**
* Increments minimum statistics by provided key and value.
* @param key unique identifier for statistics
* @param value numeric value to add to minimum statistics
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addMinimumSample,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMinimumSample(java.lang.String,long)",120,124,"/**
* Adds minimum sample data to inner statistics.
* @param key unique identifier for sample data
* @param value numerical value of the sample data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addMaximumSample,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMaximumSample(java.lang.String,long)",126,129,"/**
* Adds maximum sample with given key and value.
* @param key unique identifier for the sample
* @param value maximum sample value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setMeanStatistic,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",131,135,"/**
* Sets mean statistic with specified key.
* @param key unique identifier for mean statistic
* @param value MeanStatistic object to be stored
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addMeanStatisticSample,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMeanStatisticSample(java.lang.String,long)",137,141,"/**
* Adds mean statistic sample with specified key and value.
* @param key unique identifier of the statistic
* @param value numerical value of the statistic
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,reset,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:reset(),143,146,"/**
* Resets inner statistics to their initial state.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getCounterReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getCounterReference(java.lang.String),148,151,"/**
* Retrieves counter reference by key from inner statistics.
* @param key unique identifier of the counter to retrieve
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getMaximumReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMaximumReference(java.lang.String),153,156,"/**
* Retrieves maximum reference count by key from inner statistics.
* @param key unique identifier for statistics to fetch
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getMinimumReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMinimumReference(java.lang.String),158,161,"/**
* Retrieves minimum reference by key from inner statistics.
* @param key unique identifier for minimum reference
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getGaugeReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getGaugeReference(java.lang.String),163,166,"/**
* Retrieves gauge reference by key from inner statistics.
* @param key unique gauge identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getMeanStatistic,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMeanStatistic(java.lang.String),168,171,"/**
* Retrieves mean statistic for given key.
* @param key unique identifier for statistics to fetch
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addTimedOperation(java.lang.String,long)",173,178,"/**
* Adds timed operation to inner statistics.
* @param prefix prefix string for timed operation
* @param durationMillis duration of timed operation in milliseconds
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addTimedOperation(java.lang.String,java.time.Duration)",180,184,"/**
* Adds timed operation to inner statistics.
* @param prefix unique identifier for timed operation
* @param duration time interval for the operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreBuilderImpl.java,withDurationTracking,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withDurationTracking(java.lang.String[]),77,93,"/**
* Enables duration tracking for specified prefixes.
* @param prefixes variable number of prefix strings
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreBuilderImpl.java,withSampleTracking,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withSampleTracking(java.lang.String[]),95,105,"/**
* Enables sample tracking for specified prefix(es).
* @param prefixes one or more prefix strings to track
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextImpl.java,reset,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:reset(),101,105,"/**
* Resets IO statistics context.
* Clears stored statistics and logs the event. 
*/",* Reset the thread +.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,<init>,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:<init>(),113,115,"/**
* Initializes IO statistics snapshot with default data.",* Construct.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setCounter,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setCounter(java.lang.String,long)",226,229,"/**
* Sets counter value by key.
* @param key unique identifier
* @param value new counter value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setGauge,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setGauge(java.lang.String,long)",231,235,"/**
* Sets gauge value for given key.
* @param key unique identifier for gauge
* @param value new value for the gauge
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setMaximum,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMaximum(java.lang.String,long)",237,241,"/**
* Sets the maximum value for a given key.
* @param key unique identifier for the maximum value
* @param value the new maximum value as a long
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setMinimum,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMinimum(java.lang.String,long)",243,246,"/**
* Sets the minimum value for a given metric.
* @param key unique metric identifier
* @param value minimum allowed value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setMeanStatistic,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",248,251,"/**
* Sets a new mean statistic with the given key.
* @param key unique identifier for the mean statistic
* @param value MeanStatistic object to be stored
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsContext.java,enabled,org.apache.hadoop.fs.statistics.IOStatisticsContext:enabled(),95,97,"/**
* Checks if IO statistics are enabled at thread level.
* @return true if enabled, false otherwise
*/","* Static probe to check if the thread-level IO statistics enabled.
   *
   * @return if the thread-level IO statistics enabled.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSupport.java,retrieveIOStatistics,org.apache.hadoop.fs.statistics.IOStatisticsSupport:retrieveIOStatistics(java.lang.Object),78,88,"/**
* Retrieves IO statistics from the given source object.
* @param source Object containing IO statistics, an instance of IOStatistics or IOStatisticsSource
* @return IOStatistics object or null if invalid source or interface not implemented
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,<init>,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:<init>(),123,133,"/**
* Initializes BuiltInGzipDecompressor with default state and CRC settings.
*/",* Creates a new (pure Java) gzip decompressor.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,available,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:available(),189,192,"/**
* Calculates total available bytes by summing up file data and superclass buffer availability.
* @return Total available bytes or -1 if not available
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,available,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:available(),225,228,"/**
* Calculates total available bytes.
* @return Total available space in bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,seekToNewSource,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seekToNewSource(long),221,227,"/**
* Seeks to a new source at the specified position.
* @param targetPos target position
* @return true if successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,readChunk,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:readChunk(long,byte[],int,int,byte[])",229,267,"/**
* Reads a chunk of data from the file and calculates checksum.
* @param buf buffer to store read data
* @param offset starting position in buffer
* @param len maximum number of bytes to read
* @return actual number of bytes read or -1 if EOF reached
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,verifySums,"org.apache.hadoop.fs.FSInputChecker:verifySums(byte[],int,int)",336,359,"/**
* Verifies checksums for a given byte array chunk.
* @param b the input byte array
* @param off starting offset in the array
* @throws ChecksumException if a mismatch is found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,throwChecksumException,"org.apache.hadoop.util.DataChecksum:throwChecksumException(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,java.lang.String,long,int,int)",514,521,"/**
* Throws ChecksumException with detailed error message.
* @param type checksum type (e.g. MD5, SHA-1)
* @param algorithm used checksum algorithm
* @param filename file name where mismatch occurred
* @param errPos position of error in file
* @param expected expected checksum value
* @param computed actual checksum value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getCounter,org.apache.hadoop.crypto.CryptoInputStream:getCounter(long),288,290,"/**
* Calculates counter value based on position and cipher suite block size.
* @param position current position
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getPadding,org.apache.hadoop.crypto.CryptoInputStream:getPadding(long),292,294,"/**
* Calculates padding bytes based on position modulo block size.
* @param position current position in the data stream
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,updateEncryptor,org.apache.hadoop.crypto.CryptoOutputStream:updateEncryptor(),220,228,"/**
* Updates and initializes the encryptor with a new IV based on the stream offset and cipher suite. 
* @throws IOException if an I/O error occurs during initialization
*/",Update the {@link #encryptor}: calculate counter and {@link #padding}.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,checkBufferSize,"org.apache.hadoop.crypto.CryptoStreamUtils:checkBufferSize(org.apache.hadoop.crypto.CryptoCodec,int)",90,95,"/**
* Calculates optimal buffer size based on cipher suite block size.
* @param codec CryptoCodec object
* @param bufferSize current buffer size
* @return adjusted buffer size or MIN_BUFFER_SIZE if invalid
*/","* Check and floor buffer size.
   *
   * @param codec crypto codec.
   * @param bufferSize the size of the buffer to be used.
   * @return calc buffer size.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,link,"org.apache.hadoop.io.nativeio.NativeIO:link(java.io.File,java.io.File)",1080,1087,"/**
* Links a file to another location.
* @param src source file
* @param dst destination file
*/","* Creates a hardlink ""dst"" that points to ""src"".
   *
   * This is deprecated since JDK7 NIO can create hardlinks via the
   * {@link java.nio.file.Files} API.
   *
   * @param src source file
   * @param dst hardlink location
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,getInstance,org.apache.hadoop.fs.DelegationTokenRenewer:getInstance(),200,205,"/**
* Returns a singleton instance of DelegationTokenRenewer.
* @return shared instance or initializes one if not created yet
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BatchedRemoteIterator.java,makeRequestIfNeeded,org.apache.hadoop.fs.BatchedRemoteIterator:makeRequestIfNeeded(),84,96,"/**
* Makes request when user ID is unknown or data limit has been reached.
* @throws IOException if network request fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32GzipFileChecksum.java,<init>,"org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)",38,40,"/**
* Initializes MD5MD5CRC32GzipFileChecksum with CRC and MD5 parameters.
* @param bytesPerCRC number of bytes per CRC block
* @param crcPerBlock CRC value for each block
* @param md5 MD5 hash object
*/","* Create a MD5FileChecksum.
   *
   * @param bytesPerCRC bytesPerCRC.
   * @param crcPerBlock crcPerBlock.
   * @param md5 md5.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,<init>,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:<init>(),43,45,"/**
 * Constructs an instance of MD5MD5CRC32FileChecksum with default values.
 */","Same as this(0, 0, null)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32CastagnoliFileChecksum.java,<init>,"org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)",38,40,"/**
* Initializes an instance of MD5MD5CRC32CastagnoliFileChecksum with the specified parameters.
* @param bytesPerCRC number of bytes per CRC block
* @param crcPerBlock initial CRC value for each block
* @param md5 MD5 hash object to combine with this checksum type
*/","* Create a MD5FileChecksum.
   *
   * @param bytesPerCRC bytesPerCRC.
   * @param crcPerBlock crcPerBlock.
   * @param md5 md5.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,accept,org.apache.hadoop.fs.GlobFilter:accept(org.apache.hadoop.fs.Path),79,82,"/**
* Validates file path by matching against a regex pattern and applying a user filter. 
* @param path file system path to validate
* @return true if the path matches the pattern and passes the filter, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobPattern.java,set,org.apache.hadoop.fs.GlobPattern:set(java.lang.String),74,157,"/**
* Converts a glob pattern to a regex string.
* @param glob user input glob pattern
*/","* Set and compile a glob pattern
   * @param glob  the glob pattern string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFsStatus,org.apache.hadoop.fs.FilterFs:getFsStatus(org.apache.hadoop.fs.Path),146,150,"/**
* Retrieves file system status by path.
* @param f the file path to query
* @return FsStatus object or throws an exception if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listStatusIterator,org.apache.hadoop.fs.FilterFileSystem:listStatusIterator(org.apache.hadoop.fs.Path),291,295,"/**
* Returns an iterator over file statuses in the specified directory.
* @param f Path to the directory
* @return RemoteIterator of FileStatus objects or null if not found
*/",Return a remote iterator for listing in a directory,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,isRegularFile,org.apache.hadoop.fs.FileUtil:isRegularFile(java.io.File),632,634,"/**
* Checks if the specified file is a regular (non-directory) file.
* @param file the file to check
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,makeShellPath,"org.apache.hadoop.fs.FileUtil:makeShellPath(java.io.File,boolean)",696,703,"/**
* Creates a shell path from the given File object.
* @param file File object to convert
* @param makeCanonicalPath whether to use canonical or absolute path
*/","* Convert a os-native filename to a path that works for the shell.
   * @param file The filename to convert
   * @param makeCanonicalPath
   *          Whether to make canonical path for the file passed
   * @return The unix pathname
   * @throws IOException on windows, there can be problems with the subprocess",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,permissionsFromMode,org.apache.hadoop.fs.FileUtil:permissionsFromMode(int),783,793,"/**
* Extracts POSIX file permissions from a given mode.
* @param mode integer representing file mode
*/","* The permission operation of this method only involves users, user groups, and others.
   * If SUID is set, only executable permissions are reserved.
   * @param mode Permissions are represented by numerical values
   * @return The original permissions for files are stored in collections",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unpackEntries,"org.apache.hadoop.fs.FileUtil:unpackEntries(org.apache.commons.compress.archivers.tar.TarArchiveInputStream,org.apache.commons.compress.archivers.tar.TarArchiveEntry,java.io.File)",1130,1186,"/**
* Recursively unpacks TarArchiveEntry into the specified output directory.
* @param tis Tar archive input stream
* @param entry Tar archive entry to unpack
* @param outputDir target directory for unpacking
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,join,"org.apache.hadoop.util.StringUtils:join(char,java.lang.String[])",1084,1086,"/**
* Joins an array of strings with a specified character separator.
* @param separator character to separate elements
* @param strings array of strings to concatenate
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execute,org.apache.hadoop.util.Shell$ShellCommandExecutor:execute(),1275,1283,"/**
* Validates and executes the command by running it.
* @throws IOException if a null value is found in the command
*/","* Execute the shell command.
     * @throws IOException if the command fails, or if the command is
     * not well constructed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,checkWindowsCommandLineLength,org.apache.hadoop.util.Shell:checkWindowsCommandLineLength(java.lang.String[]),127,140,"/**
* Validates the total length of multiple Windows command lines.
* @param commands variable arguments array of strings to check
*/","* Checks if a given command (String[]) fits in the Windows maximum command
   * line length Note that the input is expected to already include space
   * delimiters, no extra count will be added for delimiters.
   *
   * @param commands command parts, including any space delimiters
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/PowerShellFencer.java,buildPSScript,"org.apache.hadoop.ha.PowerShellFencer:buildPSScript(java.lang.String,java.lang.String)",115,158,"/**
* Builds a PowerShell script to kill a process on a remote host.
* @param processName name of the process to terminate
* @param host hostname or IP address of the remote machine
* @return path to the generated PowerShell script file, or null if failed
*/","* Build a PowerShell script to kill a java.exe process in a remote machine.
   *
   * @param processName Name of the process to kill. This is an attribute in
   *                    CommandLine.
   * @param host Host where the process is.
   * @return Path of the PowerShell script.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,toString,org.apache.hadoop.fs.permission.FsPermission:toString(),272,283,"/**
* Returns a string representation of the action, combining symbols and implying sticky bit if applicable. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,join,"org.apache.hadoop.util.StringUtils:join(char,java.lang.Iterable)",1058,1060,"/**
* Joins an iterable of objects into a string using the specified separator.
* @param separator character to separate joined strings
* @param strings iterable of strings to join
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,close,org.apache.hadoop.fs.sftp.SFTPFileSystem$2:close(),710,722,"/**
* Closes the resource and shuts down the connection pool.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,close,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:close(),644,653,"/**
* Closes the resource by flushing buffer and closing underlying streams.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,close,org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:close(),391,400,"/**
* Closes this object and releases any system resources associated with it.
* Flushes buffered data, closes related streams (sums, datas), and marks the object as closed.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,<init>,"org.apache.hadoop.fs.DU:<init>(java.io.File,long,long,long)",36,41,"/**
* Initializes a DU instance with the specified file path and metrics.
* @param path directory to monitor
* @param interval time interval between updates
* @param jitter randomization factor for update timing
* @param initialUsed initial disk usage value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,refresh,org.apache.hadoop.fs.DU:refresh(),50,58,"/**
* Refreshes disk usage information by starting the refresh process.
* @throws IOException if an error occurs while refreshing
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java,connect,"org.apache.hadoop.fs.sftp.SFTPConnectionPool:connect(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)",123,183,"/**
* Establishes a secure SFTP connection to the specified host.
* @param host server hostname
* @param port optional port number, defaults to 22 if not provided
* @param user username for authentication
* @param password or keyFile for authentication (mutually exclusive)
* @return ChannelSftp object representing the established connection
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java,disconnect,org.apache.hadoop.fs.sftp.SFTPConnectionPool:disconnect(com.jcraft.jsch.ChannelSftp),185,211,"/**
* Disconnects an SFTP channel, managing live connections and returning to the pool if necessary.
* @param channel SFTP channel object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,<init>,"org.apache.hadoop.fs.FSDataOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics,long)",86,90,"/**
* Constructs an FSDataOutputStream with the specified OutputStream and
* statistics. The stream is positioned at the given start position.
* @param out OutputStream to wrap
* @param stats FileSystemStatistics object for tracking usage
* @param startPosition starting position of the stream in bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,getChecksumSize,org.apache.hadoop.fs.FSOutputSummer:getChecksumSize(),197,199,"/**
* Returns the size of the checksum.
* @return Size of the checksum (in bytes)
*/",@return the size for a checksum.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,getChecksumSize,org.apache.hadoop.util.DataChecksum:getChecksumSize(int),345,347,"/**
* Calculates checksum size based on data size and bytes per checksum.
* @param dataSize total data size
*/","* the required checksum size given the data length.
   * @param dataSize data size.
   * @return the required checksum size given the data length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,convertToByteStream,"org.apache.hadoop.fs.FSOutputSummer:convertToByteStream(java.util.zip.Checksum,int)",237,239,"/**
* Converts Checksum object to byte stream.
* @param sum Checksum object
* @param checksumSize size of the resulting byte stream
* @return byte array representation of the checksum
*/","* Converts a checksum integer value to a byte stream
   *
   * @param sum check sum.
   * @param checksumSize check sum size.
   * @return byte stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesRead,org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesRead(long),4206,4208,"/**
* Increments total bytes read by specified amount.","* Increment the bytes read in the statistics.
     * @param newBytes the additional bytes read",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesWritten,org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesWritten(long),4214,4216,"/**
* Updates thread statistics with additional bytes written.
* @param newBytes amount of bytes to add
*/","* Increment the bytes written in the statistics.
     * @param newBytes the additional bytes written",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementReadOps,org.apache.hadoop.fs.FileSystem$Statistics:incrementReadOps(int),4222,4224,"/**
* Increments read operations counter by specified amount.
* @param count number of read operations to add
*/","* Increment the number of read operations.
     * @param count number of read operations",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementLargeReadOps,org.apache.hadoop.fs.FileSystem$Statistics:incrementLargeReadOps(int),4230,4232,"/**
* Increments large read operations counter by specified amount.
* @param count number of large read operations to add
*/","* Increment the number of large read operations.
     * @param count number of large read operations",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementWriteOps,org.apache.hadoop.fs.FileSystem$Statistics:incrementWriteOps(int),4238,4240,"/**
* Increments write operations counter by specified count.
* @param count number of write operations to add
*/","* Increment the number of write operations.
     * @param count number of write operations",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesReadErasureCoded,org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesReadErasureCoded(long),4246,4248,"/**
* Increments erasure-coded bytes read by the specified amount.
* @param newBytes additional bytes read
*/","* Increment the bytes read on erasure-coded files in the statistics.
     * @param newBytes the additional bytes read",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesReadByDistance,"org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesReadByDistance(int,long)",4258,4275,"/**
* Updates thread statistics based on distance and bytes read.
* @param distance network distance (0-5)
* @param newBytes the number of bytes to increment
*/","* Increment the bytes read by the network distance in the statistics
     * In the common network topology setup, distance value should be an even
     * number such as 0, 2, 4, 6. To make it more general, we group distance
     * by {1, 2}, {3, 4} and {5 and beyond} for accounting.
     * @param distance the network distance
     * @param newBytes the additional bytes read",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,increaseRemoteReadTime,org.apache.hadoop.fs.FileSystem$Statistics:increaseRemoteReadTime(long),4281,4283,"/**
* Increments remote read time in thread statistics by given duration.
* @param durationMS time to add (in milliseconds)
*/","* Increment the time taken to read bytes from remote in the statistics.
     * @param durationMS time taken in ms to read bytes from remote",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,visitAll,org.apache.hadoop.fs.FileSystem$Statistics:visitAll(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsAggregator),4295,4302,"/**
* Aggregates statistics across the entire dataset using the provided visitor.
* @param visitor StatisticsAggregator instance to perform aggregation
* @return aggregated result of type T or null if aggregation fails
*/","* Apply the given aggregator to all StatisticsData objects associated with
     * this Statistics object.
     *
     * For each StatisticsData object, we will call accept on the visitor.
     * Finally, at the end, we will call aggregate to get the final total.
     *
     * @param         visitor to use.
     * @return        The total.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.FilterFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path),603,606,"/**
* Removes default ACL from the specified file system location.
* @param path file system path to remove default ACL from
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,org.apache.hadoop.fs.ContentSummary$Builder:<init>(),47,48,"/**
* Constructs an empty Builder instance.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeConsumed,org.apache.hadoop.fs.ContentSummary$Builder:typeConsumed(long[]),108,112,"/**
* Consumes specified types and returns this builder instance. 
* @param typeConsumed array of consumed type identifiers
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeQuota,"org.apache.hadoop.fs.ContentSummary$Builder:typeQuota(org.apache.hadoop.fs.StorageType,long)",114,118,"/**
* Sets storage type and quota for builder instance.
* @param type storage type to set
* @param quota maximum allowed storage size in bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeConsumed,"org.apache.hadoop.fs.ContentSummary$Builder:typeConsumed(org.apache.hadoop.fs.StorageType,long)",120,124,"/**
* Consumes storage of specified type and returns Builder instance. 
* @param type Storage type to consume (e.g., disk space)
* @param consumed Amount of storage consumed in bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeQuota,org.apache.hadoop.fs.ContentSummary$Builder:typeQuota(long[]),126,130,"/**
* Sets the type quota and returns this builder instance.
* @param typeQuota array of type quotas
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,build,org.apache.hadoop.fs.QuotaUsage$Builder:build(),93,95,"/**
* Constructs and returns a QuotaUsage object.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,org.apache.hadoop.fs.ContentSummary:<init>(org.apache.hadoop.fs.ContentSummary$Builder),194,204,"/**
* Initializes ContentSummary object from Builder parameters.
* @param builder content summary builder instance
*/","* Constructor for ContentSummary.Builder.
   *
   * @param builder builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,getAlgorithmName,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getAlgorithmName(),60,64,"/**
* Returns the name of the algorithm, combining CRC and MD5 parameters.
* @return Algorithm name as a string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,getChecksumOpt,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getChecksumOpt(),94,97,"/**
* Returns ChecksumOpt object with CRC type and bytes per CRC.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,<init>,org.apache.hadoop.fs.Options$ChecksumOpt:<init>(),255,257,"/**
* Initializes checksum with default type and no specified size.
* @param type Default checksum type to use
* @param size Default size value (always -1 in this constructor) 
*/",* Create a uninitialized one,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,createDisabled,org.apache.hadoop.fs.Options$ChecksumOpt:createDisabled(),287,289,"/**
* Creates a disabled checksum optimizer.
* @return ChecksumOpt instance with NULL type and invalid checksum value
*/","* Create a ChecksumOpts that disables checksum.
     *
     * @return ChecksumOpt.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CompositeCrcFileChecksum.java,getChecksumOpt,org.apache.hadoop.fs.CompositeCrcFileChecksum:getChecksumOpt(),69,72,"/**
* Returns a ChecksumOpt object with CRC settings.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,write,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:write(java.io.DataOutput),106,111,"/**
* Writes object data to output stream.
* @throws IOException if I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,hasPattern,org.apache.hadoop.fs.GlobFilter:hasPattern(),75,77,"/**
* Checks if the pattern contains a wildcard character.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CreateFlag.java,validate,org.apache.hadoop.fs.CreateFlag:validate(java.util.EnumSet),149,162,"/**
* Validates the provided CreateFlag enum set.
* @param flag enum set containing creation flags
*/","* Validate the CreateFlag and throw exception if it is invalid
   * @param flag set of CreateFlag
   * @throws HadoopIllegalArgumentException if the CreateFlag is invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/XAttrSetFlag.java,validate,"org.apache.hadoop.fs.XAttrSetFlag:validate(java.lang.String,boolean,java.util.EnumSet)",53,70,"/**
* Validates XAttribute operation based on existence and flags.
* @param xAttrName attribute name
* @param xAttrExists whether attribute exists
* @param flag one or more XAttrSetFlag enum values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,checkScheme,"org.apache.hadoop.fs.AbstractFileSystem:checkScheme(java.net.URI,java.lang.String)",291,300,"/**
* Validates the URI scheme against a supported scheme.
* @param uri URI to validate
* @param supportedScheme expected scheme (e.g. http, https)
*/","* Check that the Uri's scheme matches.
   *
   * @param uri name URI of the FS.
   * @param supportedScheme supported scheme.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/InvalidPathException.java,<init>,org.apache.hadoop.fs.InvalidPathException:<init>(java.lang.String),38,40,"/**
* Constructs an InvalidPathException with the specified invalid path.
* @param path the invalid file or directory path
*/","* Constructs exception with the specified detail message.
   * 
   * @param path invalid path.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/InvalidPathException.java,<init>,"org.apache.hadoop.fs.InvalidPathException:<init>(java.lang.String,java.lang.String)",48,51,"/**
* Constructs an InvalidPathException with the specified path and optional reason.
* @param path the invalid path
* @param reason optional reason for the invalid path
*/","* Constructs exception with the specified detail message.
   * 
   * @param path invalid path.
   * @param reason Reason <code>path</code> is invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java,findFirstValidInput,org.apache.hadoop.io.erasurecode.coder.util.HHUtil:findFirstValidInput(java.lang.Object[]),210,219,"/**
* Finds the first non-null input in an array of inputs.
* @param inputs array of inputs to search
* @return first valid input or throws exception if none found
*/","* Find the valid input from all the inputs.
   *
   * @param <T> Generics Type T.
   * @param inputs input buffers to look for valid input
   * @return the first valid input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,findFirstValidInput,org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:findFirstValidInput(java.lang.Object[]),163,172,"/**
* Returns the first non-null input from an array or throws exception.
* @param inputs array of possibly null inputs
*/","* Find the valid input from all the inputs.
   * @param inputs input buffers to look for valid input
   * @return the first valid input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayEncodingState.java,checkBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:checkBuffers(byte[][]),91,103,"/**
* Validates the provided byte buffers.
* @param buffers array of byte arrays to be validated
*/","* Check and ensure the buffers are of the desired length.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,checkOutputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:checkOutputBuffers(java.nio.ByteBuffer[]),129,145,"/**
* Validates an array of output buffers against specified criteria.
* @param buffers array of ByteBuffer objects to check
*/","* Check and ensure the buffers are of the desired length and type, direct
   * buffers or not.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,checkOutputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:checkOutputBuffers(byte[][]),121,133,"/**
* Validates the provided output buffers for consistency.
* @param buffers array of byte arrays to be validated
*/","* Check and ensure the buffers are of the desired length.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferEncodingState.java,checkBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:checkBuffers(java.nio.ByteBuffer[]),91,107,"/**
* Validates an array of ByteBuffer objects against specified criteria.
* @param buffers array of ByteBuffer objects to verify
*/","* Check and ensure the buffers are of the desired length and type, direct
   * buffers or not.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,checkPrimitive,org.apache.hadoop.io.ArrayPrimitiveWritable:checkPrimitive(java.lang.Class),71,79,"/**
* Validates the input Class as a primitive type.
*@param componentType Class to be validated
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,checkDeclaredComponentType,org.apache.hadoop.io.ArrayPrimitiveWritable:checkDeclaredComponentType(java.lang.Class),81,88,"/**
* Verifies if the provided component type matches the declared type.
* @param componentType Class of the array element
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,checkArray,org.apache.hadoop.io.ArrayPrimitiveWritable:checkArray(java.lang.Object),90,98,"/**
* Validates that the provided object is a non-null array.
* @param value Object to validate
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseGetLevelArgs,"org.apache.hadoop.log.LogLevel$CLI:parseGetLevelArgs(java.lang.String[],int)",172,188,"/**
* Parses -getlevel command arguments and updates internal state.
* @param args array of string arguments
* @param index current argument index
* @return next expected argument index
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseSetLevelArgs,"org.apache.hadoop.log.LogLevel$CLI:parseSetLevelArgs(java.lang.String[],int)",190,207,"/**
* Parses set level arguments and updates operation, host name, class name, 
* and log level. Returns next argument index.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,stopProxy,org.apache.hadoop.ipc.RPC:stopProxy(java.lang.Object),792,821,"/**
* Closes a proxy object, either by calling its close method or 
* that of its invocation handler. If the proxy is null or cannot be closed,
* throws an exception.
*/","* Stop the proxy. Proxy must either implement {@link Closeable} or must have
   * associated {@link RpcInvocationHandler}.
   * 
   * @param proxy
   *          the RPC proxy object to be stopped
   * @throws HadoopIllegalArgumentException
   *           if the proxy does not implement {@link Closeable} interface or
   *           does not have closeable {@link InvocationHandler}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,<init>,org.apache.hadoop.util.ZKUtil$BadAclFormatException:<init>(java.lang.String),206,208,"/**
 * Constructs a BadAclFormatException with the specified error message.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,<init>,org.apache.hadoop.util.ZKUtil$BadAuthFormatException:<init>(java.lang.String),216,218,"/**
* Constructs a BadAuthFormatException with the specified error message.
* @param message error detail for authentication failure",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,processChecksumOpt,"org.apache.hadoop.fs.Options$ChecksumOpt:processChecksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt,org.apache.hadoop.fs.Options$ChecksumOpt,int)",302,328,"/**
* Merges checksum options from the user and defaults.
* @param defaultOpt default checksum settings
* @param userOpt user-provided checksum settings
* @param userBytesPerChecksum user-specified bytes per checksum value
* @return merged ChecksumOpt object with prioritized values","* A helper method for processing user input and default value to 
     * create a combined checksum option. This is a bit complicated because
     * bytesPerChecksum is kept for backward compatibility.
     *
     * @param defaultOpt Default checksum option
     * @param userOpt User-specified checksum option. Ignored if null.
     * @param userBytesPerChecksum User-specified bytesPerChecksum
     *                Ignored if {@literal <} 0.
     * @return ChecksumOpt.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setPermission,"org.apache.hadoop.fs.DelegateToFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",221,226,"/**
* Sets file system permissions for a given path.
* @param f Path to modify permissions for
* @param permission New file system permissions
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setPermission,"org.apache.hadoop.fs.FilterFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",545,549,"/**
* Sets file system permissions on a given path.
* @param p Path to set permissions for
* @param permission Desired permissions (e.g., read, write, execute)",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",877,882,"/**
* Deletes a user-defined snapshot by its name.
* @param path file system path
* @param snapshotName unique snapshot identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,deleteSnapshot,"org.apache.hadoop.fs.FilterFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",409,413,"/**
* Deletes a snapshot by name from the specified file system location.
* @param path file system path where the snapshot resides
* @param snapshotName unique identifier of the snapshot to delete
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getDefaultPortIfDefined,org.apache.hadoop.fs.DelegateToFileSystem:getDefaultPortIfDefined(org.apache.hadoop.fs.FileSystem),69,72,"/**
* Returns the default port if provided by the file system implementation, 
* otherwise falls back to a predefined constant.","* Returns the default port if the file system defines one.
   * {@link FileSystem#getDefaultPort()} returns 0 to indicate the default port
   * is undefined.  However, the logic that consumes this value expects to
   * receive -1 to indicate the port is undefined, which agrees with the
   * contract of {@link URI#getPort()}.
   *
   * @param theFsImpl file system to check for default port
   * @return default port, or -1 if default port is undefined",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,canonicalizeUri,org.apache.hadoop.fs.FileSystem:canonicalizeUri(java.net.URI),402,417,"/**
* Canonicalizes a URI by setting the default port if not specified.
* @param uri input URI to canonicalize
* @return normalized URI with default port set if necessary
*/","* Canonicalize the given URI.
   *
   * This is implementation-dependent, and may for example consist of
   * canonicalizing the hostname using DNS and adding the default
   * port if not specified.
   *
   * The default implementation simply fills in the default port if
   * not specified and if {@link #getDefaultPort()} returns a
   * default port.
   *
   * @param uri url.
   * @return URI
   * @see NetUtils#getCanonicalUri(URI, int)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.DelegateToFileSystem:getInitialWorkingDirectory(),74,77,"/**
 * Returns the initial working directory based on the file system implementation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.FilterFileSystem:getInitialWorkingDirectory(),324,327,"/**
 * Returns the initial working directory from the file system.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.FilterFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),484,488,"/**
* Retrieves file link status.
* @param f file path
* @return FileStatus object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.LocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),162,165,"/**
* Retrieves file link status from Hadoop Distributed File System (HDFS).
* @param f path to file or directory in HDFS
* @return FileStatus object representing the file's status, or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.DelegateToFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),136,146,"/**
* Retrieves file link status by ID.
* @param f Path to the file
* @return FileStatus object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getLinkTarget,org.apache.hadoop.fs.DelegateToFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),257,260,"/**
* Retrieves link target from file system implementation.
* @param f Path to file or directory
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getLinkTarget,org.apache.hadoop.fs.FilterFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),494,496,"/**
* Retrieves file system link target from given path.
* @param f the input path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,getLinkTarget,org.apache.hadoop.fs.LocalFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),167,170,"/**
 * Returns the target of a symbolic link.
 * @param f file path to resolve
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,truncate,"org.apache.hadoop.fs.DelegateToFileSystem:truncate(org.apache.hadoop.fs.Path,long)",200,204,"/**
* Truncates a file to a specified length.
* @param f file path
* @param newLength desired file size in bytes
* @return true if truncation was successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,truncate,"org.apache.hadoop.fs.FilterFileSystem:truncate(org.apache.hadoop.fs.Path,long)",260,263,"/**
* Truncates a file to a specified length.
* @param f file path to truncate
* @param newLength desired file size in bytes
* @return true if truncation was successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setReplication,"org.apache.hadoop.fs.DelegateToFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",228,233,"/**
* Sets file system replication.
* @param f Path to file
* @param replication Replication factor (short value)
* @return true if successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setReplication,"org.apache.hadoop.fs.FilterFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",240,243,"/**
* Sets file system replication settings.
* @param src file path to modify
* @param replication desired replication factor (short value)
*/","* Set replication for an existing file.
   * 
   * @param src file name
   * @param replication new replication
   * @throws IOException raised on errors performing I/O.
   * @return true if successful;
   *         false if file does not exist or is a directory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setTimes,"org.apache.hadoop.fs.DelegateToFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",235,239,"/**
* Sets file timestamps (modified and access).
* @param f Path to the file
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,updateTime,org.apache.hadoop.fs.shell.TouchCommands$Touch:updateTime(org.apache.hadoop.fs.shell.PathData),177,195,"/**
* Updates the timestamp of a file or directory.
* @param item PathData containing file/directory path and timestamps
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setTimes,"org.apache.hadoop.fs.FilterFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",539,543,"/**
* Sets file times (last modified and last accessed) on a given path.
* @param p the filesystem path to update
* @param mtime the new last modified time in milliseconds
* @param atime the new last accessed time in milliseconds
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setVerifyChecksum,org.apache.hadoop.fs.DelegateToFileSystem:setVerifyChecksum(boolean),241,244,"/**
* Sets whether to verify checksum during file operations.
* @param verifyChecksum true if checksum verification is enabled, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setVerifyChecksum,org.apache.hadoop.fs.FilterFileSystem:setVerifyChecksum(boolean),512,515,"/**
 * Sets whether to verify checksums during file operations.
 * @param verifyChecksum true to enable checksum verification, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,supportsSymlinks,org.apache.hadoop.fs.DelegateToFileSystem:supportsSymlinks(),246,249,"/**
* Checks if the underlying file system implementation supports symbolic links.
* @return true if supported, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,supportsSymlinks,org.apache.hadoop.fs.FilterFileSystem:supportsSymlinks(),490,492,"/**
* Checks if file system supports symbolic links.
* @return true if supported, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,createSymlink,"org.apache.hadoop.fs.DelegateToFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",251,255,"/**
* Creates a symbolic link to the specified target.
* @param target destination path of the link
* @param link   symbolic link path
* @param createParent whether to create parent directories if needed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createSymlink,"org.apache.hadoop.fs.FilterFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",476,482,"/**
* Creates a symbolic link to the specified target path.
* @param target path to be linked
* @param link symbolic link path
* @param createParent whether to create parent directory if needed",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,createSymlink,"org.apache.hadoop.fs.LocalFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",156,160,"/**
* Creates a symbolic link to the specified target.
* @param target path to the target file or directory
* @param link path to the symbolic link
* @param createParent whether to create parent directories if needed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFs:truncate(org.apache.hadoop.fs.Path,long)",559,566,"/**
* Truncates a file to the specified length.
* @param f Path to the file
* @param newLength desired file size in bytes
* @return true if truncation was successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,create,"org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",69,75,"/**
* Creates a file output stream on the specified path with custom permissions and progress tracking.
* @param path file system path
* @param fsPermission file system permissions
* @param b unknown parameter, currently unused
* @param i unknown parameter, currently unused
* @param i1 unknown parameter, currently unsupported (short)
* @param l unknown parameter, currently unused
* @param progressable progress tracking callback
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,create,"org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",69,75,"/**
* Creates a file output stream at the specified path with custom permissions and progress tracking.
* @param path file system path
* @param fsPermission file system permissions
* @param b unknown parameter (not used)
* @param i unknown integer parameter (not used)
* @param i1 short value (purpose unclear)
* @param l long value (purpose unclear)
* @param progressable object for tracking progress (ignored)
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,append,"org.apache.hadoop.fs.http.HttpsFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",77,81,"/**
* Throws an exception when attempting to append data to a file.
* @param path file path
* @param i unused buffer size (always ignored)
* @param progressable unused progress monitor (always ignored)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,append,"org.apache.hadoop.fs.http.HttpFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",77,81,"/**
* Append data to an existing file at specified offset.
* @param path file location
* @param i offset in bytes
* @param progressable callback for progress tracking (optional)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,rename,"org.apache.hadoop.fs.http.HttpsFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",83,86,"/**
* Renames a file/directory.
* @param path original location of item to be renamed
* @param path1 new name of item
* @return true if successful; false otherwise (always false in this implementation)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,rename,"org.apache.hadoop.fs.http.HttpFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",83,86,"/**
* Renames a file to a specified destination.
* @param path source file path
* @param path1 destination file path
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,delete,"org.apache.hadoop.fs.http.HttpsFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",88,91,"/**
* Throws an exception when attempting to delete a file or directory.
* @param path file or directory path to delete
* @param b unused parameter (should be removed)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,delete,"org.apache.hadoop.fs.http.HttpFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",88,91,"/**
 * Throws an exception when attempting to delete a file or directory.
 * @param path file system path to be deleted
 * @param b unused parameter (should not be used)
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,listStatus,org.apache.hadoop.fs.http.HttpsFileSystem:listStatus(org.apache.hadoop.fs.Path),93,96,"/**
 * Returns an array of file statuses for the given path.
 * Throws UnsupportedOperationException as this implementation is not supported. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,listStatus,org.apache.hadoop.fs.http.HttpFileSystem:listStatus(org.apache.hadoop.fs.Path),93,96,"/**
* Returns an array of file statuses for the given path.
* @param path file system path to query
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,mkdirs,"org.apache.hadoop.fs.http.HttpsFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",107,111,"/**
* Creates directory with specified permissions.
* @param path directory to create
* @param fsPermission desired file system permissions
* @return true if created successfully, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,mkdirs,"org.apache.hadoop.fs.http.HttpFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",107,111,"/**
* Creates directory with specified permissions.
* @param path directory to create (must be absolute)
* @param fsPermission file system permissions for the directory
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.http.HttpsFileSystem:getWorkingDirectory(),102,105,"/**
* Returns the working directory path.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.http.HttpFileSystem:getWorkingDirectory(),102,105,"/**
* Returns the working directory path.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.http.HttpsFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),98,100,"/**
* Sets the working directory to the specified file system path.
* @param path new working directory path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.http.HttpFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),98,100,"/**
 * Sets the working directory for file operations.
 * @param path absolute Path to the new working directory
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getUri,org.apache.hadoop.fs.http.HttpsFileSystem:getUri(),56,59,"/**
 * Returns the URI associated with this object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getUri,org.apache.hadoop.fs.http.HttpFileSystem:getUri(),56,59,"/**
* Returns the URI associated with this entity.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,skip,org.apache.hadoop.fs.BufferedFSInputStream:skip(long),70,78,"/**
* Skips ahead in the data stream by a specified number of bytes.
* @param n positive byte count to skip
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,minSeekForVectorReads,org.apache.hadoop.fs.BufferedFSInputStream:minSeekForVectorReads(),169,172,"/**
* Returns minimum seek position required for vector reads.
* @return Minimum seek position as an integer.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,minSeekForVectorReads,org.apache.hadoop.fs.FSDataInputStream:minSeekForVectorReads(),294,297,"/**
* Returns minimum seek position required for vector reads.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,maxReadSizeForVectorReads,org.apache.hadoop.fs.BufferedFSInputStream:maxReadSizeForVectorReads(),174,177,"/**
* Calculates maximum read size for vector reads.
* @return Maximum read size as an integer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,maxReadSizeForVectorReads,org.apache.hadoop.fs.FSDataInputStream:maxReadSizeForVectorReads(),299,302,"/**
* Returns maximum read size for vector reads based on input stream configuration.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBPartHandle.java,from,org.apache.hadoop.fs.BBPartHandle:from(java.nio.ByteBuffer),40,42,"/**
 * Creates a PartHandle instance from the provided ByteBuffer.
 * @param byteBuffer input buffer containing part handle data
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBPartHandle.java,equals,org.apache.hadoop.fs.BBPartHandle:equals(java.lang.Object),54,62,"/**
* Checks if this PartHandle is equal to the given object.
* @param other the object to compare with
* @return true if objects are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",891,897,"/**
* Sets storage policy for a file system subtree.
* @param path file system path
* @param policyName new storage policy name
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setStoragePolicy,"org.apache.hadoop.fs.FilterFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",420,424,"/**
* Sets storage policy for the specified file system location.
* @param path file system path to apply policy to
* @param policyName name of the storage policy to use
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,disconnect,org.apache.hadoop.fs.ftp.FTPFileSystem:disconnect(org.apache.commons.net.ftp.FTPClient),248,260,"/**
* Disconnects an FTP client and logs out.
* @param client FTPClient instance to disconnect
*/","* Logout and disconnect the given FTPClient. *
   * 
   * @param client
   * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,close,org.apache.hadoop.fs.ftp.FTPFileSystem$1:close(),104,107,"/**
* Closes output stream and releases resources.
* @throws IOException if an I/O error occurs during closure
*/",* Close the underlying output stream.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPInputStream.java,close,org.apache.hadoop.fs.ftp.FTPInputStream:close(),103,121,"/**
* Closes the FTP connection and releases resources.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFsAction,"org.apache.hadoop.fs.ftp.FTPFileSystem:getFsAction(int,org.apache.commons.net.ftp.FTPFile)",440,453,"/**
* Determines file system access permissions for a given user group.
* @param accessGroup unique identifier of the user group
* @param ftpFile FTP file object containing permission data
* @return FsAction enum value representing allowed actions (READ, WRITE, EXECUTE) or NONE if none are granted",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,hasNext,org.apache.hadoop.fs.FileSystem$DirListingIterator:hasNext(),2320,2324,"/**
* Checks if there are more entries to process.
* @return true if more entries exist, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,org.apache.hadoop.fs.ContentSummary:<init>(),149,150,"/**
 * No-arg constructor (deprecated). 
 */",Constructor deprecated by ContentSummary.Builder,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,"org.apache.hadoop.fs.ContentSummary:<init>(long,long,long,long,long,long)",177,187,"/**
* Constructs a ContentSummary object with specified metrics.
* @param length total content length
* @param fileCount number of files
* @param directoryCount number of directories
* @param quota allocated storage space
* @param spaceConsumed actual used storage space
* @param spaceQuota maximum allowed storage space
*/","* Constructor, deprecated by ContentSummary.Builder.
   *
   * @param length length.
   * @param fileCount file count.
   * @param directoryCount directory count.
   * @param quota quota.
   * @param spaceConsumed space consumed.
   * @param spaceQuota space quota.
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,equals,org.apache.hadoop.fs.ContentSummary:equals(java.lang.Object),257,275,"/**
* Compares this ContentSummary object with another for equality.
* @param to the object to compare with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,hashCode,org.apache.hadoop.fs.ContentSummary:hashCode(),277,284,"/**
* Calculates the hash code for this object.
* @return unique integer representing this object's state
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",824,829,"/**
* Retrieves extended attribute by name from the file system.
* @param path file system path
* @param name extended attribute name
* @return byte array containing the attribute value or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getXAttr,"org.apache.hadoop.fs.FilterFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",371,374,"/**
* Retrieves extended attribute by name from a file or directory.
* @param path file system path to access
* @param name name of the extended attribute
* @return byte array containing the attribute value or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,getDelay,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:getDelay(java.util.concurrent.TimeUnit),82,86,"/**
* Converts time remaining until next renewal to specified TimeUnit. 
* @return delay in specified unit (e.g., seconds, minutes)",Get the delay until this event should happen.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,updateRenewalTime,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:updateRenewalTime(long),116,118,"/**
 * Updates the renewal time by adding a specified delay and applying a 10% reduction.
 */","* Set a new time for the renewal.
     * It can only be called when the action is not in the queue or any
     * collection because the hashCode may change
     * @param delay the renewal time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,touch,org.apache.hadoop.ipc.Client$Connection:touch(),465,467,"/**
* Updates the last activity timestamp.",Update lastActivity with the current time.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ThreadUtil.java,sleepAtLeastIgnoreInterrupts,org.apache.hadoop.util.ThreadUtil:sleepAtLeastIgnoreInterrupts(long),39,50,"/**
* Sleeps for at least the specified duration, ignoring interrupts.
* @param millis minimum time to sleep in milliseconds
*/","* Cause the current thread to sleep as close as possible to the provided
   * number of milliseconds. This method will log and ignore any
   * {@link InterruptedException} encountered.
   * 
   * @param millis the number of milliseconds for the current thread to sleep",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Timer.java,now,org.apache.hadoop.util.Timer:now(),39,41,"/**
* Retrieves the current time in milliseconds.
* @return The time as a Unix timestamp (milliseconds since epoch) 
*/","* Current system time.  Do not use this to calculate a duration or interval
   * to sleep, because it will be broken by settimeofday.  Instead, use
   * monotonicNow.
   * @return current time in msec.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/AsyncDiskService.java,awaitTermination,org.apache.hadoop.util.AsyncDiskService:awaitTermination(long),131,147,"/**
* Awaits termination of all thread pools for the specified time.
* @param milliseconds maximum wait time in milliseconds
* @return true if all threads are terminated, false otherwise
*/","* Wait for the termination of the thread pools.
   * 
   * @param milliseconds  The number of milliseconds to wait
   * @return   true if all thread pools are terminated without time limit
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,ceiling,"org.apache.hadoop.fs.TrashPolicyDefault$Emptier:ceiling(long,long)",322,324,"/**
* Calculates next occurrence of given interval after specified time.
* @param time target time
* @param interval duration between occurrences
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,readChunk,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readChunk(long,byte[],int,int,byte[])",266,307,"/**
* Reads a chunk of data from the file and optionally a checksum.
* @param pos current file position
* @return number of bytes read or -1 if EOF
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,seekToNewSource,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:seekToNewSource(long),258,264,"/**
* Seeks to a new source position and reports checksum failure if necessary.
* @param targetPos target file position
* @return true if seek was successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,checkBytes,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:checkBytes(java.nio.ByteBuffer,long,java.nio.ByteBuffer,long,int,org.apache.hadoop.fs.Path)",376,426,"/**
* Verifies data integrity by checking byte-level checksums.
* @param sumsBytes ByteBuffer containing checksum values
* @param sumsOffset offset of first checksum in sumsBytes
* @param data ByteBuffer to verify
* @param dataOffset offset of first byte in data
* @param bytesPerSum number of bytes per checksum value
* @param file Path to original file (for error reporting)
*/","* Check the data against the checksums.
     * @param sumsBytes the checksum data
     * @param sumsOffset where from the checksum file this buffer started
     * @param data the file data
     * @param dataOffset where the file data started (must be a multiple of
     *                  bytesPerSum)
     * @param bytesPerSum how many bytes per a checksum
     * @param file the path of the filename
     * @return the data buffer
     * @throws CompletionException if the checksums don't match",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getSumBufferSize,"org.apache.hadoop.fs.ChecksumFs:getSumBufferSize(int,int,org.apache.hadoop.fs.Path)",124,131,"/**
* Calculates optimal sum buffer size based on file size and server defaults.
* @param bytesPerSum block size for sum calculations
* @param bufferSize initial buffer size
* @param file input file path
* @return maximum of calculated sum buffer size or default/file buffer sizes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,open,org.apache.hadoop.fs.AbstractFileSystem:open(org.apache.hadoop.fs.Path),721,724,"/**
* Opens a file stream to the specified path.
* @param f the file path
*/","* The specification of this method matches that of
   * {@link FileContext#open(Path)} except that Path f must be for this
   * file system.
   *
   * @param f the path.
   * @throws AccessControlException access control exception.
   * @throws FileNotFoundException file not found exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.
   * @return input stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getServerDefaults,org.apache.hadoop.fs.FilterFs:getServerDefaults(org.apache.hadoop.fs.Path),163,166,"/**
* Retrieves server defaults from file system using MyFS.
* @param f Path to FS configuration file
* @return FsServerDefaults object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getChecksumFileLength,"org.apache.hadoop.fs.ChecksumFs:getChecksumFileLength(org.apache.hadoop.fs.Path,long)",111,113,"/**
* Calculates checksum file length.
* @param file Path to the file
* @param fileSize total size of the file in bytes
*/","* Return the length of the checksum file given the size of the
   * actual file.
   *
   * @param file the file path.
   * @param fileSize file size.
   * @return check sum file length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,listLocatedStatus,org.apache.hadoop.fs.ChecksumFs:listLocatedStatus(org.apache.hadoop.fs.Path),582,614,"/**
* Filters located file status iterator, skipping checksum files.
*@param f Path to filter
*@return Iterator of filtered LocatedFileStatus objects or empty if none found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,useStatIfAvailable,org.apache.hadoop.fs.RawLocalFileSystem:useStatIfAvailable(),96,99,"/**
* Sets flag to indicate whether to use deprecated file status based on availability of Stat API. 
* @implNote This is a testing hook, intended for internal unit tests only.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createOutputStream,"org.apache.hadoop.fs.RawLocalFileSystem:createOutputStream(org.apache.hadoop.fs.Path,boolean)",570,573,"/**
* Creates an OutputStream for writing to a file.
* @param f Path to the file
* @param append whether to append or overwrite existing content
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatus,org.apache.hadoop.fs.FileSystem:getStatus(org.apache.hadoop.fs.Path),3041,3043,"/**
* Returns a default FsStatus object representing an infinite file system.
*/","* Returns a status object describing the use and capacity of the
   * filesystem. If the filesystem has multiple partitions, the
   * use and capacity of the partition pointed to by the specified
   * path is reflected.
   * @param p Path for which status should be obtained. null means
   * the default partition.
   * @return a FsStatus object
   * @throws IOException
   *           see specific implementation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFsStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFsStatus(),1130,1133,"/**
* Returns the file system status with default values.
* @return FsStatus object with zero values for size and usage.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFsStatus,org.apache.hadoop.fs.viewfs.ViewFs:getFsStatus(),445,449,"/**
* Returns an FsStatus object with default values.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,registerCommands,org.apache.hadoop.fs.FsShellPermissions:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),50,54,"/**
* Registers file system commands with the given CommandFactory.
* @param factory instance of CommandFactory to register classes
*/","* Register the permission related commands with the factory
   * @param factory the command factory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,registerCommands,org.apache.hadoop.fs.shell.Test:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),37,39,"/**
 * Registers test command with the provided CommandFactory.
 * @param factory CommandFactory instance to add command to
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,registerCommands,org.apache.hadoop.fs.shell.SnapshotCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),41,45,"/**
* Registers commands with the provided CommandFactory.
* @param factory instance of CommandFactory to register commands
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,registerCommands,org.apache.hadoop.fs.shell.find.Find:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),49,51,"/**
 * Registers custom commands with the provided CommandFactory.
 * @param factory command factory instance
 */","* Register the names for the count command
   * 
   * @param factory the command factory that will instantiate this class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,registerCommands,org.apache.hadoop.fs.shell.Head:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),40,42,"/**
* Registers command handlers with the provided CommandFactory.
* @param factory instance of CommandFactory to add commands to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,registerCommands,org.apache.hadoop.fs.shell.Ls:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),45,48,"/**
* Registers custom commands with the CommandFactory.
* @param factory instance of CommandFactory to add classes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,registerCommands,org.apache.hadoop.fs.shell.Tail:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),42,44,"/**
* Registers custom command classes with the provided CommandFactory.
* @param factory CommandFactory instance to register commands with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,registerCommands,org.apache.hadoop.fs.shell.FsUsage:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),46,50,"/**
* Registers custom commands with the specified CommandFactory.
* @param factory CommandFactory instance to register classes with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,registerCommands,org.apache.hadoop.fs.shell.XAttrCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),42,45,"/**
* Registers custom commands with the CommandFactory.
* @param factory Factory instance to add classes to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,registerCommands,org.apache.hadoop.fs.shell.Delete:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),50,55,"/**
* Registers custom commands with the CommandFactory.
* @param factory factory instance to register classes with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,registerCommands,org.apache.hadoop.fs.shell.Count:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),46,48,"/**
* Registers command classes with the given CommandFactory.
* @param factory instance of CommandFactory to register with
*/","* Register the names for the count command
   * @param factory the command factory that will instantiate this class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,registerCommands,org.apache.hadoop.fs.shell.TouchCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),43,46,"/**
* Registers custom commands with the CommandFactory.
* @param factory CommandFactory instance to add classes to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,registerCommands,org.apache.hadoop.fs.shell.Mkdir:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),39,41,"/**
* Registers built-in commands with the CommandFactory.
* @param factory CommandFactory instance to add commands to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Concat.java,registerCommands,org.apache.hadoop.fs.shell.Concat:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),38,40,"/**
* Registers commands with the CommandFactory.
* @param factory instance of CommandFactory to register with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,registerCommands,org.apache.hadoop.fs.shell.CopyCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),44,52,"/**
* Registers custom commands with the given CommandFactory.
* @param factory CommandFactory instance to register classes with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,registerCommands,org.apache.hadoop.fs.shell.MoveCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),35,39,"/**
* Registers custom commands with the CommandFactory.
* @param factory CommandFactory instance to register classes with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Stat.java,registerCommands,org.apache.hadoop.fs.shell.Stat:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),53,55,"/**
* Registers commands with the CommandFactory.
* @param factory CommandFactory instance to register with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,registerCommands,org.apache.hadoop.fs.shell.Display:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),62,66,"/**
* Registers various commands with the provided CommandFactory.
* @param factory CommandFactory instance for registration
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,registerCommands,org.apache.hadoop.fs.shell.AclCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),47,50,"/**
* Registers custom commands with the CommandFactory.
* @param factory instance of CommandFactory to register classes with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,registerCommands,org.apache.hadoop.fs.shell.Truncate:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),35,37,"/**
 * Registers command classes with the provided CommandFactory.
 * @param factory The CommandFactory instance to register commands with
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,registerCommands,org.apache.hadoop.fs.shell.SetReplication:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),37,39,"/**
* Registers custom commands with the CommandFactory.
* @param factory instance of CommandFactory to register with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,<init>,"org.apache.hadoop.fs.shell.CommandFormat:<init>(java.lang.String,int,int,java.lang.String[])",45,48,"/**
* Creates a CommandFormat with specified minimum and maximum values.
* @param min minimum value
* @param max maximum value
*/","* @deprecated use replacement since name is an unused parameter
   * @param name of command, but never used
   * @param min see replacement
   * @param max see replacement
   * @param possibleOpt see replacement
   * @see #CommandFormat(int, int, String...)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpoint,"org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(org.apache.hadoop.fs.Path,boolean)",361,403,"/**
* Deletes checkpoints in the specified trash root directory.
* @param trashRoot path to trash root directory
* @param deleteImmediately whether to delete immediately or check deletion interval
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystemPathHandle.java,verify,org.apache.hadoop.fs.LocalFileSystemPathHandle:verify(org.apache.hadoop.fs.FileStatus),54,61,"/**
* Verifies file status, throwing an exception on invalid or changed content.
* @param stat FileStatus object to verify
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",802,807,"/**
* Sets access control list (ACL) for a file or directory.
* @param path Path to set ACL for
* @param aclSpec List of ACL entries
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setAcl,"org.apache.hadoop.fs.FilterFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",349,352,"/**
* Sets ACL on a file system object at the specified path.
* @param path file system object to set ACL for
* @param aclSpec list of access control entries to apply
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,startUpload,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:startUpload(org.apache.hadoop.fs.Path),98,110,"/**
* Starts an upload process for the given file path.
* @param filePath unique file path
* @return UploadHandle object representing the ongoing upload operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,putPart,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:putPart(org.apache.hadoop.fs.UploadHandle,int,org.apache.hadoop.fs.Path,java.io.InputStream,long)",112,122,"/**
* Uploads a file part to an ongoing upload. 
* @param uploadId unique upload identifier
* @param partNumber part sequence number
* @return CompletableFuture containing PartHandle object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,complete,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:complete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)",176,185,"/**
* Completes the specified upload and returns its associated path handle.
* @param uploadId unique identifier for the upload
* @param filePath file path to complete
* @return PathHandle object or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,eval,org.apache.hadoop.fs.impl.FutureIOSupport:eval(org.apache.hadoop.util.functional.CallableRaisingIOE),179,182,"/**
* Evaluates a callable with potential IO exceptions and returns its result as a CompletableFuture.
* @param callable function to evaluate, raising an IOException if necessary
*/","* Evaluate a CallableRaisingIOE in the current thread,
   * converting IOEs to RTEs and propagating.
   * See {@link FutureIO#eval(CallableRaisingIOE)}.
   *
   * @param callable callable to invoke
   * @param <T> Return type.
   * @return the evaluated result.
   * @throws UnsupportedOperationException fail fast if unsupported
   * @throws IllegalArgumentException invalid argument",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,concat,"org.apache.hadoop.fs.FilterFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])",188,191,"/**
* Concatenates multiple paths into one.
* @param f target path
* @param psrcs array of source paths to concatenate
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,rejectUnknownMandatoryKeys,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:rejectUnknownMandatoryKeys(java.util.Collection,java.lang.String)",358,362,"/**
* Rejects unknown mandatory keys from a collection.
* @param knownKeys collection of expected key values
* @param extraErrorText additional error message to append
*/","* Reject a configuration if one or more mandatory keys are
   * not in the set of mandatory keys.
   * The first invalid key raises the exception; the order of the
   * scan and hence the specific key raising the exception is undefined.
   * @param knownKeys a possibly empty collection of known keys
   * @param extraErrorText extra error text to include.
   * @throws IllegalArgumentException if any key is unknown.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileRangeImpl.java,toString,org.apache.hadoop.fs.impl.FileRangeImpl:toString(),54,58,"/**
* Returns a string representation of the range with offset, length, and reference.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,pathCapabilities,org.apache.hadoop.fs.impl.FlagSet:pathCapabilities(),209,213,"/**
* Retrieves capabilities by filtering keys with existing capability values.
*/","* Generate the list of capabilities.
   * @return a possibly empty list.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,buildHttpReferrer,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:buildHttpReferrer(),190,223,"/**
* Builds HTTP referrer header from attributes and evaluated values.
* @return formatted referrer header string
*/","* Build the referrer string.
   * This includes dynamically evaluating all of the evaluated
   * attributes.
   * If there is an error creating the string it will be logged once
   * per entry, and """" returned.
   * @return a referrer string or """"",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,<init>,"org.apache.hadoop.util.WeakReferenceMap:<init>(java.util.function.Function,java.util.function.Consumer)",100,106,"/**
* Creates a WeakReferenceMap instance with a specified factory and listener for lost references.
* @param factory function to create new map entries
* @param referenceLost callback when a weak reference is cleared (optional)","* instantiate.
   * @param factory supplier of new instances
   * @param referenceLost optional callback on lost references.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/StoreImplementationUtils.java,hasCapability,"org.apache.hadoop.fs.impl.StoreImplementationUtils:hasCapability(java.io.OutputStream,java.lang.String)",80,82,"/**
* Checks whether the given OutputStream has a specified capability.
* @param out OutputStream to check
* @param capability capability name to verify
*/","* Probe for an output stream having a capability; returns true
   * if the stream implements {@link StreamCapabilities} and its
   * {@code hasCapabilities()} method returns true for the capability.
   * @param out output stream
   * @param capability capability to probe for
   * @return true if the stream declares that it supports the capability.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/StoreImplementationUtils.java,hasCapability,"org.apache.hadoop.fs.impl.StoreImplementationUtils:hasCapability(java.io.InputStream,java.lang.String)",92,94,"/**
* Checks if an InputStream has a specific capability.
* @param in InputStream to check
* @param capability name of capability to verify
*/","* Probe for an input stream having a capability; returns true
   * if the stream implements {@link StreamCapabilities} and its
   * {@code hasCapabilities()} method returns true for the capability.
   * @param in input stream
   * @param capability capability to probe for
   * @return true if the stream declares that it supports the capability.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/ExecutorServiceFuturePool.java,shutdown,"org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:shutdown(org.slf4j.Logger,long,java.util.concurrent.TimeUnit)",81,83,"/**
* Shuts down executor with optional timeout.
* @param logger logging instance
* @param timeout shutdown duration in specified time unit
* @param unit time unit for timeout (e.g. seconds, milliseconds)
*/","* Utility to shutdown the {@link ExecutorService} used by this class. Will wait up to a
   * certain timeout for the ExecutorService to gracefully shutdown.
   *
   * @param logger Logger
   * @param timeout the maximum time to wait
   * @param unit the time unit of the timeout argument",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,<init>,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:<init>(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation),128,131,"/**
* Constructs an instance of End from Operation.
* @param op operation containing end event details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getSummary,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:getSummary(java.lang.StringBuilder),133,137,"/**
* Appends character 'E' and then calls superclass summary generator.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getDebugInfo,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:getDebugInfo(),139,142,"/**
* Returns condensed debug information by truncating prefix and returning remaining part.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,add,org.apache.hadoop.fs.impl.prefetch.BlockOperations:add(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation),160,166,"/**
* Adds an operation to the collection and logs debug info if in debug mode.
* @param op Operation object to be added
* @return The added Operation object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,canRelease,org.apache.hadoop.fs.impl.prefetch.BufferPool:canRelease(org.apache.hadoop.fs.impl.prefetch.BufferData),318,322,"/**
* Checks if buffer data is in DONE or READY state.
* @param data BufferData object to check
* @return true if buffer can be released, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,get,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$PrefetchTask:get(),411,420,"/**
* Prefetches a block from the block manager.
* @throws Exception if an error occurs during prefetching
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,distance,"org.apache.hadoop.fs.impl.prefetch.BufferPool:distance(org.apache.hadoop.fs.impl.prefetch.BufferData,int)",226,228,"/**
 * Calculates absolute difference between block number and stored block number.
 * @param data BufferData object containing stored block information
 * @param blockNumber target block number
 * @return absolute distance between the two numbers
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,find,org.apache.hadoop.fs.impl.prefetch.BufferPool:find(int),305,316,"/**
* Finds BufferData by block number and returns it if not completed.
* @param blockNumber unique block identifier
* @return BufferData object or null if completed or not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,close,org.apache.hadoop.fs.impl.prefetch.BufferPool:close(),257,275,"/**
* Closes the data buffer pool and cancels pending actions.
* @see #pool
* @see #allocated
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,acquire,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:acquire(),70,73,"/**
 * Acquires an instance of type T from the underlying data source.
 * @return An instance of type T or null if not available
 */",* Acquires a resource blocking if necessary until one becomes available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,tryAcquire,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:tryAcquire(),78,81,"/**
* Attempts to acquire an instance of type T.
* @return The acquired object or null if failed. 
*/",* Acquires a resource blocking if one is immediately available. Otherwise returns null.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,close,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:close(),113,124,"/**
* Releases resources by closing all created items and clearing internal lists.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,numAvailable,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:numAvailable(),148,150,"/**
* Calculates the number of available items.
* @return total count of unallocated items
*/","* Number of items available to be acquired. Mostly for testing purposes.
   * @return the number available.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,duration,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:duration(),144,146,"/**
* Calculates execution duration in seconds.
* @return time difference between current and operation timestamps
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,analyze,org.apache.hadoop.fs.impl.prefetch.BlockOperations:analyze(java.lang.StringBuilder),297,367,"/**
* Analyzes operations by block number and appends findings to the StringBuilder.
* @param sb StringBuilder to append results to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,<init>,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:<init>(org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics,int,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",223,234,"/**
* Initializes SingleFilePerBlockCache with prefetching statistics, max block count,
* and duration tracker factory.
* @param prefetchingStatistics statistics for prefetching
* @param maxBlocksCount maximum number of blocks
* @param trackerFactory factory to create duration trackers
*/","* Constructs an instance of a {@code SingleFilePerBlockCache}.
   *
   * @param prefetchingStatistics statistics for this stream.
   * @param maxBlocksCount max blocks count to be kept in cache at any time.
   * @param trackerFactory tracker with statistics to update",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,<init>,"org.apache.hadoop.util.SemaphoredDelegatingExecutor:<init>(java.util.concurrent.ExecutorService,int,boolean,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",71,82,"/**
* Initializes a SemaphoredDelegatingExecutor with the given executor service,
* permit count, fairness policy, and duration tracking factory.
* @param executorDelegatee delegatee ExecutorService instance
* @param permitCount maximum number of concurrent tasks
* @param fair whether to use fair or unfair semaphore
* @param trackerFactory DurationTrackerFactory instance (optional)
*/","* Instantiate.
   * @param executorDelegatee Executor to delegate to
   * @param permitCount number of permits into the queue permitted
   * @param fair should the semaphore be ""fair""
   * @param trackerFactory duration tracker factory.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,addToLinkedListHead,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToLinkedListHead(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry),314,321,"/**
* Adds an Entry to the head of the linked list, synchronizing access.
* @param entry new Entry object
*/","* Helper method to add the given entry to the head of the linked list.
   *
   * @param entry Block entry to add.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,validateEntry,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:validateEntry(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry,java.nio.ByteBuffer)",551,566,"/**
* Validates the consistency of an Entry object with its corresponding ByteBuffer data.
* @param entry the Entry object to validate
* @param buffer the ByteBuffer containing the data to verify against
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setDone,org.apache.hadoop.fs.impl.prefetch.BufferData:setDone(),223,231,"/**
* Sets the state to DONE, ensuring checksum integrity.
* @throws IllegalStateException if checksum changes unexpectedly
*/",* Indicates that this block is no longer of use and can be reclaimed.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,close,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:close(),502,508,"/**
* Closes this instance and deletes cache files.
* If successful, logs statistics before closing. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,toString,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:toString(),540,549,"/**
* Returns a human-readable string representation of this object.
* @return A formatted string containing stats and block IDs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,toString,org.apache.hadoop.fs.impl.prefetch.BufferData:toString(),289,299,"/**
* Formats block info into a human-readable string.
* @return String representation of the block, including ID, state, buffer, checksum, and future action
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,throwIfInvalidBuffer,org.apache.hadoop.fs.impl.prefetch.FilePosition:throwIfInvalidBuffer(),298,300,"/**
 * Validates that the provided buffer is not null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,bufferSize,org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:bufferSize(int),133,136,"/**
* Sets buffer size and returns this builder instance.
* @param bufSize new buffer size value
*/","* Set the size of the buffer to be used.
   *
   * @param bufSize buffer size.
   * @return FutureDataInputStreamBuilder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,builder,org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:builder(),146,148,"/**
* Returns a new instance of DataInputStreamBuilder.","* Get the builder.
   * This must be used after the constructor has been invoked to create
   * the actual builder: it allows for subclasses to do things after
   * construction.
   *
   * @return FutureDataInputStreamBuilder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,getForCurrentThread,org.apache.hadoop.fs.impl.WeakReferenceThreadMap:getForCurrentThread(),45,47,"/**
 * Retrieves an instance of type V associated with the current thread.
 * @return An instance of type V or null if not found
 */","* Get the value for the current thread, creating if needed.
   * @return an instance.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,removeForCurrentThread,org.apache.hadoop.fs.impl.WeakReferenceThreadMap:removeForCurrentThread(),53,55,"/**
* Removes value associated with current thread from cache.
* @return cached value or null if not present
*/","* Remove the reference for the current thread.
   * @return any reference value which existed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,setForCurrentThread,org.apache.hadoop.fs.impl.WeakReferenceThreadMap:setForCurrentThread(java.lang.Object),70,90,"/**
* Sets a value for the current thread's scope.
* @param newVal new value to set
* @return previous value or null if not previously set
*/","* Set the new value for the current thread.
   * @param newVal new reference to set for the active thread.
   * @return the previously set value, possibly null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/CombinedFileRange.java,<init>,"org.apache.hadoop.fs.impl.CombinedFileRange:<init>(long,long,org.apache.hadoop.fs.FileRange)",44,47,"/**
* Combines two file ranges into one.
* @param offset starting position of combined range
* @param end ending position of combined range
* @param original existing file range to append
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/CombinedFileRange.java,merge,"org.apache.hadoop.fs.impl.CombinedFileRange:merge(long,long,org.apache.hadoop.fs.FileRange,int,int)",79,89,"/**
* Merges a file range into the current one if they overlap or are adjacent.
* @param otherOffset offset of the other file range
* @param otherEnd end offset of the other file range
* @param other the other file range to merge
* @param minSeek minimum seek distance between ranges
* @param maxSize maximum size for merged result
* @return true if merge was successful, false otherwise
*/","* Merge this input range into the current one, if it is compatible.
   * It is assumed that otherOffset is greater or equal the current offset,
   * which typically happens by sorting the input ranges on offset.
   * @param otherOffset the offset to consider merging
   * @param otherEnd the end to consider merging
   * @param other the underlying FileRange to add if we merge
   * @param minSeek the minimum distance that we'll seek without merging the
   *                ranges together
   * @param maxSize the maximum size that we'll merge into a single range
   * @return true if we have merged the range into this one",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createBulkDelete,org.apache.hadoop.fs.FileSystem:createBulkDelete(org.apache.hadoop.fs.Path),5002,5006,"/**
* Creates a bulk delete operation on the specified path.
* @param path directory path for bulk delete operation
*/","* Create a bulk delete operation.
   * The default implementation returns an instance of {@link DefaultBulkDeleteOperation}.
   * @param path base path for the operation.
   * @return an instance of the bulk delete.
   * @throws IllegalArgumentException any argument is invalid.
   * @throws IOException if there is an IO problem.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getBufferSize,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getBufferSize(),64,67,"/**
* Returns the buffer size from the superclass.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getReplication,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getReplication(),69,72,"/**
 * Delegates replication level retrieval to superclass. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getFlags,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getFlags(),74,77,"/**
* Retrieves the set of flags associated with this object.
* @return EnumSet of CreateFlag values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getChecksumOpt,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getChecksumOpt(),79,82,"/**
 * Returns checksum option from parent class.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getBlockSize,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getBlockSize(),84,87,"/**
* Returns block size inherited from superclass.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processOptions,org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processOptions(java.util.LinkedList),153,177,"/**
* Validates and processes command-line options.
* @param args list of command-line arguments
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeXAttr,"org.apache.hadoop.fs.FilterFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",656,659,"/**
* Removes extended attribute by name from given file or directory.
* @param path file system path to modify
* @param name name of the extended attribute to remove
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processOptions,org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processOptions(java.util.LinkedList),53,59,"/**
* Validates and processes command-line options.
* @param args list of command-line arguments
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/And.java,registerExpression,org.apache.hadoop.fs.shell.find.And:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory),31,35,"/**
* Registers expression classes with the given factory.
* @param factory ExpressionFactory instance
*/",Registers this expression with the specified factory.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,registerExpression,org.apache.hadoop.fs.shell.find.Print:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory),30,34,"/**
* Registers custom print expression classes with the ExpressionFactory.
* @param factory Factory instance to register expressions
*/",Registers this expression with the specified factory.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,registerExpression,org.apache.hadoop.fs.shell.find.Name:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory),33,37,"/**
* Registers expression classes with the given ExpressionFactory.
* @param factory ExpressionFactory instance to register classes with
*/",Registers this expression with the specified factory.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,<init>,org.apache.hadoop.fs.shell.find.Print:<init>(),45,47,"/**
 * Initializes printer with newline character.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,<init>,org.apache.hadoop.fs.shell.find.Name:<init>(boolean),57,62,"/**
* Initializes a new Name command with specified sensitivity to case. 
* @param caseSensitive true if case matters, false otherwise
*/","* Construct a Name {@link Expression} with a specified case sensitivity.
   *
   * @param caseSensitive if true the comparisons are case sensitive.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,apply,"org.apache.hadoop.fs.shell.find.Name:apply(org.apache.hadoop.fs.shell.PathData,int)",82,93,"/**
* Applies glob pattern filtering to PathData item.
* @param item PathData object to filter
* @param depth unused parameter, should be removed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,addCodec,org.apache.hadoop.io.compress.CompressionCodecFactory:addCodec(org.apache.hadoop.io.compress.CompressionCodec),64,75,"/**
* Registers a compression codec with the given name and extension.
* @param codec CompressionCodec instance to register
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodec,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodec(org.apache.hadoop.fs.Path),199,217,"/**
* Retrieves the compression codec for a given file, 
* searching in reverse lower-case filename.
*@param file Path to the file
*/","* Find the relevant compression codec for the given file based on its
   * filename suffix.
   * @param file the filename to check
   * @return the codec object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsConfig:<init>(org.apache.commons.configuration2.Configuration,java.lang.String)",94,96,"/**
* Initializes metrics configuration with given prefix.
* @param c Configuration object
* @param prefix unique metric prefix
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,isLocalhost,org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:isLocalhost(java.lang.String),491,501,"/**
* Checks if the given hostname matches localhost.
* @param host hostname to check (null returns false)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,resolvePropertyName,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:resolvePropertyName(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String)",216,221,"/**
* Resolves property name from template by formatting with SSL factory mode.
* @param mode SSLFactory.Mode to be formatted (e.g. SSL_FACTORY_MODE)
* @param template message format string
* @return resolved property name or null if template is invalid
*/","* Resolves a property name to its client/server version if applicable.
   * <p>
   * NOTE: This method is public for testing purposes.
   *
   * @param mode client/server mode.
   * @param template property name template.
   * @return the resolved property name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,hasCapability,org.apache.hadoop.crypto.CryptoInputStream:hasCapability(java.lang.String),860,880,"/**
* Checks if the stream has a specific capability.
* @param capability capability to check for
* @return true if capability is supported, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CipherSuite.java,getConfigSuffix,org.apache.hadoop.crypto.CipherSuite:getConfigSuffix(),98,106,"/**
* Constructs a configuration suffix from the given name.
*/","* Returns suffix of cipher suite configuration.
   * @return String configuration suffix",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Result.java,combine,org.apache.hadoop.fs.shell.find.Result:combine(org.apache.hadoop.fs.shell.find.Result),59,62,"/**
* Combines two results by performing logical AND on pass and descend flags.
* @param other the second result to combine with
* @return a new combined Result object
*/","* Returns the combination of this and another result.
   * @param other other.
   * @return result.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Result.java,negate,org.apache.hadoop.fs.shell.find.Result:negate(),68,70,"/**
* Inverts result pass status and returns updated Result object.","* Negate this result.
   * @return Result.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Result.java,toString,org.apache.hadoop.fs.shell.find.Result:toString(),72,75,"/**
* Returns a string representation of the object's state.
* Contains success and recursion flags.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,<init>,org.apache.hadoop.fs.shell.find.Name$Iname:<init>(),97,99,"/**
* Initializes an instance with a default name.
* @param none 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,<init>,org.apache.hadoop.fs.shell.find.Print$Print0:<init>(),72,74,"/**
* Initializes a new Print0 instance with a newline character. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,createOptions,org.apache.hadoop.fs.shell.find.Find:createOptions(),244,252,"/**
* Creates FindOptions object with input/output settings and command factory.
*/",Create a new set of find options.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,isExpression,org.apache.hadoop.fs.shell.find.Find:isExpression(java.lang.String),445,448,"/**
* Checks whether an expression exists by name.
* @param expressionName unique identifier of the expression
*/",Asks the factory whether an expression is recognized.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,setOptions,org.apache.hadoop.fs.shell.find.BaseExpression:setOptions(org.apache.hadoop.fs.shell.find.FindOptions),67,73,"/**
* Sets find options and applies them to all child expressions.
* @param options FindOptions object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,prepare,org.apache.hadoop.fs.shell.find.BaseExpression:prepare(),75,80,"/**
* Recursively prepares child expressions.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,finish,org.apache.hadoop.fs.shell.find.BaseExpression:finish(),82,87,"/**
* Recursively finishes all child expressions.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,isAction,org.apache.hadoop.fs.shell.find.BaseExpression:isAction(),147,155,"/**
* Recursively checks if any child expression is an action.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,toString,org.apache.hadoop.fs.shell.find.BaseExpression:toString(),119,145,"/**
* Returns a string representation of the object in the format: <classSimpleName>(arguments;children).
* @return formatted string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,addChildren,"org.apache.hadoop.fs.shell.find.BaseExpression:addChildren(java.util.Deque,int)",219,223,"/**
* Adds 'count' child expressions to the current expression.
* @param exprs deque of Expression objects
* @param count number of children to add
*/","* Add a specific number of children to this expression. The children are
   * popped off the head of the expressions.
   *
   * @param exprs
   *          deque of expressions from which to take the children
   * @param count
   *          number of children to be added",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,addArguments,"org.apache.hadoop.fs.shell.find.BaseExpression:addArguments(java.util.Deque,int)",250,254,"/**
* Adds multiple arguments to the deque from top to bottom.
* @param args deque of strings containing arguments
* @param count number of arguments to add
*/","* Add a specific number of arguments to this expression. The children are
   * popped off the head of the expressions.
   *
   * @param args
   *          deque of arguments from which to take the argument
   * @param count
   *          number of children to be added",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,validate,org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:validate(),244,274,"/**
* Validates the credential by checking alias and interacting with user for deletion.
* @return true if valid, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,validate,org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:validate(),354,381,"/**
* Validates the deletion of a key from KeyProvider.
* @return true if deletion is valid, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,confirmForceManual,org.apache.hadoop.ha.HAAdmin:confirmForceManual(),466,479,"/**
* Confirms user intention to force manual operation, displaying warning about potential data corruption.
* @return true if confirmed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,relativize,"org.apache.hadoop.fs.shell.PathData:relativize(java.net.URI,java.net.URI,boolean)",410,435,"/**
* Computes the relative URI from a source URI to a current working directory URI.
* @param cwdUri current working directory URI
* @param srcUri source URI
* @return relative URI string or Path.CUR_DIR if both URIs are equal
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,stringToUri,org.apache.hadoop.fs.shell.PathData:stringToUri(java.lang.String),550,591,"/**
* Converts a string to a URI.
* @param pathString input string to parse into URI components
* @return URI object or throws exception if parsing fails
*/","Construct a URI from a String with unescaped special characters
   *  that have non-standard semantics. e.g. /, ?, #. A custom parsing
   *  is needed to prevent misbehavior.
   *  @param pathString The input path in string form
   *  @return URI",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setHumanReadable,org.apache.hadoop.fs.shell.FsUsage$Df:setHumanReadable(boolean),69,71,"/**
* Sets whether date values should be displayed in human-readable format.
* @param humanReadable true to display dates as strings (e.g., ""2022-01-01""), false for raw date values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setHumanReadable,org.apache.hadoop.fs.shell.FsUsage$Du:setHumanReadable(boolean),69,71,"/**
 * Sets whether to display dates in human-readable format.
 * @param humanReadable true to display dates in human-readable format, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Df:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder),65,67,"/**
* Sets the table builder for usage data.
* @param usagesTable TableBuilder instance to configure usage table
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Du:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder),65,67,"/**
 * Sets the table builder for usage statistics.
 * @param usagesTable TableBuilder instance to configure usage stats
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,getUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Df:getUsagesTable(),61,63,"/**
* Returns the table builder for usage statistics.
* @return TableBuilder instance for rendering usage data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,getUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Du:getUsagesTable(),61,63,"/**
* Returns the table builder for usage statistics.
* @return TableBuilder instance representing the usage statistics table
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,isSorted,org.apache.hadoop.fs.shell.Ls:isSorted(),248,254,"/**
* Determines if the list is sorted in a specific order (time, size, reverse).
* @return true if sorted, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,initialiseOrderComparator,org.apache.hadoop.fs.shell.Ls:initialiseOrderComparator(),374,402,"/**
* Initializes order comparator based on time or size ordering preference.
* @param isOrderTime whether to order by time
* @param isOrderSize whether to order by size
* @param isOrderReverse whether to reverse the order
*/","* Initialise the comparator to be used for sorting files. If multiple options
   * are selected then the order is chosen in the following precedence: -
   * Modification time (or access time if requested) - File size - File name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getAdditionalTokenIssuers,org.apache.hadoop.fs.FileSystem:getAdditionalTokenIssuers(),718,723,"/**
* Returns an array of additional token issuers.
* @return Array of DelegationTokenIssuer objects or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,<init>,"org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException:<init>(int,int)",218,220,"/**
* Constructs a NotEnoughArgumentsException with specified expected and actual argument counts.
* @param expected number of arguments expected
* @param actual number of arguments actually provided
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,<init>,"org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException:<init>(int,int)",202,204,"/**
* Constructs an exception with the specified expected and actual argument counts.
* @param expected expected number of arguments
* @param actual actual number of arguments passed to a method
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,getMessage,org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException:getMessage(),222,225,"/**
* Returns error message when insufficient arguments provided.
* @return error message as string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,getMessage,org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException:getMessage(),206,209,"/**
* Returns error message when too many arguments are provided.
* @return custom error message as a string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processOptions,org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processOptions(java.util.LinkedList),70,100,"/**
* Processes command-line options and initializes object state.
* @param args list of option values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getXAttrs,org.apache.hadoop.fs.FilterFileSystem:getXAttrs(org.apache.hadoop.fs.Path),640,643,"/**
* Retrieves extended attributes from the file system.
* @param path file system path to fetch attributes from
* @return map of attribute names to their corresponding values or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getXAttr,"org.apache.hadoop.fs.FilterFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",635,638,"/**
* Returns extended file attribute by name.
* @param path file system path
* @param name name of xattr to fetch
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,popPreserveOption,org.apache.hadoop.fs.shell.CopyCommands$Cp:popPreserveOption(java.util.List),191,210,"/**
* Parses command-line options and sets preserve mode.
* @param args list of arguments to parse
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processArguments,org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processArguments(java.util.LinkedList),159,171,"/**
* Renames a single snapshot under the specified path.
* @param items list of PathData objects containing a single item
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.FilterFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",579,583,"/**
* Renames a snapshot in the file system.
* @param path File path to rename
* @param snapshotOldName Old name of the snapshot
* @param snapshotNewName New name for the snapshot
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,isDeprecated,org.apache.hadoop.fs.shell.Command:isDeprecated(),557,559,"/**
* Checks if command is deprecated.
* @return true if replacement command exists, false otherwise
*/","* Is the command deprecated?
   * @return boolean",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,getName,org.apache.hadoop.fs.shell.Command:getName(),519,523,"/**
* Returns the command name, either from the local 'name' field or a command field.
*/","* The name of the command.  Will first try to use the assigned name
   * else fallback to the command's preferred name
   * @return name of the command",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getAclStatus,org.apache.hadoop.fs.FilterFileSystem:getAclStatus(org.apache.hadoop.fs.Path),618,621,"/**
* Retrieves ACL status for a given file path.
* @param path target file path
* @return AclStatus object representing the ACL status or null on error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,setPreserve,org.apache.hadoop.fs.shell.CommandWithDestination:setPreserve(boolean),125,133,"/**
* Sets preserve status for file attributes.
* @param preserve true to enable preservation, false to clear
*/","* If true, the last modified time, last access time,
   * owner, group and permission information of the source
   * file will be preserved as far as target {@link FileSystem}
   * implementation allows.
   *
   * @param preserve preserve.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setAcl,"org.apache.hadoop.fs.FilterFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",613,616,"/**
* Sets ACL (Access Control List) for a file or directory.
* @param path the file or directory to modify
* @param aclSpec list of ACL entries to apply
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,<init>,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:<init>(java.lang.String,java.lang.String)",41,45,"/**
* Initializes MBeanInfoBuilder with given name and description.
* @param name unique identifier of the MBean
* @param desc descriptive text for the MBean
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:<init>(org.apache.hadoop.metrics2.MetricsCollector,org.apache.hadoop.metrics2.MetricsInfo,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,boolean)",58,68,"/**
* Initializes MetricsRecordBuilder with given parameters.
* @param parent the parent MetricsCollector
* @param info the MetricsInfo object
* @param rf the Record Filter
* @param mf the Metric Filter
* @param acceptable whether the record is acceptable
*/","* @param parent {@link MetricsCollector} using this record builder
   * @param info metrics information
   * @param rf
   * @param mf
   * @param acceptable",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,<init>,"org.apache.hadoop.util.ChunkedArrayList:<init>(int,int)",103,107,"/**
* Initializes a ChunkedArrayList with specified chunk capacity and size constraints.
* @param initialChunkCapacity minimum number of elements per chunk
* @param maxChunkSize maximum allowed chunk size
*/","* @param initialChunkCapacity the capacity of the first chunk to be
   * allocated
   * @param maxChunkSize the maximum size of any chunk allocated",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ScopedAclEntries.java,calculatePivotOnDefaultEntries,org.apache.hadoop.fs.permission.ScopedAclEntries:calculatePivotOnDefaultEntries(java.util.List),87,94,"/**
* Finds the index of the first default ACL entry in the list.
* @return index or PIVOT_NOT_FOUND if none found
*/","* Returns the pivot point in the list between the access entries and the
   * default entries.  This is the index of the first element in the list that
   * is a default entry.
   *
   * @param aclBuilder ArrayList<AclEntry> containing entries to build
   * @return int pivot point, or -1 if list contains no default entries",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeAcl,org.apache.hadoop.fs.FilterFileSystem:removeAcl(org.apache.hadoop.fs.Path),608,611,"/**
 * Removes ACL from the specified file system path.
 * @param path Path to the file or directory.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.FilterFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",591,595,"/**
* Modifies ACL entries for a file or directory.
* @param path the file system path to modify
* @param aclSpec list of new ACL entries to apply
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.FilterFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",597,601,"/**
* Removes ACL entries from a file system path.
* @param path file system path to modify
* @param aclSpec list of AclEntry objects to remove
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processArguments,org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processArguments(java.util.LinkedList),77,88,"/**
* Creates a snapshot of the specified path and prints the result.
* @param items list containing a single PathData object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createSnapshot,org.apache.hadoop.fs.FileSystem:createSnapshot(org.apache.hadoop.fs.Path),3089,3091,"/**
* Creates a snapshot of the specified directory.
* @param path directory to snapshot
*/","* Create a snapshot with a default name.
   * @param path The directory where snapshots will be taken.
   * @return the snapshot path.
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createSnapshot,"org.apache.hadoop.fs.FilterFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",573,577,"/**
* Creates a snapshot of the specified file system location.
* @param path location in the file system to snapshot
* @param snapshotName name for the created snapshot
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,addOptionWithValue,org.apache.hadoop.fs.shell.CommandFormat:addOptionWithValue(java.lang.String),73,78,"/**
* Adds an option with a specified value to the collection.
* @param option unique identifier of the option
*/","* add option with value
   *
   * @param option option name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processArguments,org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processArguments(java.util.LinkedList),117,128,"/**
* Deletes a snapshot by ID.
* @param items list of PathData objects containing the snapshot path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.FilterFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",585,589,"/**
* Deletes a snapshot by name from the file system.
* @param path directory to delete snapshot from
* @param snapshotName unique identifier of the snapshot to delete
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,<init>,org.apache.hadoop.fs.shell.FsUsage$TableBuilder:<init>(java.lang.Object[]),274,278,"/**
* Initializes a new table builder with the given header columns.
* @param headers varargs of column headers
*/","* Create a table with headers
     * @param headers list of headers",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,isEmpty,org.apache.hadoop.fs.shell.FsUsage$TableBuilder:isEmpty(),348,350,"/**
* Checks if this collection is empty.
* @return true if no elements are present, false otherwise
*/","* Does table have any rows 
     * @return boolean",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path),831,836,"/**
* Retrieves extended attributes for the given file or directory.
* @param path Path to the resource
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getXAttrs,org.apache.hadoop.fs.FilterFs:getXAttrs(org.apache.hadoop.fs.Path),376,379,"/**
* Retrieves extended attributes from the file system.
* @param path file system path to retrieve attributes from
* @return map of attribute names to values or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,<init>,org.apache.hadoop.fs.Options$HandleOpt$Location:<init>(boolean),496,498,"/**
* Constructs Location object with specified change allowance.
* @param allowChanged true to enable location changes, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,<init>,org.apache.hadoop.fs.Options$HandleOpt$Data:<init>(boolean),471,473,"/**
* Initializes Data object with change tracking enabled status.
* @param allowChanged true to track changes, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CompositeCrcFileChecksum.java,toString,org.apache.hadoop.fs.CompositeCrcFileChecksum:toString(),84,87,"/**
* Returns a string representation of CRC data in format ""algorithmName:CRCValue"".
* @return formatted string containing algorithm name and CRC value.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell:<init>(long),910,912,"/**
 * Initializes a new shell with the specified execution interval.
 * @param interval time interval between executions (in milliseconds)
 */","* Create an instance with a minimum interval between executions; stderr is
   * not merged with stdout.
   * @param interval interval in milliseconds between command executions.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawPathHandle.java,equals,org.apache.hadoop.fs.RawPathHandle:equals(java.lang.Object),72,79,"/**
* Compares this PathHandle with another for equality.
* @param other object to compare with
* @return true if both have the same byte representation, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawPathHandle.java,hashCode,org.apache.hadoop.fs.RawPathHandle:hashCode(),81,84,"/**
* Computes hash code by delegating to the bytes representation.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawPathHandle.java,toString,org.apache.hadoop.fs.RawPathHandle:toString(),86,89,"/**
* Returns a human-readable string representation of the object.
* @return String representation or empty string if not applicable
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,run,org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:run(),209,236,"/**
* Runs a loop, periodically refreshing disk usage and sleeping for a jitter-adjusted interval.
* @throws InterruptedException if thread is interrupted while sleeping
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,setOwner,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setOwner(org.apache.hadoop.io.Text),93,99,"/**
* Sets the text owner.
* @param owner Text object to assign as owner, or a default Text object is created if null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,setRealUser,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setRealUser(org.apache.hadoop.io.Text),122,128,"/**
* Sets the real user's text representation.
* @param realUser Text object to assign or null to clear
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,"org.apache.hadoop.security.token.Token:<init>(byte[],byte[],org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",86,91,"/**
* Initializes a Token object with provided credentials and service info.
* @param identifier unique identifier bytes
* @param password password bytes
* @param kind token kind (e.g. authentication, authorization)
* @param service service or application name
*/","* Construct a token from the components.
   * @param identifier the token identifier
   * @param password the token's password
   * @param kind the kind of token
   * @param service the service for this token",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,org.apache.hadoop.security.token.Token:<init>(),96,101,"/**
* Initializes an empty token object with default values.
*/",* Default constructor.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",65,72,"/**
* Initializes Globber with file context, pattern and filter.
* @param fc FileContext instance
* @param pathPattern Path to match files against
* @param filter optional PathFilter for filtering matches
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter,boolean)",100,110,"/**
* Initializes a Globber instance for pattern-based file iteration.
* @param fc FileContext providing tracing and logging capabilities
* @param pathPattern Pattern to match against files
* @param filter Optional filter to narrow down matched files
* @param resolveSymlinks Whether to follow symbolic links in matching
*/","* File Context constructor for use by {@link GlobBuilder}.
   * @param fc file context
   * @param pathPattern path pattern
   * @param filter optional filter
   * @param resolveSymlinks should symlinks be resolved.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,getMessage,org.apache.hadoop.fs.PathIOException:getMessage(),86,104,"/**
* Constructs a human-readable error message.
* @return detailed error description
*/","Format:
   * cmd: {operation} `path' {to `target'}: error string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/UnionStorageStatistics.java,hasNext,org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:hasNext(),49,52,"/**
* Checks if iterator has more elements.
* @return true if iterator is non-null, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/UnionStorageStatistics.java,next,org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:next(),64,71,"/**
* Retrieves the next statistic from an iterator.
* @return The next LongStatistic object or throws NoSuchElementException if exhausted
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getAndIncrDirNumLastAccessed,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$Context:getAndIncrDirNumLastAccessed(),282,284,"/**
* Increments and returns directory number last accessed.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",860,866,"/**
* Creates a file system snapshot for the given path.
* @param path target file system path
* @param snapshotName name of the new snapshot
* @return Path object representing the new snapshot or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createSnapshot,"org.apache.hadoop.fs.FilterFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",397,401,"/**
* Creates a snapshot of the given file system path.
* @param path the file system path to snapshot
* @param snapshotName the name of the created snapshot
* @return a Path object representing the snapshot or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAbstractFileSystem,"org.apache.hadoop.fs.FileContext:getAbstractFileSystem(org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration)",339,363,"/**
* Retrieves an AbstractFileSystem object using user's privileges and configuration.
* @param user UserGroupInformation instance
* @param uri File URI
* @param conf Configuration object
* @return AbstractFileSystem object or throws exception if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsUser,"org.apache.hadoop.security.SecurityUtil:doAsUser(org.apache.hadoop.security.UserGroupInformation,java.security.PrivilegedExceptionAction)",552,559,"/**
* Executes the given privileged action as the current user.
* @param ugi UserGroupInformation object
* @param action Privileged action to execute
* @return Result of the action, or null if interrupted
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,handleSaslConnectionFailure,"org.apache.hadoop.ipc.Client$Connection:handleSaslConnectionFailure(int,int,java.io.IOException,java.util.Random,org.apache.hadoop.security.UserGroupInformation)",705,757,"/**
* Handles SASL connection failure by retrying authentication or throwing an exception.
* @param currRetries current retries
* @param maxRetries maximum retries
* @param ex IOException thrown during connection attempt
*/","* If multiple clients with the same principal try to connect to the same
     * server at the same time, the server assumes a replay attack is in
     * progress. This is a feature of kerberos. In order to work around this,
     * what is done is that the client backs off randomly and tries to initiate
     * the connection again. The other problem is to do with ticket expiry. To
     * handle that, a relogin is attempted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.java,isViewFileSystem,org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:isViewFileSystem(org.apache.hadoop.fs.FileSystem),50,52,"/**
* Checks if the given file system has ViewFS scheme.
* @param fileSystem file system to check
* @return true if ViewFS, false otherwise
*/","* Check if the FileSystem is a ViewFileSystem.
   *
   * @param fileSystem file system.
   * @return true if the fileSystem is ViewFileSystem",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,open,"org.apache.hadoop.fs.FilterFileSystem:open(org.apache.hadoop.fs.PathHandle,int)",171,175,"/**
* Opens a file stream with the specified buffer size.
* @param fd PathHandle to the file descriptor
* @param bufferSize Size of the read/write buffer
* @return FSDataInputStream object for reading from the file
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,primitiveMkdir,"org.apache.hadoop.fs.FilterFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",561,566,"/**
* Creates a new directory at the specified path with given permissions.
* @param f Path to create directory
* @param abdolutePermission Permissions for newly created directory
* @return true if directory creation was successful; false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setQuota,"org.apache.hadoop.fs.FileSystem:setQuota(org.apache.hadoop.fs.Path,long,long)",1965,1968,"/**
* Sets quota for a namespace and storage space.
* @param src Path to resource
* @param namespaceQuota Quota for namespace in bytes
* @param storagespaceQuota Quota for storage space in bytes
*/","* Set quota for the given {@link Path}.
   *
   * @param src the target path to set quota for
   * @param namespaceQuota the namespace quota (i.e., # of files/directories)
   *                       to set
   * @param storagespaceQuota the storage space quota to set
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setQuotaByStorageType,"org.apache.hadoop.fs.FileSystem:setQuotaByStorageType(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.StorageType,long)",1978,1981,"/**
* Sets storage quota by type.
* @param src file path
* @param type storage type (e.g. disk, ram)
* @param quota new quota value in bytes
*/","* Set per storage type quota for the given {@link Path}.
   *
   * @param src the target path to set storage type quota for
   * @param type the storage type to set
   * @param quota the quota to set for the given storage type
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createMultipartUploader,org.apache.hadoop.fs.FileSystem:createMultipartUploader(org.apache.hadoop.fs.Path),4987,4992,"/**
* Creates an MultipartUploader instance with base path.
* @param basePath base directory for multipart uploads
*/","* Create a multipart uploader.
   * @param basePath file path under which all files are uploaded
   * @return a MultipartUploaderBuilder object to build the uploader
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listCorruptFileBlocks,org.apache.hadoop.fs.FilterFileSystem:listCorruptFileBlocks(org.apache.hadoop.fs.Path),277,281,"/**
* Lists corrupt file blocks for a given file.
* @param path file path to check
* @return iterator over Path objects of corrupt file blocks or null if none found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listLocatedStatus,org.apache.hadoop.fs.FileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),2261,2264,"/**
* Lists located file status for the specified path.
* @param f file system path
*/","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   * Return the file's status and block locations If the path is a file.
   *
   * If a returned status is a file, it contains the file's block locations.
   *
   * @param f is the path
   *
   * @return an iterator that traverses statuses of the files/directories
   *         in the given path
   *
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws IOException If an I/O error occurred",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,listLocatedStatus,org.apache.hadoop.fs.ChecksumFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),980,984,"/**
* Lists located files and directories under the given path.
* @param f file system path to list
* @return iterator of LocatedFileStatus objects or null if empty
*/","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   *
   * @param f
   *          given path
   * @return the statuses of the files/directories in the given patch
   * @throws IOException if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listLocatedStatus,"org.apache.hadoop.fs.FilterFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",214,219,"/**
* Lists located file statuses in a filesystem.
* @param f directory path to search
* @param filter optional filter for status files
* @return iterator over LocatedFileStatus objects or empty if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFileSystem:readOnlyMountTable(java.lang.String,java.lang.String)",97,102,"/**
* Throws access control exception for read-only mount table operations.
* @param operation type of operation attempted
* @param p file path involved in the operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFs:readOnlyMountTable(java.lang.String,java.lang.String)",175,180,"/**
* Throws an AccessControlException for read-only mount table operations.
* @param operation the attempted file system operation
* @param p the affected file system path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,<init>,org.apache.hadoop.security.authorize.AuthorizationException:<init>(java.lang.String),41,43,"/**
* Constructs an AuthorizationException with a custom error message.
* @param message detailed description of the authorization failure.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,resolveLink,org.apache.hadoop.fs.FilterFileSystem:resolveLink(org.apache.hadoop.fs.Path),498,500,"/**
* Resolves a symbolic link to its target file.
* @param f the path to the symbolic link
* @return the resolved Path object or null if not a link
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileChecksum,org.apache.hadoop.fs.FileSystem:getFileChecksum(org.apache.hadoop.fs.Path),2980,2982,"/**
 * Retrieves checksum for the given file.
 * @param f Path to the file
 */","* Get the checksum of a file, if the FS supports checksums.
   *
   * @param f The file path
   * @return The file checksum.  The default return value is null,
   *  which indicates that no checksum algorithm is implemented
   *  in the corresponding FileSystem.
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileChecksum,"org.apache.hadoop.fs.FilterFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",507,510,"/**
* Retrieves file checksum using the underlying FS interface.
* @param f Path to the file
* @param length expected file length
* @return FileChecksum object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setXAttr,"org.apache.hadoop.fs.FileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",3244,3248,"/**
* Sets extended attribute with specified name and value.
* @param path file path
* @param name attribute name
* @param value attribute value in bytes
*/","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported
   *         (default outcome).",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setXAttr,"org.apache.hadoop.fs.FilterFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",629,633,"/**
* Sets extended attribute on a file.
* @param path file path
* @param name attribute name
* @param value attribute value as byte array
* @param flag set flags for attribute operation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getXAttrs,"org.apache.hadoop.fs.FilterFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",645,649,"/**
* Retrieves extended attributes from the file system.
* @param path file path
* @param names list of attribute names to retrieve
* @return map of attribute names to their corresponding byte values or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listXAttrs,org.apache.hadoop.fs.FilterFileSystem:listXAttrs(org.apache.hadoop.fs.Path),651,654,"/**
* Lists extended attributes for the specified file or directory.
* @param path file system Path object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.FilterFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path),661,664,"/**
 * Satisfies storage policy for file at specified path.
 * @param src Path to file or directory
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.FilterFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",666,670,"/**
* Sets storage policy for a given file path.
* @param src Path to the file
* @param policyName Name of the storage policy to apply
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.FilterFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path),672,675,"/**
* Unsets storage policy for a given file system path.
* @param src Path to file or directory on which to unset policy
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.FilterFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path),677,681,"/**
* Retrieves storage policy for given file system path.
* @param src file system path
* @return BlockStoragePolicy object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAllStoragePolicies,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAllStoragePolicies(),1925,1939,"/**
* Retrieves all storage policies from child file systems.
* @return Collection of BlockStoragePolicySpi objects or empty collection if none found.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getAllStoragePolicies,org.apache.hadoop.fs.FilterFileSystem:getAllStoragePolicies(),683,687,"/**
* Retrieves all available storage policies.
* @return collection of BlockStoragePolicySpi instances
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",4806,4816,"/**
* Opens a file with specified options and buffering.
* @param path file location
* @param parameters OpenFileParameters object
* @return FSDataInputStream or null on failure
*/","* Execute the actual open file operation.
   *
   * This is invoked from {@code FSDataInputStreamBuilder.build()}
   * and from {@link DelegateToFileSystem} and is where
   * the action of opening the file should begin.
   *
   * The base implementation performs a blocking
   * call to {@link #open(Path, int)} in this call;
   * the actual outcome is in the returned {@code CompletableFuture}.
   * This avoids having to create some thread pool, while still
   * setting up the expectation that the {@code get()} call
   * is needed to evaluate the result.
   * @param path path to the file
   * @param parameters open file parameters from the builder.
   * @return a future which will evaluate to the opened file.
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.ChecksumFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",1090,1101,"/**
* Opens a file with custom options asynchronously.
* @param path the file path to open
* @param parameters OpenFileParameters object for customization
* @return FSDataInputStream or null if not opened successfully
*/","* Open the file as a blocking call to {@link #open(Path, int)}.
   *
   * {@inheritDoc}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.AbstractFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",1603,1612,"/**
* Opens a file asynchronously with specified options.
* @param path the file path
* @param parameters OpenFileParameters containing buffering and other settings
* @return FSDataInputStream for reading or null on failure
*/","* Open a file with the given set of options.
   * The base implementation performs a blocking
   * call to {@link #open(Path, int)}in this call;
   * the actual outcome is in the returned {@code CompletableFuture}.
   * This avoids having to create some thread pool, while still
   * setting up the expectation that the {@code get()} call
   * is needed to evaluate the result.
   * @param path path to the file
   * @param parameters open file parameters from the builder.
   * @return a future which will evaluate to the opened file.
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FileSystem:openFileWithOptions(org.apache.hadoop.fs.PathHandle,org.apache.hadoop.fs.impl.OpenFileParameters)",4834,4852,"/**
* Opens a file with specified options asynchronously.
* @param pathHandle unique file identifier
* @param parameters OpenFileParameters object containing buffer size and other options
* @return CompletableFuture containing FSDataInputStream or null if not found
*/","* Execute the actual open file operation.
   * The base implementation performs a blocking
   * call to {@link #open(Path, int)} in this call;
   * the actual outcome is in the returned {@code CompletableFuture}.
   * This avoids having to create some thread pool, while still
   * setting up the expectation that the {@code get()} call
   * is needed to evaluate the result.
   * @param pathHandle path to the file
   * @param parameters open file parameters from the builder.
   * @return a future which will evaluate to the opened file.
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key
   * @throws UnsupportedOperationException PathHandles are not supported.
   * This may be deferred until the future is evaluated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,isValidName,org.apache.hadoop.fs.FilterFs:isValidName(java.lang.String),322,325,"/**
 * Validates a name using an external file system service.
 * @param src the name to validate
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",770,776,"/**
* Modifies ACL entries for a file system path.
* @param path the file system path
* @param aclSpec the list of ACL entries to apply
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,modifyAclEntries,"org.apache.hadoop.fs.FilterFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",327,331,"/**
* Modifies ACL entries for a given file system entry.
* @param path file system path to update
* @param aclSpec list of new ACL entries
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",778,784,"/**
* Removes ACL entries from the file system at the specified path.
* @param path file system path
* @param aclSpec list of ACL entries to remove
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeAclEntries,"org.apache.hadoop.fs.FilterFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",333,337,"/**
* Removes ACL entries from a file system location.
* @param path file system path to modify
* @param aclSpec list of ACL entries to remove
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path),786,792,"/**
* Removes default ACL from the specified file system location.
* @param path file system path to remove default ACL from
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeDefaultAcl,org.apache.hadoop.fs.FilterFs:removeDefaultAcl(org.apache.hadoop.fs.Path),339,342,"/**
* Removes default ACL from the specified file system location.
* @param path file system path to remove default ACL from
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",868,875,"/**
* Renames a snapshot on the file system.
* @param path snapshot location
* @param snapshotOldName current name of the snapshot
* @param snapshotNewName new name for the snapshot
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,renameSnapshot,"org.apache.hadoop.fs.FilterFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",403,407,"/**
* Renames a snapshot at the specified file system location.
* @param path file system location of the snapshot
* @param snapshotOldName current name of the snapshot
* @param snapshotNewName new desired name for the snapshot
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),401,404,"/**
* Satisfies storage policy for the given file or directory.
* @param path the file system path to enforce storage policies on
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),884,889,"/**
* Applies storage policy to the specified file system.
* @param path file system path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.FilterFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),415,418,"/**
* Satisfies storage policy for the specified directory.
* @param path directory path to enforce storage policy on
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),899,905,"/**
* Unsets the storage policy for the given file system path.
* @param src Path to unset the storage policy from
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,unsetStoragePolicy,org.apache.hadoop.fs.FilterFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),426,430,"/**
* Unsets storage policy for the given file path.
* @param src file path to unset storage policy for 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getAllStoragePolicies,org.apache.hadoop.fs.viewfs.ChRootedFs:getAllStoragePolicies(),424,428,"/**
* Retrieves all storage policies from the file system.
* @throws IOException if an I/O error occurs while retrieving policies
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAllStoragePolicies,org.apache.hadoop.fs.FileContext:getAllStoragePolicies(),2916,2919,"/**
* Retrieves all available storage policies.
* @return Collection of BlockStoragePolicySpi objects
*/","* Retrieve all the storage policies supported by this file system.
   *
   * @return all storage policies supported by this filesystem.
   * @throws IOException If an I/O error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getAllStoragePolicies,org.apache.hadoop.fs.FilterFs:getAllStoragePolicies(),438,442,"/**
* Retrieves all storage policies from the file system.
* @throws IOException if an I/O error occurs while retrieving policies.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,supportsSymlinks,org.apache.hadoop.fs.viewfs.ChRootedFs:supportsSymlinks(),436,439,"/**
* Checks whether file system supports symbolic links.
* @return true if supported, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,supportsSymlinks,org.apache.hadoop.fs.FilterFs:supportsSymlinks(),296,299,"/**
* Determines whether the underlying file system supports symbolic links.
* @return true if supported, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createSymlink,"org.apache.hadoop.fs.FilterFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",301,305,"/**
* Creates symbolic link to specified file/directory.
* @param target path of the target file/directory
* @param link   path where symlink will be created
* @param createParent whether to create parent directory if it doesn't exist
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ViewFs:getLinkTarget(org.apache.hadoop.fs.Path),669,674,"/**
* Resolves the target of a file system link.
* @param f Path object representing the file system link
* @return Path to the linked target or null if not resolved
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getLinkTarget,org.apache.hadoop.fs.FilterFs:getLinkTarget(org.apache.hadoop.fs.Path),307,310,"/**
 * Retrieves the target of a symbolic link.
 * 
 * @param f The path to the symbolic link
 * @return The target file or directory, or null if not a valid link
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getDelegationTokens,org.apache.hadoop.fs.viewfs.ChRootedFs:getDelegationTokens(java.lang.String),459,462,"/**
* Retrieves delegation tokens from file system.
* @param renewer entity performing renewal
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getDelegationTokens,org.apache.hadoop.fs.FilterFs:getDelegationTokens(java.lang.String),317,320,"/**
* Retrieves delegation tokens from an abstraction of the file system.
* @param renewer identifier of the entity requesting tokens
* @return list of Token objects or empty list if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileChecksum(org.apache.hadoop.fs.Path),1073,1078,"/**
* Throws FileNotFoundException if path points to directory instead of file.
* @param f Path object to validate
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,open,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path,int)",1301,1306,"/**
* Throws exception when attempting to open a directory as a file.
* @throws FileNotFoundException if path is a directory
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,initializeMountedFileSystems,org.apache.hadoop.fs.viewfs.ViewFileSystem:initializeMountedFileSystems(java.util.List),943,959,"/**
* Initializes and maps file systems for given mount points.
* @param mountPoints list of mounted file system targets
* @return map of file system instances keyed by mount path
*/","* Initialize the target filesystem for all mount points.
   * @param mountPoints The mount points
   * @return Mapping of mount point and the initialized target filesystems
   * @throws RuntimeException when the target file system cannot be initialized",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getDelegationTokens,org.apache.hadoop.fs.viewfs.ViewFs:getDelegationTokens(java.lang.String),732,761,"/**
* Retrieves delegation tokens for the given renewer from all mount points and fallback FS.
* @param renewer identifier of the token holder
* @return list of Token objects or empty list if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1422,1425,"/**
* Throws exception when trying to get extended attribute by name.
* @param path file system path
* @param name attribute name
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path),1427,1430,"/**
* Raises an exception when attempting to fetch extended attributes for an unmounted file.
* @param path the file path
* @throws NotInMountpointException if the file is not in a mounted mount point
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",1432,1436,"/**
 * Throws an exception when attempting to retrieve extended attributes.
 * @param path Path object specifying the file or directory
 * @throws NotInMountpointException if operation fails
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listXAttrs(org.apache.hadoop.fs.Path),1438,1441,"/**
* Lists extended attributes of a file or directory.
* @param path Path to the resource (throws exception if not in mount point)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getServerDefaults(org.apache.hadoop.fs.Path),1785,1788,"/**
* Returns server defaults for the specified file system path.
* Throws NotInMountpointException if path is not in mount point.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultBlockSize(org.apache.hadoop.fs.Path),1790,1793,"/**
* Returns default block size for the given file path.
* @param f Path to file (not in mount point)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultReplication(org.apache.hadoop.fs.Path),1795,1798,"/**
* Throws an exception when trying to get the default replication for a file outside the mount point.
* @param f Path to the file
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1848,1851,"/**
* Returns XATTR value by name for the given file or directory.
* @param path Path to the file or directory
* @param name Name of the XATTR key
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path),1853,1856,"/**
* Returns X-Attr metadata for the given file path.
* @param path File path to retrieve metadata from
* @throws NotInMountpointException if path is not in a supported mount point
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",1858,1862,"/**
* Retrieves extended attributes from a file.
* @param path file Path object to fetch XAttrs for
* @param names list of attribute names to retrieve
* @throws IOException if an I/O error occurs or the file is not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listXAttrs(org.apache.hadoop.fs.Path),1864,1867,"/**
* Lists extended attributes (xattrs) for the given file path.
* @param path file system path to query xattrs for
* @throws NotInMountpointException if the path is not in a mount point
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getQuotaUsage,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getQuotaUsage(org.apache.hadoop.fs.Path),1896,1899,"/**
* Returns quota usage for the given file path.
* @param f file path to retrieve quota information for
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStoragePolicy(org.apache.hadoop.fs.Path),1920,1923,"/**
* Returns storage policy for the given source path.
* @param src source path to check
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java,serializeToString,org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:serializeToString(),111,116,"/**
* Serializes configuration data to a string.
* @return Serialized configuration string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize(),961,964,"/**
* Returns default block size; throws exception if not in mount point.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultReplication(),966,969,"/**
* Returns default replication value; throws exception since this is not a mount point.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults(),971,974,"/**
* Returns server defaults; always throws an exception. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,delete,"org.apache.hadoop.fs.viewfs.ViewFs:delete(org.apache.hadoop.fs.Path,boolean)",366,378,"/**
* Deletes a file or directory.
* @param f Path to the file/directory
* @param recursive whether to recursively delete contents if it's a directory
* @return true if deletion was successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatusIterator,org.apache.hadoop.fs.viewfs.ViewFs:listStatusIterator(org.apache.hadoop.fs.Path),451,469,"/**
* Retrieves a remote iterator for file statuses in the specified directory.
* @param f target directory path
* @return RemoteIterator containing file status objects or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.ViewFs:listLocatedStatus(org.apache.hadoop.fs.Path),471,490,"/**
* Lists located file statuses for the given path.
* @param f path to list statuses for
* @return iterator over LocatedFileStatus objects or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,buildResolveResultForRegexMountPoint,"org.apache.hadoop.fs.viewfs.InodeTree:buildResolveResultForRegexMountPoint(org.apache.hadoop.fs.viewfs.InodeTree$ResultKind,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)",1053,1081,"/**
* Builds ResolveResult for regex mount point based on given parameters.
* @param resultKind Result kind
* @param resolvedPathStr Resolved path string
* @param targetOfResolvedPathStr Target of resolved path string
* @return ResolveResult object or null if initialization fails
*/","* Build resolve result.
   * Here's an example
   * Mountpoint: fs.viewfs.mounttable.mt
   *     .linkRegex.replaceresolveddstpath:_:-#.^/user/(??&lt;username&gt;\w+)
   * Value: /targetTestRoot/$username
   * Dir path to test:
   * viewfs://mt/user/hadoop_user1/hadoop_dir1
   * Expect path: /targetTestRoot/hadoop-user1/hadoop_dir1
   * resolvedPathStr: /user/hadoop_user1
   * targetOfResolvedPathStr: /targetTestRoot/hadoop-user1
   * remainingPath: /hadoop_dir1
   *
   * @param resultKind resultKind.
   * @param resolvedPathStr resolvedPathStr.
   * @param targetOfResolvedPathStr targetOfResolvedPathStr.
   * @param remainingPath remainingPath.
   * @return targetFileSystem or null on exceptions.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,fsGetter,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:fsGetter(),216,219,"/**
* Returns an instance of ChildFsGetter with scheme set to current scheme.
* @return ChildFsGetter object
*/","* This method is overridden because in ViewFileSystemOverloadScheme if
   * overloaded scheme matches with mounted target fs scheme, file system
   * should be created without going into {@literal fs.<scheme>.impl} based
   * resolution. Otherwise it will end up in an infinite loop as the target
   * will be resolved again to ViewFileSystemOverloadScheme as
   * {@literal fs.<scheme>.impl} points to ViewFileSystemOverloadScheme.
   * So, below method will initialize the
   * {@literal fs.viewfs.overload.scheme.target.<scheme>.impl}.
   * Other schemes can follow fs.newInstance",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,getBlockLocations,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getBlockLocations(),112,115,"/**
* Retrieves an array of block locations from the file system.
* @return Array of BlockLocation objects or null if not available
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",167,169,"/**
* Initializes an instance of INodeDir with the specified node path and user group information.
* @param pathToNode directory path to initialize
* @param aUgi user group credentials for access control
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.Object,java.lang.String[])",344,349,"/**
* Initializes INodeLink with specified parameters.
* @param pathToNode path to the node
* @param aUgi user group information
* @param targetMergeFs target file system
* @param aTargetDirLinkList list of target directory links
*/",* Construct a mergeLink or nfly.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.util.function.Function,java.lang.String)",354,362,"/**
* Initializes an INodeLink instance with the specified parameters.
* @param pathToNode path to node
* @param aUgi user group information
* @param createFileSystemMethod method to create file system
* @param aTargetDirLink target directory link
*/",* Construct a simple link (i.e. not a mergeLink).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,addLink,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addLink(java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)",222,228,"/**
* Adds a child node link to the directory structure.
* @param pathComponent unique path component identifier
* @param link INodeLink object containing link details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,buildLinkRegexEntry,"org.apache.hadoop.fs.viewfs.InodeTree:buildLinkRegexEntry(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)",818,845,"/**
* Builds a LinkEntry from configuration and mount entry data.
* @param config Hadoop Configuration object
* @param ugi UserGroupInformation instance
* @param mntEntryStrippedKey stripped key from mount entry
* @param mntEntryValue value from mount entry
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,processThrowable,"org.apache.hadoop.fs.viewfs.NflyFSystem:processThrowable(org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode,java.lang.String,java.lang.Throwable,java.util.List,org.apache.hadoop.fs.Path[])",917,933,"/**
* Creates and adds an IOException to the list for failed operation.
* @param nflyNode NflyNode object
* @param op operation being performed
* @param t Throwable exception
* @param ioExceptions list of IO exceptions
* @param f file paths involved in failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getWorkingDirectory,org.apache.hadoop.fs.viewfs.NflyFSystem:getWorkingDirectory(),861,864,"/**
* Returns the working directory of the node's file system.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,replaceRegexCaptureGroupInPath,"org.apache.hadoop.fs.viewfs.RegexMountPoint:replaceRegexCaptureGroupInPath(java.lang.String,java.util.regex.Matcher,java.lang.String,java.util.Set)",246,261,"/**
* Replaces regex capture groups in a path with actual values.
* @param parsedDestPath original destination path
* @param srcMatcher source matcher object
* @param regexGroupNameOrIndexStr group name or index to replace
* @param groupRepresentationStrSetInDest set of placeholder strings in dest path
* @return updated path with replaced capture groups
*/","* Use capture group named regexGroupNameOrIndexStr in mather to replace
   * parsedDestPath.
   * E.g. link: ^/user/(?<username>\\w+) => s3://$user.apache.com/_${user}
   * srcMatcher is from /user/hadoop.
   * Then the params will be like following.
   * parsedDestPath: s3://$user.apache.com/_${user},
   * regexGroupNameOrIndexStr: user
   * groupRepresentationStrSetInDest: {user:$user; user:${user}}
   * return value will be s3://hadoop.apache.com/_hadoop
   * @param parsedDestPath
   * @param srcMatcher
   * @param regexGroupNameOrIndexStr
   * @param groupRepresentationStrSetInDest
   * @return return parsedDestPath while ${var},$var replaced or
   * parsedDestPath nothing found.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRootDir,org.apache.hadoop.fs.viewfs.InodeTree:getRootDir(),520,523,"/**
* Retrieves the root directory of the node graph.
* @return The root directory as an INodeDir object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRootLink,org.apache.hadoop.fs.viewfs.InodeTree:getRootLink(),525,528,"/**
* Returns the root link of the tree structure.
* @return The root node as an INodeLink object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRootFallbackLink,org.apache.hadoop.fs.viewfs.InodeTree:getRootFallbackLink(),543,546,"/**
* Returns the fallback link to the root node.
* @return INodeLink instance or null if not applicable
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,tryStart,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStart(),156,184,"/**
* Starts the async call processor thread if not already running.
* @throws AssertionError if concurrent start attempt fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,kill,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:kill(org.apache.hadoop.util.Daemon),192,198,"/**
* Terminates a daemon thread and updates the running set.
* @param d Daemon thread to be killed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,offer,org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:offer(java.lang.Object),101,104,"/**
* Adds an element to the queue and asserts its successful addition.
* @param c element to be offered
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(org.apache.hadoop.crypto.Decryptor,java.nio.ByteBuffer,java.nio.ByteBuffer,byte)",246,265,"/**
* Decrypts data using provided Decryptor and stores result in output buffer.
* @param decryptor decryption engine
* @param inBuffer input data to be decrypted
* @param outBuffer output buffer for encrypted data
* @param padding encryption padding value
*/","* Do the decryption using inBuffer as input and outBuffer as output.
   * Upon return, inBuffer is cleared; the decrypted data starts at 
   * outBuffer.position() and ends at outBuffer.limit();",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,checkState,org.apache.hadoop.crypto.OpensslCipher:checkState(),289,291,"/**
 * Ensures context is initialized and non-zero.
 */",Check whether context is initialized.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,parentZNodeExists,org.apache.hadoop.ha.ActiveStandbyElector:parentZNodeExists(),341,350,"/**
* Checks if the parent ZNode exists.
* @return true if parent ZNode exists, false otherwise
*/","* @return true if the configured parent znode exists
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException interrupted exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getConfigViewFsPrefix,org.apache.hadoop.fs.viewfs.ConfigUtil:getConfigViewFsPrefix(),43,46,"/**
* Returns the default file system prefix.
* @return The configured prefix or the default mount table value.","* Get the config variable prefix for the default mount table
   * @return the config variable prefix for the default mount table",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileChecksum(org.apache.hadoop.fs.Path),1537,1542,"/**
* Throws exception if path is directory instead of a file.
* @param f Path object to validate
* @throws FileNotFoundException if path points to directory
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,open,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path,int)",1732,1737,"/**
* Throws exception when trying to open a directory as a file.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java,deserializeFromString,org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:deserializeFromString(java.lang.String),125,136,"/**
* Deserializes a string into a RegexMountPointResolvedDstPathReplaceInterceptor.
* @param serializedString String representation of the interceptor
* @return Interceptor object or null if malformed
*/","* Create interceptor from config string. The string should be in
   * replaceresolvedpath:wordToReplace:replaceString
   * Note that we'll assume there's no ':' in the regex for the moment.
   *
   * @return Interceptor instance or null on bad config.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,getReturnValue,org.apache.hadoop.io.retry.CallReturn:getReturnValue(),71,77,"/**
* Returns the result of the operation, throwing an exception if one occurred.
* @throws Throwable if an error was encountered
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputWrapper.java,getReadableByteChannel,org.apache.hadoop.net.SocketInputWrapper:getReadableByteChannel(),81,86,"/**
* Returns a readable byte channel from the socket input stream.
* @return a SocketInputStream object as a ReadableByteChannel
*/","* @return an underlying ReadableByteChannel implementation.
   * @throws IllegalStateException if this socket does not have a channel",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getChecksumFileLength,"org.apache.hadoop.fs.ChecksumFileSystem:getChecksumFileLength(org.apache.hadoop.fs.Path,long)",143,145,"/**
* Calculates the length of a checksum file.
* @param file Path to the file
* @param fileSize total size of the file in bytes
* @return length of the corresponding checksum file in bytes
*/","* Return the length of the checksum file given the size of the
   * actual file.
   *
   * @param file the file path.
   * @param fileSize file size.
   * @return checksum length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,getFilesystem,org.apache.hadoop.fs.DF:getFilesystem(),72,82,"/**
* Retrieves the filesystem identifier (Windows: drive letter, Unix/Linux: path prefix).
* @throws IOException if an I/O error occurs
*/","* @return a string indicating which filesystem volume we're checking.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,getMount,org.apache.hadoop.fs.DF:getMount(),110,127,"/**
* Determines the filesystem mount point of the specified path.
* @return mount point as a string or throws IOException if an error occurs
*/","* @return the filesystem mount point for the indicated volume.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DFCachingGetSpaceUsed.java,refresh,org.apache.hadoop.fs.DFCachingGetSpaceUsed:refresh(),44,47,"/**
 * Updates the used flag based on the current usage data. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,getPercentUsed,org.apache.hadoop.fs.DF:getPercentUsed(),100,104,"/**
* Calculates percentage of capacity currently in use.
* @return Percentage value between 0 and 100
*/","@return the amount of the volume full, as a percent.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,startPositionWithoutWindowsDrive,org.apache.hadoop.fs.Path:startPositionWithoutWindowsDrive(java.lang.String),324,330,"/**
* Calculates start position of path without Windows drive letter.
* @param path input file path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,toString,org.apache.hadoop.fs.Path:toString(),476,503,"/**
* Constructs a string representation of the URI, unescaping special characters.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,<init>,"org.apache.hadoop.fs.FSInputChecker:<init>(org.apache.hadoop.fs.Path,int,boolean,java.util.zip.Checksum,int,int)",87,91,"/**
* Initializes the input checker with file and retry details.
* @param file Path to the file being checked
* @param numOfRetries Number of retries for failed checks
*/","Constructor
   * 
   * @param file The name of the file to be read
   * @param numOfRetries Number of read retries when ChecksumError occurs
   * @param sum the type of Checksum engine
   * @param chunkSize maximun chunk size
   * @param checksumSize the number byte of each checksum
   * @param verifyChecksum verify check sum.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,seek,org.apache.hadoop.fs.FSInputChecker:seek(long),428,451,"/**
* Seeks to a specific position in the file.
* @param pos target position in bytes
* @throws IOException on failure or EOF
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ByteBufferUtil.java,streamHasByteBufferRead,org.apache.hadoop.fs.ByteBufferUtil:streamHasByteBufferRead(java.io.InputStream),37,46,"/**
* Checks if the given InputStream has a read capability via ByteBuffer.
* @param stream input stream to check
* @return true if stream is ByteBufferReadable, false otherwise
*/",* Determine if a stream can do a byte buffer read via read(ByteBuffer buf),,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,init,org.apache.hadoop.fs.audit.CommonAuditContext:init(),193,197,"/**
* Initializes initialization context with current thread ID.
*/",* Initialize.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,noteEntryPoint,org.apache.hadoop.fs.audit.CommonAuditContext:noteEntryPoint(java.lang.Object),278,288,"/**
* Sets the command based on the provided tool object.
* @param tool Object instance to extract class information
*/","* Add the entry point as a context entry with the key
   * {@link AuditConstants#PARAM_COMMAND}
   * if it has not  already been recorded.
   * This is called via ToolRunner but may be used at any
   * other entry point.
   * @param tool object loaded/being launched.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,<init>,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:<init>(),404,405,"/**
 * Private constructor to prevent direct instantiation of builder.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,int)",93,95,"/**
* Sets an integer option by key.
* @param key unique option identifier
* @param value integer value to set
*/","* Set optional int parameter for the Builder.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #opt(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,float)",108,111,"/**
* Converts floating-point value to long and fetches corresponding option.
* @param key unique option identifier
* @param value floating-point value to convert
*/","* This parameter is converted to a long and passed
   * to {@link #optLong(String, long)} -all
   * decimal precision is lost.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #opt(String, String)
   * @deprecated use {@link #optDouble(String, double)}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,long)",121,123,"/**
* Sets a long-typed option with the given key and value.
* @param key unique option identifier
* @param value long integer value to set
*/","* Set optional long parameter for the Builder.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @deprecated use  {@link #optLong(String, long)} where possible.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,double)",136,139,"/**
* Converts a double to long and passes it to the optLong method.
* @param key unique key identifier
* @param value numeric value to be stored
*/","* Pass an optional double parameter for the Builder.
   * This parameter is converted to a long and passed
   * to {@link #optLong(String, long)} -all
   * decimal precision is lost.
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #opt(String, String)
   * @deprecated use {@link #optDouble(String, double)}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,int)",207,209,"/**
* Converts integer to long with validation.
* @param key identifier key
* @param value integer value to convert
*/","* Set mandatory int option.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #must(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,float)",221,224,"/**
* Converts float to long and calls mustLong with it.
* @param key unique identifier key
* @param value float value to convert
*/","* This parameter is converted to a long and passed
   * to {@link #mustLong(String, long)} -all
   * decimal precision is lost.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @deprecated use {@link #mustDouble(String, double)} to set floating point.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,long)",234,237,"/**
* Converts a numeric value to a long and calls mustLong.
* @param key unique identifier
* @param value numeric value to convert
*/","* Set mandatory long option.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #must(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,double)",248,251,"/**
* Converts double to long and calls mustLong with it.
* @param key unique identifier
* @param value numeric value to be converted and validated
*/","* Set mandatory long option, despite passing in a floating
   * point value.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #must(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,getRow,org.apache.hadoop.tools.TableListing$Column:getRow(int),100,116,"/**
* Retrieves a row of text at the specified index, wrapping and justifying as needed.
* @param idx row index
*/","* Return the ith row of the column as a set of wrapped strings, each at
     * most wrapWidth in length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBUploadHandle.java,from,org.apache.hadoop.fs.BBUploadHandle:from(java.nio.ByteBuffer),40,42,"/**
* Creates an UploadHandle instance from a ByteBuffer.
* @param byteBuffer input buffer containing upload data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBUploadHandle.java,equals,org.apache.hadoop.fs.BBUploadHandle:equals(java.lang.Object),54,61,"/**
* Checks if this UploadHandle is equal to the given object.
* @param other object to compare, must be an instance of UploadHandle
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,startLocalOutput,"org.apache.hadoop.fs.FilterFileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",396,400,"/**
* Delegates local output setup to underlying file system.
* @param fsOutputFile path to output file on file system
* @param tmpLocalFile temporary local file path
*/","* Returns a local File that the user can write output to.  The caller
   * provides both the eventual FS target name and the local working
   * file.  If the FS is local, we write directly into the target.  If
   * the FS is remote, we write into the tmp local area.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setWriteChecksum,org.apache.hadoop.fs.FilterFileSystem:setWriteChecksum(boolean),517,520,"/**
 * Sets whether to write checksums during file operations.
 * @param writeChecksum true to enable checksum writing, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,"org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean)",86,88,"/**
* Constructs FsPermission object with specified read/write permissions.
* @param u user permission
* @param g group permission
* @param o others permission
* @param sb sticky bit flag
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,fromShort,org.apache.hadoop.fs.permission.FsPermission:fromShort(short),174,177,"/**
* Initializes FsAction array elements based on input short value.
* @param n input short value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclStatus.java,getEffectivePermission,"org.apache.hadoop.fs.permission.AclStatus:getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry,org.apache.hadoop.fs.permission.FsPermission)",247,277,"/**
* Calculates the effective permission by combining server-provided
* permissions with ACL entries. Handles group and default mask cases.
* @param entry AclEntry object
* @param permArg FsPermission argument
* @return FsAction representing the effective permission
*/","* Get the effective permission for the AclEntry. <br>
   * Recommended to use this API ONLY if client communicates with the old
   * NameNode, needs to pass the Permission for the path to get effective
   * permission, else use {@link AclStatus#getEffectivePermission(AclEntry)}.
   * @param entry AclEntry to get the effective action
   * @param permArg Permission for the path. However if the client is NOT
   *          communicating with old namenode, then this argument will not have
   *          any preference.
   * @return Returns the effective permission for the entry.
   * @throws IllegalArgumentException If the client communicating with old
   *           namenode and permission is not passed as an argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,<init>,"org.apache.hadoop.fs.permission.PermissionStatus$2:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",72,76,"/**
* Constructs a new PermissionStatus object.
* @param user      username associated with the permission
* @param group     group name associated with the permission
* @param permission FsPermission value for the given user/group
*/","* Constructor.
   *
   * @param user user.
   * @param group group.
   * @param permission permission.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,parseAclEntry,"org.apache.hadoop.fs.permission.AclEntry:parseAclEntry(java.lang.String,boolean)",263,323,"/**
* Parses ACL entry string into AclEntry object.
* @param aclStr ACL entry string
* @param includePermission whether to parse permission from the string
* @return Parsed AclEntry object or null if invalid input
*/","* Parses a string representation of an ACL into a AclEntry object.<br>
   * The expected format of ACL entries in the string parameter is the same
   * format produced by the {@link #toStringStable()} method.
   * 
   * @param aclStr
   *          String representation of an ACL.<br>
   *          Example: ""user:foo:rw-""
   * @param includePermission
   *          for setAcl operations this will be true. i.e. Acl should include
   *          permissions.<br>
   *          But for removeAcl operation it will be false. i.e. Acl should not
   *          contain permissions.<br>
   *          Example: ""user:foo,group:bar,mask::""
   * @return Returns an {@link AclEntry} object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,<init>,"org.apache.hadoop.fs.permission.FsCreateModes:<init>(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",65,70,"/**
* Initializes a new FsCreateModes instance with the specified permissions.
* @param masked primary permission mode
* @param unmasked secondary permission mode
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,equals,org.apache.hadoop.fs.permission.FsCreateModes:equals(java.lang.Object),88,101,"/**
* Compares this FsCreateModes object with another Object for equality.
* @param o the Object to compare with
* @return true if both objects have the same unmasked value, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,toStringStable,org.apache.hadoop.fs.permission.AclEntry:toStringStable(),119,136,"/**
* Returns a stable string representation of the AclEntry.
* @return String representation of the entry, e.g. ""default:read:name:write""
*/","* Returns a string representation guaranteed to be stable across versions to
   * satisfy backward compatibility requirements, such as for shell command
   * output or serialization.  The format of this string representation matches
   * what is expected by the {@link #parseAclSpec(String, boolean)} and
   * {@link #parseAclEntry(String, boolean)} methods.
   *
   * @return stable, backward compatible string representation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntryType.java,toString,org.apache.hadoop.fs.permission.AclEntryType:toString(),59,65,"/**
* Returns a potentially unstable string representation of this object (may change across versions).
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getStrings,"org.apache.hadoop.util.StringUtils:getStrings(java.lang.String,java.lang.String)",418,424,"/**
* Splits input string into an array of substrings using the specified delimiter.
* @param str input string to split
* @param delim delimiter character
* @return array of strings or null if input is empty
*/","* Returns an arraylist of strings.
   * @param str the string values
   * @param delim delimiter to separate the values
   * @return the arraylist of the separated string values",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getStringCollection,org.apache.hadoop.util.StringUtils:getStringCollection(java.lang.String),431,434,"/**
* Returns a collection of strings split from input string using comma as delimiter.
* @param str input string to be split
*/","* Returns a collection of strings.
   * @param str comma separated string values
   * @return an <code>ArrayList</code> of string values",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionParser.java,<init>,"org.apache.hadoop.fs.permission.PermissionParser:<init>(java.lang.String,java.util.regex.Pattern,java.util.regex.Pattern)",51,62,"/**
* Parses permission string into a Permission object based on symbolic or octal patterns.
* @param modeStr permission string to parse
* @param symbolic symbolic pattern for parsing
* @param octal octal pattern for parsing
*/","* Begin parsing permission stored in modeStr
   * 
   * @param modeStr Permission mode, either octal or symbolic
   * @param symbolic Use-case specific symbolic pattern to match against
   * @throws IllegalArgumentException if unable to parse modeStr",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionParser.java,combineModes,"org.apache.hadoop.fs.permission.PermissionParser:combineModes(int,boolean)",175,183,"/**
* Combines user and resource mode segments into a single integer.
* @param existing existing combined mode value
* @param exeOk execution OK flag
* @return combined mode value with updated segments",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",649,651,"/**
 * Initializes ByteBuffer block factory with directory path and Hadoop configuration.
 * @param keyToBufferDir directory path to use for buffering
 * @param conf Hadoop configuration instance
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",526,528,"/**
* Constructs an ArrayBlockFactory instance.
* @param keyToBufferDir directory key to buffer data
* @param conf Hadoop configuration object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,requestBuffer,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:requestBuffer(int),659,663,"/**
* Retrieves a buffer from the pool with specified size.
* @param limit requested buffer size
* @return ByteBuffer object or null if not available
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,releaseBuffer,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:releaseBuffer(java.nio.ByteBuffer),665,669,"/**
* Releases a ByteBuffer back to the pool and decrements outstanding buffer count.
* @param buffer The ByteBuffer to be released
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$DiskBlock:<init>(java.io.File,int,long,org.apache.hadoop.fs.store.BlockUploadStatistics)",853,863,"/**
* Initializes DiskBlock object with file buffer, upload limit, and index.
* @param bufferFile File to write disk blocks to
* @param limit Maximum number of bytes that can be written
* @param index Block index in the file
* @param statistics Upload statistics for tracking progress
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:<init>(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",574,581,"/**
* Initializes a ByteArrayBlock with specified index, limit, and upload statistics.
* @param index long index
* @param limit maximum byte array size
* @param statistics BlockUploadStatistics object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,hasCapacity,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:hasCapacity(long),869,872,"/**
* Checks if there is sufficient space available.
* @param bytes amount of data to be added
* @return true if capacity is sufficient, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,toString,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:toString(),945,955,"/**
* Returns a human-readable string representation of the FileBlock object.
* @return formatted string with key-value pairs for index, destFile, state, dataSize, and limit
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,innerClose,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:innerClose(),905,933,"/**
* Closes the inner block based on its current state.
* @throws IOException if an I/O error occurs during closing
*/","* The close operation will delete the destination file if it still
     * exists.
     *
     * @throws IOException IO problems",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,checkOpenState,org.apache.hadoop.fs.store.ByteBufferInputStream:checkOpenState(),89,92,"/**
 * Verifies that the stream is open.
 */","* Check the open state.
   * @throws IllegalStateException if the stream is closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,enterState,"org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterState(org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState,org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState)",349,355,"/**
* Transitions to a new destination state.
* @param current the current state
* @param next the target state
*/","* Atomically enter a state, verifying current state.
     *
     * @param current current state. null means ""no check""
     * @param next    next state
     * @throws IllegalStateException if the current state is not as expected",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$DataBlock:write(byte[],int,int)",424,433,"/**
* Writes bytes from buffer into output stream.
* @param buffer byte array containing data to write
* @param offset starting index in buffer
* @param length number of bytes to write
* @return 0 on success, throws IOException otherwise
*/","* Write a series of bytes from the buffer, from the offset.
     * Returns the number of bytes written.
     * Only valid in the state {@code Writing}.
     * Base class verifies the state but does no writing.
     *
     * @param buffer buffer.
     * @param offset offset.
     * @param length length of write.
     * @return number of bytes written.
     * @throws IOException trouble",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,flush,org.apache.hadoop.fs.store.DataBlocks$DataBlock:flush(),442,444,"/**
 * Flushes the writer and verifies its state to ensure it's in the Writing mode.
 */","* Flush the output.
     * Only valid in the state {@code Writing}.
     * In the base class, this is a no-op
     *
     * @throws IOException any IO problem.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,set,"org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:set(java.lang.String,java.lang.String)",246,248,"/**
* Sets an attribute with the given key and value.
* @param key unique identifier for the attribute
* @param value new value for the attribute
*/","* Set an attribute. If the value is non-null/empty,
   * it will be used as a query parameter.
   *
   * @param key key to set
   * @param value value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,extractQueryParameters,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:extractQueryParameters(java.lang.String),331,347,"/**
* Extracts query parameters from a URI header.
* @param header URI header string
* @return Map of parameter names to values or empty map if none found
*/","* Split up the string. Uses httpClient: make sure it is on the classpath.
   * Any query param with a name but no value, e.g ?something is
   * returned in the map with an empty string as the value.
   * @param header URI to parse
   * @return a map of parameters.
   * @throws URISyntaxException failure to build URI from header.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,hasCapacity,org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:hasCapacity(long),602,605,"/**
 * Checks if adding specified data capacity would exceed storage limit.
 * @param bytes data size to check (in bytes)
 * @return true if remaining space is sufficient, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,remainingCapacity,org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:remainingCapacity(),607,610,"/**
* Calculates the remaining capacity based on the total limit and used data size. 
* @return available capacity in bytes (limit - used data size)",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,dataSize,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:dataSize(),718,720,"/**
 * Returns the total data size (if cached) or recalculates it from used buffer capacity.
 */","* Get the amount of data; if there is no buffer then the size is 0.
       *
       * @return the amount of data available to upload.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,hasCapacity,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:hasCapacity(long),733,736,"/**
* Checks if available storage exceeds specified byte count.
* @param bytes target byte threshold
* @return true if capacity meets or exceeds requirement, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,hashCode,org.apache.hadoop.fs.FileSystem$Cache$Key:hashCode(),3891,3894,"/**
* Calculates hash code as a combination of scheme, authority, user ID and unique value.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,hashCode,org.apache.hadoop.security.UserGroupInformation$RealUser:hashCode(),492,495,"/**
* Returns hash code based on the real user's hash code.
* @return Hash code value of the associated real user object.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,hashCode,org.apache.hadoop.ipc.Client$ConnectionId:hashCode(),1843,1859,"/**
* Calculates a unique hash code based on connection settings and protocol.
* @param address network address
* @param doPing whether to ping the server
* @return unique hash value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,equals,org.apache.hadoop.fs.FileSystem$Cache$Key:equals(java.lang.Object),3900,3913,"/**
* Compares this Key instance with another Object for equality.
* @param obj the object to compare with
* @return true if both objects have identical scheme, authority, ugi and unique values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,equals,org.apache.hadoop.security.UserGroupInformation$RealUser:equals(java.lang.Object),481,490,"/**
* Compares this RealUser object with another for equality.
* @param o the Object to compare with
* @return true if both objects are identical, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getInitialWorkingDirectory,org.apache.hadoop.fs.FilterFs:getInitialWorkingDirectory(),78,81,"/**
* Returns initial working directory from file system.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,resolvePath,org.apache.hadoop.fs.FileContext:resolvePath(org.apache.hadoop.fs.Path),616,619,"/**
* Resolves an existing file path.
* @param f existing file path
*/","* Resolve the path following any symlinks or mount points
   * @param f to be resolved
   * @return fully qualified resolved path
   * 
   * @throws FileNotFoundException  If <code>f</code> does not exist
   * @throws AccessControlException if access denied
   * @throws IOException If an IO Error occurred
   * @throws UnresolvedLinkException If unresolved link occurred.
   *
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server
   *
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is not valid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,msync,org.apache.hadoop.fs.FileContext:msync(),1267,1269,"/**
 * Synchronizes file system cache with underlying storage.
 */","* Synchronize client metadata state.
   *
   * @throws IOException If an I/O error occurred.
   * @throws UnsupportedOperationException If file system for <code>f</code> is
   *                                       not supported.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,msync,org.apache.hadoop.fs.FilterFs:msync(),127,130,"/**
 * Synchronizes file system state with underlying storage.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,printStatistics,org.apache.hadoop.fs.FileContext:printStatistics(),2412,2414,"/**
* Prints file system statistics.
*/","* Prints the statistics to standard output. File System is identified by the
   * scheme and authority.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getStatistics,org.apache.hadoop.fs.AbstractFileSystem:getStatistics(java.net.URI),191,204,"/**
* Retrieves statistics for a given URI.
* @param uri unique identifier
*/","* Get the statistics for a particular file system.
   * 
   * @param uri
   *          used as key to lookup STATISTICS_TABLE. Only scheme and authority
   *          part of the uri are used.
   * @return a statistics object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listCorruptFileBlocks,org.apache.hadoop.fs.FilterFs:listCorruptFileBlocks(org.apache.hadoop.fs.Path),209,213,"/**
* Lists corrupt file blocks at specified path.
* @param path file system path to scan
* @return iterator over corrupt file block paths or null if none found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,createMultipartUploader,org.apache.hadoop.fs.AbstractFileSystem:createMultipartUploader(org.apache.hadoop.fs.Path),1634,1639,"/**
* Creates an uploader builder with base path.
* @param basePath directory where data will be uploaded from
*/","* Create a multipart uploader.
   * @param basePath file path under which all files are uploaded
   * @return a MultipartUploaderBuilder object to build the uploader
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,obtainContext,org.apache.hadoop.fs.LocalDirAllocator:obtainContext(java.lang.String),107,117,"/**
* Retrieves or initializes an AllocatorPerContext instance for the specified context configuration item name.
* @param contextCfgItemName unique identifier of the context configuration item
*/","This method must be used to obtain the dir allocation context for a 
   * particular value of the context name. The context name must be an item
   * defined in the Configuration object for which we want to control the 
   * dir allocations (e.g., <code>mapred.local.dir</code>). The method will
   * create a context for that name if it doesn't already exist.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getStatistics,org.apache.hadoop.fs.FilterFs:getStatistics(),68,71,"/**
* Retrieves file system statistics.
* @return Statistics object representing file system metrics
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,getPos,org.apache.hadoop.fs.FSDataOutputStream:getPos(),97,99,"/**
* Retrieves position value from cache.
* @return cached position value as a long integer
*/","* Get the current position in the output stream.
   *
   * @return the current position in the output stream",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,syncFs,org.apache.hadoop.io.SequenceFile$Writer:syncFs(),1382,1387,"/**
* Flushes buffered output to file system.
* @throws IOException if I/O error occurs
*/","* flush all currently written data to the file system.
     * @deprecated Use {@link #hsync()} or {@link #hflush()} instead
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,hflush,org.apache.hadoop.io.SequenceFile$Writer:hflush(),1396,1401,"/**
* Flushes output buffer.
* Writes any pending data to underlying stream.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,hsync,org.apache.hadoop.io.SequenceFile$Writer:hsync(),1389,1394,"/**
* Synchronizes output to device or stream.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,quota,org.apache.hadoop.fs.ContentSummary$Builder:quota(long),90,94,"/**
* Sets the quota value and returns this builder instance.
* @param quota new quota value to be assigned
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,spaceConsumed,org.apache.hadoop.fs.ContentSummary$Builder:spaceConsumed(long),96,100,"/**
* Sets consumed memory and returns builder instance.
* @param spaceConsumed total memory allocated by this object
* @return Builder to enable method chaining
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,spaceQuota,org.apache.hadoop.fs.ContentSummary$Builder:spaceQuota(long),102,106,"/**
* Sets the space quota and returns the builder instance. 
* @param spaceQuota total space allocated to the user in bytes 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,build,org.apache.hadoop.fs.ContentSummary$Builder:build(),132,136,"/**
* Builds and returns a ContentSummary object with populated data.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,<init>,"org.apache.hadoop.util.ApplicationClassLoader:<init>(java.net.URL[],java.lang.ClassLoader,java.util.List)",87,100,"/**
* Initializes the class loader with specified URLs, parent, and system classes.
* @param urls array of URL locations for class files
* @param parent parent ClassLoader instance
* @param systemClasses list of system classes (or null/empty for defaults)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getTrimmedStringCollection,org.apache.hadoop.util.StringUtils:getTrimmedStringCollection(java.lang.String),490,495,"/**
* Returns collection of trimmed strings split from input string.
* @param str input string to process
*/","* Splits a comma separated value <code>String</code>, trimming leading and
   * trailing whitespace on each value. Duplicate and empty values are removed.
   *
   * @param str a comma separated <code>String</code> with values, may be null
   * @return a <code>Collection</code> of <code>String</code> values, empty
   *         Collection if null String input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/LoggingStateChangeListener.java,<init>,org.apache.hadoop.service.LoggingStateChangeListener:<init>(),51,53,"/**
* Constructs LoggingStateChangeListener with default log handler.",* Log events to the static log for this class,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,<init>,org.apache.hadoop.service.ServiceStateException:<init>(java.lang.String),48,50,"/**
* Constructs a ServiceStateException with the given error message.
* @param message detailed error description
*/","* Instantiate
   * @param message error message",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,<init>,"org.apache.hadoop.service.ServiceStateException:<init>(int,java.lang.String,java.lang.Throwable)",78,83,"/**
* Constructs a ServiceStateException with a custom exit code.
* @param exitCode unique error code
* @param message human-readable error description
* @param cause underlying exception (optional)
*/","* Instantiate, using the specified exit code as the exit code
   * of the exception, irrespetive of any exit code supplied by any inner
   * cause.
   *
   * @param exitCode exit code to declare
   * @param message exception message
   * @param cause inner cause",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,convert,"org.apache.hadoop.service.ServiceStateException:convert(java.lang.String,java.lang.Throwable)",120,126,"/**
* Converts or wraps an exception into a RuntimeException.
* @param text error message
* @param fault original exception to be converted/wrapped
*/","* Convert any exception into a {@link RuntimeException}.
   * If the caught exception is already of that type, it is typecast to a
   * {@link RuntimeException} and returned.
   *
   * All other exception types are wrapped in a new instance of
   * {@code ServiceStateException}.
   * @param text text to use if a new exception is created
   * @param fault exception or throwable
   * @return a {@link RuntimeException} to rethrow",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,convert,org.apache.hadoop.service.ServiceStateException:convert(java.lang.Throwable),101,107,"/**
* Converts a Throwable to a RuntimeException or ServiceStateException.
* @param fault the exception to be converted
*/","* Convert any exception into a {@link RuntimeException}.
   * All other exception types are wrapped in a new instance of
   * {@code ServiceStateException}.
   * @param fault exception or throwable
   * @return a {@link RuntimeException} to rethrow",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,<init>,org.apache.hadoop.service.ServiceStateModel:<init>(java.lang.String),60,62,"/**
* Initializes ServiceStateModel with specified name and default state. 
* @param name unique service identifier
*/","* Create the service state model in the {@link Service.STATE#NOTINITED}
   * state.
   *
   * @param name input name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,isInState,org.apache.hadoop.service.AbstractService:isInState(org.apache.hadoop.service.Service$STATE),451,454,"/**
* Checks if service is in specified state.
* @param expected target state to verify
* @return true if service is in the state, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,isValidStateTransition,"org.apache.hadoop.service.ServiceStateModel:isValidStateTransition(org.apache.hadoop.service.Service$STATE,org.apache.hadoop.service.Service$STATE)",149,153,"/**
* Validates state transition from one service state to another.
* @param current current service state
* @param proposed proposed service state
* @return true if transition is valid, false otherwise
*/","* Is a state transition valid?
   * There are no checks for current==proposed
   * as that is considered a non-transition.
   *
   * using an array kills off all branch misprediction costs, at the expense
   * of cache line misses.
   *
   * @param current current state
   * @param proposed proposed new state
   * @return true if the transition to a new state is valid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,toString,org.apache.hadoop.service.ServiceStateModel:toString(),159,163,"/**
* Returns a string representation of this location,
* including name and state.
*/","* return the state text as the toString() value
   * @return the current state's description",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/HadoopUncaughtExceptionHandler.java,<init>,org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:<init>(),71,73,"/**
* Initializes HadoopUncaughtExceptionHandler with optional initial configuration.","* Basic exception handler -logs simple exceptions, then continues.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/IrqHandler.java,handle,org.apache.hadoop.service.launcher.IrqHandler:handle(sun.misc.Signal),125,131,"/**
* Handles incoming signal and notifies the interrupt handler.
* @param s Signal object containing ID and number information
*/","* Handler for the JVM API for signal handling.
   * @param s signal raised",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,getService,org.apache.hadoop.service.launcher.InterruptEscalator:getService(),84,87,"/**
* Retrieves the service instance from the owning launcher.
* @return Service object or null if no owner is found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,<init>,"org.apache.hadoop.service.launcher.InterruptEscalator$ServiceForcedShutdown:<init>(org.apache.hadoop.service.Service,int)",188,191,"/**
* Initializes forced shutdown of a service with specified timeout.
* @param service service to be shut down
* @param shutdownTimeMillis time in milliseconds before shutdown occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,lookup,org.apache.hadoop.service.launcher.InterruptEscalator:lookup(java.lang.String),153,160,"/**
* Retrieves an IRQ handler by its name.
* @param signalName the name of the IRQ handler to look up
* @return IrqHandler object or null if not found
*/","* Look up the handler for a signal.
   * @param signalName signal name
   * @return a handler if found",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.Throwable)",49,51,"/**
 * Creates a ServiceLaunchException with specified exit code and underlying cause.
 * @param exitCode service launch exit code
 * @param cause underlying exception that caused the failure
 */","* Create an exception with the specific exit code.
   * @param exitCode exit code
   * @param cause cause of the exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.String)",58,60,"/**
* Constructs a ServiceLaunchException with a custom exit code and error message.
* @param exitCode non-zero exit code indicating service launch failure
* @param message detailed description of the launch error
*/","* Create an exception with the specific exit code and text.
   * @param exitCode exit code
   * @param message message to use in exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.String,java.lang.Object[])",74,79,"/**
* Constructs a ServiceLaunchException with an exit code and formatted message.
* @param exitCode the process termination status
* @param format the error message format string
* @param args variable arguments for String.format()
*/","* Create a formatted exception.
   * <p>
   * This uses {@link String#format(String, Object...)}
   * to build the formatted exception in the ENGLISH locale.
   * <p>
   * If the last argument is a throwable, it becomes the cause of the exception.
   * It will also be used as a parameter for the format.
   * @param exitCode exit code
   * @param format format for message to use in exception
   * @param args list of arguments",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.String)",1083,1086,"/**
* Constructs a KerberosDiagsFailure object with specified category and message.
* @param category failure category
* @param message detailed error description
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.Throwable,java.lang.String,java.lang.Object[])",91,94,"/**
* Constructs ServiceLaunchException with custom error message and optional arguments.
* @param exitCode exit status code
* @param cause underlying exception
* @param format custom error message format string
* @param args variable number of arguments for the formatted message
*/","* Create a formatted exception.
   * <p>
   * This uses {@link String#format(String, Object...)}
   * to build the formatted exception in the ENGLISH locale.
   * @param exitCode exit code
   * @param cause inner cause
   * @param format format for message to use in exception
   * @param args list of arguments",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,run,org.apache.hadoop.service.launcher.ServiceShutdownHook:run(),82,85,"/**
 * Shuts down this thread by calling the shutdown() method. 
 */","* Shutdown handler.
   * Query the service hook reference -if it is still valid the 
   * {@link Service#stop()} operation is invoked.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,<init>,org.apache.hadoop.service.launcher.ServiceLauncher:<init>(java.lang.String),184,186,"/**
 * Constructs a ServiceLauncher instance with the given service class name.
 * @param serviceClassName name of the service class to be launched
 */","* Create an instance of the launcher.
   * @param serviceClassName classname of the service",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,toString,org.apache.hadoop.service.launcher.ServiceLauncher:toString(),253,264,"/**
* Returns a human-readable string representation of the ServiceLauncher instance.
* @return String containing service name, class name and/or service object details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,noteException,org.apache.hadoop.service.launcher.ServiceLauncher:noteException(org.apache.hadoop.util.ExitUtil$ExitException),331,345,"/**
* Logs and records an ExitException with its exit code.
* @param exitException exception containing exit code and optional nested cause
*/","* Record that an Exit Exception has been raised.
   * Save it to {@link #serviceException}, with its exit code in
   * {@link #serviceExitCode}
   * @param exitException exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,bindCommandOptions,org.apache.hadoop.service.launcher.ServiceLauncher:bindCommandOptions(),321,323,"/**
* Initializes command options using the createOptions() factory method.
* @see #createOptions()
*/","* Set the {@link #commandOptions} field to the result of
   * {@link #createOptions()}; protected for subclasses and test access.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,loadConfigurationClasses,org.apache.hadoop.service.launcher.ServiceLauncher:loadConfigurationClasses(),422,449,"/**
* Loads Configuration classes and returns the count of successfully created instances.
* @return number of loaded Configuration instances
*/","* @return This creates all the configurations defined by
   * {@link #getConfigurationsToCreate()} , ensuring that
   * the resources have been pushed in.
   * If one cannot be loaded it is logged and the operation continues
   * except in the case that the class does load but it isn't actually
   * a subclass of {@link Configuration}.
   * @throws ExitUtil.ExitException if a loaded class is of the wrong type",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,registerServiceListener,org.apache.hadoop.service.AbstractService:registerServiceListener(org.apache.hadoop.service.ServiceStateChangeListener),354,357,"/**
* Registers a service state change listener.
* @param l ServiceStateChangeListener to add to the list of registered listeners
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,registerGlobalListener,org.apache.hadoop.service.AbstractService:registerGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener),369,371,"/**
 * Registers a global ServiceStateChangeListener.
 * @param l listener to be added to the global listeners list
 */","* Register a global listener, which receives notifications
   * from the state change events of all services in the JVM
   * @param l listener",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,unregisterServiceListener,org.apache.hadoop.service.AbstractService:unregisterServiceListener(org.apache.hadoop.service.ServiceStateChangeListener),359,362,"/**
 * Removes a registered service state change listener. 
 * @param l ServiceStateChangeListener instance to be removed from the list of active listeners. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,unregisterGlobalListener,org.apache.hadoop.service.AbstractService:unregisterGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener),378,380,"/**
* Removes and unregisters a global Service State Change Listener.
* @param l listener to be removed
*/","* unregister a global listener.
   * @param l listener to unregister
   * @return true if the listener was found (and then deleted)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,resetGlobalListeners,org.apache.hadoop.service.AbstractService:resetGlobalListeners(),385,388,"/**
 * Resets global listeners to their default state.
 */",* Package-scoped method for testing -resets the global listener list,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,notifyListeners,org.apache.hadoop.service.AbstractService:notifyListeners(),409,416,"/**
* Notifies all registered listeners and a global listener list.
*/","* Notify local and global listeners of state changes.
   * Exceptions raised by listeners are NOT passed up.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,getServiceState,org.apache.hadoop.service.AbstractService:getServiceState(),118,121,"/**
* Retrieves the current service state from the state model.
* @return The current STATE value.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,serviceInit,org.apache.hadoop.service.AbstractService:serviceInit(org.apache.hadoop.conf.Configuration),312,317,"/**
* Initializes service with new configuration if provided.
* @param conf new Configuration object
*/","* All initialization code needed by a service.
   *
   * This method will only ever be called once during the lifecycle of
   * a specific service instance.
   *
   * Implementations do not need to be synchronized as the logic
   * in {@link #init(Configuration)} prevents re-entrancy.
   *
   * The base implementation checks to see if the subclass has created
   * a new configuration instance, and if so, updates the base class value
   * @param conf configuration
   * @throws Exception on a failure -these will be caught,
   * possibly wrapped, and will trigger a service stop",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,serviceStop,org.apache.hadoop.util.JvmPauseMonitor:serviceStop(),88,100,"/**
* Stops service by interrupting and joining the monitor thread.
* @throws Exception if an error occurs during shutdown
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,serviceStart,org.apache.hadoop.service.CompositeService:serviceStart(),115,126,"/**
* Starts all registered services.
* @throws Exception if a service fails to start
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,addIfService,org.apache.hadoop.service.CompositeService:addIfService(java.lang.Object),88,95,"/**
* Adds a service to the collection if the provided object is an instance of Service.
* @param object Object to check and potentially add as a service
*/","* If the passed object is an instance of {@link Service},
   * add it to the list of services managed by this {@link CompositeService}
   * @param object object.
   * @return true if a service is added, false otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceOperations.java,stopQuietly,"org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.commons.logging.Log,org.apache.hadoop.service.Service)",79,88,"/**
* Stops a service quietly, logging warnings if an exception occurs.
* @param log logging instance
* @param service Service to stop
* @return Exception object or null if successful","* Stop a service; if it is null do nothing. Exceptions are caught and
   * logged at warn level. (but not Throwables). This operation is intended to
   * be used in cleanup operations
   *
   * @param log the log to warn at
   * @param service a service; may be null
   * @return any exception that was caught; null if none was.
   * @deprecated to be removed with 3.4.0. Use {@link #stopQuietly(Logger, Service)} instead.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceOperations.java,stopQuietly,"org.apache.hadoop.service.ServiceOperations:stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)",100,108,"/**
* Attempts to stop a service and logs any exceptions silently.
* @param log logger instance
* @param service Service object to be stopped
* @return Exception if service cannot be stopped, or null on success
*/","* Stop a service; if it is null do nothing. Exceptions are caught and
   * logged at warn level. (but not Throwables). This operation is intended to
   * be used in cleanup operations
   *
   * @param log the log to warn at
   * @param service a service; may be null
   * @return any exception that was caught; null if none was.
   * @see ServiceOperations#stopQuietly(Service)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$ProgressableOption:<init>(org.apache.hadoop.util.Progressable),969,971,"/**
 * Constructs a ProgressableOption with the given progressable value.
 * @param value the progressable option to initialize from
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ShortWritable.java,<init>,org.apache.hadoop.io.ShortWritable:<init>(short),37,39,"/**
 * Initializes a new ShortWritable instance with the given short value.
 * @param value the initial short value to be stored in this writable object.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$LengthOption:<init>(long),1874,1876,"/**
* Constructs a new LengthOption instance with the specified value.
* @param value length value to initialize the option with",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$StartOption:<init>(long),1867,1869,"/**
 * Constructs a StartOption with the specified long value.
 * @param value the start option value to be initialized.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$BlockSizeOption:<init>(long),925,927,"/**
* Constructs a new BlockSizeOption with the specified value.
* @param value long value representing block size option
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,read,org.apache.hadoop.io.DataInputByteBuffer$Buffer:read(),31,37,"/**
* Reads and returns a single byte from the underlying input source.
* @return The read byte value or -1 if end of data is reached
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,reset,org.apache.hadoop.io.DataInputByteBuffer:reset(java.nio.ByteBuffer[]),83,85,"/**
 * Resets one or more ByteBuffer instances. 
 * @param input zero or more ByteBuffer instances to reset
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,byteBufferPositionedReadable_readFullyAvailable,org.apache.hadoop.io.wrappedio.WrappedIO:byteBufferPositionedReadable_readFullyAvailable(java.io.InputStream),233,246,"/**
* Checks if an InputStream supports reading byte buffer fully available.
* @param in input stream to check for capability
* @return true if capable, false otherwise
*/","* Probe to see if the input stream is an instance of ByteBufferPositionedReadable.
   * If the stream is an FSDataInputStream, the wrapped stream is checked.
   * @param in input stream
   * @return true if the stream implements the interface (including a wrapped stream)
   * and that it declares the stream capability.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,isAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:isAvailable(),433,435,"/**
* Checks if the singleton instance has been loaded.
* @return true if loaded, false otherwise
*/","* Is the wrapped IO class loaded?
   * @return true if the instance is loaded.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,toString,org.apache.hadoop.util.JsonSerialization:toString(java.lang.Object),352,359,"/**
* Converts an object of type T to a JSON-formatted string.
* @param instance the object to be converted
*/","* Convert an instance to a string form for output. This is a robust
   * operation which will convert any JSON-generating exceptions into
   * error text.
   * @param instance non-null instance
   * @return a JSON string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FunctionalIO.java,toUncheckedFunction,org.apache.hadoop.util.functional.FunctionalIO:toUncheckedFunction(org.apache.hadoop.util.functional.FunctionRaisingIOE),84,86,"/**
* Converts a checked function that raises IOException to an unchecked function. 
* @param fun the original function to convert
*/","* Convert a {@link FunctionRaisingIOE} as a {@link Supplier}.
   * @param fun function to wrap
   * @param <T> type of input
   * @param <R> type of return value.
   * @return a new function which invokes the inner function and wraps
   * exceptions.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,fromInstance,org.apache.hadoop.util.JsonSerialization:fromInstance(java.lang.Object),236,238,"/**
* Converts an instance to JSON and then back to the original type.
* @param instance object to convert
* @return converted object or null if conversion failed
*/","* clone by converting to JSON and back again.
   * This is much less efficient than any Java clone process.
   * @param instance instance to duplicate
   * @return a new instance
   * @throws IOException IO problems.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,fromBytes,org.apache.hadoop.util.JsonSerialization:fromBytes(byte[]),331,333,"/**
* Converts byte array to JSON object and returns deserialized instance of type T.
* @param bytes serialized data in byte format
*/","* Deserialize from a byte array.
   * @param bytes byte array
   * @throws IOException IO problems
   * @throws EOFException not enough data
   * @return byte array.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VIntWritable.java,<init>,org.apache.hadoop.io.VIntWritable:<init>(int),38,38,"/**
* Initializes VIntWritable with integer value.
* @param value int value to be stored in VIntWritable object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,equals,org.apache.hadoop.io.ElasticByteBufferPool$Key:equals(java.lang.Object),57,68,"/**
* Checks if this object is equal to the given Object.
* @param rhs the Object to compare with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$SyncIntervalOption:<init>(int),1009,1013,"/**
* Initializes SyncIntervalOption with a given value, falling back to default if negative.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$ReplicationOption:<init>(int),932,934,"/**
* Initializes ReplicationOption with given integer value.
* @param value replication option identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$BufferSizeOption:<init>(int),1881,1883,"/**
* Constructs a BufferSizeOption instance with the specified value.
* @param value numeric option value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$BufferSizeOption:<init>(int),919,921,"/**
* Constructs BufferSizeOption with specified size.
* @param value buffer size in bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,<init>,org.apache.hadoop.io.UTF8:<init>(org.apache.hadoop.io.UTF8),78,80,"/**
* Copies properties from another UTF8 instance.
* @param utf8 source UTF8 object to copy from
*/","* Construct from a given string.
   * @param utf8 input utf8.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,writeString,"org.apache.hadoop.io.UTF8:writeString(java.io.DataOutput,java.lang.String)",351,365,"/**
* Writes a string to the output stream with UTF-8 encoding.
* @param out DataOutput stream to write to
* @param s String to be written (truncated if too long)
* @return length of written string or throws IOException if invalid
*/","* @return Write a UTF-8 encoded string.
   *
   * @see DataOutput#writeUTF(String)
   * @param out input out.
   * @param s input s.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,skip,org.apache.hadoop.io.UTF8:skip(java.io.DataInput),144,147,"/**
* Skips over specified number of bytes in input stream.
* @param in input data to be skipped
*/","* Skips over one UTF8 in the input.
   * @param in datainput.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,skipCompressedByteArray,org.apache.hadoop.io.WritableUtils:skipCompressedByteArray(java.io.DataInput),54,59,"/**
* Skips compressed byte array in input stream.
* @param in DataInput stream containing compressed data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compare,"org.apache.hadoop.io.SequenceFile$Sorter$SortPass$SeqFileComparator:compare(org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable)",3258,3263,"/**
* Compares two IntWritables using a custom comparator.
* @param I first integer value
* @param J second integer value
* @return negative/positive zero if values are equal/inconsistent order
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,org.apache.hadoop.io.SetFile:<init>(),35,35,"/**
* Protected constructor to prevent instantiation from outside.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,org.apache.hadoop.io.ArrayFile:<init>(),35,35,"/**
* Private constructor to prevent instantiation of utility class.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/LongWritable.java,<init>,org.apache.hadoop.io.LongWritable:<init>(long),37,37,"/**
* Creates a new LongWritable instance with the specified long value.
* @param value the long value to be written (not read)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,seek,org.apache.hadoop.io.ArrayFile$Reader:seek(long),112,115,"/**
* Seeks to a specific file position and updates the internal key.
* @param n target file position
*/","* Positions the reader before its <code>n</code>th value.
     *
     * @param n n key.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,get,"org.apache.hadoop.io.ArrayFile$Reader:get(long,org.apache.hadoop.io.Writable)",147,151,"/**
* Retrieves and returns a writable object associated with the given key.
* @param n unique identifier
* @param value default writable object to return if not found
* @return Writable object or null if not found
*/","* Return the <code>n</code>th value in the file.
     * @param n n key.
     * @param value value.
     * @throws IOException raised on errors performing I/O.
     * @return writable.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$ValueClassOption:<init>(java.lang.Class),952,954,"/**
* Constructs a new ValueClassOption instance with the specified value class.
* @param value the value class of this option
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,org.apache.hadoop.io.MapFile$Writer$KeyClassOption:<init>(java.lang.Class),270,272,"/**
 * Constructs a new instance of KeyClassOption with the given class.
 * @param value Class object to be used as option's value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$KeyClassOption:<init>(java.lang.Class),945,947,"/**
* Constructs a KeyClassOption instance with the specified class type.
* @param value the class type to be used in this option
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ByteWritable.java,<init>,org.apache.hadoop.io.ByteWritable:<init>(byte),34,34,"/**
 * Initializes a new instance of ByteWritable with the specified byte value.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,<init>,org.apache.hadoop.io.OutputBuffer:<init>(),71,73,"/**
* Initializes an OutputBuffer instance with a default Buffer. 
* @param buffer underlying Buffer instance (default is created internally) 
*/",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,getData,org.apache.hadoop.io.OutputBuffer:getData(),86,86,"/**
* Returns raw data from internal buffer.
* @return binary data as byte array
*/","* Returns the current contents of the buffer.
   *  Data is only valid to {@link #getLength()}.
   *
   * @return the current contents of the buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,getLength,org.apache.hadoop.io.OutputBuffer:getLength(),93,93,"/**
* Returns the length of the underlying data buffer.
*/","* Returns the length of the valid data currently in the buffer.
   * @return the length of the valid data
   *          currently in the buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,reset,org.apache.hadoop.io.OutputBuffer:reset(),96,99,"/**
* Resets the output buffer to its initial state.
* @return The OutputBuffer instance itself for method chaining.",@return Resets the buffer to empty.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,compare,"org.apache.hadoop.io.WritableComparator:compare(java.lang.Object,java.lang.Object)",215,218,"/**
* Compares two objects using their Comparable interface. 
* @param a first object to compare
* @param b second object to compare
*/","* Compare two Object.
   *
   * @param a the first object to be compared.
   * @param b the second object to be compared.
   * @return compare result.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,binarySearch,org.apache.hadoop.io.MapFile$Reader:binarySearch(org.apache.hadoop.io.WritableComparable),782,799,"/**
* Performs binary search on sorted keys to find the index of a given key.
* @param key WritableComparable object to search for
* @return index of the key if found, negative offset otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,compareBytes,"org.apache.hadoop.io.WritableComparator:compareBytes(byte[],int,int,byte[],int,int)",230,233,"/**
* Compares two byte arrays with specified offsets and lengths.
* @param b1 first byte array
* @param s1 offset in first byte array
* @param l1 length of first byte array segment
* @param b2 second byte array
* @param s2 offset in second byte array
* @param l2 length of second byte array segment
* @return result of comparison (negative if less, positive if greater)
*/","* Lexicographic order of binary data.
   * @param b1 b1.
   * @param s1 s1.
   * @param l1 l1.
   * @param b2 b2.
   * @param s2 s2.
   * @param l2 l2.
   * @return compare bytes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,hashBytes,"org.apache.hadoop.io.WritableComparator:hashBytes(byte[],int)",255,257,"/**
* Calculates hash value for byte array slice.
* @param bytes input byte array
* @param length number of bytes to consider
* @return hash code as an integer
*/","* Compute hash for binary data.
   * @param bytes bytes.
   * @param length length.
   * @return hash for binary data.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readFloat,"org.apache.hadoop.io.WritableComparator:readFloat(byte[],int)",290,292,"/**
* Converts an integer from byte array to float value.
* @param bytes input byte array
* @param start starting index of the integer in the byte array
* @return float representation of the integer or NaN if invalid
*/","* Parse a float from a byte array.
   * @param bytes bytes.
   * @param start start.
   * @return float from a byte array",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readLong,"org.apache.hadoop.io.WritableComparator:readLong(byte[],int)",300,303,"/**
* Reads a 64-bit integer from the specified byte array, 
* starting at the given offset. The high and low integers are read separately.
* @param bytes byte array containing the data
* @param start offset to begin reading from
* @return the 64-bit integer value
*/","* Parse a long from a byte array.
   * @param bytes bytes.
   * @param start start.
   * @return long from a byte array",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readVInt,"org.apache.hadoop.io.WritableComparator:readVInt(byte[],int)",347,349,"/**
* Reads a signed integer value from the given byte array starting at the specified offset.
* @param bytes the byte array to read from
* @param start the offset within the byte array where the value starts
* @return the read integer value
*/","* Reads a zero-compressed encoded integer from a byte array and returns it.
   * @param bytes byte array with the encoded integer
   * @param start start index
   * @throws IOException raised on errors performing I/O.
   * @return deserialized integer",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,<init>,org.apache.hadoop.io.BytesWritable:<init>(byte[]),61,63,"/**
* Constructs BytesWritable from a byte array.
* @param bytes input byte array
* @param length ignored; always uses entire input array
*/","* Create a BytesWritable using the byte array as the initial value.
   * @param bytes This array becomes the backing storage for the object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,get,org.apache.hadoop.io.BytesWritable:get(),102,105,"/**
* Returns bytes representation of data.","* Get the data from the BytesWritable.
   * @deprecated Use {@link #getBytes()} instead.
   * @return data from the BytesWritable.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,getSize,org.apache.hadoop.io.BytesWritable:getSize(),120,123,"/**
* Returns the length of the underlying data structure.
* @deprecated Use getLength() directly instead. 
* @return Length of the data structure
*/","* Get the current size of the buffer.
   * @deprecated Use {@link #getLength()} instead.
   * @return current size of the buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,setCapacity,org.apache.hadoop.io.BytesWritable:setCapacity(int),155,160,"/**
* Updates container capacity and resizes internal storage.
* @param capacity new maximum size in bytes
*/","* Change the capacity of the backing storage. The data is preserved.
   *
   * @param capacity The new capacity in bytes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IntWritable.java,<init>,org.apache.hadoop.io.IntWritable:<init>(int),37,37,"/**
* Constructs an IntWritable with the given integer value.
* @param value the integer to be wrapped in this object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,getBuffer,"org.apache.hadoop.io.ElasticByteBufferPool:getBuffer(boolean,int)",89,101,"/**
* Retrieves a buffer of specified length and directness from the cache.
* @param direct true for direct buffer, false for non-direct
* @param length required buffer size
* @return ByteBuffer instance or new one if not found in cache
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,putBuffer,org.apache.hadoop.io.ElasticByteBufferPool:putBuffer(java.nio.ByteBuffer),103,119,"/**
* Inserts a ByteBuffer into a synchronized cache tree.
* @param buffer input buffer to store in cache
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,size,org.apache.hadoop.io.ElasticByteBufferPool:size(boolean),127,131,"/**
* Returns the size of the buffer tree.
* @param direct whether to access directly or not
*/","* Get the size of the buffer pool, for the specified buffer type.
   *
   * @param direct Whether the size is returned for direct buffers
   * @return The size",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable$Internal:<init>(),159,161,"/**
 * Initializes internal storage.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,updateProgress,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:updateProgress(long),3611,3616,"/**
* Updates progress by adding processed bytes and merging with progress per byte factor.
* @param bytesProcessed number of bytes processed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ReadaheadPool.java,readaheadStream,"org.apache.hadoop.io.ReadaheadPool:readaheadStream(java.lang.String,java.io.FileDescriptor,long,long,long,org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest)",102,149,"/**
* Triggers readahead streams with dynamic positioning and cancellation.
* @param identifier unique stream identifier
* @param fd file descriptor
* @param curPos current position in the stream
* @param readaheadLength requested readahead length
* @param maxOffsetToRead maximum allowed offset
* @param lastReadahead previous readahead request (null if none)
* @return new ReadaheadRequest or null if we've reached the end of the stream
*/","* Issue a request to readahead on the given file descriptor.
   * 
   * @param identifier a textual identifier that will be used in error
   * messages (e.g. the file name)
   * @param fd the file descriptor to read ahead
   * @param curPos the current offset at which reads are being issued
   * @param readaheadLength the configured length to read ahead
   * @param maxOffsetToRead the maximum offset that will be readahead
   *        (useful if, for example, only some segment of the file is
   *        requested by the user). Pass {@link Long#MAX_VALUE} to allow
   *        readahead to the end of the file.
   * @param lastReadahead the result returned by the previous invocation
   *        of this function on this file descriptor, or null if this is
   *        the first call
   * @return an object representing this outstanding request, or null
   *        if no readahead was performed",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,append,org.apache.hadoop.io.SetFile$Writer:append(org.apache.hadoop.io.WritableComparable),97,99,"/**
* Appends a key-value pair to this output stream.
* @param key WritableComparable object to be appended
*/","* Append a key to a set.  The key must be strictly greater than the
     * previous key added to the set.
     * @param key input key.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,next,org.apache.hadoop.io.SetFile$Reader:next(org.apache.hadoop.io.WritableComparable),144,147,"/**
* Advances to the next record in the input and returns true if successful.
* @param key WritableComparable object representing the current record
*/","* Read the next key in a set into <code>key</code>.
     *
     * @param key input key.
     * @return Returns true if such a key exists
     *    and false when at the end of the set.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,compare,"org.apache.hadoop.io.Text$Comparator:compare(byte[],int,int,byte[],int,int)",433,439,"/**
* Compares two byte arrays after decoding variable-length integers.
* @param b1 first byte array
* @param s1 starting index in b1 for comparison
* @param l1 length of bytes to compare in b1
* @param b2 second byte array
* @param s2 starting index in b2 for comparison
* @param l2 length of bytes to compare in b2
* @return integer result of comparison
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,key,org.apache.hadoop.io.ArrayFile$Reader:key(),136,138,"/**
* Retrieves the current key value.
* @throws IOException if an error occurs accessing the key
*/","* Returns the key associated with the most recent call to {@link
     * #seek(long)}, {@link #next(Writable)}, or {@link
     * #get(long,Writable)}.
     *
     * @return key key.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$InputStreamOption:<init>(org.apache.hadoop.fs.FSDataInputStream),1860,1862,"/**
* Constructs an InputStreamOption instance from a FSDataInputStream.
* @param value underlying FSDataInputStream object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,comparator,org.apache.hadoop.io.MapFile$Writer:comparator(org.apache.hadoop.io.WritableComparator),289,291,"/**
* Creates an option from a WritableComparator.
* @param value WritableComparator object to convert
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DoubleWritable.java,<init>,org.apache.hadoop.io.DoubleWritable:<init>(double),41,43,"/**
* Initializes a new instance with the given double value.
* @param value the initial double value to be written
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VersionedWritable.java,readFields,org.apache.hadoop.io.VersionedWritable:readFields(java.io.DataInput),49,54,"/**
* Reads serialized data from a DataInput stream and validates the version.
* @throws IOException if reading fails or a version mismatch is detected
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,comparator,org.apache.hadoop.io.MapFile$Reader:comparator(org.apache.hadoop.io.WritableComparator),476,478,"/**
* Creates an option from a writable comparator.
* @param value WritableComparator instance to convert
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,access,"org.apache.hadoop.io.nativeio.NativeIO$Windows:access(java.lang.String,org.apache.hadoop.io.nativeio.NativeIO$Windows$AccessRight)",815,818,"/**
* Checks file/directory accessibility by desired right.
* @param path absolute file/directory path
* @param desiredAccess requested access level (e.g. READ, WRITE, EXECUTE)","* Checks whether the current process has desired access rights on
     * the given path.
     * 
     * Longer term this native function can be substituted with JDK7
     * function Files#isReadable, isWritable, isExecutable.
     *
     * @param path input path
     * @param desiredAccess ACCESS_READ, ACCESS_WRITE or ACCESS_EXECUTE
     * @return true if access is allowed
     * @throws IOException I/O exception on error",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,isAvailable,org.apache.hadoop.io.nativeio.NativeIO$POSIX:isAvailable(),360,362,"/**
* Checks whether native code has been loaded and is available.
* @return true if native code is loaded and available, false otherwise
*/",* @return Return true if the JNI-based native IO extensions are available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,isAvailable,org.apache.hadoop.io.nativeio.NativeIO:isAvailable(),869,871,"/**
* Checks whether native code has been loaded and is available.
* @return true if native code is loaded and ready to use, false otherwise
*/",* @return Return true if the JNI-based native IO extensions are available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.java,<init>,org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:<init>(),38,48,"/**
* Initializes Unix groups mapping with fallback strategy.
* @param none
* @return none
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.java,<init>,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:<init>(),37,47,"/**
* Initializes group mapping implementation with fallback to shell-based.
* @param none
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/NativeCrc32.java,isAvailable,org.apache.hadoop.util.NativeCrc32:isAvailable(),35,41,"/**
* Checks if native code is available to load, excluding Sparc architecture.
* @return true if native code can be loaded, false otherwise
*/",* Return true if the JNI-based native CRC extensions are available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,setPmdkSupportState,org.apache.hadoop.io.nativeio.NativeIO$POSIX:setPmdkSupportState(int),173,181,"/**
* Sets PMDK support state based on the provided state code.
* @param stateCode unique identifier for the support state
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getPmdkSupportStateMessage,org.apache.hadoop.io.nativeio.NativeIO$POSIX:getPmdkSupportStateMessage(),183,189,"/**
* Returns the PMDK support state message with optional lib path.
* @return error message including lib path if available
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,isPmdkAvailable,org.apache.hadoop.io.nativeio.NativeIO$POSIX:isPmdkAvailable(),191,194,"/**
* Checks if PMDK support is available.
* @return true if PMDK is supported, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,chmod,"org.apache.hadoop.io.nativeio.NativeIO$POSIX:chmod(java.lang.String,int)",387,404,"/**
* Sets file permissions on the specified path.
* @param path file system path
* @param mode permission mode (integer value)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,posixFadviseIfPossible,"org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:posixFadviseIfPossible(java.lang.String,java.io.FileDescriptor,long,long,int)",293,298,"/**
* Advises the operating system about file access patterns.
* @param identifier native identifier
* @param fd FileDescriptor for file access
* @param offset starting position in bytes
* @param len length of data to advise on
* @param flags POSIX FADVISE flags (e.g. FADVise_WILLNEED)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,munmap,org.apache.hadoop.io.nativeio.NativeIO$POSIX:munmap(java.nio.MappedByteBuffer),491,501,"/**
* Unmaps a mapped buffer, if supported.
* @param buffer MappedByteBuffer object
*/","* Unmaps the block from memory. See munmap(2).
     *
     * There isn't any portable way to unmap a memory region in Java.
     * So we use the sun.nio method here.
     * Note that unmapping a memory region could cause crashes if code
     * continues to reference the unmapped code.  However, if we don't
     * manually unmap the memory, we are dependent on the finalizer to
     * do it, and we have no idea when the finalizer will run.
     *
     * @param buffer    The buffer to unmap.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,freeDB,org.apache.hadoop.crypto.CryptoStreamUtils:freeDB(java.nio.ByteBuffer),47,57,"/**
* Frees a database ByteBuffer, unmapping it if supported.
* @param buffer ByteBuffer object to be freed
*/","* Forcibly free the direct buffer.
   *
   * @param buffer buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getName,"org.apache.hadoop.io.nativeio.NativeIO$POSIX:getName(org.apache.hadoop.io.nativeio.NativeIO$POSIX$IdCache,int)",629,648,"/**
* Retrieves user or group name by ID from cache or native implementation.
* @param domain IdCache type (USER or GROUP)
* @param id unique identifier
* @return user/group name as a string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getOperatingSystemPageSize,org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:getOperatingSystemPageSize(),289,291,"/**
* Retrieves the operating system's page size. 
* @return The page size in bytes.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,memSync,org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:memSync(org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion),252,258,"/**
* Synchronizes memory access for the given PmemMappedRegion.
* @param region mapped memory region to synchronize
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayWritable.java,<init>,"org.apache.hadoop.io.ArrayWritable:<init>(java.lang.Class,org.apache.hadoop.io.Writable[])",57,60,"/**
* Constructs an ArrayWritable object with specified class and array of values.
* @param valueClass Class of writable objects in the array
* @param values Array of Writable objects to be stored in the ArrayWritable",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BoundedByteArrayOutputStream.java,<init>,"org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(byte[],int,int)",59,61,"/**
* Initializes buffer with specified byte array and bounds.
* @param buf byte array to initialize
* @param offset initial offset into the array
* @param limit maximum number of bytes allowed in the buffer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,write,"org.apache.hadoop.io.DataOutputBuffer:write(java.io.DataInput,int)",132,134,"/**
 * Writes data from input stream to internal buffer.
 * @param in DataInput stream containing data
 * @param length number of bytes to read from stream
 */","* Writes bytes from a DataInput directly into the buffer.
   * @param in data input.
   * @param length length.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/TwoDArrayWritable.java,<init>,"org.apache.hadoop.io.TwoDArrayWritable:<init>(java.lang.Class,org.apache.hadoop.io.Writable[][])",38,41,"/**
* Initializes a writable two-dimensional array with given class and values.
* @param valueClass data type of array elements
* @param values two-dimensional array of Writable objects",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,<init>,"org.apache.hadoop.io.EnumSetWritable:<init>(java.util.EnumSet,java.lang.Class)",70,72,"/**
 * Creates an EnumSetWritable instance from a given EnumSet and element type.
 * @param value the EnumSet to wrap
 * @param elementType the class of elements in the EnumSet
 */","* Construct a new EnumSetWritable. If the <tt>value</tt> argument is null or
   * its size is zero, the <tt>elementType</tt> argument must not be null. If
   * the argument <tt>value</tt>'s size is bigger than zero, the argument
   * <tt>elementType</tt> is not be used.
   * 
   * @param value enumSet value.
   * @param elementType elementType.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readStringArray,org.apache.hadoop.io.WritableUtils:readStringArray(java.io.DataInput),165,173,"/**
* Reads an array of strings from the given DataInput stream.
* @param in input stream containing string array length and elements
* @return null if length is -1, otherwise a String[] with len elements
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeStringArray,"org.apache.hadoop.io.WritableUtils:writeStringArray(java.io.DataOutput,java.lang.String[])",136,141,"/**
* Writes an array of strings to the output stream.
* @param out data output stream
* @param s string array to be written
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,equals,org.apache.hadoop.io.SortedMapWritable:equals(java.lang.Object),200,216,"/**
* Compares this sorted map with another object for equality.
* @param obj the object to compare with
* @return true if objects are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,write,"org.apache.hadoop.io.OutputBuffer$Buffer:write(java.io.InputStream,int)",56,65,"/**
* Resizes and writes data from InputStream to buffer.
* @param in input stream
* @param len number of bytes to read
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,fillReservoir,org.apache.hadoop.crypto.random.OsSecureRandom:fillReservoir(int),61,73,"/**
* Fills the reservoir buffer with random data from a file.
* @param min minimum number of bytes to fill
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,<init>,org.apache.hadoop.io.DataInputBuffer:<init>(),134,136,"/**
 * Constructs a new DataInputBuffer instance using a default Buffer.
 */",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,reset,"org.apache.hadoop.io.DataInputBuffer:reset(byte[],int)",149,151,"/**
 * Resets the internal buffer with new data.
 * @param input byte array containing new data
 * @param length number of bytes to read from input array
 */","* Resets the data that the buffer reads.
   *
   * @param input input.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,reset,"org.apache.hadoop.io.DataInputBuffer:reset(byte[],int,int)",160,162,"/**
* Resets the buffer with new data from the specified range of the input array.
* @param input byte array to read from
* @param start starting index in the array (inclusive)
* @param length number of bytes to reset the buffer with
*/","* Resets the data that the buffer reads.
   *
   * @param input input.
   * @param start start.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,getData,org.apache.hadoop.io.DataInputBuffer:getData(),164,166,"/**
* Retrieves data from internal buffer.
* @return raw data as a byte array
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,getPosition,org.apache.hadoop.io.DataInputBuffer:getPosition(),173,173,"/**
* Retrieves current position from underlying buffer. 
* @return current position in buffer as an integer","* Returns the current position in the input.
   *
   * @return position.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,getLength,org.apache.hadoop.io.DataInputBuffer:getLength(),181,181,"/**
* Returns the length of the underlying data buffer. 
* @return Length of the data in the buffer.","* Returns the index one greater than the last valid character in the input
   * stream buffer.
   *
   * @return length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECSchema.java,<init>,org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.util.Map),69,93,"/**
* Initializes ECSchema object from a map of options.
* @param allOptions map of configuration settings
*/","* Constructor with schema name and provided all options. Note the options may
   * contain additional information for the erasure codec to interpret further.
   * @param allOptions all schema options",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECSchema.java,<init>,"org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.lang.String,int,int)",101,103,"/**
* Initializes an ECSchema object with given parameters.
* @param codecName name of the encoding scheme
* @param numDataUnits number of data units in the scheme
* @param numParityUnits number of parity units in the scheme
*/","* Constructor with key parameters provided.
   * @param codecName codec name
   * @param numDataUnits number of data units used in the schema
   * @param numParityUnits number os parity units used in the schema",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,setCodecOptions,org.apache.hadoop.io.erasurecode.codec.ErasureCodec:setCodecOptions(org.apache.hadoop.io.erasurecode.ErasureCodecOptions),65,68,"/**
* Sets Erasure Codec options and updates schema accordingly.
* @param options ErasureCodecOptions to be applied
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,getRequiredNumDataBlocks,org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:getRequiredNumDataBlocks(),54,56,"/**
* Returns the required number of data blocks based on schema.
* @return number of data units in schema
*/","* Get required data blocks count in a BlockGroup.
   * @return count of required data blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,getRequiredNumParityBlocks,org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:getRequiredNumParityBlocks(),62,64,"/**
* Returns the number of parity blocks required.
* @return Number of parity units as defined by the schema
*/","* Get required parity blocks count in a BlockGroup.
   * @return count of required parity blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.ErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",40,47,"/**
 * Initializes an ErasureCodec instance with the given configuration and options.
 * @param conf       Hadoop Configuration object
 * @param options    ErasureCodec options
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ErasureCoderOptions.java,<init>,"org.apache.hadoop.io.erasurecode.ErasureCoderOptions:<init>(int,int)",34,36,"/**
* Constructs ErasureCoderOptions with specified number of data and parity units.
* @param numDataUnits number of data units
* @param numParityUnits number of parity units
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,getName,org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getName(),49,51,"/**
* Returns the codec name from the underlying schema.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,createBlockGrouper,org.apache.hadoop.io.erasurecode.codec.ErasureCodec:createBlockGrouper(),86,91,"/**
* Creates a new BlockGrouper instance with the current schema.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecRegistry.java,<init>,org.apache.hadoop.io.erasurecode.CodecRegistry:<init>(),62,69,"/**
* Initializes internal codec registry with default factories.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecRegistry.java,getCoderByName,"org.apache.hadoop.io.erasurecode.CodecRegistry:getCoderByName(java.lang.String,java.lang.String)",161,172,"/**
* Retrieves a RawErasureCoderFactory by its name within the specified codec.
* @param codecName unique codec identifier
* @param coderName name of the coder to retrieve
* @return RawErasureCoderFactory object or null if not found
*/","* Get a specific coder factory defined by codec name and coder name.
   * @param codecName name of the codec
   * @param coderName name of the coder
   * @return the specific coder, null if not exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,makeBlockGroup,"org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:makeBlockGroup(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[])",72,77,"/**
* Creates an ECBlockGroup instance from given data and parity blocks.
* @param dataBlocks array of data blocks
* @param parityBlocks array of corresponding parity blocks
* @return created ECBlockGroup object
*/","* Calculating and organizing BlockGroup, to be called by ECManager
   * @param dataBlocks Data blocks to compute parity blocks against
   * @param parityBlocks To be computed parity blocks
   * @return ECBlockGroup.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECBlockGroup.java,getErasedCount,org.apache.hadoop.io.erasurecode.ECBlockGroup:getErasedCount(),61,73,"/**
* Returns the total count of erased blocks.
*/","* Get erased blocks count
   * @return erased count of blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getNumErasedBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumErasedBlocks(org.apache.hadoop.io.erasurecode.ECBlock[]),143,152,"/**
* Counts the number of erased blocks in the given array.
* @param inputBlocks array of ECBlock objects
* @return number of blocks marked as erased
*/","* Find out how many blocks are erased.
   * @param inputBlocks all the input blocks
   * @return number of erased blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,hasCodec,org.apache.hadoop.io.erasurecode.CodecUtil:hasCodec(java.lang.String),163,165,"/**
* Checks if a codec with the specified name exists in the registry.
* @param codecName name of the codec to check
* @return true if the codec is found, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECChunk.java,toBuffers,org.apache.hadoop.io.erasurecode.ECChunk:toBuffers(org.apache.hadoop.io.erasurecode.ECChunk[]),83,97,"/**
* Converts an array of ECChunks to an array of ByteBuffer objects.
* @param chunks array of ECChunk objects
* @return array of ByteBuffer objects, or null if a chunk is null
*/","* Convert an array of this chunks to an array of ByteBuffers
   * @param chunks chunks to convert into buffers
   * @return an array of ByteBuffers",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,release,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:release(),61,66,"/**
 * Releases native resources associated with this object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,release,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:release(),78,86,"/**
* Releases resources held by raw encoders.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncoder.java,getOutputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),70,72,"/**
* Returns parity blocks from the specified ECBlockGroup.
* @param blockGroup ECBlockGroup instance to retrieve parity blocks from
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureDecoder.java,getOutputBlocks,org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),59,84,"/**
* Fetches erased blocks from the specified ECBlockGroup.
* @param blockGroup group of blocks to search for erasures
* @return array of ECBlock objects representing the erased blocks
*/","* Which blocks were erased ? For XOR it's simple we only allow and return one
   * erased block, either data or parity.
   * @param blockGroup blockGroup.
   * @return output blocks to recover",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncoder.java,getInputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getInputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),66,68,"/**
* Returns input blocks from specified block group.
* @param blockGroup enclosing block group
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,getNumDataUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumDataUnits(),175,177,"/**
* Retrieves the number of data units from coder options.
* @return The number of data units or -1 if not available.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,getNumDataUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumDataUnits(),152,154,"/**
* Retrieves the number of data units from coder options.
* @return Number of data units as specified in coder options.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,getNumParityUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumParityUnits(),179,181,"/**
* Retrieves the number of parity units from coder options.
* @return Number of parity units
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,getNumParityUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumParityUnits(),156,158,"/**
* Retrieves the number of parity units from coder options.
* @return Number of parity units
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getInputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getInputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),71,82,"/**
* Retrieves an array of input blocks from the given EC block group.
* @param blockGroup ECBlockGroup object containing data and parity blocks
*/","* We have all the data blocks and parity blocks as input blocks for
   * recovering by default. It's codec specific
   * @param blockGroup blockGroup.
   * @return input blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,release,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:release(),60,65,"/**
* Releases any associated resources.
* @see #rsRawDecoder 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,release,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:release(),88,96,"/**
* Releases resources held by raw decoder and encoder.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),35,37,"/**
 * Initializes the XOR raw decoder with given Erasure Coder options.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Initializes a new instance of the decoder with given ErasureCoder options.
 * @param coderOptions configuration settings for erasure coding.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),43,45,"/**
* Initializes an AbstractNativeRawDecoder instance with ErasureCoderOptions. 
* @param coderOptions options for erasure coding
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawDecoder.java,preferDirectBuffer,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:preferDirectBuffer(),101,104,"/**
* Returns whether to use direct buffer allocation.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawEncoder.java,preferDirectBuffer,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:preferDirectBuffer(),98,101,"/**
 * Determines whether to use direct buffers for data operations.
 * @return true to enable direct buffer usage, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,add,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:add(int,int)",150,153,"/**
* Calculates bitwise XOR of two integers.
* @param x first integer value
* @param y second integer value
* @return result of the bitwise XOR operation (x^y)
*/","* Compute the sum of two fields
   *
   * @param x input field
   * @param y input field
   * @return result of addition",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,multiply,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:multiply(int,int)",162,165,"/**
* Fetches the product of two integers from a precomputed table.
* @param x first multiplicand
* @param y second multiplicand
* @return their product or -1 if invalid input
*/","* Compute the multiplication of two fields
   *
   * @param x input field
   * @param y input field
   * @return result of multiplication",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,divide,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:divide(int,int)",174,177,"/**
* Returns quotient from division table based on input values.
* @param x dividend value
* @param y divisor value
* @return integer result of division or -1 if invalid inputs
*/","* Compute the division of two fields
   *
   * @param x input field
   * @param y input field
   * @return x/y",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,power,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:power(int,int)",186,200,"/**
* Calculates the power of a base number using logarithmic table lookups and modulo arithmetic.
* @param x non-negative base value
* @param n exponent value
* @return result of x to the power of n, or 0 if x is negative
*/","* Compute power n of a field
   *
   * @param x input field
   * @param n power
   * @return x^n",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/DumpUtil.java,dumpChunk,org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpChunk(org.apache.hadoop.io.erasurecode.ECChunk),93,102,"/**
* Dumps the ECChunk object to the console in hexadecimal format.
* @param chunk ECChunk object to dump
*/","* Print data in hex format in a chunk.
   * @param chunk chunk.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),43,45,"/**
 * Initializes an AbstractNativeRawEncoder instance with given ErasureCoderOptions.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),35,37,"/**
* Initializes XOR raw encoder with given Erasure Coder options.
* @param coderOptions configuration settings for encoding
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Initializes a new instance of DummyRawEncoder with specified ErasureCoderOptions.
 * @param coderOptions Erasure coder configuration options
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,getNumAllUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumAllUnits(),183,185,"/**
* Returns total number of units from coder options.
* @return Total unit count
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,getNumAllUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumAllUnits(),160,162,"/**
* Retrieves the total number of units from coder options.
* @return The overall unit count.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,allowChangeInputs,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:allowChangeInputs(),202,204,"/**
* Allows changing of input fields based on coder options.
* @return true if allowed, false otherwise
*/","* Allow change into input buffers or not while perform encoding/decoding.
   * @return true if it's allowed to change inputs, false otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,allowChangeInputs,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:allowChangeInputs(),179,181,"/**
* Checks if input fields can be changed based on coder options.
* @return true if inputs can be changed, false otherwise
*/","* Allow change into input buffers or not while perform encoding/decoding.
   * @return true if it's allowed to change inputs, false otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,allowVerboseDump,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:allowVerboseDump(),210,212,"/**
* Returns whether verbose dump is allowed based on coder options.
* @return true if verbose dump is enabled, false otherwise
*/","* Allow to dump verbose info during encoding/decoding.
   * @return true if it's allowed to do verbose dump, false otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,allowVerboseDump,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:allowVerboseDump(),187,189,"/**
 * Returns whether verbose dump is allowed based on coder options.
 */","* Allow to dump verbose info during encoding/decoding.
   * @return true if it's allowed to do verbose dump, false otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecodeImpl,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecodeImpl(byte[][],int[],int,int[],byte[][],int[])",98,109,"/**
* Decodes inputs using Vandermonde system and substitutes values.
* @param inputs input byte arrays
* @param inputOffsets input offset arrays
* @param dataLen data length
* @param erasedIndexes indexes of erased values
* @param outputs output byte arrays
* @param outputOffsets output offset arrays
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetBuffer,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetBuffer(java.nio.ByteBuffer,int)",63,69,"/**
* Resets and reinitializes a ByteBuffer to specified length.
* @param buffer the ByteBuffer to reset
* @param len new length of the buffer
* @return the modified buffer
*/","* Ensure a buffer filled with ZERO bytes from current readable/writable
   * position.
   * @param buffer a buffer ready to read / write certain size bytes
   * @return the buffer itself, with ZERO bytes written, the position and limit
   *         are not changed after the call",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetBuffer,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetBuffer(byte[],int,int)",77,82,"/**
* Resets a section of the provided buffer with a new, empty chunk.
* @param buffer input byte array to modify
* @param offset starting position in the buffer
* @param len length of the section to reset
* @return modified buffer with the specified section filled
*/","* Ensure the buffer (either input or output) ready to read or write with ZERO
   * bytes fully in specified length of len.
   * @param buffer bytes array buffer
   * @return the buffer itself",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferEncodingState.java,convertToByteArrayState,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:convertToByteArrayState(),62,84,"/**
* Converts this encoding state to a byte array-based representation.
* @return ByteArrayEncodingState object with updated inputs and outputs
*/",* Convert to a ByteArrayEncodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayEncodingState.java,convertToByteBufferState,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:convertToByteBufferState(),69,85,"/**
* Converts the current state to a ByteBuffer-based encoding state.
* @return A new ByteBufferEncodingState object
*/",* Convert to a ByteBufferEncodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,convertToByteBufferState,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:convertToByteBufferState(),73,89,"/**
* Converts the current decoding state to a byte buffer-based state.
* @return ByteBufferDecodingState object
*/",* Convert to a ByteBufferDecodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,convertToByteArrayState,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:convertToByteArrayState(),66,91,"/**
* Converts the current decoding state to a byte array representation.
* @return ByteArrayDecodingState object with updated input/output arrays and offsets
*/",* Convert to a ByteArrayDecodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,initTables,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:initTables(int,int,byte[],int,byte[])",47,58,"/**
* Initializes tables using a given coding matrix.
* @param k number of columns in the coding matrix
* @param rows number of rows to process
* @param codingMatrix input byte array representing the coding matrix
* @param matrixOffset starting offset within the coding matrix
* @param gfTables precomputed GF256 table bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,genCauchyMatrix,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:genCauchyMatrix(byte[],int,int)",67,80,"/**
* Generates a Cauchy matrix in the provided byte array.
* @param a byte array to store the matrix
* @param m size of the matrix
* @param k number of identity sub-matrices in the top-left corner
*/","* Ported from Intel ISA-L library.
   *
   * @param k k.
   * @param a a.
   * @param m m.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GF256.java,gfInvertMatrix,"org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfInvertMatrix(byte[],byte[],int)",203,262,"/**
* Inverts a square matrix using Gaussian elimination.
* @param inMatrix input matrix to invert
* @param outMatrix output matrix containing the inverted matrix
* @param n size of the square matrix (n x n)
*/","* Invert a matrix assuming it's invertible.
   *
   * Ported from Intel ISA-L library.
   *
   * @param inMatrix inMatrix.
   * @param outMatrix outMatrix.
   * @param n n",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,encodeData,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:encodeData(byte[],int,byte[][],int[],byte[][],int[])",97,143,"/**
* Encrypts data using GF256 tables and SIMD instructions.
* @param gfTables array of GF256 coefficients
* @param dataLen length of the data to encrypt
* @param inputs array of input byte arrays
* @param inputOffsets array of input offsets
* @param outputs array of output byte arrays
* @param outputOffsets array of output offsets
*/","* Encode a group of inputs data and generate the outputs. It's also used for
   * decoding because, in this implementation, encoding and decoding are
   * unified.
   *
   * The algorithm is ported from Intel ISA-L library for compatible. It
   * leverages Java auto-vectorization support for performance.
   *
   * @param gfTables gfTables.
   * @param dataLen dataLen.
   * @param inputs inputs.
   * @param inputOffsets inputOffsets.
   * @param outputs outputs.
   * @param outputOffsets outputOffsets.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,encodeData,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:encodeData(byte[],java.nio.ByteBuffer[],java.nio.ByteBuffer[])",152,200,"/**
* Performs bitwise encoding of input data using GF256 tables.
* @param gfTables array of precomputed GF256 tables
* @param inputs array of input ByteBuffer objects
* @param outputs array of output ByteBuffer objects
*/","* See above. Try to use the byte[] version when possible.
   *
   * @param gfTables gfTables.
   * @param inputs inputs.
   * @param outputs outputs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,getInstance,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getInstance(int,int)",102,115,"/**
* Retrieves or creates a Galois Field instance based on the provided field size and primitive polynomial.
* @param fieldSize size of the Galois Field
* @param primitivePolynomial defining polynomial for the Galois Field
* @return GaloisField instance or null if creation failed
*/","* Get the object performs Galois field arithmetics.
   *
   * @param fieldSize           size of the field
   * @param primitivePolynomial a primitive polynomial corresponds to the size
   * @return GaloisField.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,solveVandermondeSystem,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],int[])",210,212,"/**
* Solves a Vandermonde system using the given x and y arrays.
* @param x array of unique values
* @param y corresponding array of coefficients
*/","* Given a Vandermonde matrix V[i][j]=x[j]^i and vector y, solve for z such
   * that Vz=y. The output z will be placed in y.
   *
   * @param x the vector which describe the Vandermonde matrix
   * @param y right-hand side of the Vandermonde system equation. will be
   *          replaced the output in this vector",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECBlock.java,<init>,org.apache.hadoop.io.erasurecode.ECBlock:<init>(),37,39,"/**
 * Initializes an empty ECBlock instance with default values.
 */",* A default constructor. isParity and isErased are false by default.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/LongWritable.java,compare,"org.apache.hadoop.io.LongWritable$DecreasingComparator:compare(byte[],int,int,byte[],int,int)",110,113,"/**
* Custom comparison method for byte arrays.
* @param b1 first byte array
* @param s1 start index of first byte array
* @param l1 length of first byte array
* @param b2 second byte array
* @param s2 start index of second byte array
* @param l2 length of second byte array
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,close,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:close(),444,453,"/**
* Closes this resource, resetting state if not already needed.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,updatePos,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:updatePos(boolean),560,564,"/**
* Updates the compressed stream position based on whether to add a fixed offset.
* @param shouldAddOn true to add the offset, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,updateReportedByteCount,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:updateReportedByteCount(int),184,187,"/**
* Updates reported byte count by adding compressed stream bytes.
* @param count number of bytes to add
*/","* This method is called by the client of this
   * class in case there are any corrections in
   * the stream position.  One common example is
   * when client of this code removes starting BZ
   * characters from the compressed stream.
   *
   * @param count count bytes are added to the reported bytes
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,readAByte,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:readAByte(java.io.InputStream),196,202,"/**
* Reads a single byte from the input stream and updates processed byte count.
* @throws IOException if an I/O error occurs
*/","* This method reads a Byte from the compressed stream. Whenever we need to
  * read from the underlying compressed stream, this method should be called
  * instead of directly calling the read method of the underlying compressed
  * stream. This method does important record keeping to have the statistic
  * that how many bytes have been read off the compressed stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CRC.java,<init>,org.apache.hadoop.io.compress.bzip2.CRC:<init>(),86,88,"/**
* Initializes and returns the current CRC value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,endBlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:endBlock(),572,589,"/**
* Updates CRCs and handles block integrity errors.
* @throws IOException if error occurs during process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,createHuffmanDecodingTables,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:createHuffmanDecodingTables(int,int)",793,819,"/**
* Creates Huffman decoding tables for multiple groups.
* @param alphaSize alphabet size
* @param nGroups number of groups to process
*/",* Called by recvDecodingTables() exclusively.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,initBlock,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:initBlock(),774,786,"/**
* Initializes block state and resets CRC.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,bsPutUByte,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsPutUByte(int),940,942,"/**
* Writes unsigned byte value to binary stream.
* @param c unsigned byte value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,bsPutInt,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsPutInt(int),944,949,"/**
* Writes a 32-bit integer value to the binary stream.
* @param u the integer value to write
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues4,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues4(),1199,1241,"/**
* Writes MTF values to output stream.
* @throws IOException if I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues5,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues5(int,int)",1243,1278,"/**
* Generates and writes MTF values to output stream.
* @param nGroups number of group records
* @param nSelectors number of selector records
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues1,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues1(int,int)",1029,1145,"/**
* Initializes and returns the number of selectors based on MTF values.
* @param nGroups number of coding tables
* @param alphaSize maximum symbol frequency value
* @return number of selectors used
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues3,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues3(int,int)",1174,1197,"/**
* Initializes MTF values for multiple groups.
* @param nGroups number of groups
* @param alphaSize size of the alphabet
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,mainQSort3,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainQSort3(org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data,int,int,int)",1634,1736,"/**
 * Recursive QuickSort algorithm with a stack-based implementation.
 * Sorts the data in-place using the provided arrays and thresholds.
 * @param dataShadow Data structure containing the sorting arrays
 * @param loSt Lower bound of the current sort range
 * @param hiSt Upper bound of the current sort range
 * @param dSt Current depth level
 */","* Method ""mainQSort3"", file ""blocksort.c"", BZip2 1.0.2",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,<init>,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:<init>(),66,68,"/**
* Initializes Bzip2 decompressor with default settings.
* @param bufferedMode whether to use buffering
* @param directBufferSize size of direct buffer (default: DEFAULT_DIRECT_BUFFER_SIZE)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,setInput,"org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:setInput(byte[],int,int)",70,88,"/**
* Sets input data and triggers processing from saved state.
* @param b input byte array
* @param off offset into input array (must be >= 0)
* @param len length of input data (must be > 0 && <= remaining bytes in array) 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,needsInput,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:needsInput(),112,130,"/**
* Determines whether input is needed based on compressed and uncompressed data consumption.
* @return true if all user-input has been consumed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,getBytesWritten,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getBytesWritten(),182,185,"/**
* Retrieves total bytes written to stream.
* @return current byte count or -1 if invalid
*/","* Returns the total number of uncompressed bytes output so far.
   *
   * @return the total (non-negative) number of uncompressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,getBytesRead,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getBytesRead(),192,195,"/**
* Retrieves the total bytes read from the stream.
*/","* Returns the total number of compressed bytes input so far.
   *
   * @return the total (non-negative) number of compressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,getRemaining,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getRemaining(),204,208,"/**
* Calculates remaining data in stream.
* @return total bytes available (user buffer + compressed direct buffer)","* Returns the number of bytes remaining in the input buffers; normally
   * called when finished() is true to determine amount of post-gzip-stream
   * data.
   *
   * @return the total (non-negative) number of unprocessed bytes in input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,reset,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:reset(),213,223,"/**
* Resets the stream and associated buffers to their initial state.
*/",* Resets everything including the input buffers (user and direct).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,<init>,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>(),64,66,"/**
* Initializes Bzip2 compressor with default settings.
* @param blockSize compression block size
* @param workFactor compression work factor
* @param directBufferSize direct buffer size for compression
*/","* Creates a new compressor with a default values for the
   * compression block size and work factor.  Compressed data will be
   * generated in bzip2 format.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,setInput,"org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:setInput(byte[],int,int)",124,142,"/**
* Sets input data from a byte array, specifying offset and length.
* @param b input data as a byte array (must not be null)
* @param off starting offset in the array
* @param len number of bytes to read
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,needsInput,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:needsInput(),158,181,"/**
* Determines if input is required based on compressed and uncompressed buffer availability.
* @return true if input is needed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,getBytesWritten,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:getBytesWritten(),244,248,"/**
* Returns total bytes written to stream.
* @return total bytes written in long value
*/","* Returns the total number of compressed bytes output so far.
   *
   * @return the total (non-negative) number of compressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,getBytesRead,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:getBytesRead(),255,259,"/**
 * Retrieves the total number of bytes read from the underlying stream.
 */","* Returns the total number of uncompressed bytes input so far.
   *
   * @return the total (non-negative) number of uncompressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,reset,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reset(),261,274,"/**
* Resets stream processing to initial state.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,<init>,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:<init>(java.io.OutputStream),251,255,"/**
* Initializes BZip2 compression output stream with given OutputStream.
* @param out underlying OutputStream to compress data into
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,<init>,"org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",36,47,"/**
* Initializes CompressorStream with OutputStream, Compressor, and bufferSize.
* @param out OutputStream to write compressed data
* @param compressor Compression algorithm instance
* @param bufferSize Buffer size for compression
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,<init>,org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream),58,60,"/**
 * Initializes a new CompressorStream instance from an OutputStream.
 * @param out underlying output stream to compress data into.","* Allow derived classes to directly set the underlying stream.
   * 
   * @param out Underlying output stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,writeStreamHeader,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:writeStreamHeader(),257,261,"/**
* Writes header to output stream if it's not null.
* @throws IOException on write error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,write,"org.apache.hadoop.io.compress.CompressorStream:write(byte[],int,int)",62,78,"/**
* Compresses input data into the compressor.
* @param b input byte array
* @param off starting offset in the array
* @param len number of bytes to compress
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,finish,org.apache.hadoop.io.compress.CompressorStream:finish(),87,95,"/**
* Completes the compression process.
* Ensures the underlying compressor is finished and data is fully written. 
* @throws IOException if an I/O error occurs during writing
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,<init>,org.apache.hadoop.io.compress.snappy.SnappyDecompressor:<init>(),65,67,"/**
 * Initializes a new instance of the decompressor with default buffer size. 
 * @param directBufferSize initial size of the direct buffer (default is DEFAULT_DIRECT_BUFFER_SIZE) 
 */",* Creates a new decompressor with the default buffer size.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,setInput,"org.apache.hadoop.io.compress.snappy.SnappyDecompressor:setInput(byte[],int,int)",83,101,"/**
* Sets input data from byte array, offset, and length.
* @param b input byte array
* @param off starting offset in the array
* @param len number of bytes to process
*/","* Sets input data for decompression.
   * This should be called if and only if {@link #needsInput()} returns
   * <code>true</code> indicating that more input data is required.
   * (Both native and non-native versions of various Decompressors require
   * that the data passed in via <code>b[]</code> remain unmodified until
   * the caller is explicitly notified--via {@link #needsInput()}--that the
   * buffer may be safely modified.  With this requirement, an extra
   * buffer-copy can be avoided.)
   *
   * @param b   Input data
   * @param off Start offset
   * @param len Length",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,needsInput,org.apache.hadoop.io.compress.snappy.SnappyDecompressor:needsInput(),138,156,"/**
* Determines whether input is still required.
* @return true if no more input needed, false otherwise
*/","* Returns true if the input data buffer is empty and
   * {@link #setInput(byte[], int, int)} should be called to
   * provide more input.
   *
   * @return <code>true</code> if the input data buffer is empty and
   *         {@link #setInput(byte[], int, int)} should be called in
   *         order to provide more input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,finished,org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:finished(),310,313,"/**
* Determines whether the input has ended and no more data is available.
* @return true if end of input reached, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,decompress,"org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompress(byte[],int,int)",192,230,"/**
* Decompresses data from input buffer into user-provided byte array.
*@param b input buffer to decompress from
*@param off starting offset in the input buffer
*@param len maximum length of compressed data to retrieve
*@return actual length of decompressed data written to output buffer
*/","* Fills specified buffer with uncompressed data. Returns actual number
   * of bytes of uncompressed data. A return value of 0 indicates that
   * {@link #needsInput()} should be called in order to determine if more
   * input data is required.
   *
   * @param b   Buffer for the uncompressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of compressed data.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,decompressDirect,"org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompressDirect(java.nio.ByteBuffer,java.nio.ByteBuffer)",275,305,"/**
* Decompresses a Snappy-compressed buffer into another.
* @param src source compressed buffer
* @param dst destination uncompressed buffer
* @return number of bytes decompressed or -1 on error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,reset,org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:reset(),315,319,"/**
* Resets internal state by calling superclass reset and marking end of input.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java,<init>,org.apache.hadoop.io.compress.snappy.SnappyCompressor:<init>(),67,69,"/**
* Initializes SnappyCompressor with default direct buffer size.",* Creates a new compressor with the default buffer size.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java,compress,"org.apache.hadoop.io.compress.snappy.SnappyCompressor:compress(byte[],int,int)",177,225,"/**
* Compresses data in the provided byte array.
* @param b input data to compress
* @param off starting offset within the array
* @param len number of bytes to compress
* @return the number of compressed bytes written or 0 if no compression was performed
*/","* Fills specified buffer with compressed data. Returns actual number
   * of bytes of compressed data. A return value of 0 indicates that
   * needsInput() should be called in order to determine if more input
   * data is required.
   *
   * @param b   Buffer for the compressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of compressed data.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java,reinit,org.apache.hadoop.io.compress.snappy.SnappyCompressor:reinit(org.apache.hadoop.conf.Configuration),248,251,"/**
 * Reinitializes this object with new configuration.
 * @param conf new configuration to apply
 */","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration
   *
   * @param conf Configuration from which new setting are fetched",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,"org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int,int)",51,66,"/**
* Initializes a DecompressorStream with the given InputStream, 
* Decompressor instance, and buffer sizes.
* @param in input stream to read from
* @param decompressor decompression algorithm to apply
* @param bufferSize size of the main data buffer
* @param skipBufferSize size of the skip bytes buffer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream),85,87,"/**
 * Initializes a DecompressorStream instance from an input stream.
 * @param in input stream to decompress
 */","* Allow derived classes to directly set the underlying stream.
   * 
   * @param in Underlying input stream.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SplitCompressionInputStream.java,<init>,"org.apache.hadoop.io.compress.SplitCompressionInputStream:<init>(java.io.InputStream,long,long)",39,44,"/**
* Initializes a SplitCompressionInputStream with a specified range.
* @param in underlying input stream
* @param start starting byte position of the range
* @param end ending byte position of the range
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,getCompressedData,org.apache.hadoop.io.compress.DecompressorStream:getCompressedData(),175,180,"/**
* Reads compressed data from input stream.
* @throws IOException on read error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,available,org.apache.hadoop.io.compress.DecompressorStream:available(),215,219,"/**
* Returns whether there are unread bytes in the stream.
* @return 1 if data is available, 0 otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,resetState,org.apache.hadoop.io.compress.BlockDecompressorStream:resetState(),137,142,"/**
* Resets state to default values and calls superclass resetState.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,compress,org.apache.hadoop.io.compress.BlockCompressorStream:compress(),147,155,"/**
* Compresses and writes a chunk of data to the output stream.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,setInput,"org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:setInput(byte[],int,int)",87,104,"/**
* Sets input data and triggers processing.
* @param b input byte array
* @param off offset into the input data
* @param len length of input data to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,needsInput,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:needsInput(),134,151,"/**
* Determines whether input is still required based on consumed compressed and user data.
* @return true if all input has been consumed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,finished,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:finished(),302,305,"/**
* Checks if input is exhausted and current operation is complete.
* @return true if no more data to process and all operations are done, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,getRemaining,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:getRemaining(),216,221,"/**
* Calculates the number of bytes remaining in the stream.
* @return total bytes available for consumption or -1 if invalid
*/","* <p>Returns the number of bytes remaining in the input buffers;
   * normally called when finished() is true to determine amount of post-stream
   * data.</p>
   *
   * @return the total (non-negative) number of unprocessed bytes in input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,reset,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:reset(),226,238,"/**
* Resets internal state to initial conditions.
*/",* Resets everything including the input buffers (user and direct).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,decompress,"org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:decompress(byte[],int,int)",166,207,"/**
* Decompresses data from the provided byte array.
* @param b compressed data to decompress
* @param off offset into the input array
* @param len number of bytes to decompress
* @return the number of decompressed bytes or -1 on error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,setInput,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:setInput(byte[],int,int)",122,139,"/**
* Sets input data from a byte array.
* @param b input data
* @param off starting offset in the array (inclusive)
* @param len length of the input data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,needsInput,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:needsInput(),156,181,"/**
* Determines if additional input is required.
* @return true if all data has been consumed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,compress,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:compress(byte[],int,int)",195,243,"/**
* Compresses byte array in-place using DEFLATE algorithm.
* @param b input byte array to compress
* @param off offset into the array where compression should start
* @param len maximum number of bytes to compress
* @return actual number of compressed bytes written to the array
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,getBytesWritten,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getBytesWritten(),250,254,"/**
* Returns the total number of bytes written to the stream.
* @return Total bytes written
*/","* Returns the total number of compressed bytes output so far.
   *
   * @return the total (non-negative) number of compressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,getBytesRead,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getBytesRead(),261,265,"/**
* Returns the total number of bytes read from the underlying stream.
* @return Total byte count or -1 if invalid stream state
*/","* <p>Returns the total number of uncompressed bytes input so far.</p>
   *
   * @return the total (non-negative) number of uncompressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,reset,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:reset(),267,283,"/**
* Resets internal state to initial values.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,getCompressedData,org.apache.hadoop.io.compress.BlockDecompressorStream:getCompressedData(),114,135,"/**
* Reads compressed data from the underlying stream and returns its size.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,<init>,org.apache.hadoop.io.compress.lz4.Lz4Decompressor:<init>(),77,79,"/**
* Constructs a new Lz4Decompressor instance with default direct buffer size.
* @param bufferSize initial size of the decompression buffer
*/",* Creates a new decompressor with the default buffer size.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,setInput,"org.apache.hadoop.io.compress.lz4.Lz4Decompressor:setInput(byte[],int,int)",95,113,"/**
* Sets input data from a byte array, offset and length.
* @param b the byte array to set as input
* @param off starting position in the array
* @param len number of bytes to process
*/","* Sets input data for decompression.
   * This should be called if and only if {@link #needsInput()} returns
   * <code>true</code> indicating that more input data is required.
   * (Both native and non-native versions of various Decompressors require
   * that the data passed in via <code>b[]</code> remain unmodified until
   * the caller is explicitly notified--via {@link #needsInput()}--that the
   * buffer may be safely modified.  With this requirement, an extra
   * buffer-copy can be avoided.)
   *
   * @param b   Input data
   * @param off Start offset
   * @param len Length",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,needsInput,org.apache.hadoop.io.compress.lz4.Lz4Decompressor:needsInput(),150,168,"/**
* Determines whether input is needed to continue processing.
* @return true if input is required, false otherwise
*/","* Returns true if the input data buffer is empty and
   * {@link #setInput(byte[], int, int)} should be called to
   * provide more input.
   *
   * @return <code>true</code> if the input data buffer is empty and
   *         {@link #setInput(byte[], int, int)} should be called in
   *         order to provide more input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,decompress,"org.apache.hadoop.io.compress.lz4.Lz4Decompressor:decompress(byte[],int,int)",204,242,"/**
* Decompresses data into the provided byte array.
* @param b target byte array
* @param off starting offset
* @param len number of bytes to decompress
* @return actual number of decompressed bytes
*/","* Fills specified buffer with uncompressed data. Returns actual number
   * of bytes of uncompressed data. A return value of 0 indicates that
   * {@link #needsInput()} should be called in order to determine if more
   * input data is required.
   *
   * @param b   Buffer for the compressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of uncompressed data.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,<init>,org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>(int),95,97,"/**
* Constructs an Lz4 compressor with specified direct buffer size.
* @param directBufferSize size of direct buffer for compression
*/","* Creates a new compressor.
   *
   * @param directBufferSize size of the direct buffer to be used.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,compress,"org.apache.hadoop.io.compress.lz4.Lz4Compressor:compress(byte[],int,int)",212,260,"/**
* Compresses a byte array and writes it to the provided buffer.
* @param b input data to compress
* @param off offset into the buffer where output should be written
* @param len length of data in the buffer to compress
* @return number of bytes compressed or 0 if no compression was performed
*/","* Fills specified buffer with compressed data. Returns actual number
   * of bytes of compressed data. A return value of 0 indicates that
   * needsInput() should be called in order to determine if more input
   * data is required.
   *
   * @param b   Buffer for the compressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of compressed data.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,reinit,org.apache.hadoop.io.compress.lz4.Lz4Compressor:reinit(org.apache.hadoop.conf.Configuration),283,286,"/**
 * Reinitializes the system configuration.
 * @param conf new configuration settings to apply
 */","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration
   *
   * @param conf Configuration from which new setting are fetched",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodecByName,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecByName(java.lang.String),246,257,"/**
* Retrieves CompressionCodec instance by its name.
* @param codecName unique codec identifier
* @return CompressionCodec object or null if not found
*/","* Find the relevant compression codec for the codec's canonical class name
   * or by codec alias.
   * <p>
   * Codec aliases are case insensitive.
   * <p>
   * The code alias is the short class name (without the package name).
   * If the short class name ends with 'Codec', then there are two aliases for
   * the codec, the complete short class name and the short class name without
   * the 'Codec' ending. For example for the 'GzipCodec' codec class name the
   * alias are 'gzip' and 'gzipcodec'.
   *
   * @param codecName the canonical class name of the codec
   * @return the codec object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,payback,"org.apache.hadoop.io.compress.CodecPool:payback(java.util.Map,java.lang.Object)",105,122,"/**
* Adds a codec to the specified class-based set in the pool.
* @param pool map of classes to sets of codecs
* @param codec object to add to the corresponding set
* @return true if added, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,updateLeaseCount,"org.apache.hadoop.io.compress.CodecPool:updateLeaseCount(org.apache.hadoop.thirdparty.com.google.common.cache.LoadingCache,java.lang.Object,int)",131,137,"/**
* Updates the lease count for a given codec class.
* @param usageCounts cache of class-specific lease counts
* @param codec codec instance (null to ignore)
* @param delta change in lease count
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getLeasedCompressorsCount,org.apache.hadoop.io.compress.CodecPool:getLeasedCompressorsCount(org.apache.hadoop.io.compress.CompressionCodec),245,248,"/**
* Returns the count of leased compressors for a given compression codec.
* @param codec CompressionCodec object
*/","* Return the number of leased {@link Compressor}s for this
   * {@link CompressionCodec}.
   *
   * @param codec codec.
   * @return the number of leased.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getLeasedDecompressorsCount,org.apache.hadoop.io.compress.CodecPool:getLeasedDecompressorsCount(org.apache.hadoop.io.compress.CompressionCodec),257,260,"/**
* Returns count of leased decompressors for given compression codec.
* @param codec CompressionCodec object
*/","* Return the number of leased {@link Decompressor}s for this
   * {@link CompressionCodec}.
   *
   * @param codec codec.
   * @return the number of leased",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,checkNativeCodeLoaded,org.apache.hadoop.io.compress.ZStandardCodec:checkNativeCodeLoaded(),62,77,"/**
* Verifies native code for zStandard compression and decompression is loaded.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,isNativeCodeLoaded,org.apache.hadoop.io.compress.ZStandardCodec:isNativeCodeLoaded(),79,82,"/**
* Checks if native code libraries are loaded.
* @return true if both ZStandardCompressor and Decompressor are loaded, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,flush,org.apache.hadoop.io.file.tfile.Compression$FinishOnFlushCompressionStream:flush(),66,72,"/**
* Flushes and resets compression output stream.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,getCompressorType,org.apache.hadoop.io.compress.GzipCodec:getCompressorType(),69,74,"/**
* Determines the compressor type based on native zlib availability.
* @return Class of compressor to use (either GzipZlibCompressor or BuiltInGzipCompressor)",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,getDecompressorType,org.apache.hadoop.io.compress.GzipCodec:getDecompressorType(),102,107,"/**
* Returns the decompressor type based on native zlib loading status.
* @return Class of Decompressor implementation (native or built-in)",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibCompressorType,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressorType(org.apache.hadoop.conf.Configuration),97,101,"/**
* Returns the compressor type based on native zlib loading status.
* @param conf Hadoop Configuration object
* @return Class of either ZlibCompressor or BuiltInZlibDeflater
*/","* Return the appropriate type of the zlib compressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the zlib compressor.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibDecompressorType,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressorType(org.apache.hadoop.conf.Configuration),121,125,"/**
* Returns decompressor type based on native zlib availability.
* @param conf configuration object
* @return Decompressor class (native or built-in)","* Return the appropriate type of the zlib decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the zlib decompressor.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,loadNativeZLib,org.apache.hadoop.io.compress.zlib.ZlibFactory:loadNativeZLib(),52,64,"/**
* Initializes native zlib library.
* @VisibleForTesting
*/","* Load native library and set the flag whether to use native library. The
   * method is also used for reset the flag modified by setNativeZlibLoaded",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,init,org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:init(org.apache.hadoop.conf.Configuration),156,165,"/**
* Initializes the compressor with configuration settings.
* @param conf input configuration
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInZlibDeflater.java,reinit,org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:reinit(org.apache.hadoop.conf.Configuration),65,83,"/**
* Reinitializes the compressor with a new configuration.
* @param conf Compression configuration to use
*/","* reinit the compressor with the given configuration. It will reset the
   * compressor's compression level and compression strategy. Different from
   * <tt>ZlibCompressor</tt>, <tt>BuiltInZlibDeflater</tt> only support three
   * kind of compression strategy: FILTERED, HUFFMAN_ONLY and DEFAULT_STRATEGY.
   * It will use DEFAULT_STRATEGY as default if the configured compression
   * strategy is not supported.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,<init>,"org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader,int)",261,274,"/**
* Initializes ZlibCompressor with specified compression settings.
* @param level Compression level (e.g. best, fast)
* @param strategy Compression strategy (e.g. stream, direct)
* @param header Compression header settings
* @param directBufferSize Size of direct buffer for compressed data
*/","* Creates a new compressor using the specified compression level.
   * Compressed data will be generated in ZLIB format.
   * 
   * @param level Compression level #CompressionLevel
   * @param strategy Compression strategy #CompressionStrategy
   * @param header Compression header #CompressionHeader
   * @param directBufferSize Size of the direct buffer to be used.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,setInput,"org.apache.hadoop.io.compress.zlib.ZlibCompressor:setInput(byte[],int,int)",300,318,"/**
* Sets input data for compression, validating bounds and initialization.
* @param b input byte array
* @param off offset into the array
* @param len length of the data to be processed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,needsInput,org.apache.hadoop.io.compress.zlib.ZlibCompressor:needsInput(),340,367,"/**
* Determines if additional user input is required.
* @return true if more data is needed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,getBytesWritten,org.apache.hadoop.io.compress.zlib.ZlibCompressor:getBytesWritten(),432,436,"/**
* Returns total bytes written to stream.
* @return total bytes written as a long value
*/","* Returns the total number of compressed bytes output so far.
   *
   * @return the total (non-negative) number of compressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,getBytesRead,org.apache.hadoop.io.compress.zlib.ZlibCompressor:getBytesRead(),443,447,"/**
* Returns total bytes read from stream.
* @return Total bytes read.","* Returns the total number of uncompressed bytes input so far.
   *
   * @return the total (non-negative) number of uncompressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,reset,org.apache.hadoop.io.compress.zlib.ZlibCompressor:reset(),449,461,"/**
* Resets internal state to initial values.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,compress,"org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:compress(byte[],int,int)",81,132,"/**
* Compresses input data using gzip algorithm.
* @param b input byte array
* @param off offset into input array
* @param len length of input data to compress
* @return number of bytes written to output
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,finished,org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:finished(),360,363,"/**
* Determines whether input processing is complete.
* @return true if all data has been processed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,"org.apache.hadoop.io.compress.zlib.ZlibDecompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader,int)",107,115,"/**
* Initializes Zlib decompressor with given compression header and direct buffer size.
* @param header CompressionHeader object containing decompression settings
* @param directBufferSize Size of the direct buffers for compressed and uncompressed data
*/","* Creates a new decompressor.
   * @param header header.
   * @param directBufferSize directBufferSize.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,setInput,"org.apache.hadoop.io.compress.zlib.ZlibDecompressor:setInput(byte[],int,int)",121,139,"/**
* Sets input data from byte array, offset and length.
* @param b input byte array
* @param off starting position of valid bytes in the array
* @param len number of valid bytes in the array
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,needsInput,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:needsInput(),170,188,"/**
* Determines whether additional input is required.
* @return true if no more data needed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,getBytesWritten,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getBytesWritten(),242,245,"/**
* Returns total bytes written to stream.
* @return Total byte count or -1 if invalid stream
*/","* Returns the total number of uncompressed bytes output so far.
   *
   * @return the total (non-negative) number of uncompressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,getBytesRead,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getBytesRead(),252,255,"/**
* Returns the total bytes read from the stream.
* @return total bytes read or throws exception if invalid
*/","* Returns the total number of compressed bytes input so far.
   *
   * @return the total (non-negative) number of compressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,getRemaining,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getRemaining(),264,268,"/**
* Calculates the total remaining bytes in the stream.
* @return The sum of buffered and compressed data lengths.","* Returns the number of bytes remaining in the input buffers; normally
   * called when finished() is true to determine amount of post-gzip-stream
   * data.
   *
   * @return the total (non-negative) number of unprocessed bytes in input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,reset,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:reset(),273,283,"/**
* Resets internal state to initial conditions.
*/",* Resets everything including the input buffers (user and direct).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,finalize,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:finalize(),293,296,"/**
 * Performs cleanup and shutdown when the object is garbage collected.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,executeTrailerState,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:executeTrailerState(),367,416,"/**
* Verifies the gzip trailer fields: CRC-32 and decompressed size.
* @throws IOException on validation failure
*/","* Parse the gzip trailer (assuming we're in the appropriate state).
   * In order to deal with degenerate cases (e.g., user buffer is one byte
   * long), we copy trailer bytes (all 8 of 'em) to a local buffer.</p>
   *
   * See http://www.ietf.org/rfc/rfc1952.txt for the gzip spec.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,processBasicHeader,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:processBasicHeader(),509,524,"/**
* Validates and extracts basic header information from a gzip file.
* @throws IOException if the file is not a valid gzip file
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readCompressedString,org.apache.hadoop.io.WritableUtils:readCompressedString(java.io.DataInput),87,91,"/**
* Decompresses and converts a compressed string from input stream to UTF-8 encoded String.
* @param in input stream containing the compressed string data
* @return decompressed string or null if compression failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeVInt,"org.apache.hadoop.io.WritableUtils:writeVInt(java.io.DataOutput,int)",257,259,"/**
* Writes a variable-length integer to the output stream.
* @param stream DataOutput stream to write to
* @param i integer value to write
*/","* Serializes an integer to a binary stream with zero-compressed encoding.
   * For -112 {@literal <=} i {@literal <=} 127, only one byte is used with the
   * actual value.
   * For other values of i, the first byte value indicates whether the
   * integer is positive or negative, and the number of bytes that follow.
   * If the first byte value v is between -113 and -116, the following integer
   * is positive, with number of bytes that follow are -(v+112).
   * If the first byte value v is between -121 and -124, the following integer
   * is negative, with number of bytes that follow are -(v+120). Bytes are
   * stored in the high-non-zero-byte-first order.
   *
   * @param stream Binary output stream
   * @param i Integer to be serialized
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VLongWritable.java,write,org.apache.hadoop.io.VLongWritable:write(java.io.DataOutput),54,57,"/**
* Writes user ID as a variable-length long to output stream.
* @param out DataOutput stream to write to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readVLong,org.apache.hadoop.io.WritableUtils:readVLong(java.io.DataInput),313,326,"/**
* Reads a variable-length long integer from the input stream.
* @param stream DataInput source
* @return long value or throws IOException if an error occurs
*/","* Reads a zero-compressed encoded long from input stream and returns it.
   * @param stream Binary input stream
   * @throws IOException raised on errors performing I/O.
   * @return deserialized long from stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WeakReferencedElasticByteBufferPool.java,getBuffer,"org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:getBuffer(boolean,int)",82,107,"/**
* Retrieves a ByteBuffer from the internal pool or allocates a new one.
* @param direct whether to use direct buffers
* @param length desired buffer size
* @return allocated ByteBuffer or null if failed
*/","* {@inheritDoc}
   *
   * @param direct whether we want a direct byte buffer or a heap one.
   * @param length length of requested buffer.
   * @return returns equal or next greater than capacity buffer from
   * pool if already available and not garbage collected else creates
   * a new buffer and return it.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WeakReferencedElasticByteBufferPool.java,putBuffer,org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:putBuffer(java.nio.ByteBuffer),113,130,"/**
* Inserts a buffer into the cache with a unique (capacity, time) key.
* @param buffer ByteBuffer object to be cached
*/","* Return buffer to the pool.
   * @param buffer buffer to be returned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,charAt,org.apache.hadoop.io.Text:charAt(int),163,169,"/**
* Returns the character at specified position in the string.
* @param position index of the character to retrieve
* @return Unicode code point of the character or -1 if out-of-range
*/","* Returns the Unicode Scalar Value (32-bit integer value)
   * for the character at <code>position</code>. Note that this
   * method avoids using the converter or doing String instantiation.
   *
   * @param position input position.
   * @return the Unicode scalar value at position or -1
   *          if the position is invalid or points to a
   *          trailing byte",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,org.apache.hadoop.io.Text:set(java.lang.String),228,237,"/**
* Encodes and sets the input string as a byte array.
* @param string String to be encoded
*/","* Set to contain the contents of a string.
   *
   * @param string input string.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,encode,org.apache.hadoop.io.Text:encode(java.lang.String),514,517,"/**
* Encodes a given string into a ByteBuffer.
* @param string input string to be encoded
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,buildCacheKey,org.apache.hadoop.security.token.Token:buildCacheKey(),449,452,"/**
* Generates cache key by concatenating kind, identifier and password.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,"org.apache.hadoop.io.Text:set(byte[],int,int)",272,277,"/**
* Appends UTF-8 encoded byte array to internal buffer.
* @param utf8 encoded byte array
* @param start starting index in the byte array
* @param len length of bytes to append
*/","* Set the Text to range of bytes.
   *
   * @param utf8 the data to copy from
   * @param start the first position of the new string
   * @param len the number of bytes of the new string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,append,"org.apache.hadoop.io.Text:append(byte[],int,int)",286,294,"/**
* Appends UTF-8 encoded byte array to existing buffer.
* @param utf8 byte array to append
* @param start starting index in byte array
* @param len number of bytes to append
*/","* Append a range of bytes to the end of the given text.
   *
   * @param utf8 the data to copy from
   * @param start the first position to append from utf8
   * @param len the number of bytes to append",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readWithKnownLength,"org.apache.hadoop.io.Text:readWithKnownLength(java.io.DataInput,int)",383,388,"/**
* Reads data from input stream with known length.
* @param in DataInput stream to read from
* @param len Known length of data to be read
*/","* Read a Text object whose length is already known.
   * This allows creating Text from a stream which uses a different serialization
   * format.
   *
   * @param in input in.
   * @param len input len.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,decode,org.apache.hadoop.io.Text:decode(byte[]),457,459,"/**
* Decodes UTF-8 encoded bytes to a string.
* @param utf8 byte array containing UTF-8 encoded data
*/","* @return Converts the provided byte array to a String using the
   * UTF-8 encoding. If the input is malformed,
   * replace by a default value.
   *
   * @param utf8 input utf8.
   * @throws CharacterCodingException when a character
   *                                  encoding or decoding error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,decode,"org.apache.hadoop.io.Text:decode(byte[],int,int)",461,464,"/**
* Decodes a UTF-8 encoded byte array into a string.
* @param utf8 encoded byte array
* @param start starting index in the array
* @param length number of bytes to decode
* @return decoded string or throws CharacterCodingException if decoding fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,decode,"org.apache.hadoop.io.Text:decode(byte[],int,int,boolean)",480,483,"/**
* Decodes UTF-8 encoded bytes to a string.
* @param utf8 UTF-8 encoded byte array
* @param start starting index in the byte array
* @param length number of bytes to decode
* @param replace whether to replace unencodable characters
*/","* @return Converts the provided byte array to a String using the
   * UTF-8 encoding. If <code>replace</code> is true, then
   * malformed input is replaced with the
   * substitution character, which is U+FFFD. Otherwise the
   * method throws a MalformedInputException.
   *
   * @param utf8 input utf8.
   * @param start input start.
   * @param length input length.
   * @param replace input replace.
   * @throws CharacterCodingException when a character
   *                                  encoding or decoding error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,validateUTF8,org.apache.hadoop.io.Text:validateUTF8(byte[]),626,628,"/**
* Validates UTF-8 encoded byte array.
* @throws MalformedInputException if input is not valid UTF-8
*/","* Check if a byte array contains valid UTF-8.
   *
   * @param utf8 byte array
   * @throws MalformedInputException if the byte array contains invalid UTF-8",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,addToMap,org.apache.hadoop.io.AbstractMapWritable:addToMap(java.lang.Class),91,101,"/**
* Adds a class to the internal map with a unique ID.
* @param clazz Class object to add
*/","* Add a Class to the maps if it is not already present.
   * @param clazz clazz.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,<init>,org.apache.hadoop.io.AbstractMapWritable:<init>(),145,166,"/**
* Initializes AbstractMapWritable with default writable types and configuration.
*/",constructor.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,readFields,org.apache.hadoop.io.AbstractMapWritable:readFields(java.io.DataInput),194,216,"/**
* Reads and populates unknown classes from the input stream.
* @throws IOException if a class cannot be loaded
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,write,org.apache.hadoop.io.AbstractMapWritable:write(java.io.DataOutput),180,192,"/**
* Writes class table to output stream.
* @throws IOException if write operation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Metadata:<init>(),735,737,"/**
 * Initializes a new instance of Metadata with an empty metadata map.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VLongWritable.java,<init>,org.apache.hadoop.io.VLongWritable:<init>(long),38,38,"/**
* Constructs a VLongWritable object with the specified long value.
* @param value the underlying long value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,getSerializer,org.apache.hadoop.io.serializer.SerializationFactory:getSerializer(java.lang.Class),81,87,"/**
* Retrieves a serializer instance for the specified class type.
* @param c Class type to find serializer for
* @return Serializer instance or null if not found",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,getDeserializer,org.apache.hadoop.io.serializer.SerializationFactory:getDeserializer(java.lang.Class),89,95,"/**
* Retrieves deserializer instance for the given class type.
* @param c Class to fetch deserializer for
* @return Deserializer instance or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/JavaSerializationComparator.java,compare,"org.apache.hadoop.io.serializer.JavaSerializationComparator:compare(java.lang.Object,java.lang.Object)",47,51,"/**
* Compares two objects of type T based on their natural ordering.
* @param o1 first object to compare
* @param o2 second object to compare
* @return negative if o1 is less than o2, zero if equal, positive otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/JavaSerialization.java,deserialize,org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:deserialize(java.lang.Object),54,63,"/**
* Deserializes an object of type T from the input stream.
* @param object ignored, will be deserialized from stream instead
* @return the deserialized object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,filesystem,org.apache.hadoop.io.SequenceFile$Writer:filesystem(org.apache.hadoop.fs.FileSystem),1002,1005,"/**
* Returns an Option representing the file system.
* @param fs FileSystem object
*/","* @deprecated only used for backwards-compatibility in the createWriter methods
     * that take FileSystem.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,<init>,org.apache.hadoop.io.DataInputByteBuffer:<init>(),74,76,"/**
 * Initializes DataInputByteBuffer with default buffer. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,getData,org.apache.hadoop.io.DataInputByteBuffer:getData(),87,89,"/**
* Retrieves data from underlying buffer storage.
* @return Array of ByteBuffer objects containing the fetched data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,getPosition,org.apache.hadoop.io.DataInputByteBuffer:getPosition(),91,93,"/**
* Retrieves current buffer position.
* @return Current position in the buffer as an integer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,getLength,org.apache.hadoop.io.DataInputByteBuffer:getLength(),95,97,"/**
* Returns the length of the buffer.
* @return Length value",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,<init>,"org.apache.hadoop.util.bloom.Key:<init>(byte[],double)",98,100,"/**
 * Creates a new key instance with specified value and weight.
 * @param value byte array representing the key
 * @param weight numerical weight associated with the key
 */","* Constructor.
   * <p>
   * Builds a key with a specified weight.
   * @param value The value of <i>this</i> key.
   * @param weight The weight associated to <i>this</i> key.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,cleanup,org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:cleanup(),3916,3921,"/**
* Cleans up resources and parent container.
*/","The default cleanup. Subclasses can override this with a custom 
       * cleanup",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,grow,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:grow(),3193,3200,"/**
* Doubles the capacity of internal arrays and updates related data structures.
* @param none
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:<init>(org.apache.hadoop.io.SequenceFile$CompressionType),977,979,"/**
* Constructs CompressionOption with specified type and no filter.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compression,"org.apache.hadoop.io.SequenceFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",1056,1059,"/**
* Creates a CompressionOption instance with specified type and codec.
* @param value Compression type (e.g. GZIP, DEFLATE)
* @param codec Compression codec implementation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericsUtil.java,toArray,org.apache.hadoop.util.GenericsUtil:toArray(java.util.List),86,88,"/**
* Converts a List to an array with elements of the same type.
* @param list The input list to convert
*/","* Converts the given <code>List&lt;T&gt;</code> to a an array of 
   * <code>T[]</code>. 
   * @param list the list to convert
   * @param <T> Generics Type T.
   * @throws ArrayIndexOutOfBoundsException if the list is empty. 
   * Use {@link #toArray(Class, List)} if the list may be empty.
   * @return T Array.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,<init>,org.apache.hadoop.io.InputBuffer:<init>(),69,71,"/**
* Initializes an empty InputBuffer instance.
* @param buffer internal data structure (defaulting to new Buffer())",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,reset,"org.apache.hadoop.io.InputBuffer:reset(byte[],int)",83,85,"/**
 * Resets the internal buffer with new data.
 * @param input the byte array to populate the buffer
 * @param length the number of bytes from input to use
 */","* Resets the data that the buffer reads.
   * @param input input.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,reset,"org.apache.hadoop.io.InputBuffer:reset(byte[],int,int)",93,95,"/**
* Resets buffer with specified data.
* @param input byte array to reset from
* @param start starting index in input array
* @param length number of bytes to reset
*/","* Resets the data that the buffer reads.
   * @param input input.
   * @param start start.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,getPosition,org.apache.hadoop.io.InputBuffer:getPosition(),101,101,"/**
* Retrieves the current position in the buffer.
* @return current position as an integer value
*/","* Returns the current position in the input.
   * @return the current position in the input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,getLength,org.apache.hadoop.io.InputBuffer:getLength(),107,107,"/**
 * Returns the length of the underlying buffer.
 */","* Returns the length of the input.
   * @return length of the input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,read,org.apache.hadoop.io.MD5Hash:read(java.io.DataInput),87,91,"/**
* Reads and constructs an MD5 hash object from a DataInput stream. 
*/","* Constructs, reads and returns an instance.
   * @param in in.
   * @throws IOException raised on errors performing I/O.
   * @return MD5Hash.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(java.io.InputStream),138,147,"/**
* Computes the MD5 hash of an input stream.
* @param in InputStream to digest
* @return MD5Hash object representing the computed hash
*/","* Construct a hash value for the content from the InputStream.
   * @param in input stream.
   * @return MD5Hash.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,"org.apache.hadoop.io.MD5Hash:digest(byte[],int,int)",156,162,"/**
* Computes the MD5 hash of a given data segment.
* @param data input byte array
* @param start starting index of the data segment
* @param len length of the data segment
* @return MD5Hash object representing the computed hash
*/","* Construct a hash value for a byte array.
   * @param data data.
   * @param start start.
   * @param len len.
   * @return MD5Hash.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,"org.apache.hadoop.io.MD5Hash:digest(byte[][],int,int)",171,179,"/**
* Computes the MD5 hash of a byte array sequence.
* @param dataArr array of byte arrays to process
* @param start starting index for each byte array chunk
* @param len length of each byte array chunk
* @return MD5Hash object containing the computed digest
*/","* Construct a hash value for an array of byte array.
   * @param dataArr dataArr.
   * @param start start.
   * @param len len.
   * @return MD5Hash.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,hashCode,org.apache.hadoop.io.MD5Hash:hashCode(),234,237,"/**
* Calculates hash code based on quarter digest.
*/","Returns a hash code value for this object.
   * Only uses the first 4 bytes, since md5s are evenly distributed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,setDigest,org.apache.hadoop.io.MD5Hash:setDigest(java.lang.String),283,293,"/**
* Sets the MD5 digest from a hexadecimal string.
* @param hex hexadecimal representation of the digest
*/","* Sets the digest value from a hex string.
   * @param hex hex.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,metadata,org.apache.hadoop.io.SequenceFile$Writer:metadata(org.apache.hadoop.io.SequenceFile$Metadata),1048,1050,"/**
* Creates a MetadataOption instance from the given Metadata value.
* @param value Metadata object to be wrapped
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$StreamOption:<init>(org.apache.hadoop.fs.FSDataOutputStream),912,914,"/**
 * Constructs StreamOption instance from FSDataOutputStream.
 * @param stream output stream to configure options for.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,<init>,org.apache.hadoop.io.ObjectWritable:<init>(java.lang.Object),48,50,"/**
 * Converts an Object to a Writable representation.
 * @param instance the Object to convert
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,tryInstantiateProtobuf,"org.apache.hadoop.io.ObjectWritable:tryInstantiateProtobuf(java.lang.Class,java.io.DataInput)",351,389,"/**
* Deserializes a Message object from the given DataInput.
* @param protoClass Class of the Message to deserialize
* @param dataIn input stream containing serialized message data
* @throws IOException if serialization fails or invalid data is encountered
*/","* Try to instantiate a protocol buffer of the given message class
   * from the given input stream.
   * 
   * @param protoClass the class of the generated protocol buffer
   * @param dataIn the input stream to read from
   * @return the instantiated Message instance
   * @throws IOException if an IO problem occurs",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,fsync,org.apache.hadoop.io.IOUtils:fsync(java.io.File),392,413,"/**
* Synchronizes the specified file or directory with the file system.
* @param fileToSync file or directory to sync
*/","* Ensure that any writes to the given file is written to the storage device
   * that contains it. This method opens channel on given File and closes it
   * once the sync is done.<br>
   * Borrowed from Uwe Schindler in LUCENE-5588
   * @param fileToSync the file to fsync
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,equals,org.apache.hadoop.io.MapWritable:equals(java.lang.Object),78,94,"/**
* Checks if this MapWritable is equal to the given object.
* @param obj object to compare with
* @return true if objects are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,putAll,org.apache.hadoop.io.MapWritable:putAll(java.util.Map),123,128,"/**
* Adds all key-value pairs from the given map to this writable.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$AppendIfExistsOption:<init>(boolean),939,941,"/**
* Appends existence check option to the current options.
* @param value boolean flag indicating whether to append the option
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$OnlyHeaderOption:<init>(),1889,1891,"/**
* Initializes the OnlyHeaderOption instance with default settings.
* Calls superclass constructor to setup internal state.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$FileOption:<init>(org.apache.hadoop.fs.Path),1852,1854,"/**
* Constructs a new FileOption instance with the specified file path.
* @param value the file path to be used as option value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$FileOption:<init>(org.apache.hadoop.fs.Path),890,892,"/**
* Constructs a new FileOption instance from the specified file path.
* @param path Path to the file option
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BooleanWritable.java,<init>,org.apache.hadoop.io.BooleanWritable:<init>(boolean),41,43,"/**
* Constructs a BooleanWritable instance with the given boolean value.
* @param value boolean value to be written
*/",* @param value value.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BooleanWritable.java,toString,org.apache.hadoop.io.BooleanWritable:toString(),102,105,"/**
* Returns a human-readable representation of this instance.
* @return ""true"" if the underlying value is true, else ""false""
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,waitAsyncValue,"org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:waitAsyncValue(long,java.util.concurrent.TimeUnit)",205,217,"/**
* Waits asynchronously for the value to be ready or times out.
* @param timeout maximum time to wait
* @param unit unit of the timeout (e.g. seconds, milliseconds)
* @throws TimeoutException if timed out waiting for value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail:shouldRetry(java.lang.Exception,int,int,boolean)",225,230,"/**
* Determines if a retry action should be performed.
* @param e exception to evaluate
* @param retries maximum number of retries allowed
* @param failovers maximum number of failover attempts
* @param isIdempotentOrAtMostOnce whether operation is idempotent or at most once
* @return RetryAction indicating decision and reason for retrying
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicy.java,<init>,org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision),49,51,"/**
* Constructs a new RetryAction instance with default values.
* @param decision initial retry decision
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicy.java,<init>,"org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision,long)",53,55,"/**
* Constructs RetryAction with default context.
* @param action decision to retry or not
* @param delayTime time to wait before next attempt
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithFixedSleep:<init>(int,long,java.util.concurrent.TimeUnit)",331,333,"/**
* Configures retry policy with fixed sleep intervals and maximum retries.
* @param maxRetries maximum number of retry attempts
* @param sleepTime duration between retry attempts in specified unit
* @param timeUnit unit of time for sleep interval (e.g. SECONDS, MILLISECONDS)",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry:<init>(int,long,java.util.concurrent.TimeUnit)",627,638,"/**
* Configures exponential backoff retry with specified maximum retries and sleep time.
* @param maxRetries maximum number of retries
* @param sleepTime initial sleep duration in specified time unit
* @param timeUnit time unit for sleep time (e.g. TimeUnit.MILLISECONDS) 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithProportionalSleep:<init>(int,long,java.util.concurrent.TimeUnit)",367,369,"/**
* Initializes retry mechanism with proportional backoff.
* @param maxRetries maximum number of retries
* @param sleepTime initial sleep duration (in specified time unit)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int)",669,672,"/**
* Initializes a retry policy with failover on network exception.
* @param fallbackPolicy default retry policy to use
* @param maxFailovers maximum number of failovers allowed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int,long,long)",674,677,"/**
* Constructs FailoverOnNetworkExceptionRetry with default initial delay.
* @param fallbackPolicy Retry policy to use when failing over
* @param maxFailovers Maximum number of failovers allowed
* @param maxDelayBase Base for calculating maximum delay
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,"org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int,int,long,long)",217,222,"/**
* Returns a retry policy that fails over to fallback policy on network exceptions.
* @param fallbackPolicy default policy for non-network failures
* @param maxFailovers maximum number of failover attempts allowed
* @param maxRetries maximum total number of retries allowed
* @param delayMillis initial delay between retries in millis
* @param maxDelayBase base multiplier for exponential backoff
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryByRemoteException,"org.apache.hadoop.io.retry.RetryPolicies:retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",177,181,"/**
* Creates a custom retry policy based on remote exceptions.
* @param defaultPolicy the default retry policy
* @param exceptionToPolicyMap map of exception types to custom policies
*/","* <p>
   * A retry policy for RemoteException
   * Set a default policy with some explicit handlers for specific exceptions.
   * </p>
   *
   * @param defaultPolicy defaultPolicy.
   * @param exceptionToPolicyMap exceptionToPolicyMap.
   * @return RetryPolicy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$RemoteExceptionDependentRetry:shouldRetry(java.lang.Exception,int,int,boolean)",582,594,"/**
* Applies retry policy based on exception type or default policy.
* @param e the exception to determine policy for
* @param retries number of allowed retries
* @param failovers number of allowed failovers
* @param isIdempotentOrAtMostOnce whether operation is idempotent
* @return RetryAction indicating retry decision
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,<init>,"org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:<init>(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler,org.apache.hadoop.io.retry.AsyncCallHandler)",237,243,"/**
* Initializes an asynchronous method invocation with the given parameters.
* @param method target method to invoke
* @param args arguments for the method
* @param isRpc whether to use RPC protocol
* @param callId unique ID for this invocation
* @param retryInvocationHandler handler for retries
* @param asyncCallHandler handler for asynchronous calls
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,processWaitTimeAndRetryInfo,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:processWaitTimeAndRetryInfo(),268,277,"/**
* Processes retry information and returns call result based on wait time.
* @return CallReturn enum indicating whether to wait or retry
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:<init>(long,org.apache.hadoop.io.retry.RetryPolicy$RetryAction,long,java.lang.Exception)",250,257,"/**
* Initializes retry information with specified delay, action, and failure details.
* @param delay time to wait before next attempt
* @param action operation to perform on retry
* @param expectedFailoverCount anticipated number of retries
* @param failException exception thrown during initial attempt
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,isEmpty,org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:isEmpty(long),96,99,"/**
* Checks if an empty timeout period has elapsed.
* @param time duration to wait before considering empty
*/",Is the queue empty for more than the given time in millisecond?,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,checkEmpty,org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:checkEmpty(),106,110,"/**
* Checks if queue is empty and updates start time of emptiness.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,clearNameMaps,org.apache.hadoop.security.ShellBasedIdMapping:clearNameMaps(),154,159,"/**
* Clears name maps and updates last update time.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,isExpired,org.apache.hadoop.security.ShellBasedIdMapping:isExpired(),161,163,"/**
* Checks if the resource has expired based on time elapsed since last update.
* @return true if expired, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$MetricsUpdateRunner:run(),4207,4223,"/**
* Updates metrics by calculating requests per second.
* @param none
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,now,org.apache.hadoop.util.SysInfoWindows:now(),61,64,"/**
* Returns the current monotonic time in milliseconds.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Timer.java,monotonicNow,org.apache.hadoop.util.Timer:monotonicNow(),50,50,"/**
* Returns the current time in nanoseconds.
*/","* Current time from some arbitrary time base in the past, counting in
   * milliseconds, and not affected by settimeofday or similar system clock
   * changes.  This is appropriate to use when computing how much longer to
   * wait for an interval to expire.
   * @return a monotonic clock that counts in milliseconds.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryOtherThanRemoteAndSaslException,"org.apache.hadoop.io.retry.RetryPolicies:retryOtherThanRemoteAndSaslException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",194,199,"/**
* Creates a custom retry policy based on exceptions.
* @param defaultPolicy default retry policy
* @param exceptionToPolicyMap map of exception classes to custom policies
*/","* <p>
   * A retry policy where RemoteException and SaslException are not retried, other individual
   * exception types can have RetryPolicy overrides, and any other exception type without an
   * override is not retried.
   * </p>
   *
   * @param defaultPolicy defaultPolicy.
   * @param exceptionToPolicyMap exceptionToPolicyMap.
   * @return RetryPolicy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/DefaultFailoverProxyProvider.java,getProxy,org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:getProxy(),45,48,"/**
* Returns proxy information with specified data.
* @return ProxyInfo object containing proxy details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.RetryInvocationHandler:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",332,338,"/**
* Constructs a retry invocation handler with custom policies.
* @param proxyProvider FailoverProxyProvider instance
* @param defaultPolicy Default retry policy for all methods
* @param methodNameToPolicyMap Map of method names to custom retry policies
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,close,org.apache.hadoop.io.retry.RetryInvocationHandler:close(),457,460,"/**
* Closes the underlying proxy descriptor.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/FailoverProxyProvider.java,getString,org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:getString(java.lang.String),50,52,"/**
* Constructs a string representation of a method call.
* @param methodName name of the method to be called
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/FailoverProxyProvider.java,toString,org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:toString(),54,57,"/**
* Returns human-readable representation of this proxy object.
* @return string describing the proxy and its information.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,getFailoverCount,org.apache.hadoop.io.retry.RetryInvocationHandler:getFailoverCount(),345,347,"/**
* Retrieves failover count from the proxy descriptor.
* @return Failover count as a long value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invokeMethod,"org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])",432,443,"/**
* Invokes a method on the proxy descriptor's proxy object.
* @param method target method to invoke
* @param args arguments for the method invocation
* @return result of the method invocation or null if not accessible
* @throws Throwable any exception thrown by the invoked method
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getCallId,org.apache.hadoop.ipc.Client:getCallId(),137,139,"/**
* Retrieves and returns the current call ID.
* If no call ID is present, generates a new one. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client$Call:<init>(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable)",287,307,"/**
* Initializes a Call object with RPC parameters and call details.
* @param rpcKind the kind of RPC operation
* @param param the RPC request parameter
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,getConnectionId,org.apache.hadoop.io.retry.RetryInvocationHandler:getConnectionId(),462,465,"/**
* Retrieves connection ID for proxy instance.
* @return unique connection identifier or null if not applicable
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProxyCombiner.java,getConnectionId,org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:getConnectionId(),122,125,"/**
 * Retrieves the connection ID associated with the first proxy. 
 */","* Since this is incapable of returning multiple connection IDs, simply
     * return the first one. In most cases, the connection ID should be the same
     * for all proxies.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,isDone,org.apache.hadoop.io.retry.AsyncCallHandler$1:isDone(),226,228,"/**
 * Checks whether the asynchronous operation has completed.
 * @return true if done, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,hashCode,org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:hashCode(),451,454,"/**
* Calculates hash code based on string representation of this object.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,equals,org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:equals(java.lang.Object),456,464,"/**
* Compares this object with another for equality.
* @param that the object to compare with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,parseCommaSeparatedString,org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parseCommaSeparatedString(java.lang.String),483,514,"/**
* Parses a comma-separated string into MultipleLinearRandomRetry object.
* @param s input string containing comma-separated elements
*/","* Parse the given string as a MultipleLinearRandomRetry object.
     * The format of the string is ""t_1, n_1, t_2, n_2, ..."",
     * where t_i and n_i are the i-th pair of sleep time and number of retries.
     * Note that the white spaces in the string are ignored.
     *
     * @param s input string.
     * @return the parsed object, or null if the parsing fails.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryByException,"org.apache.hadoop.io.retry.RetryPolicies:retryByException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",162,165,"/**
* Creates a custom retry policy based on exceptions.
* @param defaultPolicy default retry policy
* @param exceptionToPolicyMap map of exception types to custom policies
* @return custom ExceptionDependentRetry policy instance
*/","* <p>
   * Set a default policy with some explicit handlers for specific exceptions.
   * </p>
   *
   * @param exceptionToPolicyMap exceptionToPolicyMap.
   * @param defaultPolicy defaultPolicy.
   * @return RetryPolicy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,calculateExponentialTime,"org.apache.hadoop.io.retry.RetryPolicies:calculateExponentialTime(long,int)",769,771,"/**
* Calculates exponential backoff time based on retries.
* @param time initial time value
* @param retries number of attempts to wait before retrying
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,getReason,org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:getReason(),353,356,"/**
* Constructs and returns reason string based on max time and unit.
* @return reason string representation of max time and unit.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,getReason,org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:getReason(),293,295,"/**
* Constructs reason string based on maximum retries.
* @param maxRetries maximum number of retry attempts
* @return constructed reason string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,hashCode,org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:hashCode(),305,308,"/**
* Computes hash code by delegating to String's hashCode.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,equals,org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:equals(java.lang.Object),310,318,"/**
* Compares this object with the specified object for equality.
* @param that the object to compare with
* @return true if both objects are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MultipleIOException.java,createIOException,org.apache.hadoop.io.MultipleIOException:createIOException(java.util.List),50,58,"/**
* Creates an IOException from a list of exceptions.
* @param exceptions list of individual IOExceptions
* @return single IOException or MultipleIOException if multiple
*/","* A convenient method to create an {@link IOException}.
   * @param exceptions IOException List.
   * @return IOException.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCompressionAlgorithmByName,org.apache.hadoop.io.file.tfile.Compression:getCompressionAlgorithmByName(java.lang.String),359,370,"/**
* Retrieves an Algorithm instance by its name.
* @param compressName unique name of the compression algorithm
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getSupportedAlgorithms,org.apache.hadoop.io.file.tfile.Compression:getSupportedAlgorithms(),372,382,"/**
* Retrieves an array of supported algorithms.
* @return Array of names of supported algorithms
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressionName,org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:getCompressionName(),524,526,"/**
* Returns compression algorithm name.
* @return Name of compression algorithm in use (e.g. ""GZIP"")
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getBlockCount,org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockCount(),687,689,"/**
* Returns the count of block regions in data index.
* @return number of block regions
*/","* Get the number of data blocks.
     * 
     * @return the number of data blocks.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,readAndVerify,org.apache.hadoop.io.file.tfile.BCFile$Magic:readAndVerify(java.io.DataInput),920,929,"/**
* Verifies the magic bytes of a BCFile by reading and comparing them to the expected value.
* @param in DataInput stream from which to read the magic bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFileDumper.java,format,"org.apache.hadoop.io.file.tfile.TFileDumper$Align:format(long,int,org.apache.hadoop.io.file.tfile.TFileDumper$Align)",73,78,"/**
* Formats a long integer value with optional zero padding.
* @param l long integer value to format
* @param width minimum field width
* @param align alignment strategy (ZERO_PADDED or other)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,addEntry,org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:addEntry(org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry),782,784,"/**
* Adds an entry to the meta index.
* @param indexEntry MetaIndexEntry object to be added
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getDefaultCompressionAlgorithm,org.apache.hadoop.io.file.tfile.BCFile$Writer:getDefaultCompressionAlgorithm(),341,343,"/**
* Retrieves default compression algorithm from data index.
* @return CompressionAlgorithm instance or null if unavailable
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getDefaultCompressionName,org.apache.hadoop.io.file.tfile.BCFile$Reader:getDefaultCompressionName(),652,654,"/**
* Returns the name of the default compression algorithm.
* @return Name of the default compression algorithm or null if not set
*/","* Get the name of the default compression algorithm.
     * 
     * @return the name of the default compression algorithm.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,hashCode,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:hashCode(),1973,1976,"/**
* Generates hash code based on key buffer contents.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/CompareUtils.java,compare,"org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator:compare(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",45,49,"/**
* Compares two RawComparable objects based on their underlying data.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:<init>(java.io.DataInput),948,952,"/**
* Initializes BlockRegion from input data stream.
* @param in DataInput object containing block region data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,readVInt,org.apache.hadoop.io.file.tfile.Utils:readVInt(java.io.DataInput),177,184,"/**
* Reads a variable-length integer from the given input stream.
* @param in data input stream
*/","* Decoding the variable-length integer. Synonymous to
   * <code>(int)Utils#readVLong(in)</code>.
   * 
   * @param in
   *          input stream
   * @return the decoded integer
   * @throws IOException raised on errors performing I/O.
   * 
   * @see Utils#readVLong(DataInput)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,makeComparator,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:makeComparator(java.lang.String),2070,2096,"/**
* Creates a BytesComparator instance based on the provided comparison strategy.
* @param comparator unique identifier for the comparison strategy (e.g. Memcmp, JClass)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:write(java.io.DataOutput),960,964,"/**
* Writes metadata (offset, compressed size, and raw size) to output stream. 
* @param out DataOutput stream for writing data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,writeVInt,"org.apache.hadoop.io.file.tfile.Utils:writeVInt(java.io.DataOutput,int)",55,57,"/**
 * Writes a variable-length integer to the output stream.
 * @param out DataOutput stream
 * @param n integer value to be written
 */","* Encoding an integer into a variable-length encoding format. Synonymous to
   * <code>Utils#writeVLong(out, n)</code>.
   * 
   * @param out
   *          output stream
   * @param n
   *          The integer to be encoded
   * @throws IOException raised on errors performing I/O.
   * @see Utils#writeVLong(DataOutput, long)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,isSorted,org.apache.hadoop.io.file.tfile.TFile$Reader:isSorted(),864,866,"/**
* Checks if the underlying file metadata is sorted.
* @return true if sorted, false otherwise
*/","* Is the TFile sorted?
     * 
     * @return true if TFile is sorted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCodec,org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:getCodec(),273,273,"/**
* Retrieves the compression codec used by this instance. 
* @throws IOException if an I/O error occurs during codec retrieval. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,equals,org.apache.hadoop.io.file.tfile.Utils$Version:equals(java.lang.Object),395,400,"/**
* Compares this version with another for equality.
* @param other the other version to compare with
* @return true if versions are equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getEntryCount,org.apache.hadoop.io.file.tfile.TFile$Reader:getEntryCount(),873,875,"/**
* Returns total entry count from file metadata.
* @return number of records in the file
*/","* Get the number of key-value pair entries in TFile.
     * 
     * @return the number of key-value pairs in TFile",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Reader:close(),824,827,"/**
* Closes the BCF reader resource.
*/","* Close the reader. The state of the Reader object is undefined after
     * close. Calling close() for multiple times has no effect.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getComparatorName,org.apache.hadoop.io.file.tfile.TFile$Reader:getComparatorName(),855,857,"/**
* Returns comparator name from file metadata.
* @return Comparator string value or null if not available.","* Get the string representation of the comparator.
     * 
     * @return If the TFile is not sorted by keys, an empty string will be
     *         returned. Otherwise, the actual comparator string that is
     *         provided during the TFile creation time will be returned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/ByteArray.java,<init>,org.apache.hadoop.io.file.tfile.ByteArray:<init>(org.apache.hadoop.io.BytesWritable),40,42,"/**
 * Copies byte array from another BytesWritable instance.
 * @param other source BytesWritable object
 */","* Constructing a ByteArray from a {@link BytesWritable}.
   * 
   * @param other other.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/ByteArray.java,<init>,org.apache.hadoop.io.file.tfile.ByteArray:<init>(byte[]),50,52,"/**
 * Initializes a new instance of ByteArray with the specified byte array.
 * @param buffer the byte array to initialize from
 */","* Wrap a whole byte array as a RawComparable.
   * 
   * @param buffer
   *          the byte array buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getBlockEntryCount,org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockEntryCount(int),2031,2033,"/**
* Retrieves entry count within block with ID 'curBid'.
* @param curBid unique block identifier
* @return number of entries or 0 if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,addEntry,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:addEntry(org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry),2262,2266,"/**
* Adds an entry to the index and updates summary statistics.
* @param keyEntry Filled TFileIndexEntry object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,register,"org.apache.hadoop.io.file.tfile.BCFile$Writer$DataBlockRegister:register(long,long,long)",468,471,"/**
* Registers a block region with specified data and size.
* @param raw raw data value
* @param begin start position of the block
* @param end end position of the block (exclusive)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getBlockIndexNear,org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockIndexNear(long),745,756,"/**
* Finds the index of the block region closest to the given offset.
* @param offset long value representing the position
* @return index of nearest block region or -1 if not found
*/","* Find the smallest Block index whose starting offset is greater than or
     * equal to the specified offset.
     * 
     * @param offset
     *          User-specific offset.
     * @return the index to the data Block if such block exists; or -1
     *         otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,lowerBound,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:lowerBound(org.apache.hadoop.io.file.tfile.RawComparable),2187,2201,"/**
* Finds the lower bound of a sorted collection for a given key.
* @param key RawComparable object to search for
* @return Index of the matching element or -1 if not found
*/","* @param key
     *          input key.
     * @return the ID of the first block that contains key >= input key. Or -1
     *         if no such block exists.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.java,write,org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:write(int),45,51,"/**
* Writes a single byte to the buffer, potentially triggering a flush.
* @param b the byte to be written
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.java,write,"org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:write(byte[],int,int)",53,65,"/**
* Writes bytes to the output stream, handling buffer overflow.
* @param b byte array
* @param off offset into the array
* @param len number of bytes to write
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.java,flush,org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:flush(),67,71,"/**
 * Flushes buffered output and underlying writer.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,upperBound,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:upperBound(org.apache.hadoop.io.file.tfile.RawComparable),2209,2223,"/**
* Returns the upper bound index of a given key in the sorted TFile.
* @param key RawComparable object to search for
* @return index of matching element or -1 if not found
*/","* @param key
     *          input key.
     * @return the ID of the first block that contains key > input key. Or -1
     *         if no such block exists.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNumByLocation,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getRecordNumByLocation(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),2244,2248,"/**
* Calculates the record number based on a given location.
* @param location Reader.Location object containing block and record indices
* @return calculated record number as a long integer value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:compareTo(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),741,744,"/**
* Compares this Location with another based on block and record indices.
* @param other Location object to compare with
*/",* @see java.lang.Comparable#compareTo(java.lang.Object),,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Location:<init>(int,long)",705,707,"/**
* Initializes location with specified block and record indices.
* @param blockIndex block index within the file
* @param recordIndex record index within the block
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,set,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:set(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),734,736,"/**
 * Sets location by copying another Location object's block and record indices.
 * @param other Location to copy from
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKey,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(byte[]),1775,1777,"/**
* Retrieves key from byte array.
* @param buf byte array containing key data
* @return key integer value or throws IOException if failed
*/","* Copy the key into user supplied buffer.
         * 
         * @param buf
         *          The buffer supplied by user. The length of the buffer must
         *          not be shorter than the key length.
         * @return The length of the key.
         * 
         * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getValue,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(byte[],int)",1859,1890,"/**
* Reads value from input stream into buffer, handling known and unknown length cases.
* @param buf target byte array
* @param offset starting position in the buffer
* @return actual number of bytes read or written
*/","* Copy value into user-supplied buffer. User supplied buffer must be
         * large enough to hold the whole value (starting from the offset). The
         * value part of the key-value pair pointed by the current cursor is not
         * cached and can only be examined once. Calling any of the following
         * functions more than once without moving the cursor will result in
         * exception: {@link #getValue(byte[])}, {@link #getValue(byte[], int)},
         * {@link #getValueStream}.
         *
         * @param buf buf.
         * @param offset offset.
         * @return the length of the value. Does not require
         *         isValueLengthKnown() to be true.
         * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:<init>(org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState),549,552,"/**
 * Initializes a BlockReader with a given RBlockState object.
 * @param rbs RBlockState instance to read from
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getRawSize,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getRawSize(),585,587,"/**
* Returns the raw size of the block region.
* @return Raw size in bytes
*/","* Get the uncompressed size of the block.
       * 
       * @return uncompressed size of the block.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressedSize,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getCompressedSize(),594,596,"/**
* Returns compressed size of current block region.
* @return Compressed size in bytes
*/","* Get the compressed size of the block.
       * 
       * @return compressed size of the block.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getStartPos,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getStartPos(),603,605,"/**
* Retrieves the start position of the current block region.
* @return The offset of the current block region
*/","* Get the starting position of the block in the file.
       * 
       * @return the starting position of the block in the file.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:write(byte[]),394,397,"/**
* Writes entire byte array to output stream.
* @param b input byte array
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputOutputStream.java,constructOutputStream,org.apache.hadoop.io.DataOutputOutputStream:constructOutputStream(java.io.DataOutput),43,49,"/**
* Wraps a DataOutput instance as an OutputStream.
* @param out DataOutput to be wrapped
* @return OutputStream wrapper or original OutputStream if already one
*/","* Construct an OutputStream from the given DataOutput. If 'out'
   * is already an OutputStream, simply returns it. Otherwise, wraps
   * it in an OutputStream.
   * @param out the DataOutput to wrap
   * @return an OutputStream instance that outputs to 'out'",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/FloatWritable.java,<init>,org.apache.hadoop.io.FloatWritable:<init>(float),34,34,"/**
* Initializes a new FloatWritable with the specified numeric value.
* @param value the numeric value to be stored in the writable object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/FastByteComparisons.java,compareTo,"org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer:compareTo(byte[],int,int,byte[],int,int)",188,242,"/**
* Compares two byte arrays lexicographically.
* @param buffer1 first byte array
* @param offset1 offset in buffer1 to start comparison
* @param length1 length of comparison in buffer1
* @param buffer2 second byte array
* @param offset2 offset in buffer2 to start comparison
* @param length2 length of comparison in buffer2
* @return negative/positive value if first/second array is less/greater, or 0 for equal","* Lexicographically compare two arrays.
       *
       * @param buffer1 left operand
       * @param buffer2 right operand
       * @param offset1 Where to start comparing in the left buffer
       * @param offset2 Where to start comparing in the right buffer
       * @param length1 How much to compare from the left buffer
       * @param length2 How much to compare from the right buffer
       * @return 0 if equal, < 0 if left is less than right, etc.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,<init>,org.apache.hadoop.io.DataOutputBuffer:<init>(),89,91,"/**
* Initializes a new instance of DataOutputBuffer with default settings.
*/",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,<init>,org.apache.hadoop.io.DataOutputBuffer:<init>(int),93,95,"/**
* Initializes a new DataOutputBuffer instance with the specified size.
* @param size buffer size in bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,getData,org.apache.hadoop.io.DataOutputBuffer:getData(),108,108,"/**
* Retrieves raw data from internal buffer.
* @return raw data as a byte array
*/","* Returns the current contents of the buffer.
   *  Data is only valid to {@link #getLength()}.
   *
   * @return data byte.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,getLength,org.apache.hadoop.io.DataOutputBuffer:getLength(),114,114,"/**
* Returns the length of the underlying data buffer.
* @return Length of the buffer in bytes
*/","* Returns the length of the valid data currently in the buffer.
   * @return length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,writeInt,"org.apache.hadoop.io.DataOutputBuffer:writeInt(int,int)",154,164,"/**
* Writes a 32-bit integer value to the buffer.
* @param v integer value to write
* @param offset current buffer position
*/","* Overwrite an integer into the internal buffer. Note that this call can only
   * be used to overwrite existing data in the buffer, i.e., buffer#count cannot
   * be increased, and DataOutputStream#written cannot be increased.
   *
   * @param v v.
   * @param offset offset.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNSDomainNameResolver.java,getHostnameByIP,org.apache.hadoop.net.DNSDomainNameResolver:getHostnameByIP(java.net.InetAddress),44,62,"/**
* Resolves an IP address to its hostname.
* @param address InetAddress object
* @return Hostname as a string, or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getDistance,"org.apache.hadoop.net.NetworkTopology:getDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",324,365,"/**
* Calculates the distance between two nodes in a tree structure.
* @param node1 first node
* @param node2 second node
* @return distance between the nodes, or MAX_VALUE if one of the nodes is null or not found
*/","Return the distance between two nodes
   * It is assumed that the distance from one node to its parent is 1
   * The distance between two nodes is calculated by summing up their distances
   * to their closest common ancestor.
   * @param node1 one node
   * @param node2 another node
   * @return the distance between node1 and node2 which is zero if they are the same
   *  or {@link Integer#MAX_VALUE} if node1 or node2 do not belong to the cluster",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,isNodeInScope,"org.apache.hadoop.net.NetworkTopology:isNodeInScope(org.apache.hadoop.net.Node,java.lang.String)",1023,1029,"/**
* Checks if a node is within the specified scope.
* @param node Node object to check
* @param scope Scope string (e.g., directory path)
* @return true if node is in scope, false otherwise
*/","* Checks whether a node belongs to the scope.
   * @param node  the node to check.
   * @param scope scope to check.
   * @return true if node lies within the scope",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,getPathComponents,org.apache.hadoop.net.NodeBase:getPathComponents(org.apache.hadoop.net.Node),124,126,"/**
* Splits the full path of a Node into individual components.
* @param node Node object with associated path
* @return array of path components or null if invalid
*/","* Get the path components of a node.
   * @param node a non-null node
   * @return the path of a node",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,equals,org.apache.hadoop.net.NodeBase:equals(java.lang.Object),128,137,"/**
* Compares this NodeBase instance with another object for equality.
* Two nodes are considered equal if they have the same path.
* @param to the object to compare with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,hashCode,org.apache.hadoop.net.NodeBase:hashCode(),139,142,"/**
* Computes hash code by delegating to getPath method.
* @return hash code of path or default hash code if null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,toString,org.apache.hadoop.net.NodeBase:toString(),145,148,"/**
* Returns a string representation of this object.
* @return Path as a string
*/",@return this node's path as its string representation,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,remove,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:remove(org.apache.hadoop.net.Node),228,254,"/**
* Removes a node from the network topology, updating internal state and logging.
* @param node Node to remove
*/","Remove a node
   * Update node counter and rack counter if necessary
   * @param node node to be removed; can be null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getDatanodesInRack,org.apache.hadoop.net.NetworkTopology:getDatanodesInRack(java.lang.String),202,215,"/**
* Retrieves a list of datanodes within the specified rack location.
* @param loc rack location string
*/","* Given a string representation of a rack, return its children
   * @param loc a path-like string representation of a rack
   * @return a newly allocated list with all the node's children",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getNode,org.apache.hadoop.net.NetworkTopology:getNode(java.lang.String),271,281,"/**
* Retrieves a node by location.
* @param loc unique node location identifier
* @return Node object or null if not found
*/","Given a string representation of a node, return its reference
   * 
   * @param loc
   *          a path-like string representation of a node
   * @return a reference to the node; null if the node is not in the tree",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,locationToDepth,org.apache.hadoop.net.NodeBase:locationToDepth(java.lang.String),209,219,"/**
* Calculates the directory depth from a normalized location string.
* @param location input path to calculate depth for
* @return directory depth or -1 if invalid location
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,toString,org.apache.hadoop.net.NetworkTopology:toString(),714,732,"/**
* Returns a string representation of the object, including number of racks and nodes.
*/",convert a network tree to a string.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,isOnSameRack,"org.apache.hadoop.net.NetworkTopology:isOnSameRack(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",409,415,"/**
* Checks if two nodes are on the same rack.
* @param node1 first node to compare
* @param node2 second node to compare
* @return true if both nodes have the same parent(s), false otherwise
*/","Check if two nodes are on the same rack
   * @param node1 one node (can be null)
   * @param node2 another node (can be null)
   * @return true if node1 and node2 are on the same rack; false otherwise
   * @exception IllegalArgumentException when either node1 or node2 is null, or
   * node1 or node2 do not belong to the cluster",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,"org.apache.hadoop.net.NetworkTopology:chooseRandom(org.apache.hadoop.net.InnerNode,org.apache.hadoop.net.Node,java.util.Collection,int,int)",569,637,"/**
* Selects a random node from the scope, excluding specified nodes.
* @param parentNode Inner node containing leaf nodes
* @param excludedScopeNode Node to exclude from scope
* @param excludedNodes Collection of nodes to exclude from selection
* @return Randomly chosen node or null if not found
*/","* Randomly choose one node under <i>parentNode</i>, considering the exclude
   * nodes and scope. Should be called with {@link #netlock}'s readlock held.
   *
   * @param parentNode        the parent node
   * @param excludedScopeNode the node corresponding to the exclude scope.
   * @param excludedNodes     a collection of nodes to be excluded from
   * @param totalInScopeNodes total number of nodes under parentNode, excluding
   *                          the excludedScopeNode
   * @param availableNodes    number of available nodes under parentNode that
   *                          could be chosen, excluding excludedNodes
   * @return the chosen node, or null if none can be chosen",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getWeightUsingNetworkLocation,"org.apache.hadoop.net.NetworkTopology:getWeightUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",807,845,"/**
* Calculates network distance between two nodes using their locations.
* @param reader Node containing the starting location
* @param node Node containing the target location
* @return Network distance (weight) between the nodes, or Integer.MAX_VALUE if invalid
*/","* Returns an integer weight which specifies how far away <i>node</i> is
   * from <i>reader</i>. A lower value signifies that a node is closer.
   * It uses network location to calculate the weight
   *
   * @param reader Node where data will be read
   * @param node Replica of data
   * @return weight",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,interAddNodeWithEmptyRack,org.apache.hadoop.net.NetworkTopology:interAddNodeWithEmptyRack(org.apache.hadoop.net.Node),1083,1097,"/**
* Adds a node to the empty rack set and updates rack map.
* @param node Node object to add
*/","* Internal function for update empty rack number
   * for add or recommission a node.
   * @param node node to be added; can be null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,<init>,"org.apache.hadoop.net.SocketIOWithTimeout:<init>(java.nio.channels.SelectableChannel,long)",58,66,"/**
 * Initializes a Socket with the specified Selectable Channel and timeout.
 * @param channel underlying network channel
 * @param timeout connection timeout in milliseconds
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,write,"org.apache.hadoop.net.SocketOutputStream:write(byte[],int,int)",111,129,"/**
* Writes bytes to the underlying stream in chunks, handling potential IOExceptions. 
* @param b byte array to write
* @param off offset into b to start writing from
* @param len number of bytes to write from b
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,transferToFully,"org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.LongWritable)",202,251,"/**
* Transfers data from a file channel to another, tracking time spent waiting and transferring. 
* @param fileCh source FileChannel
* @param position current transfer position
* @param count remaining bytes to be transferred
* @param waitForWritableTime optional LongWritable for wait time (if null, not tracked)
* @param transferToTime optional LongWritable for total transfer time (if null, not tracked) 
*/","* Transfers data from FileChannel using 
   * {@link FileChannel#transferTo(long, long, WritableByteChannel)}.
   * Updates <code>waitForWritableTime</code> and <code>transferToTime</code>
   * with the time spent blocked on the network and the time spent transferring
   * data from disk to network respectively.
   * 
   * Similar to readFully(), this waits till requested amount of 
   * data is transfered.
   * 
   * @param fileCh FileChannel to transfer data from.
   * @param position position within the channel where the transfer begins
   * @param count number of bytes to transfer.
   * @param waitForWritableTime nanoseconds spent waiting for the socket 
   *        to become writable
   * @param transferToTime nanoseconds spent transferring data
   * 
   * @throws EOFException 
   *         If end of input file is reached before requested number of 
   *         bytes are transfered.
   *
   * @throws SocketTimeoutException 
   *         If this channel blocks transfer longer than timeout for 
   *         this stream.
   *          
   * @throws IOException Includes any exception thrown by 
   *         {@link FileChannel#transferTo(long, long, WritableByteChannel)}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,normalizeHostNames,org.apache.hadoop.net.NetUtils:normalizeHostNames(java.util.Collection),662,668,"/**
* Normalizes a collection of host names.
* @param names collection of host names to normalize
* @return list of normalized host names
*/","* Given a collection of string representation of hosts, return a list of
   * corresponding IP addresses in the textual representation.
   * 
   * @param names a collection of string representations of hosts
   * @return a list of corresponding IP addresses in the string format
   * @see #normalizeHostName(String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getHostDetailsAsString,"org.apache.hadoop.net.NetUtils:getHostDetailsAsString(java.lang.String,int,java.lang.String)",977,988,"/**
* Formats destination and local host details as a string.
* @param destHost destination hostname
* @param destPort destination port number
* @param localHost local hostname
* @return formatted host details string
*/","* Get the host details as a string
   * @param destHost destinatioon host (nullable)
   * @param destPort destination port
   * @param localHost local host (nullable)
   * @return a string describing the destination host:port and the local host",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getIPs,"org.apache.hadoop.net.NetUtils:getIPs(java.lang.String,boolean)",1041,1068,"/**
* Retrieves a list of IP addresses within the specified subnet.
* @param subnet network address and mask
* @param returnSubinterfaces whether to include subinterface IPs
* @return List of InetAddress objects or an empty list if none found
*/","* Return an InetAddress for each interface that matches the
   * given subnet specified using CIDR notation.
   *
   * @param subnet subnet specified using CIDR notation
   * @param returnSubinterfaces
   *            whether to return IPs associated with subinterfaces
   * @throws IllegalArgumentException if subnet is invalid
   * @return ips.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getFreeSocketPorts,org.apache.hadoop.net.NetUtils:getFreeSocketPorts(int),1098,1113,"/**
* Retrieves a specified number of available socket ports.
* @param numOfPorts the desired number of free ports
* @return Set of unique, acquired port numbers or throws exception if unable to acquire required number of ports
*/","* Return free ports. There is no guarantee they will remain free, so
   * ports should be used immediately. The number of free ports returned by
   * this method should match argument {@code numOfPorts}. Num of ports
   * provided in the argument should not exceed 25.
   *
   * @param numOfPorts Number of free ports to acquire.
   * @return Free ports for binding a local socket.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,getConf,org.apache.hadoop.net.TableMapping:getConf(),71,74,"/**
* Retrieves configuration from raw mapping.
* @return Configuration object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,setConf,org.apache.hadoop.net.TableMapping:setConf(org.apache.hadoop.conf.Configuration),76,80,"/**
* Sets configuration for this object and its underlying mapping.
* @param conf Configuration to be applied
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:<init>(),172,172,"/**
* Initializes a new instance of RawScriptBasedMapping.","* Constructor. The mapping is not ready to use until
     * {@link #setConf(Configuration)} has been called",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/CachedDNSToSwitchMapping.java,<init>,org.apache.hadoop.net.CachedDNSToSwitchMapping:<init>(org.apache.hadoop.net.DNSToSwitchMapping),50,52,"/**
 * Initializes a new instance of CachedDNSToSwitchMapping with a given raw mapping.
 * @param rawMapping raw DNSToSwitchMapping to cache
 */","* cache a raw DNS mapping
   * @param rawMapping the raw mapping to cache",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,<init>,org.apache.hadoop.net.NodeBase:<init>(java.lang.String),53,61,"/**
* Initializes a NodeBase with the given file system path.
* @param path absolute or relative file path
*/","Construct a node from its path
   * @param path 
   *   a concatenation of this node's location, the path separator, and its name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,<init>,"org.apache.hadoop.net.NodeBase:<init>(java.lang.String,java.lang.String)",67,69,"/**
* Initializes a new NodeBase with given name and location.
* @param name unique identifier for this node
* @param location geographical coordinates of the node
*/","Construct a node from its name and its location
   * @param name this node's name (can be null, must not contain {@link #PATH_SEPARATOR})
   * @param location this node's location",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,<init>,"org.apache.hadoop.net.NodeBase:<init>(java.lang.String,java.lang.String,org.apache.hadoop.net.Node,int)",77,81,"/**
* Constructs a new NodeBase instance with given details.
* @param name node name
* @param location node location (normalized)
* @param parent parent node reference
* @param level node hierarchy level
*/","Construct a node from its name and its location
   * @param name this node's name (can be null, must not contain {@link #PATH_SEPARATOR})
   * @param location this node's location 
   * @param parent this node's parent node
   * @param level this node's level in the tree",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,getConf,org.apache.hadoop.net.ScriptBasedMapping:getConf(),116,119,"/**
* Retrieves configuration from raw mapping.
* @return Configuration object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,toString,org.apache.hadoop.net.ScriptBasedMapping:toString(),121,124,"/**
* Returns human-readable string representation of script-based mapping.
* @return formatted string containing raw mapping details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,read,org.apache.hadoop.net.unix.DomainSocket$DomainChannel:read(java.nio.ByteBuffer),602,628,"/**
* Reads data into the provided ByteBuffer.
* @param dst buffer to fill with read data
* @return number of bytes read, or -1 on error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,write,org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:write(int),563,575,"/**
* Writes an integer value to the underlying socket.
* @param val the value to be written
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,write,"org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:write(byte[],int,int)",577,587,"/**
* Writes an array of bytes to the underlying socket.
* @param b byte array to write
* @param off offset within array
* @param len number of bytes to write
* @throws IOException on write error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,read,org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:read(),507,519,"/**
* Reads a single byte from the socket.
*@throws IOException if an I/O error occurs
*@return the read byte or -1 on error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,read,"org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:read(byte[],int,int)",521,532,"/**
* Reads data from the underlying socket into a byte array.
* @param b target buffer
* @param off starting offset in buffer
* @param len number of bytes to read
* @return actual number of bytes read, or throws IOException if an error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,available,org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:available(),534,545,"/**
* Returns the number of bytes available for reading from the socket.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,<init>,"org.apache.hadoop.net.unix.DomainSocket:<init>(java.lang.String,int)",168,172,"/**
* Initializes a DomainSocket object with file descriptor and path.
* @param path socket path
* @param fd file descriptor
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,sendCallback,"org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)",386,424,"/**
* Handles callback for a file descriptor, closing it if necessary.
* @param caller context of the call
* @param entries map of active file descriptors
* @param fdSet set of active file descriptors
* @return true if fd was closed, false otherwise
*/","* Send callback and return whether or not the domain socket was closed as a
   * result of processing.
   *
   * @param caller reason for call
   * @param entries mapping of file descriptor to entry
   * @param fdSet set of file descriptors
   * @param fd file descriptor
   * @return true if the domain socket was closed as a result of processing",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,isOpen,org.apache.hadoop.net.unix.DomainSocket:isOpen(),268,270,"/**
* Checks if the underlying resource is open.
* @return true if the resource is currently open, false otherwise
*/","* Return true if the file descriptor is currently open.
   *
   * @return                 True if the file descriptor is currently open.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket:close(),344,388,"/**
* Closes the DomainSocket and its underlying file descriptor, releasing system resources.
*/",* Close the Socket.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,addNotificationSocket,"org.apache.hadoop.net.unix.DomainSocketWatcher:addNotificationSocket(java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet)",546,561,"/**
* Adds notification socket and updates FD set.
* @param entries map of file descriptors
* @param fdSet set of active file descriptors
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,trimIdleSelectors,org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:trimIdleSelectors(long),426,444,"/**
* Trims idle selectors from the provider map.
* @param now current timestamp
*/","* Closes selectors that are idle for IDLE_TIMEOUT (10 sec). It does not
     * traverse the whole list, just over the one that have crossed 
     * the timeout.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,isLeafParent,org.apache.hadoop.net.InnerNodeImpl:isLeafParent(),302,304,"/**
 * Checks if current node is a leaf parent (i.e., has no children). 
 * @return true if current node is a leaf parent, false otherwise
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,getNextAncestorName,org.apache.hadoop.net.InnerNodeImpl:getNextAncestorName(org.apache.hadoop.net.Node),113,127,"/**
* Retrieves the next ancestor's name from a given Node.
* @param n Node to fetch ancestor information from
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.net.InetAddress,int)",70,76,"/**
* Creates and connects a new Socket to the specified address and port.
* @param addr remote host InetAddress
* @param port remote port number
* @return connected Socket object or throws IOException on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",78,86,"/**
* Creates a connected socket with the specified remote and local addresses.
* @param addr remote server address
* @param port remote server port
* @param localHostAddr local host address
* @param localPort local port to bind
* @return established Socket object or throws IOException if creation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.lang.String,int)",88,95,"/**
* Establishes a new TCP connection to the specified host on the given port.
* @param host hostname or IP address of the server
* @param port port number to connect to
* @return connected Socket object or throws exception if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",97,106,"/**
* Creates a new socket and connects it to the specified host and port.
* @param host remote server hostname
* @param port remote server port number
* @param localHostAddr local server IP address
* @param localPort local server port number
* @return created Socket object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.net.InetAddress,int)",65,71,"/**
* Creates and connects a new socket to the specified address and port.
* @param addr remote server's InetAddress
* @param port remote server's port number
* @return connected Socket object or throws IOException if creation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",73,81,"/**
* Creates a new socket with specified remote and local addresses.
* @param addr remote server address
* @param port remote server port
* @param localHostAddr local host address
* @param localPort local port number
* @return created Socket object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.lang.String,int)",83,90,"/**
* Creates and connects a Socket to the specified host and port.
* @param host target hostname or IP address
* @param port target port number
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",92,101,"/**
* Creates a new TCP socket and connects it to the specified host and port.
* @param host remote server hostname
* @param port remote server port number
* @param localHostAddr local server address
* @param localPort local server port number
* @return connected Socket object or throws IOException/UnknownHostException if failed",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,toString,org.apache.hadoop.net.ScriptBasedMappingWithDependency:toString(),71,74,"/**
* Returns a string representation of this script-based mapping.
* @return human-readable description of the mapping
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,getDependency,org.apache.hadoop.net.ScriptBasedMappingWithDependency:getDependency(java.lang.String),96,115,"/**
* Retrieves a list of dependencies for the given name, normalized to an IP address.
* @param name host name to fetch dependencies for
* @return List of string dependencies or empty list if not found
*/","* Get dependencies in the topology for a given host
   * @param name - host name for which we are getting dependency
   * @return a list of hosts dependent on the provided host name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputWrapper.java,setTimeout,org.apache.hadoop.net.SocketInputWrapper:setTimeout(long),69,75,"/**
* Sets read timeout for the underlying socket.
* @param timeoutMs milliseconds to wait before timing out
*/","* Set the timeout for reads from this stream.
   * 
   * Note: the behavior here can differ subtly depending on whether the
   * underlying socket has an associated Channel. In particular, if there is no
   * channel, then this call will affect the socket timeout for <em>all</em>
   * readers of this socket. If there is a channel, then this call will affect
   * the timeout only for <em>this</em> stream. As such, it is recommended to
   * only create one {@link SocketInputWrapper} instance per socket.
   * 
   * @param timeoutMs
   *          the new timeout, 0 for no timeout
   * @throws SocketException
   *           if the timeout cannot be set",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getIPs,"org.apache.hadoop.net.DNS:getIPs(java.lang.String,boolean)",175,209,"/**
* Retrieves IP addresses for the specified network interface.
* @param strInterface name of the network interface
* @return array of IP addresses, or cached host address if default interface
*/","* Returns all the IPs associated with the provided interface, if any, in
   * textual form.
   * 
   * @param strInterface
   *            The name of the network interface or sub-interface to query
   *            (eg eth0 or eth0:0) or the string ""default""
   * @param returnSubinterfaces
   *            Whether to return IPs associated with subinterfaces of
   *            the given interface
   * @return A string vector of all the IPs associated with the provided
   *         interface. The local host IP is returned if the interface
   *         name ""default"" is specified or there is an I/O error looking
   *         for the given interface.
   * @throws UnknownHostException
   *             If the given interface is invalid
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getIPsAsInetAddressList,"org.apache.hadoop.net.DNS:getIPsAsInetAddressList(java.lang.String,boolean)",428,457,"/**
* Retrieves a list of InetAddresses for the specified interface.
* @param strInterface name of the network interface (e.g. ""eth0"")
* @param returnSubinterfaces whether to include subinterface addresses
* @return List of InetAddress objects or an empty list if not found
*/","* Returns all the IPs associated with the provided interface, if any, as
   * a list of InetAddress objects.
   *
   * @param strInterface
   *            The name of the network interface or sub-interface to query
   *            (eg eth0 or eth0:0) or the string ""default""
   * @param returnSubinterfaces
   *            Whether to return IPs associated with subinterfaces of
   *            the given interface
   * @return A list of all the IPs associated with the provided
   *         interface. The local host IP is returned if the interface
   *         name ""default"" is specified or there is an I/O error looking
   *         for the given interface.
   * @throws UnknownHostException
   *             If the given interface is invalid
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,read,"org.apache.hadoop.net.SocketInputStream:read(byte[],int,int)",129,132,"/**
* Reads data into given byte array.
* @param b the byte array to read into
* @param off starting offset
* @param len number of bytes to read
* @return actual number of bytes read
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getRack,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getRack(java.lang.String),57,80,"/**
* Resolves rack location from given string.
* @param loc input location string to resolve
* @return resolved rack string or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getNodeGroup,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getNodeGroup(java.lang.String),90,118,"/**
* Retrieves the node group for a given location.
* @param loc network location string
* @return node group name or null if not found
*/","* Given a string representation of a node group for a specific network
   * location
   * 
   * @param loc
   *            a path-like string representation of a network location
   * @return a node group string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/AbstractDNSToSwitchMapping.java,dumpTopology,org.apache.hadoop.net.AbstractDNSToSwitchMapping:dumpTopology(),112,133,"/**
* Returns a string representation of the network topology.
* @return formatted string containing mapping, switches, and nodes count
*/","* Generate a string listing the switch mapping implementation,
   * the mapping for every known node and the number of nodes and
   * unique switches known about -each entry to a separate line.
   * @return a string that can be presented to the ops team or used in
   * debug messages.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/AbstractDNSToSwitchMapping.java,isMappingSingleSwitch,org.apache.hadoop.net.AbstractDNSToSwitchMapping:isMappingSingleSwitch(org.apache.hadoop.net.DNSToSwitchMapping),150,153,"/**
* Verifies if a DNS-to-switch mapping represents a single switch.
* @param mapping DNS-to-switch mapping to check
* @return true if the mapping is for a single switch, false otherwise
*/","* Query for a {@link DNSToSwitchMapping} instance being on a single
   * switch.
   * <p>
   * This predicate simply assumes that all mappings not derived from
   * this class are multi-switch.
   * @param mapping the mapping to query
   * @return true if the base class says it is single switch, or the mapping
   * is not derived from this class.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getWeight,"org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getWeight(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",256,271,"/**
* Calculates proximity weight between two nodes.
* @param reader Node to calculate weight from
* @param node Node to calculate weight for
* @return Proximity weight (0: same node, 1: same group, 2: same rack, 3: off rack)",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,writeAttribute,"org.apache.hadoop.jmx.JMXJsonServlet:writeAttribute(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo)",330,388,"/**
* Writes a single attribute from an MBean to the JSON generator.
* @param jg JSON generator
* @param oname Object name of the MBean
* @param attr Attribute info
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,writeObject,"org.apache.hadoop.jmx.JMXJsonServlet:writeObject(com.fasterxml.jackson.core.JsonGenerator,java.lang.Object,java.lang.String)",395,436,"/**
* Writes object value to JSON generator.
* @param jg JsonGenerator instance
* @param value Object to write (may be null)
* @param attName name of attribute being written
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,getCurrentStats,"org.apache.hadoop.log.LogThrottlingHelper:getCurrentStats(java.lang.String,int)",290,297,"/**
* Retrieves the current statistics for a given recorder by name and index.
* @param recorderName name of the recorder
* @param idx index within the recorder's statistics
* @return SummaryStatistics object or null if not found
*/","* Return the summary information for given index.
   *
   * @param recorderName The name of the recorder.
   * @param idx The index value.
   * @return The summary information.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseProtocolArgs,"org.apache.hadoop.log.LogLevel$CLI:parseProtocolArgs(java.lang.String[],int)",209,228,"/**
* Parses protocol arguments and validates the provided protocol.
* @param args array of command-line arguments
* @param index current argument index
* @return next argument index after parsing
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,printUsage,org.apache.hadoop.log.LogLevel:printUsage(),87,90,"/**
* Prints command usage and help information to standard error.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ToolRunner.java,printGenericCommandUsage,org.apache.hadoop.util.ToolRunner:printGenericCommandUsage(java.io.PrintStream),105,107,"/**
* Prints generic command usage to the specified output stream.
* @param out PrintStream to write to
*/","* Prints generic command-line argurments and usage information.
   * 
   *  @param out stream to write usage information to.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericsUtil.java,isLog4jLogger,org.apache.hadoop.util.GenericsUtil:isLog4jLogger(java.lang.Class),95,100,"/**
* Checks if the given class is a Log4j logger.
* @param clazz Class to check, may be null
* @return true if the class is a Log4j logger, false otherwise
*/","* Determine whether the log of <code>clazz</code> is Log4j implementation.
   * @param clazz a class to be determined
   * @return true if the log of <code>clazz</code> is Log4j implementation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,<init>,"org.apache.hadoop.log.LogThrottlingHelper:<init>(long,java.lang.String)",159,161,"/**
* Initializes LogThrottlingHelper instance with specified minimum log period and primary recorder name.
* @param minLogPeriodMs minimum time interval between logs
* @param primaryRecorderName name of primary log recorder
*/","* Create a log helper with a specified primary recorder name; this can be
   * used in conjunction with {@link #record(String, long, double...)} to set up
   * primary and dependent recorders. See
   * {@link #record(String, long, double...)} for more details.
   *
   * @param minLogPeriodMs The minimum period with which to log; do not log
   *                       more frequently than this.
   * @param primaryRecorderName The name of the primary recorder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,record,"org.apache.hadoop.log.LogThrottlingHelper:record(java.lang.String,long,double[])",247,281,"/**
* Records log values for a given recorder, potentially triggering a periodic log.
* @param recorderName unique logger identifier
* @return LoggingAction object or DO_NOT_LOG if not logged
*/","* Record some set of values at the specified time into this helper. This can
   * be useful to avoid fetching the current time twice if the caller has
   * already done so for other purposes. This additionally allows the caller to
   * specify a name for this recorder. When multiple names are used, one is
   * denoted as the primary recorder. Only recorders named as the primary
   * will trigger logging; other names not matching the primary can <i>only</i>
   * be triggered by following the primary. This is used to coordinate multiple
   * logging points. A primary can be set via the
   * {@link #LogThrottlingHelper(long, String)} constructor. If no primary
   * is set in the constructor, then the first recorder name used becomes the
   * primary.
   *
   * If multiple names are used, they maintain entirely different sets of values
   * and summary information. For example:
   * <pre>{@code
   *   // Initialize ""pre"" as the primary recorder name
   *   LogThrottlingHelper helper = new LogThrottlingHelper(1000, ""pre"");
   *   LogAction preLog = helper.record(""pre"", Time.monotonicNow());
   *   if (preLog.shouldLog()) {
   *     // ...
   *   }
   *   double eventsProcessed = ... // perform some action
   *   LogAction postLog =
   *       helper.record(""post"", Time.monotonicNow(), eventsProcessed);
   *   if (postLog.shouldLog()) {
   *     // ...
   *     // Can use postLog.getStats(0) to access eventsProcessed information
   *   }
   * }</pre>
   * Since ""pre"" is the primary recorder name, logging to ""pre"" will trigger a
   * log action if enough time has elapsed. This will indicate that ""post""
   * should log as well. This ensures that ""post"" is always logged in the same
   * iteration as ""pre"", yet each one is able to maintain its own summary
   * information.
   *
   * <p>Other behavior is the same as {@link #record(double...)}.
   *
   * @param recorderName The name of the recorder. This is used to check if the
   *                     current recorder is the primary. Other names are
   *                     arbitrary and are only used to differentiate between
   *                     distinct recorders.
   * @param currentTimeMs The current time.
   * @param values The values to log.
   * @return The LogAction for the specified recorder.
   *
   * @see #record(double...)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,fromInternalName,org.apache.hadoop.http.ProfileServlet$Event:fromInternalName(java.lang.String),148,156,"/**
* Retrieves an Event object by its internal name.
* @param name the internal name of the desired Event
* @return Event instance or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfilerDisabledServlet.java,doGet,"org.apache.hadoop.http.ProfilerDisabledServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",34,48,"/**
* Handles GET requests by setting an internal server error status and displaying a message with troubleshooting instructions.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,<init>,org.apache.hadoop.http.ProfileServlet:<init>(),177,181,"/**
* Initializes servlet with process ID and async profiler home.
* @param none
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HtmlQuoting.java,needsQuoting,org.apache.hadoop.http.HtmlQuoting:needsQuoting(java.lang.String),68,74,"/**
* Checks if a string requires quoting.
* @param str input string
*/","* Does the given string need to be quoted?
   * @param str the string to check
   * @return does the string contain any of the active html characters?",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HtmlQuoting.java,quoteHtmlChars,org.apache.hadoop.http.HtmlQuoting:quoteHtmlChars(java.lang.String),114,131,"/**
* Escapes HTML special chars in the given string.
* @param item input string to quote
* @return quoted string or null if no changes needed
*/","* Quote the given item to make it html-safe.
   * @param item the string to quote
   * @return the quoted string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addJerseyResourcePackage,"org.apache.hadoop.http.HttpServer2:addJerseyResourcePackage(java.lang.String,java.lang.String)",1018,1022,"/**
* Adds Jersey resource package with specified path.
* @param packageName name of the package
* @param pathSpec path specification for resources
*/","* Add a Jersey resource package.
   * @param packageName The Java package name containing the Jersey resource.
   * @param pathSpec The path spec for the servlet",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addServlet,"org.apache.hadoop.http.HttpServer2:addServlet(java.lang.String,java.lang.String,java.lang.Class)",1050,1053,"/**
* Registers an internal servlet with the specified details.
* @param name servlet name
* @param pathSpec servlet path specification
* @param clazz servlet class type
*/","* Add a servlet in the server.
   * @param name The name of the servlet (can be passed as null)
   * @param pathSpec The path spec for the servlet
   * @param clazz The servlet class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addInternalServlet,"org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class)",1065,1068,"/**
* Registers an internal servlet with the specified name and class.
* @param name servlet name
* @param pathSpec servlet path specification
* @param clazz servlet implementation class
*/","* Add an internal servlet in the server.
   * Note: This method is to be used for adding servlets that facilitate
   * internal communication and not for user facing functionality. For
   * servlets added using this method, filters are not enabled.
   *
   * @param name The name of the servlet (can be passed as null)
   * @param pathSpec The path spec for the servlet
   * @param clazz The servlet class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addFilter,"org.apache.hadoop.http.HttpServer2:addFilter(java.lang.String,java.lang.String,java.util.Map)",1170,1193,"/**
* Adds a custom filter to the application context and default contexts.
* @param name filter name
* @param classname filter class name
* @param parameters filter configuration map
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addGlobalFilter,"org.apache.hadoop.http.HttpServer2:addGlobalFilter(java.lang.String,java.lang.String,java.util.Map)",1195,1206,"/**
* Adds a global filter to the web application.
* @param name filter name
* @param classname class name of the filter
* @param parameters filter parameters (as key-value pairs)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,defineFilter,"org.apache.hadoop.http.HttpServer2:defineFilter(org.eclipse.jetty.servlet.ServletContextHandler,java.lang.String,java.lang.String,java.util.Map,java.lang.String[])",1217,1222,"/**
* Defines a servlet filter with specified name, class, and URLs.
* @param ctx ServletContextHandler instance
* @param name unique filter identifier
* @param classname fully qualified filter class name
* @param parameters filter initialization parameters
* @param urls array of URLs to apply the filter to
*/","* Define a filter for a context and set up default url mappings.
   *
   * @param ctx ctx.
   * @param name name.
   * @param classname classname.
   * @param parameters parameters.
   * @param urls urls.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,bindForSinglePort,"org.apache.hadoop.http.HttpServer2:bindForSinglePort(org.eclipse.jetty.server.ServerConnector,int)",1478,1493,"/**
* Continuously binds a server connector to a single available port.
* @param listener ServerConnector instance
* @param port initial port to attempt (will increment if unavailable)
*/","* Bind using single configured port. If findPort is true, we will try to bind
   * after incrementing port till a free port is found.
   * @param listener jetty listener.
   * @param port port which is set in the listener.
   * @throws Exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,toString,org.apache.hadoop.http.HttpServer2:toString(),1631,1642,"/**
* Returns a human-readable server status string, including its state and listening addresses.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getEnum,org.apache.hadoop.http.HttpServer2$XFrameOption:getEnum(java.lang.String),1946,1954,"/**
* Retrieves XFrameOption enum based on provided string value.
* @param value string representation of XFrameOption
*/","* We cannot use valueOf since the AllowFrom enum differs from its value
     * Allow-From. This is a helper method that does exactly what valueof does,
     * but allows us to handle the AllowFrom issue gracefully.
     *
     * @param value - String must be DENY, SAMEORIGIN or ALLOW-FROM.
     * @return XFrameOption or throws IllegalException.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,init,org.apache.hadoop.http.HttpServer2$QuotingInputFilter:init(javax.servlet.FilterConfig),1860,1864,"/**
* Initializes filter with configuration and HTTP header map.
* @param config Filter configuration
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,doFilter,"org.apache.hadoop.http.HttpServer2$QuotingInputFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",1870,1893,"/**
* Sets response headers and filters incoming requests.
* @param request incoming HTTP request
* @param response outgoing HTTP response
* @param chain filter chain to continue processing
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/lib/StaticUserWebFilter.java,init,org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter:init(javax.servlet.FilterConfig),114,118,"/**
* Initializes filter with user credentials from web.xml.
* @param conf Filter configuration object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileMonitoringTimerTask.java,<init>,"org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<init>(java.util.List,java.util.function.Consumer,java.util.function.Consumer)",75,89,"/**
* Initializes the FileMonitoringTimerTask with file paths to monitor and
* callback actions for changes or failures.
* @param filePaths list of paths to monitor disk files
* @param onFileChange action to perform when a file change is detected
* @param onChangeFailure action to perform on failure
*/","* Create file monitoring task to be scheduled using a standard
   * Java {@link java.util.Timer} instance.
   *
   * @param filePaths The path to the file to monitor.
   * @param onFileChange The function to call when the file has changed.
   * @param onChangeFailure The function to call when an exception is
   *                       thrown during the file change processing.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getNonNegative,"org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getNonNegative(java.lang.String,int)",408,417,"/**
* Retrieves non-negative integer value from properties by key.
* @param key unique property identifier
* @param defaultValue default value to return if not found or invalid
*/","* Return the property value if it's non-negative and throw an exception if
   * it's not.
   *
   * @param key the property key
   * @param defaultValue the default value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,checkIfPropertyExists,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkIfPropertyExists(java.lang.String),424,429,"/**
* Verifies existence of properties by key.
* @param key unique property identifier
*/","* Throw a {@link MetricsException} if the given property is not set.
   *
   * @param key the key to validate",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,checkForErrors,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkForErrors(java.lang.String),905,910,"/**
* Throws MetricsException if an error occurs and ignoreError is false.
* @param message error message to include in exception
*/","* If the sink isn't set to ignore errors, throw a {@link MetricsException}
   * if the stream encountered an exception.  The message parameter will be used
   * as the new exception's message with the current file name
   * ({@link #currentFilePath}) appended to it.
   *
   * @param message the exception message. The message will have a colon and
   * the current file name ({@link #currentFilePath}) appended to it.
   * @throws MetricsException thrown if there was an error and the sink isn't
   * ignoring errors",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,throwMetricsException,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:throwMetricsException(java.lang.String),940,944,"/**
* Throws a MetricsException with provided error message and file path.
* @param message custom error message
*/","* If the sink isn't set to ignore errors, throw a new
   * {@link MetricsException}.  The message parameter will be used  as the
   * new exception's message with the current file name
   * ({@link #currentFilePath}) appended to it.
   *
   * @param message the exception message. The message will have a colon and
   * the current file name ({@link #currentFilePath}) appended to it.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfigException.java,<init>,org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.String),29,31,"/**
* Constructs an instance of MetricsConfigException with the specified error message.
* @param message error description
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,checkMetricName,org.apache.hadoop.metrics2.lib.MetricsRegistry:checkMetricName(java.lang.String),434,452,"/**
* Validates a metric name, checking for whitespace and duplicates.
* @param name the metric name to validate
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,checkTagName,org.apache.hadoop.metrics2.lib.MetricsRegistry:checkTagName(java.lang.String),454,458,"/**
* Validates uniqueness of given tag name.
* @param name unique tag identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsFactory.java,getInstance,org.apache.hadoop.metrics2.lib.DefaultMetricsFactory:getInstance(java.lang.Class),37,46,"/**
* Retrieves an instance of a specified metrics factory class.
* @param cls Class of the desired metrics factory
* @return Instance of the requested class, or null for MutableMetricsFactory if not initialized
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,build,org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:build(),76,92,"/**
* Builds a metrics source from the current configuration.
* @throws MetricsException if hybrid metrics are used without a registry
* or no valid @Metric annotation is found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newTag,org.apache.hadoop.metrics2.lib.MethodMetric:newTag(java.lang.Class),125,139,"/**
* Creates a mutable metric for a specific data type.
* @param resType data type of the metric (currently only String is supported)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getRollInterval,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getRollInterval(),341,399,"/**
* Converts user-specified roll interval string to milliseconds.
* @throws MetricsException if invalid format or unit used
*/","* Extract the roll interval from the configuration and return it in
   * milliseconds.
   *
   * @return the roll interval in millis",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,throwMetricsException,"org.apache.hadoop.metrics2.sink.RollingFileSystemSink:throwMetricsException(java.lang.String,java.lang.Throwable)",924,929,"/**
* Throws a MetricsException with the provided error message and original exception.
* @param message user-provided error description
* @param t original exception to be wrapped
*/","* If the sink isn't set to ignore errors, wrap the Throwable in a
   * {@link MetricsException} and throw it.  The message parameter will be used
   * as the new exception's message with the current file name
   * ({@link #currentFilePath}) and the Throwable's string representation
   * appended to it.
   *
   * @param message the exception message. The message will have a colon, the
   * current file name ({@link #currentFilePath}), and the Throwable's string
   * representation (wrapped in square brackets) appended to it.
   * @param t the Throwable to wrap",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/FileSink.java,init,org.apache.hadoop.metrics2.sink.FileSink:init(org.apache.commons.configuration2.SubsetConfiguration),45,55,"/**
* Initializes output writer based on configuration.
* @param conf SubsetConfiguration object containing output file details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfigException.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.String,java.lang.Throwable)",33,35,"/**
 * Constructs a custom exception with a user-friendly error message and original cause.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfigException.java,<init>,org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.Throwable),37,39,"/**
* Constructs a MetricsConfigException with a specified cause.
* @param cause underlying exception causing this metrics config error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsTag.java,equals,org.apache.hadoop.metrics2.MetricsTag:equals(java.lang.Object),71,78,"/**
* Compares this MetricsTag object with another for equality.
* @param obj the object to compare with
* @return true if both objects have equal info and value, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsTag.java,toString,org.apache.hadoop.metrics2.MetricsTag:toString(),84,89,"/**
* Returns a human-readable representation of the object.
* @return formatted string containing class name and key-value pairs.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,appendPrefix,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:appendPrefix(org.apache.hadoop.metrics2.MetricsRecord,java.lang.StringBuilder)",86,106,"/**
* Appends relevant metrics tags to the StringBuilder.
* @param record MetricsRecord object
* @param sb StringBuilder to append to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/AbstractPatternFilter.java,accepts,org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(org.apache.hadoop.metrics2.MetricsTag),102,119,"/**
* Evaluates whether a metrics tag matches whitelisted or blacklisted patterns.
* @param tag MetricsTag object to evaluate
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/AbstractPatternFilter.java,accepts,org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(java.lang.Iterable),121,142,"/**
* Evaluates metrics tags against inclusion and exclusion patterns.
* @param tags Iterable of MetricsTags to check
* @return true if all tags match inclusion patterns or no pattern is set, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordImpl.java,context,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:context(),72,80,"/**
* Retrieves the context information from metrics tags.
* @return Context string or default value if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,add,"org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.MetricsInfo,java.lang.Object)",61,63,"/**
* Adds a metric value to the builder.
* @param info MetricsInfo object containing name and other details
* @param value Value of the metric as an object (e.g. String, Number)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,add,org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.MetricsTag),86,89,"/**
* Adds a metrics tag to the record builder.
* @param tag Tag object containing name and value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,setContext,org.apache.hadoop.metrics2.MetricStringBuilder:setContext(java.lang.String),97,100,"/**
* Sets context value in metrics record builder.
* @param value context string to be added
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/AbstractMetric.java,equals,org.apache.hadoop.metrics2.AbstractMetric:equals(java.lang.Object),75,82,"/**
* Compares this metric with another for equality.
* @param obj the object to compare against
* @return true if both metrics have same info and value, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/FileSink.java,putMetrics,org.apache.hadoop.metrics2.sink.FileSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),57,80,"/**
* Writes a metrics record to the output stream.
* @param record MetricsRecord object containing data to write
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MetricsCache.java,update,"org.apache.hadoop.metrics2.util.MetricsCache:update(org.apache.hadoop.metrics2.MetricsRecord,boolean)",154,177,"/**
* Updates the Record cache with metrics from the given MetricsRecord.
* @param mr MetricsRecord to update from
* @param includingTags whether to include tags in the updated Record
* @return the updated Record object or null if not found
*/","* Update the cache and return the current cached record
   * @param mr the update record
   * @param includingTags cache tag values (for later lookup by name) if true
   * @return the updated cache record",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java,loadGangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:loadGangliaConf(org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaConfType),185,218,"/**
* Loads Ganglia configuration data based on the specified type.
* @param gtype ganglia configuration type (e.g. units, dmax)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java,xdr_string,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:xdr_string(java.lang.String),245,252,"/**
* Serializes a string to the output buffer.
* @param s input string
*/","* Puts a string into the buffer by first writing the size of the string as an
   * int, followed by the bytes of the string, padded if necessary to a multiple
   * of 4.
   * @param s the string to be written to buffer at offset location",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink31.java,emitMetric,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",47,104,"/**
* Emits a metric with metadata and actual value to Ganglia hosts.
* @param groupName group name for metric aggregation
* @param name metric name
* @param type metric type
* @param value metric value as string
* @param gConf Ganglia configuration
* @param gSlope slope of metric
*/","* The method sends metrics to Ganglia servers. The method has been taken from
   * org.apache.hadoop.metrics.ganglia.GangliaContext31 with minimal changes in
   * order to keep it in sync.
   * @param groupName The group name of the metric
   * @param name The metric name
   * @param type The type of the metric
   * @param value The value of the metric
   * @param gConf The GangliaConf for this metric
   * @param gSlope The slope for this metric
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,emitMetric,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",221,252,"/**
* Emits a user-defined metric with specified details and sends it to Ganglia hosts.
* @param groupName not used in this method (legacy parameter)
* @param name unique metric identifier
* @param type metric data type
* @param value metric value as string
* @param gConf Ganglia configuration
* @param gSlope Ganglia slope type
*/","* The method sends metrics to Ganglia servers. The method has been taken from
   * org.apache.hadoop.metrics.ganglia.GangliaContext30 with minimal changes in
   * order to keep it in sync.
   * @param groupName The group name of the metric
   * @param name The metric name
   * @param type The type of the metric
   * @param value The value of the metric
   * @param gConf The GangliaConf for this metric
   * @param gSlope The slope for this metric
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,calculateSlope,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:calculateSlope(org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",196,207,"/**
* Calculates GangliaSlope based on configuration and metric data.
* @param gConf Ganglia configuration object
* @param slopeFromMetric derived slope from metric data (optional)
* @return calculated GangliaSlope or default value if neither provided
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,flush,org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:flush(),176,180,"/**
 * Flushes buffered data to underlying storage.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,connect,org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:connect(),143,165,"/**
* Establishes a connection to the Graphite server.
* @throws MetricsException if already connected or too many failed connections
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,close,org.apache.hadoop.metrics2.sink.GraphiteSink:close(),124,127,"/**
* Closes the underlying Graphite connection.
* @throws IOException if an I/O error occurs during closure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,init,org.apache.hadoop.metrics2.sink.StatsDSink:init(org.apache.commons.configuration2.SubsetConfiguration),78,95,"/**
* Initializes the StatsD client with configuration settings.
* @param conf SubsetConfiguration object containing StatsD host and service details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,close,org.apache.hadoop.metrics2.sink.StatsDSink:close(),163,166,"/**
* Closes and releases resources associated with this instance.
* @throws IOException on failure to release resources
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java,putMetrics,org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),68,82,"/**
* Stores counter and gauge metrics from the given record into a Prometheus metrics map.
* @param metricsRecord Metrics data to process
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java,getMetricKey,"org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:getMetricKey(java.lang.String,org.apache.hadoop.metrics2.AbstractMetric,java.util.List)",165,174,"/**
* Extracts metric key by parsing Prometheus-style metric name.
* @param promMetricKey Prometheus-style metric name
* @param metric AbstractMetric object
* @param extendTags list of extended tags to append
* @return parsed metric key or original input if no match found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/AbstractPatternFilter.java,init,org.apache.hadoop.metrics2.filter.AbstractPatternFilter:init(org.apache.commons.configuration2.SubsetConfiguration),54,84,"/**
* Initializes configuration from SubsetConfiguration.
* Parses include and exclude patterns, as well as include and exclude tags. 
* @param conf SubsetConfiguration object",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsBufferBuilder.java,add,"org.apache.hadoop.metrics2.impl.MetricsBufferBuilder:add(java.lang.String,java.lang.Iterable)",29,31,"/**
* Adds metrics records to a buffer with specified name.
* @param name unique identifier of metric set
* @param records iterable records to be added
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsBufferBuilder.java,get,org.apache.hadoop.metrics2.impl.MetricsBufferBuilder:get(),33,35,"/**
* Returns a new instance of MetricsBuffer initialized with current metrics.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,<init>,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$WaitableMetricsBuffer:<init>(org.apache.hadoop.metrics2.impl.MetricsBuffer),240,242,"/**
* Constructs a WaitableMetricsBuffer instance from a given MetricsBuffer.
* @param metricsBuffer underlying buffer data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,dequeue,org.apache.hadoop.metrics2.impl.SinkQueue:dequeue(),101,108,"/**
* Removes and returns an item from the queue.
* @throws InterruptedException if interrupted while waiting
*/","* Dequeue one element from head of the queue, will block if queue is empty
   * @return  the first element
   * @throws InterruptedException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,clear,org.apache.hadoop.metrics2.impl.SinkQueue:clear(),154,161,"/**
* Clears all data in the collection, setting each element to null.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,waitForData,org.apache.hadoop.metrics2.impl.SinkQueue:waitForData(),110,118,"/**
* Waits until data is available and returns it.
* @throws InterruptedException if thread is interrupted
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(org.apache.hadoop.metrics2.MetricsSystem$Callback),308,311,"/**
* Registers a callback with a proxy instance.
* @param callback Callback object to be registered
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,org.apache.hadoop.metrics2.MetricsSystem$Callback)",313,315,"/**
* Registers a callback with the given name.
* @param name unique identifier for the callback
* @param callback callback to be registered and wrapped with a proxy
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterLong.java,incr,org.apache.hadoop.metrics2.lib.MutableCounterLong:incr(),42,45,"/**
 * Increments the current value by a default amount of 1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrSentBytes,org.apache.hadoop.ipc.metrics.RpcMetrics:incrSentBytes(int),256,258,"/**
* Increments total sent bytes by specified count.
* @param count number of bytes to add
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrReceivedBytes,org.apache.hadoop.ipc.metrics.RpcMetrics:incrReceivedBytes(int),265,267,"/**
* Increments total received bytes by specified amount.
* @param count number of bytes to add
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordImpl.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsRecordImpl:<init>(org.apache.hadoop.metrics2.MetricsInfo,long,java.util.List,java.lang.Iterable)",47,54,"/**
* Constructs a MetricsRecordImpl object with the given metrics info and data.
* @param info metrics information
* @param timestamp record timestamp (must be greater than zero)
* @param tags list of metric tags
* @param metrics iterable of abstract metrics
*/","* Construct a metrics record
   * @param info  {@link MetricsInfo} of the record
   * @param timestamp of the record
   * @param tags  of the record
   * @param metrics of the record",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.java,toString,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:toString(),46,54,"/**
* Returns a string representation of this object, including its properties.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.java,hashCode,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:hashCode(),42,44,"/**
* Computes hash code based on user profile attributes.
* @return Unique integer hash value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.java,equals,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:equals(java.lang.Object),29,39,"/**
* Compares this MetricsRecord with another object for equality.
* @param obj the object to compare with
* @return true if both objects have identical metrics and properties, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,newAttrInfo,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:newAttrInfo(java.lang.String,java.lang.String,java.lang.String)",53,56,"/**
* Creates a new attribute info for an MBean attribute.
* @param name attribute name
* @param type attribute data type
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,setAttrCacheTag,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttrCacheTag(org.apache.hadoop.metrics2.MetricsTag,int)",279,282,"/**
 * Stores attribute cache with given MetricsTag and record number.
 * @param tag MetricsTag object
 * @param recNo unique record identifier
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,setAttrCacheMetric,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttrCacheMetric(org.apache.hadoop.metrics2.AbstractMetric,int)",296,299,"/**
* Stores attribute cache metric by ID.
* @param metric AbstractMetric object to store
* @param recNo record number for metric
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,refreshQueueSizeGauge,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:refreshQueueSizeGauge(),168,170,"/**
 * Updates the queue size gauge with current queue size.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/UniqueNames.java,uniqueName,org.apache.hadoop.metrics2.lib.UniqueNames:uniqueName(java.lang.String),47,65,"/**
* Generates a unique name by appending incrementing values if the base name already exists.
* @param name initial name
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterInt.java,incr,org.apache.hadoop.metrics2.lib.MutableCounterInt:incr(),41,44,"/**
 * Increments the value by defaulting to +1.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterInt.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableCounterInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",59,65,"/**
* Adds metrics record to the builder, capturing current values.
* @param builder MetricsRecordBuilder instance
* @param all true to snapshot all counters, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,<init>,org.apache.hadoop.metrics2.lib.MutableQuantiles$RolloverSample:<init>(org.apache.hadoop.metrics2.lib.MutableQuantiles),236,238,"/**
* Initializes a new instance of RolloverSample with a specified parent quantile. 
* @param parent the parent quantiles to associate with this sample.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableQuantiles:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",129,146,"/**
* Captures metrics snapshot for the given builder, optionally including all quantiles.
* @param builder MetricsRecordBuilder to populate with snapshot data
* @param all whether to include all quantiles in the snapshot
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addGetGroups,org.apache.hadoop.security.UserGroupInformation$UgiMetrics:addGetGroups(long),155,162,"/**
* Adds latency to the group and, if quantile tracking is enabled, updates all tracked quantiles.
* @param latency latency value to add
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcEnQueueTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcEnQueueTime(long),277,284,"/**
* Adds RPC enqueue time to the queue and enables quantile calculation.
* @param enQTime long value representing enqueue time
*/","* Sometimes, the request time observed by the client is much longer than
   * the queue + process time on the RPC server.Perhaps the RPC request
   * 'waiting enQueue' took too long on the RPC server, so we should add
   * enQueue time to RpcMetrics. See HADOOP-18840 for details.
   * Add an RPC enqueue time sample
   * @param enQTime the queue time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcQueueTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcQueueTime(long),290,297,"/**
* Adds RPC queue time to the internal collection.
* @param qTime time value to add
*/","* Add an RPC queue time sample
   * @param qTime the queue time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcLockWaitTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcLockWaitTime(long),299,306,"/**
* Adds RPC lock wait time to the collection and updates quantiles.
* @param waitTime wait time value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcProcessingTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcProcessingTime(long),312,319,"/**
* Adds RPC processing time to the aggregated metrics.
* @param processingTime duration of RPC processing in milliseconds
*/","* Add an RPC processing time sample
   * @param processingTime the processing time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcResponseTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcResponseTime(long),321,328,"/**
* Adds RPC response time to the collection and updates quantiles if enabled.
* @param responseTime time taken by the RPC request
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addDeferredRpcProcessingTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addDeferredRpcProcessingTime(long),330,337,"/**
* Adds RPC processing time to the deferred queue.
* @param processingTime time in milliseconds
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,addWriteFileLatency,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:addWriteFileLatency(long),110,116,"/**
* Adds a write latency value to the quantile collection.
* @param writeLatency new latency value to add
*/","* Add the file write latency to {@link MutableQuantiles} metrics.
   *
   * @param writeLatency file write latency in microseconds",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,addReadFileLatency,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:addReadFileLatency(long),123,129,"/**
* Adds a read latency value to the quantile collection.
* @param readLatency latency value to be added
*/","* Add the file read latency to {@link MutableQuantiles} metrics.
   *
   * @param readLatency file read latency in microseconds",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableInverseQuantiles.java,<init>,org.apache.hadoop.metrics2.lib.MutableInverseQuantiles$InversePercentile:<init>(double),41,43,"/**
* Calculates and sets the inverse percentile values.
* @param inversePercentile percentage value to convert
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,initialize,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:initialize(java.lang.String),57,59,"/**
* Initializes the metrics system with a given prefix.
* @param prefix unique identifier prefix
* @return initialized MetricsSystem instance
*/","* Convenience method to initialize the metrics system
   * @param prefix  for the metrics system configuration
   * @return the metrics system instance",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,instance,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:instance(),68,70,"/**
* Returns singleton instance of MetricsSystem.
* @return MetricsSystem implementation instance
*/",* @return the metrics system object,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,shutdown,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:shutdown(),75,77,"/**
* Shuts down the application instance.
*/",* Shutdown the metrics system,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,setInstance,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:setInstance(org.apache.hadoop.metrics2.MetricsSystem),87,90,"/**
* Sets instance of MetricsSystem and returns it.
* @param ms MetricsSystem object to be set
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,removeMBeanName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeMBeanName(javax.management.ObjectName),113,116,"/**
* Removes a specified MBean name from the instance's object name map.
* @param name ObjectName to be removed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,removeSourceName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeSourceName(java.lang.String),118,121,"/**
 * Removes source name from internal storage.
 * @param name name to be removed
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getTag,org.apache.hadoop.ipc.metrics.RpcMetrics:getTag(java.lang.String),449,452,"/**
* Retrieves metrics tag by name from the internal registry.
* @param tagName name of the metrics tag to fetch
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,snapshot,"org.apache.hadoop.metrics2.lib.MetricsRegistry:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",465,472,"/**
* Captures current metrics and tags into a snapshot.
* @param builder MetricsRecordBuilder instance to populate
* @param all whether to include all metrics or only changed ones
*/","* Sample all the mutable metrics and put the snapshot in the builder
   * @param builder to contain the metrics snapshot
   * @param all get all the metrics even if the values are not changed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,toString,org.apache.hadoop.metrics2.lib.MetricsRegistry:toString(),474,481,"/**
* Returns a string representation of the object, including info, tags, and metrics.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,getStats,org.apache.hadoop.metrics2.lib.MutableRollingAverages:getStats(long),292,314,"/**
* Calculates statistics for each named metric by aggregating recent samples.
* @param minSamples minimum number of valid samples required per metric
* @return Map of metric names to calculated averages or null if no metrics meet the sample threshold
*/","* Retrieve a map of metric name {@literal ->} (aggregate).
   * Filter out entries that don't have at least minSamples.
   *
   * @param minSamples input minSamples.
   * @return a map of peer DataNode Id to the average latency to that
   *         node seen over the measurement period.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getProcessingSampleCount,org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingSampleCount(),396,398,"/**
* Retrieves the count of processing samples from the last RPC statistics.
* @return The number of samples in the last RPC processing time measurement
*/","* Returns the number of samples that we have seen so far.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getDeferredRpcProcessingSampleCount,org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingSampleCount(),437,439,"/**
* Retrieves the sample count of deferred RPC processing.
* @return number of samples in the last statistics
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,rollOverAvgs,org.apache.hadoop.metrics2.lib.MutableRollingAverages:rollOverAvgs(),247,274,"/**
* Updates rolling averages for each metric in current snapshot.
* @param none
*/","* Iterates over snapshot to capture all Avg metrics into rolling structure
   * {@link MutableRollingAverages#averages}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableGaugeLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",83,89,"/**
* Saves metrics snapshot to builder.
* @param builder Metrics record builder instance
* @param all whether to save all data or only since last change
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeLong:incr(),46,49,"/**
* Increments internal counter by default value (1).
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,decr,org.apache.hadoop.metrics2.lib.MutableGaugeLong:decr(),60,63,"/**
* Decrements counter by default value (1).
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/Interns.java,info,"org.apache.hadoop.metrics2.lib.Interns:info(java.lang.String,java.lang.String)",117,119,"/**
* Adds metrics information to cache with specified name and description.
* @param name unique metric identifier
* @param description metric description
*/","* Get a metric info object.
   * @param name Name of metric info object
   * @param description Description of metric info object
   * @return an interned metric info object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/Interns.java,tag,"org.apache.hadoop.metrics2.lib.Interns:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",151,153,"/**
 * Adds a metrics tag to cache with provided information and value.
 * @param info metrics details
 * @param value tag value
 */","* Get a metrics tag.
   * @param info  of the tag
   * @param value of the tag
   * @return an interned metrics tag",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableGaugeFloat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",52,58,"/**
* Updates metrics snapshot with gauge data.
* @param builder Metrics record builder to update
* @param all whether to include all data or only since last change
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeFloat:incr(float),70,79,"/**
* Atomically increments the value by the specified delta.
* @param delta amount to increment
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,add,"org.apache.hadoop.metrics2.lib.MutableStat:add(long,long)",123,126,"/**
* Updates statistical interval with new sample data.
* @param numSamples number of samples
* @param sum accumulated sum value
*/","* Add a number of samples and their sum to the running stat
   *
   * Note that although use of this method will preserve accurate mean values,
   * large values for numSamples may result in inaccurate variance values due
   * to the use of a single step of the Welford method for variance calculation.
   * @param numSamples  number of samples
   * @param sum of the samples",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,add,org.apache.hadoop.metrics2.util.SampleStat:add(double),68,71,"/**
* Adds a new data point to the sample statistic and updates the minimum/maximum value.
* @param x the new data point
*/","* Add a sample the running stat.
   * @param x the sample number
   * @return  self",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getProcessingMean,org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingMean(),404,406,"/**
* Calculates and returns the mean of last processing time statistics.
* @return mean of last processing time","* Returns mean of RPC Processing Times.
   * @return double",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getDeferredRpcProcessingMean,org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingMean(),441,443,"/**
* Calculates mean of deferred RPC processing times.
* @return mean of last recorded time in milliseconds
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,min,org.apache.hadoop.metrics2.util.SampleStat:min(),134,136,"/**
* Returns the minimum value from the internal minmax data structure.
* @return The smallest value in the collection.",* @return  the minimum value of the samples,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,max,org.apache.hadoop.metrics2.util.SampleStat:max(),141,143,"/**
* Returns the maximum value from the MinMax object.
* @return Maximum value as a double
*/",* @return  the maximum value of the samples,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,reset,org.apache.hadoop.metrics2.util.SampleStat$MinMax:reset(org.apache.hadoop.metrics2.util.SampleStat$MinMax),188,191,"/**
 * Copies minimum and maximum values from another MinMax object.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,resetMinMax,org.apache.hadoop.metrics2.lib.MutableStat:resetMinMax(),177,179,"/**
* Resets minimum and maximum values to their default state.
* Calls the reset method on the associated minMax object. 
* @see #minMax
*/",* Reset the all time min max of the metric,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,reset,org.apache.hadoop.metrics2.util.SampleStat:reset(),40,45,"/**
* Resets statistical counters to initial state.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,snapshot,"org.apache.hadoop.metrics2.lib.MethodMetric$1:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",141,143,"/**
* Creates a snapshot of metrics using the provided builder.
* @param builder MetricsRecordBuilder instance
* @param all whether to include all data or only current values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newCounter,org.apache.hadoop.metrics2.lib.MethodMetric:newCounter(java.lang.Class),72,87,"/**
* Creates a mutable counter Metric for the given primitive type.
* @param type primitive type (int or long)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,snapshot,"org.apache.hadoop.metrics2.lib.MethodMetric$2:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",141,143,"/**
 * Calls the underlying implementation to take a metrics snapshot.
 * @param builder MetricsRecordBuilder instance
 * @param all whether to include all data or not
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newGauge,org.apache.hadoop.metrics2.lib.MethodMetric:newGauge(java.lang.Class),106,123,"/**
* Creates a mutable metric for numeric gauges.
* @param t class of the gauge (int, long, float, or double)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableGaugeInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",83,89,"/**
* Snapshots metrics record with optional all flag.
* @param builder MetricsRecordBuilder to add gauge
* @param all whether to snapshot all or only changed values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getNextTgtRenewalTime,"org.apache.hadoop.security.UserGroupInformation:getNextTgtRenewalTime(long,long,org.apache.hadoop.io.retry.RetryPolicy)",1110,1117,"/**
* Calculates the next TGT renewal time based on retry policy.
* @param tgtEndTime target end time
* @param now current timestamp
* @param rp RetryPolicy instance to determine retry delay
* @return next TGT renewal time in milliseconds or -1 if not found
*/","* Get time for next login retry. This will allow the thread to retry with
   * exponential back-off, until tgt endtime.
   * Last retry is {@link #kerberosMinSecondsBeforeRelogin} before endtime.
   *
   * @param tgtEndTime EndTime of the tgt.
   * @param now Current time.
   * @param rp The retry policy.
   * @return Time for next login retry.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeInt:incr(),46,49,"/**
* Increments a value by defaulting to incrementing by 1.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,decr,org.apache.hadoop.metrics2.lib.MutableGaugeInt:decr(),60,63,"/**
* Decrements the value by default decrement amount (1).",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,reattach,"org.apache.hadoop.metrics2.source.JvmMetrics:reattach(org.apache.hadoop.metrics2.MetricsSystem,org.apache.hadoop.metrics2.source.JvmMetrics)",125,127,"/**
* Reattaches JVM metrics to the MetricsSystem.
* @param ms the MetricsSystem instance
* @param jvmMetrics the JVM metrics object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getMemoryUsage,org.apache.hadoop.metrics2.source.JvmMetrics:getMemoryUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder),157,168,"/**
* Collects and records JVM memory usage metrics.
* @param rb MetricsRecordBuilder instance to store data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/Metrics2Util.java,equals,org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:equals(java.lang.Object),56,62,"/**
* Compares this NameValuePair instance with another, considering only equality.
* @param other the object to compare with
* @return true if equal, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,stddev,org.apache.hadoop.metrics2.util.SampleStat:stddev(),127,129,"/**
* Calculates standard deviation of data.
* @return Standard deviation value as a double
*/",* @return  the standard deviation of the samples,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,compress,org.apache.hadoop.metrics2.util.SampleQuantiles:compress(),176,198,"/**
* Merges adjacent samples within a threshold of allowable error.
* @param samples collection of SampleItem objects to compress
*/","* Try to remove extraneous items from the set of sampled items. This checks
   * if an item is unnecessary based on the desired error bounds, and merges it
   * with the adjacent item if it is.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,query,org.apache.hadoop.metrics2.util.SampleQuantiles:query(double),206,228,"/**
* Computes the quantile by iterating through sorted samples and returning
* the first value whose cumulative sum exceeds the desired rank (adjusted for error).
* @param quantile target quantile value between 0 and 1
*/","* Get the estimated value at the specified quantile.
   * 
   * @param quantile Queried quantile, e.g. 0.50 or 0.99.
   * @return Estimated value at that quantile.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,insertBatch,org.apache.hadoop.metrics2.util.SampleQuantiles:insertBatch(),129,169,"/**
* Inserts batch of values into sorted sample list.
* @param buffer array of values to insert
*/","* Merges items from buffer into the samples array in one pass.
   * This is more efficient than doing an insert on every item.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/Metrics2Util.java,offer,org.apache.hadoop.metrics2.util.Metrics2Util$TopN:offer(org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair),84,95,"/**
* Offers a new item to the collection, respecting capacity and ordering.
* @param entry NameValuePair object with value to offer
* @return true if offered successfully, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MetricsCache.java,<init>,org.apache.hadoop.metrics2.util.MetricsCache:<init>(),136,138,"/**
* Initializes metrics cache with default maximum records per name.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,tag,"org.apache.hadoop.metrics2.MetricsJsonBuilder:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",66,69,"/**
* Adds a key-value pair to the metrics record.
* @param info MetricsInfo object with key details
* @param value associated metric value as string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,add,org.apache.hadoop.metrics2.MetricsJsonBuilder:add(org.apache.hadoop.metrics2.MetricsTag),71,74,"/**
* Adds a metrics tag to the builder.
* @param tag metrics tag data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,add,org.apache.hadoop.metrics2.MetricsJsonBuilder:add(org.apache.hadoop.metrics2.AbstractMetric),76,79,"/**
 * Adds an AbstractMetric to the record and returns this builder instance.
 * @param metric AbstractMetric object to be added
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,setContext,org.apache.hadoop.metrics2.MetricsJsonBuilder:setContext(java.lang.String),81,84,"/**
* Sets context value in metrics record builder.
* @param value context string to be stored
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",86,89,"/**
* Adds a counter metric with specified name and value.
* @param info metrics information
* @param value counter value
* @return updated MetricsRecordBuilder instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",91,94,"/**
* Adds counter metrics with specified value.
* @param info metrics information (name)
* @param value metric value to be added
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",96,99,"/**
* Adds gauge metric with specified name and value.
* @param info MetricsInfo object with name and other metadata
* @param value integer value of the gauge metric
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",101,104,"/**
* Adds gauge metric with specified value and name.
* @param info MetricsInfo object defining metric details
* @param value measured gauge value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",106,109,"/**
* Adds a gauge metric to the builder with specified name and value.
* @param info metrics information object
* @param value gauge value as a float
* @return updated MetricsRecordBuilder instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",111,114,"/**
* Adds gauge metric with specified name and value.
* @param info metrics information
* @param value gauge value to record
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolServerSideTranslatorPB.java,getProtocolVersion,"org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:getProtocolVersion(java.lang.String,long)",68,72,"/**
* Retrieves protocol version based on specified protocol and client version.
* @param protocol string representation of the protocol (e.g. 'ZKFC')
* @param clientVersion client's current protocol version
* @return the highest supported protocol version as a long value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,getProtocolVersion,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getProtocolVersion(java.lang.String,long)",180,184,"/**
* Retrieves the current protocol version.
* @param protocol service protocol (ignored in this implementation)
* @param clientVersion client software version (unused)
* @return current protocol version
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/NetgroupCache.java,getNetgroupNames,org.apache.hadoop.security.NetgroupCache:getNetgroupNames(),63,65,"/**
* Retrieves and returns a list of netgroup names.
* @return List of netgroup names
*/","* Get the list of cached netgroups
   *
   * @return list of cached groups",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/NetgroupCache.java,isCached,org.apache.hadoop.security.NetgroupCache:isCached(java.lang.String),81,83,"/**
* Checks if a group exists in the cached groups collection.
* @param group name of the group to check
*/","* Returns true if a given netgroup is cached
   *
   * @param group check if this group is cached
   * @return true if group is cached, false otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getDefaults,org.apache.hadoop.security.UserGroupInformation$LoginParams:getDefaults(),2097,2103,"/**
* Returns default login parameters from environment variables.
* @return LoginParams object with principal, keytab, and ccache values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,toLowerCase,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:toLowerCase(javax.servlet.http.HttpServletRequest),132,196,"/**
* Converts HttpServletRequest to lowercase, preserving original key case.
* @param request HttpServletRequest object
* @return Lowercase version of the request or original if no uppercase keys found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getServerProperties,"org.apache.hadoop.security.SaslPropertiesResolver:getServerProperties(java.net.InetAddress,int)",103,106,"/**
* Retrieves server properties based on client IP address.
* @param clientAddress client's network address
*/","* Identify the Sasl Properties to be used for a connection with a  client.
   * @param clientAddress  client's address
   * @param ingressPort the port that the client is connecting
   * @return the sasl properties to be used for the connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getClientProperties,"org.apache.hadoop.security.SaslPropertiesResolver:getClientProperties(java.net.InetAddress,int)",123,126,"/**
* Returns client properties for given server address and port.
* @param serverAddress server IP address
* @param ingressPort server port number
*/","* Identify the Sasl Properties to be used for a connection with a server.
   * @param serverAddress server's address
   * @param ingressPort the port that is used to connect to server
   * @return the sasl properties to be used for the connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,getPassword,org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:getPassword(org.apache.hadoop.security.token.TokenIdentifier),289,292,"/**
* Retrieves and encodes password for a given token identifier.
* @param tokenid unique token identifier
* @return encoded password as character array or null if retrieval fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.java,getGroupsSet,org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:getGroupsSet(java.lang.String),65,68,"/**
* Retrieves a set of group names associated with the given user.
* @param user username to fetch groups for
* @return non-empty set of group names or empty set if none found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.java,getGroupsSet,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:getGroupsSet(java.lang.String),64,67,"/**
* Retrieves unique set of groups associated with the given user.
* @param user username or identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.CompositeGroupsMapping:getGroupsSet(java.lang.String),110,131,"/**
* Retrieves a set of groups for a given user.
* @param user unique user identifier
* @return Set of group names or empty set if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HttpCrossOriginFilterInitializer.java,getEnabledConfigKey,org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getEnabledConfigKey(),71,73,"/**
* Returns enabled configuration key by concatenating prefix and suffix.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getCredentialsInternal,org.apache.hadoop.security.UserGroupInformation:getCredentialsInternal(),1759,1770,"/**
* Retrieves internal user credentials, either from existing set or creates a new instance.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/User.java,<init>,"org.apache.hadoop.security.User:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,javax.security.auth.login.LoginContext)",46,57,"/**
* Initializes a User object with the given properties.
* @param name user's full name
* @param authMethod authentication method (e.g. Kerberos)
* @param login login context for this user
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getHostFromPrincipal,org.apache.hadoop.security.SecurityUtil:getHostFromPrincipal(java.lang.String),353,355,"/**
* Extracts hostname from Kerberos principal name.
* @param principalName Kerberos principal identifier
*/","* Get the host name from the principal name of format {@literal <}service
   * {@literal >}/host@realm.
   * @param principalName principal name of format as described above
   * @return host name if the the string conforms to the above format, else null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getGroupInternal,org.apache.hadoop.security.Groups:getGroupInternal(java.lang.String),242,264,"/**
* Retrieves user groups from cache or database.
* @param user unique user identifier
* @return Set of group names or null if not found
*/","* Get the group memberships of a given user.
   * If the user's group is not cached, this method may block.
   * @param user User's name
   * @return the group memberships of the user as Set
   * @throws IOException if user does not exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,refresh,org.apache.hadoop.security.Groups:refresh(),430,441,"/**
* Refreshes the userToGroupsMap cache and clears other caches.
*/",* Refresh all user-to-groups mappings.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMapping.java,getGroups,org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroups(java.lang.String),79,82,"/**
* Retrieves list of groups associated with the specified user.
* @param user unique username or identifier
* @return List of group names or empty list if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroupsSet(java.lang.String),84,90,"/**
* Retrieves a set of group names for the specified user.
* @param user unique user identifier
* @return immutable set of group names or empty set if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,close,org.apache.hadoop.security.KDiag:close(),189,195,"/**
* Closes this output stream and releases any system resources associated with it.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,println,"org.apache.hadoop.security.KDiag:println(java.lang.String,java.lang.Object[])",854,863,"/**
* Prints formatted message to output stream or system console.
* @param format printf-style string with placeholders
* @param args variable arguments for formatting
*/","* Print a line of output. This goes to any output file, or
   * is logged at info. The output is flushed before and after, to
   * try and stay in sync with JRE logging.
   *
   * @param format format string
   * @param args any arguments",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,usage,org.apache.hadoop.security.KDiag:usage(),246,263,"/**
* Generates usage message with available command-line arguments.
* @return help text describing supported options
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMapInternal,"org.apache.hadoop.security.ShellBasedIdMapping:updateMapInternal(org.apache.hadoop.thirdparty.com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map)",224,281,"/**
* Updates a BiMap with entries from an external command output.
* @param map the BiMap to update
* @param mapName name of the map
* @param command shell command to execute for data retrieval
* @param regex regular expression to split output lines into key-value pairs
* @param staticMapping mapping of id keys to integer values used for parsing
* @return true if any updates were made, false otherwise
*/","* Get the list of users or groups returned by the specified command,
   * and save them in the corresponding map.
   *
   * @param map map.
   * @param mapName mapName.
   * @param command command.
   * @param staticMapping staticMapping.
   * @param regex regex.
   * @throws IOException raised on errors performing I/O.
   * @return updateMapInternal.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getRunScriptCommand,org.apache.hadoop.util.Shell:getRunScriptCommand(java.io.File),443,448,"/**
* Returns command to run a script, using native shell on Windows or Bash on Unix.
* @param script the script file
*/","* Returns a command to run the given script.  The script interpreter is
   * inferred by platform: cmd on Windows or bash otherwise.
   *
   * @param script File script to run
   * @return String[] command to run the script",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,valueOf,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:valueOf(org.apache.hadoop.security.SaslRpcServer$AuthMethod),1504,1512,"/**
* Returns the AuthenticationMethod enum value matching the provided AuthMethod.
* @param authMethod unique authentication method identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,switchBindUser,org.apache.hadoop.security.LdapGroupsMapping:switchBindUser(javax.naming.AuthenticationException),644,651,"/**
* Switches the bind user in case of authentication exception.
* @param e AuthenticationException that triggered switch
*/","* Switch to the next available user to bind to.
   * @param e AuthenticationException encountered when contacting LDAP",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,write,"org.apache.hadoop.security.SaslOutputStream:write(byte[],int,int)",166,193,"/**
* Writes data to output stream with optional SASL wrapping.
* @param inBuf input byte array
* @param off offset into input buffer
* @param len length of data to write
* @throws IOException if writing fails
*/","* Writes <code>len</code> bytes from the specified byte array starting at
   * offset <code>off</code> to this output stream.
   * 
   * @param inBuf
   *          the data.
   * @param off
   *          the start offset in the data.
   * @param len
   *          the number of bytes to write.
   * @exception IOException
   *              if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,close,org.apache.hadoop.security.SaslOutputStream:close(),213,217,"/**
* Closes this output stream and releases any system resources associated with it.
* Disposes of SASL connection and closes underlying output stream.","* Closes this output stream and releases any system resources associated with
   * this stream.
   * 
   * @exception IOException
   *              if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,init,org.apache.hadoop.security.SaslRpcServer:init(org.apache.hadoop.conf.Configuration),175,182,"/**
* Initializes SASL configuration.
* @param conf Configuration object for initialization
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,createSaslServer,"org.apache.hadoop.security.SaslPlainServer$SaslPlainServerFactory:createSaslServer(java.lang.String,java.lang.String,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler)",48,53,"/**
* Creates a SASL server instance for the specified mechanism.
* @param mechanism authentication mechanism (e.g. PLAIN, DIGEST-MD5)
* @return SaslServer instance or null if unsupported mechanism
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,getAuthorizationID,org.apache.hadoop.security.SaslPlainServer:getAuthorizationID(),127,131,"/**
* Returns the authorization ID.
* @return Unique identifier for authentication purposes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,getNegotiatedProperty,org.apache.hadoop.security.SaslPlainServer:getNegotiatedProperty(java.lang.String),133,137,"/**
* Returns negotiated property value (QOP) or null if not found.
* @param propName name of the property to fetch (""Sasl.QOP"")
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,wrap,"org.apache.hadoop.security.SaslPlainServer:wrap(byte[],int,int)",139,145,"/**
* Throws an exception since PLAIN Sasl authentication does not support integrity or privacy. 
* @throws IllegalStateException
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,unwrap,"org.apache.hadoop.security.SaslPlainServer:unwrap(byte[],int,int)",147,153,"/**
* Throws an exception since PLAIN authentication does not support integrity or privacy.
* @throws IllegalStateException when attempting to unwrap with PLAIN
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,createKeyStore,"org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyStore(java.lang.String,java.lang.String)",1102,1109,"/**
* Loads a pre-existing KeyStore from the specified location.
* @param location path to the KeyStore file
* @param password KeyStore protection password
* @return loaded KeyStore object or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,init,org.apache.hadoop.security.http.RestCsrfPreventionFilter:init(javax.servlet.FilterConfig),71,93,"/**
* Initializes filter with custom parameters and default values.
* @param filterConfig configuration for the filter
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,handleHttpInteraction,org.apache.hadoop.security.http.RestCsrfPreventionFilter:handleHttpInteraction(org.apache.hadoop.security.http.RestCsrfPreventionFilter$HttpInteraction),193,203,"/**
* Handles HTTP interactions by checking for required headers and proceeding or returning an error.
* @param httpInteraction Http interaction object
*/","* Handles an {@link HttpInteraction} by applying the filtering logic.
   *
   * @param httpInteraction caller's HTTP interaction
   * @throws IOException if there is an I/O error
   * @throws ServletException if the implementation relies on the servlet API
   *     and a servlet API call has failed",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,initializeAllowedMethods,org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedMethods(javax.servlet.FilterConfig),165,174,"/**
* Initializes the set of allowed HTTP methods from filter configuration.
* @param filterConfig Filter configuration object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,doCrossFilter,"org.apache.hadoop.security.http.CrossOriginFilter:doCrossFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",107,153,"/**
* Performs cross-origin filter checks and populates HttpServletResponse.
* @param req HttpServletRequest object
* @param res HttpServletResponse object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,initializeAllowedHeaders,org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedHeaders(javax.servlet.FilterConfig),176,185,"/**
 * Initializes the list of allowed headers from filter config.
 * @param filterConfig Filter configuration
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,parsePartialGroupNames,"org.apache.hadoop.security.ShellBasedUnixGroupsMapping:parsePartialGroupNames(java.lang.String,java.lang.String)",242,269,"/**
* Parses partial group names from input strings, resolving numeric values to IDs.
* @param groupNames string of group names separated by token separator
* @param groupIDs string of corresponding group IDs separated by token separator
* @return set of resolved group names or exception if not resolvable
*/","* Attempt to parse group names given that some names are not resolvable.
   * Use the group id list to identify those that are not resolved.
   *
   * @param groupNames a string representing a list of group names
   * @param groupIDs a string representing a list of group ids
   * @return a linked list of group names
   * @throws PartialGroupNameException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,newLoginContext,"org.apache.hadoop.security.UserGroupInformation:newLoginContext(java.lang.String,javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration)",503,518,"/**
* Creates a new Hadoop login context with the given parameters.
* @param appName application name
* @param subject user identity
* @param loginConf configuration for login process
* @return newly created HadoopLoginContext object or null if fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,login,org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:login(),2142,2155,"/**
* Attempts to login the user, updating metrics on success or failure.
* @throws LoginException if login attempt fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logout,org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:logout(),2157,2164,"/**
* Atomically logs out current user session.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,createSecretKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createSecretKey(byte[]),692,694,"/**
 * Creates a secret key using the provided byte array.
 * @param key byte array containing the secret key data
 */","* Convert the byte[] to a secret key
   * @param key the byte[] to create the secret key from
   * @return the secret key",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,formatTokenId,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:formatTokenId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),83,90,"/**
* Formats a TokenIdent object as a string in the format (id).
* @param id TokenIdent object to be formatted
* @return Formatted string representation of the TokenIdent or error message if formatting fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,removeStoredToken,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),204,211,"/**
* Removes stored token for given identifier.
* @param ident TokenIdent object containing sequence number and bytes
*/","* Removes the existing TokenInformation from the SQL database to
   * invalidate it.
   * @param ident TokenInformation to remove from the SQL database.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Daemon.java,newThread,org.apache.hadoop.util.Daemon$DaemonFactory:newThread(java.lang.Runnable),42,45,"/**
* Creates and returns a new daemon thread.
* @param runnable target task to execute in the thread
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,serviceStart,org.apache.hadoop.util.JvmPauseMonitor:serviceStart(),81,86,"/**
* Starts monitoring thread to track service activity.
* Subclasses this method to perform custom initialization. 
* @throws Exception if initialization fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,reset,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:reset(),182,187,"/**
* Resets internal state to default values.
*/",* Reset all data structures and mutable state.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,updateDelegationKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),350,352,"/**
* Updates the delegation key in storage.
* @param key DelegationKey object to be updated
*/","* For subclasses externalizing the storage, for example Zookeeper
   * based implementations.
   *
   * @param key DelegationKey.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,removeStoredMasterKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey),370,377,"/**
* Removes stored master key by ID and associated delegation key.
* @param key DelegationKey object containing the key ID
*/","* Removes the existing DelegationKey from the SQL database to
   * invalidate it.
   * @param key DelegationKey to remove from the SQL database.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,addKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addKey(org.apache.hadoop.security.token.delegation.DelegationKey),213,220,"/**
* Adds a DelegationKey to the SecretManager, updating the current key ID if necessary.
* @param key the DelegationKey to add
*/","* Add a previously used master key to cache (when NN restarts), 
   * should be called before activate().
   *
   * @param key delegation key.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,storeDelegationKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),338,341,"/**
* Stores a delegation key and updates the master key.
* @param key DelegationKey object to be stored
*/","* For subclasses externalizing the storage, for example Zookeeper
   * based implementations.
   *
   * @param key DelegationKey.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,<init>,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>(long,byte[])",707,709,"/**
* Constructs DelegationTokenInformation with specified renew date and password.
* @param renewDate token renewal date
* @param password token encryption password
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeExpiredKeys,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredKeys(),478,491,"/**
* Removes expired delegation keys and associated master keys.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,getTokenTrackingId,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenTrackingId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),561,567,"/**
* Retrieves token tracking ID from TokenIdent.
* @param identifier unique token identifier
* @return associated tracking ID or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeExpiredStoredToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),795,797,"/**
 * Removes any stored token that has expired.
 * @param ident unique TokenIdent identifier
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,setExternalDelegationTokenSecretManager,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager),136,141,"/**
* Sets external delegationToken secret manager.
* @param secretManager AbstractDelegationTokenSecretManager instance
*/","* Sets an external <code>DelegationTokenSecretManager</code> instance to
   * manage creation and verification of Delegation Tokens.
   * <p>
   * This is useful for use cases where secrets must be shared across multiple
   * services.
   *
   * @param secretManager a <code>DelegationTokenSecretManager</code> instance",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,destroy,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:destroy(),154,158,"/**
 * Stops threads managed by Secret Manager.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,stopThreads,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:stopThreads(),437,475,"/**
* Stops all background threads and caches.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationTokenLoadingCache.java,isEmpty,org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:isEmpty(),57,60,"/**
* Checks if the collection is empty.
* @return true if size is zero, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,<init>,"org.apache.hadoop.security.token.delegation.DelegationKey:<init>(int,long,javax.crypto.SecretKey)",51,53,"/**
* Creates a new DelegationKey instance with specified ID, expiry date, and encoded secret key.
* @param keyId unique identifier for the delegation key
* @param expiryDate timestamp when the key expires
* @param key optional SecretKey object to encode; may be null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,getConfiguration,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getConfiguration(java.lang.String,javax.servlet.FilterConfig)",114,120,"/**
* Retrieves configuration properties with custom authentication handler class.
* @param configPrefix prefix for the configuration
* @param filterConfig filter configuration object
* @return Properties object containing configuration and auth handler details
*/","* It delegates to
   * {@link AuthenticationFilter#getConfiguration(String, FilterConfig)} and
   * then overrides the {@link AuthenticationHandler} to use if authentication
   * type is set to <code>simple</code> or <code>kerberos</code> in order to use
   * the corresponding implementation with delegation token support.
   *
   * @param configPrefix parameter not used.
   * @param filterConfig parameter not used.
   * @return hadoop-auth de-prefixed configuration for the filter and handler.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,initializeAuthHandler,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:initializeAuthHandler(java.lang.String,javax.servlet.FilterConfig)",204,215,"/**
* Initializes authentication handler using CuratorFramework and ZKDelegationTokenSecretManager.
* @param authHandlerClassName name of the authentication handler class
* @throws ServletException if initialization fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/HttpUserGroupInformation.java,get,org.apache.hadoop.security.token.delegation.web.HttpUserGroupInformation:get(),36,39,"/**
 * Retrieves the current user group information from the HTTP context. 
 * @return UserGroupInformation object representing the authenticated user, or null if not available.","* Returns the remote {@link UserGroupInformation} in context for the current
   * HTTP request, taking into account proxy user requests.
   *
   * @return the remote {@link UserGroupInformation}, <code>NULL</code> if none.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/MultiSchemeDelegationTokenAuthenticationHandler.java,<init>,org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:<init>(),89,92,"/**
* Initializes authentication handler with multi-scheme configuration.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/KerberosDelegationTokenAuthenticationHandler.java,<init>,org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler:<init>(),49,52,"/**
* Initializes a new instance of KerberosDelegationTokenAuthenticationHandler.
* @param authenticationHandlerType type of the underlying authentication handler
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/PseudoDelegationTokenAuthenticationHandler.java,<init>,org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler:<init>(),50,53,"/**
* Initializes PseudoDelegationTokenAuthenticationHandler with pseudo authentication handler. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,obtainDelegationTokenAuthenticator,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:obtainDelegationTokenAuthenticator(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)",128,140,"/**
* Returns a DelegationTokenAuthenticator instance, either from the provided
* delegate token authenticator or by creating a default one.
* @param dta DelegationTokenAuthenticator to reuse, or null for default
* @param connConfigurator connection configurator to set on the authenticator
* @return DelegationTokenAuthenticator instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/PseudoDelegationTokenAuthenticator.java,<init>,org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator:<init>(),41,52,"/**
* Initializes the authenticator with a pseudo user name.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/KerberosDelegationTokenAuthenticator.java,<init>,org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator:<init>(),38,45,"/**
* Initializes Kerberos delegationToken authenticator with fallback to pseudo token. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,isManagementOperation,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:isManagementOperation(javax.servlet.http.HttpServletRequest),211,218,"/**
* Checks if the incoming request is a management operation.
* @param request HttpServletRequest object
*/","* This method checks if the given HTTP request corresponds to a management
   * operation.
   *
   * @param request The HTTP request
   * @return true if the given HTTP request corresponds to a management
   *         operation false otherwise
   * @throws IOException In case of I/O error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,getDelegationToken,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getDelegationToken(javax.servlet.http.HttpServletRequest),412,421,"/**
* Retrieves delegated token from HTTP request.
* @return Delegated token as a string, or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,hasDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:hasDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",115,132,"/**
* Checks if the provided URL contains a delegation token.
* @param url the URL to check
* @param token the AuthenticatedURL.Token object
* @return true if the URL or token contains a delegation token, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,org.apache.hadoop.crypto.key.kms.KMSClientProvider$TokenSelector:<init>(),167,169,"/**
 * Initializes TokenSelector with specified token kind.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,incrementDelegationTokenSeqNum,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementDelegationTokenSeqNum(),504,531,"/**
* Increments the delegationToken sequence number, refreshing from ZK if exhausted.
*@return next available sequence number
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,incrementCurrentKeyId,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementCurrentKeyId(),547,559,"/**
* Increments the current key ID and returns its value.
* @return The incremented key ID value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,removeStoredMasterKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey),727,754,"/**
* Removes stored master key by ID from ZooKeeper.
* @param key DelegationKey object to remove
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,equals,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:equals(java.lang.Object),166,182,"/**
* Compares two AbstractDelegationTokenIdentifier objects for equality.
* @param obj object to compare with this instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,isManaged,org.apache.hadoop.security.token.Token:isManaged(),487,489,"/**
* Checks whether this instance is managed.
* @return true if managed, false otherwise
*/","* Is this token managed so that it can be renewed or cancelled?
   * @return true, if it can be renewed and cancelled.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,renew,org.apache.hadoop.security.token.Token:renew(org.apache.hadoop.conf.Configuration),498,501,"/**
* Renew configuration with provided settings.
* @param conf Configuration object to update
*/","* Renew this delegation token.
   * @param conf configuration.
   * @return the new expiration time
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,cancel,org.apache.hadoop.security.token.Token:cancel(org.apache.hadoop.conf.Configuration),510,513,"/**
 * Cancels the current renewal task.
 * @param conf configuration object used to control cancellation
 */","* Cancel this delegation token.
   *
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,validate,org.apache.hadoop.security.token.DtUtilShell$Get:validate(),225,239,"/**
* Validates the provided service and URL.
* @return true if valid, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,getCommandUsage,org.apache.hadoop.security.token.DtUtilShell:getCommandUsage(),179,187,"/**
* Formats command usage instructions with various options.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,read,"org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[],int,int)",580,593,"/**
* Reads up to len bytes from the RPC message buffer into buf.
* @param buf byte array to store data
* @param off offset in buf where data should be written
* @param len maximum number of bytes to read
* @return actual number of bytes read
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,isValidAuthType,org.apache.hadoop.security.SaslRpcClient:isValidAuthType(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),189,199,"/**
* Validates SASL authentication type.
* @param authType SaslAuth object to validate
* @return true if valid, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getInputStream,org.apache.hadoop.security.SaslRpcClient:getInputStream(java.io.InputStream),533,538,"/**
* Wraps or returns input stream based on useWrap() condition.
* @param in original input stream
* @return possibly wrapped InputStream object
*/","* Get SASL wrapped InputStream if SASL QoP requires unwrapping,
   * otherwise return original stream.  Can be called only after
   * saslConnect() has been called.
   * 
   * @param in - InputStream used to make the connection
   * @return InputStream that may be using SASL unwrap
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getOutputStream,org.apache.hadoop.security.SaslRpcClient:getOutputStream(java.io.OutputStream),549,558,"/**
* Returns an OutputStream for writing to the client, optionally wrapping it
* with a buffered stream based on negotiated properties.
* @param out OutputStream to wrap or use directly
* @return wrapped OutputStream if negotiated properties require it, otherwise original OutputStream
*/","* Get SASL wrapped OutputStream if SASL QoP requires wrapping,
   * otherwise return original stream.  Can be called only after
   * saslConnect() has been called.
   * 
   * @param out - OutputStream used to make the connection
   * @return OutputStream that may be using wrapping
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,disposeSasl,org.apache.hadoop.ipc.Client$Connection:disposeSasl(),546,554,"/**
* Disposes of the SASL client, releasing resources and closing connections.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,noPasswordWarning,"org.apache.hadoop.security.ProviderUtils:noPasswordWarning(java.lang.String,java.lang.String)",244,247,"/**
* Generates passwordless warning message combining key values.
* @param envKey environment-specific key
* @param fileKey file-specific key
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,noPasswordError,"org.apache.hadoop.security.ProviderUtils:noPasswordError(java.lang.String,java.lang.String)",249,251,"/**
* Returns error message with password instruction.
* @param envKey environment key
* @param fileKey file key
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,readMoreData,org.apache.hadoop.security.SaslInputStream:readMoreData(),95,125,"/**
* Reads additional data from the stream, unwraps SASL token and returns its length.
* @return length of unwrapped data or -1 on EOF
*/","* Read more data and get them processed <br>
   * Entry condition: ostart = ofinish <br>
   * Exit condition: ostart <= ofinish <br>
   * 
   * return (ofinish-ostart) (we have this many bytes for you), 0 (no data now,
   * but could have more later), or -1 (absolutely no more data)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,close,org.apache.hadoop.security.SaslInputStream:close(),341,348,"/**
* Closes and disposes resources, marking the connection as closed.
*/","* Closes this input stream and releases any system resources associated with
   * the stream.
   * <p>
   * The <code>close</code> method of <code>SASLInputStream</code> calls the
   * <code>close</code> method of its underlying input stream.
   * 
   * @exception IOException
   *              if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,<init>,org.apache.hadoop.security.authorize.AuthorizationException:<init>(),37,39,"/**
* Constructs an empty AuthorizationException instance.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,<init>,org.apache.hadoop.security.authorize.AuthorizationException:<init>(java.lang.Throwable),54,56,"/**
* Constructs an AuthorizationException with the given cause.","* Constructs a new exception with the specified cause and a detail
   * message of <tt>(cause==null ? null : cause.toString())</tt> (which
   * typically contains the class and detail message of <tt>cause</tt>).
   * @param  cause the cause (which is saved for later retrieval by the
   *         {@link #getCause()} method).  (A <tt>null</tt> value is
   *         permitted, and indicates that the cause is nonexistent or
   *         unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,<init>,org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler:<init>(org.apache.hadoop.security.token.Token),664,667,"/**
* Initializes SaslClientCallbackHandler with a user token.
* @param token unique token containing identifier and password
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reset,org.apache.hadoop.security.UserGroupInformation:reset(),368,379,"/**
* Resets internal state to default values.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getLogin,org.apache.hadoop.security.UserGroupInformation:getLogin(),522,526,"/**
* Retrieves Hadoop-specific login context.
* @return HadoopLoginContext instance or null if not applicable
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isLoginSuccess,org.apache.hadoop.security.UserGroupInformation:isLoginSuccess(),537,542,"/**
* Checks if login was successful.
* @return true if logged in successfully, false otherwise
*/","This method checks for a successful Kerberos login
    * and returns true by default if it is not using Kerberos.
    *
    * @return true on successful login",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setLogin,org.apache.hadoop.security.UserGroupInformation:setLogin(javax.security.auth.login.LoginContext),528,530,"/**
 * Sets the login context for the current user.
 * @param login LoginContext object containing user credentials
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setLastLogin,org.apache.hadoop.security.UserGroupInformation:setLastLogin(long),548,550,"/**
 * Updates the user's last login timestamp.
 * @param loginTime Unix timestamp of the latest login
 */","* Set the last login time for logged in user
   * @param loginTime the number of milliseconds since the beginning of time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,<init>,org.apache.hadoop.security.UserGroupInformation:<init>(javax.security.auth.Subject),559,568,"/**
* Initializes UserGroupInformation with the given Subject, extracting and validating the User principal.
* @param subject the Subject containing the User principal
*/","* Create a UserGroupInformation for the given subject.
   * This does not change the subject or acquire new credentials.
   *
   * The creator of subject is responsible for renewing credentials.
   * @param subject the user's subject",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getUserName,org.apache.hadoop.security.UserGroupInformation:getUserName(),1667,1671,"/**
* Retrieves the username from the associated user object.
*/","* Get the user's full principal name.
   * @return the user's full principal name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,hasKerberosCredentials,org.apache.hadoop.security.UserGroupInformation:hasKerberosCredentials(),574,576,"/**
* Checks if user has Kerberos-based authentication credentials.
* @return true if user uses Kerberos, false otherwise
*/","* checks if logged in using kerberos
   * @return true if the subject logged via keytab or has a Kerberos TGT",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:getAuthenticationMethod(),1853,1855,"/**
* Retrieves the authentication method associated with the user.
* @return AuthenticationMethod object or null if not set
*/","* Get the authentication method from the subject
   * 
   * @return AuthenticationMethod in the subject, null if not present.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,fixKerberosTicketOrder,org.apache.hadoop.security.UserGroupInformation:fixKerberosTicketOrder(),1204,1233,"/**
* Fixes the order and validity of Kerberos tickets in private credentials.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,hasSufficientTimeElapsed,org.apache.hadoop.security.UserGroupInformation:hasSufficientTimeElapsed(long),1401,1410,"/**
* Checks if sufficient time has elapsed since last login.
* @param now current timestamp
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealUser,org.apache.hadoop.security.UserGroupInformation:getRealUser(),1543,1550,"/**
* Returns the real user associated with this principal, or null if not found.
*/","* get RealUser (vs. EffectiveUser)
   * @return realUser running over proxy user",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getShortUserName,org.apache.hadoop.security.UserGroupInformation:getShortUserName(),1651,1653,"/**
* Retrieves and returns the short username. 
* @return short username as string or null if undefined
*/","* Get the user's login name.
   * @return the user's name up to the first '/' or '@'.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod),1834,1837,"/**
* Sets the authentication method for the user.
* @param authMethod new authentication method to apply
*/","* Sets the authentication method in the subject
   * 
   * @param authMethod authMethod.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.security.cert.X509Certificate)",349,360,"/**
* Validates SSL certificate using host and CN/subjectAltName checks.
* @param host array of hostnames to validate against
* @param cert X509Certificate object to verify
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.lang.String[],java.lang.String[],boolean,boolean)",362,456,"/**
* Validates the hostname of a server certificate against a list of allowed hosts.
* @param hosts list of allowed hostnames
* @param cns list of subject alternative names
* @param subjectAlts list of additional subject alternative names
* @param ie6 whether to consider Internet Explorer 6's quirks
* @param strictWithSubDomains whether to be strict with subdomains
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509TrustManager.java,<init>,"org.apache.hadoop.security.ssl.ReloadingX509TrustManager:<init>(java.lang.String,java.lang.String,java.lang.String)",72,78,"/**
* Initializes ReloadingX509TrustManager with custom trust manager from file.
* @param type custom trust manager type
* @param location path to trust manager file
* @param password optional password for encrypted file (may be null)
*/","* Creates a reloadable trustmanager. The trustmanager reloads itself
   * if the underlying trustore file has changed.
   *
   * @param type type of truststore file, typically 'jks'.
   * @param location local path to the truststore file.
   * @param password password of the truststore file.
   * changed, in milliseconds.
   * @throws IOException thrown if the truststore could not be initialized due
   * to an IO error.
   * @throws GeneralSecurityException thrown if the truststore could not be
   * initialized due to a security error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509TrustManager.java,loadFrom,org.apache.hadoop.security.ssl.ReloadingX509TrustManager:loadFrom(java.nio.file.Path),115,123,"/**
* Loads a trust manager from the specified file path.
* @param path file location of the trust manager configuration
* @return the current object (this) for method chaining
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509KeystoreManager.java,<init>,"org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",69,77,"/**
* Initializes ReloadingX509KeystoreManager with custom keystore settings.
* @param type Keystore type (e.g. JKS, PKCS12)
* @param location Path to the keystore file
* @param storePassword Password for accessing the keystore
* @param keyPassword Password for accessing the private keys
*/","* Construct a <code>Reloading509KeystoreManager</code>
   *
   * @param type type of keystore file, typically 'jks'.
   * @param location local path to the keystore file.
   * @param storePassword password of the keystore file.
   * @param keyPassword The password of the key.
   * @throws IOException raised on errors performing I/O.
   * @throws GeneralSecurityException thrown if create encryptor error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509KeystoreManager.java,loadFrom,org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:loadFrom(java.nio.file.Path),123,131,"/**
* Loads a ReloadingX509KeystoreManager instance from the specified file path.
* @param path file system path of the keystore
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,getResource,org.apache.hadoop.util.FindClass:getResource(java.lang.String),163,165,"/**
* Retrieves a resource by its name from the application configuration.
* @param name name of the resource to fetch
*/","* Get the resource
   * @param name resource name
   * @return URL or null for not found",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getConfResourceAsInputStream,org.apache.hadoop.conf.Configuration:getConfResourceAsInputStream(java.lang.String),2893,2908,"/**
* Retrieves a configuration resource as an InputStream.
* @param name unique resource identifier
* @return InputStream object or null if not found
*/","* Get an input stream attached to the configuration resource with the
   * given <code>name</code>.
   * 
   * @param name configuration resource name.
   * @return an input stream attached to the resource.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getConfResourceAsReader,org.apache.hadoop.conf.Configuration:getConfResourceAsReader(java.lang.String),2917,2932,"/**
* Loads a configuration resource as a Reader, returns null on failure.
* @param name unique resource identifier
*/","* Get a {@link Reader} attached to the configuration resource with the
   * given <code>name</code>.
   * 
   * @param name configuration resource name.
   * @return a reader attached to the resource.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,createSSLEngine,org.apache.hadoop.security.ssl.SSLFactory:createSSLEngine(),256,268,"/**
* Creates an SSLEngine instance with configured settings.
* @return the created SSLEngine object
*/","* Returns a configured SSLEngine.
   *
   * @return the configured SSLEngine.
   * @throws GeneralSecurityException thrown if the SSL engine could not
   * be initialized.
   * @throws IOException thrown if and IO error occurred while loading
   * the server keystore.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,configure,org.apache.hadoop.security.ssl.SSLFactory:configure(java.net.HttpURLConnection),358,372,"/**
* Configures an HTTP connection with SSL settings.
* @param conn HttpURLConnection instance to customize
* @return configured HttpURLConnection object
*/","* If the given {@link HttpURLConnection} is an {@link HttpsURLConnection}
   * configures the connection with the {@link SSLSocketFactory} and
   * {@link HostnameVerifier} of this SSLFactory, otherwise does nothing.
   *
   * @param conn the {@link HttpURLConnection} instance to configure.
   * @return the configured {@link HttpURLConnection} instance.
   *
   * @throws IOException if an IO error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,configureConnection,org.apache.hadoop.crypto.key.kms.KMSClientProvider:configureConnection(java.net.HttpURLConnection),488,500,"/**
* Configures the connection with SSL settings if available.
* @param conn HttpURLConnection to configure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,initializeSSLContext,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeSSLContext(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode),153,185,"/**
* Initializes the SSL context based on the specified channel mode.
* @param preferredChannelMode desired SSL channel mode
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(),236,239,"/**
* Creates and configures an SSL socket.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.Socket,java.lang.String,int,boolean)",241,248,"/**
* Creates a new SSL socket based on the provided parameters.
* @param s existing Socket object (optional)
* @param host hostname or IP address of the server
* @param port server port number
* @param autoClose whether to close the underlying socket when done
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",250,257,"/**
* Creates a new SSL socket with the specified address and port.
* @param address remote server address
* @param port remote server port number
* @param localAddress local host address (optional)
* @param localPort local host port number (optional)
* @return configured SSLSocket object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",259,266,"/**
* Creates an SSL socket with specified parameters.
* @param host remote server hostname
* @param port remote server port number
* @param localHost local machine IP address
* @param localPort local machine port number
* @return configured SSLSocket object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.InetAddress,int)",268,273,"/**
* Creates an SSL socket to the specified host and port.
* @param host server's address
* @param port server's port number
* @return configured SSLSocket object or throws IOException if creation fails.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.lang.String,int)",275,280,"/**
* Creates an SSL socket to the specified host and port.
* @param host target host address
* @param port target server port number
* @return configured Socket object or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configured.java,<init>,org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration),39,41,"/**
* Initializes configuration for this object.
* @param conf Configuration instance to be used
*/","Construct a Configured.
   * @param conf the Configuration object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,handleExecutorTimeout,"org.apache.hadoop.security.ShellBasedUnixGroupsMapping:handleExecutorTimeout(org.apache.hadoop.util.Shell$ShellCommandExecutor,java.lang.String)",174,192,"/**
* Handles executor timeout for a given user.
* @param executor ShellCommandExecutor instance
* @param user user identifier string
* @return true if handled, false otherwise
*/","* Check if the executor had a timeout and logs the event.
   * @param executor to check
   * @param user user to log
   * @return true if timeout has occurred",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,toString,org.apache.hadoop.util.Shell$ShellCommandExecutor:toString(),1312,1325,"/**
* Formats command-line arguments as a string.
* @return Formatted string representation of arguments
*/","* Returns the commands of this instance.
     * Arguments with spaces in are presented with quotes round; other
     * arguments are presented raw
     *
     * @return a string representation of the object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,read,org.apache.hadoop.security.SaslRpcServer$AuthMethod:read(java.io.DataInput),262,264,"/**
* Reads and returns an AuthMethod instance from the provided DataInput stream.
* @param in input data stream
*/","* Read from in.
     *
     * @param in DataInput.
     * @throws IOException raised on errors performing I/O.
     * @return AuthMethod.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByExactName,org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByExactName(java.lang.String),687,704,"/**
* Resolves an exact hostname to an InetAddress.
* @param host the hostname to resolve
* @return InetAddress object or null on failure
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getKerberosEntry,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getKerberosEntry(),2232,2285,"/**
* Configures Kerberos login module with principal and keytab/cache details.
* @return AppConfigurationEntry object or null if configuration fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,"org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping:<init>(java.util.Map,java.util.Map)",572,576,"/**
* Initializes static mappings for user and group IDs.
* @param uidMapping mapping of user IDs to their values
* @param gidMapping mapping of group IDs to their values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,org.apache.hadoop.security.ShellBasedIdMapping$PassThroughMap:<init>(),545,547,"/**
 * Initializes an empty PassThroughMap instance. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,addUser,org.apache.hadoop.security.authorize.AccessControlList:addUser(java.lang.String),152,159,"/**
* Adds a user to the list, but only if ACLs allow it or if all users are allowed.
* @param user unique user identifier
*/","* Add user to the names of users allowed for this service.
   * 
   * @param user
   *          The user name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,addGroup,org.apache.hadoop.security.authorize.AccessControlList:addGroup(java.lang.String),167,177,"/**
* Adds a user group to the mapping, throwing an exception for wildcard ACL values.
* @param group name of the group to add
*/","* Add group to the names of groups allowed for this service.
   * 
   * @param group
   *          The group name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,removeUser,org.apache.hadoop.security.authorize.AccessControlList:removeUser(java.lang.String),185,192,"/**
* Removes a user from the system, but only if not using wild card ACL value.
* @param user username of the user to remove
*/","* Remove user from the names of users allowed for this service.
   * 
   * @param user
   *          The user name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,removeGroup,org.apache.hadoop.security.authorize.AccessControlList:removeGroup(java.lang.String),200,208,"/**
* Removes a user group by name, throwing exception for wild-card ACL values.
* @param group name of the group to remove
*/","* Remove group from the names of groups allowed for this service.
   * 
   * @param group
   *          The group name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,getUsersString,org.apache.hadoop.security.authorize.AccessControlList:getUsersString(),337,339,"/**
* Retrieves users as a string representation.
* @return comma-separated list of user IDs
*/","* Returns comma-separated concatenated single String of the set 'users'
   *
   * @return comma separated list of users",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,getGroupsString,org.apache.hadoop.security.authorize.AccessControlList:getGroupsString(),346,348,"/**
* Retrieves and formats group string.
* @return concatenated groups string or empty string if none","* Returns comma-separated concatenated single String of the set 'groups'
   *
   * @return comma separated list of groups",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,printStackTrace,org.apache.hadoop.security.authorize.AuthorizationException:printStackTrace(),65,68,"/**
* Prints stack trace to standard error output.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,getProxyGroups,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxyGroups(),175,182,"/**
* Retrieves a map of proxy groups by user ID.
* @return Map of group names to collections of affected users
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,getProxyHosts,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxyHosts(),184,193,"/**
* Retrieves a map of proxy hosts with their associated machine lists.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,innerSetCredential,"org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:innerSetCredential(java.lang.String,char[])",266,281,"/**
* Sets a credential entry with the given alias and material.
* @param alias unique identifier for the credential
* @param material character array containing the secret data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,validate,org.apache.hadoop.security.alias.CredentialShell$CheckCommand:validate(),321,346,"/**
* Validates the input parameters and credential provider.
* @return true if valid, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,validate,org.apache.hadoop.security.alias.CredentialShell$CreateCommand:validate(),411,436,"/**
* Validates the input parameters for credential retrieval.
* @return true if valid, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,execute,org.apache.hadoop.security.alias.CredentialShell$CheckCommand:execute(),348,386,"/**
* Checks password match for a given alias using the specified credential provider.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,promptForCredential,org.apache.hadoop.security.alias.CredentialShell:promptForCredential(),473,499,"/**
* Prompts user to enter and confirm a password.
* @return char[] containing the confirmed password or null on error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,warnIfTransientProvider,org.apache.hadoop.security.alias.CredentialShell$Command:warnIfTransientProvider(),172,176,"/**
* Issues warning when attempting to modify a transient provider.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,createPermissions,org.apache.hadoop.security.alias.LocalKeyStoreProvider:createPermissions(java.lang.String),80,90,"/**
* Creates file permissions from a string representation.
* @param perms String with octal permission value (e.g. ""700"")
* @throws IOException if invalid permissions format is provided
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,isOriginalTGT,org.apache.hadoop.security.SecurityUtil:isOriginalTGT(javax.security.auth.kerberos.KerberosTicket),168,170,"/**
* Checks if a Kerberos Ticket is an original TGT.
* @param ticket Kerberos Ticket to check
* @return true if the ticket's server principal is not a TGS, false otherwise
*/","* Check whether the server principal is the TGS's principal
   * @param ticket the original TGT (the ticket that is obtained when a 
   * kinit is done)
   * @return true or false",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setSslConfiguration,"org.apache.hadoop.security.SecurityUtil:setSslConfiguration(org.apache.zookeeper.client.ZKClientConfig,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore,org.apache.zookeeper.common.ClientX509Util)",825,850,"/**
* Configures the ZooKeeper client to use SSL/TLS encryption with specified truststore and keystore.
* @param zkClientConfig ZK client configuration
* @param truststoreKeystore Truststore and keystore details
* @param x509Util Client X.509 utility for SSL configuration
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KerberosAuthException.java,<init>,"org.apache.hadoop.security.KerberosAuthException:<init>(java.lang.String,java.lang.Throwable)",51,54,"/**
* Constructs a KerberosAuthException with an initial error message.
* @param initialMsg initial error message
* @param cause underlying exception (if any)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/WhitelistBasedResolver.java,getServerProperties,org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.net.InetAddress),116,122,"/**
* Returns server properties based on the client's IP address.
* @param clientAddress IP address of the client
* @return Map of server properties or default SASL props if not in whitelist
*/","* Identify the Sasl Properties to be used for a connection with a client.
   * @param clientAddress client's address
   * @return the sasl properties to be used for the connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,getIdentifier,"org.apache.hadoop.security.SaslRpcServer:getIdentifier(java.lang.String,org.apache.hadoop.security.token.SecretManager)",192,204,"/**
* Retrieves a TokenIdentifier of type T from the given secret manager.
* @param id unique identifier string
* @param secretManager SecretManager instance for token creation
* @return TokenIdentifier object or null if not found/created
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,checkCodec,org.apache.hadoop.crypto.CryptoStreamUtils:checkCodec(org.apache.hadoop.crypto.CryptoCodec),75,81,"/**
* Validates the provided CryptoCodec instance.
* @param codec CryptoCodec object to validate
*/","* AES/CTR/NoPadding or SM4/CTR/NoPadding is required.
   *
   * @param codec crypto codec.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,<init>,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:<init>(),35,40,"/**
* Initializes OpenSSL AES CTR codec, throwing exception on load failure.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getPos,org.apache.hadoop.crypto.CryptoInputStream:getPos(),580,585,"/**
* Calculates and returns current file position.
*/",Get underlying stream position.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,available,org.apache.hadoop.crypto.CryptoInputStream:available(),672,677,"/**
* Returns total bytes available to read from this stream.
* @return Total available bytes
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFromUnderlyingStream,org.apache.hadoop.crypto.CryptoInputStream:readFromUnderlyingStream(java.nio.ByteBuffer),223,231,"/**
* Reads data from underlying stream into ByteBuffer.
* @param inBuffer target buffer for read operation
* @return number of bytes read or -1 on error
*/",Read data from underlying stream.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,<init>,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:<init>(int,java.lang.String,org.apache.hadoop.crypto.CipherSuite,java.lang.String)",115,126,"/**
* Initializes JceCtrCipher with specified parameters.
* @param mode encryption mode
* @param provider name of cryptographic provider (optional)
* @param suite chosen encryption algorithm (e.g. AES, RSA)
* @param name unique identifier for cipher instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CipherSuite.java,convert,org.apache.hadoop.crypto.CipherSuite:convert(java.lang.String),84,92,"/**
* Converts a string cipher suite name to its corresponding CipherSuite enum value.
* @param name the name of the cipher suite
* @return the CipherSuite enum value or throws an exception if invalid
*/","* Convert to CipherSuite from name, {@link #algoBlockSize} is fixed for
   * certain cipher suite, just need to compare the name.
   * @param name cipher suite name
   * @return CipherSuite cipher suite",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,encrypt,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:encrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",140,143,"/**
 * Encrypts data from input buffer and writes result to output buffer.
 * @param inBuffer input data to be encrypted
 * @param outBuffer output buffer for encrypted data
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,decrypt,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:decrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",145,148,"/**
* Decrypts input data and writes result to output buffer.
* @param inBuffer input ByteBuffer containing encrypted data
* @param outBuffer output ByteBuffer for decrypted data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoProtocolVersion.java,supports,org.apache.hadoop.crypto.CryptoProtocolVersion:supports(org.apache.hadoop.crypto.CryptoProtocolVersion),54,64,"/**
* Checks if the provided crypto protocol version is supported.
* @param version target CryptoProtocolVersion instance
* @return true if version is supported, false otherwise
*/","* Returns if a given protocol version is supported.
   *
   * @param version version number
   * @return true if the version is supported, else false",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CipherOption.java,<init>,org.apache.hadoop.crypto.CipherOption:<init>(org.apache.hadoop.crypto.CipherSuite),34,36,"/**
* Constructs a CipherOption instance from a CipherSuite.
* @param suite The CipherSuite to use
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,tokenizeTransformation,org.apache.hadoop.crypto.OpensslCipher:tokenizeTransformation(java.lang.String),155,179,"/**
* Parses a cipher transformation string into its algorithm, mode, and padding components.
* @param transformation string representation of the transformation (e.g., AES/CTR/NoPadding)
* @return Transform object or throws NoSuchAlgorithmException if invalid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,finalize,org.apache.hadoop.crypto.OpensslCipher:finalize(),293,296,"/**
 * Invokes cleanup routine before JVM garbage collection.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OpensslSecureRandom.java,next,org.apache.hadoop.crypto.random.OpensslSecureRandom:next(int),105,118,"/**
* Computes the next integer value of a specified bit length.
* @param numBits number of bits in the desired value
* @return the computed integer value or an invalid result if numBits is out of range
*/","* Generates an integer containing the user-specified number of
   * random bits (right justified, with leading zeros).
   *
   * @param numBits number of random bits to be generated, where
   * 0 {@literal <=} <code>numBits</code> {@literal <=} 32.
   *
   * @return int an <code>int</code> containing the user-specified number
   * of random bits (right justified, with leading zeros).",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeyVersion(java.lang.String),327,350,"/**
* Retrieves a KeyVersion object for the specified version name.
* @param versionName unique version identifier
* @return KeyVersion object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,innerSetKeyVersion,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:innerSetKeyVersion(java.lang.String,java.lang.String,byte[],java.lang.String)",495,506,"/**
* Sets a key version with specified name and material.
* @param name unique key identifier
* @param versionName version of the key
* @param material encrypted key data
* @param cipher encryption algorithm used
* @return new KeyVersion object representing the set key
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSKeyVersion:<init>(java.lang.String,java.lang.String,byte[])",611,613,"/**
* Constructs a KMSKeyVersion object with specified parameters.
* @param keyName unique key identifier
* @param versionName version of the key
* @param material encrypted data for the key version
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,createKeyProviderCryptoExtension,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:createKeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider),605,621,"/**
* Creates a KeyProviderCryptoExtension instance.
* @param keyProvider the underlying key provider
*/","* Creates a <code>KeyProviderCryptoExtension</code> using a given
   * {@link KeyProvider}.
   * <p>
   * If the given <code>KeyProvider</code> implements the
   * {@link CryptoExtension} interface the <code>KeyProvider</code> itself
   * will provide the extension functionality.
   * If the given <code>KeyProvider</code> implements the
   * {@link KeyProviderExtension} interface and the KeyProvider being
   * extended by the <code>KeyProvider</code> implements the
   * {@link CryptoExtension} interface, the KeyProvider being extended will
   * provide the extension functionality. Otherwise, a default extension
   * implementation will be used.
   *
   * @param keyProvider <code>KeyProvider</code> to use to create the
   * <code>KeyProviderCryptoExtension</code> extension.
   * @return a <code>KeyProviderCryptoExtension</code> instance using the
   * given <code>KeyProvider</code>.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,close,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:close(),623,629,"/**
* Closes the key provider and any external providers, but not this instance.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,readObject,org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:readObject(java.io.ObjectInputStream),700,705,"/**
* Reads object from stream and initializes internal state.
* @throws IOException if reading fails
* @throws ClassNotFoundException if class not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSMetadata:<init>(java.lang.String,int,java.lang.String,java.util.Map,java.util.Date,int)",647,650,"/**
* Initializes a new instance of KMSMetadata with specified parameters.
* @param cipher encryption algorithm
* @param bitLength key length in bits
* @param description metadata description
* @param attributes additional metadata attributes
* @param created creation date and time
* @param versions number of metadata versions
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,writeObject,org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:writeObject(java.io.ObjectOutputStream),694,698,"/**
* Serializes object metadata and writes it to output stream.
* @throws IOException if serialization or writing fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,printException,org.apache.hadoop.crypto.key.KeyShell:printException(java.lang.Exception),533,537,"/**
* Prints an Exception to the error stream.
* @param e the Exception to be printed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$ListCommand:execute(),252,271,"/**
* Lists and optionally displays metadata for all keys in the KeyProvider.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,getKeysMetadata,org.apache.hadoop.crypto.key.KeyProviderExtension:getKeysMetadata(java.lang.String[]),61,64,"/**
* Retrieves metadata for specified key(s).
* @param names variable number of key names
* @return array of Metadata objects or null if none found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,cleanupNewAndOld,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:cleanupNewAndOld(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",600,605,"/**
* Cleans up temporary directories by renaming and deleting them.
* @param newPath new directory path to be renamed to CURRENT
* @param oldPath old directory path to be deleted
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,backupToOld,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:backupToOld(org.apache.hadoop.fs.Path),622,630,"/**
* Renames the current backup to an ""old"" location.
* @param oldPath new path for the existing backup
* @return true if successful, false if the old backup doesn't exist
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,revertFromOld,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:revertFromOld(org.apache.hadoop.fs.Path,boolean)",632,637,"/**
* Reverts to an older version of a file by renaming it back.
* @param oldPath Path to the original file
* @param fileExisted true if the file existed before, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,deleteKey,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:deleteKey(java.lang.String),462,493,"/**
* Deletes a key by name and all its versions.
* @param name unique key identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,getAlgorithm,org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:getAlgorithm(),679,682,"/**
* Retrieves the algorithm used by this cipher.
* @return the cipher's algorithm name",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,getCurrentKey,org.apache.hadoop.crypto.key.KeyProvider:getCurrentKey(java.lang.String),496,502,"/**
* Retrieves the current key version for a given user by ID.
* @param name unique user identifier
* @return KeyVersion object or null if metadata not found
*/","* Get the current version of the key, which should be used for encrypting new
   * data.
   * @param name the base name of the key
   * @return the version name of the current version of the key or null if the
   *    key version doesn't exist
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,generateKey,"org.apache.hadoop.crypto.key.KeyProvider:generateKey(int,java.lang.String)",545,552,"/**
* Generates a cryptographic key with specified size and algorithm.
* @param size length of the generated key
* @param algorithm name of the used encryption algorithm (e.g. AES, RSA)
* @return byte array representing the generated key
*/","* Generates a key material.
   *
   * @param size length of the key.
   * @param algorithm algorithm to use for generating the key.
   * @return the generated key.
   * @throws NoSuchAlgorithmException no such algorithm exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,<init>,"org.apache.hadoop.crypto.key.CachingKeyProvider:<init>(org.apache.hadoop.crypto.key.KeyProvider,long,long)",91,95,"/**
* Initializes caching key provider with given key provider and timeout values.
* @param keyProvider Key provider instance
* @param keyTimeoutMillis Milliseconds before key expires
* @param currKeyTimeoutMillis Current milliseconds before key expires
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,invalidateCache,org.apache.hadoop.crypto.key.CachingKeyProvider:invalidateCache(java.lang.String),156,164,"/**
* Invalidates cache for a given key.
* @param name unique key identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,invalidateCache,org.apache.hadoop.crypto.key.KeyProviderExtension:invalidateCache(java.lang.String),120,123,"/**
 * Invalidates cache with specified name.
 * @param name unique cache identifier
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand:execute(),511,525,"/**
* Invalidates cache on KeyProvider for a given key name.
* @throws IOException if cache invalidation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,toJSON,org.apache.hadoop.util.KMSUtil:toJSON(org.apache.hadoop.crypto.key.KeyProvider$KeyVersion),95,108,"/**
* Converts KeyProvider.KeyVersion to JSON Map.
* @param keyVersion KeyVersion object to convert
* @return JSON representation of the KeyVersion or an empty map if null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,generateEncryptedKey,"org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:generateEncryptedKey(org.apache.hadoop.crypto.Encryptor,org.apache.hadoop.crypto.key.KeyProvider$KeyVersion,byte[],byte[])",307,325,"/**
* Encrypts a key using the provided encryptor and encryption key.
* @param encryptor encryption engine
* @param encryptionKey key used for encryption
* @param key data to be encrypted
* @param iv initialization vector for encryption
* @return EncryptedKeyVersion object containing encrypted key metadata
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,createForDecryption,"org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:createForDecryption(java.lang.String,java.lang.String,byte[],byte[])",103,110,"/**
* Creates an EncryptedKeyVersion for decryption.
* @param keyName unique key identifier
* @param encryptionKeyVersionName name of the encryption key version
* @param encryptedKeyIv initialization vector for encrypted key material
* @param encryptedKeyMaterial encrypted key data
* @return new EncryptedKeyVersion object","* Factory method to create a new EncryptedKeyVersion that can then be
     * passed into {@link #decryptEncryptedKey}. Note that the fields of the
     * returned EncryptedKeyVersion will only partially be populated; it is not
     * necessarily suitable for operations besides decryption.
     *
     * @param keyName Key name of the encryption key use to encrypt the
     *                encrypted key.
     * @param encryptionKeyVersionName Version name of the encryption key used
     *                                 to encrypt the encrypted key.
     * @param encryptedKeyIv           Initialization vector of the encrypted
     *                                 key. The IV of the encryption key used to
     *                                 encrypt the encrypted key is derived from
     *                                 this IV.
     * @param encryptedKeyMaterial     Key material of the encrypted key.
     * @return EncryptedKeyVersion suitable for decryption.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,decryptEncryptedKey,"org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.Decryptor,org.apache.hadoop.crypto.key.KeyProvider$KeyVersion,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)",410,431,"/**
* Decrypts an encrypted key using the provided decryptor and encryption key.
* @param decryptor decryption utility
* @param encryptionKey encryption key used for decryption
* @param encryptedKeyVersion encrypted key to be decrypted
* @return decrypted KeyVersion object or null if unsuccessful
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderDelegationTokenExtension.java,createKeyProviderDelegationTokenExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:createKeyProviderDelegationTokenExtension(org.apache.hadoop.crypto.key.KeyProvider),138,148,"/**
* Creates a KeyProvider with DelegationToken extension.
* @param keyProvider existing KeyProvider instance
*/","* Creates a <code>KeyProviderDelegationTokenExtension</code> using a given 
   * {@link KeyProvider}.
   * <p>
   * If the given <code>KeyProvider</code> implements the 
   * {@link DelegationTokenExtension} interface the <code>KeyProvider</code> 
   * itself will provide the extension functionality, otherwise a default 
   * extension implementation will be used.
   * 
   * @param keyProvider <code>KeyProvider</code> to use to create the 
   * <code>KeyProviderDelegationTokenExtension</code> extension.
   * @return a <code>KeyProviderDelegationTokenExtension</code> instance 
   * using the given <code>KeyProvider</code>.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,validate,org.apache.hadoop.crypto.key.KeyShell$CreateCommand:validate(),431,454,"/**
* Validates input parameters and key provider.
*@return true if valid, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,writeJson,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:writeJson(java.lang.Object,java.io.OutputStream)",253,257,"/**
* Serializes an Object to JSON and writes it to the specified OutputStream.
* @param obj object to serialize
* @param os output stream for the serialized JSON data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,checkNotEmpty,"org.apache.hadoop.util.KMSUtil:checkNotEmpty(java.lang.String,java.lang.String)",133,141,"/**
* Verifies that the input string is not empty.
* @param s the input string to verify
* @param name the name of the parameter being checked
* @return the non-empty string, or throws IllegalArgumentException if empty
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,warmUpEncryptedKeys,org.apache.hadoop.crypto.key.kms.KMSClientProvider:warmUpEncryptedKeys(java.lang.String[]),951,959,"/**
* Initializes and warms up encrypted keys.
* @param keyNames variable number of key names to initialize
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,close,org.apache.hadoop.crypto.key.kms.KMSClientProvider:close(),1195,1207,"/**
* Closes the encryption key version queue and SSL factory resources.
* @throws IOException if shutdown or destruction fails
*/",* Shutdown valueQueue executor threads,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,submitRefillTask,"org.apache.hadoop.crypto.key.kms.ValueQueue:submitRefillTask(java.lang.String,java.util.Queue)",401,443,"/**
* Submits refill task to queue for execution.
* @param keyName unique identifier for refilling
* @param keyQueue Queue of elements to be refilled
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,deleteByName,org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:deleteByName(java.lang.String),187,194,"/**
* Cancels and removes a named runnable by its name.
* @param name unique identifier of the runnable
* @return the cancelled Runnable object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getLock,org.apache.hadoop.crypto.key.kms.ValueQueue:getLock(java.lang.String),136,138,"/**
* Retrieves a ReadWriteLock instance based on the provided key name.
* @param keyName unique identifier for locking purposes
*/","* Get the stripped lock given a key name.
   *
   * @param keyName The key name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,flush,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:flush(),557,567,"/**
* Flushes all KMS client providers and logs any errors.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,isTransient,org.apache.hadoop.crypto.key.KeyProviderExtension:isTransient(),56,59,"/**
* Checks whether this entity's persistence state is transient.
* @return true if transient, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,warnIfTransientProvider,org.apache.hadoop.crypto.key.KeyShell$Command:warnIfTransientProvider(),219,223,"/**
* Warns about modifying a transient data provider.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,createKeyProviderFromUri,"org.apache.hadoop.util.KMSUtil:createKeyProviderFromUri(org.apache.hadoop.conf.Configuration,java.net.URI)",81,93,"/**
* Creates a KeyProvider instance from the given URI and configuration.
* @param conf Hadoop Configuration object
* @param providerUri URI of the KeyProvider to create
* @return KeyProvider instance or throws IOException if instantiation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,append,org.apache.hadoop.ipc.CallerContext$Builder:append(java.lang.String),207,215,"/**
* Appends a field to the builder.
* @param field string value of the field
*/","* Append new field to the context.
     * @param field one of fields to append.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,append,"org.apache.hadoop.ipc.CallerContext$Builder:append(java.lang.String,java.lang.String)",223,231,"/**
* Appends a key-value pair to the builder's string buffer.
* @param key unique identifier
* @param value associated value
* @return Builder instance for method chaining
*/","* Append new field which contains key and value to the context.
     * @param key the key of field.
     * @param value the value of field.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,appendIfAbsent,"org.apache.hadoop.ipc.CallerContext$Builder:appendIfAbsent(java.lang.String,java.lang.String)",240,251,"/**
* Adds a key-value pair to the builder if it's not already present.
* @param key unique key
* @param value associated value
* @return Builder instance for chaining
*/","* Append new field which contains key and value to the context
     * if the key(""key:"") is absent.
     * @param key the key of field.
     * @param value the value of field.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,"org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,java.lang.String)",148,154,"/**
* Constructs a new Builder instance with initial context and field separator.
* @param context initial context to append
* @param separator field separator string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RefreshResponse.java,successResponse,org.apache.hadoop.ipc.RefreshResponse:successResponse(),37,39,"/**
 * Returns a default RefreshResponse with a successful status.
 */","* Convenience method to create a response for successful refreshes.
   * @return void response",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,unpack,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:unpack(org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto),82,104,"/**
* Unpacks protobuf data into a RefreshResponse object.
* @param proto GenericRefreshResponseProto object containing unpacked data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolServerSideTranslatorPB.java,pack,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:pack(java.util.Collection),66,83,"/**
* Converts a collection of RefreshResponse objects into a GenericRefreshResponseCollectionProto.
* @param responses Collection of refresh response data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ObserverRetryOnActiveException.java,<init>,org.apache.hadoop.ipc.ObserverRetryOnActiveException:<init>(java.lang.String),32,34,"/**
* Creates an observer retry exception with custom message.
* @param msg error message to be displayed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientId.java,toString,org.apache.hadoop.ipc.ClientId:toString(byte[]),52,62,"/**
* Converts a byte array client ID to its string representation.
* @param clientId unique client identifier as a byte array
*/","* @return Convert a clientId byte[] to string.
   * @param clientId input clientId.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntry:<init>(byte[],int,long)",72,82,"/**
* Constructs a CacheEntry object from client ID, call ID, and expiration time.
* @param clientId unique 16-byte identifier
* @param callId call identifier
* @param expirationTime cache entry expiration time
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,isServerFailOverEnabledByQueue,org.apache.hadoop.ipc.CallQueueManager:isServerFailOverEnabledByQueue(),242,249,"/**
* Checks if server fail-over is enabled via the queue.
* @return true if enabled, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getPriorityLevel,org.apache.hadoop.ipc.Server:getPriorityLevel(org.apache.hadoop.ipc.Schedulable),722,725,"/**
* Retrieves priority level from schedulable object.
* @param e Schedulable object to fetch priority level from
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isClientBackoffEnabled,org.apache.hadoop.ipc.Server:isClientBackoffEnabled(),3872,3874,"/**
* Checks whether client backoff is enabled.
* @return true if enabled, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,addInternal,"org.apache.hadoop.ipc.CallQueueManager:addInternal(org.apache.hadoop.ipc.Schedulable,boolean)",306,321,"/**
* Adds an internal element to the collection, handling backoff and overflow exceptions.
* @param e the element to add
* @param checkBackoff whether to enforce client backoff rules
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,offer,org.apache.hadoop.ipc.CallQueueManager:offer(java.lang.Object),335,338,"/**
* Offers an element to the underlying reference queue.
* @param e element to be offered
*/","* Insert e into the backing queue.
   * Return true if e is queued.
   * Return false if the queue is full.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,offer,"org.apache.hadoop.ipc.CallQueueManager:offer(java.lang.Object,long,java.util.concurrent.TimeUnit)",340,344,"/**
* Offers an element to the blocking queue using the given timeout.
* @param e element to be offered
* @param timeout time limit for offering (in specified units)
* @param unit unit of the timeout value
* @return true if successfully offered, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getCallQueueLen,org.apache.hadoop.ipc.Server:getCallQueueLen(),3868,3870,"/**
* Retrieves the current length of the call queue.
* @return Size of the callQueue list
*/","* The number of rpc calls in the queue.
   * @return The number of rpc calls in the queue.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolInterfaces,org.apache.hadoop.ipc.RPC:getProtocolInterfaces(java.lang.Class),144,147,"/**
* Retrieves super interfaces of a given protocol class.
* @param protocol the protocol class to fetch interfaces for
* @return array of super interface classes or empty array if none found
*/","* Get all interfaces that the given protocol implements or extends
   * which are assignable from VersionedProtocol.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getServerAddress,org.apache.hadoop.ipc.RPC:getServerAddress(java.lang.Object),740,742,"/**
* Retrieves server address from proxy connection ID.
* @param proxy object containing proxy connection information
*/","* @return Returns the server address for a given proxy.
   * @param proxy input proxy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,org.apache.hadoop.ipc.CallerContext:<init>(org.apache.hadoop.ipc.CallerContext$Builder),71,74,"/**
* Constructs a new instance of CallerContext using provided Builder.
* @param builder context and signature builder
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,toString,org.apache.hadoop.ipc.CallerContext:toString(),112,123,"/**
* Returns a string representation of the object.
* @return A formatted string containing the context and optional signature.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,sendPing,org.apache.hadoop.ipc.Client$Connection:sendPing(),1071,1080,"/**
* Sends periodic ping request to server.
* @throws IOException if communication fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,registerProtocolEngine,"org.apache.hadoop.ipc.Server:registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcInvoker)",288,300,"/**
* Registers a protocol engine for the specified RPC kind.
* @param rpcKind unique identifier of the RPC kind
* @param rpcRequestWrapperClass wrapper class for RPC requests
* @param rpcInvoker RPC invoker instance
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ExternalCall.java,get,org.apache.hadoop.ipc.ExternalCall:get(),47,53,"/**
* Retrieves the computed result.
* @throws InterruptedException if interrupted while waiting for completion
* @throws ExecutionException if an error occurred during computation",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Timer.java,monotonicNowNanos,org.apache.hadoop.util.Timer:monotonicNowNanos(),58,60,"/**
* Returns the current monotonic time in nanoseconds.
* @return Monotonic timestamp as a long integer
*/","* Same as {@link #monotonicNow()} but returns its result in nanoseconds.
   * Note that this is subject to the same resolution constraints as
   * {@link System#nanoTime()}.
   * @return a monotonic clock that counts in nanoseconds.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getUserGroupInformation,org.apache.hadoop.ipc.Server$Call:getUserGroupInformation(),1112,1115,"/**
* Returns user group information based on remote user.
* @return UserGroupInformation object or null if not applicable
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemoteUser,org.apache.hadoop.ipc.Server:getRemoteUser(),445,448,"/**
* Retrieves remote user information from the current call context.
* @return UserGroupInformation object or null if not applicable
*/","Returns the RPC remote user when invoked inside an RPC.  Note this
   *  may be different than the current user if called within another doAs
   *  @return connection's UGI or null if not an RPC",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doResponse,org.apache.hadoop.ipc.Server$Call:doResponse(java.lang.Throwable),1105,1107,"/**
* Handles fatal response to an RPC request.
* @param t Throwable exception causing the error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,<init>,"org.apache.hadoop.ipc.DecayRpcScheduler$DecayTask:<init>(org.apache.hadoop.ipc.DecayRpcScheduler,java.util.Timer)",210,213,"/**
* Initializes a DecayTask instance with a DecayRpcScheduler and Timer.
* @param scheduler RPC scheduler to use
* @param timer system timer for scheduling tasks
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,putVersionSignatureMap,"org.apache.hadoop.ipc.RpcClientUtil:putVersionSignatureMap(java.net.InetSocketAddress,java.lang.String,java.lang.String,java.util.Map)",86,89,"/**
* Stores a version signature map in the cache for a given address and RPC kind.
* @param addr server address
* @param protocol communication protocol
* @param rpcKind type of remote procedure call
* @param map map of signatures to store
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,getVersionSignatureMap,"org.apache.hadoop.ipc.RpcClientUtil:getVersionSignatureMap(java.net.InetSocketAddress,java.lang.String,java.lang.String)",91,94,"/**
* Retrieves version-specific protocol signature map.
* @param addr server address
* @param protocol communication protocol
* @param rpcKind RPC kind (e.g. ""full"", ""light"")
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getFingerprints,org.apache.hadoop.ipc.ProtocolSignature:getFingerprints(java.lang.reflect.Method[]),121,130,"/**
* Computes an array of method fingerprints from the given methods.
* @param methods array of Method objects
* @return int[] of method fingerprints or null if input is null
*/","* Convert an array of Method into an array of hash codes
   * 
   * @param methods
   * @return array of hash codes",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,convertProtocolSignatureProtos,org.apache.hadoop.ipc.RpcClientUtil:convertProtocolSignatureProtos(java.util.List),149,161,"/**
* Converts a list of protocol signature proto objects to a map by version.
* @param protoList list of ProtocolSignatureProto objects
* @return Map of ProtocolSignature objects keyed by version
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,methodExists,"org.apache.hadoop.ipc.RpcClientUtil:methodExists(int,long,java.util.Map)",163,174,"/**
* Checks if a method with given hash exists in protocol version.
* @param methodHash unique method identifier
* @param version protocol version number
* @param versionMap map of protocol versions to signatures
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RefreshRegistry.java,dispatch,"org.apache.hadoop.ipc.RefreshRegistry:dispatch(java.lang.String,java.lang.String[])",94,131,"/**
* Dispatches refresh requests to registered handlers for a given identifier.
* @param identifier unique handler ID
* @param args array of arguments passed to handlers
* @return Collection of RefreshResponse objects from each handler
*/","* Lookup the responsible handler and return its result.
   * This should be called by the RPC server when it gets a refresh request.
   * @param identifier the resource to refresh
   * @param args the arguments to pass on, not including the program name
   * @throws IllegalArgumentException on invalid identifier
   * @return the response from the appropriate handler",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,<init>,"org.apache.hadoop.ipc.RemoteException:<init>(java.lang.String,java.lang.String)",40,42,"/**
* Constructs a RemoteException with specified class name and message.
* @param className the name of the class that threw the exception
* @param msg the detail message of the exception
*/","* @param className wrapped exception, may be null
   * @param msg may be null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,unwrapRemoteException,org.apache.hadoop.ipc.RemoteException:unwrapRemoteException(java.lang.Class[]),81,96,"/**
* Unwraps a remote IOException by looking for matching subtypes.
* @param lookupTypes classes to check for subtype match
* @return unwrapped IOException or this if no match found
*/","* If this remote exception wraps up one of the lookupTypes
   * then return this exception.
   * <p>
   * Unwraps any IOException.
   * 
   * @param lookupTypes the desired exception class. may be null.
   * @return IOException, which is either the lookupClass exception or this.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,unwrapRemoteException,org.apache.hadoop.ipc.RemoteException:unwrapRemoteException(),107,115,"/**
* Unwraps a remote IOException and returns an instance of IOException.
* @return IOException instance or the current object if unwrapping fails
*/","* Instantiate and return the exception wrapped up by this remote exception.
   * 
   * <p> This unwraps any <code>Throwable</code> that has a constructor taking
   * a <code>String</code> as a parameter.
   * Otherwise it returns this.
   * 
   * @return <code>Throwable</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getNumInProcessHandler,org.apache.hadoop.ipc.metrics.RpcMetrics:getNumInProcessHandler(),157,160,"/**
* Retrieves the number of handlers currently being processed.
* @return Count of active handlers
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getTotalRequests,org.apache.hadoop.ipc.metrics.RpcMetrics:getTotalRequests(),175,178,"/**
 * Retrieves the total number of requests from the server.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getTotalRequestsPerSecond,org.apache.hadoop.ipc.metrics.RpcMetrics:getTotalRequestsPerSecond(),180,183,"/**
* Returns the total number of requests processed by the server per second.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$Call:<init>(int,int,org.apache.hadoop.ipc.RPC$RpcKind,byte[],org.apache.hadoop.tracing.Span,org.apache.hadoop.ipc.CallerContext)",1000,1012,"/**
* Initializes a new Call object with given parameters.
* @param id unique call identifier
* @param retryCount initial retry count
* @param kind RPC operation type (e.g. request, response)
* @param clientId client identifier byte array
* @param span tracing information
* @param callerContext caller context data
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProcessingDetails.java,get,"org.apache.hadoop.ipc.ProcessingDetails:get(org.apache.hadoop.ipc.ProcessingDetails$Timing,java.util.concurrent.TimeUnit)",73,75,"/**
* Converts timing to specified unit.
* @param type Timing type (e.g. DAY, HOUR)
* @param timeUnit Target time unit
* @return Time in target unit
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProcessingDetails.java,toString,org.apache.hadoop.ipc.ProcessingDetails:toString(),97,108,"/**
* Returns a string representation of the object, listing all Timing types with their corresponding times.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedTimeCostProvider.java,getCost,org.apache.hadoop.ipc.WeightedTimeCostProvider:getCost(org.apache.hadoop.ipc.ProcessingDetails),100,109,"/**
* Calculates total processing cost based on provided ProcessingDetails and pre-initialized weights.
* @param details ProcessingDetails object containing timing values
*/","* Calculates a weighted sum of the times stored on the provided processing
   * details to be used as the cost in {@link DecayRpcScheduler}.
   *
   * @param details Processing details
   * @return The weighted sum of the times. The returned unit is the same
   *         as the default unit used by the provided processing details.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProcessingDetails.java,set,"org.apache.hadoop.ipc.ProcessingDetails:set(org.apache.hadoop.ipc.ProcessingDetails$Timing,long,java.util.concurrent.TimeUnit)",81,83,"/**
* Converts and sets timing value using specified unit.
* @param type Timing type
* @param value Value to be converted and set
* @param timeUnit Unit of the value (e.g. TimeUnit.SECONDS)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getSchedulingDecisionSummary,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getSchedulingDecisionSummary(),904,912,"/**
* Retrieves scheduling decision summary from active scheduler.
* @return Summary string or default message if no scheduler is available
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getUniqueIdentityCount,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getUniqueIdentityCount(),924,932,"/**
* Retrieves count of unique identities from the RPC scheduler.
* @return Unique identity count or -1 if scheduler is unavailable
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getTotalCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getTotalCallVolume(),934,942,"/**
* Retrieves total call volume from the current RPC scheduler.
* @return Total call volume or -1 if no scheduler is available
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getAverageResponseTime,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getAverageResponseTime(),944,952,"/**
* Calculates the average response time for the scheduled tasks.
* @return array of average response times or default values if no scheduler is available
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getResponseTimeCountInLastWindow,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getResponseTimeCountInLastWindow(),954,961,"/**
* Retrieves response time count for the last window.
* @return array of long values or default values if no scheduler is available
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getNumDroppedConnections,org.apache.hadoop.ipc.Server:getNumDroppedConnections(),3859,3862,"/**
* Retrieves the total number of dropped connections.
* @return count of dropped connections
*/","* The number of RPC connections dropped due to
   * too many connections.
   * @return the number of dropped rpc connections",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isFull,org.apache.hadoop.ipc.Server$ConnectionManager:isFull(),4088,4091,"/**
* Checks if the pool is full based on current size and maximum connections.
* @return true if pool capacity reached, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getNumOpenConnections,org.apache.hadoop.ipc.Server:getNumOpenConnections(),3837,3839,"/**
* Returns the number of open connections.
* @return Count of active connections managed by ConnectionManager
*/","* The number of open RPC conections
   * @return the number of open rpc connections",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getConnections,org.apache.hadoop.ipc.Server:getConnections(),756,759,"/**
* Returns an array of connections managed by the connection manager.
* @return Array of Connection objects
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,startIdleScan,org.apache.hadoop.ipc.Server$ConnectionManager:startIdleScan(),4159,4161,"/**
 * Initiates an idle scan process by scheduling a dedicated task.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,putQueue,"org.apache.hadoop.ipc.FairCallQueue:putQueue(int,org.apache.hadoop.ipc.Schedulable)",229,233,"/**
* Adds an element to the queue with specified priority.
* @param priority queue index
* @param e data to enqueue
*/","* Put the element in a queue of a specific priority.
   * @param priority - queue priority
   * @param e - element to add",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offerQueue,"org.apache.hadoop.ipc.FairCallQueue:offerQueue(int,org.apache.hadoop.ipc.Schedulable)",241,248,"/**
* Offers an element to a queue based on priority.
* @param priority queue priority level
* @param e element to offer
* @return true if offered successfully, false otherwise
*/","* Offer the element to queue of a specific priority.
   * @param priority - queue priority
   * @param e - element to add
   * @return boolean if added to the given queue",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,"org.apache.hadoop.ipc.FairCallQueue:offer(org.apache.hadoop.ipc.Schedulable,long,java.util.concurrent.TimeUnit)",269,279,"/**
* Offers an element to the corresponding priority queue with a specified timeout.
* @param e element to offer
* @param timeout maximum time to wait for insertion
* @param unit TimeUnit for the timeout
* @return true if successfully offered, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,org.apache.hadoop.ipc.FairCallQueue:offer(org.apache.hadoop.ipc.Schedulable),281,290,"/**
* Offers an element to the corresponding queue based on its priority level.
* @param e the element to offer
* @return true if added successfully, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,drainTo,org.apache.hadoop.ipc.FairCallQueue:drainTo(java.util.Collection),366,369,"/**
* Drains all elements from this collection into the specified target.
* @param c the target collection to drain into
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,addTerseExceptions,org.apache.hadoop.ipc.Server:addTerseExceptions(java.lang.Class[]),174,176,"/**
* Adds terse logging exceptions for specified classes.
* @param exceptionClass one or more Class objects to log exceptions for
*/","* Add exception classes for which server won't log stack traces.
   *
   * @param exceptionClass exception classes",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,addSuppressedLoggingExceptions,org.apache.hadoop.ipc.Server:addSuppressedLoggingExceptions(java.lang.Class[]),183,185,"/**
 * Adds suppressed logging exceptions to be handled by ExceptionsHandler.
 * @param exceptionClass one or more Class objects of exceptions to suppress
 */","* Add exception classes which server won't log at all.
   *
   * @param exceptionClass exception classes",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,logException,"org.apache.hadoop.ipc.Server:logException(org.slf4j.Logger,java.lang.Throwable,org.apache.hadoop.ipc.Server$Call)",3244,3262,"/**
* Logs an exception with optional suppression or terse logging based on exception type.
* @param logger Logger instance for logging
* @param e Throwable to log
* @param call Call context for logging
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getSupportedProtocolVersions,"org.apache.hadoop.ipc.RPC$Server:getSupportedProtocolVersions(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",1146,1164,"/**
* Retrieves supported protocol versions for the given RPC kind and protocol name.
* @param rpcKind RPC kind
* @param protocolName protocol name to filter by
* @return array of VerProtocolImpl objects or null if no matches found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getHighestSupportedProtocol,"org.apache.hadoop.ipc.RPC$Server:getHighestSupportedProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",1166,1187,"/**
* Retrieves the highest supported protocol version and implementation.
* @param rpcKind type of RPC
* @param protocolName name of the protocol
* @return VerProtocolImpl object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/UnexpectedServerException.java,<init>,org.apache.hadoop.ipc.UnexpectedServerException:<init>(java.lang.String),32,34,"/**
 * Constructs an instance of UnexpectedServerException with the given error message.
 * @param message detailed description of the server-related exception
 */","* Constructs exception with the specified detail message.
   * 
   * @param messages detailed message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcServerException.java,<init>,org.apache.hadoop.ipc.RpcServerException:<init>(java.lang.String),33,35,"/**
* Constructs an RpcServerException with the specified error message.
* @param message detailed description of the exception cause","* Constructs exception with the specified detail message.
   * @param message detailed message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientException.java,<init>,org.apache.hadoop.ipc.RpcClientException:<init>(java.lang.String),31,33,"/**
* Constructs an RpcClientException with the specified error message.
* @param message error description
*/","* Constructs exception with the specified detail message.
   * 
   * @param messages detailed message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/UnexpectedServerException.java,<init>,"org.apache.hadoop.ipc.UnexpectedServerException:<init>(java.lang.String,java.lang.Throwable)",45,47,"/**
* Constructs an unexpected server exception with a custom error message and root cause.
* @param message custom error description
* @param cause underlying exception that caused this exception
*/","* Constructs exception with the specified detail message and cause.
   * 
   * @param message message.
   * @param cause that cause this exception
   * @param cause the cause (can be retried by the {@link #getCause()} method).
   *          (A <tt>null</tt> value is permitted, and indicates that the cause
   *          is nonexistent or unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcServerException.java,<init>,"org.apache.hadoop.ipc.RpcServerException:<init>(java.lang.String,java.lang.Throwable)",45,47,"/**
* Constructs an RpcServerException with a custom error message and optional root cause.
* @param message human-readable description of the exception
* @param cause underlying exception (optional)
*/","* Constructs exception with the specified detail message and cause.
   * 
   * @param message message.
   * @param cause the cause (can be retried by the {@link #getCause()} method).
   *          (A <tt>null</tt> value is permitted, and indicates that the cause
   *          is nonexistent or unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientException.java,<init>,"org.apache.hadoop.ipc.RpcClientException:<init>(java.lang.String,java.lang.Throwable)",44,46,"/**
* Constructs an RpcClient exception with custom error message and root cause.
* @param message custom error message
* @param cause underlying throwable causing the exception
*/","* Constructs exception with the specified detail message and cause.
   * 
   * @param message message.
   * @param cause that cause this exception
   * @param cause the cause (can be retried by the {@link #getCause()} method).
   *          (A <tt>null</tt> value is permitted, and indicates that the cause
   *          is nonexistent or unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,setCapacity,org.apache.hadoop.ipc.ResponseBuffer:setCapacity(int),60,62,"/**
 * Sets the buffer's capacity.
 * @param capacity new capacity value
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,reset,org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:reset(),98,102,"/**
 * Resets the object state to its initial values.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,getFramedBuffer,org.apache.hadoop.ipc.ResponseBuffer:getFramedBuffer(),42,46,"/**
* Returns a framed buffer with updated size.
* @return FramedBuffer object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addCost,"org.apache.hadoop.ipc.DecayRpcScheduler:addCost(java.lang.Object,long)",568,600,"/**
* Updates the total call costs for a given identity.
* @param identity unique identifier (Object)
* @param costDelta change in cost to apply
*/","* Adjust the stored cost for a given identity.
   *
   * @param identity the identity of the user whose cost should be adjusted
   * @param costDelta the cost to add for the given identity",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,computePriorityLevel,"org.apache.hadoop.ipc.DecayRpcScheduler:computePriorityLevel(long,java.lang.Object)",609,634,"/**
* Calculates priority level based on cost and user identity.
* @param cost monetary cost associated with the entity
* @param identity unique identifier for the entity (e.g. user)
* @return numerical priority level (0-9) or 0 if unknown
*/","* Given the cost for an identity, compute a scheduling decision.
   *
   * @param cost the cost for an identity
   * @param identity the identity of the user
   * @return scheduling decision from 0 to numLevels - 1",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,setPriorityLevel,"org.apache.hadoop.ipc.DecayRpcScheduler:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",693,699,"/**
* Sets the priority level for a user.
* @param ugi UserGroupInformation object
* @param priority new priority value (clamped to valid range)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getCallVolumeSummary,org.apache.hadoop.ipc.DecayRpcScheduler:getCallVolumeSummary(),1127,1133,"/**
* Returns a JSON string summarizing call volume data.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,wrap,org.apache.hadoop.ipc.RpcWritable$Buffer:wrap(java.nio.ByteBuffer),145,147,"/**
* Wraps a ByteBuffer into a Buffer instance.
* @param bb underlying ByteBuffer to be wrapped
* @return newly created Buffer object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:<init>(),514,515,"/**
* Constructs an empty RPC request.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:<init>(org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto,com.google.protobuf.Message)",517,520,"/**
* Initializes an RpcProtobufRequest object with a request header and payload.
* @param header RequestHeaderProto instance
* @param payload Message payload to be sent
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:<init>(),652,653,"/**
* Initializes an RPC request using Protobuf data.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:<init>(org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto,org.apache.hadoop.thirdparty.protobuf.Message)",655,658,"/**
* Initializes an RPC request with the given header and payload.
* @param header Request header protocol buffer object
* @param payload Payload message to be sent over the RPC channel
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getRemoteException,org.apache.hadoop.ipc.ProtobufHelper:getRemoteException(org.apache.hadoop.thirdparty.protobuf.ServiceException),56,58,"/**
 * Wraps ServiceException into a remote IOException.
 * @param se exception to wrap
 */","* Return the IOException thrown by the remote server wrapped in
   * ServiceException as cause.
   * @param se ServiceException that wraps IO exception thrown by the server
   * @return Exception wrapped in ServiceException or
   *         a new IOException that wraps the unexpected ServiceException.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,ipc,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:ipc(org.apache.hadoop.ipc.internal.ShadedProtobufHelper$IpcCall),158,164,"/**
* Executes an IPC Call and returns result or re-throws remote exception.
*/","* Evaluate a protobuf call, converting any ServiceException to an IOException.
   * @param call invocation to make
   * @return the result of the call
   * @param <T> type of the result
   * @throws IOException any translated protobuf exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getFixedByteString,org.apache.hadoop.ipc.ProtobufHelper:getFixedByteString(java.lang.String),94,96,"/**
* Retrieves a fixed byte string from the Protobuf helper.
* @param key unique identifier
*/","* Get the ByteString for frequently used fixed and small set strings.
   * @param key string
   * @return ByteString for frequently used fixed and small set strings.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getByteString,org.apache.hadoop.ipc.ProtobufHelper:getByteString(byte[]),104,107,"/**
* Returns a byte string representation of the given byte array.
* @param bytes input byte array
*/","* Get the byte string of a non-null byte array.
   * If the array is 0 bytes long, return a singleton to reduce object allocation.
   * @param bytes bytes to convert.
   * @return a value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,skipRetryCache,"org.apache.hadoop.ipc.RetryCache:skipRetryCache(byte[],int)",206,211,"/**
* Determines whether to skip caching a retry.
* @param clientId unique client identifier
* @param callId invocation ID
* @return true if should skip, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,setState,"org.apache.hadoop.ipc.RetryCache:setState(org.apache.hadoop.ipc.RetryCache$CacheEntry,boolean)",382,387,"/**
* Sets completion state in cache entry.
* @param e CacheEntry object to update
* @param success true for successful operation, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,toString,org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:toString(),538,547,"/**
* Returns a string representation of the method, combining protocol name and method name.
* @throws IllegalArgumentException if an I/O error occurs while fetching request header
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setCallIdAndRetryCount,"org.apache.hadoop.ipc.Client:setCallIdAndRetryCount(int,int,java.lang.Object)",122,128,"/**
* Sets call ID and retry count with validation.
* @param cid valid call identifier
* @param rc valid retry count value
* @param externalHandler arbitrary handler object
*/","* Set call id and retry count for the next call.
   * @param cid input cid.
   * @param rc input rc.
   * @param externalHandler input externalHandler.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,close,org.apache.hadoop.ipc.Client:close(),1881,1885,"/**
 * Closes and stops the underlying process.
 * @throws Exception on failure
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,checkAsyncCall,org.apache.hadoop.ipc.Client:checkAsyncCall(),1430,1442,"/**
* Validates async call limit and throws exception if exceeded.
* @throws AsyncCallLimitExceededException if maximum allowed calls reached
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getListenerAddress,org.apache.hadoop.ipc.Server:getListenerAddress(),3751,3753,"/**
* Retrieves the address of the network listener.
* @return InetSocketAddress representing the listener's address or null if not set
*/","* Return the socket (ip+port) on which the RPC server is listening to.
   * @return the socket (ip+port) on which the RPC server is listening to.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuxiliaryListenerAddresses,org.apache.hadoop.ipc.Server:getAuxiliaryListenerAddresses(),3762,3770,"/**
* Retrieves a set of auxiliary listener addresses.
* @return Set<InetSocketAddress> containing addresses or empty set if none found
*/","* Return the set of all the configured auxiliary socket addresses NameNode
   * RPC is listening on. If there are none, or it is not configured at all, an
   * empty set is returned.
   * @return the set of all the auxiliary addresses on which the
   *         RPC server is listening on.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doStop,org.apache.hadoop.ipc.Server$Listener:doStop(),1673,1688,"/**
* Stops the server by waking up selector, closing accept channel and shutting down all readers.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,stopClient,org.apache.hadoop.ipc.ClientCache:stopClient(org.apache.hadoop.ipc.Client),99,120,"/**
* Removes a client from the cache and stops it if there are no remaining references.
* @param client the client to stop
*/","* Stop a RPC client connection 
   * A RPC client is closed only when its reference count becomes zero.
   *
   * @param client input client.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,clearClientCache,org.apache.hadoop.ipc.ProtobufRpcEngine2:clearClientCache(),392,395,"/**
* Clears client-side cache.
* @visibleForTesting only intended for unit testing purposes. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getHostInetAddress,org.apache.hadoop.ipc.Server$RpcCall:getHostInetAddress(),1224,1227,"/**
* Returns the host IP address of the underlying network connection. 
* @return InetAddress object representing the host IP address or null if not available.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemotePort,org.apache.hadoop.ipc.Server$RpcCall:getRemotePort(),1229,1232,"/**
* Retrieves the remote port of an established network connection.
* @return Remote port number (default is 0 if not connected)",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setDeferredResponse,org.apache.hadoop.ipc.Server$RpcCall:setDeferredResponse(org.apache.hadoop.io.Writable),1347,1365,"/**
* Sets a deferred response to the server if it is running.
* @param response Writable response object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,toString,org.apache.hadoop.ipc.Server$RpcCall:toString(),1404,1407,"/**
* Returns a human-readable string representation of this object.
* Includes RPC request and connection information.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,waitForWork,org.apache.hadoop.ipc.Client$Connection:waitForWork(),1036,1062,"/**
* Waits for work to be available, handling idle timeouts and connection closure.
* @return true if work is available, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,handleConnectionTimeout,"org.apache.hadoop.ipc.Client$Connection:handleConnectionTimeout(int,int,java.io.IOException)",921,932,"/**
* Handles connection timeout by retrying or throwing exception.
* @param curRetries current number of retries
* @param maxRetries maximum allowed retries
* @param ioe IOException to be processed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,handleConnectionFailure,"org.apache.hadoop.ipc.Client$Connection:handleConnectionFailure(int,java.io.IOException)",934,968,"/**
* Handles failed connections, potentially retrying with a delay.
* @param curRetries current number of retries
* @throws IOException if the action fails or an interrupt occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,equals,org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:equals(java.lang.Object),168,171,"/**
* Delegates equality check to superclass.
* @param obj Object to compare with this instance
*/",Override equals to avoid findbugs warnings,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getQueueSizes,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getQueueSizes(),438,446,"/**
* Retrieves sizes of schedulables in the call queue.
* @return array of queue size integers or empty array if queue is null
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getOverflowedCalls,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getOverflowedCalls(),448,456,"/**
* Returns an array of overflowed call IDs.
* @return Array of long values representing overflowed call IDs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufWrapperLegacy.java,<init>,org.apache.hadoop.ipc.ProtobufWrapperLegacy:<init>(java.lang.Object),51,56,"/**
* Wraps a Protocol Buffers legacy message into a ProtobufWrapperLegacy object.
* @param message the legacy protocol buffers message to wrap
*/","* Construct.
   * The type of the parameter is Object so as to keep the casting internal
   * to this class.
   * @param message message to wrap.
   * @throws IllegalArgumentException if the class is not a protobuf message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,switchToSimple,org.apache.hadoop.ipc.Server$Connection:switchToSimple(),2401,2405,"/**
* Switches to simple authentication protocol.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,close,org.apache.hadoop.ipc.Server$Connection:close(),3082,3094,"/**
* Closes the underlying connections and resources.
* @throws Exception on socket shutdown error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processSaslToken,org.apache.hadoop.ipc.Server$Connection:processSaslToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto),2387,2399,"/**
* Processes SASL token and returns response.
* @param saslMessage input SASL message
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,checkDataLength,org.apache.hadoop.ipc.Server$Connection:checkDataLength(int),2446,2459,"/**
* Validates data length against maximum allowed value.
* @param dataLength the actual data length
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponseOldVersionFatal,"org.apache.hadoop.ipc.Server:setupResponseOldVersionFatal(java.io.ByteArrayOutputStream,org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)",3627,3639,"/**
* Writes fatal response with old version compatibility.
* @param response ByteArrayOutputStream to store the response
* @param call RpcCall object containing call ID and settings
* @param rv ignored (kept for API compatibility)
* @param errorClass class of the error
* @param error detailed error message
*/","* Setup response for the IPC Call on Fatal Error from a 
   * client that is using old version of Hadoop.
   * The response is serialized using the previous protocol's response
   * layout.
   * 
   * @param response buffer to serialize the response into
   * @param call {@link Call} to which we are setting up the response
   * @param rv return value for the IPC Call, if the call was successful
   * @param errorClass error class, if the the call failed
   * @param error error message, if the call failed
   * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRpcRequestWrapper,org.apache.hadoop.ipc.Server:getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto),302,308,"/**
* Returns the RPC request wrapper class for the given protocol kind.
* @param rpcKind protocol kind
* @return Class<? extends Writable> or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,toString,org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:toString(),676,685,"/**
* Returns a string representation of the request method, 
* including protocol and method name.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,hashCode,org.apache.hadoop.ipc.RetryCache$CacheEntry:hashCode(),94,97,"/**
* Calculates combined hash code using client ID and call ID.
* @return unique hash value representing the caller's session
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.java,advanceIndex,org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:advanceIndex(),121,132,"/**
* Advances index by decrementing requests left and moving to next queue when exhausted.
*/","* Advances the index, which will change the current index
   * if called enough times.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:<init>(),398,403,"/**
* Initializes the callback with current call information and setup time.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:<init>(),430,435,"/**
* Initializes the callback with server, call, and method name details.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,error,org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:error(java.lang.Throwable),412,418,"/**
* Reports an error to the server and updates deferred metrics.
* @param t the Throwable instance containing error details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,error,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:error(java.lang.Throwable),444,450,"/**
* Logs an error with associated metrics and sets a deferred error on the caller.
* @param t Throwable instance describing the error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,capacity,org.apache.hadoop.ipc.ResponseBuffer:capacity(),56,58,"/**
* Returns the buffer's capacity.
* @return The maximum number of bytes that can be written to the output.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,ensureCapacity,org.apache.hadoop.ipc.ResponseBuffer:ensureCapacity(int),64,68,"/**
* Ensures the output buffer has sufficient capacity.
* @param capacity required buffer size
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setException,org.apache.hadoop.ipc.Client$Call:setException(java.io.IOException),341,344,"/**
* Sets an IOException and notifies listeners of completion.
* @param error IOException to be recorded
*/","Set the exception when there is an error.
     * Notify the caller the call is done.
     * 
     * @param error exception thrown by the call; either local or remote",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setRpcResponse,org.apache.hadoop.ipc.Client$Call:setRpcResponse(org.apache.hadoop.io.Writable),351,354,"/**
* Sets an RPC response and notifies listeners that processing is complete.
* @param rpcResponse Writable RPC response object
*/","Set the return value when there is no error. 
     * Notify the caller the call is done.
     * 
     * @param rpcResponse return value of the rpc call.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,read,org.apache.hadoop.ipc.Client$Connection$PingInputStream:read(),513,524,"/**
* Repeatedly attempts to read a byte from the underlying socket,
* handling SocketTimeoutExceptions by incrementing a timeout counter.
* @throws IOException if an I/O error occurs
*/","Read a byte from the stream.
       * Send a ping if timeout on read. Retries if no failure is detected
       * until a byte is read.
       * @throws IOException for any IO problem other than socket timeout",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,read,"org.apache.hadoop.ipc.Client$Connection$PingInputStream:read(byte[],int,int)",532,543,"/**
* Repeatedly attempts to read from socket with timeout handling.
* @param buf byte array to store data
* @param off offset in buffer to write data
* @param len maximum number of bytes to read
* @return number of bytes read or -1 if end-of-stream reached
*/","Read bytes into a buffer starting from offset <code>off</code>
       * Send a ping if timeout on read. Retries if no failure is detected
       * until a byte is read.
       * 
       * @return the total number of bytes read; -1 if the connection is closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getHostAddress,org.apache.hadoop.ipc.Server$Call:getHostAddress(),1063,1066,"/**
* Retrieves the host IP address.
* @return Host IP address as a string or null if unavailable
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemoteIp,org.apache.hadoop.ipc.Server:getRemoteIp(),385,388,"/**
* Retrieves the remote IP address of the current call.
* @return InetAddress object representing the remote IP or null if not available
*/","* @return Returns the remote side ip address when invoked inside an RPC
   *  Returns null in case of an error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getServerRpcInvoker,org.apache.hadoop.ipc.Server:getServerRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind),310,312,"/**
* Retrieves server-side RPC invoker based on specified kind.
* @param rpcKind type of RPC invocation (e.g. synchronous, asynchronous)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemotePort,org.apache.hadoop.ipc.Server:getRemotePort(),394,397,"/**
* Retrieves the remote port from the current call.
* @return Remote port number or 0 if no call is active
*/","* @return Returns the remote side port when invoked inside an RPC
   * Returns 0 in case of an error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuxiliaryPortEstablishedQOP,org.apache.hadoop.ipc.Server:getAuxiliaryPortEstablishedQOP(),411,423,"/**
* Retrieves the established Quality of Protection (QOP) on the auxiliary port.
* @return Established QOP as a string or null if not applicable
*/","* Returns the SASL qop for the current call, if the current call is
   * set, and the SASL negotiation is done. Otherwise return null
   * Note this only returns established QOP for auxiliary port, and
   * returns null for primary (non-auxiliary) port.
   *
   * Also note that CurCall is thread local object. So in fact, different
   * handler threads will process different CurCall object.
   *
   * Also, only return for RPC calls, not supported for other protocols.
   * @return the QOP of the current connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getProtocol,org.apache.hadoop.ipc.Server:getProtocol(),450,453,"/**
* Retrieves the protocol of the current call.
* @return Protocol string or null if no call is active
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getPriorityLevel,org.apache.hadoop.ipc.Server:getPriorityLevel(),465,468,"/**
* Retrieves the priority level of the current caller.
* @return Priority level (0 if unknown)
*/","* @return Return the priority level assigned by call queue to an RPC
   * Returns 0 in case no priority is assigned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setLogSlowRPCThresholdTime,org.apache.hadoop.ipc.Server:setLogSlowRPCThresholdTime(long),556,560,"/**
* Sets threshold time for logging slow RPCs in milliseconds.
* @param logSlowRPCThresholdMs time threshold value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setClientBackoffEnabled,org.apache.hadoop.ipc.Server:setClientBackoffEnabled(boolean),3876,3878,"/**
* Enables/disables client backoff on the calling queue.
* @param value true to enable, false to disable
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,addAuxiliaryListener,org.apache.hadoop.ipc.Server:addAuxiliaryListener(int),3427,3443,"/**
* Adds an auxiliary listener to the map using the given port.
* @param auxiliaryPort unique identifier for the listener
* @throws IOException if a listener is already bound to this port
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponseForProtobuf,"org.apache.hadoop.ipc.Server:setupResponseForProtobuf(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",3585,3607,"/**
* Generates a Protobuf response by serializing the given RpcResponseHeaderProto and optional payload.
* @param header RpcResponseHeaderProto to serialize
* @param rv Optional Writable object containing the payload data
* @return serialized response as a byte array
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getNumOpenConnectionsPerUser,org.apache.hadoop.ipc.Server:getNumOpenConnectionsPerUser(),3844,3852,"/**
* Returns a JSON string representing the number of open connections per user.
* @return JSON string or null on failure
*/",* @return Get the NumOpenConnections/User.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isServerFailOverEnabled,org.apache.hadoop.ipc.Server:isServerFailOverEnabled(),3880,3883,"/**
* Checks if server fail-over is enabled based on configured settings.
* @return true if enabled, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processResponse,"org.apache.hadoop.ipc.Server$Responder:processResponse(java.util.LinkedList,boolean)",1844,1922,"/**
* Processes a response from the RPC queue, handling writes to the socket channel.
* @param responseQueue linked list of RPC calls
* @param inHandler boolean indicating if processing is within a handler thread
* @return true if there are no more responses for this channel, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,equals,org.apache.hadoop.ipc.Client$ConnectionId:equals(java.lang.Object),1823,1841,"/**
* Compares two ConnectionId objects for equality.
* @param obj object to compare with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/Tracer.java,build,org.apache.hadoop.tracing.Tracer$Builder:build(),91,96,"/**
* Creates and returns the shared Tracer instance.
* @return a singleton Tracer object or the existing one if already built
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/Tracer.java,newSpan,"org.apache.hadoop.tracing.Tracer:newSpan(java.lang.String,org.apache.hadoop.tracing.SpanContext)",55,57,"/**
* Creates a new span with given description and context.
* @param description human-readable description of the span
* @param spanCtx context information for the span (e.g. trace ID)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/NullTraceScope.java,<init>,org.apache.hadoop.tracing.NullTraceScope:<init>(),23,25,"/**
* Initializes an empty trace scope with no parent.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/TraceScope.java,close,org.apache.hadoop.tracing.TraceScope:close(),53,57,"/**
 * Closes associated Span instance.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,"org.apache.hadoop.util.MachineList:<init>(java.util.Collection,org.apache.hadoop.util.MachineList$InetAddressFactory)",95,136,"/**
* Initializes MachineList object with a collection of host entries and an address factory.
* @param hostEntries collection of host entries (IP addresses or CIDR ranges)
* @param addressFactory factory for creating InetAddress objects
*/","* Accepts a collection of ip/cidr/host addresses
   * 
   * @param hostEntries hostEntries.
   * @param addressFactory addressFactory to convert host to InetAddress",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,includes,org.apache.hadoop.util.MachineList:includes(java.lang.String),145,160,"/**
* Checks if IP address belongs to a network.
* @param ipAddress IP address to check
*/","* Accepts an ip address and return true if ipAddress is in the list.
   * {@link #includes(InetAddress)} should be preferred
   * to avoid possibly re-resolving the ip address.
   *
   * @param ipAddress ipAddress.
   * @return true if ipAddress is part of the list",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfTest.java,checkConf,org.apache.hadoop.util.ConfTest:checkConf(java.io.InputStream),136,216,"/**
* Validates configuration file and reports any errors.
* @param in InputStream containing the configuration file
* @return List of error messages or empty list if valid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfTest.java,listFiles,org.apache.hadoop.util.ConfTest:listFiles(java.io.File),218,225,"/**
* Retrieves an array of XML files within the specified directory.
* @param dir The directory to search for XML files
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,<init>,"org.apache.hadoop.util.SysInfoLinux:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,long)",195,210,"/**
* Initializes SysInfoLinux object with file paths and time tracking settings.
* @param procfsMemFile path to memory usage file
* @param procfsCpuFile path to CPU usage file
* @param procfsStatFile path to system statistics file
* @param procfsNetFile path to network usage file
* @param procfsDisksFile path to disk usage file
* @param jiffyLengthInMillis time interval for CPU tracking in milliseconds
*/","* Constructor which allows assigning the /proc/ directories. This will be
   * used only in unit tests.
   * @param procfsMemFile fake file for /proc/meminfo
   * @param procfsCpuFile fake file for /proc/cpuinfo
   * @param procfsStatFile fake file for /proc/stat
   * @param procfsNetFile fake file for /proc/net/dev
   * @param procfsDisksFile fake file for /proc/diskstats
   * @param jiffyLengthInMillis fake jiffy length value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcMemInfoFile,org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile(boolean),238,305,"/**
* Reads memory information from ""/proc/memInfo"" file.
* @param readAgain if true, reads the file again even if it was recently read
*/","* Read /proc/meminfo, parse and compute memory information.
   * @param readAgain if false, read only on the first time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNumProcessors,org.apache.hadoop.util.SysInfoLinux:getNumProcessors(),625,629,"/**
* Retrieves the number of processors available.
* @return Number of processor cores.",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNumCores,org.apache.hadoop.util.SysInfoLinux:getNumCores(),632,636,"/**
* Returns the number of CPU cores.
* @return Number of available CPU cores
*/",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getCpuFrequency,org.apache.hadoop.util.SysInfoLinux:getCpuFrequency(),639,643,"/**
* Retrieves CPU frequency in Hz.
* @return current CPU frequency or -1 on failure
*/",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcStatFile,org.apache.hadoop.util.SysInfoLinux:readProcStatFile(),376,421,"/**
* Reads and parses CPU utilization from ""/proc/stat"" file.
*/","* Read /proc/stat file, parse and calculate cumulative CPU.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNetworkBytesRead,org.apache.hadoop.util.SysInfoLinux:getNetworkBytesRead(),675,679,"/**
* Retrieves total network bytes read from system statistics.
* @return total network bytes read in bytes
*/",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNetworkBytesWritten,org.apache.hadoop.util.SysInfoLinux:getNetworkBytesWritten(),682,686,"/**
* Returns the total number of bytes written to network.
*/",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcDisksInfoFile,org.apache.hadoop.util.SysInfoLinux:readProcDisksInfoFile(),483,543,"/**
* Reads disk statistics from ""/proc/diskstats"" file and updates numDisksBytesRead and numDisksBytesWritten counters.
*/","* Read /proc/diskstats file, parse and calculate amount
   * of bytes read and written from/to disks.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,realloc,org.apache.hadoop.util.IdentityHashStore:realloc(int),74,90,"/**
* Reallocates internal buffer to new capacity, copying existing data.
* @param newCapacity the new size of the buffer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,get,org.apache.hadoop.util.IdentityHashStore:get(java.lang.Object),152,158,"/**
* Retrieves a value from the buffer using the given key.
* @param k unique key identifier
* @return Value object or null if not found
*/","* Retrieve a value associated with a given key.
   *
   * @param k Generics Type k.
   * @return Generics Type V.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,remove,org.apache.hadoop.util.IdentityHashStore:remove(java.lang.Object),167,177,"/**
* Removes and returns the value associated with the specified key.
* @param k unique key identifier
* @return Value object or null if not found
*/","* Retrieve a value associated with a given key, and delete the
   * relevant entry.
   *
   * @param k Generics Type k.
   * @return Generics Type V.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,ensureNext,org.apache.hadoop.util.LightWeightGSet$SetIterator:ensureNext(),312,327,"/**
* Ensures the existence of a valid next iteration.
* @throws ConcurrentModificationException if modification occurs while iterating
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,remove,"org.apache.hadoop.util.LightWeightGSet:remove(int,java.lang.Object)",188,219,"/**
* Removes the specified key from the map if it exists at the given index.
* @param index unique identifier for the linked list entry
* @param key value to be removed (null if not found)
* @return the removed element or null if not present
*/","* Remove the element corresponding to the key,
   * given key.hashCode() == index.
   *
   * @param key key.
   * @param index index.
   * @return If such element exists, return it.
   *         Otherwise, return null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MergeSort.java,mergeSort,"org.apache.hadoop.util.MergeSort:mergeSort(int[],int[],int,int)",42,83,"/**
* Mergesorts an array using insertion sort on small subarrays and recursive merge on larger ones.
* @param src temporary array for sorting, will be overwritten
* @param dest original array to be sorted
* @param low starting index of the subarray to sort
* @param high ending index of the subarray to sort
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,setOptionalSecureTransformerAttributes,org.apache.hadoop.util.XMLUtils:setOptionalSecureTransformerAttributes(javax.xml.transform.TransformerFactory),180,186,"/**
* Sets optional secure transformer attributes.
* @param transformerFactory TransformerFactory instance
*/","* These attributes are recommended for maximum security but some JAXP transformers do
   * not support them. If at any stage, we fail to set these attributes, then we won't try again
   * for subsequent transformers.
   *
   * @param transformerFactory to update",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,string2long,org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:string2long(java.lang.String),906,927,"/**
* Converts string representation to long value, handling prefixes k, m, g, t, p, e.
* @param s input string with optional size prefix
*/","* Convert a string to long.
     * The input string is first be trimmed
     * and then it is parsed with traditional binary prefix.
     *
     * For example,
     * ""-1230k"" will be converted to -1230 * 1024 = -1259520;
     * ""891g"" will be converted to 891 * 1024^3 = 956703965184;
     *
     * @param s input string
     * @return a long value represented by the input string.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,long2String,"org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:long2String(long,java.lang.String,int)",937,977,"/**
* Converts long value to string with specified unit and decimal places.
* @param n the number to convert
* @param unit the unit of measurement (e.g. ""byte"", ""kilobyte"")
* @param decimalPlaces the number of decimal places to display
* @return a human-readable string representation of the number, including unit","* Convert a long integer to a string with traditional binary prefix.
     * 
     * @param n the value to be converted
     * @param unit The unit, e.g. ""B"" for bytes.
     * @param decimalPlaces The number of decimal places.
     * @return a string with traditional binary prefix.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,formatPercent,"org.apache.hadoop.util.StringUtils:formatPercent(double,int)",153,155,"/**
* Formats a double as a percentage with specified decimal places.
* @param fraction value to be formatted (0-1)
* @param decimalPlaces number of decimal places in the result
* @return string representation of the percentage (e.g. ""12.34%"")
*/","* Format a percentage for presentation to the user.
   * @param fraction the percentage as a fraction, e.g. 0.1 = 10%
   * @param decimalPlaces the number of decimal places
   * @return a string representation of the percentage",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteToHexString,"org.apache.hadoop.util.StringUtils:byteToHexString(byte[],int,int)",183,192,"/**
* Converts a byte array to a hexadecimal string, starting from the specified index and ending at the given index.
* @param bytes byte array to convert
* @param start starting index (inclusive)
* @param end ending index (exclusive)
* @return hexadecimal string representation or null if input is invalid
*/","* Given an array of bytes it will convert the bytes to a hex string
   * representation of the bytes
   * @param bytes bytes.
   * @param start start index, inclusively
   * @param end end index, exclusively
   * @return hex string representation of the byte array",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,limitDecimalTo2,org.apache.hadoop.util.StringUtils:limitDecimalTo2(double),1033,1036,"/**
* Formats double to 2 decimal places.
* @return string representation of input number with 2 decimal places
*/","* limitDecimalTo2.
   *
   * @param d double param.
   * @return string value (""%.2f"").
   * @deprecated use StringUtils.format(""%.2f"", d).",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HeapSort.java,sort,"org.apache.hadoop.util.HeapSort:sort(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable)",56,74,"/**
* Performs in-place heap sort on IndexedSortable object with progress updates.
* @param s IndexedSortable object to be sorted
* @param p starting index of the array
* @param r ending index of the array
* @param rep Progressable callback for progress updates
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,subtract,org.apache.hadoop.util.JvmPauseMonitor$GcTimes:subtract(org.apache.hadoop.util.JvmPauseMonitor$GcTimes),166,169,"/**
* Subtracts another GcTimes instance from this one.
* @param other GcTimes instance to subtract
* @return New GcTimes instance with updated count and time
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,org.apache.hadoop.util.ExitUtil:terminate(org.apache.hadoop.util.ExitUtil$ExitException),233,272,"/**
* Terminates the application with an ExitException.
* @param ee the exception to terminate with
*/","* Exits the JVM if exit is enabled, rethrow provided exception or any raised error otherwise.
   * Inner termination: either exit with the exception's exit code,
   * or, if system exits are disabled, rethrow the exception.
   * @param ee exit exception
   * @throws ExitException if {@link System#exit(int)} is disabled and not suppressed by an Error
   * @throws Error if {@link System#exit(int)} is disabled and one Error arise, suppressing
   * anything else, even <code>ee</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,org.apache.hadoop.util.ExitUtil:halt(org.apache.hadoop.util.ExitUtil$HaltException),286,326,"/**
* Handles {@link HaltException} and potentially halts the JVM.
* @param he the exception containing the exit code and message
*/","* Halts the JVM if halt is enabled, rethrow provided exception or any raised error otherwise.
   * If halt is disabled, this method throws either the exception argument if no
   * error arise, the first error if at least one arise, suppressing <code>he</code>.
   * If halt is enabled, all throwables are caught, even errors.
   *
   * @param he the exception containing the status code, message and any stack
   * trace.
   * @throws HaltException if {@link Runtime#halt(int)} is disabled and not suppressed by an Error
   * @throws Error if {@link Runtime#halt(int)} is disabled and one Error arise, suppressing
   * anyuthing else, even <code>he</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,<init>,org.apache.hadoop.util.SysInfoWindows:<init>(),56,59,"/**
 * Initializes Windows system information with default values and resets internal state.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,addShutdownHook,"org.apache.hadoop.util.ShutdownHookManager:addShutdownHook(java.lang.Runnable,int,long,java.util.concurrent.TimeUnit)",319,331,"/**
* Adds a shutdown hook with specified priority and timeout.
* @param shutdownHook task to be executed on shutdown
* @param priority hook execution order
* @param timeout duration before hook is cancelled
* @param unit time unit for timeout
*/","*
   * Adds a shutdownHook with a priority and timeout the higher the priority
   * the earlier will run. ShutdownHooks with same priority run
   * in a non-deterministic order. The shutdown hook will be terminated if it
   * has not been finished in the specified period of time.
   *
   * @param shutdownHook shutdownHook <code>Runnable</code>
   * @param priority priority of the shutdownHook
   * @param timeout timeout of the shutdownHook
   * @param unit unit of the timeout <code>TimeUnit</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,removeShutdownHook,org.apache.hadoop.util.ShutdownHookManager:removeShutdownHook(java.lang.Runnable),340,350,"/**
* Removes a shutdown hook from the execution environment.
* @param shutdownHook the hook to remove
*/","* Removes a shutdownHook.
   *
   * @param shutdownHook shutdownHook to remove.
   * @return TRUE if the shutdownHook was registered and removed,
   * FALSE otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,hasShutdownHook,org.apache.hadoop.util.ShutdownHookManager:hasShutdownHook(java.lang.Runnable),358,363,"/**
* Checks if a shutdown hook is registered.
* @param shutdownHook the Runnable to check for registration
* @return true if registered, false otherwise
*/","* Indicates if a shutdownHook is registered or not.
   *
   * @param shutdownHook shutdownHook to check if registered.
   * @return TRUE/FALSE depending if the shutdownHook is is registered.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,compareTo,org.apache.hadoop.util.ComparableVersion:compareTo(org.apache.hadoop.util.ComparableVersion),460,463,"/**
 * Compares this version with another based on their item versions.
 * @param o the other version to compare with
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ThreadUtil.java,getResourceAsStream,org.apache.hadoop.util.ThreadUtil:getResourceAsStream(java.lang.String),91,99,"/**
* Retrieves an input stream for a given resource name.
* @param resourceName unique identifier of the requested resource
*/","* Convenience method that returns a resource as inputstream from the
   * classpath.
   * <p>
   * Uses the Thread's context classloader to load resource.
   *
   * @param resourceName resource to retrieve.
   *
   * @throws IOException thrown if resource cannot be loaded
   * @return inputstream with the resource.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,logWarning,"org.apache.hadoop.util.InstrumentedLock:logWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)",151,161,"/**
* Logs a warning for excessive lock held time.
* @param lockHeldTime duration of lock hold in milliseconds
* @param stats SuppressedSnapshot object containing suppression details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,logWaitWarning,"org.apache.hadoop.util.InstrumentedLock:logWaitWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)",163,172,"/**
* Logs a warning for excessive lock wait time.
* @param lockWaitTime duration of lock acquisition attempt
* @param stats suppressed snapshot details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/QuickSort.java,sortInternal,"org.apache.hadoop.util.QuickSort:sortInternal(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable,int)",69,136,"/**
* Recursively sorts the specified IndexedSortable within the given range.
* @param s IndexedSortable object to be sorted
* @param p start index of the range (inclusive)
* @param r end index of the range (exclusive)
* @param rep Progressable object for progress tracking (optional)
* @param depth recursion depth limit",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,<init>,org.apache.hadoop.util.LineReader:<init>(java.io.InputStream),69,71,"/**
* Initializes LineReader with specified input stream and buffer size.
* @param in InputStream to read from
* @param bufferSize default buffer size (default value is used if not provided)
*/","* Create a line reader that reads from the given stream using the
   * default buffer-size (64k).
   * @param in The input stream",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,run,org.apache.hadoop.util.Shell$1:run(),951,960,"/**
* Executes the command, respecting interval between runs.
* @throws IOException if an I/O error occurs
*/","* Check to see if a command needs to be executed and execute if needed.
   *
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,newDaemonThreadFactory,org.apache.hadoop.util.BlockingThreadPoolExecutorService:newDaemonThreadFactory(java.lang.String),86,102,"/**
* Creates a thread factory that produces daemon threads with normal priority.
* @param prefix unique identifier for the thread pool
*/","* Get a named {@link ThreadFactory} that just builds daemon threads.
   *
   * @param prefix name prefix for all threads created from the factory
   * @return a thread factory that creates named, daemon threads with
   * the supplied exception handler and normal priority",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,<init>,"org.apache.hadoop.util.LightWeightResizableGSet:<init>(int,float)",66,81,"/**
* Initializes a LightWeightResizableGSet with specified initial capacity and load factor.
* @param initCapacity positive integer capacity, auto-adjusted if negative
* @param loadFactor ratio of elements to array size (0 < loadFactor <= 1.0f)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,size,org.apache.hadoop.util.LightWeightResizableGSet:size(),108,111,"/**
 * Returns the total number of elements in this collection.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,getIterator,org.apache.hadoop.util.LightWeightResizableGSet:getIterator(java.util.function.Consumer),113,115,"/**
* Passes an iterator over values to a consumer. 
* @param consumer callback function to process the iterator
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,expandIfNecessary,org.apache.hadoop.util.LightWeightResizableGSet:expandIfNecessary(),148,152,"/**
* Dynamically resizes the collection to accommodate growth beyond threshold.
* @param none
* @return none
*/","* Checks if we need to expand, and expands if necessary.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayList,org.apache.hadoop.util.Lists:newArrayList(java.util.Iterator),109,113,"/**
* Creates a new ArrayList from an iterator of elements.
* @param elements Iterator containing the initial elements
*/","* Creates a <i>mutable</i> {@code ArrayList} instance containing the
   * given elements; a very thin shortcut for creating an empty list
   * and then calling Iterators#addAll.
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return ArrayList Generics Type E.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,addAll,"org.apache.hadoop.util.Lists:addAll(java.util.Collection,java.lang.Iterable)",248,258,"/**
* Adds all elements from the specified iterable to the target collection.
* @param addTo target collection
* @param elementsToAdd iterable containing elements to be added
* @return true if addition was successful, false otherwise
*/","* Adds all elements in {@code iterable} to {@code collection}.
   *
   * @return {@code true} if {@code collection} was modified as a result of
   *     this operation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayListWithCapacity,org.apache.hadoop.util.Lists:newArrayListWithCapacity(int),128,132,"/**
* Creates an empty ArrayList with specified capacity.
* @param initialArraySize the initial size of the list
*/","* Creates an {@code ArrayList} instance backed by an array with the
   * specified initial size;
   * simply delegates to {@link ArrayList#ArrayList(int)}.
   *
   * @param <E> Generics Type E.
   * @param initialArraySize the exact size of the initial backing array for
   *     the returned array list
   *     ({@code ArrayList} documentation calls this value the ""capacity"").
   * @return a new, empty {@code ArrayList} which is guaranteed not to
   *     resize itself unless its size reaches {@code initialArraySize + 1}.
   * @throws IllegalArgumentException if {@code initialArraySize} is negative.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,computeArrayListCapacity,org.apache.hadoop.util.Lists:computeArrayListCapacity(int),192,195,"/**
* Calculates optimal ArrayList capacity based on provided size.
* @param arraySize input size
* @return computed capacity value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,getResource,org.apache.hadoop.util.ApplicationClassLoader:getResource(java.lang.String),128,153,"/**
* Resolves resource by name, searching classpath and parent resources.
* @param name Resource identifier
* @return URL to the resource or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,loadClass,"org.apache.hadoop.util.ApplicationClassLoader:loadClass(java.lang.String,boolean)",160,204,"/**
* Loads a class by name, attempting to find it in this loader's URLs and
* then the parent loader. If not found, throws ClassNotFoundException.
* @param name Class name to load
* @param resolve Whether to resolve the loaded class
* @return Loaded Class object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,writeJsonAsBytes,"org.apache.hadoop.util.JsonSerialization:writeJsonAsBytes(java.lang.Object,java.io.OutputStream)",305,312,"/**
* Writes JSON representation of an object to output stream.
* @param instance the object to serialize
* @param dataOutputStream the output stream to write to
*/","* Write the JSON as bytes, then close the stream.
   * @param instance instance to write
   * @param dataOutputStream an output stream that will always be closed
   * @throws IOException on any failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,<init>,org.apache.hadoop.util.OperationDuration:<init>(),48,51,"/**
* Initializes operation duration tracking with current timestamp.
* Sets both start and finish timestamps to the initial execution time.
*/","* Instantiate.
   * The start time and finished time are both set
   * to the current clock time.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,finished,org.apache.hadoop.util.OperationDuration:finished(),64,66,"/**
 * Records completion timestamp. 
 */",* Update the finished time with the current system time.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,asDuration,org.apache.hadoop.util.OperationDuration:asDuration(),114,116,"/**
* Converts value to a duration in milliseconds. 
* @return Duration object representing the time interval
*/","* Get the duration of an operation as a java Duration
   * instance.
   * @return a duration.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,getDurationString,org.apache.hadoop.util.OperationDuration:getDurationString(),72,74,"/**
* Returns duration as human-readable string.
* @return formatted time string in MM:SS or HH:MM:SS format
*/","* Return the duration as {@link #humanTime(long)}.
   * @return a printable duration.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,iterator,org.apache.hadoop.util.LightWeightCache:iterator(),234,256,"/**
* Returns an iterator over the cached entries, with unsupported remove operation. 
* @return Iterator<E> instance to iterate over cache entries
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,iterator,org.apache.hadoop.util.LightWeightGSet$Values:iterator(),240,243,"/**
* Returns an iterator over this lightweight GSet's elements.
* @return iterator over E objects in this GSet
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,getMonomial,"org.apache.hadoop.util.CrcUtil:getMonomial(long,int)",52,77,"/**
* Computes the monomial for a given lengthBytes and modulus.
* @param lengthBytes positive number of bytes
* @param mod modulus value
* @return monomial result or MULTIPLICATIVE_IDENTITY if input is zero
*/","* Compute x^({@code lengthBytes} * 8) mod {@code mod}, where {@code mod} is
   * in ""reversed"" (little-endian) format such that {@code mod & 1} represents
   * x^31 and has an implicit term x^32.
   *
   * @param lengthBytes lengthBytes.
   * @param mod mod.
   * @return monomial.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,composeWithMonomial,"org.apache.hadoop.util.CrcUtil:composeWithMonomial(int,int,int,int)",88,91,"/**
* Composes CRC values using a monomial polynomial in Galois field arithmetic.
* @param crcA first CRC value
* @param crcB second CRC value
* @param monomial monomial polynomial coefficient
* @param mod modulus for Galois field operations
*/","* composeWithMonomial.
   *
   * @param crcA crcA.
   * @param crcB crcB.
   * @param monomial Precomputed x^(lengthBInBytes * 8) mod {@code mod}
   * @param mod mod.
   * @return compose with monomial.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,intToBytes,org.apache.hadoop.util.CrcUtil:intToBytes(int),113,125,"/**
* Converts an integer value into a byte array representation (4 bytes).
* @param value integer value to convert
* @return byte[] containing the integer's binary representation
*/","* @return 4-byte array holding the big-endian representation of
   *     {@code value}.
   *
   * @param value value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,toSingleCrcString,org.apache.hadoop.util.CrcUtil:toSingleCrcString(byte[]),182,190,"/**
* Converts a 4-byte array to a hexadecimal string representing a single CRC value.
* @param bytes the input byte array (must be exactly 4 bytes)
*/","* For use with debug statements; verifies bytes.length on creation,
   * expecting it to represent exactly one CRC, and returns a hex
   * formatted value.
   *
   * @param bytes bytes.
   * @throws IOException raised on errors performing I/O.
   * @return a list of hex formatted values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,toMultiCrcString,org.apache.hadoop.util.CrcUtil:toMultiCrcString(byte[]),201,218,"/**
* Converts byte array to a comma-separated string of hexadecimal CRC values.
* @param bytes input byte array, length must be divisible by 4
*/","* For use with debug statements; verifies bytes.length on creation,
   * expecting it to be divisible by CRC byte size, and returns a list of
   * hex formatted values.
   *
   * @param bytes bytes.
   * @throws IOException raised on errors performing I/O.
   * @return a list of hex formatted values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJar,"org.apache.hadoop.util.RunJar:unJar(java.io.InputStream,java.io.File,java.util.regex.Pattern)",119,152,"/**
* Unpacks JAR entries to a specified directory based on a regex filter.
* @param inputStream input JAR stream
* @param toDir target directory for unpacking
* @param unpackRegex pattern to match files for unpacking
*/","* Unpack matching files from a jar. Entries inside the jar that do
   * not match the given pattern will be skipped.
   *
   * @param inputStream the jar stream to unpack
   * @param toDir the destination directory into which to unpack the jar
   * @param unpackRegex the pattern to match jar entries against
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJar,"org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File,java.util.regex.Pattern)",191,222,"/**
* Unjars a JAR file to the specified directory, unpacking files matching the given regex pattern.
* @param jarFile JAR file to unjar
* @param toDir target directory for unpacked files
* @param unpackRegex regular expression for files to unpack
*/","* Unpack matching files from a jar. Entries inside the jar that do
   * not match the given pattern will be skipped.
   *
   * @param jarFile the .jar file to unpack
   * @param toDir the destination directory into which to unpack the jar
   * @param unpackRegex the pattern to match jar entries against
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newCachedThreadPool,org.apache.hadoop.util.concurrent.HadoopExecutors:newCachedThreadPool(java.util.concurrent.ThreadFactory),36,42,"/**
* Creates a cached thread pool with dynamic thread creation.
* @param threadFactory factory for creating threads
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newFixedThreadPool,"org.apache.hadoop.util.concurrent.HadoopExecutors:newFixedThreadPool(int,java.util.concurrent.ThreadFactory)",44,50,"/**
* Creates a fixed-thread executor service with the specified number of threads.
* @param nThreads number of worker threads to create
* @param threadFactory factory for creating threads
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newFixedThreadPool,org.apache.hadoop.util.concurrent.HadoopExecutors:newFixedThreadPool(int),52,56,"/**
* Creates an ExecutorService with a fixed thread pool of size nThreads.
* @param nThreads number of threads in the pool
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newScheduledThreadPool,org.apache.hadoop.util.concurrent.HadoopExecutors:newScheduledThreadPool(int),71,74,"/**
* Creates a scheduled thread pool executor with the specified core size.
* @param corePoolSize the number of threads to keep in the pool
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newScheduledThreadPool,"org.apache.hadoop.util.concurrent.HadoopExecutors:newScheduledThreadPool(int,java.util.concurrent.ThreadFactory)",76,79,"/**
* Creates a scheduled thread pool executor with specified core size and factory.
* @param corePoolSize number of threads to keep in pool
* @param threadFactory factory for creating new threads
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopScheduledThreadPoolExecutor.java,afterExecute,"org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)",66,70,"/**
* Logs and handles exceptions thrown by executed runnables.
* @param r Runnable that was executed
* @param t Throwable exception (if any)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopThreadPoolExecutor.java,afterExecute,"org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)",87,91,"/**
* Logs thrown exceptions from executed tasks.
* @param r Runnable object that was being executed
* @param t Throwable exception that occurred (or null if no exception)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/AsyncGetFuture.java,get,org.apache.hadoop.util.concurrent.AsyncGetFuture:get(),56,60,"/**
* Retrieves the result of an asynchronous operation.
* @throws InterruptedException if interrupted while waiting
* @throws ExecutionException if execution failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/AsyncGetFuture.java,get,"org.apache.hadoop.util.concurrent.AsyncGetFuture:get(long,java.util.concurrent.TimeUnit)",62,67,"/**
* Executes asynchronous GET operation with specified timeout and returns result.
* @param timeout time to wait for operation completion
* @param unit unit of time (e.g. milliseconds)
* @return result object or throws exception if failed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/AsyncGetFuture.java,isDone,org.apache.hadoop.util.concurrent.AsyncGetFuture:isDone(),69,73,"/**
* Checks if task is complete by calling async get and delegating to superclass. 
* @return true if task is done, false otherwise.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,<init>,org.apache.hadoop.util.StopWatch:<init>(),33,35,"/**
* Initializes StopWatch with a new instance of Timer.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/UTF8ByteArrayUtils.java,findNthByte,"org.apache.hadoop.util.UTF8ByteArrayUtils:findNthByte(byte[],int,int,byte,int)",78,89,"/**
* Finds the nth occurrence of a byte within a UTF-8 encoded byte array.
* @param utf UTF-8 encoded byte array
* @param start starting index to search from
* @param length number of bytes to consider in search
* @param b target byte value
* @param n occurrence position to find (1-indexed)
* @return index of nth byte or -1 if not found","* Find the nth occurrence of the given byte b in a UTF-8 encoded string
   * @param utf a byte array containing a UTF-8 encoded string
   * @param start starting offset
   * @param length the length of byte array
   * @param b the byte to find
   * @param n the desired occurrence of the given byte
   * @return position that nth occurrence of the given byte if exists; otherwise -1",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CacheableIPList.java,<init>,"org.apache.hadoop.util.CacheableIPList:<init>(org.apache.hadoop.util.FileBasedIPList,long)",33,37,"/**
* Initializes a cacheable IP list with a specified timeout and file-based IP list.
* @param ipList File-based IP list to be cached
* @param cacheTimeout Timeout in milliseconds for the cached IP list
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,toString,org.apache.hadoop.util.WeakReferenceMap:toString(),108,115,"/**
* Returns a string representation of the WeakReferenceMap, including its size and creation counts.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,put,"org.apache.hadoop.util.WeakReferenceMap:put(java.lang.Object,java.lang.Object)",246,248,"/**
* Puts a value into the map with a weak reference to allow garbage collection.
* @param key unique key
* @param value value to store in map
* @return the previous value associated with the key or null if not present","* Put a value under the key.
   * A null value can be put, though on a get() call
   * a new entry is generated
   *
   * @param key key
   * @param value value
   * @return any old non-null reference.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,remove,org.apache.hadoop.util.WeakReferenceMap:remove(java.lang.Object),255,257,"/**
* Removes an entry from the map and returns its associated value.
* @param key key to be removed from the map
* @return associated value or null if not present in the map","* Remove any value under the key.
   * @param key key
   * @return any old non-null reference.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,containsKey,org.apache.hadoop.util.WeakReferenceMap:containsKey(java.lang.Object),266,269,"/**
* Checks if map contains a given key.
* @param key unique key to search for
* @return true if key is present, false otherwise
*/","* Does the map have a valid reference for this object?
   * no-side effects: there's no attempt to notify or cleanup
   * if the reference is null.
   * @param key key to look up
   * @return true if there is a valid reference.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,create,org.apache.hadoop.util.WeakReferenceMap:create(java.lang.Object),197,235,"/**
* Creates a persistent reference to an object of type V associated with the given key K.
* @param key unique identifier
* @return strong reference to the created object or null if lost during creation
*/","* Create a new instance under a key.
   * <p>
   * The instance is created, added to the map and then the
   * map value retrieved.
   * This ensures that the reference returned is that in the map,
   * even if there is more than one entry being created at the same time.
   * If that race does occur, it will be logged the first time it happens
   * for this specific map instance.
   * <p>
   * HADOOP-18456 highlighted the risk of a concurrent GC resulting a null
   * value being retrieved and so returned.
   * To prevent this:
   * <ol>
   *   <li>A strong reference is retained to the newly created instance
   *       in a local variable.</li>
   *   <li>That variable is used after the resolution process, to ensure
   *       the JVM doesn't consider it ""unreachable"" and so eligible for GC.</li>
   *   <li>A check is made for the resolved reference being null, and if so,
   *       the put() is repeated</li>
   * </ol>
   * @param key key
   * @return the created value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,prune,org.apache.hadoop.util.WeakReferenceMap:prune(),288,300,"/**
* Removes and notes lost entries with null values from the map.
* @return number of removed entries
*/","* Prune all null weak references, calling the referenceLost
   * callback for each one.
   *
   * non-atomic and non-blocking.
   * @return the number of entries pruned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,snapshot,org.apache.hadoop.util.InstrumentedLock$SuppressedStats:snapshot(),262,268,"/**
* Returns a snapshot of the suppressed count and wait time.
* @return SuppressedSnapshot object containing current values
*/","* Captures the current value of the counts into a SuppressedSnapshot object
     * and resets the values to zero.
     *
     * @return SuppressedSnapshot containing the current value of the counters",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,formatTimeDiff,"org.apache.hadoop.util.StringUtils:formatTimeDiff(long,long)",294,297,"/**
* Calculates and formats the time difference between two timestamps.
* @param finishTime end timestamp
* @param startTime start timestamp
* @return formatted time duration string
*/","* 
   * Given a finish and start time in long milliseconds, returns a 
   * String in the format Xhrs, Ymins, Z sec, for the time difference between two times. 
   * If finish time comes before start time then negative valeus of X, Y and Z wil return. 
   * 
   * @param finishTime finish time
   * @param startTime start time
   * @return a String in the format Xhrs, Ymins, Z sec,
   *         for the time difference between two times.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getTrimmedStringCollectionSplitByEquals,org.apache.hadoop.util.StringUtils:getTrimmedStringCollectionSplitByEquals(java.lang.String),505,525,"/**
* Extracts key-value pairs from a string collection, trimming and splitting by equals sign.
* @param str input string collection
*/","* Splits an ""="" separated value <code>String</code>, trimming leading and
   * trailing whitespace on each value after splitting by comma and new line separator.
   *
   * @param str a comma separated <code>String</code> with values, may be null
   * @return a <code>Map</code> of <code>String</code> keys and values, empty
   * Collection if null String input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,split,"org.apache.hadoop.util.StringUtils:split(java.lang.String,char,char)",581,601,"/**
* Splits a string into substrings using a specified separator, 
* handling escaped separators and preserving empty splits.
* @param str input string to split
* @param escapeChar character that escapes the separator
* @param separator character used for splitting
* @return array of split strings or null if input is null
*/","* Split a string using the given separator
   * @param str a string that may have escaped separator
   * @param escapeChar a char that be used to escape the separator
   * @param separator a separator char
   * @return an array of strings",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,escapeString,"org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char[])",701,716,"/**
* Escapes special characters in a string by prefixing them with a specified character.
* @param str input string to escape
* @param escapeChar character used for escaping
* @param charsToEscape array of characters to be escaped
* @return escaped string or null if input is null
*/","* escapeString.
   *
   * @param str str.
   * @param escapeChar escapeChar.
   * @param charsToEscape array of characters to be escaped
   * @return escapeString.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,unEscapeString,"org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char[])",748,782,"/**
* Unescapes a string with custom escape character and characters.
* @param str input string to unescape
* @param escapeChar special character that begins an escape sequence
* @param charsToEscape array of characters to be escaped
* @return the unescaped string or null if input is null
*/","* unEscapeString.
   * @param str str.
   * @param escapeChar escapeChar.
   * @param charsToEscape array of characters to unescape
   * @return escape string.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getVersion,org.apache.hadoop.util.VersionInfo:getVersion(),105,107,"/**
* Retrieves application version from COMMON_VERSION_INFO.
* @return string containing application version information
*/","* Get the Hadoop version.
   * @return the Hadoop version string, eg. ""0.6.3-dev""",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getRevision,org.apache.hadoop.util.VersionInfo:getRevision(),113,115,"/**
 * Retrieves the version revision information.
 */","* Get the Git commit hash of the repository when compiled.
   * @return the commit hash, eg. ""18f64065d5db6208daf50b02c1b5ed4ee3ce547a""",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getBranch,org.apache.hadoop.util.VersionInfo:getBranch(),121,123,"/**
* Retrieves the Git branch associated with the application.
* @return The current branch name or null if not available
*/","* Get the branch on which this originated.
   * @return The branch name, e.g. ""trunk"" or ""branches/branch-0.20""",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getDate,org.apache.hadoop.util.VersionInfo:getDate(),129,131,"/**
* Returns the current date string from COMMON_VERSION_INFO.
*/","* The date that Hadoop was compiled.
   * @return the compilation date in unix date format",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getUser,org.apache.hadoop.util.VersionInfo:getUser(),137,139,"/**
* Retrieves user information from the common version info.
* @return User string representation or empty string if not available
*/","* The user that compiled Hadoop.
   * @return the username of the user",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getUrl,org.apache.hadoop.util.VersionInfo:getUrl(),145,147,"/**
* Retrieves URL from COMMON_VERSION_INFO.
* @return The retrieved URL as a string.","* Get the URL for the Hadoop repository.
   * @return the URL of the Hadoop repository",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,_getBuildVersion,org.apache.hadoop.util.VersionInfo:_getBuildVersion(),85,90,"/**
* Constructs build version string combining version, revision, user, and source checksum.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getSrcChecksum,org.apache.hadoop.util.VersionInfo:getSrcChecksum(),153,155,"/**
* Retrieves source checksum from common version info.
*/","* Get the checksum of the source files from which Hadoop was built.
   * @return the checksum of the source files",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getProtocVersion,org.apache.hadoop.util.VersionInfo:getProtocVersion(),170,172,"/**
* Retrieves the Protobuf compiler version.
* @return The Protobuf compiler version as a string
*/","* Returns the protoc version used for the build.
   * @return the protoc version",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getCompilePlatform,org.apache.hadoop.util.VersionInfo:getCompilePlatform(),178,180,"/**
* Retrieves the compile platform from the common version info.
* @return string representing the compilation environment
*/","* Returns the OS platform used for the build.
   * @return the OS platform",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)",604,610,"/**
* Constructs a filtering iterator from a remote iterator and a predicate function.
* @param source the underlying remote iterator
* @param filter the function to apply for filtering elements
*/","* An iterator which combines filtering with transformation.
     * All source elements for which filter = true are returned,
     * transformed via the mapper.
     * @param source source iterator.
     * @param filter filter predicate.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)",521,526,"/**
* Creates a new iterator that maps each element from the source iterator.
* @param source input iterator to transform
* @param mapper function to apply to each source element
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,java.io.Closeable)",677,682,"/**
* Constructs a CloseRemoteIterator with a remote iterator and an object to close.
* @param source the remote iterator
* @param toClose object that will be closed when iterator is closed
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.CallableRaisingIOE)",773,778,"/**
* Constructs a haltable remote iterator with a specified continuation check.
* @param source the underlying remote iterator
* @param continueWork callable to determine if iteration should continue
*/","* Wrap an iterator with one which adds a continuation probe.
     * The probe will be called in the {@link #hasNext()} method, before
     * the source iterator is itself checked and in {@link #next()}
     * before retrieval.
     * That is: it may be called multiple times per iteration.
     * @param source source iterator.
     * @param continueWork predicate which will trigger a fast halt if it returns false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator),553,556,"/**
* Constructs a TypeCastingRemoteIterator from an existing RemoteIterator.
* @param source iterator to wrap and cast results
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,hasNext,org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:hasNext(),634,640,"/**
* Checks if there is another record available.
* @return true if next record exists, false otherwise
*/","* Trigger a fetch if an entry is needed.
     * @return true if there was already an entry return,
     * or there was not but one could then be retrieved.set
     * @throws IOException failure in fetch operation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,org.apache.hadoop.util.functional.RemoteIterators$MaybeClose:<init>(java.lang.Object),723,725,"/**
* Creates a MaybeClose instance with the given object.
* @param o object to be wrapped
*/","* Construct.
     * @param o object to close.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,close,org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:close(),415,419,"/**
* Closes the underlying data source.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,close,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:close(),454,457,"/**
* Closes the underlying data source.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,next,org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator:next(),829,837,"/**
* Returns the next unique ID in sequence.
* @throws IOException if an I/O error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,submit,"org.apache.hadoop.util.functional.CommonCallableSupplier:submit(java.util.concurrent.Executor,java.util.concurrent.Callable)",82,87,"/**
* Submits a task to the specified executor, returning a future result.
* @param executor executor to run the task on
* @param call task to execute and retrieve result from
* @return CompletableFuture containing the result or null if failed
*/","* Submit a callable into a completable future.
   * RTEs are rethrown.
   * Non RTEs are caught and wrapped; IOExceptions to
   * {@code RuntimeIOException} instances.
   * @param executor executor.
   * @param call     call to invoke
   * @param <T>      type
   * @return the future to wait for",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAutoCloseableReference.java,<init>,org.apache.hadoop.util.functional.LazyAutoCloseableReference:<init>(org.apache.hadoop.util.functional.CallableRaisingIOE),43,45,"/**
* Creates a lazy reference to an IO resource.
* @param constructor factory that creates the resource
*/","* Constructor for this instance.
   * @param constructor method to invoke to actually construct the inner object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAtomicReference.java,lazyAtomicReferenceFromSupplier,org.apache.hadoop.util.functional.LazyAtomicReference:lazyAtomicReferenceFromSupplier(java.util.function.Supplier),148,151,"/**
* Creates a lazy atomic reference from a supplier.
* @param supplier function that generates the initial value
*/","* Create from a supplier.
   * This is not a constructor to avoid ambiguity when a lambda-expression is
   * passed in.
   * @param supplier supplier implementation.
   * @return a lazy reference.
   * @param <T> type of reference",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAutoCloseableReference.java,eval,org.apache.hadoop.util.functional.LazyAutoCloseableReference:eval(),51,55,"/**
* Evaluates the expression and returns its result.
* Throws an IOException if the reference is closed.
*/","* {@inheritDoc}
   * @throws IllegalStateException if the reference is closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAtomicReference.java,apply,org.apache.hadoop.util.functional.LazyAtomicReference:apply(),104,107,"/**
* Evaluates and returns result of computation.
* @throws IOException if evaluation fails
*/","* Implementation of {@code CallableRaisingIOE.apply()}.
   * Invoke {@link #eval()}.
   * @return the value
   * @throws IOException on any evaluation failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FunctionalIO.java,uncheckIOExceptions,org.apache.hadoop.util.functional.FunctionalIO:uncheckIOExceptions(org.apache.hadoop.util.functional.CallableRaisingIOE),45,47,"/**
* Suppresses IOExceptions raised by the given callable.
* @param call function that may throw IO exceptions
* @return result of calling the function, ignoring IO exceptions
*/","* Invoke any operation, wrapping IOExceptions with
   * {@code UncheckedIOException}.
   * @param call callable
   * @param <T> type of result
   * @return result
   * @throws UncheckedIOException if an IOE was raised.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FunctionalIO.java,toUncheckedIOExceptionSupplier,org.apache.hadoop.util.functional.FunctionalIO:toUncheckedIOExceptionSupplier(org.apache.hadoop.util.functional.CallableRaisingIOE),55,57,"/**
* Converts a callable raising IOException into an unchecked supplier. 
* @param call function that returns T and raises IOException if fails
*/","* Wrap a {@link CallableRaisingIOE} as a {@link Supplier}.
   * @param call call to wrap
   * @param <T> type of result
   * @return a supplier which invokes the call.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,next,org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:next(),340,349,"/**
* Returns the next element from the iterator, or throws an exception if exhausted.
* @throws IOException if a read error occurs
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,foreach,org.apache.hadoop.util.functional.TaskPool:foreach(org.apache.hadoop.fs.RemoteIterator),591,593,"/**
* Creates a builder instance from an iterator of remote items.
* @param items RemoteIterator object containing items to process
*/","* Create a task builder for the remote iterator.
   * @param items item source.
   * @param <I> type of result.
   * @return builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,throwOne,org.apache.hadoop.util.functional.TaskPool:throwOne(java.util.Collection),607,622,"/**
* Combines and throws a single exception from the collection, 
* suppressing any incompatible exceptions.
* @param exceptions Collection of exceptions to combine
*/","* Throw one exception, adding the others as suppressed
   * exceptions attached to the one thrown.
   * This method never completes normally.
   * @param exceptions collection of exceptions
   * @param <E> class of exceptions
   * @throws E an extracted exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,<init>,org.apache.hadoop.util.functional.TaskPool$Builder:<init>(java.lang.Iterable),161,163,"/**
* Initializes builder with iterable of items.
* @param items Iterable containing I-type objects
*/","* Create the builder.
     * @param items items to process",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,suppressExceptions,org.apache.hadoop.util.functional.TaskPool$Builder:suppressExceptions(),197,199,"/**
* Suppresses exceptions in the builder flow.
* @return Builder instance with suppressed exceptions
*/","* Suppress exceptions from tasks.
     * RemoteIterator exceptions are not suppressable.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,raiseInnerCause,org.apache.hadoop.util.functional.FutureIO:raiseInnerCause(java.util.concurrent.ExecutionException),254,257,"/**
* Re-throws inner cause of an ExecutionException.
* @throws IOException if inner exception is not caught
*/","* From the inner cause of an execution exception, extract the inner cause
   * if it is an IOE or RTE.
   * This will always raise an exception, either the inner IOException,
   * an inner RuntimeException, or a new IOException wrapping the raised
   * exception.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,raiseInnerCause,org.apache.hadoop.util.functional.FutureIO:raiseInnerCause(java.util.concurrent.CompletionException),269,272,"/**
* Rethrows the inner exception of a CompletionException.
* @throws IOException if an I/O-related cause is found
*/","* Extract the cause of a completion failure and rethrow it if an IOE
   * or RTE.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,setJobConf,"org.apache.hadoop.util.ReflectionUtils:setJobConf(java.lang.Object,org.apache.hadoop.conf.Configuration)",89,115,"/**
* Configures a JobConfigurable object with a JobConf configuration.
* @param theObject JobConfigurable instance to be configured
* @param conf JobConf configuration for the job
*/","* This code is to support backward compatibility and break the compile  
   * time dependency of core on mapred.
   * This should be made deprecated along with the mapred package HADOOP-1230. 
   * Should be removed when mapred package is removed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClassByName,org.apache.hadoop.conf.Configuration:getClassByName(java.lang.String),2638,2644,"/**
* Retrieves a class by its fully qualified name, throwing an exception if not found.
* @param name fully qualified class name
* @return Class object or throws ClassNotFoundException if not found
*/","* Load a class by name.
   * 
   * @param name the class name.
   * @return the class object.
   * @throws ClassNotFoundException if the class is not found.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,printThreadInfo,"org.apache.hadoop.util.ReflectionUtils:printThreadInfo(java.io.PrintStream,java.lang.String)",183,221,"/**
* Prints detailed thread information to the specified PrintStream.
* @param stream output destination
* @param title optional title for thread dump
*/","* Print all of the thread's information and stack traces.
   * 
   * @param stream the stream to
   * @param title a string title for the stack trace",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration:<init>(boolean),830,836,"/**
* Initializes a Configuration instance with optional default values.
* @param loadDefaults whether to load default configuration settings
*/","A new configuration where the behavior of reading from the default 
   * resources can be turned off.
   * 
   * If the parameter {@code loadDefaults} is false, the new instance
   * will not load resources from the default files. 
   * @param loadDefaults specifies whether to load from the default files",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,createServletExceptionResponse,"org.apache.hadoop.util.HttpExceptionUtils:createServletExceptionResponse(javax.servlet.http.HttpServletResponse,int,java.lang.Throwable)",72,86,"/**
* Returns a JSON error response for the given exception.
* @param response HTTP response object
* @param status HTTP status code
* @param ex Throwable exception to serialize
*/","* Creates a HTTP servlet response serializing the exception in it as JSON.
   *
   * @param response the servlet response
   * @param status the error code to set in the response
   * @param ex the exception to serialize in the response
   * @throws IOException thrown if there was an error while creating the
   * response",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,createJerseyExceptionResponse,"org.apache.hadoop.util.HttpExceptionUtils:createJerseyExceptionResponse(javax.ws.rs.core.Response$Status,java.lang.Throwable)",95,104,"/**
* Creates a Jersey exception response with error details.
* @param status HTTP status code
* @param ex Throwable instance containing error information
*/","* Creates a HTTP JAX-RPC response serializing the exception in it as JSON.
   *
   * @param status the error code to set in the response
   * @param ex the exception to serialize in the response
   * @return the JAX-RPC response with the set error and JSON encoded exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,throwEx,org.apache.hadoop.util.HttpExceptionUtils:throwEx(java.lang.Throwable),119,121,"/**
* Wraps and re-throws an exception as a RuntimeException.
* @param ex the original Throwable to be re-thrown
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PureJavaCrc32C.java,<init>,org.apache.hadoop.util.PureJavaCrc32C:<init>(),41,43,"/**
 * Initializes a new instance of the CRC-32C calculator.",Create a new PureJavaCrc32 object.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,remove,org.apache.hadoop.util.IntrusiveCollection:remove(java.lang.Object),326,338,"/**
* Removes an element from the collection.
* @param o Element object to be removed
* @return true if removal was successful, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,toArray,org.apache.hadoop.util.IntrusiveCollection:toArray(),256,264,"/**
* Converts the collection to an array.
* @return Array of elements or null if empty
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,retainAll,org.apache.hadoop.util.IntrusiveCollection:retainAll(java.util.Collection),372,384,"/**
* Removes elements not present in the specified collection.
* @param collection collection to retain elements from
* @return true if any elements were removed, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,clear,org.apache.hadoop.util.IntrusiveCollection:clear(),389,395,"/**
* Removes all elements from this collection.
*/",* Remove all elements.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,containsAll,org.apache.hadoop.util.IntrusiveCollection:containsAll(java.util.Collection),340,348,"/**
* Checks if this set contains all elements of a given collection.
* @param collection the collection to check against
* @return true if all collection elements are in this set, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CleanerUtil.java,unmapHackImpl,org.apache.hadoop.util.CleanerUtil:unmapHackImpl(),89,160,"/**
* Fetches and returns a buffer cleaner for unmap operations.
*@return MethodHandle object or error message if unmapping is not supported
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,setIncludesFile,org.apache.hadoop.util.HostsFileReader:setIncludesFile(java.lang.String),313,319,"/**
* Updates host details with a new includes file.
* @param includesFile path to the updated includes file
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,setExcludesFile,org.apache.hadoop.util.HostsFileReader:setExcludesFile(java.lang.String),321,328,"/**
* Updates the host details with a new excludes file.
* @param excludesFile path to the excludes file
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,updateFileNames,"org.apache.hadoop.util.HostsFileReader:updateFileNames(java.lang.String,java.lang.String)",330,337,"/**
* Updates host details by modifying includes and excludes files.
* @param includesFile new includes file path
* @param excludesFile new excludes file path
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getExcludedHosts,org.apache.hadoop.util.HostsFileReader:getExcludedHosts(),266,269,"/**
* Retrieves excluded hosts from the current host details.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getGroupsForUserCommand,org.apache.hadoop.util.Shell:getGroupsForUserCommand(java.lang.String),229,239,"/**
* Returns command to fetch groups for the given user.
* @param user username
*/","* A command to get a given user's groups list.
   * If the OS is not WINDOWS, the command will get the user's primary group
   * first and finally get the groups list which includes the primary group.
   * i.e. the user's primary group will be included twice.
   *
   * @param user user.
   * @return groups for user command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getGroupsIDForUserCommand,org.apache.hadoop.util.Shell:getGroupsIDForUserCommand(java.lang.String),251,261,"/**
* Generates command to fetch user group IDs.
* @param user username
* @return array of strings representing the command
*/","* A command to get a given user's group id list.
   * The command will get the user's primary group
   * first and finally get the groups list which includes the primary group.
   * i.e. the user's primary group will be included twice.
   * This command does not support Windows and will only return group names.
   *
   * @param user user.
   * @return groups id for user command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getGetPermissionCommand,org.apache.hadoop.util.Shell:getGetPermissionCommand(),279,282,"/**
* Returns file listing command based on operating system.
* @return Array of command strings for Windows and Unix-like OS
*/","* Return a command to get permission information.
   *
   * @return permission command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSetPermissionCommand,"org.apache.hadoop.util.Shell:getSetPermissionCommand(java.lang.String,boolean)",291,301,"/**
* Generates 'chmod' command for setting permissions.
* @param perm permission string
* @param recursive true for recursive operation, false otherwise
*/","* Return a command to set permission.
   *
   * @param perm permission.
   * @param recursive recursive.
   * @return set permission command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSetOwnerCommand,org.apache.hadoop.util.Shell:getSetOwnerCommand(java.lang.String),326,330,"/**
* Generates the command to set file ownership.
* @param owner user or group name
* @return array of command arguments or null if not applicable
*/","* Return a command to set owner.
   *
   * @param owner owner.
   * @return set owner command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSymlinkCommand,"org.apache.hadoop.util.Shell:getSymlinkCommand(java.lang.String,java.lang.String)",339,343,"/**
* Generates cross-platform command to create a symbolic link.
* @param target path of the file or directory to be linked
* @param link path where the symbolic link will be created
*/","* Return a command to create symbolic links.
   *
   * @param target target.
   * @param link link.
   * @return symlink command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getReadlinkCommand,org.apache.hadoop.util.Shell:getReadlinkCommand(java.lang.String),351,355,"/**
* Constructs 'readlink' command for Unix or Windows platforms.
* @param link path to resolve
*/","* Return a command to read the target of the a symbolic link.
   *
   * @param link link.
   * @return read link command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSignalKillCommand,"org.apache.hadoop.util.Shell:getSignalKillCommand(int,java.lang.String)",373,394,"/**
* Generates shell command to signal or kill process.
* @param code signal value (0 for alive check, others for kill)
* @param pid unique process identifier
*/","* Return a command to send a signal to a given pid.
   *
   * @param code code.
   * @param pid pid.
   * @return signal kill command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,appendScriptExtension,"org.apache.hadoop.util.Shell:appendScriptExtension(java.io.File,java.lang.String)",419,421,"/**
* Appends script extension to a file with the given basename.
* @param parent directory containing the file
* @param basename base name of the file without extension
*/","* Returns a File referencing a script with the given basename, inside the
   * given parent directory.  The file extension is inferred by platform:
   * <code>"".cmd""</code> on Windows, or <code>"".sh""</code> otherwise.
   *
   * @param parent File parent directory
   * @param basename String script file basename
   * @return File referencing the script in the directory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,checkHadoopHome,org.apache.hadoop.util.Shell:checkHadoopHome(),483,493,"/**
* Resolves Hadoop home directory by checking JVM property and environment variables.
* @return File object representing the resolved Hadoop home directory
*/","*  Centralized logic to discover and validate the sanity of the Hadoop
   *  home directory.
   *
   *  This does a lot of work so it should only be called
   *  privately for initialization once per process.
   *
   * @return A directory that exists and via was specified on the command line
   * via <code>-Dhadoop.home.dir</code> or the <code>HADOOP_HOME</code>
   * environment variable.
   * @throws FileNotFoundException if the properties are absent or the specified
   * path is not a reference to a valid directory.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getHadoopHomeDir,org.apache.hadoop.util.Shell:getHadoopHomeDir(),620,627,"/**
* Retrieves the Hadoop home directory.
* @throws FileNotFoundException if failure cause is not null
*/","* Get the Hadoop home directory. If it is invalid,
   * throw an exception.
   * @return a path referring to hadoop home.
   * @throws FileNotFoundException if the directory doesn't exist.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getQualifiedBinInner,"org.apache.hadoop.util.Shell:getQualifiedBinInner(java.io.File,java.lang.String)",656,685,"/**
* Finds the qualified path for the specified executable within Hadoop's bin directory.
* @param hadoopHomeDir Hadoop home directory
* @param executable name of executable (e.g. ""hdfs"")
* @return File object representing the executable or null if not found
*/","* Inner logic of {@link #getQualifiedBin(String)}, accessible
   * for tests.
   * @param hadoopHomeDir home directory (assumed to be valid)
   * @param executable executable
   * @return path to the binary
   * @throws FileNotFoundException if the executable was not found/valid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getWinUtilsFile,org.apache.hadoop.util.Shell:getWinUtilsFile(),800,808,"/**
* Returns the Windows Utils configuration file.
* @throws FileNotFoundException if an error occurs while loading the file
*/","* Get a file reference to winutils.
   * Always raises an exception if there isn't one
   * @return the file instance referring to the winutils bin.
   * @throws FileNotFoundException on any failure to locate that file.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,destroyAllShellProcesses,org.apache.hadoop.util.Shell:destroyAllShellProcesses(),1428,1437,"/**
* Destroys all child shell processes and clears the shell registry.
*/","* Static method to destroy all running <code>Shell</code> processes.
   * Iterates through a map of all currently running <code>Shell</code>
   * processes and destroys them one by one. This method is thread safe",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,run,org.apache.hadoop.util.Shell$ShellTimeoutTimerTask:run(),1406,1420,"/**
* Terminates or destroys the process if it times out or doesn't complete.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownThreadsHelper.java,shutdownThread,org.apache.hadoop.util.ShutdownThreadsHelper:shutdownThread(java.lang.Thread),43,45,"/**
* Shuts down a given thread with a specified timeout.
* @param thread Thread to shut down
* @return true if thread was successfully terminated, false otherwise
*/","* @param thread {@link Thread to be shutdown}
   * @return <tt>true</tt> if the thread is successfully interrupted,
   * <tt>false</tt> otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownThreadsHelper.java,shutdownExecutorService,org.apache.hadoop.util.ShutdownThreadsHelper:shutdownExecutorService(java.util.concurrent.ExecutorService),78,81,"/**
* Shuts down ExecutorService with a specified wait timeout.
* @param service ExecutorService instance to shut down
*/","* shutdownExecutorService.
   *
   * @param service {@link ExecutorService to be shutdown}
   * @return <tt>true</tt> if the service is terminated,
   * <tt>false</tt> otherwise
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addNewPhase,org.apache.hadoop.util.Progress:addNewPhase(),80,85,"/**
* Creates and adds a new progress phase to the existing tree structure.
* @return the newly created Progress object
*/",Adds a new phase. Caller needs to set progress weightage,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,org.apache.hadoop.util.Progress:addPhase(float),107,123,"/**
* Adds a new progress phase with specified weightage and returns the phase.
* @param weightage the weightage for the new phase
*/","* Adds a node with a specified progress weightage to the tree.
   *
   * @param weightage weightage.
   * @return Progress.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,getInternal,org.apache.hadoop.util.Progress:getInternal(),244,269,"/**
* Calculates internal progress by summing completed and current phase progress.
* @return Internal progress as a float or the overall progress if no phases exist
*/",Computes progress in this node.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,toString,org.apache.hadoop.util.Progress:toString(java.lang.StringBuilder),282,288,"/**
* Asynchronously appends task status and phase details to the provided StringBuilder.
* @param buffer StringBuilder object to append output to
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,complete,org.apache.hadoop.util.Progress:complete(),170,184,"/**
* Completes the current progress phase and initiates the next phase in its parent.
* @param none
*/","Completes this node, moving the parent node to its next child.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getStringData,org.apache.hadoop.util.curator.ZKCuratorManager:getStringData(java.lang.String),260,266,"/**
* Reads string data from specified file or resource.
* @param path unique identifier for the file or resource
* @return string content or null if failed to read
*/","* Get the data in a ZNode.
   * @param path Path of the ZNode.
   * @return The data in the ZNode.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getStringData,"org.apache.hadoop.util.curator.ZKCuratorManager:getStringData(java.lang.String,org.apache.zookeeper.data.Stat)",275,281,"/**
* Retrieves string data from a specified file path.
* @param path the file path to access
* @param stat file status object
* @return string representation of the data or null if not available
*/","* Get the data in a ZNode.
   * @param path Path of the ZNode.
   * @param stat Output statistics of the ZNode.
   * @return The data in the ZNode.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,setData,"org.apache.hadoop.util.curator.ZKCuratorManager:setData(java.lang.String,java.lang.String,int)",301,304,"/**
* Sets data with specified encoding and calls underlying implementation.
* @param path unique identifier for the data
* @param data string data to set (UTF-8 encoded)
* @param version current data version
*/","* Set data into a ZNode.
   * @param path Path of the ZNode.
   * @param data Data to set as String.
   * @param version Version of the data to store.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,create,"org.apache.hadoop.util.curator.ZKCuratorManager:create(java.lang.String,java.util.List)",343,353,"/**
* Creates a persistent ZooKeeper node at the specified path with provided ACL.
* @param path unique node identifier
* @param zkAcl list of access control lists for the node
* @return true if node was successfully created, false otherwise
*/","* Create a ZNode.
   * @param path Path of the ZNode.
   * @param zkAcl ACL for the node.
   * @return If the ZNode was created.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,delete,org.apache.hadoop.util.curator.ZKCuratorManager:delete(java.lang.String),392,398,"/**
* Deletes a directory at specified path, recursively deleting children if present.
* @param path directory path to delete
* @return true if deletion was successful, false otherwise
*/","* Delete a ZNode.
   * @param path Path of the ZNode.
   * @return If the znode was deleted.
   * @throws Exception If it cannot contact ZooKeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,safeCreate,"org.apache.hadoop.util.curator.ZKCuratorManager:safeCreate(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode,java.util.List,java.lang.String)",410,419,"/**
* Creates a file at the specified path with given data and ACLs within a safe transaction.
* @param path file system path
* @param data binary file contents
* @param acl list of access control lists for the file
* @param mode create mode (e.g. overwrite, append)
* @param fencingACL ACLs for the fencing node
* @param fencingNodePath path to the fencing node
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,safeDelete,"org.apache.hadoop.util.curator.ZKCuratorManager:safeDelete(java.lang.String,java.util.List,java.lang.String)",429,437,"/**
* Deletes a file at the specified path with atomicity and ACL fencing.
* @param path file path to delete
* @param fencingACL list of access control lists for fencing
* @param fencingNodePath path of the node performing the deletion
*/","* Deletes the path. Checks for existence of path as well.
   *
   * @param path Path to be deleted.
   * @param fencingNodePath fencingNodePath.
   * @param fencingACL fencingACL.
   * @throws Exception if any problem occurs while performing deletion.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,safeSetData,"org.apache.hadoop.util.curator.ZKCuratorManager:safeSetData(java.lang.String,byte[],int,java.util.List,java.lang.String)",439,446,"/**
* Commits data to a safe transaction with specified ACL and fencing node.
* @param path data storage location
* @param data byte array containing the data
* @param version data version number
* @param fencingACL list of access control lists for fencing
* @param fencingNodePath path of the fencing node
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,addClass,"org.apache.hadoop.util.ProgramDriver:addClass(java.lang.String,java.lang.Class,java.lang.String)",101,104,"/**
* Registers a program with given details.
* @param name unique program identifier
* @param mainClass the main class of the program
* @param description human-readable program description
*/","* This is the method that adds the classed to the repository.
   * @param name The name of the string you want the class instance to be called with
   * @param mainClass The class that you want to add to the repository
   * @param description The description of the class
   * @throws NoSuchMethodException when a particular method cannot be found.
   * @throws SecurityException security manager to indicate a security violation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,impl,"org.apache.hadoop.util.dynamic.DynConstructors$Builder:impl(java.lang.String,java.lang.Class[])",134,149,"/**
* Dynamically loads and configures an implementation class.
* @param className name of the implementation class to load
* @param types variable arguments of type Class<?> representing method parameter types
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,hiddenImpl,org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.Class[]),166,169,"/**
* Overloaded hidden implementation constructor.
* @param types variable number of class types
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.String,java.lang.Class[])",171,186,"/**
* Tries to find a hidden implementation class.
* @param className name of the target class
* @param types parameterized types for the constructor
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,ctorImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:ctorImpl(java.lang.Class,java.lang.Class[])",348,362,"/**
* Constructs a DynConstructors instance for a target class with specified argument classes.
* @param targetClass Class containing the constructor to be constructed
* @param argClasses Classes of arguments for the constructor (variable arity)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,newInstanceChecked,org.apache.hadoop.util.dynamic.DynConstructors$Ctor:newInstanceChecked(java.lang.Object[]),56,66,"/**
* Creates a new instance of the class, handling exceptions and wrapping causes.
* @throws Exception if instantiation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeChecked,"org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invokeChecked(java.lang.Object,java.lang.Object[])",73,89,"/**
* Invokes a method on the target object with variable arguments, 
* checking for expected argument count and re-throwing exceptions. 
* @param target object to invoke method on
* @param args variable number of arguments to pass to method
* @return result of method invocation or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.Class,java.lang.String,java.lang.Class[])",320,333,"/**
* Resolves a method implementation for the given target class and method name.
* @param targetClass class containing the method
* @param methodName method name to resolve
* @param argClasses parameter types of the method (varargs)
* @return Builder instance with resolved method or null if not found
*/","* Checks for a method implementation.
     * @param targetClass the class to check for an implementation
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,<init>,"org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod$1:<init>(java.lang.reflect.Method,java.lang.String)",66,71,"/**
* Initializes an unbound method with a given method and name.
* @param method the underlying method
* @param name the method's name
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,<init>,"org.apache.hadoop.util.dynamic.DynConstructors$Ctor:<init>(java.lang.reflect.Constructor,java.lang.Class)",46,50,"/**
* Initializes Constructor subclass with provided constructor and class.
* @param constructor constructor to wrap
* @param constructed class for wrapped constructor
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.Class,java.lang.String,java.lang.Class[])",423,438,"/**
* Finds and sets a hidden implementation of a method on a target class.
* @param targetClass class containing the method
* @param methodName name of the method to find
* @param argClasses classes of method arguments (varargs)
*/","* Checks for a method implementation.
     * @param targetClass the class to check for an implementation
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,noop,org.apache.hadoop.util.dynamic.BindingUtils:noop(java.lang.String),158,160,"/**
* Creates an unbound method with the given name that does nothing.
* @param name unique method identifier
*/","* Create a no-op method.
   *
   * @param name method name
   *
   * @return a no-op method.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,implemented,org.apache.hadoop.util.dynamic.BindingUtils:implemented(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod[]),169,176,"/**
* Verifies that all provided unbound methods are not no-ops.
* @param methods array of unbound methods to check
* @return true if all methods have implementation, false otherwise
*/","* Given a sequence of methods, verify that they are all available.
   *
   * @param methods methods
   *
   * @return true if they are all implemented",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,available,org.apache.hadoop.util.dynamic.BindingUtils:available(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod),195,197,"/**
* Checks if the given unbound method is available.
* @param method UnboundMethod instance to check
* @return true if method is not a noop, false otherwise
*/","* Is a method available?
   * @param method method to probe
   * @return true iff the method is found and loaded.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,bind,org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:bind(java.lang.Object),118,125,"/**
* Binds a method to an object instance.
* @param receiver the object instance
*/","* Returns this method as a BoundMethod for the given receiver.
     * @param receiver an Object to receive the method invocation
     * @return a {@link BoundMethod} for this method and the receiver
     * @throws IllegalStateException if the method is static
     * @throws IllegalArgumentException if the receiver's class is incompatible",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,asStatic,org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:asStatic(),146,149,"/**
* Converts instance method to static method.
* @return a new static method wrapper
*/","* Returns this method as a StaticMethod.
     * @return a {@link StaticMethod} for this method
     * @throws IllegalStateException if the method is not static",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ClassUtil.java,findContainingJar,org.apache.hadoop.util.ClassUtil:findContainingJar(java.lang.Class),38,40,"/**
* Finds the JAR file containing the specified class.
* @param clazz The class whose containing JAR is searched
*/","* Find a jar that contains a class of the same name, if any.
   * It will return a jar file, even if that is not the first thing
   * on the class path that has a class with the same name.
   * 
   * @param clazz the class to find.
   * @return a jar file that contains the class, or null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ClassUtil.java,findClassLocation,org.apache.hadoop.util.ClassUtil:findClassLocation(java.lang.Class),48,50,"/**
* Resolves class location from its Class object.
* @param clazz Class object to resolve location for
*/","* Find the absolute location of the class.
   *
   * @param clazz the class to find.
   * @return the class file with absolute location, or null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,makeRpcRequestHeader,"org.apache.hadoop.util.ProtoUtil:makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto,int,int,byte[],org.apache.hadoop.ipc.AlignmentContext)",178,212,"/**
* Creates an RPC request header with given parameters.
* @param rpcKind RPC kind enum value
* @param operation operation proto object
* @param callId unique call identifier
* @param retryCount number of retries
* @param uuid client UUID byte array
* @param alignmentContext optional alignment context
* @return built RpcRequestHeaderProto object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,getHeader,org.apache.hadoop.util.DataChecksum:getHeader(),226,235,"/**
* Creates a checksum header array with type ID and bytes per checksum.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,mapByteToChecksumType,org.apache.hadoop.util.DataChecksum:mapByteToChecksumType(int),204,212,"/**
* Converts byte size to corresponding Type enum.
* @param type byte size
* @throws InvalidChecksumSizeException if invalid type
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,writeValue,"org.apache.hadoop.util.DataChecksum:writeValue(java.io.DataOutputStream,boolean)",246,263,"/**
* Writes a value to the output stream based on the data type.
* @param out DataOutputStream instance
* @param reset whether to reset internal state after writing
* @return size of written data type
*/","* Writes the current checksum to the stream.
   * If <i>reset</i> is true, then resets the checksum.
   *
   * @param out out.
   * @param reset reset.
   * @return number of bytes written. Will be equal to getChecksumSize();
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,writeValue,"org.apache.hadoop.util.DataChecksum:writeValue(byte[],int,boolean)",275,296,"/**
* Writes value to buffer with optional checksum and resets state.
* @param buf buffer to write to
* @param offset starting position in buffer
* @param reset whether to reset internal state after writing
* @return size of written value
*/","* Writes the current checksum to a buffer.
    * If <i>reset</i> is true, then resets the checksum.
    *
    * @param buf buf.
    * @param offset offset.
    * @param reset reset.
    * @return number of bytes written. Will be equal to getChecksumSize();
    * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RateLimitingFactory.java,create,org.apache.hadoop.util.RateLimitingFactory:create(int),95,100,"/**
* Creates a rate limiting instance with specified capacity.
* @param capacity maximum allowed requests per time unit; 0 for unlimited rate
*/","* Create an instance.
   * If the rate is 0; return the unlimited rate.
   * @param capacity capacity in permits/second.
   * @return limiter restricted to the given capacity.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SignalLogger.java,register,org.apache.hadoop.util.SignalLogger:register(org.slf4j.Logger),71,92,"/**
* Installs UNIX signal handlers for termination, hangup, and interrupt.
* @param log Logger instance for logging
*/","* Register some signal handlers.
   *
   * @param log The log4j logfile to use in the signal handlers.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,parseItem,"org.apache.hadoop.util.ComparableVersion:parseItem(boolean,java.lang.String)",455,458,"/**
* Creates an item from the given buffer, either as a digit or string item.
* @param isDigit true if the item represents a digit, false otherwise
* @param buf buffer containing the item data
* @return Item object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,<init>,org.apache.hadoop.util.LightWeightGSet:<init>(int),90,97,"/**
* Initializes a LightWeightGSet with the specified recommended length.
* @param recommended_length target size of the set
*/",* @param recommended_length Recommended size of the internal array.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,get,org.apache.hadoop.util.LightWeightGSet:get(java.lang.Object),126,142,"/**
* Retrieves an element of type E associated with the given key K.
* @param key unique identifier for the desired element
* @return Element object or null if not found
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,clear,org.apache.hadoop.util.LightWeightGSet$Values:clear(),256,259,"/**
 * Clears this set by calling the clear operation on its backing collection.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,toString,org.apache.hadoop.util.SemaphoredDelegatingExecutor:toString(),200,209,"/**
* Returns a string representation of the SemaphoredDelegatingExecutor, including permit count, availability, and waiting tasks.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readFileToSet,"org.apache.hadoop.util.HostsFileReader:readFileToSet(java.lang.String,java.lang.String,java.util.Set)",77,82,"/**
* Populates a set with lines from a specified file.
* @param type file content type (e.g. ""CSV"", ""XML"")
* @param filename path to the file to read
* @param set collection to store unique file contents
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readXmlFileToMapWithFileInputStream,"org.apache.hadoop.util.HostsFileReader:readXmlFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map)",146,183,"/**
* Parses XML file into a map of host names to timeouts.
* @param type indicates the type of hosts being read
* @param filename name of the XML file being parsed
* @param fileInputStream input stream containing the XML data
* @param map map to store host names and their corresponding timeouts
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getHosts,org.apache.hadoop.util.HostsFileReader:getHosts(),261,264,"/**
* Returns a set of included hosts.
* @return Set of host names
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getHostDetails,"org.apache.hadoop.util.HostsFileReader:getHostDetails(java.util.Set,java.util.Set)",278,283,"/**
* Merges host inclusion and exclusion details from the current configuration.
* @param includes set of included hosts
* @param excludes set of excluded hosts
*/","* Retrieve an atomic view of the included and excluded hosts.
   *
   * @param includes set to populate with included hosts
   * @param excludes set to populate with excluded hosts
   * @deprecated use {@link #getHostDetails() instead}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getHostDetails,"org.apache.hadoop.util.HostsFileReader:getHostDetails(java.util.Set,java.util.Map)",292,298,"/**
* Merges host details from the current object into provided maps.
* @param includeHosts set of hosts to include
* @param excludeHosts map of hosts to exclude (by ID)
*/","* Retrieve an atomic view of the included and excluded hosts.
   *
   * @param includeHosts set to populate with included hosts
   * @param excludeHosts map to populate with excluded hosts
   * @deprecated use {@link #getHostDetails() instead}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/JenkinsHash.java,hash,"org.apache.hadoop.util.hash.JenkinsHash:hash(byte[],int,int)",86,245,"/**
 * Computes the hash of a given key.
 * 
 * @param key     the byte array to be hashed
 * @param nbytes  the number of bytes in the key
 * @param initval the initial value for the hash calculation
 * @return        the computed hash value as an integer
 */","* taken from  hashlittle() -- hash a variable-length key into a 32-bit value
   * 
   * @param key the key (the unaligned variable-length array of bytes)
   * @param nbytes number of bytes to include in hash
   * @param initval can be any integer value
   * @return a 32-bit value.  Every bit of the key affects every bit of the
   * return value.  Two keys differing by one or two bits will have totally
   * different hash values.
   * 
   * <p>The best hash table sizes are powers of 2.  There is no need to do mod
   * a prime (mod is sooo slow!).  If you need less than 32 bits, use a bitmask.
   * For example, if you need only 10 bits, do
   * <code>h = (h &amp; hashmask(10));</code>
   * In which case, the hash table should have hashsize(10) elements.
   * 
   * <p>If you are hashing n strings byte[][] k, do it like this:
   * for (int i = 0, h = 0; i &lt; n; ++i) h = hash( k[i], h);
   * 
   * <p>By Bob Jenkins, 2006.  bob_jenkins@burtleburtle.net.  You may use this
   * code any way you wish, private, educational, or commercial.  It's free.
   * 
   * <p>Use for hash table lookup, or anything where one collision in 2^^32 is
   * acceptable.  Do NOT use for cryptographic purposes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/Hash.java,getInstance,org.apache.hadoop.util.hash.Hash:getInstance(int),75,84,"/**
* Retrieves a hash instance based on the specified type.
* @param type unique identifier for the desired hash algorithm
* @return Hash object or null if invalid type is provided
*/","* Get a singleton instance of hash function of a given type.
   * @param type predefined hash type
   * @return hash function instance, or null if type is invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/MurmurHash.java,hash,"org.apache.hadoop.util.hash.MurmurHash:hash(byte[],int,int)",40,43,"/**
* Overloaded hash function variant with specified array bounds.
* @param data input byte array
* @param length array length to process (must be non-negative)
* @param seed initial hash value (optional)
* @return hash code integer
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,<init>,org.apache.hadoop.util.bloom.CountingBloomFilter:<init>(),84,84,"/**
* Initializes an empty Counting Bloom Filter instance.",Default constructor - use with readFields,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,<init>,org.apache.hadoop.util.bloom.BloomFilter:<init>(),99,101,"/**
* Initializes a new instance of the BloomFilter class.",Default constructor - use with readFields,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,<init>,org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>(),113,113,"/**
 * Initializes an empty Dynamic Bloom Filter instance. 
 */",* Zero-args constructor for the serialization.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,and,org.apache.hadoop.util.bloom.CountingBloomFilter:and(org.apache.hadoop.util.bloom.Filter),162,176,"/**
* Combines two CountingBloomFilter instances using bitwise AND operation.
* @param filter another CountingBloomFilter instance with matching parameters
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,or,org.apache.hadoop.util.bloom.CountingBloomFilter:or(org.apache.hadoop.util.bloom.Filter),246,261,"/**
* Combines the given filter with the current one using bitwise OR operation.
* @param filter CountingBloomFilter instance to combine
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,write,org.apache.hadoop.util.bloom.CountingBloomFilter:write(java.io.DataOutput),292,299,"/**
* Writes the vector's bucket data to the output stream.
* @throws IOException if write operation fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,and,org.apache.hadoop.util.bloom.DynamicBloomFilter:and(org.apache.hadoop.util.bloom.Filter),155,173,"/**
* Combines two DynamicBloomFilters using logical AND operation.
* @param filter another filter to combine with this one
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,not,org.apache.hadoop.util.bloom.DynamicBloomFilter:not(),190,195,"/**
* Applies logical NOT operation to each row of the matrix.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,or,org.apache.hadoop.util.bloom.DynamicBloomFilter:or(org.apache.hadoop.util.bloom.Filter),197,214,"/**
* Combines two Bloom filters using logical OR operation.
* @param filter another DynamicBloomFilter to combine with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,xor,org.apache.hadoop.util.bloom.DynamicBloomFilter:xor(org.apache.hadoop.util.bloom.Filter),216,233,"/**
* Performs bitwise XOR operation with another filter.
* @param filter filter to perform XOR operation with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,write,org.apache.hadoop.util.bloom.BloomFilter:write(java.io.DataOutput),199,216,"/**
* Writes vector data to output stream.
* @throws IOException on write error
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/HashFunction.java,hash,org.apache.hadoop.util.bloom.HashFunction:hash(org.apache.hadoop.util.bloom.Key),108,122,"/**
* Computes a hash array for the given key.
* @param k Key object
* @return int[] of length nbHash representing the hash values
*/","* Hashes a specified key into several integers.
   * @param k The specified key.
   * @return The array of hashed values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,compareTo,org.apache.hadoop.util.bloom.Key:compareTo(org.apache.hadoop.util.bloom.Key),172,183,"/**
* Compares two Key objects based on their byte length and weight.
* @param other the key to compare with
* @return a negative/positive value if this key is less/greater, or zero if equal. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,getWeight,org.apache.hadoop.util.bloom.RetouchedBloomFilter:getWeight(java.util.List),381,387,"/**
* Calculates total weight by summing weights of all keys in the list.
* @param keyList collection of Key objects to aggregate weights from
* @return total weight as a double value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,formatMessage,"org.apache.hadoop.util.JvmPauseMonitor:formatMessage(long,java.util.Map,java.util.Map)",118,143,"/**
* Formats message for pause detection, including GC activity.
* @param extraSleepTime approximate duration of pause
* @param gcTimesAfterSleep and gcTimesBeforeSleep GC times maps before/after sleep
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/AutoCloseableLock.java,<init>,org.apache.hadoop.util.AutoCloseableLock:<init>(),38,40,"/**
* Constructs an instance of AutoCloseableLock with a new ReentrantLock.
*/","* Creates an instance of {@code AutoCloseableLock}, initializes
   * the underlying lock instance with a new {@code ReentrantLock}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/AutoCloseableLock.java,close,org.apache.hadoop.util.AutoCloseableLock:close(),94,97,"/**
 * Closes the resource by releasing any held resources.
 */","* Attempts to release the lock by making a call to {@code release()}.
   *
   * This is to implement {@code close()} method from {@code AutoCloseable}
   * interface. This allows users to user a try-with-resource syntax, where
   * the lock can be automatically released.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,isNull,org.apache.hadoop.util.ComparableVersion$StringItem:isNull(),208,211,"/**
* Checks if version qualifier matches release version index.
* @return true if match found, false otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,compareTo,org.apache.hadoop.util.ComparableVersion$StringItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item),232,253,"/**
* Compares this Item to the given Item based on its type and value.
* @param item Item object to compare with
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,printStack,"org.apache.hadoop.util.FindClass:printStack(java.lang.Throwable,java.lang.String,java.lang.Object[])",234,237,"/**
* Prints error message and stack trace to stderr.
* @param e Throwable object
* @param text error message with placeholders for args
* @param args variable arguments to fill placeholders in text
*/","* print a stack trace with text
   * @param e the exception to print
   * @param text text to print",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,explainResult,"org.apache.hadoop.util.FindClass:explainResult(int,java.lang.String)",368,370,"/**
* Logs an error message with code and description.
* @param errorcode unique error identifier
* @param text human-readable error description
*/","* Explain an error code as part of the usage
   * @param errorcode error code returned
   * @param text error text",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,loadedClass,"org.apache.hadoop.util.FindClass:loadedClass(java.lang.String,java.lang.Class)",266,271,"/**
* Logs loaded class details with its source location.
* @param name class name
* @param clazz loaded Class object
*/","* Log that a class has been loaded, and where from.
   * @param name classname
   * @param clazz class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,calculateGCTimePercentageWithinObservedInterval,org.apache.hadoop.util.GcTimeMonitor:calculateGCTimePercentageWithinObservedInterval(),186,224,"/**
* Calculates GC time percentage within the observed time interval.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,getLatestGcData,org.apache.hadoop.util.GcTimeMonitor:getLatestGcData(),182,184,"/**
* Returns a clone of the current GC data.
* @return GcData object representing latest available data
*/","* Returns a copy of the most recent data measured by this monitor.
   * @return a copy of the most recent data measured by this monitor",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PureJavaCrc32.java,<init>,org.apache.hadoop.util.PureJavaCrc32:<init>(),45,47,"/**
* Initializes CRC-32 calculator to default state.
*/",Create a new PureJavaCrc32 object.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,executeShutdown,org.apache.hadoop.util.ShutdownHookManager:executeShutdown(),117,136,"/**
* Executes shutdown hooks and returns number of timed-out hooks.
*/","* Execute the shutdown.
   * This is exposed purely for testing: do not invoke it.
   * @return the number of shutdown hooks which timed out.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,put,org.apache.hadoop.util.PriorityQueue:put(java.lang.Object),61,65,"/**
 * Inserts an element into the heap, maintaining the heap property.
 * @param element the element to insert
 */","* Adds an Object to a PriorityQueue in log(size) time.
   * If one tries to add more objects than maxSize from initialize
   * a RuntimeException (ArrayIndexOutOfBound) is thrown.
   * @param element element.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,pop,org.apache.hadoop.util.PriorityQueue:pop(),104,114,"/**
* Removes and returns the top element from the heap.
* @return T object or null if empty
*/","* Removes and returns the least element of the PriorityQueue in log(size)
      time.
   * @return T Generics Type T.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,adjustTop,org.apache.hadoop.util.PriorityQueue:adjustTop(),123,125,"/**
 * Adjusts the top element in the heap data structure.
 */","Should be called when the Object at top changes values.  Still log(n)
   * worst case, but it's at least twice as fast to <pre>
   *  { pq.top().change(); pq.adjustTop(); }
   * </pre> instead of <pre>
   *  { o = pq.pop(); o.change(); pq.push(o); }
   * </pre>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,addAll,"org.apache.hadoop.util.Sets:addAll(java.util.TreeSet,java.lang.Iterable)",161,171,"/**
* Adds all elements from the given iterable to the TreeSet.
* @param addTo set to add elements to
* @param elementsToAdd iterable containing elements to add
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSet,org.apache.hadoop.util.Sets:newHashSet(java.util.Iterator),191,195,"/**
* Creates a new hash set from an iterator of elements.
* @param elements iterable collection of elements to include
*/","* Creates a <i>mutable</i> {@code HashSet} instance containing the given
   * elements. A very thin convenience for creating an empty set and then
   * calling Iterators#addAll.
   *
   * <p><b>Note:</b> if mutability is not required and the elements are
   * non-null, use ImmutableSet#copyOf(Iterator) instead.</p>
   *
   * <p><b>Note:</b> if {@code E} is an {@link Enum} type, you should create
   * an {@link EnumSet} instead.</p>
   *
   * <p>Overall, this method is not very useful and will likely be deprecated
   * in the future.</p>
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return a new, empty thread-safe {@code Set}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSetWithExpectedSize,org.apache.hadoop.util.Sets:newHashSetWithExpectedSize(int),213,215,"/**
* Creates a new HashSet with an initial capacity based on the expected size.
* @param expectedSize The anticipated number of elements to store
*/","* Returns a new hash set using the smallest initial table size that can hold
   * {@code expectedSize} elements without resizing. Note that this is not what
   * {@link HashSet#HashSet(int)} does, but it is what most users want and
   * expect it to do.
   *
   * <p>This behavior can't be broadly guaranteed, but has been tested with
   * OpenJDK 1.7 and 1.8.</p>
   *
   * @param expectedSize the number of elements you expect to add to the
   *     returned set
   * @param <E> Generics Type E.
   * @return a new, empty hash set with enough capacity to hold
   *     {@code expectedSize} elements without resizing
   * @throws IllegalArgumentException if {@code expectedSize} is negative",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SequentialNumber.java,skipTo,org.apache.hadoop.util.SequentialNumber:skipTo(long),78,91,"/**
* Atomically sets currentValue to newValue, throwing an exception if newValue is less than the current value.
* @param newValue new value to set
* @throws IllegalStateException if newValue is less than the current value
*/","* Skip to the new value.
   * @param newValue newValue.
   * @throws IllegalStateException
   *         Cannot skip to less than the current value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,printUsage,org.apache.hadoop.util.ProgramDriver:printUsage(java.util.Map),85,91,"/**
* Prints valid program names and descriptions.
* @param programs a map of program names to their descriptions
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/CommandShell.java,run,org.apache.hadoop.tools.CommandShell:run(java.lang.String[]),63,84,"/**
* Executes a command with optional subcommand, printing usage and error messages as needed.
* @param args array of command-line arguments
* @return non-zero exit code on failure, zero otherwise
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,<init>,"org.apache.hadoop.tools.TableListing$Column:<init>(java.lang.String,org.apache.hadoop.tools.TableListing$Justification,boolean)",52,58,"/**
* Initializes a column with the specified title and settings.
* @param title column header text
* @param justification alignment of subsequent rows (LEFT, CENTER, RIGHT)
* @param wrap whether to automatically wrap long row contents
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,build,org.apache.hadoop.tools.TableListing$Builder:build(),194,197,"/**
* Constructs and returns a TableListing instance with specified columns.
* @return TableListing object
*/","* Create a new TableListing.
     *
     * @return TableListing.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$7:getDefault(double),522,522,"/**
* Returns a default double value based on the input.
* This is an abstract method and should be implemented by subclasses. 
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$3:getDefault(double),522,522,"/**
* Returns the default value based on input. 
* This is an abstract method and must be implemented by subclasses. 
* @param value input value to determine default
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,isDeprecated,org.apache.hadoop.conf.Configuration:isDeprecated(java.lang.String),670,672,"/**
* Checks if a given API key has been deprecated.
* @param key unique API key identifier
*/","* checks whether the given <code>key</code> is deprecated.
   * 
   * @param key the parameter which is to be checked for deprecation
   * @return <code>true</code> if the key is deprecated and 
   *         <code>false</code> otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getDeprecatedKeyInfo,org.apache.hadoop.conf.Configuration:getDeprecatedKeyInfo(java.lang.String),678,680,"/**
* Retrieves deprecated key info by identifier from deprecation context.
* @param key unique key identifier
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,dumpDeprecatedKeys,org.apache.hadoop.conf.Configuration:dumpDeprecatedKeys(),4009,4019,"/**
* Prints deprecated keys and their replacements.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,hasWarnedDeprecation,org.apache.hadoop.conf.Configuration:hasWarnedDeprecation(java.lang.String),4027,4035,"/**
* Checks if a given class or method has been warned for deprecation.
* @param name identifier of the class or method to check
* @return true if deprecation warning was issued, false otherwise
*/","* Returns whether or not a deprecated name has been warned. If the name is not
   * deprecated then always return false
   * @param name proprties.
   * @return true if name is a warned deprecation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getDeprecatedKey,org.apache.hadoop.conf.Configuration:getDeprecatedKey(java.lang.String),674,676,"/**
* Retrieves deprecated key by its original value.
* @param key original key value
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,reloadExistingConfigurations,org.apache.hadoop.conf.Configuration:reloadExistingConfigurations(),880,888,"/**
* Reloads all existing configurations in the registry.
*/",* Reload existing configuration instances.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDefaultResource,org.apache.hadoop.conf.Configuration:addDefaultResource(java.lang.String),895,904,"/**
* Adds a default resource by name and reloads configurations as needed.
* @param name name of the default resource to add
*/","* Add a default resource. Resources are loaded in the order of the resources 
   * added.
   * @param name file name. File should be present in the classpath.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,boolean)",257,259,"/**
 * Constructs a Resource instance with the provided object and parser settings.
 * @param resource Object to be wrapped as a Resource
 * @param useRestrictedParser Whether to use restricted parser for parsing
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDurationHelper,"org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",1951,1969,"/**
* Converts time duration string to specified unit, handling parsing and conversion.
* @param name identifier for logging purposes
* @param vStr input time duration value as string (e.g. ""1h"", ""2d"")
* @param defaultUnit fallback unit if input string is invalid
* @return converted time duration in specified unit (e.g. seconds, milliseconds)","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d).
   *
   * @param name Property name
   * @param vStr The string value with time unit suffix to be converted.
   * @param defaultUnit Unit to convert the stored property, if it exists.
   * @param returnUnit Unit for the returned value.
   * @return time duration in given time unit.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,parse,"org.apache.hadoop.conf.Configuration:parse(java.net.URL,boolean)",3045,3063,"/**
* Parses XML from the given URL, returning an XMLStreamReader or null if invalid.
* @param url URL to parse (null returns null)
* @param restricted whether to restrict parsing
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleInclude,org.apache.hadoop.conf.Configuration$Parser:handleInclude(),3296,3372,"/**
* Handles xi:include element by fetching and parsing the included resource.
* @throws XMLStreamException if parsing fails
* @throws IOException if resource access fails
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadProperty,"org.apache.hadoop.conf.Configuration:loadProperty(java.util.Properties,java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String[])",3546,3568,"/**
* Sets or updates a property in the given Properties object.
* @param properties target Properties object
* @param name property name
* @param attr attribute to set
* @param value new value (or DEFAULT_STRING_CHECK if null)
* @param finalParameter whether this is a final parameter
* @param source optional array of sources for updating resources
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,toString,org.apache.hadoop.conf.Configuration:toString(),3901,3913,"/**
* Returns a human-readable string representation of the configuration.
* Includes default resources if loadDefaults is true.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getAllPropertiesByTags,org.apache.hadoop.conf.Configuration:getAllPropertiesByTags(java.util.List),4056,4062,"/**
* Combines properties by all specified tags.
* @param tagList list of unique tags
* @return combined Properties object or null if empty
*/","* Get all properties belonging to list of input tags. Calls
   * getAllPropertiesByTag internally.
   * @param tagList list of input tags
   * @return Properties with matching tags",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$6:getDefault(double),522,522,"/**
* Returns the default value as a double. 
* This is an abstract method that must be implemented by subclasses.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$2:getDefault(double),522,522,"/**
* Returns the default value based on the provided input.
* @param value the input value to determine the default value from. 
*              The exact logic is implementation-specific and defined by subclasses.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigRedactor.java,redact,"org.apache.hadoop.conf.ConfigRedactor:redact(java.lang.String,java.lang.String)",65,70,"/**
* Redacts sensitive configuration values.
* @param key unique configuration key
* @param value configuration value to be redacted or returned as is
* @return the original value if not sensitive, otherwise 'REDACTED_TEXT'","* Given a key / value pair, decides whether or not to redact and returns
   * either the original value or text indicating it has been redacted.
   *
   * @param key param key.
   * @param value param value, will return if conditions permit.
   * @return Original value, or text indicating it has been redacted",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigRedactor.java,redactXml,"org.apache.hadoop.conf.ConfigRedactor:redactXml(java.lang.String,java.lang.String)",97,102,"/**
* Redacts sensitive XML data by replacing it with a fixed string.
* @param key   unique identifier for the sensitive data
* @return      either the original value or the redacted string
*/","* Given a key / value pair, decides whether or not to redact and returns
   * either the original value or text indicating it has been redacted.
   *
   * @param key param key.
   * @param value param value, will return if conditions permit.
   * @return Original value, or text indicating it has been redacted",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,getReconfigurationTaskStatus,org.apache.hadoop.conf.ReconfigurableBase:getReconfigurationTaskStatus(),189,196,"/**
* Retrieves the current reconfiguration task status.
* @return ReconfigurationTaskStatus object with task details
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$4:getDefault(double),522,522,"/**
* Returns default value based on the provided input. 
* This is an abstract method and should be implemented by subclasses.
* @param value The input value to determine the default
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,startReconfigurationTask,org.apache.hadoop.conf.ReconfigurableBase:startReconfigurationTask(),169,187,"/**
* Starts server reconfiguration task.
* @throws IOException if server is stopped or another task is running
*/","* Start a reconfiguration task to reload configuration in background.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationContext:<init>(org.apache.hadoop.conf.Configuration$DeprecationContext,org.apache.hadoop.conf.Configuration$DeprecationDelta[])",487,517,"/**
* Constructs a DeprecationContext by merging another context and deltas.
* @param other the context to merge, or null
* @param deltas array of deprecation changes
*/","* Create a new DeprecationContext by copying a previous DeprecationContext
     * and adding some deltas.
     *
     * @param other   The previous deprecation context to copy, or null to start
     *                from nothing.
     * @param deltas  The deltas to apply.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationException.java,<init>,"org.apache.hadoop.conf.ReconfigurationException:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.Throwable)",67,74,"/**
* Constructs a ReconfigurationException with specific details.
* @param property affected property name
* @param newVal new value being configured
* @param oldVal previous value (for comparison)
* @param cause optional root cause of the exception
*/","* Create a new instance of {@link ReconfigurationException}.
   * @param property property name.
   * @param newVal new value.
   * @param oldVal old value.
   * @param cause original exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationException.java,<init>,"org.apache.hadoop.conf.ReconfigurationException:<init>(java.lang.String,java.lang.String,java.lang.String)",82,88,"/**
* Constructs a ReconfigurationException with the specified property, new value, and old value.
* @param property affected configuration property
* @param newVal new value that caused the exception
* @param oldVal original value
*/","* Create a new instance of {@link ReconfigurationException}.
   * @param property property name.
   * @param newVal new value.
   * @param oldVal old value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleEndProperty,org.apache.hadoop.conf.Configuration$Parser:handleEndProperty(),3415,3446,"/**
* Handles end property by reading tags and updating new keys if deprecated.
* @param name property name
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getWarningMessage,org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:getWarningMessage(java.lang.String),382,384,"/**
* Returns warning message based on given key.
* @param key unique identifier for warning message
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,iterator,org.apache.hadoop.conf.Configuration$IntegerRanges:iterator(),2283,2286,"/**
* Returns an iterator over the ranges of numbers.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration$IntegerRanges:<init>(java.lang.String),2195,2217,"/**
* Parses string representation of integer ranges and adds them to the list.
* @param newValue comma-separated string of integer ranges (e.g., 1-10,20)
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageSize.java,parse,org.apache.hadoop.conf.StorageSize:parse(java.lang.String),50,96,"/**
* Parses human-readable storage size string into StorageSize object.
* @param value input string in format <number><unit> (e.g. ""1000MB"")
* @return StorageSize object or throws IllegalArgumentException if invalid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$5:getDefault(double),522,522,"/**
* Returns the default value for the given input. 
* This implementation is left to subclasses.
* @param value input value subject to default calculation
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$1:getDefault(double),522,522,"/**
* Returns a default value based on a given input value.
* @param value input value to determine the default
* @return default value, type and behavior determined by subclass
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reset,org.apache.hadoop.ha.ActiveStandbyElector:reset(),931,934,"/**
* Resets application state to INIT and terminates existing connection.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,convert,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:convert(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),144,162,"/**
* Converts user StateChangeRequestInfo to HAStateChangeRequestInfoProto.
* @param reqInfo input request information
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,convert,org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:convert(org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto),87,105,"/**
* Converts HAStateChangeRequestInfoProto to StateChangeRequestInfo.
* @param proto HAStateChangeRequestInfoProto object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,createReqInfo,org.apache.hadoop.ha.HAAdmin:createReqInfo(),264,266,"/**
 * Creates a new StateChangeRequestInfo instance based on the request source.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,createReqInfo,org.apache.hadoop.ha.ZKFailoverController:createReqInfo(),510,512,"/**
 * Creates a new StateChangeRequestInfo instance with RequestSource.REQUEST_BY_ZKFC. 
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,createReqInfo,org.apache.hadoop.ha.FailoverController:createReqInfo(),158,160,"/**
 * Creates a new StateChangeRequestInfo instance with the specified request source.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,getServiceStatus,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getServiceStatus(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusRequestProto)",143,178,"/**
* Fetches HA service status and returns a GetServiceStatusResponseProto object.
* @param request GetServiceStatusRequestProto object
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,startRPC,org.apache.hadoop.ha.ZKFailoverController:startRPC(),336,338,"/**
 * Starts the RPC server.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,parseConfiggedPort,org.apache.hadoop.ha.SshFenceByTcpPort$Args:parseConfiggedPort(java.lang.String),258,266,"/**
* Parses a string representation of a port number.
* @param portStr string containing the port number
* @return parsed port number or throws exception if invalid
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,checkArgs,org.apache.hadoop.ha.ShellCommandFencer:checkArgs(java.lang.String),72,79,"/**
* Validates input arguments for shell fencing.
* @param args input string or null if empty
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,"org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.lang.String,java.util.Map)",138,149,"/**
* Prints command usage information to the specified output stream.
* @param pStr output stream
* @param cmd current command
* @param helpEntries map of commands and their usage info
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkManualStateManagementOK,org.apache.hadoop.ha.HAAdmin:checkManualStateManagementOK(org.apache.hadoop.ha.HAServiceTarget),245,262,"/**
* Checks if manual state management is allowed.
* @param target HAServiceTarget to evaluate
* @return true if allowed, false otherwise
*/","* Ensure that we are allowed to manually manage the HA state of the target
   * service. If automatic failover is configured, then the automatic
   * failover controllers should be doing state management, and it is generally
   * an error to use the HAAdmin command line to do so.
   * 
   * @param target the target to check
   * @return true if manual state management is allowed",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,execCommand,"org.apache.hadoop.ha.SshFenceByTcpPort:execCommand(com.jcraft.jsch.Session,java.lang.String)",177,203,"/**
* Executes a command in the SSH session and returns its exit status.
* @param session active SSH connection
* @param cmd command to execute
* @return exit status or -1 on error
*/","* Execute a command through the ssh session, pumping its
   * stderr and stdout to our own logs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,enteredState,org.apache.hadoop.ha.ZKFailoverController$HealthCallbacks:enteredState(org.apache.hadoop.ha.HealthMonitor$State),988,992,"/**
* Updates health state and reevaluates electability.",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,badArg,org.apache.hadoop.ha.ZKFailoverController:badArg(java.lang.String),272,276,"/**
* Throws exception for invalid input argument.
* @param arg offending user-provided string
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,checkEligibleForFailover,org.apache.hadoop.ha.ZKFailoverController:checkEligibleForFailover(),763,776,"/**
* Verifies the service can be a failover target.
* @throws ServiceFailedException if not healthy or in observer state
*/","* If the local node is an observer or is unhealthy it
   * is not eligible for graceful failover.
   * @throws ServiceFailedException if the node is an observer or unhealthy",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,fence,org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget),92,94,"/**
* Fences HA service target by calling fence with no additional data.
* @param fromSvc HA Service Target to fence
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getFencingParameters,org.apache.hadoop.ha.HAServiceTarget:getFencingParameters(),175,179,"/**
* Returns a map of fencing parameters.
* @return Map of parameter names to their values
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,monitorActiveStatus,org.apache.hadoop.ha.ActiveStandbyElector:monitorActiveStatus(),774,781,"/**
* Monitors and updates the active status of a node.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,zkDoWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:zkDoWithRetries(org.apache.hadoop.ha.ActiveStandbyElector$ZKAction,org.apache.zookeeper.KeeperException$Code)",1145,1159,"/**
* Performs a ZooKeeper action with retries on KeeperExceptions.
* @param action the ZKAction to execute
* @param retryCode additional retry code (if any)
* @return result of the action, or throws exception if max retries exceeded
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readInDirectBuffer,"org.apache.hadoop.fs.VectoredReadUtils:readInDirectBuffer(org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer,org.apache.hadoop.util.functional.Function4RaisingIOE)",189,216,"/**
* Reads data from a specified range into a direct ByteBuffer using the provided operation.
* @param range FileRange object specifying the source and length of data to read
* @param buffer ByteBuffer to store the read data
* @param operation Function to apply to each chunk of data (must raise IOE if an error occurs)
*/","* Read bytes from stream into a byte buffer using an
   * intermediate byte array.
   *   <pre>
   *     (position, buffer, buffer-offset, length): Void
   *     position:= the position within the file to read data.
   *     buffer := a buffer to read fully `length` bytes into.
   *     buffer-offset := the offset within the buffer to write data
   *     length := the number of bytes to read.
   *   </pre>
   * The passed in function MUST block until the required length of
   * data is read, or an exception is thrown.
   * @param range range to read
   * @param buffer buffer to fill.
   * @param operation operation to use for reading data.
   * @throws IOException any IOE.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,validateAndSortRanges,"org.apache.hadoop.fs.VectoredReadUtils:validateAndSortRanges(java.util.List,java.util.Optional)",292,337,"/**
* Validates and sorts a list of FileRanges, ensuring they do not overlap.
* @param input List of FileRanges to validate
* @return Sorted list of validated FileRanges or null if empty
*/","* Validate a list of ranges (including overlapping checks) and
   * return the sorted list.
   * <p>
   * Two ranges overlap when the start offset
   * of second is less than the end offset of first.
   * End offset is calculated as start offset + length.
   * @param input input list
   * @param fileLength file length if known
   * @return a new sorted list.
   * @throws IllegalArgumentException if there are overlapping ranges or
   * a range element is invalid (other than with negative offset)
   * @throws EOFException if the last range extends beyond the end of the file supplied
   *                          or a range offset is negative",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,readVectored,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:readVectored(java.util.List,java.util.function.IntFunction)",319,345,"/**
* Performs a vectored read operation on the file, using asynchronous I/O.
* @param ranges List of FileRange objects to read from
* @param allocate Function to allocate ByteBuffer for each range
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,<init>,org.apache.hadoop.fs.impl.prefetch.BlockManager:<init>(org.apache.hadoop.fs.impl.prefetch.BlockData),49,53,"/**
* Initializes BlockManager with given block data.
* @param blockData block data object
*/","* Constructs an instance of {@code BlockManager}.
   *
   * @param blockData information about each block of the underlying file.
   *
   * @throws IllegalArgumentException if blockData is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,release,org.apache.hadoop.fs.impl.prefetch.BlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData),107,111,"/**
* Releases allocated BufferData, but does nothing since it's always reallocated. 
*/","* Releases resources allocated to the given block.
   *
   * @param data the {@code BufferData} to release.
   *
   * @throws IllegalArgumentException if data is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,release,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:release(java.lang.Object),88,111,"/**
* Releases an item from the pool, adding it back to storage.
* @param item item to release
*/","* Releases a previously acquired resource.
   *
   * @throws IllegalArgumentException if item is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,throwIfStateIncorrect,org.apache.hadoop.fs.impl.prefetch.BufferData:throwIfStateIncorrect(org.apache.hadoop.fs.impl.prefetch.BufferData$State[]),259,275,"/**
* Throws an exception if the current state does not match any of the provided states.
* @param states one or more expected states
*/","* Helper that asserts the current state is one of the expected values.
   *
   * @param states the collection of allowed states.
   *
   * @throws IllegalArgumentException if states is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.String,java.lang.String)",103,109,"/**
* Validates that both input arguments are non-null and non-empty.
* @param arg input string to validate
* @param argName name of the argument for error messages
*/","* Validates that the given string is not null and has non-zero length.
   * @param arg the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNumberOfElements,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNumberOfElements(java.util.Collection,int,java.lang.String)",182,192,"/**
* Validates a collection's existence and element count.
* @param collection the collection to validate
* @param numElements expected number of elements
* @param argName name of the collection for error messages
*/","* Validates that the given set is not null and has an exact number of items.
   * @param <T> the type of collection's elements.
   * @param collection the argument reference to validate.
   * @param numElements the expected number of elements in the collection.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPathExists,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExists(java.nio.file.Path,java.lang.String)",346,350,"/**
* Verifies if the specified file or directory path exists.
* @param path the file system path to check
* @param argName name of the argument that uses this path (for error message)
*/","* Validates that the given path exists.
   * @param path the path to check.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,<init>,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:<init>(int),57,65,"/**
* Initializes a bounded resource pool with the specified capacity.
* @param size maximum number of resources in the pool
*/","* Constructs a resource pool of the given size.
   *
   * @param size the size of this pool. Cannot be changed post creation.
   *
   * @throws IllegalArgumentException if size is zero or negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,<init>,"org.apache.hadoop.fs.impl.prefetch.BufferPool:<init>(int,int,org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics)",90,108,"/**
* Initializes BufferPool with specified size, buffer size, and prefetching statistics.
* @param size total number of buffers in the pool
* @param bufferSize individual buffer capacity
* @param prefetchingStatistics statistics for memory allocation
*/","* Initializes a new instance of the {@code BufferPool} class.
   * @param size number of buffer in this pool.
   * @param bufferSize size in bytes of each buffer.
   * @param prefetchingStatistics statistics for this stream.
   * @throws IllegalArgumentException if size is zero or negative.
   * @throws IllegalArgumentException if bufferSize is zero or negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,requestPrefetch,org.apache.hadoop.fs.impl.prefetch.BlockManager:requestPrefetch(int),120,124,"/**
* Requests prefetch of data associated with a specific block number.
* @param blockNumber unique identifier for the block to prefetch
*/","* Requests optional prefetching of the given block.
   *
   * @param blockNumber the id of the block to prefetch.
   *
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,<init>,"org.apache.hadoop.fs.impl.prefetch.BufferData:<init>(int,java.nio.ByteBuffer)",111,118,"/**
* Initializes a new BufferData object with the specified block number and data.
* @param blockNumber unique block identifier
* @param buffer ByteBuffer containing data
*/","* Constructs an instances of this class.
   *
   * @param blockNumber Number of the block associated with this buffer.
   * @param buffer The buffer associated with this block.
   *
   * @throws IllegalArgumentException if blockNumber is negative.
   * @throws IllegalArgumentException if buffer is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Retryer.java,<init>,"org.apache.hadoop.fs.impl.prefetch.Retryer:<init>(int,int,int)",55,63,"/**
* Initializes a Retryer with custom delay and update intervals.
* @param perRetryDelay time to wait between retries
* @param maxDelay maximum allowed delay before retrying
* @param statusUpdateInterval interval for updating retry status
*/","* Initializes a new instance of the {@code Retryer} class.
   *
   * @param perRetryDelay per retry delay (in ms).
   * @param maxDelay maximum amount of delay (in ms) before retry fails.
   * @param statusUpdateInterval time interval (in ms) at which status update would be made.
   *
   * @throws IllegalArgumentException if perRetryDelay is zero or negative.
   * @throws IllegalArgumentException if maxDelay is less than or equal to perRetryDelay.
   * @throws IllegalArgumentException if statusUpdateInterval is zero or negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,throwIfInvalidBlockNumber,org.apache.hadoop.fs.impl.prefetch.BlockData:throwIfInvalidBlockNumber(int),243,245,"/**
* Validates if block number is within valid range.
* @param blockNumber target block number
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,throwIfInvalidOffset,org.apache.hadoop.fs.impl.prefetch.BlockData:throwIfInvalidOffset(long),247,249,"/**
* Validates offset value against file size range.
* @param offset position to be validated
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Object[],java.lang.String)",117,120,"/**
* Validates that an array is not null and has at least one element.
* @param array the input array
* @param argName name of the argument for error messages
*/","* Validates that the given array is not null and has at least one element.
   * @param <T> the type of array's elements.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(byte[],java.lang.String)",127,130,"/**
* Verifies that the provided byte array is not null and non-empty.
* @param array the byte array to validate
* @param argName name of the argument for error messages
*/","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(short[],java.lang.String)",137,140,"/**
* Validates that an array is not null and not empty.
* @param array short array to be validated
* @param argName name of the argument for error messages
*/","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(int[],java.lang.String)",147,150,"/**
* Validates that an integer array is not null and has elements.
* @param array The input array to validate
* @param argName Name of the argument being validated (for error messages)
*/","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(long[],java.lang.String)",157,160,"/**
* Validates that input array is not null and has at least one element.
* @param array input array to validate
* @param argName name of the argument for error messages
*/","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Iterable,java.lang.String)",168,173,"/**
* Validates Iterable<T> not null and contains at least one element.
* @param iter the iterable to validate
* @param argName name of the argument for error messages
*/","* Validates that the given buffer is not null and has non-zero capacity.
   * @param <T> the type of iterable's elements.
   * @param iter the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/DefaultBulkDeleteOperation.java,bulkDelete,org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:bulkDelete(java.util.Collection),74,91,"/**
* Deletes files in bulk by paths.
* @param paths collection of file paths to delete
* @return list of deleted file paths and any exceptions that occurred
*/","* {@inheritDoc}.
     * The default impl just calls {@code FileSystem.delete(path, false)}
     * on the single path in the list.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,applyToIOStatisticsSnapshot,"org.apache.hadoop.io.wrappedio.WrappedStatistics:applyToIOStatisticsSnapshot(java.io.Serializable,org.apache.hadoop.util.functional.FunctionRaisingIOE)",339,344,"/**
* Applies a function to an IO statistics snapshot.
* @param source any serializable object
* @param fun a function that raises IO exceptions and returns result of type T
* @return result of applying the function, or null if it throws an exception
*/","* Apply a function to an object which may be an IOStatisticsSnapshot.
   * @param <T> return type
   * @param source statistics snapshot
   * @param fun function to invoke if {@code source} is valid.
   * @return the applied value
   * @throws UncheckedIOException Any IO exception.
   * @throws IllegalArgumentException if the supplied class is not a snapshot",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,<init>,"org.apache.hadoop.fs.impl.FlagSet:<init>(java.lang.Class,java.lang.String,java.util.EnumSet)",83,92,"/**
* Initializes FlagSet with the given enum class, prefix, and optional initial flags.
* @param enumClass Class of the enumeration
* @param prefix Prefix for flag names
* @param flags Optional initial EnumSet of flags (null to start empty)
*/","* Create a FlagSet.
   * @param enumClass class of enum
   * @param prefix prefix (with trailing ""."") for path capabilities probe
   * @param flags flags. A copy of these are made.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,seek,org.apache.hadoop.fs.sftp.SFTPInputStream:seek(long),60,67,"/**
* Seeks the file to a specific position.
* @param position target position in bytes
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,available,org.apache.hadoop.fs.sftp.SFTPInputStream:available(),69,77,"/**
* Returns the number of bytes available for reading.
* @return The maximum number of bytes that can be read without blocking
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,close,org.apache.hadoop.fs.FileSystem:close(),2701,2712,"/**
* Closes the object, deleting files and logging a close event.
* @throws IOException if an I/O error occurs during closing
*/","* Close this FileSystem instance.
   * Will release any held locks, delete all files queued for deletion
   * through calls to {@link #deleteOnExit(Path)}, and remove this FS instance
   * from the cache, if cached.
   *
   * After this operation, the outcome of any method call on this FileSystem
   * instance, or any input/output stream created by it is <i>undefined</i>.
   * @throws IOException IO failure",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,equals,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:equals(java.lang.Object),153,160,"/**
* Compares this MRNflyNode instance with another for equality.
* @param o object to compare, must be MRNflyNode
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,listStatus,org.apache.hadoop.fs.ChecksumFs:listStatus(org.apache.hadoop.fs.Path),567,580,"/**
* Filters and returns file status array for the given path, excluding checksum files.
* @param f Path to list file statuses from
* @return Array of FileStatus objects or empty array if none found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,compareTo,org.apache.hadoop.fs.FileStatus:compareTo(java.lang.Object),425,429,"/**
* Compares this file status with another.
* @param o The file status to compare with
*/","* Compare this FileStatus to another FileStatus based on lexicographical
   * order of path.
   * This method was added back by HADOOP-14683 to keep binary compatibility.
   *
   * @param   o the FileStatus to be compared.
   * @return  a negative integer, zero, or a positive integer as this object
   *   is less than, equal to, or greater than the specified object.
   * @throws ClassCastException if the specified object is not FileStatus",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,compareTo,org.apache.hadoop.fs.LocatedFileStatus:compareTo(org.apache.hadoop.fs.FileStatus),181,184,"/**
* Compares this FileStatus with the given one.
* @param o FileStatus to compare with
* @return negative/positive value if different, zero if equal
*/","* Compare this FileStatus to another FileStatus
   * @param   o the FileStatus to be compared.
   * @return  a negative integer, zero, or a positive integer as this object
   *   is less than, equal to, or greater than the specified object.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,stat2Paths,"org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.Path)",133,138,"/**
* Converts FileStatus array to Path array.
* If input is empty or null, returns an array containing the provided file path.
* @param stats array of file statuses
* @param path base directory for paths in the output array
* @return array of Paths corresponding to the input FileStatuses
*/","* convert an array of FileStatus to an array of Path.
   * If stats if null, return path
   * @param stats
   *          an array of FileStatus objects
   * @param path
   *          default path to return in stats is null
   * @return an array of paths corresponding to the input",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.HarFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),1300,1303,"/**
* Retrieves default replication settings for the specified file.
* @param f Path to the file
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.FilterFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),447,450,"/**
* Retrieves default replication settings for a file.
* @param f Path to the file
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,run,org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner:run(),4166,4181,"/**
* Continuously cleans up StatisticsDataReference objects from queue.
* @throws InterruptedException if interrupted while waiting for data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandler.java,openConnection,org.apache.hadoop.fs.FsUrlStreamHandler:openConnection(java.net.URL),46,49,"/**
* Opens an FsUrlConnection to the specified URL.
* @param url URL to connect to
* @return FsUrlConnection object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,read,"org.apache.hadoop.fs.FSInputStream:read(long,byte[],int,int)",66,89,"/**
* Reads data from the underlying store into a byte array.
* @param position current file position
* @param buffer destination byte array
* @param offset starting index in buffer
* @param length number of bytes to read
* @return actual number of bytes read or -1 on error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.HarFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",896,905,"/**
* Checks if a path has the specified capability.
* @param path Path object to check
* @param capability Capability string to verify
* @return True if path has capability, false otherwise
*/","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,serializer,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:serializer(),262,264,"/**
* Creates a JSON serializer instance for IOStatisticsSnapshot objects.
*/","* Get a JSON serializer for this class.
   * @return a serializer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,publishAsStorageStatistics,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:publishAsStorageStatistics(java.lang.String,java.lang.String,org.apache.hadoop.fs.statistics.IOStatistics)",701,704,"/**
* Converts given IO statistics to storage statistics with specified name and scheme.
* @param name unique identifier for the statistics
* @param scheme scheme of the statistics (e.g. 'read', 'write')
* @param source IO statistics to convert from
* @return StorageStatistics object
*/","* Publish the IOStatistics as a set of storage statistics.
   * This is dynamic.
   * @param name storage statistics name.
   * @param scheme FS scheme; may be null.
   * @param source IOStatistics source.
   * @return a dynamic storage statistics object.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStorageStatistics,org.apache.hadoop.fs.FileSystem:getStorageStatistics(),4651,4653,"/**
* Retrieves storage statistics with an empty profile.
* @return EmptyStorageStatistics object representing storage stats
*/","* Get the StorageStatistics for this FileSystem object.  These statistics are
   * per-instance.  They are not shared with any other FileSystem object.
   *
   * <p>This is a default method which is intended to be overridden by
   * subclasses. The default implementation returns an empty storage statistics
   * object.</p>
   *
   * @return    The StorageStatistics for this FileSystem instance.
   *            Will never be null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIsDirectoryException.java,<init>,org.apache.hadoop.fs.PathIsDirectoryException:<init>(java.lang.String),24,26,"/**
* Constructs an exception indicating the specified path is a directory.
* @param path file system path that caused the exception
*/",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIsNotDirectoryException.java,<init>,org.apache.hadoop.fs.PathIsNotDirectoryException:<init>(java.lang.String),24,26,"/**
* Constructs an exception indicating the specified path is not a directory.
* @param path the invalid file path
*/",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathOperationException.java,<init>,org.apache.hadoop.fs.PathOperationException:<init>(java.lang.String),24,26,"/**
* Constructs an OperationNotSupported exception with the specified file system path.",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIsNotEmptyDirectoryException.java,<init>,org.apache.hadoop.fs.PathIsNotEmptyDirectoryException:<init>(java.lang.String),23,25,"/**
* Constructs an exception for a non-empty directory at the specified path.
* @param path absolute directory path that caused the error
*/",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,bufferSize,org.apache.hadoop.fs.FSDataOutputStreamBuilder:bufferSize(int),175,178,"/**
* Sets buffer size and returns this builder instance.
* @param bufSize new buffer size value
*/","* Set the size of the buffer to be used.
   *
   * @param bufSize buffer size.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,replication,org.apache.hadoop.fs.FSDataOutputStreamBuilder:replication(short),190,193,"/**
* Sets replication configuration and returns this builder.
* @param replica short value representing the replication setting
*/","* Set replication factor.
   *
   * @param replica replica.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,blockSize,org.apache.hadoop.fs.FSDataOutputStreamBuilder:blockSize(long),205,208,"/**
* Sets block size and returns this builder instance.
* @param blkSize the new block size
*/","* Set block size.
   *
   * @param blkSize block size.
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,recursive,org.apache.hadoop.fs.FSDataOutputStreamBuilder:recursive(),224,227,"/**
* Recursively sets this builder to itself and returns it.
* @return This builder instance (this)
*/","* Create the parent directory if they do not exist.
   *
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,create,org.apache.hadoop.fs.FSDataOutputStreamBuilder:create(),254,257,"/**
* Creates and initializes a new instance of type B.
* @return A builder object representing the newly created instance
*/","* Create an FSDataOutputStream at the specified path.
   *
   * @return return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,overwrite,org.apache.hadoop.fs.FSDataOutputStreamBuilder:overwrite(boolean),267,274,"/**
* Sets or clears OVERWRITE flag based on the provided boolean value.
* @param overwrite true to set, false to clear the flag
*/","* Set to true to overwrite the existing file.
   * Set it to false, an exception will be thrown when calling {@link #build()}
   * if the file exists.
   *
   * @param overwrite overrite.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,append,org.apache.hadoop.fs.FSDataOutputStreamBuilder:append(),281,284,"/**
 * Appends a new element to the collection being built.
 */","* Append to an existing file (optional operation).
   *
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,hashCode,org.apache.hadoop.fs.permission.FsCreateModes:hashCode(),103,108,"/**
* Computes hash code by combining superclass and unmasked values.
* @return unique hash value representing this object's state.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],org.apache.hadoop.fs.StorageType[],long,long,boolean)",161,197,"/**
* Initializes BlockLocation object with provided data.
* @param names array of block names
* @param hosts array of host names
* @param cachedHosts array of cached host names
* @param topologyPaths array of topology paths
* @param storageIds array of storage IDs
* @param storageTypes array of storage types
* @param offset block offset in bytes
* @param length block length in bytes
* @param corrupt whether block is corrupt
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setHosts,org.apache.hadoop.fs.BlockLocation:setHosts(java.lang.String[]),312,318,"/**
* Sets the list of hosts to be used, interning strings for efficient storage.
* @param hosts array of host names or null to clear
*/","* Set the hosts hosting this block.
   * @param hosts hosts array.
   * @throws IOException If an I/O error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setCachedHosts,org.apache.hadoop.fs.BlockLocation:setCachedHosts(java.lang.String[]),324,330,"/**
* Sets the array of cached hosts to be interned and stored.
* @param cachedHosts array of host strings to cache
*/","* Set the hosts hosting a cached replica of this block.
   * @param cachedHosts cached hosts.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setNames,org.apache.hadoop.fs.BlockLocation:setNames(java.lang.String[]),337,343,"/**
* Sets array of user names. Interns strings to conserve memory.
* @param names array of name strings
*/","* Set the names (host:port) hosting this block.
   * @param names names.
   * @throws IOException If an I/O error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setTopologyPaths,org.apache.hadoop.fs.BlockLocation:setTopologyPaths(java.lang.String[]),351,357,"/**
* Sets the topology paths array.
* @param topologyPaths array of path strings or null to clear
*/","* Set the network topology paths of the hosts.
   *
   * @param topologyPaths topology paths.
   * @throws IOException If an I/O error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setStorageIds,org.apache.hadoop.fs.BlockLocation:setStorageIds(java.lang.String[]),359,365,"/**
* Sets the array of storage IDs, optionally interning them for efficient comparison.
* @param storageIds array of unique storage identifiers
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPathInternal,org.apache.hadoop.fs.shell.Command:processPathInternal(org.apache.hadoop.fs.shell.PathData),382,388,"/**
* Processes a single path recursively or not, depending on configuration.
* @param item PathData object to be processed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processPath,org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processPath(org.apache.hadoop.fs.shell.PathData),250,272,"/**
* Processes file system path by applying ACL operations based on command-line options.
* @param item PathData object containing file system information
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Stat.java,processPath,org.apache.hadoop.fs.shell.Stat:processPath(org.apache.hadoop.fs.shell.PathData),91,155,"/**
* Formats and prints file status information.
* @param item PathData containing file status info
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,isFile,org.apache.hadoop.fs.FileSystem:isFile(org.apache.hadoop.fs.Path),1895,1902,"/**
* Checks if a file exists at the specified path.
* @param f Path to the file to check
* @return true if the file exists, false otherwise
*/","True iff the named path is a regular file.
   * Note: Avoid using this method. Instead reuse the FileStatus
   * returned by {@link #getFileStatus(Path)} or listStatus() methods.
   *
   * @param f path to check
   * @throws IOException IO failure
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @return if f is file true, not false.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,isFile,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isFile(),479,482,"/**
* Checks if the file system entity represents a regular file.
* @return true if the entity is a file, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,isFile,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isFile(),55,58,"/**
* Checks if the underlying file system entity is a regular file.
* @return true if a file, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,toString,org.apache.hadoop.fs.FileStatus:toString(),458,488,"/**
* Returns a string representation of this file object.
* @return human-readable description as a String
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getSymlink,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getSymlink(),539,542,"/**
* Returns the symlink associated with this status.
* @return Path object representing the symlink or null if not applicable
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getSymlink,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getSymlink(),115,118,"/**
* Retrieves file system symlink.
* @return Path object representing the symlink or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,<init>,"org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type)",64,71,"/**
* Initializes FsServerDefaults object with specified parameters.
* @param blockSize block size in bytes
* @param bytesPerChecksum number of bytes per checksum
* @param writePacketSize packet size for writes
* @param replication replication factor
* @param fileBufferSize buffer size for files
* @param encryptDataTransfer whether to encrypt data transfer
* @param trashInterval interval for trash collection
* @param checksumType type of data checksum
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,<init>,"org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String)",73,80,"/**
* Initializes FsServerDefaults with given parameters.
* @param blockSize block size
* @param bytesPerChecksum checksum interval
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setXAttr,"org.apache.hadoop.fs.FilterFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",359,363,"/**
* Sets extended attribute with given name and value on specified file.
* @param path file path
* @param name attribute name
* @param value attribute value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathAccessDeniedException.java,<init>,"org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String,java.lang.Throwable)",28,30,"/**
* Constructs a PathAccessDeniedException with the specified path and cause.
* @param path URL or file path that caused access denial
* @param cause underlying exception causing access denial
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,"org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.Throwable)",30,32,"/**
* Constructs a PathPermissionException with the specified path and cause.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,"org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.Throwable)",30,32,"/**
* Constructs a PathNotFoundException with the specified path and cause.
* @param path the path that caused the exception
* @param cause the underlying error that led to this exception
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Concat.java,processArguments,org.apache.hadoop.fs.shell.Concat:processArguments(java.util.LinkedList),49,85,"/**
* Concatenates source files onto target file in the same filesystem.
* @param args list of source and target paths
* @throws IOException if arguments are invalid or operation fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,wrapException,"org.apache.hadoop.io.IOUtils:wrapException(java.lang.String,java.lang.String,java.io.IOException)",460,480,"/**
* Wraps an IOException with additional context.
* @param path file/directory path involved in the error
* @param methodName method name where the error occurred
* @return wrapped IOException or original if it's Interrupted/PathIOException
*/","* Takes an IOException, file/directory path, and method name and returns an
   * IOException with the input exception as the cause and also include the
   * file,method details. The new exception provides the stack trace of the
   * place where the exception is thrown and some extra diagnostics
   * information.
   *
   * Return instance of same exception if exception class has a public string
   * constructor; Otherwise return an PathIOException.
   * InterruptedIOException and PathIOException are returned unwrapped.
   *
   * @param path file/directory path
   * @param methodName method name
   * @param exception the caught exception.
   * @return an exception to throw",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sync,org.apache.hadoop.io.SequenceFile$Reader:sync(long),2831,2864,"/**
* Scans for synchronization marker at specified position.
* @param position offset to search for sync marker
*/","* Seek to the next sync mark past a given position.
     * @param position position.
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,reset,org.apache.hadoop.io.MapFile$Reader:reset(),638,640,"/**
* Resets file pointer to the first position.
*/","* Re-positions the reader before its first key.
     *
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.java,read,org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[]),82,85,"/**
* Reads data into given byte array.
* @param b byte array to store data in
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,registerExpressions,org.apache.hadoop.fs.shell.find.Find:registerExpressions(org.apache.hadoop.fs.shell.find.ExpressionFactory),102,106,"/**
* Registers all supported expression types with the given factory.
* @param factory ExpressionFactory instance to register expressions with
*/",Register the expressions with the expression factory.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,registerCommands,org.apache.hadoop.fs.shell.FsCommand:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),52,74,"/**
* Registers various file system commands with the CommandFactory.
* @param factory instance of CommandFactory to register commands with
*/","* Register the command classes used by the fs subcommand
   * @param factory where to register the class",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,registerCommands,org.apache.hadoop.fs.FsShell:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),111,118,"/**
* Registers FsCommand instances with the provided CommandFactory.
* @param factory CommandFactory instance to register commands with
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobExpander.java,expand,org.apache.hadoop.fs.GlobExpander:expand(java.lang.String),63,77,"/**
* Expands a file pattern into individual files.
* @param filePattern glob-style pattern
*/","* Expand globs in the given <code>filePattern</code> into a collection of
   * file patterns so that in the expanded set no file pattern has a slash
   * character (""/"") in a curly bracket pair.
   * <p>
   * Some examples of how the filePattern is expanded:<br>
   * <pre>
   * <b>
   * filePattern         - Expanded file pattern </b>
   * {a/b}               - a/b
   * /}{a/b}             - /}a/b
   * p{a/b,c/d}s         - pa/bs, pc/ds
   * {a/b,c/d,{e,f}}     - a/b, c/d, {e,f}
   * {a/b,c/d}{e,f}      - a/b{e,f}, c/d{e,f}
   * {a,b}/{b,{c/d,e/f}} - {a,b}/b, {a,b}/c/d, {a,b}/e/f
   * {a,b}/{c/\d}        - {a,b}/c/d
   * </pre>
   * 
   * @param filePattern file pattern.
   * @return expanded file patterns
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,fetchMore,org.apache.hadoop.fs.FileSystem$DirListingIterator:fetchMore(),2326,2330,"/**
* Fetches additional data from the server using the provided token.
* @param token authentication token
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,printXAttr,"org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:printXAttr(java.lang.String,byte[])",120,128,"/**
* Prints an x-attribute with a given name and value.
* @param name attribute name
* @param value encoded attribute value (null or empty for no output)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,"org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2119,2124,"/**
* Retrieves file statuses for the specified path and filter.
* @param f directory to scan
* @param filter optional filter to apply to results
* @return array of FileStatus objects or null on error
*/","* Filter files/directories in the given path using the user-supplied path
   * filter.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   *
   * @param f
   *          a path name
   * @param filter
   *          the user-supplied path filter
   * @return an array of FileStatus objects for the files under the given path
   *         after applying the filter
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,"org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)",2161,2168,"/**
* Lists status of multiple files based on a given filter.
* @param files array of paths to fetch status for
* @param filter optional filter to apply to the listing
* @return array of file statuses or empty if no files match
*/","* Filter files/directories in the given list of paths using user-supplied
   * path filter.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   *
   * @param files
   *          a list of paths
   * @param filter
   *          the user-supplied path filter
   * @return a list of statuses for the files under the given paths after
   *         applying the filter
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterInt.java,<init>,"org.apache.hadoop.metrics2.lib.MutableCounterInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",36,39,"/**
* Initializes counter with specified initial value and metrics information.
* @param info MetricsInfo object
* @param initValue Initial count value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterLong.java,<init>,"org.apache.hadoop.metrics2.lib.MutableCounterLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",37,40,"/**
* Initializes a MutableCounterLong instance with an initial value.
* @param info MetricsInfo object
* @param initValue Initial long value to add to the counter
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,<init>,"org.apache.hadoop.metrics2.lib.MutableGaugeLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",37,40,"/**
 * Initializes a mutable gauge with a given initial value.
 * @param info MetricsInfo object
 * @param initValue initial gauge value
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,<init>,"org.apache.hadoop.metrics2.lib.MutableGaugeFloat:<init>(org.apache.hadoop.metrics2.MetricsInfo,float)",33,36,"/**
* Initializes mutable gauge with initial value.
* @param info MetricsInfo object
* @param initValue initial float value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,<init>,"org.apache.hadoop.metrics2.lib.MutableGaugeInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",37,40,"/**
 * Initializes a mutable gauge integer with the given initial value.
 * @param initValue initial value of the gauge
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricCounterLong.java,<init>,"org.apache.hadoop.metrics2.impl.MetricCounterLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",29,32,"/**
 * Initializes a counter with the specified value and metrics information.
 * @param info Metrics details
 * @param value Long value to be counted
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeLong.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",29,32,"/**
* Initializes metric gauge with specified value and metrics info.
* @param info MetricsInfo object containing metric details
* @param value long value associated with the metric gauge
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricCounterInt.java,<init>,"org.apache.hadoop.metrics2.impl.MetricCounterInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",29,32,"/**
* Initializes Metric Counter with given Metrics Info and integer value.
* @param info Metrics information
* @param value Integer metric value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeFloat.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeFloat:<init>(org.apache.hadoop.metrics2.MetricsInfo,float)",29,32,"/**
* Initializes a metric gauge with a float value.
* @param info Metrics information
* @param value float value to display in gauge
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeDouble.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeDouble:<init>(org.apache.hadoop.metrics2.MetricsInfo,double)",29,32,"/**
* Initializes a gauge metric with a specified double value.
* @param info MetricsInfo object containing metric details
* @param value double value to be measured by the gauge",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeInt.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",29,32,"/**
 * Initializes metric gauge with integer value.
 * @param info Metrics information
 * @param value Integer value of the metric gauge
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",369,373,"/**
* Retrieves a delegation token.
* @param url URL to use for token retrieval
* @param token Initial token (may be null)
* @param renewer User or service that can renew the token
* @throws IOException If an I/O error occurs
* @throws AuthenticationException If authentication fails
*/","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @return a delegation token.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",416,419,"/**
* Renews an existing delegationToken.
* @param url service URL
* @param token existing delegationToken to renew
*/","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",457,460,"/**
* Cancels a delegated token.
* @param url URL of the token service
* @param token Token to be cancelled
*/","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @throws IOException if an IO error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,<init>,"org.apache.hadoop.crypto.key.kms.ValueQueue:<init>(int,float,long,int,org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller)",262,266,"/**
* Initializes a ValueQueue with specified parameters.
* @param numValues number of values in the queue
* @param lowWaterMark minimum value count for refill
* @param expiry time to live for each value
* @param numFillerThreads number of threads for refilling
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileEncryptionInfo.java,<init>,"org.apache.hadoop.fs.FileEncryptionInfo:<init>(org.apache.hadoop.crypto.CipherSuite,org.apache.hadoop.crypto.CryptoProtocolVersion,byte[],byte[],java.lang.String,java.lang.String)",57,74,"/**
* Initializes FileEncryptionInfo object with specified parameters.
* @param suite CipherSuite instance
* @param version CryptoProtocolVersion instance
* @param edek encrypted data encryption key byte array
* @param iv initialization vector byte array
* @param keyName string identifier for the encryption key
* @param ezKeyVersionName string identifier for EZ key version
*/","* Create a FileEncryptionInfo.
   *
   * @param suite CipherSuite used to encrypt the file
   * @param edek encrypted data encryption key (EDEK) of the file
   * @param iv initialization vector (IV) used to encrypt the file
   * @param keyName name of the key used for the encryption zone
   * @param ezKeyVersionName name of the KeyVersion used to encrypt the
   *                         encrypted data encryption key.
   * @param version version.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,getFS,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getFS(),106,109,"/**
 * Returns the file system instance.
 * @return The file system instance, guaranteed to be non-null.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,permission,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:permission(org.apache.hadoop.fs.permission.FsPermission),121,126,"/**
* Sets file system permission and returns this builder.
* @param perm FsPermission object to set
*/",* Set permission for the file.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,checksumOpt,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:checksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt),211,216,"/**
* Builds and returns this builder with checksum options applied.
* @param chksumOpt Checksum optimization settings
*/",* Set checksum opt.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WrappedIOException.java,<init>,org.apache.hadoop.fs.impl.WrappedIOException:<init>(java.io.IOException),47,49,"/**
 * Wraps an IOException instance with additional context.
 * @param cause underlying exception to be wrapped
 */","* Construct from a non-null IOException.
   * @param cause inner cause
   * @throws NullPointerException if the cause is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FsLinkResolution.java,<init>,org.apache.hadoop.fs.impl.FsLinkResolution:<init>(org.apache.hadoop.fs.impl.FsLinkResolution$FsLinkResolutionFunction),50,52,"/**
* Initializes FsLinkResolver with a given function.
* @param fn functional interface to resolve file system links
*/","* Construct an instance with the given function.
   * @param fn function to invoke.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,org.apache.hadoop.fs.Globber$GlobBuilder:<init>(org.apache.hadoop.fs.FileContext),437,440,"/**
 * Initializes the GlobBuilder with the provided FileContext.
 * @param fc the FileContext instance
 */","* Construct bonded to a file context.
     * @param fc file context.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,org.apache.hadoop.fs.Globber$GlobBuilder:<init>(org.apache.hadoop.fs.FileSystem),446,449,"/**
* Initializes a new GlobBuilder instance with the specified file system.
* @param fs the file system to use
*/","* Construct bonded to a filesystem.
     * @param fs file system.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,getFS,org.apache.hadoop.fs.FSDataOutputStreamBuilder:getFS(),141,144,"/**
* Returns the file system instance.
* @return Filesystem object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,permission,org.apache.hadoop.fs.FSDataOutputStreamBuilder:permission(org.apache.hadoop.fs.permission.FsPermission),159,163,"/**
* Sets the file system permissions and returns this builder.
* @param perm FsPermission object to set
*/","* Set permission for the file.
   *
   * @param perm permission.
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,progress,org.apache.hadoop.fs.FSDataOutputStreamBuilder:progress(org.apache.hadoop.util.Progressable),239,243,"/**
* Sets and returns the builder with the specified progress.
* @param prog the new progress to set
*/","* Set the facility of reporting progress.
   *
   * @param prog progress.
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,checksumOpt,org.apache.hadoop.fs.FSDataOutputStreamBuilder:checksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt),296,300,"/**
* Creates a new instance of B with specified checksum optimization.
* @param chksumOpt checksum optimization settings
*/","* Set checksum opt.
   *
   * @param chksumOpt check sum opt.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,validateWriteArgs,"org.apache.hadoop.fs.store.DataBlocks:validateWriteArgs(byte[],int,int)",110,118,"/**
* Validates write arguments for byte array operation.
* @param b non-null byte array
* @param off starting index, must be >= 0 and <= b.length
* @param len length to write, must be >= 0 and not exceed remaining bytes in b
*/","* Validate args to a write command. These are the same validation checks
   * expected for any implementation of {@code OutputStream.write()}.
   *
   * @param b   byte array containing data.
   * @param off offset in array where to start.
   * @param len number of bytes to be written.
   * @throws NullPointerException      for a null buffer
   * @throws IndexOutOfBoundsException if indices are out of range
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,set,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:set(java.lang.Object),219,224,"/**
* Sets the value and notifies waiting threads.
* @param v new value to be stored
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,getLowerLayerAsyncReturn,org.apache.hadoop.io.retry.AsyncCallHandler:getLowerLayerAsyncReturn(),78,83,"/**
* Retrieves and returns the lower layer async result, resetting the cache afterwards.
* @return The cached AsyncGet object or null if previously reset
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,setGcTimeMonitor,org.apache.hadoop.metrics2.source.JvmMetrics:setGcTimeMonitor(org.apache.hadoop.util.GcTimeMonitor),107,110,"/**
* Sets the GC time monitor instance.
* @param gcTimeMonitor the new monitor to use
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,init,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:init(byte[],byte[])",136,142,"/**
* Initializes encryption context with provided key and IV.
* @param key encryption key
* @param iv initialization vector
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,init,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:init(byte[],byte[])",128,138,"/**
* Initializes the cipher with a secret key and initialization vector.
* @param key secret key bytes
* @param iv initialization vector bytes
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,equalsIgnoreCase,"org.apache.hadoop.util.StringUtils:equalsIgnoreCase(java.lang.String,java.lang.String)",1259,1264,"/**
* Compares two strings for equality, ignoring case differences.
* @param s1 first string to compare (must not be null)
* @param s2 second string to compare
* @return true if strings are equal, false otherwise
*/","* Compare strings locale-freely by using String#equalsIgnoreCase.
   *
   * @param s1  Non-null string to be converted
   * @param s2  string to be converted
   * @return     the str, converted to uppercase.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LimitInputStream.java,<init>,"org.apache.hadoop.util.LimitInputStream:<init>(java.io.InputStream,long)",43,48,"/**
* Constructs a limited InputStream with specified capacity.
* @param in underlying input stream
* @param limit maximum bytes to read from the stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String[],java.lang.String)",432,439,"/**
* Initializes a deprecation delta object with the given key, new keys, and custom message.
* @param key unique identifier for the deprecation
* @param newKeys array of replacement keys
* @param customMessage optional user-provided message explaining the change
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,setReconfigurationUtil,org.apache.hadoop.conf.ReconfigurableBase:setReconfigurationUtil(org.apache.hadoop.conf.ReconfigurationUtil),88,91,"/**
* Sets the ReconfigurationUtil instance.
* @param ru non-null ReconfigurationUtil instance to be set
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,isStaleClient,org.apache.hadoop.ha.ActiveStandbyElector:isStaleClient(java.lang.Object),1173,1181,"/**
* Checks if the provided context is a stale ZooKeeper client.
* @param ctx object containing the ZooKeeper instance
* @return true if the client is stale, false otherwise
*/","* The callbacks and watchers pass a reference to the ZK client
   * which made the original call. We don't want to take action
   * based on any callbacks from prior clients after we quit
   * the election.
   * @param ctx the ZK client passed into the watcher
   * @return true if it matches the current client",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatistics,"org.apache.hadoop.fs.FileSystem:getStatistics(java.lang.String,java.lang.Class)",4586,4605,"/**
* Retrieves or initializes Statistics for the specified file system scheme.
* @param scheme unique file system identifier
* @param cls Class of the file system
* @return initialized Statistics object
*/","* Get the statistics for a particular file system.
   * @param scheme scheme.
   * @param cls the class to lookup
   * @return a statistics object
   * @deprecated use {@link #getGlobalStorageStatistics()}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeCompressedByteArray,"org.apache.hadoop.io.WritableUtils:writeCompressedByteArray(java.io.DataOutput,byte[])",61,83,"/**
 * Writes compressed byte array to output stream.
 * @param out DataOutput stream to write to
 * @param bytes Byte array to compress and write
 * @return Compression ratio (100% if no compression) or -1 if input is null
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,int,boolean)",64,81,"/**
* Copies bytes from input stream to output stream.
* @param in InputStream containing data
* @param out OutputStream to write data to
* @param buffSize buffer size for copying
* @param close whether to close streams after operation
*/","* Copies from one stream to another.
   *
   * @param in InputStrem to read from
   * @param out OutputStream to write to
   * @param buffSize the size of the buffer 
   * @param close whether or not close the InputStream and 
   * OutputStream at the end. The streams are closed in the finally clause.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,long,boolean)",145,175,"/**
* Copies bytes from input stream to output stream.
* @param in InputStream to read from
* @param out OutputStream to write to
* @param count total number of bytes to copy
* @param close whether to close streams after copying
*/","* Copies count bytes from one stream to another.
   *
   * @param in InputStream to read from
   * @param out OutputStream to write to
   * @param count number of bytes to copy
   * @param close whether to close the streams
   * @throws IOException if bytes can not be read or written",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,close,org.apache.hadoop.ipc.Client$IpcStreams:close(),1955,1959,"/**
 * Closes output and input streams to release system resources.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,<init>,org.apache.hadoop.util.VersionInfo:<init>(java.lang.String),41,55,"/**
* Loads component-specific version information from properties file.
* @param component name of the component (e.g. ""myComponent"")
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stopSinks,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSinks(),471,479,"/**
 * Stops and clears all registered metrics sinks.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,finalize,org.apache.hadoop.crypto.random.OsSecureRandom:finalize(),128,131,"/**
* Closes resources in the finalizer.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,doDiskIo,org.apache.hadoop.util.DiskChecker:doDiskIo(java.io.File),255,274,"/**
* Performs disk IO checks on the specified directory until successful or max iterations reached.
* @param dir directory to check
* @throws DiskErrorException if all checks fail
*/","* Performs some disk IO by writing to a new file in the given directory
   * and sync'ing file contents to disk.
   *
   * This increases the likelihood of catching catastrophic disk/controller
   * failures sooner.
   *
   * @param dir directory to be checked.
   * @throws DiskErrorException if we hit an error while trying to perform
   *         disk IO against the file.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,<init>,"org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,java.util.List)",44,46,"/**
 * Initializes PartialListing with given path and partial listing data.
 * @param listedPath Path to the listed entity
 * @param partialListing partial listing data
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,<init>,"org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.ipc.RemoteException)",48,50,"/**
* Constructs a partial listing with an exception.
* @param listedPath path to list
* @param exception remote exception",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Object),50,52,"/**
* Initializes call return state with given result and no error. 
* @param r result of the call
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Throwable),53,56,"/**
* Constructs a call return with an exception state and a non-null throwable. 
* @param result ignored, this call return represents an exception
* @param t the non-null throwable that caused the exception
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,org.apache.hadoop.io.retry.CallReturn:<init>(org.apache.hadoop.io.retry.CallReturn$State),57,59,"/**
* Initializes a new CallReturn instance from given State.
* @param s the State to initialize with",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:calculateIV(byte[],long,byte[])",66,70,"/**
* Calculates IV (Initialization Vector) using parent class's implementation.
* @param initIV initialization vector
* @param counter counter value
* @param iv calculated initialization vector
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:calculateIV(byte[],long,byte[])",52,56,"/**
* Calculates IV using provided parameters and cipher suite.
* @param initIV initialization vector
* @param counter counter value
* @param iv output IV to be calculated
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceSm4CtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:calculateIV(byte[],long,byte[])",48,52,"/**
* Calculates IV using parent class's implementation.
* @param initIV initial initialization vector
* @param counter counter value
* @param iv output IV to be calculated
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.JceAesCtrCryptoCodec:calculateIV(byte[],long,byte[])",48,52,"/**
* Calculates IV (Initialization Vector) based on provided parameters.
* @param initIV initial initialization vector
* @param counter incremental counter value
* @param iv resulting IV byte array
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,build,org.apache.hadoop.util.GcTimeMonitor$Builder:build(),95,98,"/**
* Constructs and returns a configured GcTimeMonitor instance.
* @param observationWindowMs time window for GC observations
* @param sleepIntervalMs interval between GC checks
* @param maxGcTimePercentage maximum allowed GC time percentage
* @param handler event handling callback
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,isTypeQuotaSet,org.apache.hadoop.fs.QuotaUsage:isTypeQuotaSet(),193,202,"/**
* Checks if type quota is set.
* @return true if any supported types have non-zero quota, false otherwise
*/","* Return true if any storage type quota has been set.
   *
   * @return if any storage type quota has been set true, not false.
   *",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,isTypeConsumedAvailable,org.apache.hadoop.fs.QuotaUsage:isTypeConsumedAvailable(),210,219,"/**
* Checks if type consumption is available.
* @return True if at least one supported storage type has consumed amount, false otherwise
*/","* Return true if any storage type consumption information is available.
   *
   * @return if any storage type consumption information
   * is available, not false.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,getAndCheckStorageTypes,org.apache.hadoop.fs.shell.Count:getAndCheckStorageTypes(java.lang.String),180,193,"/**
* Retrieves and validates storage types from a comma-separated string.
* @param types comma-separated list of storage types or ""all"" for all supported types
* @return List of validated StorageType objects
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,equals,org.apache.hadoop.fs.LocatedFileStatus:equals(java.lang.Object),190,193,"/**
* Calls superclass's equals() method to compare this object with another.
* @param o Object to be compared
* @return true if objects are equal, false otherwise
*/","Compare if this object is equal to another object
   * @param   o the object to be compared.
   * @return  true if two file status has the same path name; false if not.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,equals,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:equals(java.lang.Object),549,552,"/**
* Compares this Status with another Object for equality.
* @return true if both objects are equal (same status), false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,equals,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:equals(java.lang.Object),40,43,"/**
* Calls superclass's equals method to compare this object with another.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,hashCode,org.apache.hadoop.fs.LocatedFileStatus:hashCode(),201,204,"/**
* Returns hash code by delegating to superclass implementation.
*/","* Returns a hash code value for the object, which is defined as
   * the hash code of the path name.
   *
   * @return  a hash code value for the path name.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,hashCode,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:hashCode(),554,557,"/**
* Returns hash code based on underlying status object.
* @return Hash code of realStatus object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,hashCode,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:hashCode(),45,48,"/**
* Returns hash code based on superclass implementation.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,getFolderUsage,org.apache.hadoop.fs.DUHelper:getFolderUsage(java.lang.String),34,36,"/**
* Calculates total usage of a given folder.
* @param folder path to folder
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,clear,org.apache.hadoop.fs.statistics.MeanStatistic:clear(),147,149,"/**
 * Resets samples and sum to zero.
 */",* Set the values to 0.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,set,org.apache.hadoop.fs.statistics.MeanStatistic:set(org.apache.hadoop.fs.statistics.MeanStatistic),168,170,"/**
 * Sets mean statistic values from another MeanStatistic object.
 * @param other MeanStatistic instance to copy values from
 */","* Set the statistic to the values of another.
   * Synchronized.
   * @param other the source.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,ioStatisticsToString,org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToString(org.apache.hadoop.fs.statistics.IOStatistics),77,91,"/**
* Converts IOStatistics to a human-readable string.
* @param statistics optional IOStatistics object
*/","* Convert IOStatistics to a string form.
   * @param statistics A statistics instance.
   * @return string value or the empty string if null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,mapToSortedString,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:mapToSortedString(java.lang.StringBuilder,java.lang.String,java.util.Map,java.util.function.Predicate)",159,164,"/**
* Maps a sorted map to a string.
* @param sb StringBuilder instance
* @param type Type label (e.g., ""Users"")
* @param map Source map with key-value pairs
* @param isEmpty Predicate for empty values
*/","* Given a map, produce a string with all the values, sorted.
   * Needs to create a treemap and insert all the entries.
   * @param sb string buffer to append to
   * @param type type (for output)
   * @param map map to evaluate
   * @param <E> type of values of the map",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationTrackerFactory.java,trackDuration,org.apache.hadoop.fs.statistics.DurationTrackerFactory:trackDuration(java.lang.String),60,62,"/**
* Tracks duration with specified key and increments by one.
* @param key unique identifier for tracked duration
*/","* Initiate a duration tracking operation by creating/returning
   * an object whose {@code close()} call will
   * update the statistics.
   * The expected use is within a try-with-resources clause.
   * @param key statistic key
   * @return an object to close after an operation completes.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/PairedDurationTrackerFactory.java,trackDuration,"org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory:trackDuration(java.lang.String,long)",50,55,"/**
* Tracks duration for a given key with a specified count using both global and local trackers.
* @param key unique identifier for the tracked duration
* @param count number of times to track the duration
* @return DurationTracker object containing merged results from global and local trackers
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,getLongStatistics,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:getLongStatistics(),66,78,"/**
* Returns an iterator over a set of LongStatistics, including those from counters and gauges.
* @return Iterator over LongStatistics objects
*/","* Take a snapshot of the current counter values
   * and return an iterator over them.
   * @return all the counter statistics.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addTimedOperation(java.lang.String,long)",440,445,"/**
* Adds timed operation samples with mean, min, and max values.
* @param prefix unique identifier for the sample series
* @param durationMillis time duration in milliseconds to be measured
*/","* Add a duration to the min/mean/max statistics, using the
   * given prefix and adding a suffix for each specific value.
   * <p>
   * The update is non -atomic, even though each individual statistic
   * is updated thread-safely. If two threads update the values
   * simultaneously, at the end of each operation the state will
   * be correct. It is only during the sequence that the statistics
   * may be observably inconsistent.
   * </p>
   * @param prefix statistic prefix
   * @param durationMillis duration in milliseconds.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,build,org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:build(),51,56,"/**
* Builds and returns IO statistics object.
* @return IOSTatistics object or null if not found
*/","* Build the IOStatistics instance.
   * @return an instance.
   * @throws IllegalStateException if the builder has already been built.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionCounter(java.lang.String,java.util.function.ToLongFunction)",74,78,"/**
* Adds a long counter function to the statistics builder.
* @param key unique identifier for the counter
* @param eval function to evaluate as a long value
* @return this instance for method chaining
*/","* Add a new evaluator to the counter statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionGauge,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionGauge(java.lang.String,java.util.function.ToLongFunction)",125,129,"/**
* Adds long function gauge to DynIO statistics.
* @param key gauge identifier
* @param eval ToLongFunction to evaluate and store as gauge value
*/","* Add a new evaluator to the gauge statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionMinimum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionMinimum(java.lang.String,java.util.function.ToLongFunction)",163,167,"/**
* Adds minimum function to fetch long value from input string.
* @param key unique function identifier
* @param eval ToLongFunction instance for evaluation
*/","* Add a new evaluator to the minimum statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionMaximum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionMaximum(java.lang.String,java.util.function.ToLongFunction)",202,206,"/**
* Adds maximum function for long values based on given key and evaluation function.
* @param key unique key identifier
* @param eval function to evaluate as long value
*/","* Add a new evaluator to the maximum statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withMeanStatisticFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withMeanStatisticFunction(java.lang.String,java.util.function.Function)",242,246,"/**
* Adds mean statistic function to the builder.
* @param key unique identifier for the function
* @param eval functional interface for calculating mean statistics
*/","* Add a new evaluator to the mean statistics.
   *
   * This is a function which must return the mean and the sample count.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,register,org.apache.hadoop.service.launcher.InterruptEscalator:register(java.lang.String),142,146,"/**
* Registers an IRQ handler with the specified name.
* @param signalName unique identifier for the IRQ handler
*/","* Register an interrupt handler.
   * @param signalName signal name
   * @throws IllegalArgumentException if the registration failed",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,unreference,org.apache.hadoop.net.unix.DomainSocket:unreference(boolean),176,182,"/**
* Unreferences the channel reference, checking for closed status if requested.
* @param checkClosed whether to verify the channel is not closed before unreferring
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EvaluatingStatisticsMap.java,snapshot,org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:snapshot(),146,148,"/**
* Creates a snapshot of the current data structure.
* @return A map representation of the data or null if failed
*/","* Take a snapshot.
   * @return a map snapshot.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,snapshotMap,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:snapshotMap(java.util.Map),200,204,"/**
* Serializes and returns a map snapshot.
* @param source original map to serialize
*/","* Take a snapshot of a supplied map, where the copy option simply
   * uses the existing value.
   *
   * For this to be safe, the map must refer to immutable objects.
   * @param source source map
   * @param <E> type of values.
   * @return a new map referencing the same values.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EmptyIOStatisticsContextImpl.java,snapshot,org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:snapshot(),44,47,"/**
* Returns an empty IOStatisticsSnapshot instance.
*/","* Create a new empty snapshot.
   * A new one is always created for isolation.
   *
   * @return a statistics snapshot",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextImpl.java,<init>,"org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:<init>(long,long)",64,67,"/**
* Creates an IO statistics context instance with specified thread and ID.
* @param threadId unique identifier of the running thread
* @param id unique identifier of the data transfer operation
*/","* Constructor.
   * @param threadId thread ID
   * @param id instance ID.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSupport.java,snapshotIOStatistics,org.apache.hadoop.fs.statistics.IOStatisticsSupport:snapshotIOStatistics(),59,63,"/**
* Creates an empty IO statistics snapshot.
*/","* Create a snapshot statistics instance ready to aggregate data.
   *
   * The instance can be serialized, and its
   * {@code toString()} method lists all the values.
   * @return an empty snapshot",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_enabled,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_enabled(),279,281,"/**
* Checks if IO statistics context is enabled.
* @return true if enabled, false otherwise
*/","* Static probe to check if the thread-level IO statistics enabled.
   * @return true if the thread-level IO statistics are enabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/BufferedIOStatisticsOutputStream.java,getIOStatistics,org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:getIOStatistics(),86,89,"/**
 * Retrieves I/O statistics.
 * @return IOStatistics object containing relevant data.","* Ask the inner stream for their IOStatistics.
   * @return any IOStatistics offered by the inner stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/BufferedIOStatisticsInputStream.java,getIOStatistics,org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:getIOStatistics(),64,67,"/**
* Retrieves input I/O statistics. 
* @return InputIOStatistics object containing relevant data.","* Return any IOStatistics offered by the inner stream.
   * @return inner IOStatistics or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,getIOStatistics,org.apache.hadoop.fs.BufferedFSInputStream:getIOStatistics(),156,159,"/**
 * Retrieves input I/O statistics. 
 * @return IOStatistics object representing input I/O metrics.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getIOStatistics,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getIOStatistics(),315,318,"/**
* Retrieves I/O statistics.
* @return IOStatistics object containing current statistics
*/","* Get the IO Statistics of the nested stream, falling back to
     * null if the stream does not implement the interface
     * {@link IOStatisticsSource}.
     * @return an IOStatistics instance or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getIOStatistics,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:getIOStatistics(),676,679,"/**
* Retrieves I/O statistics from the underlying data source.
* @return IOStatistics object containing relevant metrics.","* Get the IO Statistics of the nested stream, falling back to
     * null if the stream does not implement the interface
     * {@link IOStatisticsSource}.
     * @return an IOStatistics instance or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,getIOStatistics,org.apache.hadoop.fs.FSDataInputStream:getIOStatistics(),289,292,"/**
* Retrieves I/O statistics.
* @return IOStatistics object containing I/O metrics
*/","* Get the IO Statistics of the nested stream, falling back to
   * null if the stream does not implement the interface
   * {@link IOStatisticsSource}.
   * @return an IOStatistics instance or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,getIOStatistics,org.apache.hadoop.fs.FSDataOutputStream:getIOStatistics(),167,170,"/**
* Retrieves input/output statistics from the wrapped stream.
* @return IOStatistics object containing I/O metrics or null if not available
*/","* Get the IO Statistics of the nested stream, falling back to
   * empty statistics if the stream does not implement the interface
   * {@link IOStatisticsSource}.
   * @return an IOStatistics instance.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionOutputStream.java,getIOStatistics,org.apache.hadoop.io.compress.CompressionOutputStream:getIOStatistics(),107,110,"/**
* Retrieves I/O statistics from the underlying output stream.
* @return IOStatistics object containing I/O metrics
*/","* Return any IOStatistics provided by the underlying stream.
   * @return IO stats from the inner stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionInputStream.java,getIOStatistics,org.apache.hadoop.io.compress.CompressionInputStream:getIOStatistics(),81,84,"/**
* Retrieves I/O statistics from the input stream.
* @return IOStatistics object or null if not available
*/","* Return any IOStatistics provided by the underlying stream.
   * @return IO stats from the inner stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getIOStatistics,org.apache.hadoop.crypto.CryptoInputStream:getIOStatistics(),882,885,"/**
 * Retrieves input/output statistics.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,getIOStatistics,org.apache.hadoop.crypto.CryptoOutputStream:getIOStatistics(),321,324,"/**
* Retrieves I/O statistics.
* @return IOStatistics object or null if not available
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,getIOStatistics,org.apache.hadoop.util.LineReader:getIOStatistics(),159,162,"/**
* Retrieves I/O statistics.
* @return I/OStatistics object","* Return any IOStatistics provided by the source.
   * @return IO stats from the input stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,getIOStatistics,org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:getIOStatistics(),351,354,"/**
* Retrieves I/O statistics.
* @return IOStatistics object representing current I/O metrics.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,getIOStatistics,org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:getIOStatistics(),405,408,"/**
 * Retrieves and returns input/output statistics.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,getIOStatistics,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:getIOStatistics(),449,452,"/**
 * Retrieves I/O statistics.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,verifyChunked,"org.apache.hadoop.util.DataChecksum:verifyChunked(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,java.nio.ByteBuffer,int,java.nio.ByteBuffer,java.lang.String,long)",429,472,"/**
* Verifies chunked checksum for the given data and CRCs.
* @param type Type of verification (e.g. file)
* @param algorithm Checksum algorithm used
* @param data Data to verify
* @param bytesPerCrc Size of each CRC block
* @param crcs Array of expected CRC values
* @param filename Name of the verified entity
* @param basePos Base position of the verification data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,verifyChunked,"org.apache.hadoop.util.DataChecksum:verifyChunked(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,byte[],int,int,int,byte[],int,java.lang.String,long)",478,512,"/**
* Verifies chunked checksums for a given file.
* @param type Type of checksum (e.g. MD5, SHA-1)
* @param algorithm Checksum algorithm instance
* @param data File data
* @param dataOffset Offset into data where verification starts
* @param dataLength Length of data to verify
* @param bytesPerCrc Number of bytes per CRC value
* @param crcs Array of expected CRC values
* @param crcsOffset Offset into crcs array where verification starts
* @param filename Name of file being verified
* @param basePos Base position in file (for error reporting)
*/","* Implementation of chunked verification specifically on byte arrays. This
   * is to avoid the copy when dealing with ByteBuffers that have array backing.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,updateDecryptor,"org.apache.hadoop.crypto.CryptoInputStream:updateDecryptor(org.apache.hadoop.crypto.Decryptor,long,byte[])",297,302,"/**
* Updates the Decryptor instance with new IV and key.
* @param decryptor Decryptor instance to update
* @param position current position (used to generate counter)
* @param iv new initialization vector
*/","Calculate the counter and iv, update the decryptor.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,encrypt,org.apache.hadoop.crypto.CryptoOutputStream:encrypt(),178,217,"/**
* Encrypts buffered data using provided encryptor.
* @throws IOException on write operation failure
*/","* Do the encryption, input is {@link #inBuffer} and output is 
   * {@link #outBuffer}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BatchedRemoteIterator.java,hasNext,org.apache.hadoop.fs.BatchedRemoteIterator:hasNext(),98,102,"/**
* Checks if there are more entries available.
* @return true if entries exist, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BatchedRemoteIterator.java,next,org.apache.hadoop.fs.BatchedRemoteIterator:next(),111,120,"/**
* Retrieves the next object from an iterator, throwing exception if exhausted.
* @return Object of type E or throws IOException
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32GzipFileChecksum.java,<init>,org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:<init>(),27,29,"/**
* Initializes an empty MD5MD5CRC32GzipFileChecksum object.","Same as this(0, 0, null)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32CastagnoliFileChecksum.java,<init>,org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:<init>(),27,29,"/**
* Constructs an instance of MD5MD5CRC32CastagnoliFileChecksum with default values.
*/","Same as this(0, 0, null)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobPattern.java,<init>,org.apache.hadoop.fs.GlobPattern:<init>(java.lang.String),41,43,"/**
* Initializes a new GlobPattern instance with the given pattern.
* @param globPattern regular expression pattern string
*/","* Construct the glob pattern object with a glob pattern string
   * @param globPattern the glob pattern string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,makeShellPath,org.apache.hadoop.fs.FileUtil:makeShellPath(java.io.File),668,670,"/**
* Constructs shell-compatible path from a File object.
* @param file input file
*/","* Convert a os-native filename to a path that works for the shell.
   * @param file The filename to convert
   * @return The unix pathname
   * @throws IOException on windows, there can be problems with the subprocess",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,makeSecureShellPath,org.apache.hadoop.fs.FileUtil:makeSecureShellPath(java.io.File),679,686,"/**
* Creates a secure SSH path from the given file.
* @param file input file
* @return SSH path or throws exception if not supported on Windows
*/","* Convert a os-native filename to a path that works for the shell
   * and avoids script injection attacks.
   * @param file The filename to convert
   * @return The unix pathname
   * @throws IOException on windows, there can be problems with the subprocess",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HardLink.java,linkCount,org.apache.hadoop.fs.HardLink$HardLinkCGUnix:linkCount(java.io.File),108,116,"/**
* Creates an array of shell commands to count links for a given file.
* @param file target file
* @return array of command strings or throws IOException if error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unZip,"org.apache.hadoop.fs.FileUtil:unZip(java.io.InputStream,java.io.File)",739,775,"/**
* Extracts and writes zip archive contents to the specified directory.
* @param inputStream input stream containing the zip archive
* @param toDir target directory where files will be extracted
*/","* Given a stream input it will unzip the it in the unzip directory.
   * passed as the second parameter
   * @param inputStream The zip file as input
   * @param toDir The unzip directory where to unzip the zip file.
   * @throws IOException an exception occurred",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unZip,"org.apache.hadoop.fs.FileUtil:unZip(java.io.File,java.io.File)",827,871,"/**
* Unzips a file to a specified directory.
* @param inFile Zip archive file
* @param unzipDir target directory for unzipped files
* @throws IOException if any I/O error occurs during unzipping
*/","* Given a File input it will unzip it in the unzip directory.
   * passed as the second parameter
   * @param inFile The zip file as input
   * @param unzipDir The unzip directory where to unzip the zip file.
   * @throws IOException An I/O exception has occurred",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingJava,"org.apache.hadoop.fs.FileUtil:unTarUsingJava(java.io.File,java.io.File,boolean)",1086,1109,"/**
* Extracts tar archive to specified directory.
* @param inFile input file containing the tar archive
* @param untarDir target directory for extraction
* @param gzipped whether the archive is gzip-compressed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingJava,"org.apache.hadoop.fs.FileUtil:unTarUsingJava(java.io.InputStream,java.io.File,boolean)",1111,1128,"/**
* Extracts Tar archive from InputStream to specified directory.
* @param inputStream stream containing the compressed archive
* @param untarDir target directory for extracted files
* @param gzipped true if input is GZip or not
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/PowerShellFencer.java,tryFence,"org.apache.hadoop.ha.PowerShellFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",57,105,"/**
* Attempts to fence a process on the specified HAServiceTarget using PowerShell.
* @param target HAServiceTarget containing remote service details
* @param argsStr name of the process to be fenced as a string
* @return true if fencing was successful, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,toString,org.apache.hadoop.fs.permission.FsCreateModes:toString(),82,86,"/**
* Formats a string representation of this object.
* @return Formatted string with masked and unmasked values
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,disconnect,org.apache.hadoop.fs.sftp.SFTPFileSystem:disconnect(com.jcraft.jsch.ChannelSftp),165,167,"/**
* Disconnects an SFTP channel from the pool.
* @param channel active SFTP channel to be disconnected
*/","* Logout and disconnect the given channel.
   *
   * @param client
   * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java,shutdown,org.apache.hadoop.fs.sftp.SFTPConnectionPool:shutdown(),87,113,"/**
* Shuts down the system by disconnecting all connections and releasing resources.
* @throws IOException if an error occurs while closing a connection
*/",Shutdown the connection pool and close all open connections.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,<init>,"org.apache.hadoop.fs.FSDataOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics)",82,84,"/**
* Constructs an FSDataOutputStream instance.
* @param out OutputStream to write data to
* @param stats File system statistics
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,<init>,org.apache.hadoop.fs.FSOutputSummer:<init>(org.apache.hadoop.util.DataChecksum),53,58,"/**
* Initializes FSOutputSummer with a DataChecksum.
* @param sum DataChecksum object containing checksum data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,setChecksumBufSize,org.apache.hadoop.fs.FSOutputSummer:setChecksumBufSize(int),257,261,"/**
* Sets the buffer size for checksum calculation.
* @param size new size in bytes
*/","* Resets existing buffer with a new one of the specified size.
   *
   * @param size size.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,read,org.apache.hadoop.fs.sftp.SFTPInputStream:read(),108,124,"/**
* Reads a single byte from the underlying stream.
* @return the read byte value or -1 at EOF
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPInputStream.java,read,org.apache.hadoop.fs.ftp.FTPInputStream:read(),70,84,"/**
* Reads one byte from the underlying stream, updating position and statistics.
* @return the read byte value or -1 for end of stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPInputStream.java,read,"org.apache.hadoop.fs.ftp.FTPInputStream:read(byte[],int,int)",86,101,"/**
* Reads bytes from the underlying stream into a buffer.
* @param buf buffer to read into
* @param off offset within the buffer to start reading at
* @param len number of bytes to read
* @return number of bytes read, or -1 on end-of-stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,read,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(),217,231,"/**
* Reads a single byte from the underlying file stream.
* @return The read byte value or -1 if end of stream is reached
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,read,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(byte[],int,int)",233,249,"/**
* Reads data from the underlying file system into a byte array.
* @param b target byte array
* @param off starting offset in byte array
* @param len number of bytes to read
* @return actual number of bytes read or -1 on error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,read,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(long,byte[],int,int)",251,272,"/**
* Reads data from underlying stream into given byte array.
* @param position offset in the file
* @param b buffer to read into
* @param off starting index of buffer
* @param len number of bytes to read
* @return actual number of bytes read or 0 if end of stream reached
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,write,org.apache.hadoop.fs.FSDataOutputStream$PositionCache:write(int),51,58,"/**
* Writes a single byte to the output stream and updates internal state.
* @param b the byte to be written
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,write,"org.apache.hadoop.fs.FSDataOutputStream$PositionCache:write(byte[],int,int)",60,67,"/**
* Writes bytes to output stream and updates position.
* @param b byte array to write
* @param off starting offset in the array
* @param len number of bytes to write
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,org.apache.hadoop.fs.FileSystem$Statistics:<init>(org.apache.hadoop.fs.FileSystem$Statistics),4110,4125,"/**
* Copies statistics from another instance, aggregating all thread-specific data.
* @param other source Statistics object
*/","* Copy constructor.
     *
     * @param other    The input Statistics object which is cloned.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesRead,org.apache.hadoop.fs.FileSystem$Statistics:getBytesRead(),4308,4321,"/**
* Aggregates total bytes read across all statistics.
* @return Total bytes read in long value
*/","* Get the total number of bytes read.
     * @return the number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesWritten,org.apache.hadoop.fs.FileSystem$Statistics:getBytesWritten(),4327,4340,"/**
* Aggregates total bytes written across all statistics.
* @return Total bytes written in long value
*/","* Get the total number of bytes written.
     * @return the number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getReadOps,org.apache.hadoop.fs.FileSystem$Statistics:getReadOps(),4346,4360,"/**
* Accumulates total read operations from all statistics.
*/","* Get the number of file system read operations such as list files.
     * @return number of read operations",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getLargeReadOps,org.apache.hadoop.fs.FileSystem$Statistics:getLargeReadOps(),4367,4380,"/**
* Aggregates large read operations across all statistics.
* @return Total large read operations count
*/","* Get the number of large file system read operations such as list files
     * under a large directory.
     * @return number of large read operations",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getWriteOps,org.apache.hadoop.fs.FileSystem$Statistics:getWriteOps(),4387,4400,"/**
* Calculates total write operations across all nodes.
*@return Total number of write operations
*/","* Get the number of file system write operations such as create, append
     * rename etc.
     * @return number of write operations",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getRemoteReadTime,org.apache.hadoop.fs.FileSystem$Statistics:getRemoteReadTime(),4436,4449,"/**
* Accumulates total remote read time (in ms).
* @return Total remote read time in milliseconds
*/","* Get total time taken in ms for bytes read from remote.
     * @return time taken in ms for remote bytes read.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getData,org.apache.hadoop.fs.FileSystem$Statistics:getData(),4456,4469,"/**
* Aggregates statistics by visiting and accumulating all data.
* @return aggregated StatisticsData object
*/","* Get all statistics data.
     * MR or other frameworks can use the method to get all statistics at once.
     * @return the StatisticsData",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesReadErasureCoded,org.apache.hadoop.fs.FileSystem$Statistics:getBytesReadErasureCoded(),4475,4488,"/**
* Aggregates total bytes read for erasure-coded statistics.
* @return Total bytes read or 0 if none found
*/","* Get the total number of bytes read on erasure-coded files.
     * @return the number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,toString,org.apache.hadoop.fs.FileSystem$Statistics:toString(),4490,4504,"/**
* Aggregates statistics from all items and returns a string representation.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,reset,org.apache.hadoop.fs.FileSystem$Statistics:reset(),4524,4539,"/**
* Accumulates and negates statistics across all nodes, then adds to root node.
*/","* Resets all statistics to 0.
     *
     * In order to reset, we add up all the thread-local statistics data, and
     * set rootData to the negative of that.
     *
     * This may seem like a counterintuitive way to reset the statistics.  Why
     * can't we just zero out all the thread-local data?  Well, thread-local
     * data can only be modified by the thread that owns it.  If we tried to
     * modify the thread-local data from this thread, our modification might get
     * interleaved with a read-modify-write operation done by the thread that
     * owns the data.  That would result in our update getting lost.
     *
     * The approach used here avoids this problem because it only ever reads
     * (not writes) the thread-local data.  Both reads and writes to rootData
     * are done under the lock, so we're free to modify rootData from any thread
     * that holds the lock.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,toString,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:toString(),113,116,"/**
* Returns a string representation of this algorithm instance.
* @return Unique identifier in format ""algorithm_name:md5_hash""
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CreateFlag.java,validate,"org.apache.hadoop.fs.CreateFlag:validate(java.lang.Object,boolean,java.util.EnumSet)",172,187,"/**
* Validates a file path with given flags.
* @param path the file path to validate
* @param pathExists whether the file exists
* @param flag create flags (CREATE, APPEND, OVERWRITE)
*/","* Validate the CreateFlag for create operation
   * @param path Object representing the path; usually String or {@link Path}
   * @param pathExists pass true if the path exists in the file system
   * @param flag set of CreateFlag
   * @throws IOException on error
   * @throws HadoopIllegalArgumentException if the CreateFlag is invalid",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CreateFlag.java,validateForAppend,org.apache.hadoop.fs.CreateFlag:validateForAppend(java.util.EnumSet),195,201,"/**
* Validates whether the provided flags allow appending.
* @param flag EnumSet of CreateFlags, checked for APPEND permission
*/","* Validate the CreateFlag for the append operation. The flag must contain
   * APPEND, and cannot contain OVERWRITE.
   *
   * @param flag enum set flag.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getUri,"org.apache.hadoop.fs.AbstractFileSystem:getUri(java.net.URI,java.lang.String,boolean,int)",316,341,"/**
* Constructs a URI with validated scheme and authority.
* @param uri input URI
* @param supportedScheme allowed scheme for this URI
* @param authorityNeeded whether authority is required
* @param defaultPort default port to use if not specified in URI
* @return constructed URI or null if invalid
*/","* Get the URI for the file system based on the given URI. The path, query
   * part of the given URI is stripped out and default file system port is used
   * to form the URI.
   * 
   * @param uri FileSystem URI.
   * @param authorityNeeded if true authority cannot be null in the URI. If
   *          false authority must be null.
   * @param defaultPort default port to use if port is not specified in the URI.
   * 
   * @return URI of the file system
   * 
   * @throws URISyntaxException <code>uri</code> has syntax error",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecodeImpl,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecodeImpl(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",85,96,"/**
* Performs decoding using Reed-Solomon algorithm.
* @param inputs array of input ByteBuffers
* @param erasedIndexes indices of erased data in inputs
* @param outputs array to store decoded output ByteBuffers
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayEncodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,byte[][],byte[][])",36,50,"/**
* Initializes ByteArrayEncodingState with encoder and input/output arrays.
* @param encoder RawErasureEncoder instance
* @param inputs array of byte arrays to encode
* @param outputs array of byte arrays for encoded data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferEncodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,java.nio.ByteBuffer[],java.nio.ByteBuffer[])",35,47,"/**
* Initializes ByteBufferEncodingState with encoder and input/output buffers.
* @param encoder RawErasureEncoder instance
* @param inputs array of input ByteBuffers
* @param outputs array of output ByteBuffers
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable:<init>(java.lang.Class),113,116,"/**
* Creates an instance of ArrayPrimitiveWritable with specified primitive type.
* @param componentType primitive type (e.g. Integer, Float, etc.)
*/","* Construct an instance of known type but no value yet
   * for use with type-specific wrapper classes.
   *
   * @param componentType componentType.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,set,org.apache.hadoop.io.ArrayPrimitiveWritable:set(java.lang.Object),142,150,"/**
* Sets the array's type and value to the provided object.
* @param value the array object to set
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/DefaultFailoverProxyProvider.java,close,org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:close(),55,58,"/**
* Stops and closes the RPC proxy.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:close(),49,52,"/**
 * Terminates the RPC proxy connection.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:close(),54,57,"/**
 * Stops and closes the RPC proxy instance.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:close(),52,55,"/**
 * Stops and releases resources associated with the RPC proxy.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:close(),49,52,"/**
 * Stops and releases system resources associated with the RPC proxy.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:close(),47,50,"/**
 * Terminates the RPC proxy service.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:close(),72,75,"/**
* Stops the RPC proxy.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:close(),165,168,"/**
* Stops and closes the RPC proxy connection.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,getPermFromString,org.apache.hadoop.util.ZKUtil:getPermFromString(java.lang.String),44,71,"/**
* Parses a string into a permissions integer.
* @param permString string representation of permissions (e.g. 'rw')
* @return bit mask of allowed operations or -1 if invalid
*/","* Parse ACL permission string, partially borrowed from
   * ZooKeeperMain private method",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,processChecksumOpt,"org.apache.hadoop.fs.Options$ChecksumOpt:processChecksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt,org.apache.hadoop.fs.Options$ChecksumOpt)",339,342,"/**
* Merges default and user checksum options.
* @param defaultOpt default checksum options
* @param userOpt user-provided checksum options
*/","* A helper method for processing user input and default value to 
     * create a combined checksum option. 
     *
     * @param defaultOpt Default checksum option
     * @param userOpt User-specified checksum option
     *
     * @return ChecksumOpt.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getUriDefaultPort,org.apache.hadoop.fs.DelegateToFileSystem:getUriDefaultPort(),174,177,"/**
* Retrieves the default port number for URI connections.
* @return The default port (or -1 if not defined)",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,canonicalizeUri,org.apache.hadoop.fs.HarFileSystem:canonicalizeUri(java.net.URI),323,326,"/**
* Canonicalizes a given URI.
* @param uri input URI to be normalized
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getCanonicalUri,org.apache.hadoop.fs.FileSystem:getCanonicalUri(),383,385,"/**
* Returns the canonicalized URI.
* @return Canonicalized URI instance
*/","* Return a canonicalized form of this FileSystem's URI.
   *
   * The default implementation simply calls {@link #canonicalizeUri(URI)}
   * on the filesystem's own URI, so subclasses typically only need to
   * implement that method.
   *
   * @see #canonicalizeUri(URI)
   * @return the URI of this filesystem.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,canonicalizeUri,org.apache.hadoop.fs.FilterFileSystem:canonicalizeUri(java.net.URI),118,121,"/**
* Resolves URI to its canonical form.
* @param uri input URI to be resolved
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,"org.apache.hadoop.fs.ContentSummary:<init>(long,long,long)",162,165,"/**
* Constructs a ContentSummary object with provided statistics.
* @param length total content length in bytes
* @param fileCount number of files
* @param directoryCount number of directories
*/","*  Constructor, deprecated by ContentSummary.Builder
   *  This constructor implicitly set spaceConsumed the same as length.
   *  spaceConsumed and length must be set explicitly with
   *  ContentSummary.Builder.
   *
   * @param length length.
   * @param fileCount file count.
   * @param directoryCount directory count.
   *",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,toString,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:toString(),160,166,"/**
* Returns a human-readable string representation of the token renewal state.
* @return descriptive string or null if no token is available
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,<init>,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:<init>(org.apache.hadoop.fs.FileSystem),71,75,"/**
* Initializes and updates renewal action with provided file system.
* @param fs file system object to wrap in a weak reference
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getStatus,org.apache.hadoop.fs.HarFileSystem:getStatus(org.apache.hadoop.fs.Path),283,286,"/**
* Delegates file system status retrieval to underlying FS implementation.
* @param p path to query for status information
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFsStatus,org.apache.hadoop.fs.DelegateToFileSystem:getFsStatus(org.apache.hadoop.fs.Path),153,156,"/**
* Retrieves file system status for given path.
* @param f the path to fetch status for
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatus,org.apache.hadoop.fs.FileSystem:getStatus(),3026,3028,"/**
* Retrieves file system status.
*/","* Returns a status object describing the use and capacity of the
   * filesystem. If the filesystem has multiple partitions, the
   * use and capacity of the root partition is reflected.
   *
   * @return a FsStatus object
   * @throws IOException
   *           see specific implementation",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getStatus,org.apache.hadoop.fs.FilterFileSystem:getStatus(org.apache.hadoop.fs.Path),329,332,"/**
* Delegates file system status retrieval to underlying storage.
* @param p the path to query
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/CombinedFileRange.java,toString,org.apache.hadoop.fs.impl.CombinedFileRange:toString(),91,96,"/**
* Returns a string representation of this object.
* Includes superclass details and custom metrics (range count, data size).
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,<init>,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:<init>(org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder),151,180,"/**
* Initializes an HttpReferrerAuditHeader object from a builder.
* @param builder Builder containing audit header data
*/","* Instantiate.
   * <p>
   * All maps/enums passed down are copied into thread safe equivalents.
   * as their origin is unknown and cannot be guaranteed to
   * not be shared.
   * <p>
   * Context and operationId are expected to be well formed
   * numeric/hex strings, at least adequate to be
   * used as individual path elements in a URL.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,<init>,"org.apache.hadoop.fs.impl.WeakReferenceThreadMap:<init>(java.util.function.Function,java.util.function.Consumer)",36,39,"/**
* Constructs a thread-local map using the given factory and reference lost callback.
* @param factory function to create new values
* @param referenceLost callback when a value is garbage collected
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,hasCapability,org.apache.hadoop.fs.FSDataOutputStream:hasCapability(java.lang.String),128,131,"/**
* Checks if the wrapped stream has a specified capability.
* @param capability capability to check for
* @return true if capability is present, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,hasCapability,org.apache.hadoop.crypto.CryptoOutputStream:hasCapability(java.lang.String),316,319,"/**
* Checks if user has specified capability.
* @param capability unique capability identifier
* @return true if capability exists, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,hasCapability,org.apache.hadoop.fs.FSDataInputStream:hasCapability(java.lang.String),242,245,"/**
* Checks if user has specified capability.
* @param capability name of capability to check for
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getPrefetched,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getPrefetched(int),168,172,"/**
* Retrieves prefetched operation by block number.
* @param blockNumber block identifier
* @return Operation object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getCached,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getCached(int),174,178,"/**
* Retrieves cached operation by block number.
* @param blockNumber unique block identifier
* @return Operation object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getRead,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getRead(int),180,184,"/**
* Retrieves an Operation instance for reading at specified block number.
* @param blockNumber positive block index
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,release,org.apache.hadoop.fs.impl.prefetch.BlockOperations:release(int),186,190,"/**
* Releases an operation at the specified block number.
* @param blockNumber unique block identifier
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,requestPrefetch,org.apache.hadoop.fs.impl.prefetch.BlockOperations:requestPrefetch(int),192,196,"/**
* Requests prefetch of blockchain data for specified block.
* @param blockNumber unique block identifier
* @return the requested Operation object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,prefetch,org.apache.hadoop.fs.impl.prefetch.BlockOperations:prefetch(int),198,202,"/**
* Prefetches an operation at a specified block number.
* @param blockNumber the target block number
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,cancelPrefetches,org.apache.hadoop.fs.impl.prefetch.BlockOperations:cancelPrefetches(),204,206,"/**
* Cancels pending prefetch operations.
* @return new Operation object representing cancellation
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,close,org.apache.hadoop.fs.impl.prefetch.BlockOperations:close(),208,210,"/**
* Creates an operation to close this operation.
* @return new Operation object representing the closure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,requestCaching,org.apache.hadoop.fs.impl.prefetch.BlockOperations:requestCaching(int),212,216,"/**
* Issues an operation to cache data at specified blockchain block number.
* @param blockNumber the target blockchain block number
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,addToCache,org.apache.hadoop.fs.impl.prefetch.BlockOperations:addToCache(int),218,222,"/**
* Adds operation to cache with specified block number.
* @param blockNumber unique block identifier
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,end,org.apache.hadoop.fs.impl.prefetch.BlockOperations:end(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation),224,226,"/**
* Ends an operation and returns its result.
* @param op the operation to be ended
* @return the resulting Operation object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,fromSummary,org.apache.hadoop.fs.impl.prefetch.BlockOperations:fromSummary(java.lang.String),377,424,"/**
* Parses summary string into BlockOperations.
* @param summary string containing operations separated by semicolons
* @return BlockOperations object representing parsed operations
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,release,org.apache.hadoop.fs.impl.prefetch.BufferPool:release(org.apache.hadoop.fs.impl.prefetch.BufferData),236,255,"/**
* Releases a BufferData object, returning it to the pool.
* @param data BufferData object to be released
*/","* Releases a previously acquired resource.
   * @param data the {@code BufferData} instance to release.
   * @throws IllegalArgumentException if data is null.
   * @throws IllegalArgumentException if data cannot be released due to its state.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,toString,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:toString(),153,158,"/**
* Returns a human-readable string representation of the queue's state.
* @return String containing queue metrics (size, created, in-queue, available)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getDurationInfo,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getDurationInfo(java.lang.StringBuilder),252,295,"/**
* Appends duration information for each operation kind to the StringBuilder.
* @param sb StringBuilder to append to
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,createCache,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:createCache(int,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",554,556,"/**
* Creates a block cache instance with specified maximum block count and tracking factory.
* @param maxBlocksCount maximum number of blocks in the cache
* @param trackerFactory factory for creating duration trackers
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,<init>,"org.apache.hadoop.util.SemaphoredDelegatingExecutor:<init>(java.util.concurrent.ExecutorService,int,boolean)",90,95,"/**
* Constructs SemaphoredDelegatingExecutor with specified parameters.
* @param executorDelegatee delegatee ExecutorService instance
* @param permitCount maximum number of concurrent executions
* @param fair whether to use fair scheduling policy
*/","* Instantiate without collecting executor aquisition duration information.
   * @param executorDelegatee Executor to delegate to
   * @param permitCount number of permits into the queue permitted
   * @param fair should the semaphore be ""fair""",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,getEntry,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getEntry(int),297,307,"/**
* Retrieves an Entry object from the cache by its block number.
* @param blockNumber unique identifier of the block to retrieve
* @return cached Entry object or throws exception if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,releaseReadyBlock,org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseReadyBlock(int),205,224,"/**
* Releases the nearest ready BufferData block.
* @param blockNumber target distance
*/","* If no blocks were released after calling releaseDoneBlocks() a few times,
   * we may end up waiting forever. To avoid that situation, we try releasing
   * a 'ready' block farthest away from the given block.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,toString,org.apache.hadoop.fs.impl.prefetch.BufferPool:toString(),278,292,"/**
* Returns a formatted string representation of the pool and its data.
* @return A multi-line string containing pool info and sorted data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,buffer,org.apache.hadoop.fs.impl.prefetch.FilePosition:buffer(),130,133,"/**
* Returns the underlying buffer.
* @return The internal buffer instance.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,data,org.apache.hadoop.fs.impl.prefetch.FilePosition:data(),135,138,"/**
* Returns buffer data.
* @return The BufferData object associated with this buffer.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,relative,org.apache.hadoop.fs.impl.prefetch.FilePosition:relative(),172,175,"/**
* Returns the current position within the buffer.
* @return Current position in bytes.","* Gets the current position within this file relative to the start of the associated buffer.
   *
   * @return the current position within this file relative to the start of the associated buffer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,isWithinCurrentBuffer,org.apache.hadoop.fs.impl.prefetch.FilePosition:isWithinCurrentBuffer(long),183,187,"/**
* Checks if a file position is within the current buffer.
* @param pos file position to check
* @return true if position is within buffer, false otherwise
*/","* Determines whether the given absolute position lies within the current buffer.
   *
   * @param pos the position to check.
   * @return true if the given absolute position lies within the current buffer, false otherwise.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,bufferStartOffset,org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferStartOffset(),231,234,"/**
* Returns the start offset of the current buffer.
* @return The start offset in bytes
*/","* Gets the start of the current block's absolute offset.
   *
   * @return the start of the current block's absolute offset.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextIntegration.java,getCurrentIOStatisticsContext,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:getCurrentIOStatisticsContext(),122,126,"/**
* Retrieves current I/O statistics context based on thread configuration.
* @return Active or empty I/O stats context depending on thread settings.","* Get the current thread's IOStatisticsContext instance. If no instance is
   * present for this thread ID, create one using the factory.
   * @return instance of IOStatisticsContext.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextIntegration.java,setThreadIOStatisticsContext,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:setThreadIOStatisticsContext(org.apache.hadoop.fs.statistics.IOStatisticsContext),133,145,"/**
* Sets IO statistics context for current thread.
* @param statisticsContext IOStatisticsContext object to set or null to clear
*/","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,mergeSortedRanges,"org.apache.hadoop.fs.VectoredReadUtils:mergeSortedRanges(java.util.List,int,int,int)",380,398,"/**
* Merges sorted file ranges into combined chunks.
* @param sortedRanges list of file ranges to merge
* @param chunkSize size of each merged chunk
* @return List of CombinedFileRange objects representing merged chunks
*/","* Merge sorted ranges to optimize the access from the underlying file
   * system.
   * The motivations are that:
   * <ul>
   *   <li>Upper layers want to pass down logical file ranges.</li>
   *   <li>Fewer reads have better performance.</li>
   *   <li>Applications want callbacks as ranges are read.</li>
   *   <li>Some file systems want to round ranges to be at checksum boundaries.</li>
   * </ul>
   *
   * @param sortedRanges already sorted list of ranges based on offset.
   * @param chunkSize round the start and end points to multiples of chunkSize
   * @param minimumSeek the smallest gap that we should seek over in bytes
   * @param maxSize the largest combined file range in bytes
   * @return the list of sorted CombinedFileRanges that cover the input",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,findChecksumRanges,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:findChecksumRanges(java.util.List,int,int,int)",344,362,"/**
* Merges checksum ranges from input data into combined ranges.
* @param dataRanges list of file ranges to process
* @param bytesPerSum size of each checksum chunk
* @return List of CombinedFileRange objects representing merged checksums
*/","* Find the checksum ranges that correspond to the given data ranges.
     * @param dataRanges the input data ranges, which are assumed to be sorted
     *                   and non-overlapping
     * @return a list of AsyncReaderUtils.CombinedFileRange that correspond to
     *         the checksum ranges",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,<init>,org.apache.hadoop.fs.shell.find.Name:<init>(),48,50,"/**
* Initializes a new instance of Name with default settings.
*/",Creates a case sensitive name expression.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,subset,org.apache.hadoop.metrics2.impl.MetricsConfig:subset(java.lang.String),144,147,"/**
* Creates a subset of metrics with the given prefix.
* @param prefix filter prefix to apply to metric names
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/And.java,apply,"org.apache.hadoop.fs.shell.find.And:apply(org.apache.hadoop.fs.shell.PathData,int)",57,68,"/**
* Recursively traverses and combines results of child expressions.
* @param item PathData object to evaluate
* @param depth Current recursion level (not used in this implementation)
* @return Result object representing overall evaluation outcome
*/","* Applies child expressions to the {@link PathData} item. If all pass then
   * returns {@link Result#PASS} else returns the result of the first
   * non-passing expression.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,getOptions,org.apache.hadoop.fs.shell.find.Find:getOptions(),235,241,"/**
* Retrieves and caches FindOptions instance.
* @return cached or freshly created FindOptions object
*/","Returns the current find options, creating them if necessary.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,parse,org.apache.hadoop.fs.shell.CommandFormat:parse(java.util.List),99,140,"/**
* Parses command-line arguments and options.
* @param args list of argument strings
*/","Parse parameters from the given list of args.  The list is
   *  destructively modified to remove the options.
   * 
   * @param args as a list of input arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,getDescription,org.apache.hadoop.fs.shell.Command:getDescription(),547,551,"/**
* Retrieves command description, or deprecation notice if deprecated.
*/","* The long usage suitable for help output
   * @return text of the usage",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,displayWarning,org.apache.hadoop.fs.shell.Command:displayWarning(java.lang.String),510,512,"/**
* Displays a warning message to the console.
* @param message user-visible warning text
*/","* Display an warning string prefaced with the command name.
   * @param message warning message to display",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,getUsage,org.apache.hadoop.fs.shell.Command:getUsage(),537,541,"/**
* Returns command usage string, optionally including deprecated notice.
* @return Command usage string or deprecation note
*/","* The short usage suitable for the synopsis
   * @return ""name options""",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource,java.lang.Iterable,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,long,boolean)",71,88,"/**
* Initializes MetricsSourceAdapter with configuration and dependencies.
* @param prefix metric prefix
* @param name metric name
* @param description metric description
* @param source metrics source
* @param injectedTags additional tags to include
* @param recordFilter filter for records
* @param metricFilter filter for metrics
* @param jmxCacheTTL JMX cache time-to-live (seconds)
* @param startMBeans whether to start MBean creation immediately
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsCollectorImpl.java,addRecord,org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:addRecord(org.apache.hadoop.metrics2.MetricsInfo),42,50,"/**
* Adds a metrics record to the builder with optional filtering.
* @param info metrics information
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,<init>,org.apache.hadoop.util.ChunkedArrayList:<init>(),94,96,"/**
* Initializes a new ChunkedArrayList instance with default chunk capacity and size.
* @param initialChunkCapacity initial number of elements per chunk
* @param maxChunkSize maximum allowed chunk size
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ScopedAclEntries.java,<init>,org.apache.hadoop.fs.permission.ScopedAclEntries:<init>(java.util.List),47,57,"/**
* Segregates ACL entries into 'access' and 'default' categories.
* @param aclEntries list of AclEntry objects to process
*/","* Creates a new ScopedAclEntries from the given list.  It is assumed that the
   * list is already sorted such that all access entries precede all default
   * entries.
   *
   * @param aclEntries List&lt;AclEntry&gt; to separate",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,printToStream,org.apache.hadoop.fs.shell.FsUsage$TableBuilder:printToStream(java.io.PrintStream),312,334,"/**
* Prints formatted table to specified PrintStream.
* @param out target output stream
*/","* Render the table to a stream.
     * @param out PrintStream for output",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,moved,org.apache.hadoop.fs.Options$HandleOpt:moved(boolean),432,434,"/**
* Creates a new Location instance based on the provided flag.
* @param allow boolean indicating whether to create a new location or not
*/","* @param allow If true, resolve references to this entity anywhere in
     *              the namespace.
     * @return Handle option encoding parameter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,changed,org.apache.hadoop.fs.Options$HandleOpt:changed(boolean),423,425,"/**
* Creates a new Data instance based on the provided boolean flag.
* @param allow true to enable data modification, false otherwise
*/","* @param allow If true, resolve references to this entity even if it has
     *             been modified.
     * @return Handle option encoding parameter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,<init>,"org.apache.hadoop.fs.DF:<init>(java.io.File,long)",54,59,"/**
* Initializes a new Data Fetcher instance with the specified file path and data interval.
* @param path directory containing the data files
* @param dfInterval time interval for data fetching (in milliseconds)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell:<init>(),901,903,"/**
* Default constructor with default timeout value.
* @param timeout default connection timeout in milliseconds","* Create an instance with no minimum interval between runs; stderr is
   * not merged with stdout.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,<init>,"org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:<init>(org.apache.hadoop.fs.CachingGetSpaceUsed,boolean)",204,207,"/**
* Initializes thread to refresh space used cache.
* @param spaceUsed CachingGetSpaceUsed object
* @param runImmediately flag to execute immediately
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,"org.apache.hadoop.security.token.Token$PrivateToken:<init>(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",255,263,"/**
* Clones a private token from a public token.
* @param publicToken the public token to clone
* @param newService the service associated with the cloned token
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,generateDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:generateDelegationToken(org.apache.hadoop.security.token.Token),1144,1153,"/**
* Creates a delegationToken for authentication.
* @param dToken delegate token to generate from
*/","* Generate a DelegationTokenAuthenticatedURL.Token from the given generic
   * typed delegation token.
   *
   * @param dToken The delegation token.
   * @return The DelegationTokenAuthenticatedURL.Token, with its delegation
   *         token set to the delegation token passed in.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:listLocatedStatus(org.apache.hadoop.fs.Path),847,852,"/**
* Lists located file statuses in a given directory.
* @param f Path to the directory
* @return Iterator of LocatedFileStatus objects or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listLocatedStatus,org.apache.hadoop.fs.FilterFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),284,288,"/**
* Lists located file statuses for the given file.
* @param f Path to the file
* @return iterator over LocatedFileStatus objects
*/",List files and its block locations in a directory.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setVerifyChecksum,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setVerifyChecksum(boolean),1368,1372,"/**
* Sets verification of checksums during mount operations.
* @param verifyChecksum true to enable, false to disable
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFileChecksum,org.apache.hadoop.fs.DelegateToFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),124,128,"/**
* Retrieves file checksum using the underlying implementation.
* @param f Path to the file
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileChecksum,org.apache.hadoop.fs.FilterFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),502,505,"/**
* Retrieves file checksum using the underlying filesystem.
* @param f Path to the file
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processPath,org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processPath(org.apache.hadoop.fs.shell.PathData),179,186,"/**
* Updates or removes an extended attribute on a file path.
* @param item PathData object containing file system and path information
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setXAttr,"org.apache.hadoop.fs.FilterFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",623,627,"/**
* Sets extended attribute on file system object.
* @param path file system path
* @param name attribute name
* @param value attribute value as a byte array
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.DelegateToFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",282,285,"/**
* Opens a file with specified options asynchronously.
* @param path file system path
* @param parameters open file parameters
* @return CompletableFuture containing FSDataInputStream or null if failed
*/","* Open a file by delegating to
   * {@link FileSystem#openFileWithOptions(Path, org.apache.hadoop.fs.impl.OpenFileParameters)}.
   * @param path path to the file
   * @param parameters open file parameters from the builder.
   *
   * @return a future which will evaluate to the opened file.ControlAlpha
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FilterFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",721,726,"/**
* Opens file with options using underlying FS API.
* @param path file location
* @param parameters open file options
* @return CompletableFuture for FSDataInputStream or null if failed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,openFileWithOptions,"org.apache.hadoop.fs.FilterFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",444,449,"/**
* Opens file with specified options asynchronously.
* @param path file location
* @param parameters open file settings
* @return FSDataInputStream or null if failed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FilterFileSystem:openFileWithOptions(org.apache.hadoop.fs.PathHandle,org.apache.hadoop.fs.impl.OpenFileParameters)",728,733,"/**
* Opens a file with specified options using the underlying file system.
* @param pathHandle unique file identifier
* @param parameters file opening parameters
* @return FSDataInputStream instance for reading or null on failure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDirLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)",249,252,"/**
* Constructs an instance of INodeDirLink with specified parameters.
* @param pathToNode directory path
* @param aUgi UserGroupInformation object
* @param link INodeLink object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,addDir,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addDir(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",211,220,"/**
* Adds directory with specified path component and UGI.
* @param pathComponent directory name
* @param aUgi user group information for the directory
* @return newly created INodeDir object or null on failure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getChildFileSystems,org.apache.hadoop.fs.viewfs.ViewFileSystem:getChildFileSystems(),1035,1058,"/**
* Retrieves an array of child FileSystems.
*@return Array of child FileSystems, or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getFallbackFileSystem,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getFallbackFileSystem(),400,411,"/**
* Retrieves a fallback file system if root fallback link is configured.
*@return FileSystem object or null if not configured
*/","* @return Gets the fallback file system configured. Usually, this will be the
   * default cluster.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,addCall,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:addCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall),118,124,"/**
* Adds an asynchronous call to the processing queue and triggers its start.
* @param call AsyncCall object representing the task to be executed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,update,"org.apache.hadoop.crypto.OpensslCipher:update(java.nio.ByteBuffer,java.nio.ByteBuffer)",231,241,"/**
* Updates the context with data from a source ByteBuffer and writes to a destination ByteBuffer.
* @param input source ByteBuffer containing data
* @param output destination ByteBuffer for written data
* @return number of bytes updated or -1 on error
*/","* Continues a multiple-part encryption or decryption operation. The data
   * is encrypted or decrypted, depending on how this cipher was initialized.
   * <p>
   * 
   * All <code>input.remaining()</code> bytes starting at 
   * <code>input.position()</code> are processed. The result is stored in
   * the output buffer.
   * <p>
   * 
   * Upon return, the input buffer's position will be equal to its limit;
   * its limit will not have changed. The output buffer's position will have
   * advanced by n, when n is the value returned by this method; the output
   * buffer's limit will not have changed.
   * <p>
   * 
   * If <code>output.remaining()</code> bytes are insufficient to hold the
   * result, a <code>ShortBufferException</code> is thrown.
   * 
   * @param input the input ByteBuffer
   * @param output the output ByteBuffer
   * @return int number of bytes stored in <code>output</code>
   * @throws ShortBufferException if there is insufficient space in the
   * output buffer",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,doFinal,org.apache.hadoop.crypto.OpensslCipher:doFinal(java.nio.ByteBuffer),270,277,"/**
* Encrypts and writes final data to the provided direct ByteBuffer.
* @param output ByteBuffer containing space for encrypted output
* @return length of written data or throws an exception if failed
*/","* Finishes a multiple-part operation. The data is encrypted or decrypted,
   * depending on how this cipher was initialized.
   * <p>
   * The result is stored in the output buffer. Upon return, the output buffer's
   * position will have advanced by n, where n is the value returned by this
   * method; the output buffer's limit will not have changed.
   * </p>
   * If <code>output.remaining()</code> bytes are insufficient to hold the result,
   * a <code>ShortBufferException</code> is thrown.
   * <p>
   * Upon finishing, this method resets this cipher object to the state it was
   * in when previously initialized. That is, the object is available to encrypt
   * or decrypt more data.
   * </p>
   * If any exception is thrown, this cipher object need to be reset before it
   * can be used again.
   *
   * @param output the output ByteBuffer
   * @return int number of bytes stored in <code>output</code>
   * @throws ShortBufferException      if there is insufficient space in the output buffer.
   * @throws IllegalBlockSizeException This exception is thrown when the length
   *                                   of data provided to a block cipher is incorrect.
   * @throws BadPaddingException       This exception is thrown when a particular
   *                                   padding mechanism is expected for the input
   *                                   data but the data is not padded properly.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPointInterceptorFactory.java,create,org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorFactory:create(java.lang.String),41,66,"/**
* Creates a RegexMountPointInterceptor instance from a settings string.
* @param interceptorSettingsString configuration data as a string
* @return interceptor object or null on invalid input
*/","* interceptorSettingsString string should be like ${type}:${string},
   * e.g. replaceresolveddstpath:word1,word2.
   *
   * @param interceptorSettingsString
   * @return Return interceptor based on setting or null on bad/unknown config.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,toString,org.apache.hadoop.fs.DF:toString(),129,139,"/**
* Returns a string representation of the disk usage statistics.
* @return formatted string with capacity, used, available, and percent used details
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,normalizePath,"org.apache.hadoop.fs.Path:normalizePath(java.lang.String,java.lang.String)",297,318,"/**
* Normalizes a URI path by removing duplicated slashes and converting backslashes to forward slashes.
* @param scheme the URI scheme (e.g., ""http"", null for file paths)
* @param path the URI path to normalize
* @return the normalized path string
*/","* Normalize a path string to use non-duplicated forward slashes as
   * the path separator and remove any trailing path separators.
   *
   * @param scheme the URI scheme. Used to deduce whether we
   * should replace backslashes or not
   * @param path the scheme-specific part
   * @return the normalized path string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isWindowsAbsolutePath,"org.apache.hadoop.fs.Path:isWindowsAbsolutePath(java.lang.String,boolean)",341,348,"/**
* Checks if a given path string is an absolute Windows path.
* @param pathString the input path string to check
* @param slashed whether the path uses forward slashes
*/","* Determine whether a given path string represents an absolute path on
   * Windows. e.g. ""C:/a/b"" is an absolute path. ""C:a/b"" is not.
   *
   * @param pathString the path string to evaluate
   * @param slashed true if the given path is prefixed with ""/""
   * @return true if the supplied path looks like an absolute path with a Windows
   * drive-specifier",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isUriPathAbsolute,org.apache.hadoop.fs.Path:isUriPathAbsolute(),388,391,"/**
* Checks if URI path is absolute.
* @return true if path starts with a separator, false otherwise
*/","* Returns true if the path component (i.e. directory) of this URI is
   * absolute.
   *
   * @return whether this URI's path is absolute",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getHarHash,org.apache.hadoop.fs.HarFileSystem:getHarHash(org.apache.hadoop.fs.Path),488,490,"/**
* Calculates hash value of file path.
* @param p Path object to hash
*/","* the hash of the path p inside  the filesystem
   * @param p the path in the harfilesystem
   * @return the hash code of the path.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,build,org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:build(),4694,4714,"/**
* Creates a file system data output stream.
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FileStatus),108,135,"/**
* Converts a Hadoop file status object into a protocol buffer representation.
* @param stat Hadoop file status to convert
* @return FileStatusProto object or null if conversion fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,<init>,"org.apache.hadoop.fs.sftp.SFTPInputStream:<init>(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem$Statistics)",46,58,"/**
* Initializes an SFTP input stream for the specified file.
* @param channel SFTP channel connection
* @param path Path to the remote file
* @param stats Optional statistics object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkPath,org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPath(org.apache.hadoop.fs.Path),69,73,"/**
* Validates that the provided path is a sub-path of the base path.
*@param path the path to validate
*/","* Validate a path.
   * @param path path to check.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Stat.java,getExecString,org.apache.hadoop.fs.Stat:getExecString(),91,108,"/**
* Generates stat command string based on OS and dereference flag.
* @return Array of strings containing stat command and arguments
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFileSystem:readOnlyMountTable(java.lang.String,org.apache.hadoop.fs.Path)",103,106,"/**
* Convenience wrapper to convert path to string and invoke read-only mount table logic.
* @param operation not used (legacy parameter)
* @param p the file system path
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFs:readOnlyMountTable(java.lang.String,org.apache.hadoop.fs.Path)",181,184,"/**
* Convenience wrapper to convert path to string before calling read-only mount table.
* @param operation unused operation parameter
* @param p the file system path as a path object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,getPathToResolve,"org.apache.hadoop.fs.viewfs.RegexMountPoint:getPathToResolve(java.lang.String,boolean)",217,227,"/**
* Resolves a file system path to its parent directory.
* @param srcPath input file path
* @param resolveLastComponent whether to include the last component in the resolved path
* @return parent directory path or null if invalid input
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,checkDependencies,"org.apache.hadoop.fs.FileContext:checkDependencies(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2284,2298,"/**
* Validates dependencies between source and destination paths.
* @param qualSrc qualified source path
* @param qualDst qualified destination path
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compareTo,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:compareTo(java.lang.Object),3794,3805,"/**
* Compares SegmentDescriptor objects based on segment length, offset, and path name.
* @param o object to compare with this descriptor
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:equals(java.lang.Object),3807,3820,"/**
* Checks equality of two SegmentDescriptors based on segment length, offset and path name.
* @param o the object to compare with
* @return true if equal, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getNextIdToTry,"org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getNextIdToTry(org.apache.hadoop.fs.Path,int)",731,753,"/**
* Retrieves the next available unique identifier by scanning files in the specified directory.
* @param initial base path to search
* @param lastId last known unique identifier
* @return next available unique identifier
*/","* Return the next ID suffix to use when creating the log file. This method
   * will look at the files in the directory, find the one with the highest
   * ID suffix, and 1 to that suffix, and return it. This approach saves a full
   * linear probe, which matters in the case where there are a large number of
   * log files.
   *
   * @param initial the base file path
   * @param lastId the last ID value that was used
   * @return the next ID to try
   * @throws IOException thrown if there's an issue querying the files in the
   * directory",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,getPathAsString,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getPathAsString(),144,146,"/**
* Converts path to string representation.
* @return String representation of the current path
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createServiceURL,org.apache.hadoop.crypto.key.kms.KMSClientProvider:createServiceURL(org.apache.hadoop.fs.Path),447,453,"/**
* Constructs service URL by appending version to provided path.
* @param path directory path
* @return fully qualified service URL or null if invalid
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,seek,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seek(long),313,319,"/**
* Seeks to a specified position within the file.
* @param pos target position in bytes
* @throws IOException if seeking beyond end of file
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,skip,org.apache.hadoop.fs.FSInputChecker:skip(long),405,413,"/**
* Skips ahead by specified number of bytes in the underlying stream.
* @param n number of bytes to skip
* @return number of bytes skipped, or 0 if invalid position
* @throws IOException if an I/O error occurs during seek operation
*/","* Skips over and discards <code>n</code> bytes of data from the
   * input stream.
   *
   * <p>This method may skip more bytes than are remaining in the backing
   * file. This produces no exception and the number of bytes skipped
   * may include some number of bytes that were beyond the EOF of the
   * backing file. Attempting to read from the stream after skipping past
   * the end will result in -1 indicating the end of the file.
   *
   *<p>If <code>n</code> is negative, no bytes are skipped.
   *
   * @param      n   the number of bytes to be skipped.
   * @return     the actual number of bytes skipped.
   * @exception  IOException  if an I/O error occurs.
   *             ChecksumException if the chunk to skip to is corrupted",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ByteBufferUtil.java,fallbackRead,"org.apache.hadoop.fs.ByteBufferUtil:fallbackRead(java.io.InputStream,org.apache.hadoop.io.ByteBufferPool,int)",57,117,"/**
* Reads from an InputStream using a ByteBuffer from a pool, 
* with optional direct access.
* @param stream InputStream to read from
* @param bufferPool ByteBufferPool to obtain and reuse buffers from
* @param maxLength maximum number of bytes to read
* @return the used ByteBuffer or null if an error occurred
*/","* Perform a fallback read.
   *
   * @param stream input stream.
   * @param bufferPool bufferPool.
   * @param maxLength maxLength.
   * @throws IOException raised on errors performing I/O.
   * @return byte buffer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,reset,org.apache.hadoop.fs.audit.CommonAuditContext:reset(),185,188,"/**
* Resets and reinitializes the evaluation state.
* Clears existing entries and restarts initialization process.","* Rest the context; will set the standard options again.
   * Primarily for testing.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,createInstance,org.apache.hadoop.fs.audit.CommonAuditContext:createInstance(),212,216,"/**
* Creates and initializes a new instance of CommonAuditContext. 
*/","* Demand invoked to create the instance for this thread.
   * @return an instance.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,toString,org.apache.hadoop.tools.TableListing:toString(),229,292,"/**
* Formats table data into a human-readable string representation.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,"org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction)",82,84,"/**
* Constructs an FsPermission object with specified actions.
* @param u owner's permissions
* @param g group's permissions
* @param o other's permissions
*/","* Construct by the given {@link FsAction}.
   * @param u user action
   * @param g group action
   * @param o other action",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission:<init>(short),95,95,"/**
* Constructs FsPermission object from short mode value.
* @param mode short integer representing file system permissions.","* Construct by the given mode.
   * @param mode mode.
   * @see #toShort()",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,readFields,org.apache.hadoop.fs.permission.FsPermission:readFields(java.io.DataInput),185,189,"/**
* Reads user profile data fields from input stream.
* @throws IOException if I/O operation fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,read,org.apache.hadoop.fs.permission.FsPermission:read(java.io.DataInput),214,218,"/**
* Creates FsPermission object from input stream.
* @param in DataInput stream containing permission data
* @return FsPermission object representing file permissions
*/","* Create and initialize a {@link FsPermission} from {@link DataInput}.
   *
   * @param in data input.
   * @throws IOException raised on errors performing I/O.
   * @return FsPermission.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclStatus.java,getEffectivePermission,org.apache.hadoop.fs.permission.AclStatus:getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry),230,232,"/**
* Retrieves the effective file system action based on ACL entry and permissions.
* @param entry AclEntry object containing access control information
*/","* Get the effective permission for the AclEntry
   * @param entry AclEntry to get the effective action
   * @return FsAction.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,createImmutable,"org.apache.hadoop.fs.permission.PermissionStatus:createImmutable(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",49,57,"/**
* Creates an immutable PermissionStatus instance with specified details.
* @param user  user identifier (immutable)
* @param group group identifier (immutable)
* @param permission file system permissions (immutable)
*/","* Create an immutable {@link PermissionStatus} object.
   * @param user user.
   * @param group group.
   * @param permission permission.
   * @return PermissionStatus.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,parseAclSpec,"org.apache.hadoop.fs.permission.AclEntry:parseAclSpec(java.lang.String,boolean)",235,245,"/**
* Parses ACL specification into a list of AclEntry objects.
* @param aclSpec comma-separated ACL string
* @param includePermission whether to include permission details in AclEntry
*/","* Parses a string representation of an ACL spec into a list of AclEntry
   * objects. Example: ""user::rwx,user:foo:rw-,group::r--,other::---""
   * The expected format of ACL entries in the string parameter is the same
   * format produced by the {@link #toStringStable()} method.
   * 
   * @param aclSpec
   *          String representation of an ACL spec.
   * @param includePermission
   *          for setAcl operations this will be true. i.e. AclSpec should
   *          include permissions.<br>
   *          But for removeAcl operation it will be false. i.e. AclSpec should
   *          not contain permissions.<br>
   *          Example: ""user:foo,group:bar""
   * @return Returns list of {@link AclEntry} parsed",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,create,"org.apache.hadoop.fs.permission.FsCreateModes:create(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",58,63,"/**
* Creates a new FsCreateModes instance from given permission modes.
* @param masked the permission mode to mask
* @param unmasked the unmasked permission mode
*/","* Create from masked and unmasked modes.
   *
   * @param masked masked.
   * @param unmasked unmasked.
   * @return FsCreateModes.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,printExtendedAclEntry,"org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:printExtendedAclEntry(org.apache.hadoop.fs.permission.AclStatus,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.AclEntry)",137,152,"/**
* Prints extended ACL entry details.
* @param aclStatus AclStatus object
* @param fsPerm FsPermission object
* @param entry AclEntry object to print
*/","* Prints a single extended ACL entry.  If the mask restricts the
     * permissions of the entry, then also prints the restricted version as the
     * effective permissions.  The mask applies to all named entries and also
     * the unnamed group entry.
     * @param aclStatus AclStatus for the path
     * @param fsPerm FsPermission for the path
     * @param entry AclEntry extended ACL entry to print",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,toString,org.apache.hadoop.fs.permission.AclEntry:toString(),102,108,"/**
* Returns a stable string representation of this object.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getStrings,org.apache.hadoop.util.StringUtils:getStrings(java.lang.String),407,410,"/**
* Splits input string into array of substrings using comma as delimiter.
* @param str input string to be split
*/","* Returns an arraylist of strings.
   * @param str the comma separated string values
   * @return the arraylist of the comma separated string values",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/UmaskParser.java,<init>,org.apache.hadoop.fs.permission.UmaskParser:<init>(java.lang.String),41,45,"/**
* Initializes UmaskParser instance from a string representation of the mode.
* @param modeStr String containing mode value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/RawParser.java,<init>,org.apache.hadoop.fs.permission.RawParser:<init>(java.lang.String),35,38,"/**
* Initializes RawParser with specified mode.
* @param modeStr string representation of mode
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ChmodParser.java,<init>,org.apache.hadoop.fs.permission.ChmodParser:<init>(java.lang.String),38,40,"/**
 * Initializes ChmodParser with a string representing file permissions.
 * @param modeStr string describing file permissions (e.g., ""755"" or ""u+x"")
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,create,"org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",530,535,"/**
* Creates a DataBlock with specified limit and upload statistics.
* @param index unused unique identifier (always 0)
* @param limit maximum block size
* @param statistics BlockUploadStatistics object
* @return created DataBlock or throws IOException if error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,available,org.apache.hadoop.fs.store.ByteBufferInputStream:available(),116,120,"/**
* Returns the number of bytes remaining in the buffer.
* @return Remaining byte count or -1 if closed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,position,org.apache.hadoop.fs.store.ByteBufferInputStream:position(),126,129,"/**
* Returns current buffer position.
* @return Position within the buffer as an integer.","* Get the current buffer position.
   * @return the buffer position",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,hasRemaining,org.apache.hadoop.fs.store.ByteBufferInputStream:hasRemaining(),135,138,"/**
* Checks if there are remaining bytes in the buffer.
* @return true if buffer has remaining bytes, false otherwise
*/","* Check if there is data left.
   * @return true if there is data remaining in the buffer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,reset,org.apache.hadoop.fs.store.ByteBufferInputStream:reset(),147,152,"/**
* Resets internal state to its initial condition.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$DataBlock:startUpload(),454,458,"/**
* Initiates data block upload process and transitions state.
* @throws IOException on upload failure
*/","* Switch to the upload state and return a stream for uploading.
     * Base class calls {@link #enterState(DestState, DestState)} to
     * manage the state machine.
     *
     * @return the stream.
     * @throws IOException trouble",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,enterClosedState,org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterClosedState(),466,473,"/**
* Enters closed state if current state is not already Closed.
* @return true if successfully transitioned to Closed state, false otherwise
*/","* Enter the closed state.
     *
     * @return true if the class was in any other state, implying that
     * the subclass should do its close operations.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$DiskBlock:write(byte[],int,int)",878,885,"/**
* Writes a portion of the given byte array to the output stream.
* @param b  byte array data
* @param offset starting index in the array
* @param len length of data to write
* @return number of bytes actually written
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:write(byte[],int,int)",747,753,"/**
* Writes data to the underlying output stream and updates block buffer.
* @param b byte array to write
* @param offset starting index in the array
* @param len number of bytes to write
* @return actual number of bytes written
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,flush,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:flush(),940,943,"/**
 * Flushes buffered output to underlying stream. 
 * Calls superclass and PrintWriter flush methods.
 */","* Flush operation will flush to disk.
     *
     * @throws IOException IOE raised on FileOutputStream",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:write(byte[],int,int)",612,618,"/**
* Writes up to 'len' bytes from the given byte array into the internal buffer.
* @param b byte array containing data to write
* @param offset starting index in 'b' to read from
* @param len number of bytes to write (clamped to available capacity)
* @return actual number of bytes written
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,toString,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:toString(),767,776,"/**
* Returns a human-readable string representation of the ByteBufferBlock.
* @return String containing key-value pairs for index, state, data size, buffer size, and remaining capacity.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getStatistics,org.apache.hadoop.fs.FileContext:getStatistics(java.net.URI),2396,2398,"/**
* Retrieves file system statistics for a given URI.
* @param uri identifier of the file system resource
*/","* Get the statistics for a particular file system
   * 
   * @param uri
   *          the uri to lookup the statistics. Only scheme and authority part
   *          of the uri are used as the key to store and lookup.
   * @return a statistics object",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createMultipartUploader,org.apache.hadoop.fs.FilterFs:createMultipartUploader(org.apache.hadoop.fs.Path),457,461,"/**
* Creates an MultipartUploader instance with the given base path. 
* @param basePath base directory for multipart uploads
* @return MultipartUploaderBuilder instance or null if creation fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getCurrentDirectoryIndex,org.apache.hadoop.fs.LocalDirAllocator:getCurrentDirectoryIndex(),257,260,"/**
* Retrieves current directory index from context.
* @return Current directory index
*/","* Get the current directory index for the given configuration item.
   * @return the current directory index for the given configuration item.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,getPos,org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:getPos(),49,52,"/**
 * Retrieves the current file position from the output stream. 
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sync,org.apache.hadoop.io.SequenceFile$Writer:sync(),1369,1375,"/**
* Writes synchronization data to output stream if necessary.
* @throws IOException on I/O error
*/","* create a sync point.
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getLength,org.apache.hadoop.io.SequenceFile$Writer:getLength(),1530,1532,"/**
* Retrieves the current output position in bytes.
* @throws IOException if an I/O error occurs
*/","@return Returns the current length of the output file.
     *
     * <p>This always returns a synchronized position.  In other words,
     * immediately after calling {@link SequenceFile.Reader#seek(long)} with a position
     * returned by this method, {@link SequenceFile.Reader#next(Writable)} may be called.  However
     * the key may be earlier in the file than key last written when this
     * method was called (e.g., with block-compression, it may be the first key
     * in the block that was being written when this method was called).</p>
     *
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCurrentPos,org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getCurrentPos(),156,158,"/**
* Returns the current output position.
* @return total bytes written to underlying file and buffered output
*/","* Get the current position in file.
       * 
       * @return The current byte offset in underlying file.
       * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getContentSummary,org.apache.hadoop.fs.FileSystem:getContentSummary(org.apache.hadoop.fs.Path),1923,1945,"/**
* Calculates content summary for a given path (file or directory).
* @param f the path to calculate summary for
* @return ContentSummary object containing total length, file count, and directory count
*/","Return the {@link ContentSummary} of a given {@link Path}.
   * @param f path to use
   * @throws FileNotFoundException if the path does not resolve
   * @throws IOException IO failure
   * @return content summary.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,buildACL,org.apache.hadoop.security.authorize.AccessControlList:buildACL(java.lang.String[]),107,126,"/**
* Parses ACL string array into user and group sets.
* @param userGroupStrings string array containing user and/or group permissions
*/","* Build ACL from the given array of strings.
   * The strings contain comma separated values.
   *
   * @param userGroupStrings build ACL from array of Strings",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfigurationHelper.java,parseEnumSet,"org.apache.hadoop.util.ConfigurationHelper:parseEnumSet(java.lang.String,java.lang.String,java.lang.Class,boolean)",68,99,"/**
* Parses a string into an EnumSet of values.
* @param key identifier for logging
* @param valueString input string to parse
* @param enumClass class of the enum values
* @param ignoreUnknown whether to ignore unknown values or raise an exception
* @return EnumSet containing parsed values, or null if invalid
*/","* Given a comma separated list of enum values,
   * trim the list, map to enum values in the message (case insensitive)
   * and return the set.
   * Special handling of ""*"" meaning: all values.
   * @param key Configuration object key -used in error messages.
   * @param valueString value from Configuration
   * @param enumClass class of enum
   * @param ignoreUnknown should unknown values be ignored?
   * @param <E> enum type
   * @return a mutable set of enum values parsed from the valueString, with any unknown
   * matches stripped if {@code ignoreUnknown} is true.
   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,
   * or there are two entries in the enum which differ only by case.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,ensureCurrentState,org.apache.hadoop.service.ServiceStateModel:ensureCurrentState(org.apache.hadoop.service.Service$STATE),97,104,"/**
* Verifies that the current service state matches the specified expected state.
* @param expectedState desired service state (e.g. STARTED, STOPPED)
*/","* Verify that that a service is in a given state.
   * @param expectedState the desired state
   * @throws ServiceStateException if the service state is different from
   * the desired state",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,<init>,org.apache.hadoop.service.AbstractService:<init>(java.lang.String),113,116,"/**
* Initializes an AbstractService instance with a given name.
* @param name unique identifier for the service
*/","* Construct the service.
   * @param name service name",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,checkStateTransition,"org.apache.hadoop.service.ServiceStateModel:checkStateTransition(java.lang.String,org.apache.hadoop.service.Service$STATE,org.apache.hadoop.service.Service$STATE)",128,135,"/**
* Validates a service state transition.
* @param name the service name
* @param state current service state
* @param proposed proposed next service state
*/","* Check that a state tansition is valid and
   * throw an exception if not
   * @param name name of the service (can be null)
   * @param state current state
   * @param proposed proposed new state",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,serviceCreationFailure,org.apache.hadoop.service.launcher.ServiceLauncher:serviceCreationFailure(java.lang.Exception),745,747,"/**
* Creates and returns a ServiceLaunchException instance on service creation failure.
* @param exception underlying exception causing the failure
*/","* Generate an exception announcing a failure to create the service.
   * @param exception inner exception.
   * @return a new exception, with the exit code
   * {@link LauncherExitCodes#EXIT_SERVICE_CREATION_FAILURE}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,verifyConfigurationFilesExist,org.apache.hadoop.service.launcher.ServiceLauncher:verifyConfigurationFilesExist(java.lang.String[]),989,1003,"/**
* Verifies existence of specified configuration files.
* @param filenames array of configuration file names to check
*/","* Verify that all the specified filenames exist.
   * @param filenames a list of files
   * @throws ServiceLaunchException if a file is not found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.String,java.lang.Object[])",1088,1092,"/**
* Constructs KerberosDiagsFailure object with formatted message.
* @param category failure category
* @param message error message template
* @param args variable arguments for message formatting
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,convertToExitException,org.apache.hadoop.service.launcher.ServiceLauncher:convertToExitException(java.lang.Throwable),714,737,"/**
* Converts a Throwable to an ExitUtil.ExitException.
* @param thrown the Throwable to convert
*/","* Convert an exception to an {@code ExitException}.
   *
   * This process may just be a simple pass through, otherwise a new
   * exception is created with an exit code, the text of the supplied
   * exception, and the supplied exception as an inner cause.
   * 
   * <ol>
   *   <li>If is already the right type, pass it through.</li>
   *   <li>If it implements {@link ExitCodeProvider#getExitCode()},
   *   the exit code is extracted and used in the new exception.</li>
   *   <li>Otherwise, the exit code
   *   {@link LauncherExitCodes#EXIT_EXCEPTION_THROWN} is used.</li>
   * </ol>
   *  
   * @param thrown the exception thrown
   * @return an {@code ExitException} with a status code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,<init>,org.apache.hadoop.service.launcher.ServiceShutdownHook:<init>(org.apache.hadoop.service.Service),52,54,"/**
 * Installs a shutdown hook to gracefully terminate the specified service.
 * @param service the service to be shut down on JVM termination
 */","* Create an instance.
   * @param service the service",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,toString,org.apache.hadoop.service.launcher.InterruptEscalator:toString(),89,101,"/**
* Returns a human-readable string representation of the InterruptEscalator.
* @return A formatted string containing details about signal receipt, owner service, shutdown time, and forced shutdown timeout.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,noteFailure,org.apache.hadoop.service.AbstractService:noteFailure(java.lang.Exception),257,272,"/**
* Records and logs service failure details.
* @param exception the Exception object to record
*/","* Failure handling: record the exception
   * that triggered it -if there was not one already.
   * Services are free to call this themselves.
   * @param exception the exception",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,recordLifecycleEvent,org.apache.hadoop.service.AbstractService:recordLifecycleEvent(),421,426,"/**
* Records current service state as a lifecycle event.
*/",* Add a state change event to the lifecycle history,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,serviceInit,org.apache.hadoop.service.CompositeService:serviceInit(org.apache.hadoop.conf.Configuration),104,113,"/**
* Initializes services with provided configuration.
* @param conf Configuration object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,stop,"org.apache.hadoop.service.CompositeService:stop(int,boolean)",147,170,"/**
* Stops specified started services in reverse order.
* @param numOfServicesStarted number of services to stop
* @param stopOnlyStartedServices true to only stop started services, false to also stop initialized ones
*/","* Stop the services in reverse order
   *
   * @param numOfServicesStarted index from where the stop should work
   * @param stopOnlyStartedServices flag to say ""only start services that are
   * started, not those that are NOTINITED or INITED.
   * @throws RuntimeException the first exception raised during the
   * stop process -<i>after all services are stopped</i>",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceOperations.java,stopQuietly,org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.hadoop.service.Service),65,67,"/**
* Stops the given Service instance quietly.
* @param service Service instance to be stopped
*/","* Stop a service; if it is null do nothing. Exceptions are caught and
   * logged at warn level. (but not Throwables). This operation is intended to
   * be used in cleanup operations
   *
   * @param service a service; may be null
   * @return any exception that was caught; null if none was.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,progressable,org.apache.hadoop.io.SequenceFile$Writer:progressable(org.apache.hadoop.util.Progressable),1036,1038,"/**
* Wraps a Progressable into an Option.
* @param value Progressable instance to wrap
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,blockSize,org.apache.hadoop.io.SequenceFile$Writer:blockSize(long),1032,1034,"/**
* Creates an option representing block size.
* @param value block size value
* @return BlockSizeOption object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,syncInterval,org.apache.hadoop.io.SequenceFile$Writer:syncInterval(int),1061,1063,"/**
* Creates an option with a synchronization interval value.
* @param value interval in milliseconds
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,replication,org.apache.hadoop.io.SequenceFile$Writer:replication(short),1024,1026,"/**
 * Creates a ReplicationOption instance with the specified short value.
 * @param value short integer value to initialize the option
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,bufferSize,org.apache.hadoop.io.SequenceFile$Writer:bufferSize(int),1016,1018,"/**
* Creates an option with buffer size.
* @param value buffer size value
* @return BufferSizeOption object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,write,org.apache.hadoop.io.ObjectWritable$NullInstance:write(java.io.DataOutput),126,129,"/**
* Writes class name to output stream.
* @throws IOException if I/O operation fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,write,org.apache.hadoop.io.ArrayPrimitiveWritable:write(java.io.DataOutput),171,200,"/**
* Writes component data to output stream.
* @param out DataOutput stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,valueClass,org.apache.hadoop.io.SequenceFile$Writer:valueClass(java.lang.Class),1044,1046,"/**
* Creates an option instance representing a value class.
* @param value Class object to be represented
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,keyClass,org.apache.hadoop.io.MapFile$Writer:keyClass(java.lang.Class),285,287,"/**
* Creates an option with the provided key class.
* @param value Class of the writable comparable
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,keyClass,org.apache.hadoop.io.SequenceFile$Writer:keyClass(java.lang.Class),1040,1042,"/**
* Creates an option with the given class as its value.
* @param value the Class object to create an option from
* @return a new KeyClassOption instance or null if not applicable (not used)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,compareTo,org.apache.hadoop.io.UTF8:compareTo(org.apache.hadoop.io.UTF8),156,160,"/**
* Compares this UTF8 object with another based on their byte sequences.
* @param o the other UTF8 object to compare with
*/",Compare two UTF8s.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,equals,org.apache.hadoop.io.UTF8:equals(java.lang.Object),193,203,"/**
* Compares this UTF-8 byte sequence with another for equality.
* @param o the object to compare with (must be a UTF-8 instance)
* @return true if both sequences have equal length and content
*/",Returns true iff <code>o</code> is a UTF8 with the same contents.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,compareTo,org.apache.hadoop.io.MD5Hash:compareTo(org.apache.hadoop.io.MD5Hash),241,245,"/**
* Compares this MD5 hash with another by comparing their byte arrays.
* @param that the other MD5 hash to compare with
* @return negative/positive value if this < > that, zero if equal
*/",Compares this object with the specified object for order.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,compareTo,org.apache.hadoop.io.BinaryComparable:compareTo(org.apache.hadoop.io.BinaryComparable),50,56,"/**
* Compares this BinaryComparable instance with another.
* @param other the object to compare with
* @return a negative integer if less, zero if equal, or positive integer if greater
*/","* Compare bytes from {#getBytes()}.
   * @see org.apache.hadoop.io.WritableComparator#compareBytes(byte[],int,int,byte[],int,int)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,compareTo,"org.apache.hadoop.io.BinaryComparable:compareTo(byte[],int,int)",66,69,"/**
* Compares this byte array with another.
* @param other the other byte array to compare
* @param off offset in other array to start comparison from
* @param len length of bytes to compare
* @return a negative/zero/positive integer if this is less than/equal to/greater than other","* Compare bytes from {#getBytes()} to those provided.
   *
   * @param other other.
   * @param off off.
   * @param len len.
   * @return compareBytes.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/CompareUtils.java,compare,"org.apache.hadoop.io.file.tfile.CompareUtils$MemcmpRawComparator:compare(byte[],int,int,byte[],int,int)",89,92,"/**
* Compares two byte arrays using a comparator.
* @param b1 first byte array
* @param s1 starting index in b1
* @param l1 length of bytes to compare in b1
* @param b2 second byte array
* @param s2 starting index in b2
* @param l2 length of bytes to compare in b2
* @return comparison result (negative, zero, or positive)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,hashCode,org.apache.hadoop.io.UTF8:hashCode(),205,208,"/**
 * Calculates and returns the hash code of this object.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,hashCode,org.apache.hadoop.io.BinaryComparable:hashCode(),88,91,"/**
* Computes the hash code of this Writable object.
* @return hash value based on byte array and length
*/","* Return a hash of the bytes returned from {#getBytes()}.
   * @see org.apache.hadoop.io.WritableComparator#hashBytes(byte[],int)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,hashCode,org.apache.hadoop.security.token.Token:hashCode(),402,405,"/**
* Generates hash code based on user identifier bytes.
* @return unique hash value as an integer
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readDouble,"org.apache.hadoop.io.WritableComparator:readDouble(byte[],int)",311,313,"/**
* Converts a long value from byte array to double.
* @param bytes input byte array
* @param start starting index in the byte array
* @return double representation of the long value or NaN if invalid
*/","* Parse a double from a byte array.
   * @param bytes bytes.
   * @param start start.
   * @return double from a byte array.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,setSize,org.apache.hadoop.io.BytesWritable:setSize(int),131,138,"/**
* Updates the container size while preventing integer overflow.
* @param size new container size
*/","* Change the size of the buffer. The values in the old range are preserved
   * and any new values are undefined. The capacity is changed if it is 
   * necessary.
   * @param size The new number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MergeSort.java,<init>,org.apache.hadoop.util.MergeSort:<init>(java.util.Comparator),38,40,"/**
* Initializes MergeSort with a custom comparator.
* @param comparator user-defined comparison function for IntWritable objects
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,canRead,org.apache.hadoop.fs.FileUtil:canRead(java.io.File),1412,1423,"/**
* Checks if a file can be read, using platform-specific methods.
* @param f the File object to check
* @return true if readable, false otherwise
*/","* Platform independent implementation for {@link File#canRead()}
   * @param f input file
   * @return On Unix, same as {@link File#canRead()}
   *         On Windows, true if process has read access on the path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,canWrite,org.apache.hadoop.fs.FileUtil:canWrite(java.io.File),1431,1442,"/**
* Checks if a file can be written to.
* @param f the target file
* @return true if writable, false otherwise
*/","* Platform independent implementation for {@link File#canWrite()}
   * @param f input file
   * @return On Unix, same as {@link File#canWrite()}
   *         On Windows, true if process has write access on the path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,canExecute,org.apache.hadoop.fs.FileUtil:canExecute(java.io.File),1450,1461,"/**
* Checks if a file can be executed.
* @param f the file to check
* @return true if executable, false otherwise
*/","* Platform independent implementation for {@link File#canExecute()}
   * @param f input file
   * @return On Unix, same as {@link File#canExecute()}
   *         On Windows, true if process has execute access on the path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,assertCodeLoaded,org.apache.hadoop.io.nativeio.NativeIO$POSIX:assertCodeLoaded(),364,368,"/**
* Verifies native library availability and loads it if necessary.
* @throws IOException if native library is unavailable
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ReadaheadPool.java,getInstance,org.apache.hadoop.io.ReadaheadPool:getInstance(),55,62,"/**
* Returns the singleton instance of the readahead pool, lazily initializing it if necessary.
* @return the singleton instance of the readahead pool
*/",* @return Return the singleton instance for the current process.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,verifyCanMlock,org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:verifyCanMlock(),300,302,"/**
* Checks if native file locking is available.
* @return true if supported, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/SharedFileDescriptorFactory.java,getLoadingFailureReason,org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:getLoadingFailureReason(),53,61,"/**
* Returns the reason for loading failure, or null if successful.
* @return Reason string (null on success)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getMemlockLimit,org.apache.hadoop.io.nativeio.NativeIO:getMemlockLimit(),884,886,"/**
* Returns the memory locking limit.
* @return The maximum amount of memory that can be locked or 0 if unavailable
*/","* Get the maximum number of bytes that can be locked into memory at any
   * given point.
   *
   * @return 0 if no bytes can be locked into memory;
   *         Long.MAX_VALUE if there is no limit;
   *         The number of bytes that can be locked into memory otherwise.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,calculateChunkedSums,"org.apache.hadoop.util.DataChecksum:calculateChunkedSums(byte[],int,int,byte[],int)",576,600,"/**
* Calculates chunked checksums for the provided data.
* @param data input byte array
* @param dataOffset starting offset in data
* @param dataLength length of data to process
* @param sums output array for checksum values
* @param sumsOffset starting offset in sums array","* Implementation of chunked calculation specifically on byte arrays. This
   * is to avoid the copy when dealing with ByteBuffers that have array backing.
   *
   * @param data data.
   * @param dataOffset dataOffset.
   * @param dataLength dataLength.
   * @param sums sums.
   * @param sumsOffset sumsOffset.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getCreateForWriteFileOutputStream,"org.apache.hadoop.io.nativeio.NativeIO:getCreateForWriteFileOutputStream(java.io.File,int)",1001,1037,"/**
* Creates a FileOutputStream for writing to the specified file with given permissions.
* @param f File object representing the target file
* @param permissions Integer value of file permissions
* @return FileOutputStream object or throws IOException if an error occurs
*/","* @return Create the specified File for write access, ensuring that it does not exist.
   * @param f the file that we want to create
   * @param permissions we want to have on the file (if security is enabled)
   *
   * @throws AlreadyExistsException if the file already exists
   * @throws IOException if any other error occurred",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ReadaheadPool.java,run,org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:run(),210,232,"/**
* Performs POSIX FADV_WILLNEED readahead operation on a file descriptor.
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,cleanBufferPool,org.apache.hadoop.crypto.CryptoInputStream:cleanBufferPool(),816,821,"/**
* Releases and frees all buffers in the pool.
*/",Clean direct buffer pool,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,freeBuffers,org.apache.hadoop.crypto.CryptoOutputStream:freeBuffers(),311,314,"/**
* Frees allocated buffers.
* @param inBuffer input buffer to release
* @param outBuffer output buffer to release
*/",Forcibly free the direct buffers.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getFstat,org.apache.hadoop.io.nativeio.NativeIO$POSIX:getFstat(java.io.FileDescriptor),575,596,"/**
* Retrieves file statistics for the given FileDescriptor.
* @param fd unique file descriptor
* @return Stat object containing file metadata or null on failure
*/","* Returns the file stat for a file descriptor.
     *
     * @param fd file descriptor.
     * @return the file descriptor file stat.
     * @throws IOException thrown if there was an IO error while obtaining the file stat.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getStat,org.apache.hadoop.io.nativeio.NativeIO$POSIX:getStat(java.lang.String),606,627,"/**
* Retrieves file statistics by path.
* @param path file or directory path
* @return Stat object containing file metadata; null if invalid path
*/","* Return the file stat for a file path.
     *
     * @param path  file path
     * @return  the file stat
     * @throws IOException  thrown if there is an IO error while obtaining the
     * file stat",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BoundedByteArrayOutputStream.java,<init>,"org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(int,int)",55,57,"/**
* Initializes a BoundedByteArrayOutputStream with initial buffer size and write limit.
* @param capacity initial buffer size
* @param limit maximum allowed bytes to write","* Create a BoundedByteArrayOutputStream with the specified
   * capacity and limit.
   * @param capacity The capacity of the underlying byte array
   * @param limit The maximum limit upto which data can be written",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,<init>,org.apache.hadoop.io.EnumSetWritable:<init>(java.util.EnumSet),80,82,"/**
* Creates an instance from an existing EnumSet.
* @param value the initial set of values
*/","* Construct a new EnumSetWritable. Argument <tt>value</tt> should not be null
   * or empty.
   * 
   * @param value enumSet value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,write,"org.apache.hadoop.io.OutputBuffer:write(java.io.InputStream,int)",107,109,"/**
* Writes data from input stream to internal buffer.
* @param in InputStream containing data
* @param length number of bytes to read and write
*/","* Writes bytes from a InputStream directly into the buffer.
   * @param in input in.
   * @param length input length.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,nextBytes,org.apache.hadoop.crypto.random.OsSecureRandom:nextBytes(byte[]),97,108,"/**
* Fills the given byte array with random bytes.
* @param bytes target byte array
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,next,org.apache.hadoop.crypto.random.OsSecureRandom:next(int),110,118,"/**
* Generates a random integer of specified length.
* @param nbits desired bit length
* @return random int with specified number of bits
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,fromString,org.apache.hadoop.io.DefaultStringifier:fromString(java.lang.String),75,81,"/**
* Deserializes object of type T from a base64-encoded string.
* @param str base64-encoded string to deserialize
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKeyStream,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKeyStream(),1806,1809,"/**
* Resets and returns a data input stream for key data.
* @return DataInputStream object
*/","* Streaming access to the key. Useful for desrializing the key into
         * user objects.
         * 
         * @return The input stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,decodeWritable,"org.apache.hadoop.security.token.Token:decodeWritable(org.apache.hadoop.io.Writable,java.lang.String)",355,366,"/**
* Decodes and reads Hadoop Writable object from a Base64-encoded string.
* @param obj the writable object to read fields into
* @param newValue the base64-encoded string to decode
*/","* Modify the writable to the value from the newValue.
   * @param obj the object to read into
   * @param newValue the string with the url-safe base64 encoded bytes
   * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeUncompressedBytes,org.apache.hadoop.io.SequenceFile$CompressedBytes:writeUncompressedBytes(java.io.DataOutputStream),699,715,"/**
* Writes decompressed data to output stream.
* @throws IOException if I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,compare,"org.apache.hadoop.io.WritableComparator:compare(byte[],int,int,byte[],int,int)",177,192,"/**
* Compares two byte arrays as keys.
* @param b1 first byte array
* @param s1 start index of first byte array
* @param l1 length of first byte array
* @param b2 second byte array
* @param s2 start index of second byte array
* @param l2 length of second byte array
* @return negative/positive value if the keys are not equal, zero if they are equal","Optimization hook.  Override this to make SequenceFile.Sorter's scream.
   *
   * <p>The default implementation reads the data into two {@link
   * WritableComparable}s (using {@link
   * Writable#readFields(DataInput)}, then calls {@link
   * #compare(WritableComparable,WritableComparable)}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/RSErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",34,36,"/**
* Initializes an instance of RSErasureCodec with configuration and options.
* @param conf Hadoop Configuration object
* @param options Erasure codec specific options
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/HHXORErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",34,36,"/**
* Initializes the HHXORErasureCodec with configuration and options.
* @param conf Hadoop Configuration object
* @param options Erasure Codec Options
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/DummyErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",32,34,"/**
 * Initializes the DummyErasureCodec with configuration and options.
 * @param conf Hadoop Configuration object
 * @param options ErasureCodec-specific options
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/XORErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",34,37,"/**
* Initializes XORErasureCodec with configuration and options.
* @param conf overall Hadoop configuration
* @param options erasure codec options
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawCoderFactory,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawCoderFactory(java.lang.String,java.lang.String)",154,161,"/**
* Retrieves a RawErasureCoderFactory instance by name.
* @param coderName raw erasure coder type
* @param codecName codec implementation name
* @return RawErasureCoderFactory object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,anyRecoverable,org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:anyRecoverable(org.apache.hadoop.io.erasurecode.ECBlockGroup),86,90,"/**
* Checks if a recoverable ECBlockGroup exists.
* @param blockGroup group to check
* @return true if recoverable, false otherwise
*/","* Given a BlockGroup, tell if any of the missing blocks can be recovered,
   * to be called by ECManager
   * @param blockGroup a blockGroup that may contain erased blocks but not sure
   *                   recoverable or not
   * @return true if any erased block recoverable, false otherwise",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getNumErasedBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumErasedBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),132,136,"/**
* Calculates total number of erased blocks in a block group.
* @param blockGroup ECBlockGroup object containing parity and data blocks
*/","* Get the number of erased blocks in the block group.
   * @param blockGroup blockGroup.
   * @return number of erased blocks",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getErasedIndexes,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getErasedIndexes(org.apache.hadoop.io.erasurecode.ECBlock[]),159,174,"/**
* Retrieves indices of blocks marked as erased from the given array.
* @param inputBlocks array of ECBlock objects
* @return array of indices or empty array if none found
*/","* Get indexes of erased blocks from inputBlocks
   * @param inputBlocks inputBlocks.
   * @return indexes of erased blocks from inputBlocks",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,checkInputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:checkInputBuffers(java.nio.ByteBuffer[]),98,122,"/**
* Validates input buffers for decoding.
* @param buffers array of ByteBuffer objects to check
*/","* Check and ensure the buffers are of the desired length and type, direct
   * buffers or not.
   * @param buffers the buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,checkInputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:checkInputBuffers(byte[][]),95,115,"/**
* Validates input buffers by checking lengths and count.
* @param buffers array of byte[] buffers to validate
*/","* Check and ensure the buffers are of the desired length.
   * @param buffers the buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,<init>,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],int[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",52,65,"/**
* Initializes ErasureDecodingStep with input blocks, erased indexes,
* and encoding/decoding components.
* @param inputBlocks array of EC blocks to process
* @param erasedIndexes array of indices representing erased data units
*/","* The constructor with all the necessary info.
   * @param inputBlocks inputBlocks.
   * @param erasedIndexes the indexes of erased blocks in inputBlocks array
   * @param outputBlocks outputBlocks.
   * @param rawDecoder underlying RS decoder for hitchhiker decoding
   * @param rawEncoder underlying XOR encoder for hitchhiker decoding",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DecodingState.java,checkParameters,"org.apache.hadoop.io.erasurecode.rawcoder.DecodingState:checkParameters(java.lang.Object[],int[],java.lang.Object[])",38,54,"/**
* Validates input parameters for data recovery.
* @param inputs array of input values
* @param erasedIndexes indices of corrupted elements
* @param outputs corresponding recovered values
*/","* Check and validate decoding parameters, throw exception accordingly. The
   * checking assumes it's a MDS code. Other code  can override this.
   * @param inputs input buffers to check
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,<init>,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",48,57,"/**
* Initializes the Erasure Encoding Step with input/output blocks, raw encoders for Reed-Solomon and XOR operations,
* and calculates the piggyback index.
* @param inputBlocks array of EC blocks to encode
* @param outputBlocks array of EC blocks to store encoded data
* @param rsRawEncoder encoder for Reed-Solomon operation
* @param xorRawEncoder encoder for XOR operation
*/","* The constructor with all the necessary info.
   *
   * @param inputBlocks inputBlocks.
   * @param outputBlocks outputBlocks.
   * @param rsRawEncoder  underlying RS encoder for hitchhiker encoding
   * @param xorRawEncoder underlying XOR encoder for hitchhiker encoding",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/EncodingState.java,checkParameters,"org.apache.hadoop.io.erasurecode.rawcoder.EncodingState:checkParameters(java.lang.Object[],java.lang.Object[])",36,43,"/**
* Validates input and output array lengths against encoding parameters.
* @param inputs input data units
* @param outputs parity data units
*/","* Check and validate decoding parameters, throw exception accordingly.
   * @param inputs input buffers to check
   * @param outputs output buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,40,"/**
* Creates a raw decoder instance using specified erasure coding options.
* @param coderOptions configuration settings for erasure decoding
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),36,45,"/**
* Prepares decoding step for erasure-coded data.
* @param blockGroup ECBlockGroup object
* @return ErasureDecodingStep object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,39,"/**
* Creates a decoder instance based on provided ErasureCoder options.
* @param coderOptions configuration object for the decoder
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
* Initializes NativeXORRawDecoder with ErasureCoderOptions.
* @param coderOptions configuration for the decoder
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
* Initializes NativeRSRawDecoder with ErasureCoderOptions.
* @param coderOptions Erasure coding options for initialization
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java,getPiggyBackForDecode,"org.apache.hadoop.io.erasurecode.coder.util.HHUtil:getPiggyBackForDecode(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int,int,int,int)",150,201,"/**
* Computes and returns the piggyback for decoding.
* @param inputs input matrices
* @param outputs output matrices
* @param pbParityIndex parity index of interest
* @param numDataUnits number of data units
* @param numParityUnits number of parity units
* @param pbIndex last piggyback set index
* @return ByteBuffer containing the computed piggyback",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,add,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:add(int[],int[])",371,384,"/**
* Merges two arrays element-wise.
* @param p first array
* @param q second array
* @return new array with merged elements
*/","* Compute the sum of two polynomials. The index in the array corresponds to
   * the power of the entry. For example p[0] is the constant term of the
   * polynomial p.
   *
   * @param p input polynomial
   * @param q input polynomial
   * @return polynomial represents p+q",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,multiply,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:multiply(int[],int[])",327,340,"/**
* Multiplies two integer arrays using the grade school multiplication algorithm.
* @param p first array
* @param q second array
* @return product array of same length as sum of input arrays' lengths minus one.","* Compute the multiplication of two polynomials. The index in the array
   * corresponds to the power of the entry. For example p[0] is the constant
   * term of the polynomial p.
   *
   * @param p input polynomial
   * @param q input polynomial
   * @return polynomial represents p*q",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,gaussianElimination,org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:gaussianElimination(int[][]),549,588,"/**
* Performs Gaussian Elimination on a square matrix.
* @param matrix input matrix (must be square and have non-zero leading entries)
*/","* Perform Gaussian elimination on the given matrix. This matrix has to be a
   * fat matrix (number of rows &gt; number of columns).
   *
   * @param matrix matrix.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,getPrimitivePower,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:getPrimitivePower(int,int)",38,45,"/**
* Computes primitive power array for given data and parity units.
* @param numDataUnits number of data units
* @param numParityUnits number of parity units
* @return int array containing powers of the primitive root
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/DumpUtil.java,dumpChunks,"org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpChunks(java.lang.String,org.apache.hadoop.io.erasurecode.ECChunk[])",80,87,"/**
* Prints chunk data with a given header.
* @param header display header
* @param chunks array of ECChunk objects to print
*/","* Print data in hex format in an array of chunks.
   * @param header header.
   * @param chunks chunks.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
* Initializes native XOR raw encoder with specified options.
* @param coderOptions ErasureCoderOptions object containing configuration
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
* Initializes NativeRSRawEncoder with ErasureCoderOptions.
* @param coderOptions configuration settings for encoding
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,35,"/**
* Creates a raw encoder using XOR algorithm based on provided options.
* @param coderOptions configuration for the encoder
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),36,44,"/**
* Prepares Erasure Coding step for block group.
* @param blockGroup EC block group object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),31,34,"/**
* Creates a raw erasure encoder instance with given options.
* @param coderOptions Erasure coding configuration
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetOutputBuffers,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetOutputBuffers(java.nio.ByteBuffer[],int)",87,91,"/**
* Resets multiple output buffers with specified data length.
* @param buffers array of ByteBuffers to reset
* @param dataLen target data length for each buffer
*/",* Initialize the output buffers with ZERO bytes.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,toBuffers,org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:toBuffers(org.apache.hadoop.io.erasurecode.ECChunk[]),108,125,"/**
* Converts ECChunk array to ByteBuffer array, handling null chunks and resetting zero-filled buffers.
* @param chunks array of ECChunks
*/","* Convert an array of this chunks to an array of ByteBuffers
   * @param chunks chunks to convertToByteArrayState into buffers
   * @return an array of ByteBuffers",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetOutputBuffers,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetOutputBuffers(byte[][],int[],int)",96,101,"/**
* Resets multiple output buffers with given offsets and total data length.
* @param buffers array of byte buffers to reset
* @param offsets array of buffer offset indices
* @param dataLen total data length for all buffers
*/",* Initialize the output buffers with ZERO bytes.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),83,96,"/**
* Encodes the provided state into a ByteBuffer-based format.
* @param encodingState current encoding state
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),86,99,"/**
 * Decodes the provided ByteArrayDecodingState into a ByteBuffer representation.
 * @param decodingState state object containing input/output data and offsets
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),57,71,"/**
* Initializes RS raw decoder with given ErasureCoderOptions.
* @param coderOptions configuration options for erasure coding
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),42,61,"/**
* Initializes RS Raw Encoder with specified ErasureCoderOptions.
* @param coderOptions configuration for erasure coding
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,generateDecodeMatrix,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:generateDecodeMatrix(int[]),143,176,"/**
* Reconstructs decode matrix by removing error rows and inverting remaining matrix.
* @param erasedIndexes array of indices of removed data units
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,getInstance,org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getInstance(),121,123,"/**
 * Returns an instance of GaloisField with default field size and primitive polynomial.
 */","* Get the object performs Galois field arithmetic with default setting.
   * @return GaloisField.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsR,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsR(long),616,638,"/**
* Extracts and returns a specified number of bytes from the input stream.
* @param n number of bytes to extract
* @return extracted byte sequence or throws IOException if end of stream reached
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsGetBit,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetBit(),640,658,"/**
* Fetches the next bit from the input stream.
* @throws IOException on unexpected end of stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,init,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:init(),756,772,"/**
* Initializes the stream with magic bytes and blockSize100k.
* @throws IOException on write errors
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,endCompression,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:endCompression(),833,849,"/**
* Writes end-of-compression marker and CRC.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues(),951,992,"/**
* Generates and transmits MTF values for compression.
* @throws IOException if transmission fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,mainSort,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainSort(),1738,1901,"/**
* Performs main sorting on the provided data, initializing 
* running order and completing initial radix sort.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,<init>,"org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",49,51,"/**
* Initializes a new CompressorStream with default buffer size.
* @param out OutputStream to write compressed data
* @param compressor Compressor instance for compression
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockCompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int,int)",54,58,"/**
* Initializes a new BlockCompressorStream with specified parameters.
* @param out OutputStream to write compressed data to
* @param compressor Compressor instance for compression
* @param bufferSize maximum input buffer size
* @param compressionOverhead overhead bytes consumed by compression
*/","* Create a {@link BlockCompressorStream}.
   * 
   * @param out stream
   * @param compressor compressor to be used
   * @param bufferSize size of buffer
   * @param compressionOverhead maximum 'overhead' of the compression 
   *                            algorithm with given bufferSize",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,write,org.apache.hadoop.io.compress.CompressorStream:write(int),115,119,"/**
* Writes single byte to output stream.
* @param b the byte to be written (0-255)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,"org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",68,72,"/**
* Initializes DecompressorStream with specified settings and buffer size.
* @param in input stream to be decompressed
* @param decompressor Decompressor instance for compression algorithm
* @param bufferSize size of buffer for data processing
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,<init>,org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:<init>(java.io.InputStream),162,166,"/**
 * Initializes decompression stream with given input stream.
 * @param input input stream to pass through decompression
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,<init>,org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream),64,66,"/**
* Initializes BlockDecompressorStream from an InputStream.
* @param in input stream to decompress
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,decompress,"org.apache.hadoop.io.compress.DecompressorStream:decompress(byte[],int,int)",108,173,"/**
* Decompresses a byte array using a provided decompressor.
* @param b the byte array to decompress
* @param off the offset into the byte array to start decompression from
* @param len the length of the byte array to decompress
* @return the decompressed data length or -1 if end-of-file is reached
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,finish,org.apache.hadoop.io.compress.BlockCompressorStream:finish(),136,145,"/**
* Completes the compression process and writes remaining data.
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<init>(int),78,85,"/**
* Initializes ZStandard decompressor with specified buffer size.
* @param bufferSize direct memory buffer allocation size
*/","* Creates a new decompressor.
   * @param bufferSize bufferSize.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,finalize,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:finalize(),248,251,"/**
 * Performs cleanup and resets resources when garbage collection is imminent.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,reset,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:reset(),307,311,"/**
* Resets processing state and marks input as complete.
* Calls superclass reset method to clear internal state.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,<init>,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(int,int,int)",94,103,"/**
 * Initializes ZStandardCompressor with specified compression level and buffer sizes.
 * @param level compression level
 * @param inputBufferSize size of direct buffer for uncompressed data
 * @param outputBufferSize size of direct buffer for compressed data
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,decompress,"org.apache.hadoop.io.compress.BlockDecompressorStream:decompress(byte[],int,int)",68,112,"/**
* Decompresses compressed data using a provided decompressor.
* @param b compressed byte array
* @param off offset into compressed data
* @param len length of compressed data
* @return number of decompressed bytes or -1 if EOF
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,<init>,org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>(),102,104,"/**
 * Initializes a new instance of the Lz4Compressor with default buffer size.
 * @param directBufferSize initial direct buffer size
 */",* Creates a new compressor with the default buffer size.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodecClassByName,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClassByName(java.lang.String),274,281,"/**
* Returns the class of a compression codec by name.
* @param codecName unique codec identifier
* @return Class<? extends CompressionCodec> or null if not found
*/","* Find the relevant compression codec for the codec's canonical class name
   * or by codec alias and returns its implemetation class.
   * <p>
   * Codec aliases are case insensitive.
   * <p>
   * The code alias is the short class name (without the package name).
   * If the short class name ends with 'Codec', then there are two aliases for
   * the codec, the complete short class name and the short class name without
   * the 'Codec' ending. For example for the 'GzipCodec' codec class name the
   * alias are 'gzip' and 'gzipcodec'.
   *
   * @param codecName the canonical class name of the codec
   * @return the codec class",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getCompressor,"org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration)",149,165,"/**
* Retrieves a Compressor instance based on the provided CompressionCodec and Configuration.
* @param codec Compression Codec to determine compressor type
* @param conf Configuration for compressor initialization
* @return Compressor object or null if creation fails
*/","* Get a {@link Compressor} for the given {@link CompressionCodec} from the 
   * pool or a new one.
   *
   * @param codec the <code>CompressionCodec</code> for which to get the 
   *              <code>Compressor</code>
   * @param conf the <code>Configuration</code> object which contains confs for creating or reinit the compressor
   * @return <code>Compressor</code> for the given 
   *         <code>CompressionCodec</code> from the pool or a new one",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getDecompressor,org.apache.hadoop.io.compress.CodecPool:getDecompressor(org.apache.hadoop.io.compress.CompressionCodec),180,195,"/**
* Retrieves a Decompressor instance based on the provided CompressionCodec.
* @param codec CompressionCodec that dictates the type of decompressor to use
* @return Decompressor object or null if creation fails
*/","* Get a {@link Decompressor} for the given {@link CompressionCodec} from the
   * pool or a new one.
   *  
   * @param codec the <code>CompressionCodec</code> for which to get the 
   *              <code>Decompressor</code>
   * @return <code>Decompressor</code> for the given 
   *         <code>CompressionCodec</code> the pool or a new one",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,returnCompressor,org.apache.hadoop.io.compress.CodecPool:returnCompressor(org.apache.hadoop.io.compress.Compressor),202,215,"/**
* Returns a reusable Compressor instance to the pool.
* @param compressor Compressor object to be returned
*/","* Return the {@link Compressor} to the pool.
   * 
   * @param compressor the <code>Compressor</code> to be returned to the pool",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,returnDecompressor,org.apache.hadoop.io.compress.CodecPool:returnDecompressor(org.apache.hadoop.io.compress.Decompressor),223,236,"/**
* Reclaims and optionally pools a Decompressor instance.
*@param decompressor Decompressor to process
*/","* Return the {@link Decompressor} to the pool.
   * 
   * @param decompressor the <code>Decompressor</code> to be returned to the 
   *                     pool",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getCompressorType,org.apache.hadoop.io.compress.ZStandardCodec:getCompressorType(),151,155,"/**
* Returns the compressor class type (ZStandardCompressor).
* @return Class of ZStandardCompressor compressor implementation.","* Get the type of {@link Compressor} needed by this {@link CompressionCodec}.
   *
   * @return the type of compressor needed by this codec.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getDecompressorType,org.apache.hadoop.io.compress.ZStandardCodec:getDecompressorType(),209,213,"/**
* Returns decompressor type class.
*/","* Get the type of {@link Decompressor} needed by
   * this {@link CompressionCodec}.
   *
   * @return the type of decompressor needed by this codec.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,getCompressorType,org.apache.hadoop.io.compress.DefaultCodec:getCompressorType(),69,72,"/**
* Returns the type of compressor to use based on configuration.
* @return Class representing the compressor type (e.g. ZlibCompressor)",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,getDecompressorType,org.apache.hadoop.io.compress.DefaultCodec:getDecompressorType(),95,98,"/**
* Returns type of decompressor based on configuration.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,<init>,org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:<init>(org.apache.hadoop.conf.Configuration),66,68,"/**
 * Initializes BuiltInGzipCompressor with given configuration.
 * @param conf Hadoop Configuration object",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,reinit,org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:reinit(org.apache.hadoop.conf.Configuration),167,175,"/**
* Resets and reinitializes internal state with the given configuration.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,<init>,org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>(),122,126,"/**
* Initializes a new instance of the GzipZlibCompressor with default settings.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,<init>,org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>(org.apache.hadoop.conf.Configuration),128,133,"/**
* Initializes GzipZlibCompressor with configuration settings.
* @param conf Hadoop Configuration object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(),234,239,"/**
* Initializes ZlibCompressor with default compression settings.
*/","* Creates a new compressor with the default compression level.
   * Compressed data will be generated in ZLIB format.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(org.apache.hadoop.conf.Configuration),245,250,"/**
* Initializes ZlibCompressor with configuration settings.
* @param conf Configuration object containing compression level and strategy
*/","* Creates a new compressor, taking settings from the configuration.
   * @param conf configuration.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,reinit,org.apache.hadoop.io.compress.zlib.ZlibCompressor:reinit(org.apache.hadoop.conf.Configuration),283,298,"/**
* Reinitializes the compressor with a new configuration.
* @param conf new configuration to use
*/","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration. It will reset the compressor's compression level
   * and compression strategy.
   * 
   * @param conf Configuration storing new settings",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:<init>(),352,354,"/**
* Initializes a new instance of ZlibDirectDecompressor with default header and initial window size.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,"org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader,int)",356,358,"/**
 * Constructs a ZlibDirectDecompressor instance with given compression header and direct buffer size.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:<init>(),117,119,"/**
* Constructs a new ZlibDecompressor with default header and buffer size.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,<init>,org.apache.hadoop.io.compress.GzipCodec$GzipZlibDecompressor:<init>(),137,139,"/**
* Initializes Zlib decompressor with autodetection and large buffer size.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,reset,org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:reset(),365,369,"/**
 * Resets the input state and marks it as ended.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,executeHeaderState,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:executeHeaderState(),257,358,"/**
* Processes gzip header states: basic, extra field, filename, comment, and CRC.
* @throws IOException if CRC verification fails or other errors occur
*/","* Parse the gzip header (assuming we're in the appropriate state).
   * In order to deal with degenerate cases (e.g., user buffer is one byte
   * long), we copy (some) header bytes to another buffer.  (Filename,
   * comment, and extra-field bytes are simply skipped.)</p>
   *
   * See http://www.ietf.org/rfc/rfc1952.txt for the gzip spec.  Note that
   * no version of gzip to date (at least through 1.4.0, 2010-01-20) supports
   * the FHCRC header-CRC16 flagbit; instead, the implementation treats it
   * as a multi-file continuation flag (which it also doesn't support). :-(
   * Sun's JDK v6 (1.6) supports the header CRC, however, and so do we.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readCompressedStringArray,org.apache.hadoop.io.WritableUtils:readCompressedStringArray(java.io.DataInput),181,189,"/**
* Reads a compressed array of strings from the input stream.
* @param in DataInput object containing the compressed string data
* @return null-terminated array of strings or null if length is -1
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VIntWritable.java,write,org.apache.hadoop.io.VIntWritable:write(java.io.DataOutput),54,57,"/**
* Writes user value to output stream.
* @param out DataOutput stream to write to",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,write,org.apache.hadoop.io.Text:write(java.io.DataOutput),395,399,"/**
* Writes object data to output stream.
* @param out DataOutput stream for writing
*/","* Serialize. Write this object to out length uses zero-compressed encoding.
   *
   * @see Writable#write(DataOutput)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,write,"org.apache.hadoop.io.Text:write(java.io.DataOutput,int)",401,409,"/**
* Writes data to output stream with optional max length check.
* @param out DataOutput stream
* @param maxLength maximum allowed byte count (optional)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,write,org.apache.hadoop.security.token.delegation.DelegationKey:write(java.io.DataOutput),94,104,"/**
* Writes this object's data to the output stream.
* @throws IOException if write operation fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,write,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:write(java.io.DataOutput),737,747,"/**
* Writes user data to output stream.
* @param out DataOutput stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readVInt,org.apache.hadoop.io.WritableUtils:readVInt(java.io.DataInput),334,340,"/**
* Reads a variable-length int from the specified input stream.
* @param stream DataInput stream containing the VInt data
* @return the read VInt value, or throws IOException if out of range
*/","* Reads a zero-compressed encoded integer from input stream and returns it.
   * @param stream Binary input stream
   * @throws IOException raised on errors performing I/O.
   * @return deserialized integer from stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readVIntInRange,"org.apache.hadoop.io.WritableUtils:readVIntInRange(java.io.DataInput,int,int)",354,370,"/**
* Reads a variable-length signed int from the stream, ensuring it's within the specified range.
* @param stream input stream
* @param lower minimum allowed value
* @param upper maximum allowed value
* @return read integer value or throws IOException if out of range
*/","* Reads an integer from the input stream and returns it.
   *
   * This function validates that the integer is between [lower, upper],
   * inclusive.
   *
   * @param stream Binary input stream
   * @param lower input lower.
   * @param upper input upper.
   * @throws IOException raised on errors performing I/O.
   * @return deserialized integer from stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VLongWritable.java,readFields,org.apache.hadoop.io.VLongWritable:readFields(java.io.DataInput),49,52,"/**
* Reads a VLong from the input stream and assigns it to the 'value' field.
* @throws IOException if an I/O error occurs during reading.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text:<init>(java.lang.String),95,97,"/**
 * Initializes a new Text instance with the given string.
 * @param string input text to be processed
 */","* Construct from a string.
   * @param string input string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,find,"org.apache.hadoop.io.Text:find(java.lang.String,int)",187,221,"/**
* Searches for the specified string within a byte array starting from a given offset.
* @param what string to search for
* @param start offset in bytes where search begins
* @return index of first matching byte or -1 if not found
*/","* Finds any occurrence of <code>what</code> in the backing
   * buffer, starting as position <code>start</code>. The starting
   * position is measured in bytes and the return value is in
   * terms of byte position in the buffer. The backing buffer is
   * not converted to a string for this operation.
   *
   * @param what input what.
   * @param start input start.
   * @return byte position of the first occurrence of the search
   *         string in the UTF-8 buffer or -1 if not found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,writeString,"org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)",582,588,"/**
* Writes a string to the output stream.
* @param out DataOutput stream
* @param s String to write
* @return Length of written string
*/","* Write a UTF8 encoded string to out.
   *
   * @param out input out.
   * @param s input s.
   * @throws IOException raised on errors performing I/O.
   * @return a UTF8 encoded string to out.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,writeString,"org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String,int)",598,610,"/**
* Writes a string to the output stream with a maximum length.
* @param out DataOutput stream
* @param s String to write
* @param maxLength Maximum allowed bytes in the string
* @return Actual written byte count or throws IOException if too long
*/","* @return Write a UTF8 encoded string with a maximum size to out.
   *
   * @param out input out.
   * @param s input s.
   * @param maxLength input maxLength.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,org.apache.hadoop.io.Text:set(byte[]),246,254,"/**
* Initializes or resets UTF-8 encoded byte array.
* @param utf8 UTF-8 encoded byte array
*/","* Set to a utf8 byte array. If the length of <code>utf8</code> is
   * <em>zero</em>, actually clear {@link #bytes} and any existing
   * data is lost.
   *
   * @param utf8 input utf8.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,org.apache.hadoop.io.Text:set(org.apache.hadoop.io.Text),260,263,"/**
* Sets the text content from another Text object.
* @param other Text object to copy from
*/","* Copy a text.
   * @param other other.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readDefaultLine,"org.apache.hadoop.util.LineReader:readDefaultLine(org.apache.hadoop.io.Text,int,int)",197,263,"/**
* Reads a line from the input stream, consuming up to maxBytesToConsume bytes.
* @param maxLineLength maximum allowed length of the line
* @return number of bytes consumed before newline or maxBytesToConsume exceeded
*/","* Read a line terminated by one of CR, LF, or CRLF.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readCustomLine,"org.apache.hadoop.util.LineReader:readCustomLine(org.apache.hadoop.io.Text,int,int)",268,371,"/**
* Reads a custom line from the input stream, consuming up to maxBytesToConsume
* bytes and returning the length of the consumed data.
* @param str Text buffer to append read data to
* @param maxLineLength maximum allowed length for appended text
* @param maxBytesToConsume maximum number of bytes to consume
* @return length of consumed data or -1 on EOF
*/",* Read a line terminated by a custom delimiter.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,toString,org.apache.hadoop.io.Text:toString(),337,344,"/**
* Decodes byte array into a string using character encoding.
* @throws RuntimeException if decoding fails for unknown reason
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,<init>,org.apache.hadoop.io.SortedMapWritable:<init>(),45,48,"/**
* Initializes an empty sorted map with default comparator.
*/",default constructor.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,<init>,org.apache.hadoop.io.MapWritable:<init>(),43,46,"/**
* Initializes an empty map for storing key-value pairs of Writable objects.
*/",Default constructor.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,write,org.apache.hadoop.io.SortedMapWritable:write(java.io.DataOutput),182,198,"/**
* Writes data to output stream, serializing map contents.
* @throws IOException if write operation fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,write,org.apache.hadoop.io.MapWritable:write(java.io.DataOutput),147,163,"/**
 * Serializes the map of entries to the output stream.
 * @throws IOException if serialization fails
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getDeserializer,"org.apache.hadoop.io.SequenceFile$Reader:getDeserializer(org.apache.hadoop.io.serializer.SerializationFactory,java.lang.Class)",2163,2166,"/**
* Retrieves deserializer instance from factory based on provided class.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,<init>,org.apache.hadoop.util.bloom.Key:<init>(byte[]),87,89,"/**
 * Initializes a new Key instance with the given byte array and default weight.
 * @param value the key's value as a byte array
 */","* Constructor.
   * <p>
   * Builds a key with a default weight.
   * @param value The byte value of <i>this</i> key.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compression,org.apache.hadoop.io.SequenceFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType),1052,1054,"/**
* Creates a CompressionOption instance based on the specified type.
* @param value Compression type (e.g. GZIP, ZIP)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,compression,"org.apache.hadoop.io.MapFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",302,306,"/**
* Sets the compression options for writing to a SequenceFile.
* @param type Compression type (e.g. GZIP, SNAPPY)
* @param codec Compression codec instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/DeserializerComparator.java,<init>,org.apache.hadoop.io.serializer.DeserializerComparator:<init>(org.apache.hadoop.io.serializer.Deserializer),52,57,"/**
* Initializes the comparator with a deserializer.
* @param deserializer deserializer to use for comparisons
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/DeserializerComparator.java,compare,"org.apache.hadoop.io.serializer.DeserializerComparator:compare(byte[],int,int,byte[],int,int)",59,73,"/**
* Compares two byte arrays as serialized keys.
* @param b1 first byte array
* @param s1 start index of b1
* @param l1 length of b1
* @param b2 second byte array
* @param s2 start index of b2
* @param l2 length of b2
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,readFields,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:readFields(java.io.DataInput),99,104,"/**
* Reads CRC and hash values from input stream.
* @throws IOException if I/O operation fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(byte[]),118,120,"/**
* Computes the MD5 hash of the given byte array.
* @param data input data to be hashed
*/","* Construct a hash value for a byte array.
   * @param data data.
   * @return MD5Hash.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(org.apache.hadoop.io.UTF8),195,197,"/**
* Generates MD5 hash from UTF-8 encoded string.
* @param utf8 UTF-8 encoded string
*/","* Construct a hash value for a String.
   * @param utf8 utf8.
   * @return MD5Hash.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,<init>,org.apache.hadoop.io.MD5Hash:<init>(java.lang.String),61,63,"/**
* Initializes an MD5 hash with a hexadecimal string.
* @param hex hexadecimal representation of the hash value
*/","* Constructs an MD5Hash from a hex string.
   * @param hex input hex.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,stream,org.apache.hadoop.io.SequenceFile$Writer:stream(org.apache.hadoop.fs.FSDataOutputStream),1020,1022,"/**
* Creates a new StreamOption with the given output stream.
* @param value FSDataOutputStream to be used in the option
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,appendIfExists,org.apache.hadoop.io.SequenceFile$Writer:appendIfExists(boolean),1028,1030,"/**
* Creates an option to append a file only if it exists.
* @param value true to enable, false to disable
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,file,org.apache.hadoop.io.SequenceFile$Writer:file(org.apache.hadoop.fs.Path),994,996,"/**
* Creates an instance of FileOption with the given path.
* @param value Path to the file or directory
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:shouldRetry(java.lang.Exception,int,int,boolean)",421,436,"/**
* Calculates next retry action based on current retry count.
* @param curRetry current retry attempt
* @param failovers total allowed retries
* @param isIdempotentOrAtMostOnce whether operation is idempotent or at-most-once
* @return RetryAction object indicating decision and sleep time, or failure to retry",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryForeverWithFixedSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryForeverWithFixedSleep(long,java.util.concurrent.TimeUnit)",82,86,"/**
* Returns a RetryPolicy with infinite retries and fixed exponential backoff.
* @param sleepTime initial delay in milliseconds
* @param timeUnit time unit for the delay (e.g. TimeUnit.MILLISECONDS)
*/","* <p>
   * Keep trying forever with a fixed time between attempts.
   * </p>
   *
   * @param sleepTime sleepTime.
   * @param timeUnit timeUnit.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryUpToMaximumCountWithFixedSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumCountWithFixedSleep(int,long,java.util.concurrent.TimeUnit)",99,101,"/**
* Creates a retry policy that attempts up to maxRetries with a fixed sleep period.
* @param maxRetries maximum number of retries
* @param sleepTime duration between retries in specified time unit
* @param timeUnit unit of time for sleep interval (e.g. SECONDS, MILLISECONDS)
*/","* <p>
   * Keep trying a limited number of times, waiting a fixed time between attempts,
   * and then fail by re-throwing the exception.
   * </p>
   *
   * @param maxRetries maxRetries.
   * @param sleepTime sleepTime.
   * @param timeUnit timeUnit.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:<init>(long,long,java.util.concurrent.TimeUnit)",346,351,"/**
* Initializes RetryUpToMaximumTimeWithFixedSleep with maximum execution time and fixed sleep duration.
* @param maxTime total allowed execution time
* @param sleepTime fixed interval between retries
* @param timeUnit unit of maxTime and sleepTime (e.g. TimeUnit.SECONDS)",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,exponentialBackoffRetry,"org.apache.hadoop.io.retry.RetryPolicies:exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)",148,151,"/**
* Creates an exponential backoff retry policy.
* @param maxRetries maximum number of retries
* @param sleepTime initial delay between retries (in specified unit)
* @param timeUnit unit of the sleep time (e.g. SECONDS, MILLISECONDS) 
*/","* <p>
   * Keep trying a limited number of times, waiting a growing amount of time between attempts,
   * and then fail by re-throwing the exception.
   * The time between attempts is <code>sleepTime</code> mutliplied by a random
   * number in the range of [0, 2 to the number of retries)
   * </p>
   *
   *
   * @param timeUnit timeUnit.
   * @param maxRetries maxRetries.
   * @param sleepTime sleepTime.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryUpToMaximumCountWithProportionalSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumCountWithProportionalSleep(int,long,java.util.concurrent.TimeUnit)",130,132,"/**
* Creates a retry policy with proportional sleep between attempts.
* @param maxRetries maximum number of retries
* @param sleepTime initial sleep duration
* @param timeUnit unit of time for sleep duration
*/","* <p>
   * Keep trying a limited number of times, waiting a growing amount of time between attempts,
   * and then fail by re-throwing the exception.
   * The time between attempts is <code>sleepTime</code> mutliplied by the number of tries so far.
   * </p>
   *
   * @param sleepTime sleepTime.
   * @param maxRetries maxRetries.
   * @param timeUnit timeUnit.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,"org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int,long,long)",210,215,"/**
* Creates a retry policy that fails over to a fallback policy on network exceptions.
* @param fallbackPolicy default retry policy
* @param maxFailovers maximum number of failovers allowed
* @param delayMillis initial delay in milliseconds
* @param maxDelayBase base value for exponential backoff
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,newAsyncCall,"org.apache.hadoop.io.retry.AsyncCallHandler:newAsyncCall(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler)",322,327,"/**
* Creates a new asynchronous call instance.
* @param method the remote method to invoke
* @param args parameters for the method invocation
* @param isRpc whether to use RPC protocol
* @param callId unique identifier for the call
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,newRetryInfo,"org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:newRetryInfo(org.apache.hadoop.io.retry.RetryPolicy,java.lang.Exception,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,boolean,long)",273,302,"/**
* Determines retry info based on policy and exceptions.
* @param policy RetryPolicy to apply
* @param e Exception(s) that occurred
* @param counters Counters for retries and failovers
* @param idempotentOrAtMostOnce Whether operation is idempotent or at-most-once
* @return RetryInfo object with max retry delay, action, expected failover count, and exception (if failed)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,tryStop,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStop(org.apache.hadoop.util.Daemon),186,190,"/**
* Attempts to stop a daemon and kills it if queue is empty after grace period.
* @param d Daemon instance to attempt stopping
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,read,org.apache.hadoop.security.Groups$TimerToTickerAdapter:read(),292,296,"/**
* Returns current time in milliseconds.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,unlock,org.apache.hadoop.util.InstrumentedWriteLock:unlock(),59,69,"/**
* Releases the write lock and optionally reports on write lock duration.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,startLockTiming,org.apache.hadoop.util.InstrumentedWriteLock:startLockTiming(),74,79,"/**
* Records the timestamp when the write lock is acquired.
* Called when the read-write lock's write hold count reaches 1.
*/",* Starts timing for the instrumented write lock.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,<init>,"org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.Lock,long,long,org.apache.hadoop.util.Timer)",87,99,"/**
* Initializes an InstrumentedLock with given parameters.
* @param name lock identifier
* @param logger logging instance
* @param lock underlying synchronization primitive
* @param minLoggingGapMs minimum time gap between log messages (ms)
* @param lockWarningThresholdMs threshold for lock warning (ms)
* @param clock monotonic timer instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,startLockTiming,org.apache.hadoop.util.InstrumentedLock:startLockTiming(),177,179,"/**
* Records the current time when starting lock timing.
*/",* Starts timing for the instrumented lock.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,unlock,org.apache.hadoop.util.InstrumentedReadLock:unlock(),65,75,"/**
* Releases the lock and optionally reports usage metrics.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,startLockTiming,org.apache.hadoop.util.InstrumentedReadLock:startLockTiming(),81,86,"/**
* Records lock timing information when read-only lock is acquired.
*/","* Starts timing for the instrumented read lock.
   * It records the time to ThreadLocal.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.RetryInvocationHandler:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",327,330,"/**
* Initializes a RetryInvocationHandler with given FailoverProxyProvider and RetryPolicy.
* @param proxyProvider provider for failover proxies
* @param retryPolicy policy for retrying failed invocations
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,org.apache.hadoop.io.retry.FailoverProxyProvider,java.util.Map,org.apache.hadoop.io.retry.RetryPolicy)",100,110,"/**
* Creates a dynamic proxy instance for the specified interface.
* @param iface target interface
* @param proxyProvider failover proxy provider
* @param methodNameToPolicyMap map of method names to retry policies
* @param defaultPolicy default retry policy
* @return proxy object or null if creation fails
*/","* Create a proxy for an interface of implementations of that interface using
   * the given {@link FailoverProxyProvider} and the a set of retry policies
   * specified by method name. If no retry policy is defined for a method then a
   * default of {@link RetryPolicies#TRY_ONCE_THEN_FAIL} is used.
   * 
   * @param iface the interface that the retry will implement
   * @param proxyProvider provides implementation instances whose methods should be retried
   * @param methodNameToPolicyMap map of method names to retry policies
   * @param defaultPolicy defaultPolicy.
   * @param <T> T.
   * @return the retry proxy",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,failover,"org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:failover(long,java.lang.reflect.Method,int)",217,229,"/**
* Performs a failover when expected count matches actual count.
* @param expectedFailoverCount expected failover count
* @param method current method being executed
* @param callId current call ID
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,log,"org.apache.hadoop.io.retry.RetryInvocationHandler:log(java.lang.reflect.Method,boolean,int,int,long,java.lang.Exception)",398,430,"/**
* Logs an exception and retry details after invoking a method on a proxy.
* @param method the invoked method
* @param isFailover whether this is a failover attempt
* @param failovers number of failover attempts made so far
* @param retries current retry count
* @param delay time to sleep before retrying (in ms)
* @param ex the exception that occurred
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/LossyRetryInvocationHandler.java,invokeMethod,"org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])",48,65,"/**
* Invokes a method with optional retries on failure.
* @param method method to invoke
* @param args method arguments
* @return result of invocation or null if dropped
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,hashCode,org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:hashCode(),150,153,"/**
* Returns hash code based on underlying linear random retry instance.
*/","* Similarly, remoteExceptionToRetry is ignored as part of hashCode since it
     * does not affect connection failure handling.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,equals,org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:equals(java.lang.Object),135,144,"/**
* Determines whether the current object is equal to another WrapperRetryPolicy.
* @param obj the object to compare with, must be of type WrapperRetryPolicy
*/","* remoteExceptionToRetry is ignored as part of equals since it does not
     * affect connection failure handling.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:shouldRetry(java.lang.Exception,int,int,boolean)",283,291,"/**
* Determines retry decision based on exception and retry configuration.
* @param e the exception that occurred
* @param retries current attempt number
* @param failovers total attempts allowed
* @param isIdempotentOrAtMostOnce whether operation is idempotent or at-most-once
* @return RetryAction containing retry decision and sleep time",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,mayThrow,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:mayThrow(java.util.List),314,324,"/**
* Logs or re-throws IOExceptions if replication count is below threshold.
* @param ioExceptions list of IOExceptions to process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:getFileStatus(org.apache.hadoop.fs.Path),875,915,"/**
* Retrieves file status information from a list of MRNfly nodes.
* @param f the file path to check
* @return FileStatus object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MultipleIOException.java,build,org.apache.hadoop.io.MultipleIOException$Builder:build(),83,85,"/**
* Creates an IOException instance from collected exceptions. 
* @return IOException instance containing aggregated exceptions.","* @return null if nothing is added to this builder;
     *         otherwise, return an {@link IOException}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$DataIndex:<init>(java.lang.String),878,883,"/**
* Initializes DataIndex with default compression algorithm.
* @param defaultCompressionAlgorithmName name of the compression algorithm to use
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getSupportedCompressionAlgorithms,org.apache.hadoop.io.file.tfile.TFile:getSupportedCompressionAlgorithms(),198,200,"/**
* Returns an array of supported compression algorithms.
* @return Array of algorithm names (e.g. ""gzip"", ""deflate"")
*/","* Get names of supported compression algorithms. The names are acceptable by
   * TFile.Writer.
   * 
   * @return Array of strings, each represents a supported compression
   *         algorithm. Currently, the following compression algorithms are
   *         supported.
   *         <ul>
   *         <li>""none"" - No compression.
   *         <li>""lzo"" - LZO compression.
   *         <li>""gz"" - GZIP compression.
   *         </ul>",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressionName,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getCompressionName(),576,578,"/**
* Returns compression name based on current R-Block state.
* @return Compression name as a string or null if not applicable
*/","* Get the name of the compression algorithm used to compress the block.
       * 
       * @return name of the compression algorithm.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,register,"org.apache.hadoop.io.file.tfile.BCFile$Writer$MetaBlockRegister:register(long,long,long)",451,455,"/**
* Registers a block of data with specified compression algorithm and region.
* @param raw raw data value
* @param begin start position of the block
* @param end end position of the block
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,readString,org.apache.hadoop.io.file.tfile.Utils:readString(java.io.DataInput),276,282,"/**
* Reads a string from the input stream.
* @param in DataInput stream to read from
* @return String value or null if length indicates end of data
*/","* Read a String as a VInt n, followed by n Bytes in Text format.
   * 
   * @param in
   *          The input stream.
   * @return The string
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:<init>(java.io.DataInput),2302,2307,"/**
* Reads a TFileIndexEntry object from the given DataInput stream.
* @param in input stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,readLength,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:readLength(),102,109,"/**
* Reads and interprets a variable-length integer from the input stream.
* Updates 'remain' with the value and sets 'lastChunk' flag accordingly. 
* @throws IOException if an I/O error occurs during reading
*/","* Reading the length of next chunk.
     * 
     * @throws java.io.IOException
     *           when no more data is available.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:<init>(java.lang.String),2051,2057,"/**
* Constructs a TFileMeta object with the specified comparator and initializes its properties. 
* @param comparator string value for comparison (null if default comparator is used)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,makeComparator,org.apache.hadoop.io.file.tfile.TFile:makeComparator(java.lang.String),176,178,"/**
* Creates a comparator based on file meta data.
* @param name file meta data field to compare
*/","* Make a raw comparator from a string name.
   * 
   * @param name
   *          Comparator name
   * @return A RawComparable comparator.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,writeChunk,"org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:writeChunk(byte[],int,int,boolean)",253,266,"/**
* Writes a chunk of bytes to the output stream.
* @param chunk data to write
* @param offset starting position in the chunk
* @param len length of the chunk
* @param last true if this is the final chunk
*/","* Write out a chunk.
     * 
     * @param chunk
     *          The chunk buffer.
     * @param offset
     *          Offset to chunk buffer for the beginning of chunk.
     * @param len
     * @param last
     *          Is this the last call to flushBuffer?",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,writeBufData,"org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:writeBufData(byte[],int,int)",279,287,"/**
* Writes buffered data to output stream.
* @param data buffer containing data to write
* @param offset starting position in the buffer
* @param len length of data to write
*/","* Write out a chunk that is a concatenation of the internal buffer plus
     * user supplied data. This will never be the last block.
     * 
     * @param data
     *          User supplied data buffer.
     * @param offset
     *          Offset to user data buffer.
     * @param len
     *          User data buffer size.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,write,org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:write(java.io.DataOutput),2335,2339,"/**
* Writes KV entry data to output stream.
* @param out DataOutput stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,<init>,"org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:<init>(java.io.DataOutputStream,int)",377,382,"/**
* Initializes SingleChunkEncoder with output stream and chunk size.
* @param out DataOutputStream for writing data
* @param size Chunk size in bytes
*/","* Constructor.
     * 
     * @param out
     *          the underlying output stream.
     * @param size
     *          The total # of bytes to be written as a single chunk.
     * @throws java.io.IOException
     *           if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getEntryComparator,org.apache.hadoop.io.file.tfile.TFile$Reader:getEntryComparator(),925,944,"/**
* Returns a comparator for Entries, throwing an exception if not sorted.
* @return Comparator for Scanner.Entry objects
*/","* Get a Comparator object to compare Entries. It is useful when you want
     * stores the entries in a collection (such as PriorityQueue) and perform
     * sorting or comparison among entries based on the keys without copying out
     * the key.
     * 
     * @return An Entry Comparator..",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareKeys,"org.apache.hadoop.io.file.tfile.TFile$Reader:compareKeys(byte[],int,int,byte[],int,int)",1007,1012,"/**
* Compares two key segments in a sorted context.
* @param a first byte array
* @param o1 offset into first array
* @param l1 length of first segment
* @param b second byte array
* @param o2 offset into second array
* @param l2 length of second segment
* @return comparison result or throws exception if context is unsorted
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareKeys,"org.apache.hadoop.io.file.tfile.TFile$Reader:compareKeys(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1014,1019,"/**
* Compares two keys in a sorted context.
* @param a first key to compare
* @param b second key to compare
* @return comparison result (negative, zero, or positive)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,setFirstKey,"org.apache.hadoop.io.file.tfile.TFile$TFileIndex:setFirstKey(byte[],int,int)",2250,2253,"/**
* Sets the first key to a specified byte array slice.
* @param key source byte array
* @param offset starting index of slice in key
* @param length number of bytes to copy from key
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLastKey,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLastKey(),2255,2260,"/**
* Retrieves and returns the last key from the index.
* @return RawComparable object representing the last key, or null if empty.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,atEnd,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:atEnd(),1588,1590,"/**
* Checks if current location is at or past end location.
* @return true if at end, false otherwise
*/","* Is cursor at the end location?
       * 
       * @return true if the cursor is at the end location.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLocationNear,org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationNear(long),1031,1035,"/**
* Retrieves location near specified offset.
* @param offset long value to search for nearby location
* @return Location object at nearest block index or null if not found
*/","* Get the location pointing to the beginning of the first key-value pair in
     * a compressed block whose byte offset in the TFile is greater than or
     * equal to the specified offset.
     * 
     * @param offset
     *          the user supplied offset.
     * @return the location to the corresponding entry; or end() if no such
     *         entry exists.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,clone,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:clone(),759,762,"/**
* Creates a deep copy of this location.
* @return cloned Location object
*/",* @see java.lang.Object#clone(),,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLocationByRecordNum,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLocationByRecordNum(long),2238,2242,"/**
* Retrieves location information by record number.
* @param recNum unique record identifier
* @return Location object with index and offset or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),713,715,"/**
* Copies another location's coordinates to this instance.
* @param other Location object to copy from
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getValue,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(byte[]),1840,1842,"/**
* Reads an integer value from a byte array buffer.
* @param buf input byte array
*/","* Copy value into user-supplied buffer. User supplied buffer must be
         * large enough to hold the whole value. The value part of the key-value
         * pair pointed by the current cursor is not cached and can only be
         * examined once. Calling any of the following functions more than once
         * without moving the cursor will result in exception:
         * {@link #getValue(byte[])}, {@link #getValue(byte[], int)},
         * {@link #getValueStream}.
         *
         * @param buf buf.
         * @return the length of the value. Does not require
         *         isValueLengthKnown() to be true.
         * @throws IOException raised on errors performing I/O.
         *",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,set,org.apache.hadoop.io.UTF8:set(java.lang.String),96,118,"/**
* Sets the string value, truncating if too long and computing UTF-8 byte length.
* @param string input string to set
*/","* Set to contain the contents of a string.
   * @param string input string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,toByteArray,org.apache.hadoop.io.WritableUtils:toByteArray(org.apache.hadoop.io.Writable[]),461,472,"/**
* Converts one or more Writable objects to a byte array.
* @param writables Writable objects to be converted
* @return Byte array representation of the Writables
*/","* Convert writables to a byte array.
   * @param writables input writables.
   * @return ByteArray.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,getBytes,org.apache.hadoop.io.UTF8:getBytes(java.lang.String),238,249,"/**
* Converts a String to a byte array.
* @param string input string
*/","* @return Convert a string to a UTF-8 encoded byte array.
   * @see String#getBytes(String)
   * @param string input string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeBuffer,org.apache.hadoop.io.SequenceFile$BlockCompressWriter:writeBuffer(org.apache.hadoop.io.DataOutputBuffer),1623,1635,"/**
* Writes compressed data to output stream.
* @param uncompressedDataBuffer input buffer with uncompressed data
*/",Workhorse to check and write out compressed data/lengths,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,byteArrayForBloomKey,org.apache.hadoop.io.BloomMapFile:byteArrayForBloomKey(org.apache.hadoop.io.DataOutputBuffer),71,79,"/**
* Creates a trimmed byte array from the input buffer.
* @param buf DataOutputBuffer instance
* @return Trimmed byte array or new byte array if original length mismatched
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,lessThan,"org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:lessThan(java.lang.Object,java.lang.Object)",3537,3548,"/**
* Compares two SegmentDescriptors based on their keys.
* @param a first descriptor to compare
* @param b second descriptor to compare
* @return true if 'a' is less than 'b', false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,copy,org.apache.hadoop.io.AbstractMapWritable:copy(org.apache.hadoop.io.Writable),125,142,"/**
* Copies the contents of another Writable object into this instance.
* @param other source object to copy from
*/","* Used by child copy constructors.
   * @param other other.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$RecordCompressWriter:append(java.lang.Object,java.lang.Object)",1545,1575,"/**
* Appends a serialized key-value pair to the output stream.
* @param key unique identifier (must be of type {@link #keyClass})
* @param val associated value (must be of type {@link #valClass}) 
* @throws IOException if serialization or compression fails
*/",Append a key/value pair.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,toString,org.apache.hadoop.io.DefaultStringifier:toString(java.lang.Object),83,90,"/**
* Serializes an object to a base64-encoded string.
* @param obj object to serialize
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,checkKey,org.apache.hadoop.io.MapFile$Writer:checkKey(org.apache.hadoop.io.WritableComparable),418,429,"/**
* Validates and updates the last key, ensuring it remains well-ordered.
* @param key WritableComparable to verify and replace lastKey if necessary
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/TokenIdentifier.java,getBytes,org.apache.hadoop.security.token.TokenIdentifier:getBytes(),60,68,"/**
* Converts object to byte array representation.
* @return Byte array containing object data
*/","* Get the bytes for the token identifier
   * @return the bytes of the identifier",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,encodeWritable,org.apache.hadoop.security.token.Token:encodeWritable(org.apache.hadoop.io.Writable),340,347,"/**
* Encodes a Writable object into a base64-encoded string.
* @param obj the Writable object to encode
*/","* Generate a string with the url-quoted base64 encoded serialized form
   * of the Writable.
   * @param obj the object to serialize
   * @return the encoded string
   * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,moveData,org.apache.hadoop.util.ReflectionUtils$CopyInCopyOutBuffer:moveData(),314,316,"/**
 * Resets the input buffer with data from the output buffer.
 */",* Move the data from the output buffer to the input buffer.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNSDomainNameResolver.java,getAllResolvedHostnameByDomainName,"org.apache.hadoop.net.DNSDomainNameResolver:getAllResolvedHostnameByDomainName(java.lang.String,boolean)",64,80,"/**
* Retrieves an array of resolved hostnames by domain name.
* @param domainName the target domain
* @param useFQDN whether to use fully qualified domain names
* @return array of hostnames or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getDistanceByPath,"org.apache.hadoop.net.NetworkTopology:getDistanceByPath(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",376,400,"/**
* Calculates distance between two nodes based on their path components.
* @param node1 first Node object
* @param node2 second Node object
* @return calculated distance or MAX_VALUE if one node is null
*/","Return the distance between two nodes by comparing their network paths
   * without checking if they belong to the same ancestor node by reference.
   * It is assumed that the distance from one node to its parent is 1
   * The distance between two nodes is calculated by summing up their distances
   * to their closest common ancestor.
   * @param node1 one node
   * @param node2 another node
   * @return the distance between node1 and node2",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,equals,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:equals(java.lang.Object),108,112,"/**
* Delegates equality check to superclass implementation.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,equals,org.apache.hadoop.net.InnerNodeImpl:equals(java.lang.Object),316,319,"/**
* Calls superclass's implementation of equals() to compare this instance with another. 
* @param to object to be compared with this instance 
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,hashCode,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:hashCode(),114,118,"/**
* Returns the hash code of this object.
* This implementation simply calls the superclass's hashCode method. 
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,hashCode,org.apache.hadoop.net.InnerNodeImpl:hashCode(),311,314,"/**
* Returns hash code based on superclass implementation.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getNodeForNetworkLocation,org.apache.hadoop.net.NetworkTopology:getNodeForNetworkLocation(org.apache.hadoop.net.Node),193,195,"/**
* Retrieves the Node instance associated with the specified network location.
* @param node Node instance containing the network location
*/","* Return a reference to the node given its string representation.
   * Default implementation delegates to {@link #getNode(String)}.
   * 
   * <p>To be overridden in subclasses for specific NetworkTopology 
   * implementations, as alternative to overriding the full {@link #add(Node)}
   *  method.
   * 
   * @param node The string representation of this node's network location is
   * used to retrieve a Node object. 
   * @return a reference to the node; null if the node is not in the tree
   * 
   * @see #add(Node)
   * @see #getNode(String)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getLeaves,org.apache.hadoop.net.NetworkTopology:getLeaves(java.lang.String),643,655,"/**
* Retrieves a list of leaf nodes within the specified scope.
* @param scope unique identifier for the node subtree
* @return List of Node objects or empty if no leaves found
*/","return leaves in <i>scope</i>
   * @param scope a path string
   * @return leaves nodes under specific scope",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,countNumOfAvailableNodes,"org.apache.hadoop.net.NetworkTopology:countNumOfAvailableNodes(java.lang.String,java.util.Collection)",664,711,"/**
* Counts the number of available nodes within a given scope, excluding specified nodes.
* @param scope the node path or ""~nodePath"" to indicate exclusion
* @param excludedNodes collection of nodes to exclude from count
* @return total count of available nodes in the scope
*/","return the number of leaves in <i>scope</i> but not in <i>excludedNodes</i>
   * if scope starts with ~, return the number of nodes that are not
   * in <i>scope</i> and <i>excludedNodes</i>; 
   * @param scope a path string that may start with ~
   * @param excludedNodes a list of nodes
   * @return number of available nodes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,interRemoveNodeWithEmptyRack,org.apache.hadoop.net.NetworkTopology:interRemoveNodeWithEmptyRack(org.apache.hadoop.net.Node),1104,1122,"/**
* Removes a node from the graph and updates empty rack counts if applicable.
* @param node Node object to be removed
*/","* Internal function for update empty rack number
   * for remove or decommission a node.
   * @param node node to be removed; can be null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistance,"org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer,boolean)",970,1000,"/**
* Sorts nodes by distance, using a secondary sort if necessary.
* @param reader node used to calculate weights
* @param nodes array of nodes to be sorted
* @param activeLen length of the active portion of the array
* @param secondarySort optional function for secondary sorting
* @param nonDataNodeReader whether to use network location or not
*/","* Sort nodes array by network distance to <i>reader</i>.
   * <p>
   * As an additional twist, we also randomize the nodes at each network
   * distance. This helps with load balancing when there is data skew.
   * And it helps choose node with more fast storage type.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array
   * @param nonDataNodeReader True if the reader is not a datanode",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,recommissionNode,org.apache.hadoop.net.NetworkTopology:recommissionNode(org.apache.hadoop.net.Node),1040,1055,"/**
* Recommissions a node by removing it from the decommissioned set and re-adding it with an empty rack.
* @param node Node object to recommission
*/","* Update empty rack number when add a node like recommission.
   * @param node node to be added; can be null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,"org.apache.hadoop.net.SocketInputStream$Reader:<init>(java.nio.channels.ReadableByteChannel,long)",50,53,"/**
* Initializes a new Reader instance from a ReadableByteChannel.
* @param channel source byte channel
* @param timeout operation timeout in milliseconds
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,<init>,"org.apache.hadoop.net.SocketOutputStream$Writer:<init>(java.nio.channels.WritableByteChannel,long)",55,58,"/**
* Initializes a Writer with a WritableByteChannel and timeout.
* @param channel channel to write to
* @param timeout timeout in milliseconds for the underlying channel
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,write,org.apache.hadoop.net.SocketOutputStream:write(int),101,109,"/**
* Writes a single byte to the underlying output stream.
* @param b the byte to be written
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,transferToFully,"org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int)",264,267,"/**
* Transfers data from a FileChannel to fully fill a buffer.
* @param fileCh the source FileChannel
* @param position the starting position in the channel
* @param count the number of bytes to transfer
*/","* Call
   * {@link #transferToFully(FileChannel, long, int, LongWritable, LongWritable)
   * }
   * with null <code>waitForWritableTime</code> and <code>transferToTime</code>.
   *
   * @param fileCh input fileCh.
   * @param position input position.
   * @param count input count.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/CachedDNSToSwitchMapping.java,resolve,org.apache.hadoop.net.CachedDNSToSwitchMapping:resolve(java.util.List),106,125,"/**
* Resolves a list of host names to IP addresses.
* @param names list of host names to resolve
* @return List of IP addresses or empty list if input is empty
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,wrapException,"org.apache.hadoop.net.NetUtils:wrapException(java.lang.String,int,java.lang.String,int,java.io.IOException)",850,949,"/**
* Wraps an IOException with a detailed error message based on the type of exception.
* @param destHost destination host
* @param destPort destination port
* @param localHost local host
* @param localPort local port
* @param exception original IOException to wrap
* @return wrapped IOException with additional details
*/","* Take an IOException , the local host port and remote host port details and
   * return an IOException with the input exception as the cause and also
   * include the host details. The new exception provides the stack trace of the
   * place where the exception is thrown and some extra diagnostics information.
   * If the exception is of type BindException, ConnectException,
   * UnknownHostException, SocketTimeoutException or has a String constructor,
   * return a new one of the same type; Otherwise return an IOException.
   *
   * @param destHost target host (nullable)
   * @param destPort target port
   * @param localHost local host (nullable)
   * @param localPort local port
   * @param exception the caught exception.
   * @return an exception to throw",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,<init>,org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:<init>(),144,144,"/**
* Initializes an empty RawScriptBasedMappingWithDependency object.","* Constructor. The mapping is not ready to use until
     * {@link #setConf(Configuration)} has been called",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,<init>,org.apache.hadoop.net.TableMapping:<init>(),63,65,"/**
* Initializes a new instance of the TableMapping class.
* Uses a RawTableMapping as its base mapping.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.net.DNSToSwitchMapping),95,97,"/**
* Constructs a ScriptBasedMapping instance from a DNSToSwitchMapping.
* @param rawMap underlying mapping data
*/","* Create an instance from the given raw mapping
   * @param rawMap raw DNSTOSwithMapping",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,<init>,org.apache.hadoop.net.InnerNodeImpl:<init>(java.lang.String),48,50,"/**
 * Initializes an InnerNodeImpl instance with the given file system path.
 */","* Construct an InnerNode from a path-like string.
   * @param path input path.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.ChRootedFileSystem)",99,102,"/**
* Initializes a new NflyNode instance with specified host name and rack name.
* @param hostName the hostname of the node
* @param rackName the rack name of the node
* @param fs ChRootedFileSystem object for file system operations
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,<init>,"org.apache.hadoop.net.InnerNodeImpl:<init>(java.lang.String,java.lang.String,org.apache.hadoop.net.InnerNode,int)",60,63,"/**
* Initializes an inner node with specified details.
* @param name inner node's name
* @param location inner node's location
* @param parent parent node (can be null)
* @param level nesting level of the node
*/","* Construct an InnerNode
   * from its name, its network location, its parent, and its level.
   * @param name input name.
   * @param location input location.
   * @param parent input parent.
   * @param level input level.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,kick,org.apache.hadoop.net.unix.DomainSocketWatcher:kick(),359,374,"/**
* Signals the user's disconnection by sending a notification and marking as kicked.
*/",* Wake up the DomainSocketWatcher thread.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,handle,org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler:handle(org.apache.hadoop.net.unix.DomainSocket),103,130,"/**
* Handles a DomainSocket by attempting to read from it.
* @param sock the socket to handle
* @return true if the socket was successfully handled, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,bindAndListen,org.apache.hadoop.net.unix.DomainSocket:bindAndListen(java.lang.String),191,200,"/**
* Binds and listens on a socket at the specified path.
* @param path socket path
* @return newly created DomainSocket instance or throws exception if invalid
*/","* Create a new DomainSocket listening on the given path.
   *
   * @param path         The path to bind and listen on.
   * @return             The new DomainSocket.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,socketpair,org.apache.hadoop.net.unix.DomainSocket:socketpair(),210,216,"/**
* Creates a pair of sockets using the underlying operating system's 
* socketpair facility, returning two connected sockets.
*/","* Create a pair of UNIX domain sockets which are connected to each other
   * by calling socketpair(2).
   *
   * @return                An array of two UNIX domain sockets connected to
   *                        each other.
   * @throws IOException    on error.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,connect,org.apache.hadoop.net.unix.DomainSocket:connect(java.lang.String),255,261,"/**
* Establishes a connection to the specified DomainSocket at the given path.
* @param path URL or file path of the socket
*/","* Create a new DomainSocket connected to the given path.
   *
   * @param path              The path to connect to.
   * @throws IOException      If there was an I/O error performing the connect.
   *
   * @return                  The new DomainSocket.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,sendCallbackAndRemove,"org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallbackAndRemove(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)",435,440,"/**
* Sends callback and removes entry from map.
* @param caller callback sender
* @param entries TreeMap of entries to remove from
* @param fdSet FD set
* @param fd file descriptor to remove","* Send callback, and if the domain socket was closed as a result of
   * processing, then also remove the entry for the file descriptor.
   *
   * @param caller reason for call
   * @param entries mapping of file descriptor to entry
   * @param fdSet set of file descriptors
   * @param fd file descriptor",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,isOpen,org.apache.hadoop.net.unix.DomainSocket$DomainChannel:isOpen(),592,595,"/**
* Returns whether the underlying socket is open.
* @return true if the socket is open, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket$DomainChannel:close(),597,600,"/**
* Closes the underlying socket connection.
* @throws IOException if an I/O error occurs during shutdown
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:close(),558,561,"/**
* Closes this domain socket.
* Calls underlying socket's close operation. 
* @throws IOException if closing fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,close,org.apache.hadoop.net.unix.DomainSocketWatcher:close(),266,284,"/**
* Closes this instance, releasing resources and signaling select thread termination. 
* @throws IOException if an I/O error occurs during closure
*/","* Close the DomainSocketWatcher and wait for its thread to terminate.
   *
   * If there is more than one close, all but the first will be ignored.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:close(),547,550,"/**
* Closes the underlying domain socket connection.
* @throws IOException if an I/O error occurs during closure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,get,org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:get(java.nio.channels.SelectableChannel),387,403,"/**
* Retrieves a SelectorInfo object associated with the given channel.
* @param channel SelectableChannel instance
* @return SelectorInfo object or newly created one if not found
*/","* Takes one selector from end of LRU list of free selectors.
     * If there are no selectors awailable, it creates a new selector.
     * Also invokes trimIdleSelectors(). 
     * 
     * @param channel
     * @return 
     * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,release,org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:release(org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool$SelectorInfo),411,417,"/**
* Adds a SelectorInfo to the end of its provider's queue, updating last activity time.
* @param info SelectorInfo object containing user identifier and provider information
*/","* puts selector back at the end of LRU list of free selectos.
     * Also invokes trimIdleSelectors().
     * 
     * @param info",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,getLeaf,"org.apache.hadoop.net.InnerNodeImpl:getLeaf(int,org.apache.hadoop.net.Node)",253,300,"/**
* Fetches a leaf node by index, excluding a specific node if present.
* @param leafIndex zero-based index of the desired leaf node
* @param excludedNode node to exclude from search results
* @return Leaf Node object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,remove,org.apache.hadoop.net.InnerNodeImpl:remove(org.apache.hadoop.net.Node),189,234,"/**
* Removes a node from the tree structure.
* @param n Node to be removed (must be an ancestor)
* @return true if node was successfully removed, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getIPs,org.apache.hadoop.net.DNS:getIPs(java.lang.String),152,155,"/**
* Retrieves all IP addresses associated with the specified network interface.
* @param strInterface name of the network interface
*/","* @return Like {@link DNS#getIPs(String, boolean)}, but returns all
   * IPs associated with the given interface and its subinterfaces.
   *
   * @param strInterface input strInterface.
   * @throws UnknownHostException
   * If no IP address for the local host could be found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getHosts,"org.apache.hadoop.net.DNS:getHosts(java.lang.String,java.lang.String,boolean)",248,277,"/**
* Retrieves hostnames associated with the given network interface.
* @param strInterface network interface name
* @param nameserver DNS server to use (optional)
* @param tryfallbackResolution whether to attempt fallback resolution if no hostname found
* @return array of hostnames or empty array if unknown
*/","* Returns all the host names associated by the provided nameserver with the
   * address bound to the specified network interface
   *
   * @param strInterface
   *            The name of the network interface or subinterface to query
   *            (e.g. eth0 or eth0:0)
   * @param nameserver
   *            The DNS host name
   * @param tryfallbackResolution
   *            if true and if reverse DNS resolution fails then attempt to
   *            resolve the hostname with
   *            {@link InetAddress#getCanonicalHostName()} which includes
   *            hosts file resolution.
   * @return A string vector of all host names associated with the IPs tied to
   *         the specified interface
   * @throws UnknownHostException if the given interface is invalid",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,read,org.apache.hadoop.net.SocketInputStream:read(),112,127,"/**
* Reads a single byte from the underlying stream.
* @throws IOException on read failure or unexpected result
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,listBeans,"org.apache.hadoop.jmx.JMXJsonServlet:listBeans(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)",235,328,"/**
* Lists beans matching a JMX query and writes their attributes to the JSON response.
* @param qry JMX query object
* @param attribute optional attribute name to include in output
* @param response HTTP response object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseArguments,org.apache.hadoop.log.LogLevel$CLI:parseArguments(java.lang.String[]),141,170,"/**
* Parses command-line arguments for level and protocol settings.
* @param args array of command-line arguments
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,"org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.util.Map)",117,132,"/**
* Prints program usage and help entries to the specified output stream.
* @param pStr output stream
* @param helpEntries map of command names to their usage information
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,<init>,org.apache.hadoop.log.LogThrottlingHelper:<init>(long),145,147,"/**
* Initializes the log throttling helper with the specified minimum log period in milliseconds.
* @param minLogPeriodMs minimum time interval between logs (in ms)
*/","* Create a log helper without any primary recorder.
   *
   * @see #LogThrottlingHelper(long, String)
   * @param minLogPeriodMs input minLogPeriodMs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,record,org.apache.hadoop.log.LogThrottlingHelper:record(double[]),195,197,"/**
* Records log action with current timestamp and provided values.
* @param values variable number of double values to be recorded
*/","* Record some set of values at the current time into this helper. Note that
   * this does <i>not</i> actually write information to any log. Instead, this
   * will return a LogAction indicating whether or not the caller should write
   * to its own log. The LogAction will additionally contain summary information
   * about the values specified since the last time the caller was expected to
   * write to its log.
   *
   * <p>Specifying multiple values will maintain separate summary statistics
   * about each value. For example:
   * <pre>{@code
   *   helper.record(1, 0);
   *   LogAction action = helper.record(3, 100);
   *   action.getStats(0); // == 2
   *   action.getStats(1); // == 50
   * }</pre>
   *
   * @param values The values about which to maintain summary information. Every
   *               time this method is called, the same number of values must
   *               be specified.
   * @return A LogAction indicating whether or not the caller should write to
   *         its log.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,getEvent,org.apache.hadoop.http.ProfileServlet:getEvent(javax.servlet.http.HttpServletRequest),366,373,"/**
* Retrieves an Event object from the request, using the ""event"" parameter.
* If present, it's used to fetch a specific Event; otherwise, returns CPU Event. 
* @return Event object or CPU Event if not specified in request
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HtmlQuoting.java,main,org.apache.hadoop.http.HtmlQuoting:main(java.lang.String[]),215,224,"/**
* Demonstrates HTML quoting and unquoting of command-line arguments.
* @param args array of input arguments
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getParameter,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameter(java.lang.String),1804,1808,"/**
* Extracts and sanitizes a request parameter by name.
* @param name the parameter name to fetch
* @return the sanitized parameter value or null if not present
*/",* Unquote the name and quote the value.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getParameterValues,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterValues(java.lang.String),1810,1822,"/**
* Retrieves quoted parameter values by name.
* @param name unquoted parameter name
* @return array of quoted parameter values or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getParameterMap,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterMap(),1824,1838,"/**
* Returns a parameter map with quoted HTML characters.
* @return Map of parameter names to arrays of quoted values
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getRequestURL,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getRequestURL(),1844,1848,"/**
* Constructs and returns the request URL with HTML quoting.
*/","* Quote the url so that users specifying the HOST HTTP header
       * can't inject attacks.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getServerName,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getServerName(),1854,1857,"/**
 * Returns the server name from the raw request after HTML quoting.
 * @return server name as quoted string or null if not found
 */","* Quote the server name so that users specifying the HOST HTTP header
       * can't inject attacks.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addAsyncProfilerServlet,"org.apache.hadoop.http.HttpServer2:addAsyncProfilerServlet(org.eclipse.jetty.server.handler.ContextHandlerCollection,org.apache.hadoop.conf.Configuration)",797,816,"/**
* Adds asynchronous profiler servlet with optional output directory.
* @param contexts collection of context handlers
* @param conf configuration object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addNoCacheFilter,org.apache.hadoop.http.HttpServer2:addNoCacheFilter(org.eclipse.jetty.servlet.ServletContextHandler),888,891,"/**
* Adds a no-cache filter to the servlet context.
* @param ctxt ServletContextHandler instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,makeConfigurationChangeMonitor,"org.apache.hadoop.http.HttpServer2$Builder:makeConfigurationChangeMonitor(long,org.eclipse.jetty.util.ssl.SslContextFactory$Server)",634,663,"/**
* Schedules periodic monitoring and reloading of key stores.
* @param reloadInterval interval between reloads in milliseconds
* @param sslContextFactory Jetty SSL context factory
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileMonitoringTimerTask.java,<init>,"org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<init>(java.nio.file.Path,java.util.function.Consumer,java.util.function.Consumer)",61,64,"/**
* Initializes file monitoring timer task with single file path.
* @param filePath absolute file path to monitor
* @param onFileChange callback invoked when file changes
* @param onChangeFailure callback invoked on failure to monitor file
*/","* See {@link #FileMonitoringTimerTask(List, Consumer, Consumer)}.
   *
   * @param filePath The file to monitor.
   * @param onFileChange What to do when the file changes.
   * @param onChangeFailure What to do when <code>onFileChange</code>
   *                        throws an exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,close,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:close(),877,892,"/**
* Closes resources and checks for errors upon exit.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRatesWithAggregation,org.apache.hadoop.metrics2.lib.MetricsRegistry:newRatesWithAggregation(java.lang.String),331,337,"/**
* Creates and registers a new mutable rate aggregation object.
* @param name unique metric identifier
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,add,"org.apache.hadoop.metrics2.lib.MetricsRegistry:add(java.lang.String,org.apache.hadoop.metrics2.lib.MutableMetric)",348,351,"/**
* Adds a new mutable metric to the map with a given name.
* @param name unique metric identifier
* @param metric Metric object to be added
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsFactory.java,getAnnotatedMetricsFactory,org.apache.hadoop.metrics2.lib.DefaultMetricsFactory:getAnnotatedMetricsFactory(),33,35,"/**
* Returns an instance of MutableMetricsFactory with annotations enabled.
* @return AnnotatedMutableMetricsFactory object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,flush,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:flush(),863,875,"/**
* Flushes the current output stream, ensuring data is written to disk.
* @throws MetricsException if flushing fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,currentConfig,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:currentConfig(),348,358,"/**
* Returns the current configuration as a string.
* @return serialized configuration data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getPluginLoader,org.apache.hadoop.metrics2.impl.MetricsConfig:getPluginLoader(),228,264,"/**
* Retrieves the ClassLoader for loading plugins.
* @return ClassLoader instance or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,toString,org.apache.hadoop.metrics2.impl.MetricsConfig:toString(org.apache.commons.configuration2.Configuration),287,298,"/**
* Converts Configuration object to a human-readable string representation.
* @param c the Configuration object to convert
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,consume,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:consume(org.apache.hadoop.metrics2.impl.MetricsBuffer),172,200,"/**
* Consumes metrics from a buffer, filtering and processing records.
* @param buffer MetricsBuffer object containing metrics to process
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,tag,"org.apache.hadoop.metrics2.MetricStringBuilder:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",81,84,"/**
 * Adds a key-value pair to the metrics record using the provided information and value. 
 * @param info MetricsInfo object containing metadata about the metric
 * @param value string value associated with the metric
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,add,org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.AbstractMetric),91,95,"/**
* Adds a custom abstract metric to the metrics record.
* @param metric AbstractMetric object to be added
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricStringBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",102,105,"/**
* Adds a counter with specified value to the metrics record.
* @param info metrics information
* @param value counter value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricStringBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",107,110,"/**
* Adds a counter metric to the builder.
* @param info Metric information
* @param value Counter value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",112,115,"/**
* Adds gauge metrics with specified value to builder.
* @param info metrics information
* @param value gauge value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",117,120,"/**
* Adds gauge metric with specified value and metrics info.
* @param info metrics information
* @param value gauge value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",122,125,"/**
* Adds gauge metric with specified value.
* @param info metrics information
* @param value current gauge value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",127,130,"/**
* Adds gauge metric with specified value and metrics info.
* @param info metrics information
* @param value gauge value to record
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MetricsCache.java,update,org.apache.hadoop.metrics2.util.MetricsCache:update(org.apache.hadoop.metrics2.MetricsRecord),184,186,"/**
* Updates an existing MetricsRecord.
* @param mr record to be updated
* @return updated Record object
*/","* Update the cache and return the current cache record
   * @param mr the update record
   * @return the updated cache record",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,flush,org.apache.hadoop.metrics2.sink.GraphiteSink:flush(),110,122,"/**
* Flushes metrics to Graphite, handling exceptions and closing the connection if an error occurs. 
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,write,org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:write(java.lang.String),167,174,"/**
* Writes a message to the output stream after ensuring connection is established.
* @param msg message to be written
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,init,org.apache.hadoop.metrics2.sink.GraphiteSink:init(org.apache.commons.configuration2.SubsetConfiguration),54,68,"/**
* Initializes the component with Graphite host and metric configuration.
* @param conf SubsetConfiguration containing Graphite settings
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java,writeMetrics,org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:writeMetrics(java.io.Writer),111,163,"/**
* Writes metrics to the provided writer.
* @param writer output stream for writing metrics
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,consume,org.apache.hadoop.metrics2.impl.SinkQueue:consume(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer),65,75,"/**
* Consumes data by executing the provided Consumer function and dequeuing afterwards.
* @param consumer Consumer function to execute on fetched data
*/","* Consume one element, will block if queue is empty
   * Only one consumer at a time is allowed
   * @param consumer  the consumer callback object",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,consumeAll,org.apache.hadoop.metrics2.impl.SinkQueue:consumeAll(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer),82,94,"/**
* Consumes all available data using the provided consumer.
* @param consumer Consumer function to process individual elements
*/","* Consume all the elements, will block if queue is empty
   * @param consumer  the consumer callback object
   * @throws InterruptedException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,incrCacheHit,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheHit(),64,66,"/**
* Increments cache hit counter.
*/",* One cache hit event,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,incrCacheCleared,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheCleared(),71,73,"/**
 * Increments the count of cache cleared events.
 */",* One cache cleared,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,incrCacheUpdated,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheUpdated(),78,80,"/**
* Increments the cache updated counter.
*/",* One cache updated,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthenticationFailures,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthenticationFailures(),215,217,"/**
* Increments authentication failure counter.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthenticationSuccesses,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthenticationSuccesses(),223,225,"/**
* Increments authentication successes counter.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthorizationSuccesses,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthorizationSuccesses(),231,233,"/**
 * Increments authorization successes counter.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthorizationFailures,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthorizationFailures(),239,241,"/**
* Increments authorization failure counter.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrClientBackoff,org.apache.hadoop.ipc.metrics.RpcMetrics:incrClientBackoff(),343,345,"/**
 * Increments client backoff counter.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrClientBackoffDisconnected,org.apache.hadoop.ipc.metrics.RpcMetrics:incrClientBackoffDisconnected(),350,352,"/**
 * Increments client backoff disconnected counter.
 */",* Client was disconnected due to backoff,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrSlowRpc,org.apache.hadoop.ipc.metrics.RpcMetrics:incrSlowRpc(),366,368,"/**
* Increments slow RPC call counter. 
* @param none 
* @return none 
*/",* Increments the Slow RPC counter.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrRequeueCalls,org.apache.hadoop.ipc.metrics.RpcMetrics:incrRequeueCalls(),373,375,"/**
* Increments requeue calls counter.
*/",* Increments the Requeue Calls counter.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrRpcCallSuccesses,org.apache.hadoop.ipc.metrics.RpcMetrics:incrRpcCallSuccesses(),380,382,"/**
* Increments RPC call successes counter.
*/",* One RPC call success event.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,channelWrite,"org.apache.hadoop.ipc.Server:channelWrite(java.nio.channels.WritableByteChannel,java.nio.ByteBuffer)",3923,3932,"/**
* Writes data from a ByteBuffer to a WritableByteChannel.
* @param channel the destination channel
* @param buffer the source byte buffer
* @return number of bytes written or -1 on failure
*/","* This is a wrapper around {@link WritableByteChannel#write(ByteBuffer)}.
   * If the amount of data is large, it writes to channel in smaller chunks. 
   * This is to avoid jdk from creating many direct buffers as the size of 
   * buffer increases. This also minimizes extra copies in NIO layer
   * as a result of multiple write operations required to write a large 
   * buffer.  
   *
   * @see WritableByteChannel#write(ByteBuffer)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,channelRead,"org.apache.hadoop.ipc.Server:channelRead(java.nio.channels.ReadableByteChannel,java.nio.ByteBuffer)",3943,3952,"/**
* Reads data from the given channel into the provided buffer.
* @param channel ReadableByteChannel to read from
* @param buffer ByteBuffer to store the read data
* @return number of bytes read (0 or -1 on error)
*/","* This is a wrapper around {@link ReadableByteChannel#read(ByteBuffer)}.
   * If the amount of data is large, it writes to channel in smaller chunks. 
   * This is to avoid jdk from creating many direct buffers as the size of 
   * ByteBuffer increases. There should not be any performance degredation.
   * 
   * @see ReadableByteChannel#read(ByteBuffer)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,getRecord,org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:getRecord(),152,157,"/**
* Returns a MetricsRecord object or null based on filter criteria.
* @return MetricsRecordImpl object or null if not accepted
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,newAttrInfo,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:newAttrInfo(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",58,60,"/**
* Creates a new attribute information object for MBean attributes.
* @param info MetricsInfo instance
* @param type attribute type (e.g. long, int, etc.)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,get,org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:get(),96,112,"/**
* Constructs MBeanInfo by iterating through records and metrics.
* @return MBeanInfo object representing the monitored resource
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,updateAttrCache,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateAttrCache(java.lang.Iterable),250,268,"/**
* Updates attribute cache with records from the provided iterable.
* @param lastRecs collection of MetricsRecordImpl objects
* @return number of updated metric tags and values
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,newObjectName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newObjectName(java.lang.String),128,137,"/**
* Creates a new object name with the given name, ensuring uniqueness if not in mini-cluster mode.
* @param name the desired object name
* @return the newly created ObjectName or throws MetricsException if duplicate exists
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,newSourceName,"org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newSourceName(java.lang.String,boolean)",147,156,"/**
* Generates a unique or existing metrics source name.
* @param name desired source name
* @param dupOK whether to allow duplicate names
* @return the generated source name
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,putMetrics,"org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,long)",96,107,"/**
* Enqueues metrics buffer at specified logical time, refreshing queue size gauge if enqueue is successful.
* @param buffer MetricsBuffer object to be enqueued
* @param logicalTimeMs logical timestamp in milliseconds
* @return true if successfully enqueued or already up-to-date, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,putMetricsImmediate,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer),109,126,"/**
* Immediately adds metrics to the queue and waits for notification.
* @param buffer MetricsBuffer object
* @return true if successful, false on queue full or timeout
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,diskCheckFailed,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:diskCheckFailed(),146,149,"/**
* Increments failure count and records last failure time.
* @see failureCount
*/",* Increase the failure count and update the last failure timestamp.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,fetchGroupSet,org.apache.hadoop.security.Groups$GroupCacheLoader:fetchGroupSet(java.lang.String),413,424,"/**
* Fetches user group set for a given user.
* @param user unique user identifier
*/","* Queries impl for groups belonging to the user.
     * This could involve I/O and take awhile.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,shutdown,org.apache.hadoop.metrics2.source.JvmMetrics$Singleton:shutdown(),66,69,"/**
* Shuts down the metrics system and releases resources.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,registerIfNeeded,org.apache.hadoop.metrics2.source.JvmMetrics:registerIfNeeded(),72,79,"/**
* Registers JVM metrics implementation if not already registered.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,create,org.apache.hadoop.security.UserGroupInformation$UgiMetrics:create(),147,149,"/**
* Creates and registers a new instance of UgiMetrics.
* @return registered UgiMetrics object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,shutdown,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:shutdown(),105,107,"/**
* Unregisters metrics source with given name.
*/",* Shutdown the instrumentation process.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,shutdown,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:shutdown(),108,110,"/**
 * Unregisters the metrics source with the specified name.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,shutdown,org.apache.hadoop.ipc.metrics.RpcMetrics:shutdown(),247,249,"/**
* Unregisters metrics source with given name.
* @param name unique identifier of the metrics source to unregister
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,registerMetrics2Source,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:registerMetrics2Source(java.lang.String),891,894,"/**
* Registers metrics to source under specified namespace.
* @param namespace unique namespace identifier
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,unregister,org.apache.hadoop.metrics2.util.MBeans:unregister(javax.management.ObjectName),136,149,"/**
* Unregisters an MBean by ID.
* @param mbeanName unique ObjectName identifier
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2Metrics.java,remove,org.apache.hadoop.http.HttpServer2Metrics:remove(),161,163,"/**
* Removes HTTP server metrics source name from system.
* @param port server port number
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,snapshot,"org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",228,230,"/**
 * Takes a metrics snapshot using the provided MetricsRecordBuilder.
 * @param rb MetricsRecordBuilder instance to capture snapshot data
 * @param all whether to include all registered metrics in the snapshot
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableRates:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",80,83,"/**
 * Sends metrics snapshot to the provided MetricsRecordBuilder.
 * @param rb builder to populate with metrics data
 * @param all whether to include all metrics or only those changed since last snapshot
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,setQuantiles,"org.apache.hadoop.metrics2.lib.MutableQuantiles:setQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.text.DecimalFormat)",117,125,"/**
* Sets quantiles based on input parameters.
* @param ucName upper case name prefix
* @param uvName upper case name suffix
* @param desc description of quantile
* @param lvName lower case name for interval
* @param pDecimalFormat decimal format for quantile display
*/","* Sets quantileInfo.
   *
   * @param ucName capitalized name of the metric
   * @param uvName capitalized type of the values
   * @param desc uncapitalized long-form textual description of the metric
   * @param lvName uncapitalized type of the values
   * @param pDecimalFormat Number formatter for percentile value",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableInverseQuantiles.java,setQuantiles,"org.apache.hadoop.metrics2.lib.MutableInverseQuantiles:setQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.text.DecimalFormat)",75,83,"/**
* Sets quantiles by name and calculates corresponding inverse percentiles.
* @param ucName upper-case name prefix
* @param uvName upper-case name suffix
* @param desc description for quantiles
* @param lvName lower-case name suffix
* @param df decimal format for quantile display
*/","* Sets quantileInfo.
   *
   * @param ucName capitalized name of the metric
   * @param uvName capitalized type of the values
   * @param desc uncapitalized long-form textual description of the metric
   * @param lvName uncapitalized type of the values
   * @param df Number formatter for inverse percentile value",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,<init>,org.apache.hadoop.metrics2.lib.MetricsRegistry:<init>(java.lang.String),49,51,"/**
 * Initializes a new MetricsRegistry instance with the specified name.
 * @param name unique identifier for this registry
 */","* Construct the registry with a record name
   * @param name  of the record of the metrics",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableRollingAverages:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",169,197,"/**
* Captures metrics snapshot for all or changed averages.
* @param builder MetricsRecordBuilder instance
* @param all whether to capture all averages or only changed ones
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/Interns.java,tag,"org.apache.hadoop.metrics2.lib.Interns:tag(java.lang.String,java.lang.String,java.lang.String)",162,164,"/**
* Creates a new metrics tag with specified name, description, and value.
* @param name unique tag identifier
* @param description human-readable tag description
* @param value numeric tag value
*/","* Get a metrics tag.
   * @param name  of the tag
   * @param description of the tag
   * @param value of the tag
   * @return an interned metrics tag",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(java.lang.Class,org.apache.hadoop.metrics2.annotation.Metrics)",141,146,"/**
* Retrieves metrics info based on the provided class and annotation.
* @param cls Class object
* @param annotation Metrics annotation
* @return MetricsInfo object or null if not applicable
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.String)",162,174,"/**
* Retrieves MetricsInfo based on Metric annotation and default name.
* @param annotation Metric annotation with possible name and description
* @param defaultName fallback metric name when annotation is incomplete
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,<init>,"org.apache.hadoop.metrics2.lib.MutableStat:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)",64,85,"/**
* Initializes MutableStat object with name, description, sample name and value name.
* @param name        Statistic name
* @param description Description of statistic
* @param sampleName   Sample name
* @param valueName    Value name
* @param extended     Whether to include additional information (true/false)
*/","* Construct a sample statistics metric
   * @param name        of the metric
   * @param description of the metric
   * @param sampleName  of the metric (e.g. ""Ops"")
   * @param valueName   of the metric (e.g. ""Time"", ""Latency"")
   * @param extended    create extended stats (stdev, min/max etc.) by default.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,metricInfo,org.apache.hadoop.metrics2.lib.MethodMetric:metricInfo(java.lang.reflect.Method),145,147,"/**
 * Retrieves metrics information based on a given method.
 * @param method the method to fetch metrics for
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getGcInfo,org.apache.hadoop.metrics2.source.JvmMetrics:getGcInfo(java.lang.String),209,222,"/**
* Retrieves GC metrics info by name from cache or initializes and caches it.
* @param gcName unique garbage collector identifier
* @return MetricsInfo array containing GC count and time metrics
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addUniqueIdentityCount,org.apache.hadoop.ipc.DecayRpcScheduler:addUniqueIdentityCount(org.apache.hadoop.metrics2.MetricsRecordBuilder),1030,1033,"/**
* Increments counter for total unique callers.
* @param rb MetricsRecordBuilder instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addDecayedCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addDecayedCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1036,1039,"/**
* Adds decayed call volume metric to the metrics record.
* @param rb MetricsRecordBuilder instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addRawCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addRawCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1041,1044,"/**
* Adds raw call volume metric to record builder.
* @param rb MetricsRecordBuilder instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addServiceUserDecayedCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addServiceUserDecayedCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1047,1051,"/**
* Adds service user decayed call volume metric.
* @param rb MetricsRecordBuilder instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addServiceUserRawCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addServiceUserRawCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1054,1058,"/**
* Adds raw service user call volume metric to the metrics record.
* @param rb MetricsRecordBuilder instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addCallVolumePerPriority,org.apache.hadoop.ipc.DecayRpcScheduler:addCallVolumePerPriority(org.apache.hadoop.metrics2.MetricsRecordBuilder),1061,1067,"/**
* Records call volumes per priority in the last window.
* @param rb MetricsRecordBuilder to add data to
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addAvgResponseTimePerPriority,org.apache.hadoop.ipc.DecayRpcScheduler:addAvgResponseTimePerPriority(org.apache.hadoop.metrics2.MetricsRecordBuilder),1070,1076,"/**
* Adds average response times per priority to metrics record.
* @param rb MetricsRecordBuilder instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configureSystem,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSystem(),488,490,"/**
* Adds system-specific tag to configuration.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,tag,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",73,79,"/**
* Adds a metric tag with specified information and value.
* @param info Metric details
* @param value Tag value to be added
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String,boolean)",415,420,"/**
* Adds or updates a metric tag with the specified name and value.
* @param info MetricsInfo object describing the metric
* @param value new tag value to be associated with the metric
* @param override whether to override existing tag if it already exists
* @return this instance for method chaining
*/","* Add a tag to the metrics
   * @param info  metadata of the tag
   * @param value of the tag
   * @param override existing tag if true
   * @return the registry (for keep adding tags etc.)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeFloat:incr(),42,45,"/**
* Increments internal value by default 1.0.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,decr,org.apache.hadoop.metrics2.lib.MutableGaugeFloat:decr(),47,50,"/**
 * Decrements the value by calling increment with -1.0f.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,add,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat:add(double),178,180,"/**
 * Adds a numerical value to the statistic accumulator.
 * @param x the value to be added to the statistic
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,add,org.apache.hadoop.metrics2.lib.MutableStat:add(long),132,136,"/**
* Adds a long value to multiple statistics and updates changed flag.
* @param value statistical value to be added
*/","* Add a snapshot to the metric.
   * @param value of the metric",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,reset,"org.apache.hadoop.metrics2.util.SampleStat:reset(long,double,double,org.apache.hadoop.metrics2.util.SampleStat$MinMax)",48,53,"/**
* Resets statistical summary with new values.
* @param numSamples1 updated number of samples
* @param mean1 updated sample mean
* @param s1 updated standard deviation
* @param minmax1 updated minimum and maximum bounds
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,snapshotInto,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat:snapshotInto(org.apache.hadoop.metrics2.lib.MutableRate),182,187,"/**
* Captures and records statistical data into a MutableRate object.
* @param metric target rate object to populate
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newImpl,org.apache.hadoop.metrics2.lib.MethodMetric:newImpl(org.apache.hadoop.metrics2.annotation.Metric$Type),55,70,"/**
* Creates a new MutableMetric instance based on the given metric type.
* @param metricType Metric.Type value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,toString,org.apache.hadoop.metrics2.util.SampleStat:toString(),145,156,"/**
* Formats object state into a human-readable string.
* @return A formatted string containing key statistics and errors if any
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getProcessingStdDev,org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingStdDev(),412,414,"/**
* Calculates standard deviation of processing times.
* @return Standard deviation value in milliseconds
*/","* Return Standard Deviation of the Processing Time.
   * @return  double",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getDeferredRpcProcessingStdDev,org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingStdDev(),445,447,"/**
* Calculates the standard deviation of the last RPC processing time.
* @return Standard deviation of RPC processing times in milliseconds
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,insert,org.apache.hadoop.metrics2.util.SampleQuantiles:insert(long),113,123,"/**
* Inserts a value into the buffer, potentially triggering batch insertion and compression.
* @param v long integer value to be inserted
*/","* Add a new value from the stream.
   * 
   * @param v v.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,snapshot,org.apache.hadoop.metrics2.util.SampleQuantiles:snapshot(),236,250,"/**
* Retrieves a snapshot of Quantile values.
* @return Map of Quantile to Long value or null if no samples exist
*/","* Get a snapshot of the current values of all the tracked quantiles.
   * 
   * @return snapshot of the tracked quantiles. If no items are added
   * to the estimator, returns null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,getTopTokenRealOwners,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTopTokenRealOwners(int),880,898,"/**
* Retrieves the top N token real owners based on statistics.
* @param n maximum number of owners to retrieve
* @return List of NameValuePair objects representing top owners or an empty list if none found.","* Return top token real owners list as well as the tokens count.
   *
   * @param n top number of users
   * @return map of owners to counts",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getTopCallers,org.apache.hadoop.ipc.DecayRpcScheduler:getTopCallers(int),1099,1112,"/**
* Retrieves the top N callers based on their costs.
* @param n number of top callers to retrieve
* @return TopN object containing the top caller names and costs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java,cacheGroupsAdd,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsAdd(java.util.List),90,103,"/**
* Adds groups to the cache.
* @param groups list of group names to add (can be netgroups or Unix groups)
*/","* Add a group to cache, only netgroups are cached
   *
   * @param groups list of group names to add to cache",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getTokens,org.apache.hadoop.security.UserGroupInformation:getTokens(),1724,1729,"/**
* Retrieves an unmodifiable collection of tokens associated with the subject.
* @return immutable collection of tokens or empty collection if none found
*/","* Obtain the collection of tokens associated with this user.
   * 
   * @return an unmodifiable collection of tokens associated with user",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/User.java,<init>,org.apache.hadoop.security.User:<init>(java.lang.String),42,44,"/**
* Constructs a new User object with the given name.
* @param name user's full name",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getGroups,org.apache.hadoop.security.Groups:getGroups(java.lang.String),213,217,"/**
* Retrieves groups associated with a given user.
* @param user unique user identifier
* @return unmodifiable list of group names or empty list if not found
*/","* Get the group memberships of a given user.
   * If the user's group is not cached, this method may block.
   * Note this method can be expensive as it involves Set {@literal ->} List
   * conversion. For user with large group membership
   * (i.e., {@literal >} 1000 groups), we recommend using getGroupSet
   * to avoid the conversion and fast membership look up via contains().
   * @param user User's name
   * @return the group memberships of the user as list
   * @throws IOException if user does not exist
   * @deprecated Use {@link #getGroupsSet(String user)} instead.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getGroupsSet,org.apache.hadoop.security.Groups:getGroupsSet(java.lang.String),231,233,"/**
 * Returns an unmodifiable set of groups associated with the given user.
 * @param user unique user identifier
 */","* Get the group memberships of a given user.
   * If the user's group is not cached, this method may block.
   * This provide better performance when user has large group membership via
   * <br>
   * 1) avoid {@literal set->list->set} conversion for the caller
   * UGI/PermissionCheck <br>
   * 2) fast lookup using contains() via Set instead of List
   * @param user User's name
   * @return the group memberships of the user as set
   * @throws IOException if user does not exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java,getGroups,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String),67,73,"/**
* Retrieves Unix and net group IDs for the specified user.
* @param user unique user identifier
* @return list of group names or empty list if not found
*/","* Gets unix groups and netgroups for the user.
   *
   * It gets all unix groups as returned by id -Gn but it
   * only returns netgroups that are used in ACLs (there is
   * no way to get all netgroups for a given user, see
   * documentation for getent netgroup)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,println,org.apache.hadoop.security.KDiag:println(),868,870,"/**
* Prints an empty line to the console. 
*/",* Print a new line,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printSysprop,org.apache.hadoop.security.KDiag:printSysprop(java.lang.String),897,900,"/**
* Prints system property value to console.
* @param property name of system property
*/","* Print a system property, or {@link #UNSET} if unset.
   * @param property property to print",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printEnv,org.apache.hadoop.security.KDiag:printEnv(java.lang.String),916,919,"/**
* Prints environment variable value to console.
* @param variable name of environment variable
*/","* Print an environment variable's name and value; printing
   * {@link #UNSET} if it is not set.
   * @param variable environment variable",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dump,org.apache.hadoop.security.KDiag:dump(java.io.File),926,932,"/**
* Dumps the contents of a file to the console.
* @param file input file to read from
*/","* Dump any file to standard out.
   * @param file file to dump
   * @throws IOException IO problems",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,error,"org.apache.hadoop.security.KDiag:error(java.lang.String,java.lang.String,java.lang.Object[])",1011,1013,"/**
* Logs an error with specified category and formatted message.
* @param category Error category
* @param message Error message with optional arguments for formatting
*/","* Print a message as an error
   * @param category error category
   * @param message format string
   * @param args list of arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,warn,"org.apache.hadoop.security.KDiag:warn(java.lang.String,java.lang.String,java.lang.Object[])",1020,1022,"/**
* Logs a warning message with specified category and parameters.
* @param category warning category
* @param message warning message with placeholders for arguments
* @param args variable number of arguments to fill message placeholders
*/","* Print a message as an warning
   * @param category error category
   * @param message format string
   * @param args list of arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,loadFullUserMap,org.apache.hadoop.security.ShellBasedIdMapping:loadFullUserMap(),359,370,"/**
* Loads the full user map into memory.
* @throws IOException if an I/O error occurs during loading
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,loadFullGroupMap,org.apache.hadoop.security.ShellBasedIdMapping:loadFullGroupMap(),372,384,"/**
* Loads and updates the full group map.
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod),1844,1846,"/**
* Sets the authentication method for the user.
* @param authMethod authentication method to be applied (e.g. password, OAuth)
*/","* Sets the authentication method in the subject
   * 
   * @param authMethod authMethod.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,write,org.apache.hadoop.security.SaslOutputStream:write(int),123,131,"/**
* Writes a single byte to the output stream. If useWrap is false,
* writes directly; otherwise, wraps in buffer and writes via block.
* @throws IOException if IO error occurs
*/","* Writes the specified byte to this output stream.
   * 
   * @param b
   *          the <code>byte</code>.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,write,org.apache.hadoop.security.SaslOutputStream:write(byte[]),148,151,"/**
* Writes an array of bytes to the underlying stream.
* @param b byte array to be written
*/","* Writes <code>b.length</code> bytes from the specified byte array to this
   * output stream.
   * <p>
   * The <code>write</code> method of <code>SASLOutputStream</code> calls the
   * <code>write</code> method of three arguments with the three arguments
   * <code>b</code>, <code>0</code>, and <code>b.length</code>.
   * 
   * @param b
   *          the data.
   * @exception NullPointerException
   *              if <code>b</code> is null.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,createKeyManagers,org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyManagers(),1078,1088,"/**
* Creates an array of KeyManagers from the specified key store.
* @throws IOException if key store access fails
* @throws GeneralSecurityException on security-related errors
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,createTrustManagers,org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createTrustManagers(),1090,1100,"/**
* Creates an array of trust managers from a key store.
* @return TrustManager[] or null if no location specified
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,doFilter,"org.apache.hadoop.security.http.RestCsrfPreventionFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",205,212,"/**
* Executes filter logic for incoming HTTP request.
* @param request servlet request object
* @param response servlet response object
* @param chain next filter in the chain
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,doFilter,"org.apache.hadoop.security.http.CrossOriginFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",92,98,"/**
* Executes cross-filter logic and passes request to next filter in chain.
* @param req incoming servlet request
* @param res outgoing servlet response
* @param chain filter chain to be executed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,init,org.apache.hadoop.security.http.CrossOriginFilter:init(javax.servlet.FilterConfig),84,90,"/**
* Initializes servlet filter with configuration.
* @param filterConfig Filter configuration object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,getKey,org.apache.hadoop.security.token.delegation.DelegationKey:getKey(),75,82,"/**
* Retrieves a SecretKey instance from stored key bytes.
* @return SecretKey object or null if no valid key exists
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,checkToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),528,546,"/**
* Verifies and retrieves DelegationTokenInformation for the given token.
* @param identifier TokenIdent object containing token details
* @return DelegationTokenInformation if valid, null otherwise
*/","* Find the DelegationTokenInformation for the given token id, and verify that
   * if the token is expired. Note that this method should be called with 
   * acquiring the secret manager's monitor.
   *
   * @param identifier identifier.
   * @throws InvalidToken invalid token exception.
   * @return DelegationTokenInformation.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>(),703,705,"/**
* Initializes DelegationTokenInformation with default values.
* @param timeout default token timeout in seconds (default=0)
* @param renewer default token renewer (default=null) 
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,logExpireTokens,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:logExpireTokens(java.util.Collection),786,793,"/**
* Logs and removes expired tokens from storage.
* @param expiredTokens Collection of TokenIdent objects to expire
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,setExternalDelegationTokenSecretManager,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager),143,146,"/**
* Sets an external delegationToken secret manager.
* @param secretManager Secret manager to use for tokens
*/","* Sets an external <code>DelegationTokenSecretManager</code> instance to
   * manage creation and verification of Delegation Tokens.
   * <p>
   * This is useful for use cases where secrets must be shared across multiple
   * services.
   *
   * @param secretManager a <code>DelegationTokenSecretManager</code> instance",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,destroy,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:destroy(),189,193,"/**
* Destroys this instance by destroying associated token and authentication handlers.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,updateCurrentKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateCurrentKey(),440,456,"/**
 * Updates the current master key for generating delegation tokens.
 * @throws IOException if an I/O error occurs during key storage
 */","* Update the current master key 
   * This is called once by startThreads before tokenRemoverThread is created, 
   * and only by tokenRemoverThread afterwards.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,<init>,org.apache.hadoop.security.token.delegation.DelegationKey:<init>(),47,49,"/**
* Constructs an empty DelegationKey with default values.
* @param delegationId default delegation ID
* @param creationTime default creation time in milliseconds
* @param secretKey null secret key by default
*/",Default constructore required for Writable,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)",183,188,"/**
* Initializes a new URL authenticated with a delegated token.
* @param authenticator DelegationTokenAuthenticator instance
* @param connConfigurator Connection configuration settings
*/","* Creates an <code>DelegationTokenAuthenticatedURL</code>.
   *
   * @param authenticator the {@link DelegationTokenAuthenticator} instance to
   * use, if <code>null</code> the default one will be used.
   * @param connConfigurator a connection configurator.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,renew,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:renew(),126,151,"/**
* Tries to renew the delegation token, updating renewal time if successful.
* @return true if token was successfully renewed, false otherwise
*/","* Renew or replace the delegation token for this file system.
     * It can only be called when the action is not in the queue.
     * @return
     * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,cancel,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:cancel(),153,158,"/**
* Cancels the current file system operation using the provided token.
* @throws IOException on I/O error
* @throws InterruptedException if interrupted while waiting for cancellation
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,read,org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(),568,573,"/**
* Reads a single character from the input stream.
* @return the next character or -1 if end of stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,read,org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[]),575,578,"/**
 * Reads bytes from underlying stream into specified byte array.
 * @param b the byte array to fill with data
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setSaslClient,org.apache.hadoop.ipc.Client$IpcStreams:setSaslClient(org.apache.hadoop.security.SaslRpcClient),1905,1910,"/**
* Configures SASL RPC client and sets up input/output streams.
* @param client SaslRpcClient instance to configure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,noPasswordWarning,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:noPasswordWarning(),348,352,"/**
* Generates password-related warning message. 
* @return Warning string or null if not set.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,noPasswordWarning,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:noPasswordWarning(),315,319,"/**
* Generates password-less warning message.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,noPasswordError,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:noPasswordError(),354,358,"/**
* Returns password error message.
* @return error message or null if not configured
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,noPasswordError,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:noPasswordError(),321,325,"/**
* Returns error message if password is not provided.
*@return Error message or empty string if password is set.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,org.apache.hadoop.security.SaslInputStream:read(),193,207,"/**
* Reads a single byte from the stream, handling wrapped mode and blocking.
* @throws IOException on I/O error
*/","* Reads the next byte of data from this input stream. The value byte is
   * returned as an <code>int</code> in the range <code>0</code> to
   * <code>255</code>. If no byte is available because the end of the stream has
   * been reached, the value <code>-1</code> is returned. This method blocks
   * until input data is available, the end of the stream is detected, or an
   * exception is thrown.
   * <p>
   * 
   * @return the next byte of data, or <code>-1</code> if the end of the stream
   *         is reached.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,"org.apache.hadoop.security.SaslInputStream:read(byte[],int,int)",248,275,"/**
* Reads data into the provided byte array.
* @param b target buffer
* @param off offset in the buffer
* @param len number of bytes to read
* @return actual number of bytes read or -1 on error, 0 if no more data available
*/","* Reads up to <code>len</code> bytes of data from this input stream into an
   * array of bytes. This method blocks until some input is available. If the
   * first argument is <code>null,</code> up to <code>len</code> bytes are read
   * and discarded.
   * 
   * @param b
   *          the buffer into which the data is read.
   * @param off
   *          the start offset of the data.
   * @param len
   *          the maximum number of bytes read.
   * @return the total number of bytes read into the buffer, or <code>-1</code>
   *         if there is no more data because the end of the stream has been
   *         reached.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ImpersonationProvider.java,authorize,"org.apache.hadoop.security.authorize.ImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",51,58,"/**
* Authorizes the specified UserGroupInformation for the given remote address.
* @param user authenticated UserGroupInformation
* @param remoteAddress IP address of the client
*/","* Authorize the superuser which is doing doAs.
   * {@link #authorize(UserGroupInformation, InetAddress)} should
   *             be preferred to avoid possibly re-resolving the ip address.
   * @param user ugi of the effective or proxy user which contains a real user.
   * @param remoteAddress the ip address of client.
   * @throws AuthorizationException Authorization Exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getKeytab,org.apache.hadoop.security.UserGroupInformation:getKeytab(),814,819,"/**
* Retrieves Keytab value from LoginContext configuration.
* @return Keytab string or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isHadoopLogin,org.apache.hadoop.security.UserGroupInformation:isHadoopLogin(),825,828,"/**
* Checks if Hadoop login context is active.
* @return true if Hadoop login context is managing UGI, false otherwise
*/","* Is the ugi managed by the UGI or an external subject?
   * @return true if managed by UGI.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createProxyUser,"org.apache.hadoop.security.UserGroupInformation:createProxyUser(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",1522,1537,"/**
* Creates a proxy user with specified identity.
* @param user unique user identifier
* @param realUser actual user behind the proxy (may be null)
* @return UserGroupInformation object representing the proxy user
*/","* Create a proxy user using username of the effective user and the ugi of the
   * real user.
   * @param user user.
   * @param realUser realUser.
   * @return proxyUser ugi",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getName,org.apache.hadoop.security.UserGroupInformation$RealUser:getName(),472,475,"/**
* Retrieves the name of the associated User.
* @return The user's name as a string.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,shouldBackOff,org.apache.hadoop.ipc.DecayRpcScheduler:shouldBackOff(org.apache.hadoop.ipc.Schedulable),720,745,"/**
* Determines whether to trigger back off based on response time thresholds.
* @param obj Schedulable object containing priority and user information
* @return true if back off is triggered, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealUserOrSelf,org.apache.hadoop.security.UserGroupInformation:getRealUserOrSelf(org.apache.hadoop.security.UserGroupInformation),1558,1564,"/**
* Returns the real user or self if no real user is found.
* @param user UserGroupInformation object to verify
* @return the real user's information or self if not found
*/","* If this is a proxy user, get the real user. Otherwise, return
   * this user.
   * @param user the user to check
   * @return the real user or self",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,toString,org.apache.hadoop.security.UserGroupInformation:toString(),1819,1827,"/**
* Returns a human-readable string representation of the user, including name and authentication method.
* @return User description string
*/",* Return the username.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:getRealAuthenticationMethod(),1863,1869,"/**
* Retrieves the real authentication method for the current user.
*/","* Get the authentication method from the real user's subject.  If there
   * is no real user, return the given user's authentication method.
   * 
   * @return AuthenticationMethod in the subject, null if not present.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation),1878,1885,"/**
* Retrieves the actual authentication method for a user.
* @param ugi UserGroupInformation instance
* @return AuthenticationMethod enum value or null if unknown
*/","* Returns the authentication method of a ugi. If the authentication method is
   * PROXY, returns the authentication method of the real user.
   * 
   * @param ugi ugi.
   * @return AuthenticationMethod",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,makeIpcConnectionContext,"org.apache.hadoop.util.ProtoUtil:makeIpcConnectionContext(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",91,122,"/**
* Creates IpcConnectionContextProto object based on protocol and user information.
* @param protocol connection protocol
* @param ugi UserGroupInformation object
* @param authMethod authentication method used
* @return IpcConnectionContextProto object or null if invalid parameters
*/","* This method creates the connection context  using exactly the same logic
   * as the old connection context as was done for writable where
   * the effective and real users are set based on the auth method.
   *
   * @param protocol protocol.
   * @param ugi ugi.
   * @param authMethod authMethod.
   * @return IpcConnectionContextProto.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,close,org.apache.hadoop.ipc.Server$ConnectionManager:close(org.apache.hadoop.ipc.Server$Connection),4110,4126,"/**
* Closes a database connection and associated user context.
* @param connection the Connection object to close
* @return true if the connection was successfully closed, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/UserIdentityProvider.java,makeIdentity,org.apache.hadoop.ipc.UserIdentityProvider:makeIdentity(org.apache.hadoop.ipc.Schedulable),28,35,"/**
* Retrieves short user name from Schedulable object.
* @param obj Schedulable object containing user info
* @return Short user name or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,verify,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:verify(java.lang.String,javax.net.ssl.SSLSession)",262,273,"/**
* Verifies SSL connection based on peer certificate.
* @param host server hostname
* @param session SSLSession object
* @return true if verification successful, false otherwise
*/","* The javax.net.ssl.HostnameVerifier contract.
         *
         * @param host    'hostname' we used to create our socket
         * @param session SSLSession with the remote server
         * @return true if the host matched the one in the certificate.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,java.security.cert.X509Certificate)",280,284,"/**
* Checks an SSL certificate against a single host.
* @param host hostname to verify
* @param cert X509Certificate to validate
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],javax.net.ssl.SSLSocket)",292,347,"/**
* Verifies SSL/TLS connection by checking the peer's certificate chain.
* @param host array of hosts to verify
* @param ssl SSLSocket object to use for verification
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,loadResource,org.apache.hadoop.util.FindClass:loadResource(java.lang.String),172,180,"/**
* Loads and returns the resource identifier for the given name.
* @param name unique resource identifier
* @return int resource ID or E_NOT_FOUND if not found
*/","* Load a resource
   * @param name resource name
   * @return the status code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,<init>,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:<init>(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode),132,151,"/**
* Initializes DelegatingSSLSocketFactory with preferred SSL channel mode.
* @param preferredChannelMode the desired SSL channel mode
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,<init>,org.apache.hadoop.fs.shell.Command:<init>(org.apache.hadoop.conf.Configuration),86,88,"/**
* Initializes the command with configuration settings.
* @param conf configuration details for this command instance.","* Constructor.
   *
   * @param conf configuration.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,<init>,org.apache.hadoop.fs.shell.CommandFactory:<init>(org.apache.hadoop.conf.Configuration),53,55,"/**
 * Initializes a new instance of the CommandFactory class with the given configuration.
 * @param conf the application configuration","* Factory constructor for commands
   * @param conf the hadoop configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,org.apache.hadoop.fs.FileSystem:<init>(),777,779,"/**
 * Initializes a new FileSystem instance with no underlying file system.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,<init>,org.apache.hadoop.fs.FsShell:<init>(org.apache.hadoop.conf.Configuration),75,77,"/**
* Initializes FsShell with configuration.
* @param conf Hadoop Configuration instance
*/","* Construct a FsShell with the given configuration.  Commands can be
   * executed via {@link #run(String[])}
   * @param conf the hadoop configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,<init>,org.apache.hadoop.io.ObjectWritable$NullInstance:<init>(),109,109,"/**
* Initializes an instance with no value (null).",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,<init>,"org.apache.hadoop.io.ObjectWritable$NullInstance:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)",110,113,"/**
* Initializes a new instance with the specified class and configuration.
* @param declaredClass Class being instantiated
* @param conf Configuration object for initialization
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag:<init>(org.apache.hadoop.conf.Configuration,java.io.PrintWriter,java.io.File,java.lang.String,long,boolean)",171,184,"/**
* Initializes KDiag with configuration and output settings.
* @param conf Hadoop Configuration object
* @param out PrintWriter for logging output
* @param keytab File containing Kerberos keytab
* @param principal User principal for authentication
* @param minKeyLength Minimum length of keys to log
* @param securityRequired Whether security is required",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,<init>,org.apache.hadoop.util.FindClass:<init>(org.apache.hadoop.conf.Configuration),131,133,"/**
 * Initializes the FindClass instance with the given configuration.
 */","* Create a class with a specified configuration
   * @param conf configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,<init>,"org.apache.hadoop.tools.GetGroupsBase:<init>(org.apache.hadoop.conf.Configuration,java.io.PrintStream)",53,56,"/**
 * Initializes GetGroupsBase with configuration and output stream.
 * @param conf Configuration object
 * @param out Output stream to print results to
 */","* Used exclusively for testing.
   * 
   * @param conf The configuration to use.
   * @param out The PrintStream to write to, instead of System.out",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configured.java,<init>,org.apache.hadoop.conf.Configured:<init>(),32,34,"/**
* Initializes default configuration with no values set.",Construct a Configured.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,<init>,org.apache.hadoop.ha.HAAdmin:<init>(org.apache.hadoop.conf.Configuration),103,105,"/**
 * Initializes an HA Admin with the given configuration.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByNameWithSearch,org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByNameWithSearch(java.lang.String),706,718,"/**
* Resolves host name with optional domain search.
* @param host hostname to resolve
* @return InetAddress object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getAppConfigurationEntry,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getAppConfigurationEntry(java.lang.String),2211,2230,"/**
* Retrieves configuration entries for the specified app.
* @param appName name of application to fetch config for
* @return array of AppConfigurationEntry objects or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,parseStaticMap,org.apache.hadoop.security.ShellBasedIdMapping:parseStaticMap(java.io.File),588,630,"/**
* Parses a static mapping file into UID and GID mappings.
* @param staticMapFile the file to parse
* @return StaticMapping object with parsed mappings or null if invalid
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,getAclString,org.apache.hadoop.security.authorize.AccessControlList:getAclString(),301,312,"/**
* Generates ACL string representation.
* @return String containing '*' for all allowed or user and group IDs
*/","* Returns the access control list as a String that can be used for building a
   * new instance by sending it to the constructor of {@link AccessControlList}.
   * @return acl string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,createCredentialEntry,"org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:createCredentialEntry(java.lang.String,char[])",228,244,"/**
* Creates a new CredentialEntry for the given alias and credential.
* @param alias unique identifier for credential
* @param credential encrypted credential data
* @return newly created CredentialEntry or null on failure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,execute,org.apache.hadoop.security.alias.CredentialShell$CreateCommand:execute(),438,465,"/**
* Creates a credential entry with the specified alias.
* @throws IOException if I/O operations fail
* @throws NoSuchAlgorithmException if algorithm not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getTGT,org.apache.hadoop.security.UserGroupInformation:getTGT(),852,861,"/**
* Retrieves the Ticket-Granting Ticket (TGT) from private credentials.
* @return Kerberos TGT as a KerberosTicket object or null if not found
*/","* Get the Kerberos TGT
   * @return the user's TGT or null if none was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setSslConfiguration,"org.apache.hadoop.security.SecurityUtil:setSslConfiguration(org.apache.zookeeper.client.ZKClientConfig,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",819,823,"/**
* Sets SSL/TLS configuration for the ZK client.
* @param zkClientConfig ZK client configuration
* @param truststoreKeystore Truststore and keystore to use
*/","* Configure ZooKeeper Client with SSL/TLS connection.
   * @param zkClientConfig ZooKeeper Client configuration
   * @param truststoreKeystore truststore keystore, that we use to set the SSL configurations
   * @throws ConfigurationException if the SSL configs are empty",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,unprotectedRelogin,"org.apache.hadoop.security.UserGroupInformation:unprotectedRelogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean)",1347,1377,"/**
* Initiates re-login for the current user, clearing previous kerberos state.
* @param login HadoopLoginContext instance
* @param ignoreLastLoginTime whether to disregard last login time
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/WhitelistBasedResolver.java,getServerProperties,org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.lang.String),124,129,"/**
* Retrieves server properties for a given client address.
* @param clientAddress IP address of the client
* @return Map of server properties or default SASL properties if null
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,handle,org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:handle(javax.security.auth.callback.Callback[]),294,348,"/**
* Handles SASL DIGEST-MD5 callbacks, extracting user credentials and
* authentication status. @param callbacks array of Callback objects
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long,boolean)",97,118,"/**
* Initializes a CryptoOutputStream with specified parameters.
* @param out OutputStream to wrap
* @param codec encryption codec
* @param bufferSize buffer size for input/output operations
* @param key encryption key
* @param iv initialization vector
* @param streamOffset starting offset in the encrypted stream
* @param closeOutputStream whether to close the underlying OutputStream on close
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceSm4CtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:createEncryptor(),54,58,"/**
* Creates and returns an SM4-based encryptor instance.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceSm4CtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:createDecryptor(),60,64,"/**
* Creates a new SM4 decryption instance.
* @return Decryptor object used for decryption
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.JceAesCtrCryptoCodec:createEncryptor(),54,58,"/**
* Creates an instance of JceCtrCipher with AES encryption mode.
* @return Encryptor object or null if creation fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.JceAesCtrCryptoCodec:createDecryptor(),60,64,"/**
* Creates a decryptor instance using AES cipher in CTR mode.
* @throws GeneralSecurityException if encryption initialization fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,getInstance,"org.apache.hadoop.crypto.OpensslCipher:getInstance(java.lang.String,java.lang.String)",131,140,"/**
* Creates an instance of OpensslCipher with specified transformation and engine.
* @param transformation cipher transformation string
* @param engineId optional engine ID (null for default)
* @return OpensslCipher object or throws exception if invalid parameters
*/","* Return an <code>OpensslCipher</code> object that implements the specified
   * transformation.
   * 
   * @param transformation the name of the transformation, e.g., 
   * AES/CTR/NoPadding.
   * @param engineId the openssl engine to use.if not set,
   * defalut engine will be used.
   * @return OpensslCipher an <code>OpensslCipher</code> object
   * @throws NoSuchAlgorithmException if <code>transformation</code> is null, 
   * empty, in an invalid format, or if Openssl doesn't implement the 
   * specified algorithm.
   * @throws NoSuchPaddingException if <code>transformation</code> contains 
   * a padding scheme that is not available.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,isSupported,org.apache.hadoop.crypto.OpensslCipher:isSupported(org.apache.hadoop.crypto.CipherSuite),181,193,"/**
* Determines if a Cipher Suite is supported based on its components.
* @param suite the Cipher Suite to evaluate
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeyVersions(java.lang.String),376,398,"/**
* Retrieves a list of key versions by name.
* @param name unique identifier for the key
* @return List of KeyVersion objects or empty list if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,createKey,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",433,460,"/**
* Creates a new key with the specified name and material.
* @param name unique key identifier
* @param material key material (bytes)
* @param options creation options (cipher, bit length, description, attributes)
* @return KeyVersion object representing the newly created key
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:rollNewVersion(java.lang.String,byte[])",508,527,"/**
* Creates a new key version with the given name and material.
* @param name unique key identifier
* @param material byte array representing the key material
* @return KeyVersion object for the newly created version, or throws IOException if an error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSEncryptedKeyVersion:<init>(java.lang.String,java.lang.String,byte[],java.lang.String,byte[])",246,250,"/**
* Initializes a KMSEncryptedKeyVersion object with the provided parameters.
* @param keyName unique key identifier
* @param keyVersionName version name of the key material
* @param iv initialization vector for encryption
* @param encryptedVersionName name of the encrypted key version
* @param keyMaterial raw key material bytes
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONKeyVersion,org.apache.hadoop.util.KMSUtil:parseJSONKeyVersion(java.util.Map),185,202,"/**
* Parses JSON to create a KeyVersion object.
* @param valueMap map containing key version data
* @return KeyVersion object or null if input is empty
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONMetadata,org.apache.hadoop.util.KMSUtil:parseJSONMetadata(java.util.Map),204,218,"/**
* Parses JSON metadata from a Map.
* @param valueMap map containing metadata fields
* @return KeyProvider.Metadata object or null if empty
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,getCurrentKey,org.apache.hadoop.crypto.key.KeyProviderExtension:getCurrentKey(java.lang.String),66,69,"/**
* Retrieves current key version by name.
* @param name unique key identifier
* @return KeyVersion object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,createKey,"org.apache.hadoop.crypto.key.KeyProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",567,571,"/**
* Creates a new key with specified name and cryptographic properties.
* @param name unique key identifier
* @param options Key creation options (bit length & cipher)","* Create a new key generating the material for it.
   * The given key must not already exist.
   * <p>
   * This implementation generates the key material and calls the
   * {@link #createKey(String, byte[], Options)} method.
   *
   * @param name the base name of the key
   * @param options the options for the new key.
   * @return the version name of the first version of the key.
   * @throws IOException raised on errors performing I/O.
   * @throws NoSuchAlgorithmException no such algorithm exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.KeyProvider:rollNewVersion(java.lang.String),612,621,"/**
* Generates a new version of the given key.
* @param name unique key identifier
*/","* Roll a new version of the given key generating the material for it.
   * <p>
   * This implementation generates the key material and calls the
   * {@link #rollNewVersion(String, byte[])} method.
   *
   * @param name the basename of the key
   * @return the name of the new version of the key
   * @throws IOException              raised on errors performing I/O.
   * @throws NoSuchAlgorithmException This exception is thrown when a particular
   *                                  cryptographic algorithm is requested
   *                                  but is not available in the environment.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String,byte[])",140,146,"/**
* Rolls a new version of the key with specified name and material.
* @param name unique key identifier
* @param material byte array representing key material
* @return KeyVersion object or throws IOException if failure occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,toJSON,org.apache.hadoop.util.KMSUtil:toJSON(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),110,122,"/**
* Converts EncryptedKeyVersion to JSON Map.
*@param encryptedKeyVersion object containing encryption data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,warmUpEncryptedKeys,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:warmUpEncryptedKeys(java.lang.String[]),288,308,"/**
* Warm-up encrypted keys across multiple KMS providers.
* @param keyNames variable-length array of key names to warm up
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,close,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:close(),544,554,"/**
* Closes all KMS client providers and handles any IO exceptions.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,readLock,org.apache.hadoop.crypto.key.kms.ValueQueue:readLock(java.lang.String),115,117,"/**
* Acquires a read lock on the specified resource.
* @param keyName unique identifier of the resource
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,readUnlock,org.apache.hadoop.crypto.key.kms.ValueQueue:readUnlock(java.lang.String),119,121,"/**
 * Releases a shared lock on the specified resource.
 * @param keyName unique identifier of the locked resource
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,writeUnlock,org.apache.hadoop.crypto.key.kms.ValueQueue:writeUnlock(java.lang.String),123,125,"/**
 * Unlocks the write lock associated with the specified key.
 * @param keyName unique identifier for the lock
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,writeLock,org.apache.hadoop.crypto.key.kms.ValueQueue:writeLock(java.lang.String),127,129,"/**
* Acquires a write lock for the specified key.
* @param keyName unique identifier for the key being locked
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String),139,141,"/**
* Initializes builder with specified application context.
* @param context application context string
* @param separator default separator used between caller and application context (default value)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,unpack,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:unpack(org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto),70,80,"/**
* Unpacks a collection of refresh responses into a list.
* @param collection Collection of GenericRefreshResponse objects
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolServerSideTranslatorPB.java,refresh,"org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:refresh(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto)",44,62,"/**
* Refreshes data using the provided request and returns a collection of responses.
* @param controller RpcController instance
* @param request GenericRefreshRequestProto object containing refresh parameters
* @return GenericRefreshResponseCollectionProto object or throws ServiceException if error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,newEntry,"org.apache.hadoop.ipc.RetryCache:newEntry(long,byte[],int)",335,339,"/**
* Creates a new cache entry with specified expiration time and client ID.
* @param expirationTime time until entry expires
* @param clientId unique client identifier
* @param callId related call identifier
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:<init>(byte[],int,java.lang.Object,long)",155,159,"/**
* Constructs a cache entry with custom payload and expiration time.
* @param clientId unique client identifier
* @param callId call identifier
* @param payload custom data to be stored in the cache entry
* @param expirationTime when the cache entry will expire (in milliseconds)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntry:<init>(byte[],int,long,boolean)",84,88,"/**
* Creates a cache entry with specified attributes.
* @param clientId unique client identifier
* @param callId identifier for the current operation
* @param expirationTime timestamp until entry is valid
* @param success flag indicating successful operation
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isServerFailOverEnabledByQueue,org.apache.hadoop.ipc.Server:isServerFailOverEnabledByQueue(),3885,3888,"/**
* Checks server fail-over enabled status via queue API.
* @return true if enabled, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,put,org.apache.hadoop.ipc.CallQueueManager:put(org.apache.hadoop.ipc.Schedulable),289,299,"/**
* Stores an element in the map with optional client-side backoff handling.
* @param e the element to store
*/","* Insert e into the backing queue or block until we can.  If client
   * backoff is enabled this method behaves like add which throws if
   * the queue overflows.
   * If we block and the queue changes on us, we will insert while the
   * queue is drained.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,add,org.apache.hadoop.ipc.CallQueueManager:add(org.apache.hadoop.ipc.Schedulable),301,304,"/**
* Adds an element to the collection.
* @param e the element to be added
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,callQueueLength,org.apache.hadoop.ipc.metrics.RpcMetrics:callQueueLength(),167,169,"/**
* Returns the length of the server's call queue.
* @return The number of calls currently in the queue.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,initialize,org.apache.hadoop.ipc.WritableRpcEngine:initialize(),80,84,"/**
* Initializes the RPC server with writable protocol engine.
*/",* Register the rpcRequest deserializer for WritableRpcEngine,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,registerProtocolEngine,org.apache.hadoop.ipc.ProtobufRpcEngine2:registerProtocolEngine(),70,77,"/**
* Registers the protocol buffer engine for RPC invoker.
* @param RpcKind enum value representing protocol buffer type
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,setExpirationTime,"org.apache.hadoop.util.LightWeightCache:setExpirationTime(org.apache.hadoop.util.LightWeightCache$Entry,long)",147,149,"/**
* Sets the expiration time of an entry based on the provided period.
* @param e the entry to update
* @param expirationPeriod time in nanoseconds until expiration
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,start,org.apache.hadoop.util.StopWatch:start(),58,65,"/**
* Starts the stopwatch, throwing an exception if it's already running.
* @return this StopWatch instance for method chaining
*/","* Start to measure times and make the state of stopwatch running.
   * @return this instance of StopWatch.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,stop,org.apache.hadoop.util.StopWatch:stop(),71,79,"/**
* Stops the StopWatch and returns elapsed time in nanoseconds.
*/","* Stop elapsed time and make the state of stopwatch stop.
   * @return this instance of StopWatch.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,now,org.apache.hadoop.util.StopWatch:now(),105,109,"/**
* Calculates elapsed time in nanoseconds since execution started.
* @return elapsed time or current elapsed time if not started
*/",* @return current elapsed time in nanosecond.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,sendResponse,org.apache.hadoop.ipc.Server$Call:sendResponse(),1086,1094,"/**
* Sends a response when all pending requests have finished.
* @throws IOException if an I/O error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,abortResponse,org.apache.hadoop.ipc.Server$Call:abortResponse(java.lang.Throwable),1096,1103,"/**
* Aborts an ongoing response to a remote client, propagating any exception. 
* @throws IOException on failure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getFingerprint,org.apache.hadoop.ipc.ProtocolSignature:getFingerprint(java.lang.reflect.Method[]),140,142,"/**
* Calculates a unique fingerprint from an array of Method objects.
* @param methods array of Method objects to process
*/","* Get the hash code of an array of methods
   * Methods are sorted before hashcode is calculated.
   * So the returned value is irrelevant of the method order in the array.
   * 
   * @param methods an array of methods
   * @return the hash code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getSigFingerprint,"org.apache.hadoop.ipc.ProtocolSignature:getSigFingerprint(java.lang.Class,long)",186,200,"/**
* Retrieves protocol signature fingerprint based on class and server version.
* @param protocol Class of the protocol to retrieve fingerprint for
* @param serverVersion Server version associated with the fingerprint
* @return ProtocolSigFingerprint object or null if not cached
*/","* Return a protocol's signature and finger print from cache
   * 
   * @param protocol a protocol class
   * @param serverVersion protocol version
   * @return its signature and finger print",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,valueOf,org.apache.hadoop.ipc.RemoteException:valueOf(org.xml.sax.Attributes),131,134,"/**
* Constructs a RemoteException from Attributes.
* @param attrs attributes containing exception details
*/","* Create RemoteException from attributes.
   * @param attrs may not be null.
   * @return RemoteException.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,readResponse,org.apache.hadoop.ipc.Client$IpcStreams:readResponse(),1922,1944,"/**
* Reads RPC response from input stream and returns as ByteBuffer.
* @throws IOException if read operation fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldFailoverOnException,org.apache.hadoop.io.retry.RetryPolicies:shouldFailoverOnException(java.lang.Exception),773,781,"/**
* Determines whether to failover on an exception.
* @param e the exception to evaluate
* @return true if the exception is a standby exception, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,getWrappedRetriableException,org.apache.hadoop.io.retry.RetryPolicies:getWrappedRetriableException(java.lang.Exception),795,803,"/**
* Extracts wrapped RetriableException from RemoteException.
* @param e Exception to unwrap
* @return RetriableException object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,monitorHealth,"org.apache.hadoop.ha.HAServiceProtocolHelper:monitorHealth(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",34,42,"/**
* Monitors HA service health and handles exceptions.
* @param svc HAServiceProtocol instance to interact with
* @throws IOException if communication error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,transitionToActive,"org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",44,52,"/**
* Transitions a HA Service to active state.
* @param svc HAServiceProtocol instance
* @param reqInfo StateChangeRequestInfo containing transition details
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,transitionToStandby,"org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",54,62,"/**
* Transitions HA service to standby state.
* @param svc HAServiceProtocol instance
* @param reqInfo StateChangeRequestInfo object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,transitionToObserver,"org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",64,71,"/**
* Transitions HA service to observer state.
* @param svc HAServiceProtocol instance
* @param reqInfo StateChangeRequestInfo object containing transition details
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,isHealthCheckFailedException,org.apache.hadoop.ha.HealthMonitor:isHealthCheckFailedException(java.lang.Throwable),229,235,"/**
* Checks if the provided exception is a HealthCheckFailedException or its wrapper.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,get,org.apache.hadoop.fs.PartialListing:get(),67,72,"/**
* Returns paginated list of elements.
* @throws IOException if an I/O error occurs
*/","* Partial listing of the path being listed. In the case where the path is
   * a file. The list will be a singleton with the file itself.
   *
   * @return Partial listing of the path being listed.
   * @throws IOException if there was an exception getting the listing.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,org.apache.hadoop.ipc.Server$Call:<init>(org.apache.hadoop.ipc.Server$Call),985,988,"/**
* Constructs a new Call instance from an existing one.
* @param call existing Call object to copy properties from
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$Call:<init>(int,int,org.apache.hadoop.ipc.RPC$RpcKind,byte[])",990,992,"/**
* Initializes an RPC call with specified parameters.
* @param id unique request ID
* @param retryCount maximum retries allowed
* @param kind type of RPC operation (e.g. sync/async)
* @param clientId client identifier (byte array)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$Call:<init>(int,int,java.lang.Void,java.lang.Void,org.apache.hadoop.ipc.RPC$RpcKind,byte[])",994,998,"/**
* Creates a new Call instance for testing purposes.
* @param id unique call identifier
* @param retryCount number of retries allowed
* @param kind RPC operation type
* @param clientId client identifier as byte array
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcScheduler.java,addResponseTime,"org.apache.hadoop.ipc.RpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",65,78,"/**
* Backwards-compatible implementation of add response time with old method invocation.
* @param callName service name
* @param schedulable scheduling details
* @param details processing metrics
*/","* Store a processing time value for an RPC call into this scheduler.
   *
   * @param callName The name of the call.
   * @param schedulable The schedulable representing the incoming call.
   * @param details The details of processing time.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,numDroppedConnections,org.apache.hadoop.ipc.metrics.RpcMetrics:numDroppedConnections(),171,173,"/**
* Retrieves the total number of dropped connections.
* @return The count of dropped connections
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,register,"org.apache.hadoop.ipc.Server$ConnectionManager:register(java.nio.channels.SocketChannel,int,boolean)",4097,4108,"/**
* Establishes a new server connection.
* @param channel SocketChannel representing the client
* @param ingressPort port where data is received
* @param isOnAuxiliaryPort whether on auxiliary port
* @return Connection object or null if full",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,numOpenConnections,org.apache.hadoop.ipc.metrics.RpcMetrics:numOpenConnections(),153,155,"/**
* Returns the current number of open server connections.
* @return Number of active connections
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offerQueues,"org.apache.hadoop.ipc.FairCallQueue:offerQueues(int,org.apache.hadoop.ipc.Schedulable,boolean)",257,267,"/**
* Offers elements to queues with specified priority and includes/excludes the last queue.
* @param priority minimum queue index to offer to
* @param e element to offer
* @param includeLast whether to include the last queue in offering
* @return true if any queue was successfully offered to, false otherwise
*/","* Offer the element to queue of the given or lower priority.
   * @param priority - starting queue priority
   * @param e - element to add
   * @param includeLast - whether to attempt last queue
   * @return boolean if added to a queue",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,"org.apache.hadoop.ipc.FairCallQueue:offer(java.lang.Object,long,java.util.concurrent.TimeUnit)",269,279,"/**
* Offers an element to the designated priority queue with a specified timeout.
* @param e element to offer
* @param timeout maximum time to wait for offer operation
* @param unit unit of time for timeout (e.g. TimeUnit.SECONDS)
* @return true if offered successfully, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,org.apache.hadoop.ipc.FairCallQueue:offer(java.lang.Object),281,290,"/**
* Offers an element to the queue based on its priority level.
* @param e the element to offer
* @return true if offered successfully, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,populateResponseParamsOnError,"org.apache.hadoop.ipc.Server$RpcCall:populateResponseParamsOnError(java.lang.Throwable,org.apache.hadoop.ipc.Server$RpcCall$ResponseParams)",1279,1302,"/**
* Handles errors by logging and populating error details into ResponseParams.
* @param t the Throwable instance containing error information
* @param responseParams container for error response parameters
*/","* @param t              the {@link java.lang.Throwable} to use to set
     *                       errorInfo
     * @param responseParams the {@link ResponseParams} instance to populate",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.java,getProtocolVersionForRpcKind,"org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersionForRpcKind(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",106,120,"/**
* Retrieves protocol version(s) for the specified RPC kind and protocol.
* @param rpcKind RPC operation type
* @param protocol name of the protocol to fetch versions for
* @return array of long values representing supported protocol versions or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcNoSuchProtocolException.java,<init>,org.apache.hadoop.ipc.RpcNoSuchProtocolException:<init>(java.lang.String),29,31,"/**
* Constructs an RpcNoSuchProtocolException with the specified error message.
* @param message detailed description of the protocol not found issue
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcNoSuchMethodException.java,<init>,org.apache.hadoop.ipc.RpcNoSuchMethodException:<init>(java.lang.String),30,32,"/**
* Constructs an RpcNoSuchMethodException with the given error message.
* @param message detailed description of the exception
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,<init>,"org.apache.hadoop.ipc.RPC$VersionMismatch:<init>(java.lang.String,long,long)",248,255,"/**
* Constructs a VersionMismatch exception with protocol and version details.
* @param interfaceName name of the protocol
* @param clientVersion client-side version number
* @param serverVersion server-side version number
*/","* Create a version mismatch exception
     * @param interfaceName the name of the protocol mismatch
     * @param clientVersion the client's version of the protocol
     * @param serverVersion the server's version of the protocol",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$FatalRpcServerException:<init>(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,java.io.IOException)",1986,1989,"/**
* Constructs a FatalRpcServerException with error code and cause.
* @param errCode Rpc error code
* @param ioe underlying IOException",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,<init>,org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:<init>(int),78,81,"/**
* Initializes a new FramedBuffer instance with specified capacity.
* @param capacity buffer size in bytes
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,reset,org.apache.hadoop.ipc.ResponseBuffer:reset(),70,74,"/**
* Resets response buffer to initial state.
* @return this instance for method chaining
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,writeTo,org.apache.hadoop.ipc.ResponseBuffer:writeTo(java.io.OutputStream),48,50,"/**
 * Writes framed buffer contents to specified OutputStream.
 * @param out OutputStream to write to
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,toByteArray,org.apache.hadoop.ipc.ResponseBuffer:toByteArray(),52,54,"/**
* Converts this object's internal buffer into a raw byte array.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,recomputeScheduleCache,org.apache.hadoop.ipc.DecayRpcScheduler:recomputeScheduleCache(),545,560,"/**
* Recomputes the schedule cache with updated priority levels.
*/",* Update the scheduleCache to match current conditions in callCosts.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,cachedOrComputedPriorityLevel,org.apache.hadoop.ipc.DecayRpcScheduler:cachedOrComputedPriorityLevel(java.lang.Object),642,661,"/**
* Computes or retrieves cached priority level for a given object.
* @param identity unique identifier of the object
* @return priority level (cached or computed)
*/","* Returns the priority level for a given identity by first trying the cache,
   * then computing it.
   * @param identity an object responding to toString and hashCode
   * @return integer scheduling decision from 0 to numLevels - 1",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,setPriorityLevel,"org.apache.hadoop.ipc.CallQueueManager:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",272,276,"/**
* Sets priority level for a given user.
* @param user User information
* @param priority New priority value
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getCallVolumeSummary,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getCallVolumeSummary(),914,922,"/**
* Retrieves call volume summary from the active RPC scheduler.
* @return Call volume summary string or ""No Active Scheduler"" if none found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,constructRpcRequest,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:constructRpcRequest(java.lang.reflect.Method,com.google.protobuf.Message)",296,299,"/**
* Constructs an RPC request with a protobuf header.
* @param method the invoked method
* @param theRequest the original message
* @return a Writable RPC request object
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,constructRpcRequest,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:constructRpcRequest(java.lang.reflect.Method,org.apache.hadoop.thirdparty.protobuf.Message)",306,309,"/**
* Constructs an RPC request from a given method and message.
* @param method the method associated with the request
* @param theRequest underlying message data
* @return RpcProtobufRequest object containing the constructed header and body
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.java,refreshServiceAcl,org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:refreshServiceAcl(),54,58,"/**
* Refreshes service ACL by invoking RPC proxy.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,refreshUserToGroupsMappings,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:refreshUserToGroupsMappings(),59,63,"/**
* Refreshes user-to-groups mappings.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,refreshSuperUserGroupsConfiguration,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:refreshSuperUserGroupsConfiguration(),65,69,"/**
* Refreshes super user groups configuration.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.java,refreshCallQueue,org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:refreshCallQueue(),54,58,"/**
* Refreshes call queue via IPC.
* @throws IOException if communication error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolClientSideTranslatorPB.java,getGroupsForUser,org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:getGroupsForUser(java.lang.String),52,59,"/**
* Retrieves user groups by ID.
* @param user unique user identifier
* @return array of group names or empty array if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,cedeActive,org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:cedeActive(int),56,63,"/**
* Initiates active session cession after a specified time interval.
* @param millisToCede time to wait before ceding active status
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,gracefulFailover,org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:gracefulFailover(),65,69,"/**
* Initiates a graceful failover to NULL_CONTROLLER.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,monitorHealth,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:monitorHealth(),84,87,"/**
* Calls RPC to monitor health.
* @throws IOException on communication failure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,getServiceStatus,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:getServiceStatus(),114,128,"/**
* Retrieves HA service status via RPC.
* @return HAServiceStatus object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invokeMethod,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod(),165,171,"/**
* Invokes a method with retry logic and RPC call ID setup if enabled.
* @throws Throwable if an invocation error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,close,org.apache.hadoop.ipc.WritableRpcEngine$Invoker:close(),264,270,"/**
* Closes the client and marks this instance as closed.
* @throws No exception, but will stop any active clients. 
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,close,org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:close(),325,331,"/**
* Closes the client and marks this instance as closed.
* @throws IOException if an I/O error occurs during closure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,close,org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:close(),335,341,"/**
* Closes the client and marks this instance as closed.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getMetrics,"org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",462,477,"/**
* Adds Fair Call Queue metrics to the collector.
* @param collector MetricsCollector instance
* @param all whether to collect all metrics (implies single point in time)
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,wrap,org.apache.hadoop.ipc.RpcWritable:wrap(java.lang.Object),40,53,"/**
* Wraps the given object into a RpcWritable instance, 
* handling various types including protobuf and writable. 
* @param o object to be wrapped
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,hashCode,org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:hashCode(),174,177,"/**
* Returns the hash code of this object.
* Delegates to superclass implementation.",Override hashcode to avoid findbugs warnings,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.java,getAndAdvanceCurrentIndex,org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:getAndAdvanceCurrentIndex(),145,149,"/**
* Returns current index and advances to next one.
* @return current index value
*/",* Use the mux by getting and advancing index.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,registerForDeferredResponse,org.apache.hadoop.ipc.ProtobufRpcEngine$Server:registerForDeferredResponse(),421,426,"/**
* Registers a callback for deferred response handling.
* @return A ProtobufRpcEngineCallback instance to handle responses
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,registerForDeferredResponse2,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:registerForDeferredResponse2(),453,458,"/**
* Registers a callback for deferred responses.
* @return ProtobufRpcEngineCallback2 instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,writeTo,org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:writeTo(org.apache.hadoop.ipc.ResponseBuffer),110,116,"/**
* Serializes the message and writes it to the response buffer.
* @param out ResponseBuffer to write the serialized message to
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,writeTo,org.apache.hadoop.ipc.RpcWritable$Buffer:writeTo(org.apache.hadoop.ipc.ResponseBuffer),159,163,"/**
* Writes remaining buffer contents to response output.
* @param out Response buffer for writing data
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufWrapperLegacy.java,writeTo,org.apache.hadoop.ipc.ProtobufWrapperLegacy:writeTo(org.apache.hadoop.ipc.ResponseBuffer),63,70,"/**
* Writes the serialized protocol buffer message to the given output stream.
* @param out output stream to write to
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,cleanupCalls,org.apache.hadoop.ipc.Client$Connection:cleanupCalls(),1307,1314,"/**
* Removes and updates exceptions for all stored calls.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemoteAddress,org.apache.hadoop.ipc.Server:getRemoteAddress(),436,439,"/**
* Retrieves the remote IP address as a string.
* @return Remote IP address or null if not available
*/","@return Returns remote address as a string when invoked inside an RPC.
   *  Returns null in case of an error.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,numOpenConnectionsPerUser,org.apache.hadoop.ipc.metrics.RpcMetrics:numOpenConnectionsPerUser(),162,165,"/**
* Retrieves number of open connections per user.
* @return Number of open connections per user as a string representation.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doAsyncWrite,org.apache.hadoop.ipc.Server$Responder:doAsyncWrite(java.nio.channels.SelectionKey),1798,1821,"/**
* Processes an async write operation for a given RPC call.
* @param key SelectionKey representing the asynchronous operation
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRespond,org.apache.hadoop.ipc.Server$Responder:doRespond(org.apache.hadoop.ipc.Server$RpcCall),1927,1939,"/**
* Adds response to connection's queue and processes it.
* @param call RpcCall object containing the response
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsTracer.java,get,org.apache.hadoop.fs.FsTracer:get(org.apache.hadoop.conf.Configuration),39,47,"/**
* Returns a shared singleton Tracer instance for tracing HDFS client activity.
* @param conf HDFS client configuration
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,"org.apache.hadoop.util.MachineList:<init>(java.lang.String,org.apache.hadoop.util.MachineList$InetAddressFactory)",77,79,"/**
* Constructs a MachineList instance from a comma-separated list of host entries.
* @param hostEntries comma-separated list of machine identifiers
* @param addressFactory factory for creating InetAddress instances
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,org.apache.hadoop.util.MachineList:<init>(java.util.Collection),85,87,"/**
* Initializes a new MachineList instance from given host entries.
* @param hostEntries collection of host entries
*/","*
   * @param hostEntries collection of separated ip/cidr/host addresses",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FileBasedIPList.java,isIn,org.apache.hadoop.util.FileBasedIPList:isIn(java.lang.String),71,77,"/**
* Checks if IP address is in allowed list.
* @param ipAddress IP address to check
* @return true if IP is allowed, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,<init>,org.apache.hadoop.util.SysInfoLinux:<init>(),180,183,"/**
* Initializes Linux system info with default file paths.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcMemInfoFile,org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile(),215,217,"/**
 * Reads process memory information from file.
 * Calls readProcMemInfoFile(true) to enable logging.
 */","* Read /proc/meminfo, parse and compute memory information only once.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getAvailablePhysicalMemorySize,org.apache.hadoop.util.SysInfoLinux:getAvailablePhysicalMemorySize(),609,616,"/**
* Calculates the total available physical memory size in bytes.
* @return Available physical memory size in bytes, or 0 if failed
*/",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getCumulativeCpuTime,org.apache.hadoop.util.SysInfoLinux:getCumulativeCpuTime(),646,650,"/**
* Retrieves cumulative CPU time.
*/",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getCpuUsagePercentage,org.apache.hadoop.util.SysInfoLinux:getCpuUsagePercentage(),653,661,"/**
* Calculates and returns the overall CPU usage percentage.
* @return CPU usage as a float value between 0.0 and 100.0
*/",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNumVCoresUsed,org.apache.hadoop.util.SysInfoLinux:getNumVCoresUsed(),664,672,"/**
* Calculates the overall CPU core usage as a percentage.
* @return normalized CPU core usage value (0-1) or UNAVAILABLE if not available
*/",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getStorageBytesRead,org.apache.hadoop.util.SysInfoLinux:getStorageBytesRead(),688,692,"/**
* Retrieves total bytes read from storage.
* @return Total bytes read in long format
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getStorageBytesWritten,org.apache.hadoop.util.SysInfoLinux:getStorageBytesWritten(),694,698,"/**
* Returns total storage bytes written across all disks.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,<init>,org.apache.hadoop.util.IdentityHashStore:<init>(int),62,72,"/**
* Initializes an IdentityHashStore with specified capacity.
* @param capacity maximum number of entries
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,put,"org.apache.hadoop.util.IdentityHashStore:put(java.lang.Object,java.lang.Object)",118,126,"/**
* Inserts a key-value pair into the buffer.
* @param k unique key
* @param v associated value
*/","* Add a new (key, value) mapping.
   *
   * Inserting a new (key, value) never overwrites a previous one.
   * In other words, you can insert the same key multiple times and it will
   * lead to multiple entries.
   *
   * @param k Generics Type k.
   * @param v Generics Type v.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,releaseBuffer,org.apache.hadoop.fs.FSDataInputStream:releaseBuffer(java.nio.ByteBuffer),222,235,"/**
* Releases a ByteBuffer, potentially returning it to its pool.
* @param buffer the ByteBuffer to release
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,hasNext,org.apache.hadoop.util.LightWeightGSet$SetIterator:hasNext(),329,333,"/**
* Checks if there is another element available.
* @return true if an element exists, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,next,org.apache.hadoop.util.LightWeightGSet$SetIterator:next(),335,344,"/**
* Retrieves the next element from the iterator, converting it to type E.
* @return The next element in the iteration or throws an exception if exhausted
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,put,org.apache.hadoop.util.LightWeightGSet:put(java.lang.Object),149,177,"/**
* Inserts a LinkedElement at the beginning of the linked list.
* @param element LinkedElement to be inserted
* @return previous instance of the same element, or null if new insertion
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,remove,org.apache.hadoop.util.LightWeightGSet:remove(java.lang.Object),221,228,"/**
* Removes an entry associated with the given key.
* @param key unique identifier
* @return element of type E or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sort,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:sort(int),3253,3256,"/**
* Sorts first 'count' elements of pointers array using merge sort.
* @param count number of elements to sort
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,newSecureTransformerFactory,org.apache.hadoop.util.XMLUtils:newSecureTransformerFactory(),148,154,"/**
* Creates a secure transformer factory with enabled processing and optional security attributes.
* @return TransformerFactory instance or throws TransformerConfigurationException if initialization fails
*/","* This method should be used if you need a {@link TransformerFactory}. Use this method
   * instead of {@link TransformerFactory#newInstance()}. The factory that is returned has
   * secure configuration enabled.
   *
   * @return a {@link TransformerFactory} with secure configuration enabled
   * @throws TransformerConfigurationException if the {@code JAXP} transformer does not
   * support the secure configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,newSecureSAXTransformerFactory,org.apache.hadoop.util.XMLUtils:newSecureSAXTransformerFactory(),165,171,"/**
* Creates a secure SAX transformer factory instance.
* @return SAXTransformerFactory object with security features enabled
*/","* This method should be used if you need a {@link SAXTransformerFactory}. Use this method
   * instead of {@link SAXTransformerFactory#newInstance()}. The factory that is returned has
   * secure configuration enabled.
   *
   * @return a {@link SAXTransformerFactory} with secure configuration enabled
   * @throws TransformerConfigurationException if the {@code JAXP} transformer does not
   * support the secure configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,formatSize,"org.apache.hadoop.fs.ContentSummary:formatSize(long,boolean)",477,481,"/**
* Formats file size in bytes to a human-readable string or long value.
* @param size file size in bytes
* @param humanReadable true for human-readable format (e.g., KB, MB), false for raw byte count
* @return formatted size string or long byte count","* Formats a size to be human readable or in bytes.
   * @param size value to be formatted
   * @param humanReadable flag indicating human readable or not
   * @return String representation of the size",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,formatSize,org.apache.hadoop.fs.shell.Ls:formatSize(long),126,130,"/**
* Formats file size in human-readable format or as a raw number.
* @param size file size in bytes
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,formatSize,org.apache.hadoop.fs.shell.FsUsage:formatSize(long),55,59,"/**
* Formats file size in human-readable units (e.g., bytes, KB, MB).
* @param size file size in bytes
* @return formatted string representation of the size
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,formatSize,"org.apache.hadoop.fs.QuotaUsage:formatSize(long,boolean)",394,398,"/**
* Formats file size in bytes to a human-readable format or raw value.
* @param size file size in bytes
* @param humanReadable true for human-readable format (e.g. KB, MB), false for raw value
* @return formatted string representation of the file size
*/","* Formats a size to be human readable or in bytes.
   * @param size value to be formatted
   * @param humanReadable flag indicating human readable or not
   * @return String representation of the size",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,humanReadableInt,org.apache.hadoop.util.StringUtils:humanReadableInt(long),132,135,"/**
* Converts long integer to human-readable string representation.
* @param number large integer value
*/","* Given an integer, return a string that is in an approximate, but human 
   * readable format. 
   * @param number the number to format
   * @return a human readable form of the integer
   *
   * @deprecated use {@link TraditionalBinaryPrefix#long2String(long, String, int)}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteDesc,org.apache.hadoop.util.StringUtils:byteDesc(long),1022,1024,"/**
* Converts byte count to human-readable string representation.
* @param len bytes to convert
* @return string description (e.g. ""1K"", ""5M"", etc.)
*/","* a byte description of the given long interger value.
   *
   * @param len len.
   * @return a byte description of the given long interger value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,computeCapacity,"org.apache.hadoop.util.LightWeightGSet:computeCapacity(long,double,java.lang.String)",379,417,"/**
* Calculates capacity based on max memory and percentage.
* @param maxMemory maximum available memory
* @param percentage percentage of max memory to use
* @param mapName name of the map for logging purposes
* @return calculated capacity or 0 if invalid inputs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,addToUsagesTable,"org.apache.hadoop.fs.shell.FsUsage$Df:addToUsagesTable(java.net.URI,org.apache.hadoop.fs.FsStatus,java.lang.String)",114,127,"/**
* Adds a row to the usages table with capacity and usage statistics.
* @param uri URI of the file system
* @param fsStatus FsStatus object containing capacity and usage metrics
* @param mountedOnPath path where the file system is mounted
*/","* Add a new row to the usages table for the given FileSystem URI.
     *
     * @param uri - FileSystem URI
     * @param fsStatus - FileSystem status
     * @param mountedOnPath - FileSystem mounted on path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,readChecksumChunk,"org.apache.hadoop.fs.FSInputChecker:readChecksumChunk(byte[],int,int)",293,334,"/**
* Reads a chunk of data and verifies its checksum.
* @param b input byte array
* @param off offset in byte array
* @param len length of chunk to read
* @return number of bytes read, or throws IOException if failed.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,readChars,"org.apache.hadoop.io.UTF8:readChars(java.io.DataInput,java.lang.StringBuilder,int)",279,332,"/**
* Reads and decodes a sequence of Unicode characters from the input stream.
* @param in input stream containing encoded data
* @param buffer StringBuilder to accumulate decoded characters
* @param nBytes number of bytes to read from the input stream
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteToHexString,org.apache.hadoop.util.StringUtils:byteToHexString(byte[]),199,201,"/**
* Converts a byte array to its hexadecimal string representation.
* @param bytes input byte array
*/","* Same as byteToHexString(bytes, 0, bytes.length).
   * @param bytes bytes.
   * @return byteToHexString.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HeapSort.java,sort,"org.apache.hadoop.util.HeapSort:sort(org.apache.hadoop.util.IndexedSortable,int,int)",51,54,"/**
* Calls recursive sorting algorithm with default comparator.
* @param s IndexedSortable object to be sorted
* @param p left index of the subarray
* @param r right index of the subarray","* Sort the given range of items using heap sort.
   * {@inheritDoc}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exit,org.apache.hadoop.service.launcher.ServiceLauncher:exit(org.apache.hadoop.util.ExitUtil$ExitException),874,876,"/**
 * Terminates application with specified exception.
 * @param ee ExitException instance containing termination details
 */","* Exit the JVM using an exception for the exit code and message,
   * invoking {@link ExitUtil#terminate(ExitUtil.ExitException)}.
   *
   * This is the standard way a launched service exits.
   * An error code of 0 means success -nothing is printed.
   *
   * If {@link ExitUtil#disableSystemExit()} has been called, this
   * method will throw the exception.
   *
   * The method <i>may</i> be subclassed for testing
   * @param ee exit exception
   * @throws ExitUtil.ExitException if ExitUtil exceptions are disabled",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exitWithMessage,"org.apache.hadoop.service.launcher.ServiceLauncher:exitWithMessage(int,java.lang.String)",1024,1026,"/**
 * Terminates application with a service launch exception and custom error message.
 * @param status termination status code
 * @param message human-readable error description
 */","* Exit with a printed message. 
   * @param status status code
   * @param message message message to print before exiting
   * @throws ExitUtil.ExitException if exceptions are disabled",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,"org.apache.hadoop.util.ExitUtil:terminate(int,java.lang.Throwable)",338,344,"/**
* Recursively terminates with an ExitException, propagating the cause.
* @param status exit status
* @param t Throwable exception to propagate (or null for default)
*/","* Like {@link #terminate(int, String)} but uses the given throwable to
   * build the message to display or throw as an
   * {@link ExitException}.
   * <p>
   * @param status exit code to use if the exception is not an ExitException.
   * @param t throwable which triggered the termination. If this exception
   * is an {@link ExitException} its status overrides that passed in.
   * @throws ExitException if {@link System#exit(int)}  is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,"org.apache.hadoop.util.ExitUtil:terminate(int,java.lang.String)",380,382,"/**
 * Terminates the application with a specified exit status and message.
 * @param status exit status code
 * @param msg error message to display on termination
 */","* Terminate the current process. Note that terminate is the *only* method
   * that should be used to terminate the daemon processes.
   *
   * @param status exit code
   * @param msg message used to create the {@code ExitException}
   * @throws ExitException if {@link System#exit(int)} is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,"org.apache.hadoop.util.ExitUtil:halt(int,java.lang.Throwable)",354,360,"/**
* Recursively halts execution with the given status and exception.
* @param status exit status code
* @param t throwable causing termination
*/","* Forcibly terminates the currently running Java virtual machine.
   *
   * @param status exit code to use if the exception is not a HaltException.
   * @param t throwable which triggered the termination. If this exception
   * is a {@link HaltException} its status overrides that passed in.
   * @throws HaltException if {@link System#exit(int)}  is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,"org.apache.hadoop.util.ExitUtil:halt(int,java.lang.String)",399,401,"/**
 * Throws an HaltException with specified status and message.
 * @param status exit status code
 * @param message descriptive error message
 */","* Forcibly terminates the currently running Java virtual machine.
   * @param status status code
   * @param message message
   * @throws HaltException if {@link Runtime#halt(int)} is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,unregister,org.apache.hadoop.service.launcher.ServiceShutdownHook:unregister(),69,75,"/**
* Unregisters this object from the JVM's shutdown hook mechanism.
*/",* Unregister the hook.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/QuickSort.java,sort,"org.apache.hadoop.util.QuickSort:sort(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable)",63,67,"/**
* Recursively sorts a portion of an indexed sortable object.
* @param s the IndexedSortable to be sorted
* @param p starting index
* @param r ending index
* @param rep progress reporter
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,<init>,org.apache.hadoop.util.LightWeightResizableGSet:<init>(),83,85,"/**
 * Initializes an empty GSet with default capacity and load factor.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,<init>,org.apache.hadoop.util.LightWeightResizableGSet:<init>(int),87,89,"/**
* Initializes a new instance of LightWeightResizableGSet with specified initial capacity.
* @param initCapacity initial number of elements in the set
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayList,org.apache.hadoop.util.Lists:newArrayList(java.lang.Iterable),91,98,"/**
* Creates a new ArrayList instance from the given iterable.
* @param elements Iterable containing elements to be added
*/","* Creates a <i>mutable</i> {@code ArrayList} instance containing the
   * given elements; a very thin shortcut for creating an empty list then
   * calling Iterables#addAll.
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return ArrayList Generics Type E.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newLinkedList,org.apache.hadoop.util.Lists:newLinkedList(java.lang.Iterable),185,190,"/**
* Creates a new linked list populated with the given iterable elements.
* @param elements iterable collection of elements to be added
* @return newly created linked list
*/","* Creates a <i>mutable</i> {@code LinkedList} instance containing the given
   * elements; a very thin shortcut for creating an empty list then calling
   * Iterables#addAll.
   *
   * <p><b>Performance note:</b> {@link ArrayList} and
   * {@link java.util.ArrayDeque} consistently
   * outperform {@code LinkedList} except in certain rare and specific
   * situations. Unless you have spent a lot of time benchmarking your
   * specific needs, use one of those instead.</p>
   *
   * @param elements elements.
   * @param <E> Generics Type E.
   * @return Generics Type E List.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclUtil.java,getAclFromPermAndEntries,"org.apache.hadoop.fs.permission.AclUtil:getAclFromPermAndEntries(org.apache.hadoop.fs.permission.FsPermission,java.util.List)",42,90,"/**
* Constructs a comprehensive ACL list from permission bits and entries.
* @param perm FsPermission object containing permission bits
* @param entries List of AclEntry objects or null if not applicable
* @return List of AclEntry objects representing the constructed ACL
*/","* Given permissions and extended ACL entries, returns the full logical ACL.
   *
   * @param perm FsPermission containing permissions
   * @param entries List&lt;AclEntry&gt; containing extended ACL entries
   * @return List&lt;AclEntry&gt; containing full logical ACL",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,addChunk,org.apache.hadoop.util.ChunkedArrayList:addChunk(int),156,160,"/**
* Initializes and adds a new chunk with specified capacity.
* @param capacity size of the new chunk
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayList,org.apache.hadoop.util.Lists:newArrayList(java.lang.Object[]),70,80,"/**
* Creates a new ArrayList with specified elements.
* @param elements variable number of elements to initialize the list with
*/","* Creates a <i>mutable</i> {@code ArrayList} instance containing the given
   * elements.
   *
   * <p>Note that even when you do need the ability to add or remove,
   * this method provides only a tiny bit of syntactic sugar for
   * {@code newArrayList(}
   * {@link Arrays#asList asList}
   * {@code (...))}, or for creating an empty list then calling
   * {@link Collections#addAll}.
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return ArrayList Generics Type E.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayListWithExpectedSize,org.apache.hadoop.util.Lists:newArrayListWithExpectedSize(int),148,151,"/**
* Creates an empty ArrayList with expected size.
* @param estimatedSize initial capacity estimate
*/","* Creates an {@code ArrayList} instance to hold {@code estimatedSize}
   * elements, <i>plus</i> an unspecified amount of padding;
   * you almost certainly mean to call {@link
   * #newArrayListWithCapacity} (see that method for further advice on usage).
   *
   * @param estimatedSize an estimate of the eventual {@link List#size()}
   *     of the new list.
   * @return a new, empty {@code ArrayList}, sized appropriately to hold the
   *     estimated number of elements.
   * @throws IllegalArgumentException if {@code estimatedSize} is negative.
   *
   * @param <E> Generics Type E.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,loadClass,org.apache.hadoop.util.ApplicationClassLoader:loadClass(java.lang.String),155,158,"/**
* Loads a class by name, delegating to internal load mechanism.
* @param name fully qualified class name
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,save,"org.apache.hadoop.util.JsonSerialization:save(java.io.File,java.lang.Object)",201,204,"/**
* Saves an object as JSON to a file.
* @param file target file path
* @param instance object to serialize and save
*/","* Save to a local file. Any existing file is overwritten unless
   * the OS blocks that.
   * @param file file
   * @param instance instance
   * @throws IOException IO exception",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,<init>,"org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String,long)",72,81,"/**
* Initializes StatisticDurationTracker with given statistics store and parameters.
* @param iostats IOStatisticsStore instance
* @param key unique statistic identifier
* @param count positive counter value (optional)
*/","* Constructor.
   * If the supplied count is greater than zero, the counter
   * of the key name is updated.
   * @param iostats statistics to update
   * @param key Key to use as prefix of values.
   * @param count #of times to increment the matching counter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DurationInfo.java,<init>,"org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[])",69,83,"/**
* Initializes a DurationInfo object with logging and text formatting.
* @param log Logger instance for logging messages
* @param logAtInfo Whether to log at INFO level or not
* @param format Text format string with placeholders for args
* @param args Variable number of arguments for text formatting
*/","* Create the duration text from a {@code String.format()} code call
   * and log either at info or debug.
   * @param log log to write to
   * @param logAtInfo should the log be at info, rather than debug
   * @param format format string
   * @param args list of arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,toString,org.apache.hadoop.util.OperationDuration:toString(),91,94,"/**
* Returns a string representation of this duration.
*/","* Return the duration as {@link #humanTime(long)}.
   * @return a printable duration.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,newStripedCrcComposer,"org.apache.hadoop.util.CrcComposer:newStripedCrcComposer(org.apache.hadoop.util.DataChecksum$Type,long,long)",83,92,"/**
* Creates a striped CRC composer with specified parameters.
* @param type checksum type
* @param bytesPerCrcHint hint for CRC computation
* @param stripeLength length of each stripe in the output stream
*/","* Returns a CrcComposer which will collapse CRCs for every combined
   * underlying data size which aligns with the specified stripe boundary. For
   * example, if ""update"" is called with 20 CRCs and bytesPerCrc == 5, and
   * stripeLength == 10, then every two (10 / 5) consecutive CRCs will be
   * combined with each other, yielding a list of 10 CRC ""stripes"" in the
   * final digest, each corresponding to 10 underlying data bytes. Using
   * a stripeLength greater than the total underlying data size is equivalent
   * to using a non-striped CrcComposer.
   *
   * @param type type.
   * @param bytesPerCrcHint bytesPerCrcHint.
   * @param stripeLength stripeLength.
   * @return a CrcComposer which will collapse CRCs for every combined.
   * underlying data size which aligns with the specified stripe boundary.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,compose,"org.apache.hadoop.util.CrcUtil:compose(int,int,long,int)",102,105,"/**
* Composes two CRC values using a monomial polynomial.
* @param crcA first CRC value
* @param crcB second CRC value
* @param lengthB length of the data being computed
* @param mod modulus for the computation
* @return combined CRC value
*/","* compose.
   *
   * @param crcA crcA.
   * @param crcB crcB.
   * @param lengthB length of content corresponding to {@code crcB}, in bytes.
   * @param mod mod.
   * @return compose result.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CompositeCrcFileChecksum.java,getBytes,org.apache.hadoop.fs.CompositeCrcFileChecksum:getBytes(),64,67,"/**
* Converts CRC value to bytes.
* @return byte array representation of CRC code
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,digest,org.apache.hadoop.util.CrcComposer:digest(),204,213,"/**
* Computes and returns the CRC-based digest value.
* @return byte array containing the computed digest
*/","* Returns byte representation of composed CRCs; if no stripeLength was
   * specified, the digest should be of length equal to exactly one CRC.
   * Otherwise, the number of CRCs in the returned array is equal to the
   * total sum bytesPerCrc divided by stripeLength. If the sum of bytesPerCrc
   * is not a multiple of stripeLength, then the last CRC in the array
   * corresponds to totalLength % stripeLength underlying data bytes.
   *
   * @return byte representation of composed CRCs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJarAndSave,"org.apache.hadoop.util.RunJar:unJarAndSave(java.io.InputStream,java.io.File,java.lang.String,java.util.regex.Pattern)",168,178,"/**
* Decompresses a JAR file and saves it to a specified directory.
* @param inputStream input stream containing the JAR data
* @param toDir target directory for saving the decompressed content
* @param name desired filename for the saved contents
* @param unpackRegex regular expression for filtering unpacked files
*/","* Unpack matching files from a jar. Entries inside the jar that do
   * not match the given pattern will be skipped. Keep also a copy
   * of the entire jar in the same directory for backward compatibility.
   * TODO remove this feature in a new release and do only unJar
   *
   * @param inputStream the jar stream to unpack
   * @param toDir the destination directory into which to unpack the jar
   * @param unpackRegex the pattern to match jar entries against
   * @param name name.
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJar,"org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File)",104,106,"/**
* Unjars contents of specified JAR file into target directory.
* @param jarFile JAR archive to extract
* @param toDir directory to place extracted files in
*/","* Unpack a jar file into a directory.
   *
   * This version unpacks all files inside the jar regardless of filename.
   *
   * @param jarFile the .jar file to unpack
   * @param toDir the destination directory into which to unpack the jar
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/UTF8ByteArrayUtils.java,findNthByte,"org.apache.hadoop.util.UTF8ByteArrayUtils:findNthByte(byte[],byte,int)",98,100,"/**
* Finds the nth occurrence of a byte in a UTF-8 encoded array.
* @param utf UTF-8 encoded byte array
* @param b target byte value
* @param n occurrence index (1-based)
*/","* Find the nth occurrence of the given byte b in a UTF-8 encoded string
   * @param utf a byte array containing a UTF-8 encoded string
   * @param b the byte to find
   * @param n the desired occurrence of the given byte
   * @return position that nth occurrence of the given byte if exists; otherwise -1",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,get,org.apache.hadoop.util.WeakReferenceMap:get(java.lang.Object),147,171,"/**
* Retrieves cached value by key, resolving weak reference if present; 
* otherwise creates and adds a new value to the cache. 
* @param key unique identifier for cached value 
* @return cached or newly created value of type V","* Get the value, creating if needed.
   * @param key key.
   * @return an instance.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,check,"org.apache.hadoop.util.InstrumentedLock:check(long,long,boolean)",190,226,"/**
* Logs a warning for long-held lock or wait time.
* @param acquireTime lock acquisition timestamp
* @param releaseTime lock release timestamp
* @param checkLockHeld true to check held locks, false to check waiting threads
*/","* Log a warning if the lock was held for too long.
   *
   * Should be invoked by the caller immediately AFTER releasing the lock.
   *
   * @param acquireTime  - timestamp just after acquiring the lock.
   * @param releaseTime - timestamp just before releasing the lock.
   * @param checkLockHeld checkLockHeld.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getFormattedTimeWithDiff,"org.apache.hadoop.util.StringUtils:getFormattedTimeWithDiff(java.lang.String,long,long)",390,400,"/**
* Formats time with optional difference from start time.
* @param formattedFinishTime formatted finish time string
* @param finishTime finish time in milliseconds
* @param startTime start time in milliseconds
* @return formatted time string with optional time difference
*/","* Formats time in ms and appends difference (finishTime - startTime)
   * as returned by formatTimeDiff().
   * If finish time is 0, empty string is returned, if start time is 0
   * then difference is not appended to return value.
   * @param formattedFinishTime formattedFinishTime to use
   * @param finishTime finish time
   * @param startTime start time
   * @return formatted value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,split,org.apache.hadoop.util.StringUtils:split(java.lang.String),570,572,"/**
 * Splits input string into an array using comma as delimiter.
 */","* Split a string using the default separator
   * @param str a string that may have escaped separator
   * @return an array of strings",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,camelize,org.apache.hadoop.util.StringUtils:camelize(java.lang.String),1094,1102,"/**
* Converts a string to camel case.
* @param s input string
* @return camelize'd string
*/","* Convert SOME_STUFF to SomeStuff
   *
   * @param s input string
   * @return camelized string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,escapeString,"org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char)",678,681,"/**
* Escapes occurrences of a specified character in a string.
* @param str input string
* @param escapeChar character used for escaping
* @param charToEscape character to be escaped
* @return modified string with escapes
*/","* Escape <code>charToEscape</code> in the string 
   * with the escape char <code>escapeChar</code>
   * 
   * @param str string
   * @param escapeChar escape char
   * @param charToEscape the char to be escaped
   * @return an escaped string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,unEscapeString,"org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char)",736,739,"/**
* Unescapes a string by replacing occurrences of escapeChar followed by charToEscape.
* @param str the input string
* @param escapeChar character used as an escape indicator
* @param charToEscape character to be unescaped
* @return unescaped string or original string if no replacements needed
*/","* Unescape <code>charToEscape</code> in the string 
   * with the escape char <code>escapeChar</code>
   * 
   * @param str string
   * @param escapeChar escape char
   * @param charToEscape the escaped char
   * @return an unescaped string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,createStartupShutdownMessage,"org.apache.hadoop.util.StringUtils:createStartupShutdownMessage(java.lang.String,java.lang.String,java.lang.String[])",837,851,"/**
* Creates a startup/shutdown message with details about the application.
* @param classname name of the Java class
* @param hostname host machine where the application is running
* @param args command-line arguments (may be null)
* @return formatted string containing startup information
*/","* Generate the text for the startup/shutdown message of processes.
   * @param classname short classname of the class
   * @param hostname hostname
   * @param args Command arguments
   * @return a string to log.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getBuildVersion,org.apache.hadoop.util.VersionInfo:getBuildVersion(),162,164,"/**
* Retrieves build version information.
* @return The current build version as a string.","* Returns the buildVersion which includes version,
   * revision, user and date.
   * @return the buildVersion",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,next,org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:next(),650,658,"/**
* Retrieves the next object from this iterator, or throws an exception if exhausted. 
* @return the next object in the iteration, or null if hasNext() returns false
*/","* Return the next value.
     * Will retrieve the next elements if needed.
     * This is where the mapper takes place.
     * @return true if there is another data element.
     * @throws IOException failure in fetch operation or the transformation.
     * @throws NoSuchElementException no more data",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,close,org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:close(),695,707,"/**
* Closes the object, calling superclass and dependent close methods.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,sourceHasNext,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceHasNext(),466,479,"/**
* Checks if the source has more elements to read.
* @throws IOException if an I/O error occurs
*/","* Check for the source having a next element.
     * If it does not, this object's close() method
     * is called and false returned
     * @return true if there is a new value
     * @throws IOException failure to retrieve next value",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAutoCloseableReference.java,lazyAutoCloseablefromSupplier,org.apache.hadoop.util.functional.LazyAutoCloseableReference:lazyAutoCloseablefromSupplier(java.util.function.Supplier),99,101,"/**
* Creates a lazily initialized auto-closeable reference from a supplier.
* @param supplier function to create the initial AutoCloseable instance
*/","* Create from a supplier.
   * This is not a constructor to avoid ambiguity when a lambda-expression is
   * passed in.
   * @param supplier supplier implementation.
   * @return a lazy reference.
   * @param <T> type of reference",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,bulkDelete_pageSize,"org.apache.hadoop.io.wrappedio.WrappedIO:bulkDelete_pageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",72,79,"/**
* Returns page size for bulk deletion on the given file system and path.
* @param fs the file system to operate on
* @param path the path to delete in bulk
*/","* Get the maximum number of objects/files to delete in a single request.
   * @param fs filesystem
   * @param path path to delete under.
   * @return a number greater than or equal to zero.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws IllegalArgumentException path not valid.
   * @throws UncheckedIOException if an IOE was raised.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,bulkDelete_delete,"org.apache.hadoop.io.wrappedio.WrappedIO:bulkDelete_delete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Collection)",104,113,"/**
* Deletes multiple files in bulk.
* @param fs FileSystem instance
* @param base Base directory for deletion
* @param paths Collection of file paths to delete
* @return List of deleted file entries (path, message) or empty list if none deleted
*/","* Delete a list of files/objects.
   * <ul>
   *   <li>Files must be under the path provided in {@code base}.</li>
   *   <li>The size of the list must be equal to or less than the page size.</li>
   *   <li>Directories are not supported; the outcome of attempting to delete
   *       directories is undefined (ignored; undetected, listed as failures...).</li>
   *   <li>The operation is not atomic.</li>
   *   <li>The operation is treated as idempotent: network failures may
   *        trigger resubmission of the request -any new objects created under a
   *        path in the list may then be deleted.</li>
   *    <li>There is no guarantee that any parent directories exist after this call.
   *    </li>
   * </ul>
   * @param fs filesystem
   * @param base path to delete under.
   * @param paths list of paths which must be absolute and under the base path.
   * @return a list of all the paths which couldn't be deleted for a reason other
   *          than ""not found"" and any associated error message.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws UncheckedIOException if an IOE was raised.
   * @throws IllegalArgumentException if a path argument is invalid.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,fileSystem_openFile,"org.apache.hadoop.io.wrappedio.WrappedIO:fileSystem_openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.FileStatus,java.lang.Long,java.util.Map)",165,191,"/**
* Opens a file in the specified file system with customizable policy and options.
* @param fs file system instance
* @param path file path to open
* @param policy read access policy (e.g. ""READ"", ""WRITE"")
* @param status optional file status object
* @param length optional file length
* @param options additional file opening options
* @return FSDataInputStream object or null on failure
*/","* OpenFile assistant, easy reflection-based access to
   * {@link FileSystem#openFile(Path)} and blocks
   * awaiting the operation completion.
   * @param fs filesystem
   * @param path path
   * @param policy read policy
   * @param status optional file status
   * @param length optional file length
   * @param options nullable map of other options
   * @return stream of the opened file
   * @throws UncheckedIOException if an IOE was raised.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,byteBufferPositionedReadable_readFully,"org.apache.hadoop.io.wrappedio.WrappedIO:byteBufferPositionedReadable_readFully(java.io.InputStream,long,java.nio.ByteBuffer)",213,224,"/**
* Reads fully from InputStream into provided ByteBuffer at specified position.
* @param in InputStream to read from
* @param position current buffer position
* @param buf ByteBuffer to fill with data
*/","* Delegate to {@link ByteBufferPositionedReadable#read(long, ByteBuffer)}.
   * @param in input stream
   * @param position position within file
   * @param buf the ByteBuffer to receive the results of the read operation.
   * Note: that is the default behaviour of {@link FSDataInputStream#readFully(long, ByteBuffer)}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_load,"org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",134,139,"/**
* Loads I/O statistics snapshot from file system at specified location.
* @param fs file system instance
* @param path file system path to load snapshot from
* @return Serializable object containing loaded snapshot data or null if failed
*/","* Load IOStatisticsSnapshot from a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @return the loaded snapshot
   * @throws UncheckedIOException Any IO exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_fromJsonString,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_fromJsonString(java.lang.String),192,196,"/**
* Creates an IO statistics snapshot from a JSON string.
* @param json The JSON data to deserialize
* @return An IOStatisticsSnapshot object or null on error
*/","* Load IOStatisticsSnapshot from a JSON string.
   * @param json JSON string value.
   * @return deserialized snapshot.
   * @throws UncheckedIOException Any IO/jackson exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAtomicReference.java,get,org.apache.hadoop.util.functional.LazyAtomicReference:get(),120,123,"/**
* Evaluates and returns the result.
* @throws UncheckedIOException if an IOException occurs during evaluation
*/","* Implementation of {@code Supplier.get()}.
   * <p>
   * Invoke {@link #eval()} and convert IOEs to
   * UncheckedIOException.
   * <p>
   * This is the {@code Supplier.get()} implementation, which allows
   * this class to passed into anything taking a supplier.
   * @return the value
   * @throws UncheckedIOException if the constructor raised an IOException.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,foreach,org.apache.hadoop.util.functional.TaskPool:foreach(java.lang.Iterable),581,583,"/**
* Creates a builder instance from an iterable of items.
* @param items Iterable containing items to be processed
*/","* Create a task builder for the iterable.
   * @param items item source.
   * @param <I> type of result.
   * @return builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,foreach,org.apache.hadoop.util.functional.TaskPool:foreach(java.lang.Object[]),595,597,"/**
* Creates a new builder instance from an array of items.
* @param items input array of type I
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,raiseInnerCause,org.apache.hadoop.fs.impl.FutureIOSupport:raiseInnerCause(java.util.concurrent.ExecutionException),106,110,"/**
* Raises inner cause of an ExecutionException.
* @param e Exception containing inner cause to be raised
*/","* From the inner cause of an execution exception, extract the inner cause
   * if it is an IOE or RTE.
   * See {@link FutureIO#raiseInnerCause(ExecutionException)}.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitFuture,org.apache.hadoop.util.functional.FutureIO:awaitFuture(java.util.concurrent.Future),97,110,"/**
* Waits for a future result, handling potential exceptions and cancellations.
* @param future Future object to await completion of
* @return Result value or throws exception if cancelled or an error occurs
*/","* Given a future, evaluate it.
   * <p>
   * Any exception generated in the future is
   * extracted and rethrown.
   * </p>
   * If this thread is interrupted while waiting for the future to complete,
   * an {@code InterruptedIOException} is raised.
   * However, if the future is cancelled, a {@code CancellationException}
   * is raised in the {code Future.get()} call. This is
   * passed up as is -so allowing the caller to distinguish between
   * thread interruption (such as when speculative task execution is aborted)
   * and future cancellation.
   * @param future future to evaluate
   * @param <T> type of the result.
   * @return the result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitFuture,"org.apache.hadoop.util.functional.FutureIO:awaitFuture(java.util.concurrent.Future,long,java.util.concurrent.TimeUnit)",129,145,"/**
* Waits for a future to complete within the specified timeout.
* @param future asynchronous computation to wait for
* @param timeout maximum time in the given unit to wait
* @param unit time unit of the timeout (e.g. milliseconds, seconds)
* @return result of the future or throws an exception if failed
*/","* Given a future, evaluate it.
   * <p>
   * Any exception generated in the future is
   * extracted and rethrown.
   * </p>
   * @param future future to evaluate
   * @param timeout timeout to wait.
   * @param unit time unit.
   * @param <T> type of the result.
   * @return the result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown
   * @throws TimeoutException the future timed out.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,raiseInnerCause,org.apache.hadoop.fs.impl.FutureIOSupport:raiseInnerCause(java.util.concurrent.CompletionException),123,127,"/**
* Raises inner cause of CompletionException.
* @param e CompletionException to extract and re-throw cause
*/","* Extract the cause of a completion failure and rethrow it if an IOE
   * or RTE.
   * See {@link FutureIO#raiseInnerCause(CompletionException)}.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,setConf,"org.apache.hadoop.util.ReflectionUtils:setConf(java.lang.Object,org.apache.hadoop.conf.Configuration)",74,81,"/**
* Sets configuration for a configurable object.
* @param theObject object to configure
* @param conf Configuration instance
*/","* Check and set 'configuration' if necessary.
   * 
   * @param theObject object for which to set configuration
   * @param conf Configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableName.java,getClass,"org.apache.hadoop.io.WritableName:getClass(java.lang.String,org.apache.hadoop.conf.Configuration)",91,103,"/**
* Retrieves a Class instance by name from the cache or configuration.
* @param name unique class identifier
* @param conf Hadoop Configuration object
* @return Class<?> instance or throws IOException if not found
*/","* Return the class for a name.
   * Default is {@link Class#forName(String)}.
   *
   * @param name input name.
   * @param conf input configuration.
   * @return class for a name.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createCodec,"org.apache.hadoop.io.erasurecode.CodecUtil:createCodec(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",232,254,"/**
* Creates a custom ErasureCodec instance based on the provided class name and options.
* @param conf Hadoop Configuration object
* @param codecClassName name of custom codec class
* @param options codec configuration options
* @return ErasureCodec instance or throws RuntimeException if creation fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,loadClass,"org.apache.hadoop.io.ObjectWritable:loadClass(org.apache.hadoop.conf.Configuration,java.lang.String)",412,424,"/**
* Loads a Java class by name from the configuration or system classpath.
* @param conf Configuration object with class loading enabled
* @param className name of the class to load
* @return loaded Class object, or null if not found
*/","* Find and load the class with given name <tt>className</tt> by first finding
   * it in the specified <tt>conf</tt>. If the specified <tt>conf</tt> is null,
   * try load it directly.
   *
   * @param conf configuration.
   * @param className classname.
   * @return Class.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getProtocolClass,"org.apache.hadoop.ipc.Server:getProtocolClass(java.lang.String,org.apache.hadoop.conf.Configuration)",331,339,"/**
* Retrieves the class for a specific protocol.
* @param protocolName name of the protocol
* @param conf configuration object
* @return Class<?> representing the protocol or throws ClassNotFoundException if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,getClass,org.apache.hadoop.util.FindClass:getClass(java.lang.String),154,156,"/**
* Retrieves class by name from configuration.
* @param name class name to fetch
* @return Class object or null if not found
*/","* Get a class fromt the configuration
   * @param name the class name
   * @return the class
   * @throws ClassNotFoundException if the class was not found
   * @throws Error on other classloading problems",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,logThreadInfo,"org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.apache.commons.logging.Log,java.lang.String,long)",232,254,"/**
* Logs thread information at a specified interval.
* @param log logging instance
* @param title log message title
* @param minInterval minimum time interval between logs in milliseconds
*/","* Log the current thread stacks at INFO level.
   * @param log the logger that logs the stack trace
   * @param title a descriptive title for the call stacks
   * @param minInterval the minimum time from the last
   * @deprecated to be removed with 3.4.0. Use {@link #logThreadInfo(Logger, String, long)} instead.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,logThreadInfo,"org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.slf4j.Logger,java.lang.String,long)",262,283,"/**
* Logs thread information with a specified interval.
* @param log Logger instance
* @param title Thread info title
* @param minInterval Minimum logging interval in seconds
*/","* Log the current thread stacks at INFO level.
   * @param log the logger that logs the stack trace
   * @param title a descriptive title for the call stacks
   * @param minInterval the minimum time from the last",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(java.util.Optional,java.util.Optional)",105,113,"/**
* Initializes AbstractFSBuilderImpl with a path and/or a path handle.
* @param optionalPath directory path (optional)
* @param optionalPathHandle path handle (optional) 
*/","* Constructor with both optional path and path handle.
   * Either or both argument may be empty, but it is an error for
   * both to be defined.
   * @param optionalPath a path or empty
   * @param optionalPathHandle a path handle/empty
   * @throws IllegalArgumentException if both parameters are set.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration:<init>(),819,821,"/**
* Initializes configuration with default values.",A new configuration.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,validateResponse,"org.apache.hadoop.util.HttpExceptionUtils:validateResponse(java.net.HttpURLConnection,int)",143,191,"/**
* Validates HTTP response by checking status code and throwing a custom exception.
* @param conn HttpURLConnection object to validate
* @param expectedStatus expected HTTP status code
*/","* Validates the status of an <code>HttpURLConnection</code> against an
   * expected HTTP status code. If the current status code is not the expected
   * one it throws an exception with a detail message using Server side error
   * messages if available.
   * <p>
   * <b>NOTE:</b> this method will throw the deserialized exception even if not
   * declared in the <code>throws</code> of the method signature.
   *
   * @param conn the <code>HttpURLConnection</code>.
   * @param expectedStatus the expected HTTP status code.
   * @throws IOException thrown if the current status code does not match the
   * expected one.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newCrc32C,org.apache.hadoop.util.DataChecksum:newCrc32C(),102,112,"/**
* Creates a CRC32C checksum instance using either Java 9 factory or pure Java implementation.
* @return Checksum object
*/","* The flag is volatile to avoid synchronization here.
   * Re-entrancy is unlikely except in failure mode (and inexpensive).",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,removeAll,org.apache.hadoop.util.IntrusiveCollection:removeAll(java.util.Collection),361,370,"/**
* Removes all elements from the collection, returning true if any were removed.
* @param collection collection of objects to remove
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,toArray,org.apache.hadoop.util.IntrusiveCollection:toArray(java.lang.Object[]),266,278,"/**
* Converts internal collection to given array type.
* @param array the array to fill, must be at least as large as size
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroupsForUserCommand,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsForUserCommand(java.lang.String),143,145,"/**
* Retrieves groups for a user by name.
* @param userName unique username
*/","* Returns just the shell command to be used to fetch a user's groups list.
   * This is mainly separate to make some tests easier.
   * @param userName The username that needs to be passed into the command built
   * @return An appropriate shell command with arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroupsIDForUserCommand,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsIDForUserCommand(java.lang.String),164,166,"/**
* Retrieves groups IDs associated with a user command.
* @param userName name of the user
*/","* Returns just the shell command to be used to fetch a user's group IDs list.
   * This is mainly separate to make some tests easier.
   * @param userName The username that needs to be passed into the command built
   * @return An appropriate shell command with arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSetPermissionCommand,"org.apache.hadoop.util.Shell:getSetPermissionCommand(java.lang.String,boolean,java.lang.String)",311,318,"/**
* Generates set permission command with optional file path.
* @param perm permission value
* @param recursive whether to apply recursively
* @param file target file path (optional)
* @return command array or null if not found
*/","* Return a command to set permission for specific file.
   *
   * @param perm String permission to set
   * @param recursive boolean true to apply to all sub-directories recursively
   * @param file String file to set
   * @return String[] containing command and arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getCheckProcessIsAliveCommand,org.apache.hadoop.util.Shell:getCheckProcessIsAliveCommand(java.lang.String),362,364,"/**
* Generates signal kill command to check process alive status.
* @param pid unique process identifier
*/","* Return a command for determining if process with specified pid is alive.
   * @param pid process ID
   * @return a <code>kill -0</code> command or equivalent",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getHadoopHome,org.apache.hadoop.util.Shell:getHadoopHome(),610,612,"/**
* Returns the canonical path of Hadoop Home directory.
*/","* Get the Hadoop home directory. Raises an exception if not found
   * @return the home dir
   * @throws IOException if the home directory cannot be located.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getQualifiedBin,org.apache.hadoop.util.Shell:getQualifiedBin(java.lang.String),642,646,"/**
* Constructs qualified Hadoop binary file path for a given executable.
* @param executable name of the executable (e.g. ""hdfs"")
* @return File object representing the executable in the Hadoop bin directory
*/","*  Fully qualify the path to a binary that should be in a known hadoop
   *  bin location. This is primarily useful for disambiguating call-outs
   *  to executable sub-components of Hadoop to avoid clashes with other
   *  executables that may be in the path.  Caveat:  this call doesn't
   *  just format the path to the bin directory.  It also checks for file
   *  existence of the composed path. The output of this call should be
   *  cached by callers.
   *
   * @param executable executable
   * @return executable file reference
   * @throws FileNotFoundException if the path does not exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HardLink.java,linkCount,org.apache.hadoop.fs.HardLink$HardLinkCGWin:linkCount(java.io.File),137,146,"/**
* Constructs command for link count operation.
* @param file target file
* @return array of command elements or throws IOException on error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell$ShellTimeoutTimerTask:<init>(org.apache.hadoop.util.Shell),1402,1404,"/**
 * Initializes a new instance of ShellTimeoutTimerTask.
 * @param shell the associated Shell object.",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,org.apache.hadoop.util.Progress:addPhase(),71,77,"/**
* Adds a new phase to the progress and sets equal weightage for all existing phases.
* @return The newly added Progress object.","* Adds a node to the tree. Gives equal weightage to all phases.
   * @return Progress.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhases,org.apache.hadoop.util.Progress:addPhases(int),130,137,"/**
* Adds multiple phases with equal weightage.
* @param n number of phases to add
*/","* Adds n nodes to the tree. Gives equal weightage to all phases.
   *
   * @param n n.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,"org.apache.hadoop.util.Progress:addPhase(java.lang.String,float)",94,99,"/**
* Adds a new phase to the progress with specified weightage and status.
* @param status the current status of the phase
* @param weightage the weightage value for this phase
* @return the newly added Progress object
*/","* Adds a named node with a specified progress weightage to the tree.
   *
   * @param status status.
   * @param weightage weightage.
   * @return Progress.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,get,org.apache.hadoop.util.Progress:get(),225,231,"/**
* Retrieves the internal progress value from the root node.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,getProgress,org.apache.hadoop.util.Progress:getProgress(),239,241,"/**
 * Returns the current progress as a floating-point value.
 */","* Returns progress in this node. get() would give overall progress of the
   * root node(not just given current node).
   *
   * @return progress.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,toString,org.apache.hadoop.util.Progress:toString(),275,280,"/**
* Generates human-readable string representation of this object.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,create,org.apache.hadoop.util.curator.ZKCuratorManager:create(java.lang.String),332,334,"/**
* Creates a new entity at the specified file path.
* @param path absolute file path to create the entity at
*/","* Create a ZNode.
   * @param path Path of the ZNode.
   * @return If the ZNode was created.
   * @throws Exception If it cannot contact Zookeeper.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,createRootDirRecursively,"org.apache.hadoop.util.curator.ZKCuratorManager:createRootDirRecursively(java.lang.String,java.util.List)",372,384,"/**
* Recursively creates the root directory and its subdirectories.
* @param path absolute path to create
* @param zkAcl list of ACLs for ZooKeeper access control
*/","* Utility function to ensure that the configured base znode exists.
   * This recursively creates the znode as well as all of its parents.
   * @param path Path of the znode to create.
   * @param zkAcl ACLs for ZooKeeper.
   * @throws Exception If it cannot create the file.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,ctorImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:ctorImpl(java.lang.String,java.lang.Class[])",364,378,"/**
* Finds and sets the constructor implementation based on class name and parameter types.
* @param className the name of the class to find the constructor for
* @param argClasses variable arguments representing the constructor's parameter types
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,newInstance,org.apache.hadoop.util.dynamic.DynConstructors$Ctor:newInstance(java.lang.Object[]),68,75,"/**
* Creates a new instance of class C with the provided arguments.
* @param args variable number of constructor arguments
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,invokeChecked,"org.apache.hadoop.util.dynamic.DynConstructors$Ctor:invokeChecked(java.lang.Object,java.lang.Object[])",84,89,"/**
* Invokes a constructor with the given arguments and returns the constructed object casted to type R.
* @param target ignored, should be null for constructors
* @param args variable number of arguments passed to the constructor
* @return constructed object of type R or throws an Exception if creation fails",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeChecked,org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:invokeChecked(java.lang.Object[]),215,217,"/**
* Invokes checked method with varargs.
* @param args variable arguments
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invoke,"org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invoke(java.lang.Object,java.lang.Object[])",91,98,"/**
* Invokes a method on the specified target with variable arguments.
* @param target object to invoke the method on
* @param args method parameters
* @return result of the invoked method
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeChecked,org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:invokeChecked(java.lang.Object[]),198,200,"/**
 * Invokes checked method on underlying object with given arguments.
 * @param args variable number of arguments to pass to method
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.String,java.lang.String,java.lang.Class[])",284,298,"/**
* Dynamically loads and implements a method from the specified class.
* @param className name of the target class
* @param methodName name of the method to implement
* @param argClasses classes of the method's arguments (varargs)
*/","* Checks for an implementation, first finding the given class by name.
     * @param className name of a class
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.Class,java.lang.Class[])",343,346,"/**
* Builds an instance of the specified class with given argument classes.
* @param targetClass class to instantiate
* @param varargClasses classes for arguments
*/","* Checks for a method implementation.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param targetClass the class to check for an implementation
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.String,java.lang.String,java.lang.Class[])",387,401,"/**
* Tries to find and invoke a hidden implementation for the specified method.
* @param className name of the class containing the method
* @param methodName name of the method to invoke
* @param argClasses classes of the method's arguments (varargs)
*/","* Checks for an implementation, first finding the given class by name.
     * @param className name of a class
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.Class,java.lang.Class[])",448,451,"/**
* Creates a new instance of the builder with specified target class and argument classes.
* @param targetClass the main class being built
* @param argClasses  variable number of argument classes
*/","* Checks for a method implementation.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param targetClass the class to check for an implementation
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,loadInvocation,"org.apache.hadoop.util.dynamic.BindingUtils:loadInvocation(java.lang.Class,java.lang.Class,java.lang.String,java.lang.Class[])",101,121,"/**
* Loads a method with given parameters and returns type.
* @param source class containing the method
* @param returnType expected return type of the method
* @param name method name
* @param parameterTypes method parameter types
* @return UnboundMethod object or null if not found/loaded","* Get an invocation from the source class, which will be unavailable() if
   * the class is null or the method isn't found.
   *
   * @param <T> return type
   * @param source source. If null, the method is a no-op.
   * @param returnType return type class (unused)
   * @param name method name
   * @param parameterTypes parameters
   *
   * @return the method or ""unavailable""",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,requireAllMethodsAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:requireAllMethodsAvailable(),225,242,"/**
* Verifies availability of all required dynamic methods.
* @throws UnsupportedOperationException if any method is unavailable
*/","* For testing: verify that all methods were found.
   * @throws UnsupportedOperationException if the method was not found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,bulkDelete_available,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_available(),249,251,"/**
* Checks availability of bulk delete operation.
* @return true if available, false otherwise
*/","* Are the bulk delete methods available?
   * @return true if the methods were found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,fileSystem_openFile_available,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:fileSystem_openFile_available(),306,308,"/**
* Checks availability of open file system operation.
* @return true if open file is available, false otherwise
*/","* Is the {@link #fileSystem_openFile(FileSystem, Path, String, FileStatus, Long, Map)}
   * method available.
   * @return true if the optimized open file method can be invoked.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,byteBufferPositionedReadable_available,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_available(),380,382,"/**
* Checks availability of readable data in positioned buffer.
* @return true if data is available, false otherwise
*/","* Are the ByteBufferPositionedReadable methods loaded?
   * This does not check that a specific stream implements the API;
   * use {@link #byteBufferPositionedReadable_readFullyAvailable(InputStream)}.
   * @return true if the hadoop libraries have the method.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,byteBufferPositionedReadable_readFullyAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_readFullyAvailable(java.io.InputStream),392,400,"/**
* Checks if input stream is fully readable.
* @param in InputStream to check
* @return true if fully readable, false otherwise
*/","* Probe to see if the input stream is an instance of ByteBufferPositionedReadable.
   * If the stream is an FSDataInputStream, the wrapped stream is checked.
   * @param in input stream
   * @return true if the API is available, the stream implements the interface
   * (including the innermost wrapped stream) and that it declares the stream capability.
   * @throws IOException if the operation was attempted and failed.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,ioStatisticsAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:ioStatisticsAvailable(),360,362,"/**
* Checks if I/O statistics are available.
* @return true if statistics are available, false otherwise
*/","* Are the core IOStatistics methods and classes available.
   * @return true if the relevant methods are loaded.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,ioStatisticsContextAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:ioStatisticsContextAvailable(),368,370,"/**
* Checks if IO statistics context is enabled.
* @return true if available, false otherwise
*/","* Are the IOStatisticsContext methods and classes available?
   * @return true if the relevant methods are loaded.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,checkAvailable,org.apache.hadoop.util.dynamic.BindingUtils:checkAvailable(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod),183,188,"/**
* Throws an exception if the specified unbound method is not available.
* @param method Unbound method to check availability for
*/","* Require a method to be available.
   * @param method method to probe
   * @throws UnsupportedOperationException if the method was not found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,buildChecked,org.apache.hadoop.util.dynamic.DynMethods$Builder:buildChecked(java.lang.Object),490,492,"/**
 * Builds and binds a checked method invocation to the specified receiver object.
 * @param receiver target object for method invocation
 */","* Returns the first valid implementation as a BoundMethod or throws a
     * NoSuchMethodException if there is none.
     * @param receiver an Object to receive the method invocation
     * @return a {@link BoundMethod} with a valid implementation and receiver
     * @throws IllegalStateException if the method is static
     * @throws IllegalArgumentException if the receiver's class is incompatible
     * @throws NoSuchMethodException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,build,org.apache.hadoop.util.dynamic.DynMethods$Builder:build(java.lang.Object),503,505,"/**
* Builds and binds a bound method to an object.
* @param receiver the object to bind the method to
*/","* Returns the first valid implementation as a BoundMethod or throws a
     * RuntimeError if there is none.
     * @param receiver an Object to receive the method invocation
     * @return a {@link BoundMethod} with a valid implementation and receiver
     * @throws IllegalStateException if the method is static
     * @throws IllegalArgumentException if the receiver's class is incompatible
     * @throws RuntimeException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,buildStaticChecked,org.apache.hadoop.util.dynamic.DynMethods$Builder:buildStaticChecked(),514,516,"/**
 * Creates a static-checked method from a checked method.
 */","* Returns the first valid implementation as a StaticMethod or throws a
     * NoSuchMethodException if there is none.
     * @return a {@link StaticMethod} with a valid implementation
     * @throws IllegalStateException if the method is not static
     * @throws NoSuchMethodException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,buildStatic,org.apache.hadoop.util.dynamic.DynMethods$Builder:buildStatic(),525,527,"/**
* Builds static instance of the class.
* @return StaticClass instance or null if failed
*/","* Returns the first valid implementation as a StaticMethod or throws a
     * RuntimeException if there is none.
     * @return a {@link StaticMethod} with a valid implementation
     * @throws IllegalStateException if the method is not static
     * @throws RuntimeException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,loadFileSystems,org.apache.hadoop.fs.FileSystem:loadFileSystems(),3516,3545,"/**
* Loads and registers available file system implementations.
*/","* Load the filesystem declarations from service resources.
   * This is a synchronized operation.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,main,org.apache.hadoop.util.VersionInfo:main(java.lang.String[]),182,193,"/**
* Prints build and configuration information.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,makeRpcRequestHeader,"org.apache.hadoop.util.ProtoUtil:makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto,int,int,byte[])",171,176,"/**
* Creates an RPC request header with the given parameters.
* @param rpcKind type of RPC request
* @param operation operation being performed
* @param callId unique identifier for this call
* @param retryCount number of retries (if any)
* @param uuid unique identifier for the client
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,parseVersion,org.apache.hadoop.util.ComparableVersion:parseVersion(java.lang.String),360,453,"/**
* Parses a semantic version string into a structured format.
* @param version the version string to parse
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,<init>,"org.apache.hadoop.util.LightWeightCache:<init>(int,int,long,long,org.apache.hadoop.util.Timer)",120,145,"/**
* Initializes LightWeightCache with recommended length and expiration periods.
* @param recommendedLength cache length recommendation
* @param sizeLimit maximum queue size
* @param creationExpirationPeriod time to live for cached entries (ms)
* @param accessExpirationPeriod time to idle before expiring (ms)
* @param timer external timer instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,get,org.apache.hadoop.util.LightWeightResizableGSet:get(java.lang.Object),98,101,"/**
* Delegates retrieval of an entity by its unique identifier to superclass.
* @param key unique entity identifier
* @return Entity object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,contains,org.apache.hadoop.util.LightWeightGSet:contains(java.lang.Object),144,147,"/**
 * Checks if a given key exists in the data structure.
 * @param key unique identifier of the element to search for
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,toString,org.apache.hadoop.util.BlockingThreadPoolExecutorService:toString(),158,166,"/**
* Returns a string representation of the BlockingThreadPoolExecutorService.
* @return formatted string with executor details
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readFileToMapWithFileInputStream,"org.apache.hadoop.util.HostsFileReader:readFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map)",130,144,"/**
* Populates a map with file data using InputStream.
* Supports text and XML files. If XML, uses specialized parsing.
* @param type data type (e.g., ""text"" or ""xml"")
* @param filename input file name
* @param inputStream input stream to read from
* @param map map to store key-value pairs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/JenkinsHash.java,main,org.apache.hadoop.util.hash.JenkinsHash:main(java.lang.String[]),252,266,"/**
* Calculates Jenkins Hash using provided file.
* @param args single filename argument
*/","* Compute the hash of the specified file
   * @param args name of file to compute hash of.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/HashFunction.java,<init>,"org.apache.hadoop.util.bloom.HashFunction:<init>(int,int,int)",83,97,"/**
* Initializes a hash function with the given max value, number of hashes,
* and type. Validates input values.
* @param maxValue maximum value for hashing
* @param nbHash number of hash functions to use
* @param hashType type of hash function (e.g., MD5, SHA-256)
*/","* Constructor.
   * <p>
   * Builds a hash function that must obey to a given maximum number of returned values and a highest value.
   * @param maxValue The maximum highest returned value.
   * @param nbHash The number of resulting hashed values.
   * @param hashType type of the hashing function (see {@link Hash}).",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,<init>,org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>(),102,102,"/**
* Initializes a new instance of RetouchedBloomFilter.
*/",Default constructor - use with readFields,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,write,org.apache.hadoop.util.bloom.DynamicBloomFilter:write(java.io.DataOutput),248,257,"/**
* Writes this Matrix object to the output stream.
* @throws IOException if write operation fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,write,org.apache.hadoop.util.bloom.RetouchedBloomFilter:write(java.io.DataOutput),407,427,"/**
* Writes object data to output stream, including fpVector, keyVector, and ratio arrays.
* @throws IOException if write operation fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,add,org.apache.hadoop.util.bloom.CountingBloomFilter:add(org.apache.hadoop.util.bloom.Key),104,127,"/**
* Increments the count for a given key in the hash table.
* @param key unique identifier to increment count for
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,membershipTest,org.apache.hadoop.util.bloom.CountingBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key),178,200,"/**
* Verifies membership in a hash-based data structure.
* @param key unique identifier to test for presence
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,approximateCount,org.apache.hadoop.util.bloom.CountingBloomFilter:approximateCount(org.apache.hadoop.util.bloom.Key),220,238,"/**
* Approximates the count of a Key by searching all buckets.
* @param key the Key to approximate the count for
* @return an estimated count or 0 if not found, -1 on failure
*/","* This method calculates an approximate count of the key, i.e. how many
   * times the key was added to the filter. This allows the filter to be
   * used as an approximate <code>key -&gt; count</code> map.
   * <p>NOTE: due to the bucket size of this filter, inserting the same
   * key more than 15 times will cause an overflow at all filter positions
   * associated with this key, and it will significantly increase the error
   * rate for this and other keys. For this reason the filter can only be
   * used to store small count values <code>0 &lt;= N &lt;&lt; 15</code>.
   * @param key key to be tested
   * @return 0 if the key is not present. Otherwise, a positive value v will
   * be returned such that <code>v == count</code> with probability equal to the
   * error rate of this filter, and <code>v &gt; count</code> otherwise.
   * Additionally, if the filter experienced an underflow as a result of
   * {@link #delete(Key)} operation, the return value may be lower than the
   * <code>count</code> with the probability of the false negative rate of such
   * filter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,add,org.apache.hadoop.util.bloom.BloomFilter:add(org.apache.hadoop.util.bloom.Key),116,128,"/**
* Adds a key to the set.
* @param key unique identifier
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,membershipTest,org.apache.hadoop.util.bloom.BloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key),142,156,"/**
* Validates membership of a given Key object.
* @param key unique identifier to test
* @return true if member, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,add,org.apache.hadoop.util.bloom.RetouchedBloomFilter:add(org.apache.hadoop.util.bloom.Key),118,131,"/**
* Adds a Key object to the storage, hashing and distributing it across multiple buckets.
* @param key unique identifier to be stored
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(org.apache.hadoop.util.bloom.Key),139,150,"/**
* Adds a false positive to the hash table.
* @param key unique identifier of the false positive
*/","* Adds a false positive information to <i>this</i> retouched Bloom filter.
   * <p>
   * <b>Invariant</b>: if the false positive is <code>null</code>, nothing happens.
   * @param key The false positive key to add.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,removeKey,"org.apache.hadoop.util.bloom.RetouchedBloomFilter:removeKey(org.apache.hadoop.util.bloom.Key,java.util.List[])",351,365,"/**
* Removes a key from the specified hash table.
* @param k Key to be removed
* @param vector Hash table array of keys
*/","* Removes a given key from <i>this</i> filer.
   * @param k The key to remove.
   * @param vector The counting vector associated to the key.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,equals,org.apache.hadoop.util.bloom.Key:equals(java.lang.Object),137,143,"/**
* Compares this key with another object for equality.
* @param o the object to compare with, must be a Key instance
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,minimumFnRemove,org.apache.hadoop.util.bloom.RetouchedBloomFilter:minimumFnRemove(int[]),250,265,"/**
* Finds the index of the hash with minimum weight.
* @return index of the minimum weighted hash or -1 if not found
*/","* Chooses the bit position that minimizes the number of false negative generated.
   * @param h The different bit positions.
   * @return The position that minimizes the number of false negative generated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,maximumFpRemove,org.apache.hadoop.util.bloom.RetouchedBloomFilter:maximumFpRemove(int[]),272,286,"/**
* Finds the index of the hash with maximum weight.
* @param h array of hash indices
*/","* Chooses the bit position that maximizes the number of false positive removed.
   * @param h The different bit positions.
   * @return The position that maximizes the number of false positive removed.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,computeRatio,org.apache.hadoop.util.bloom.RetouchedBloomFilter:computeRatio(),370,379,"/**
* Calculates the ratio of key weights to feature weights for each vector index.
*/",* Computes the ratio A/FP.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,dumpResource,org.apache.hadoop.util.FindClass:dumpResource(java.lang.String),187,209,"/**
* Dumps a named resource to the output stream.
* @param name unique resource identifier
* @return success code (SUCCESS or error code)
*/","* Dump a resource to out
   * @param name resource name
   * @return the status code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,usage,org.apache.hadoop.util.FindClass:usage(java.lang.String[]),342,361,"/**
* Prints usage message and returns error code.
* @return Error code indicating successful execution or failure reason
*/","* Print a usage message
   * @param args the command line arguments
   * @return an exit code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,run,org.apache.hadoop.util.GcTimeMonitor:run(),153,172,"/**
* Periodically calculates and alerts on GC time percentage exceeding threshold.
*@param sleepIntervalMs interval between checks in milliseconds
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,put,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:put(org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor),3510,3520,"/**
* Puts a SegmentDescriptor into the merged file, updating compression settings as needed.
* @param stream SegmentDescriptor to add
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,insert,org.apache.hadoop.util.PriorityQueue:insert(java.lang.Object),73,85,"/**
* Inserts an element into the heap, potentially adjusting the top.
* @param element object to insert
* @return true if insertion was successful, false otherwise
*/","* Adds element to the PriorityQueue in log(size) time if either
   * the PriorityQueue is not full, or not lessThan(element, top()).
   * @param element element.
   * @return true if element is added, false otherwise.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newTreeSet,org.apache.hadoop.util.Sets:newTreeSet(java.lang.Iterable),154,159,"/**
* Creates a new TreeSet from an iterable of comparable elements.
* @param elements iterable of elements to populate the set
*/","* Creates a <i>mutable</i> {@code TreeSet} instance containing the given
   * elements sorted by their natural ordering.
   *
   * <p><b>Note:</b> if mutability is not required, use
   * ImmutableSortedSet#copyOf(Iterable) instead.
   *
   * <p><b>Note:</b> If {@code elements} is a {@code SortedSet} with an
   * explicit comparator, this method has different behavior than
   * {@link TreeSet#TreeSet(SortedSet)}, which returns a {@code TreeSet}
   * with that comparator.
   *
   * <p><b>Note for Java 7 and later:</b> this method is now unnecessary and
   * should be treated as deprecated. Instead, use the {@code TreeSet}
   * constructor directly, taking advantage of the new
   * <a href=""http://goo.gl/iz2Wi"">""diamond"" syntax</a>.
   *
   * <p>This method is just a small convenience for creating an empty set and
   * then calling Iterables#addAll. This method is not very useful and will
   * likely be deprecated in the future.
   *
   * @param <E> Generics Type E.
   * @param elements the elements that the set should contain
   * @return a new {@code TreeSet} containing those elements (minus duplicates)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSet,org.apache.hadoop.util.Sets:newHashSet(java.lang.Iterable),123,127,"/**
* Creates a new HashSet from an iterable collection of elements.
* @param elements input iterable to be converted into a set
*/","* Creates a <i>mutable</i> {@code HashSet} instance containing the given
   * elements. A very thin convenience for creating an empty set then calling
   * {@link Collection#addAll} or Iterables#addAll.
   *
   * <p><b>Note:</b> if mutability is not required and the elements are
   * non-null, use ImmutableSet#copyOf(Iterable) instead. (Or, change
   * {@code elements} to be a FluentIterable and call {@code elements.toSet()}.)</p>
   *
   * <p><b>Note:</b> if {@code E} is an {@link Enum} type, use
   * newEnumSet(Iterable, Class) instead.</p>
   *
   * @param <E> Generics Type E.
   * @param elements the elements that the set should contain.
   * @return a new, empty thread-safe {@code Set}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSet,org.apache.hadoop.util.Sets:newHashSet(java.lang.Object[]),100,105,"/**
* Creates a new hash set from the given elements.
* @param elements variable number of elements to include in the set
*/","* Creates a <i>mutable</i> {@code HashSet} instance initially containing
   * the given elements.
   *
   * <p><b>Note:</b> if elements are non-null and won't be added or removed
   * after this point, use ImmutableSet#of() or ImmutableSet#copyOf(Object[])
   * instead. If {@code E} is an {@link Enum} type, use
   * {@link EnumSet#of(Enum, Enum[])} instead. Otherwise, strongly consider
   * using a {@code LinkedHashSet} instead, at the cost of increased memory
   * footprint, to get deterministic iteration behavior.</p>
   *
   * <p>This method is just a small convenience, either for
   * {@code newHashSet(}{@link Arrays#asList}{@code (...))}, or for creating an
   * empty set then calling {@link Collections#addAll}.</p>
   *
   * @param <E> Generics Type E.
   * @param elements the elements that the set should contain.
   * @return a new, empty thread-safe {@code Set}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,run,org.apache.hadoop.util.ProgramDriver:run(java.lang.String[]),120,146,"/**
* Runs a program from the provided list.
* @param args array of command line arguments
* @return 0 on success, -1 on failure
*/","* This is a driver for the example programs.
   * It looks at the first command line argument and tries to find an
   * example program with that name.
   * If it is found, it calls the main method in that class with the rest 
   * of the command line arguments.
   * @param args The argument from the user. args[0] is the command to run.
   * @return -1 on error, 0 on success
   * @throws NoSuchMethodException  when a particular method cannot be found.
   * @throws SecurityException security manager to indicate a security violation.
   * @throws IllegalAccessException for backward compatibility.
   * @throws IllegalArgumentException if the arg is invalid.
   * @throws Throwable Anything thrown by the example program's main",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,"org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,org.apache.hadoop.tools.TableListing$Justification,boolean)",151,155,"/**
* Adds a column to the report with specified title and justification.
* @param title column header text
* @param justification column justification (e.g. ""Required"")
* @param wrap whether to wrap long titles
*/","* Add a new field to the Table under construction.
     *
     * @param title Field title.
     * @param justification Right or left justification. Defaults to left.
     * @param wrap Width at which to auto-wrap the content of the cell.
     *        Defaults to Integer.MAX_VALUE.
     * @return This Builder object",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,logDeprecationOnce,"org.apache.hadoop.conf.Configuration:logDeprecationOnce(java.lang.String,java.lang.String)",1465,1470,"/**
* Logs deprecation warning once per unique key.
* @param name deprecated attribute or method name
* @param source location of usage
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDurationHelper,"org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)",1936,1938,"/**
* Calculates time duration based on given name and value string.
* @param name descriptive name of the duration
* @param vStr value string representing duration (e.g. ""1h"", ""2d"", etc.)
* @param unit TimeUnit to apply to duration calculation
*/","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d).
   *
   * @param name Property name
   * @param vStr The string value with time unit suffix to be converted.
   * @param unit Unit to convert the stored property, if it exists.
   * @return time duration in given time unit.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStreamReader,"org.apache.hadoop.conf.Configuration:getStreamReader(org.apache.hadoop.conf.Configuration$Resource,boolean)",3149,3177,"/**
* Fetches an XML stream reader for the given resource.
* @param wrapper Resource wrapper containing the resource to parse
* @param quiet Whether to suppress parsing log messages
* @return An XMLStreamReader2 object or null if not found
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleStartElement,org.apache.hadoop.conf.Configuration$Parser:handleStartElement(),3240,3265,"/**
* Handles start element of XML stream, triggering corresponding actions.
* @throws XMLStreamException if parsing error occurs
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,appendXMLProperty,"org.apache.hadoop.conf.Configuration:appendXMLProperty(org.w3c.dom.Document,org.w3c.dom.Element,java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",3695,3733,"/**
* Appends XML property to the given Document, using provided ConfigRedactor for redaction if necessary.
* @param doc Document to append to
* @param conf Configuration element to append to
* @param propertyName Property name to fetch and append
* @param redactor Optional redactor to process property value
*/","*  Append a property with its attributes to a given {#link Document}
   *  if the property is found in configuration.
   *
   * @param doc
   * @param conf
   * @param propertyName",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecations,org.apache.hadoop.conf.Configuration:addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]),566,572,"/**
* Updates the deprecation context with the given deltas in a thread-safe manner.
* @param deltas array of deprecation changes to apply
*/","* Adds a set of deprecated keys to the global deprecations.
   *
   * This method is lockless.  It works by means of creating a new
   * DeprecationContext based on the old one, and then atomically swapping in
   * the new context.  If someone else updated the context in between us reading
   * the old context and swapping in the new one, we try again until we win the
   * race.
   *
   * @param deltas   The deprecations to add.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleEndElement,org.apache.hadoop.conf.Configuration$Parser:handleEndElement(),3374,3413,"/**
* Handles end element events based on XML tag name.
* @throws IOException if include tag is encountered without a valid fallback
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,bindForPortRange,"org.apache.hadoop.http.HttpServer2:bindForPortRange(org.eclipse.jetty.server.ServerConnector,int)",1502,1531,"/**
* Binds a server listener across multiple ports in the specified range,
* starting from the given port and skipping it.
* @param listener ServerConnector instance
* @param startPort starting port number (skipped)
*/","* Bind using port ranges. Keep on looking for a free port in the port range
   * and throw a bind exception if no port in the configured range binds.
   * @param listener jetty listener.
   * @param startPort initial port which is set in the listener.
   * @throws Exception",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,fatalError,org.apache.hadoop.ha.ActiveStandbyElector:fatalError(java.lang.String),768,772,"/**
 * Handles fatal errors by logging and notifying clients.
 * @param errorMessage error message to log and notify
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,transitionToActive,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),89,95,"/**
* Transitions a user to active state.
* @param reqInfo StateChangeRequestInfo containing transition details
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,transitionToStandby,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),97,103,"/**
* Transitions to standby state based on provided request information.
* @param reqInfo StateChangeRequestInfo object containing transition details
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,transitionToObserver,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),105,112,"/**
* Transitions the current state to observer mode.
* @param reqInfo StateChangeRequestInfo object containing transition details
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,transitionToActive,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToActive(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto)",107,117,"/**
* Transitions user to active state.
* @param request TransitionToActiveRequestProto object
* @return TransitionToActiveResponseProto object or throws ServiceException on failure
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,transitionToStandby,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToStandby(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto)",119,129,"/**
* Transitions a server to standby mode based on the provided request.
* @param controller RPC request controller
* @param request TransitionToStandbyRequestProto object
* @throws ServiceException if an error occurs during transition
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,transitionToObserver,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToObserver(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto)",131,141,"/**
* Transitions to observer by processing the given request.
* @param controller RPC controller
* @param request TransitionToObserverRequestProto object
* @throws ServiceException if an error occurs during transition
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,<init>,org.apache.hadoop.ha.SshFenceByTcpPort$Args:<init>(java.lang.String),237,256,"/**
* Parses optional user and SSH port from input string.
* @param arg input string containing user and SSH port (e.g., ""user:port"")
* @throws BadFencingConfigurationException if parsing fails
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,"org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.lang.String)",151,153,"/**
* Prints command usage information to the specified output stream.
* @param pStr output stream
* @param cmd current command being processed
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,parseOpts,"org.apache.hadoop.ha.HAAdmin:parseOpts(java.lang.String,org.apache.commons.cli.Options,java.lang.String[],java.util.Map)",491,503,"/**
* Parses command line options and arguments.
* @param cmdName name of the command
* @param opts command line options to parse
* @param argv array of command line arguments (excluding command name)
* @return parsed CommandLine object or null on error
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,doFence,"org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)",130,171,"/**
* Attempts to kill a process running on a specific port via SSH.
* @param session SSH connection
* @param serviceAddr target service address and port
* @return true if process is successfully fenced, false otherwise
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,addTargetInfoAsEnvVars,"org.apache.hadoop.ha.ShellCommandFencer:addTargetInfoAsEnvVars(org.apache.hadoop.ha.HAServiceTarget,java.util.Map)",220,243,"/**
* Configures environment variables from HA service target info.
* @param target HAServiceTarget object
* @param environment map of environment variables to update
*/","* Add information about the target to the the environment of the
   * subprocess.
   * 
   * @param target
   * @param environment",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,setAclsWithRetries,org.apache.hadoop.ha.ActiveStandbyElector:setAclsWithRetries(java.lang.String),1124,1138,"/**
* Sets ACLs on a ZK node with retries.
* @param path ZK node path
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,zkDoWithRetries,org.apache.hadoop.ha.ActiveStandbyElector:zkDoWithRetries(org.apache.hadoop.ha.ActiveStandbyElector$ZKAction),1140,1143,"/**
* Performs ZooKeeper operation with retries.
* @param action ZKAction to execute
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readNonByteBufferPositionedReadable,"org.apache.hadoop.fs.VectoredReadUtils:readNonByteBufferPositionedReadable(org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer)",151,170,"/**
* Reads data from the specified PositionedReadable stream into the provided ByteBuffer.
* @param stream positioned readable stream to read from
* @param range file range to read, including offset and length
* @param buffer byte buffer to store the read data
*/","* Read into a direct tor indirect buffer using {@code PositionedReadable.readFully()}.
   * @param stream stream
   * @param range file range
   * @param buffer destination buffer
   * @throws IOException IO problems.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,validateVectoredReadRanges,org.apache.hadoop.fs.VectoredReadUtils:validateVectoredReadRanges(java.util.List),82,85,"/**
* Validates and sorts vectored read ranges.
* @param ranges list of file ranges to validate
*/","* Validate a list of vectored read ranges.
   * @param ranges list of ranges.
   * @throws EOFException any EOF exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setCaching,org.apache.hadoop.fs.impl.prefetch.BufferData:setCaching(java.util.concurrent.Future),195,201,"/**
* Sets caching state and schedules future action.
* @param actionFuture asynchronous operation to execute
*/","* Indicates that a caching operation is in progress.
   *
   * @param actionFuture the {@code Future} of a caching action.
   *
   * @throws IllegalArgumentException if actionFuture is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,updateState,"org.apache.hadoop.fs.impl.prefetch.BufferData:updateState(org.apache.hadoop.fs.impl.prefetch.BufferData$State,org.apache.hadoop.fs.impl.prefetch.BufferData$State[])",243,250,"/**
* Updates the state with a new value while ensuring it matches one of the expected current states.
* @param newState new state to apply
* @param expectedCurrentState multiple valid current states
*/","* Updates the current state to the specified value.
   * Asserts that the current state is as expected.
   * @param newState the state to transition to.
   * @param expectedCurrentState the collection of states from which
   *        transition to {@code newState} is allowed.
   *
   * @throws IllegalArgumentException if newState is null.
   * @throws IllegalArgumentException if expectedCurrentState is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPathExistsAsDir,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsDir(java.nio.file.Path,java.lang.String)",357,364,"/**
* Verifies the given path exists and is a directory.
* @param path Path object to check
* @param argName Name of the argument associated with the path
*/","* Validates that the given path exists and is a directory.
   * @param path the path to check.
   * @param argName the name of the argument being validated.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPathExistsAsFile,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsFile(java.nio.file.Path,java.lang.String)",371,375,"/**
* Verifies the given path exists and is a regular file.
* @param path Path object to validate
* @param argName Name of command-line argument associated with this path
*/","* Validates that the given path exists and is a file.
   * @param path the path to check.
   * @param argName the name of the argument being validated.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,isLastBlock,org.apache.hadoop.fs.impl.prefetch.BlockData:isLastBlock(int),127,135,"/**
* Checks if a given block number is the last block in the file.
* @param blockNumber block identifier
* @return true if block is last, false otherwise
*/","* Indicates whether the given block is the last block in the associated file.
   * @param blockNumber the id of the desired block.
   * @return true if the given block is the last block in the associated file, false otherwise.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getStartOffset,org.apache.hadoop.fs.impl.prefetch.BlockData:getStartOffset(int),181,185,"/**
* Calculates start offset in bytes by multiplying block number with block size.
* @param blockNumber unique block identifier
*/","* Gets the start offset of the given block.
   * @param blockNumber the id of the given block.
   * @return the start offset of the given block.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getState,org.apache.hadoop.fs.impl.prefetch.BlockData:getState(int),206,210,"/**
 * Retrieves the blockchain state at the specified block number.
 * @param blockNumber unique block identifier
 */","* Gets the state of the given block.
   * @param blockNumber the id of the given block.
   * @return the state of the given block.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,setState,"org.apache.hadoop.fs.impl.prefetch.BlockData:setState(int,org.apache.hadoop.fs.impl.prefetch.BlockData$State)",218,222,"/**
* Sets the state of the system at a specific block number.
* @param blockNumber unique block identifier
* @param blockState new state to be assigned
*/","* Sets the state of the given block to the given value.
   * @param blockNumber the id of the given block.
   * @param blockState the target state.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getBlockNumber,org.apache.hadoop.fs.impl.prefetch.BlockData:getBlockNumber(long),143,147,"/**
* Calculates block number based on given offset.
* @param offset starting position in bytes
*/","* Gets the id of the block that contains the given absolute offset.
   * @param offset the absolute offset to check.
   * @return the id of the block that contains the given absolute offset.
   * @throws IllegalArgumentException if offset is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_aggregate,"org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_aggregate(java.io.Serializable,java.lang.Object)",94,107,"/**
* Aggregates IO statistics snapshot with provided data.
* @param snapshot IO statistics snapshot to aggregate
* @param statistics IO statistics instance for aggregation
* @return true if aggregation was successful, false otherwise
*/","* Aggregate an existing {@link IOStatisticsSnapshot} with
   * the supplied statistics.
   * @param snapshot snapshot to update
   * @param statistics IOStatistics to add
   * @return true if the snapshot was updated.
   * @throws IllegalArgumentException if the {@code statistics} argument is not
   * null but not an instance of IOStatistics, or if  {@code snapshot} is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_save,"org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_save(java.io.Serializable,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",162,171,"/**
* Saves IO statistics snapshot to file system.
* @param snapshot IO statistics snapshot data
* @param fs file system instance
* @param path output file path
* @param overwrite whether to overwrite existing file
*/","* Save IOStatisticsSnapshot to a Hadoop filesystem as a JSON file.
   * @param snapshot statistics
   * @param fs filesystem
   * @param path path
   * @param overwrite should any existing file be overwritten?
   * @throws UncheckedIOException Any IO exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_counters,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_counters(java.io.Serializable),203,206,"/**
* Retrieves IO statistics counters from a snapshot.
* @param source IOStatisticsSnapshot object or serializable data
*/","* Get the counters of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of counters.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_gauges,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_gauges(java.io.Serializable),213,216,"/**
* Returns gauges statistics from an IOStatisticsSnapshot.
* @param source IOStatisticsSnapshot object
*/","* Get the gauges of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of gauges.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_minimums,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_minimums(java.io.Serializable),223,226,"/**
* Computes minimum IO statistics.
* @param source input data to process
*/","* Get the minimums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of minimums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_maximums,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_maximums(java.io.Serializable),233,236,"/**
* Extracts maximum statistics from an IOStatisticsSnapshot.
* @param source IOStatisticsSnapshot instance
* @return Map of maximum statistics (key: statistic name, value: maximum value)
*/","* Get the maximums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of maximums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_means,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_means(java.io.Serializable),245,253,"/**
* Computes mean statistics for I/O operations.
* @param source IO statistics snapshot
* @return Map of operation types to (sample count, total sum) pairs
*/","* Get the means of an IOStatisticsSnapshot.
   * Each value in the map is the (sample, sum) tuple of the values;
   * the mean is then calculated by dividing sum/sample wherever sample count is non-zero.
   * @param source source of statistics.
   * @return a map of mean key to (sample, sum) tuples.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,copy,org.apache.hadoop.fs.impl.FlagSet:copy(),253,255,"/**
 * Creates a deep copy of this FlagSet instance.
 */","* Create a copy of the FlagSet.
   * @return a new mutable instance with a separate copy of the flags",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,createFlagSet,"org.apache.hadoop.fs.impl.FlagSet:createFlagSet(java.lang.Class,java.lang.String,java.util.EnumSet)",276,281,"/**
* Creates a FlagSet instance with the specified enumeration class and flag set.
* @param enumClass enumeration class for flag values
* @param prefix prefix string for flag names
* @param flags initial EnumSet of flags
*/","* Create a FlagSet.
   * @param enumClass class of enum
   * @param prefix prefix (with trailing ""."") for path capabilities probe
   * @param flags flags
   * @param <E> enum type
   * @return a mutable FlagSet",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,close,org.apache.hadoop.fs.HarFileSystem:close(),733,744,"/**
* Closes the underlying file system and closes any remaining resources.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,close,org.apache.hadoop.fs.RawLocalFileSystem:close(),895,898,"/**
* Closes the underlying resource.
* Calls superclass's close method to perform any necessary cleanup.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,closeAll,org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:closeAll(),155,163,"/**
* Closes all FileSystems in the map.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,close,org.apache.hadoop.fs.FsShell:close(),371,376,"/**
* Closes file system and releases resources.
* @throws IOException if closing operation fails
*/","*  Performs any necessary cleanup
   * @throws IOException upon error",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,close,org.apache.hadoop.fs.FilterFileSystem:close(),527,531,"/**
 * Closes this file system and underlying resources.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem$Cache:closeAll(boolean),3799,3831,"/**
* Closes all FileSystems, handling automatic and manual closings.
* @param onlyAutomatic true to skip non-automatic closings
*/","* Close all FileSystem instances in the Cache.
     * @param onlyAutomatic only close those that are marked for automatic closing
     * @throws IOException a problem arose closing one or more FileSystem.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem$Cache:closeAll(org.apache.hadoop.security.UserGroupInformation),3844,3868,"/**
* Closes FileSystems associated with the given UserGroupInformation.
* @param ugi user group information to filter by
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,compareTo,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:compareTo(org.apache.hadoop.fs.FileStatus),117,120,"/**
* Compares this file status to another using default ordering.
* @param o FileStatus object to compare with
* @return negative, zero, or positive if this is less than, equal to, or greater than the other status
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,readFully,"org.apache.hadoop.fs.FSInputStream:readFully(long,byte[],int,int)",118,133,"/**
* Reads fully into the specified byte array, starting from the given position.
* @param position offset to read from
* @param buffer output buffer
* @param offset buffer offset
* @param length number of bytes to read
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,read,"org.apache.hadoop.fs.BufferedFSInputStream:read(long,byte[],int,int)",115,118,"/**
* Reads data from input stream at specified position into the given buffer.
* @param position position to start reading
* @param buffer array to store read data
* @param offset starting index in buffer for storing data
* @param length maximum number of bytes to read
* @return actual number of bytes read or -1 if end-of-stream reached
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_toJsonString,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_toJsonString(java.io.Serializable),180,184,"/**
* Converts IO statistics snapshot to JSON string.
* @param snapshot IO statistics data
*/","* Save IOStatisticsSnapshot to a JSON string.
   * @param snapshot statistics; may be null or of an incompatible type
   * @return JSON string value
   * @throws UncheckedIOException Any IO/jackson exception.
   * @throws IllegalArgumentException if the supplied class is not a snapshot",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,byte[])",1878,1890,"/**
* Writes bytes to a file at the specified path in the given FileContext.
* @param fileContext file system context
* @param path target file location
* @param bytes data to be written
* @return updated FileContext instance
*/","* Writes bytes to a file. This utility method opens the file for writing,
   * creating the file if it does not exist, or overwrites an existing file. All
   * bytes in the byte array are written to the file.
   *
   * @param fileContext the file context with which to create the file
   * @param path the path to the file
   * @param bytes the byte array with the bytes to write
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)",1948,1966,"/**
* Writes a list of lines to a file using the specified charset.
* @param path target file path
* @param lines iterable of lines to write
* @param cs charset for encoding
* @return original FileContext instance
*/","* Write lines of text to a file. Each line is a char sequence and is written
   * to the file in sequence with each line terminated by the platform's line
   * separator, as defined by the system property {@code
   * line.separator}. Characters are encoded into bytes using the specified
   * charset. This utility method opens the file for writing, creating the file
   * if it does not exist, or overwrites an existing file.
   *
   * @param fileContext the file context with which to create the file
   * @param path the path to the file
   * @param lines a Collection to iterate over the char sequences
   * @param cs the charset to use for encoding
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence,java.nio.charset.Charset)",2014,2028,"/**
* Writes CharSequence to a file.
* @param path file location
* @param charseq text content
* @param cs character encoding scheme
* @return FileContext object
*/","* Write a line of text to a file. Characters are encoded into bytes using the
   * specified charset. This utility method opens the file for writing, creating
   * the file if it does not exist, or overwrites an existing file.
   *
   * @param fs the file context with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   * @param cs the charset to use for encoding
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createFile,org.apache.hadoop.fs.FileSystem:createFile(org.apache.hadoop.fs.Path),4732,4735,"/**
* Creates an output file at the specified path.
* @param path absolute file path
*/","* Create a new FSDataOutputStreamBuilder for the file with path.
   * Files are overwritten by default.
   *
   * @param path file path
   * @return a FSDataOutputStreamBuilder object to build the file
   *
   * HADOOP-14384. Temporarily reduce the visibility of method before the
   * builder interface becomes stable.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,createFile,org.apache.hadoop.fs.ChecksumFileSystem:createFile(org.apache.hadoop.fs.Path),1109,1112,"/**
* Creates an FSDataOutputStreamBuilder for the given file path.
* @param path file system path to create output stream for
*/","* This is overridden to ensure that this class's create() method is
   * ultimately called.
   *
   * {@inheritDoc}",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,appendFile,org.apache.hadoop.fs.FileSystem:appendFile(org.apache.hadoop.fs.Path),4742,4744,"/**
* Appends file to output stream.
* @param path file path to be appended
*/","* Create a Builder to append a file.
   * @param path file path.
   * @return a {@link FSDataOutputStreamBuilder} to build file append request.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,appendFile,org.apache.hadoop.fs.ChecksumFileSystem:appendFile(org.apache.hadoop.fs.Path),1120,1122,"/**
* Creates an output stream builder to append data to a file.
* @param path absolute path to the file
*/","* This is overridden to ensure that this class's create() method is
   * ultimately called.
   *
   * {@inheritDoc}",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],long,long,boolean)",155,159,"/**
* Constructs a BlockLocation object from input arrays.
* @param names array of block names
* @param hosts array of hostnames
* @param cachedHosts array of cached hostnames
* @param topologyPaths array of topology paths
* @param offset block offset (null for default)
* @param length block length (null for default)
* @param corrupt flag indicating corrupted data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,toString,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:toString(),559,562,"/**
* Returns a string representation of the real status.
* @return status string or empty string if not set
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpConfigKeys.java,getServerDefaults,org.apache.hadoop.fs.ftp.FtpConfigKeys:getServerDefaults(),60,71,"/**
* Retrieves default server settings for file system operations.
* @return FsServerDefaults object containing various defaults
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/LocalConfigKeys.java,getServerDefaults,org.apache.hadoop.fs.local.LocalConfigKeys:getServerDefaults(),60,71,"/**
* Returns a new FsServerDefaults object with default settings.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.java,read,org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(),75,80,"/**
* Reads one byte from the underlying input source and returns its value.
* @return The read byte or -1 if end of stream reached
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,next,org.apache.hadoop.fs.FileSystem$DirListingIterator:next(),2332,2342,"/**
* Retrieves the next item from the iterator, fetching more data if necessary.
* @throws NoSuchElementException if no more items are available
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processPath,org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processPath(org.apache.hadoop.fs.shell.PathData),102,118,"/**
* Prints extended attribute from a PathData object.
* @param item PathData object containing file information
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,listStatus,org.apache.hadoop.fs.ChecksumFileSystem:listStatus(org.apache.hadoop.fs.Path),959,962,"/**
* Lists file statuses for the specified path.
* @param f Path to list status from
* @return Array of FileStatus objects or null if error occurs
*/","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   *
   * @param f
   *          given path
   * @return the statuses of the files/directories in the given path
   * @throws IOException if an I/O error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path[]),2140,2143,"/**
* Lists status of given paths.
* @param files array of file paths
*/","* Filter files/directories in the given list of paths using default
   * path filter.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   *
   * @param files
   *          a list of paths
   * @return a list of statuses for the files under the given paths after
   *         applying the filter default Path filter
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",103,108,"/**
* Creates and registers a new synchronized counter with the specified value.
* @param info MetricsInfo object containing metric name
* @param iVal initial value for the counter
* @return newly created MutableCounterInt instance
*/","* Create a mutable integer counter
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new counter object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",127,133,"/**
* Creates and registers a synchronized mutable counter with the given value.
* @param info MetricsInfo object containing metric name
* @param iVal initial value for the counter
* @return MutableCounterLong instance or null if already registered
*/","* Create a mutable long integer counter
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new counter object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",176,181,"/**
* Registers a new gauge with the given value.
* @param info MetricsInfo object
* @param iVal initial value for the gauge
*/","* Create a mutable long integer gauge
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new gauge object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",200,205,"/**
* Creates a synchronized gauge for the specified metric with initial value.
* @param info MetricsInfo object containing metric details
* @param iVal Initial float value for the gauge
* @return MutableGaugeFloat instance associated with the metric name
*/","* Create a mutable float gauge
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new gauge object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",152,157,"/**
* Creates and registers a synchronized gauge with the given initial value.
* @param info MetricsInfo object containing metric details
* @param iVal Initial integer value for the gauge
* @return MutableGaugeInt instance, synchronously updated by all threads
*/","* Create a mutable integer gauge
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new gauge object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addCounter,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",102,109,"/**
* Adds a counter metric to the builder.
* @param info metric details
* @param value counter value
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",120,127,"/**
* Adds a gauge metric with specified value and info.
* @param info MetricsInfo object
* @param value long value of the gauge
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addCounter,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",93,100,"/**
* Adds a counter to the metrics builder.
* @param info MetricsInfo object containing counter details
* @param value Counter value to be added
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",129,136,"/**
* Adds a gauge metric with specified value and info.
* @param info MetricsInfo object
* @param value float value for the gauge
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",138,145,"/**
* Adds a gauge metric with specified value and info.
* @param info MetricsInfo object
* @param value double value of the gauge
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",111,118,"/**
* Adds a gauge metric with specified value.
* @param info MetricsInfo object
* @param value integer value of the gauge
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getFS,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getFS(),54,57,"/**
* Retrieves the underlying file system.
* @return the parent class's file system instance
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FsLinkResolution.java,resolve,"org.apache.hadoop.fs.impl.FsLinkResolution:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.FsLinkResolution$FsLinkResolutionFunction)",91,96,"/**
* Resolves a file system link to an object of type T.
* @param fileContext File context
* @param path Path to resolve
* @return Object of type T or null if unresolved
*/","* Apply the given function to the resolved path under the the supplied
   * FileContext.
   * @param fileContext file context to resolve under
   * @param path path to resolve
   * @param fn function to invoke
   * @param <T> return type.
   * @return the return value of the function as revoked against the resolved
   * path.
   * @throws UnresolvedLinkException link resolution failure
   * @throws IOException other IO failure.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,createGlobber,org.apache.hadoop.fs.Globber:createGlobber(org.apache.hadoop.fs.FileContext),413,415,"/**
* Creates a new GlobBuilder instance with the given FileContext.
* @param fileContext context of the current file system
*/","* Create a builder for a Globber, bonded to the specific file
   * context.
   * @param fileContext file context.
   * @return the builder to finish configuring.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,createGlobber,org.apache.hadoop.fs.Globber:createGlobber(org.apache.hadoop.fs.FileSystem),403,405,"/**
* Creates a new GlobBuilder instance.
* @param filesystem reference to the file system
*/","* Create a builder for a Globber, bonded to the specific filesystem.
   * @param filesystem filesystem
   * @return the builder to finish configuring.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,isDone,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:isDone(),246,266,"/**
* Checks if an asynchronous call has completed.
* @return true if the async call is done, false otherwise
*/","@return true if the call is done; otherwise, return false.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,getAsyncReturn,org.apache.hadoop.io.retry.AsyncCallHandler:getAsyncReturn(),56,66,"/**
* Retrieves an asynchronous getter instance from the cache or a lower layer.
* @return AsyncGet object or null if not found
*/","* @return the async return value from {@link AsyncCallHandler}.
   * @param <T> T.
   * @param <R> R.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String,java.lang.String)",441,443,"/**
* Convenience constructor to create a DeprecationDelta with a single replacement key. 
* @param key original deprecation key
* @param newKey replacement key
* @param customMessage custom message for the deprecation change
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String)",445,447,"/**
* Creates a DeprecationDelta with a single replacement key.
* @param key original key
* @param newKey replacement key to be used in deprecation process
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeCompressedString,"org.apache.hadoop.io.WritableUtils:writeCompressedString(java.io.DataOutput,java.lang.String)",94,96,"/**
* Writes compressed string to output stream.
* @param out DataOutput stream
* @param s input string to compress
* @return length of written data or -1 on error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,concat,"org.apache.hadoop.fs.RawLocalFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])",612,622,"/**
* Concatenates multiple input files into a single output file.
* @param trg output file path
* @param psrcs array of input file paths to concatenate
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,invoke,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:invoke(),279,316,"/**
* Invokes the call with optional asynchronous mode.
* @return CallReturn object or ASYNC_CALL_IN_PROGRESS if still processing
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,equals,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:equals(java.lang.Object),122,125,"/**
* Calls superclass's implementation of equals to compare this object with another.
* @param o object to compare with.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,hashCode,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:hashCode(),127,130,"/**
* Returns hash code based on superclass implementation.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,main,org.apache.hadoop.fs.DUHelper:main(java.lang.String[]),85,90,"/**
* Prints disk usage for the specified folder on Windows or other platforms.
* @param args[0] path to the folder to analyze
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/WindowsGetSpaceUsed.java,refresh,org.apache.hadoop.fs.WindowsGetSpaceUsed:refresh(),45,48,"/**
* Updates folder usage statistics and stores in 'used' variable.
* @see DUHelper#getFolderUsage(String) 
*/",* Override to hook in DUHelper class.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,<init>,org.apache.hadoop.fs.statistics.MeanStatistic:<init>(org.apache.hadoop.fs.statistics.MeanStatistic),107,111,"/**
* Copies properties from another MeanStatistic instance.
* @param that MeanStatistic object to copy from
*/","* Create from another statistic.
   * @param that source",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setMeanStatistic,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",245,251,"/**
* Updates an existing mean statistic by key.
* @param key unique identifier for the mean statistic
* @param value new MeanStatistic object to update with
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,ioStatisticsSourceToString,org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsSourceToString(java.lang.Object),63,70,"/**
* Converts IO statistics source to string representation.
* @param source IO statistics source object or null
* @return String representation of IO statistics or empty string on failure
*/","* Extract the statistics from a source object -or """"
   * if it is not an instance of {@link IOStatistics},
   * {@link IOStatisticsSource} or the retrieved
   * statistics are null.
   * <p>
   * Exceptions are caught and downgraded to debug logging.
   * @param source source of statistics.
   * @return a string for logging.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,toString,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:toString(),103,106,"/**
* Converts statistics to human-readable string representation.
*/","* Return the statistics dump of the wrapped statistics.
   * @return the statistics for logging.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,toString,org.apache.hadoop.fs.statistics.IOStatisticsLogging$StatisticsToString:toString(),325,330,"/**
* Returns string representation of this object's IO statistics.
* @return Human-readable string or NULL_SOURCE if not initialized.","* Evaluate and stringify the statistics.
     * @return a string value.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,toString,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:toString(),253,256,"/**
* Returns a string representation of IO statistics.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,ioStatisticsToPrettyString,org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToPrettyString(org.apache.hadoop.fs.statistics.IOStatistics),102,121,"/**
* Converts IOStatistics to a human-readable string.
* @param statistics IOStatistics object or null
*/","* Convert IOStatistics to a string form, with all the metrics sorted
   * and empty value stripped.
   * This is more expensive than the simple conversion, so should only
   * be used for logging/output where it's known/highly likely that the
   * caller wants to see the values. Not for debug logging.
   * @param statistics A statistics instance.
   * @return string value or the empty string if null",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,createTracker,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:createTracker(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String)",672,678,"/**
* Creates a duration tracker using the provided factory or returns a stub implementation.
* @param factory optional tracker factory, or null for default behavior
* @return created DurationTracker instance, or pre-configured stub if no factory provided
*/","* Create the tracker. If the factory is null, a stub
   * tracker is returned.
   * @param factory tracker factory
   * @param statistic statistic to track
   * @return a duration tracker.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,deleteBlockFileAndEvictCache,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:deleteBlockFileAndEvictCache(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry),446,471,"/**
* Deletes block file and evicts cache entry.
* @param elementToPurge Entry object containing path and block number
*/","* Delete cache file as part of the block cache LRU eviction.
   *
   * @param elementToPurge Block entry to evict.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,submit,org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.util.concurrent.Callable),128,138,"/**
* Submits a task with permit release, acquiring necessary resources.
* @param task the callable task to execute
* @return Future containing result or exception
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,submit,"org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable,java.lang.Object)",140,150,"/**
* Submits a task for execution, acquiring queueing permits and tracking duration.
* @param task Runnable to be executed
* @param result Result type parameter (not used in this method)
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,submit,org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable),152,162,"/**
* Submits a task for execution, acquiring necessary permits.
* @param task the task to execute
* @return a Future representing the result, or failed if interrupted
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,execute,org.apache.hadoop.util.SemaphoredDelegatingExecutor:execute(java.lang.Runnable),164,173,"/**
* Executes a command with permit release.
* @param command the command to be executed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,iterator,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:iterator(),56,59,"/**
* Returns an iterator over long statistics.
* @return Iterator of LongStatistic objects or null if empty
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addTimedOperation(java.lang.String,java.time.Duration)",447,450,"/**
* Wraps timed operation addition with automatic millisecond conversion.
* @param prefix prefix to identify timed operations
* @param duration time interval for the operation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,fromStorageStatistics,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:fromStorageStatistics(org.apache.hadoop.fs.StorageStatistics),72,83,"/**
* Builds IO statistics from given storage statistics.
* @param storageStatistics input statistics to process
*/","* Create  IOStatistics from a storage statistics instance.
   *
   * This will be updated as the storage statistics change.
   * @param storageStatistics source data.
   * @return an IO statistics source.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongCounter(java.lang.String,java.util.concurrent.atomic.AtomicLong)",87,91,"/**
* Adds atomic long counter to statistics.
* @param key unique identifier for counter
* @param source AtomicLong instance to fetch values from
*/","* Add a counter statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long counter
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerCounter(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",100,104,"/**
* Adds atomic integer counter to statistics builder.
* @param key unique identifier for counter
* @param source atomic integer value source
*/","* Add a counter statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int counter
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withMutableCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withMutableCounter(java.lang.String,org.apache.hadoop.metrics2.lib.MutableCounterLong)",113,117,"/**
* Adds mutable counter to builder.
* @param key unique counter identifier
* @param source mutable counter value supplier
*/","* Build a dynamic counter statistic from a
   * {@link MutableCounterLong}.
   * @param key key of this statistic
   * @param source mutable long counter
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongGauge,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongGauge(java.lang.String,java.util.concurrent.atomic.AtomicLong)",138,142,"/**
* Adds an atomic long gauge to the builder.
* @param key gauge identifier
* @param source atomic long value source
*/","* Add a gauge statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long gauge
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerGauge,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerGauge(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",151,155,"/**
* Adds an atomic integer gauge to the builder.
* @param key unique metric identifier
* @param source AtomicInteger value source
*/","* Add a gauge statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int gauge
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongMinimum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongMinimum(java.lang.String,java.util.concurrent.atomic.AtomicLong)",176,180,"/**
* Configures minimum value of atomic long statistic by key.
* @param key unique statistic identifier
* @param source atomic long value to use as minimum
*/","* Add a minimum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long minimum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerMinimum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerMinimum(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",189,193,"/**
* Sets minimum value of dynamic IO statistics to atomic integer's current value.
* @param key unique identifier
* @param source atomic integer containing the value
*/","* Add a minimum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int minimum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongMaximum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongMaximum(java.lang.String,java.util.concurrent.atomic.AtomicLong)",215,219,"/**
* Adds atomic long maximum statistic with specified key.
* @param key unique statistic identifier
* @param source atomic long value to track maximum of
*/","* Add a maximum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long maximum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerMaximum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerMaximum(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",228,232,"/**
* Sets maximum value from AtomicInteger source.
* @param key unique identifier
* @param source AtomicInteger to fetch value from
* @return DynamicIOStatisticsBuilder instance for chaining
*/","* Add a maximum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int maximum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,registerFailureHandling,org.apache.hadoop.service.launcher.ServiceLauncher:registerFailureHandling(),760,772,"/**
* Registers failure handling mechanisms for interrupted threads.
* @throws IllegalArgumentException if configuration is invalid
*/","* Override point: register this class as the handler for the control-C
   * and SIGINT interrupts.
   *
   * Subclasses can extend this with extra operations, such as
   * an exception handler:
   * <pre>
   *  Thread.setDefaultUncaughtExceptionHandler(
   *     new YarnUncaughtExceptionHandler());
   * </pre>",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,accept,org.apache.hadoop.net.unix.DomainSocket:accept(),233,243,"/**
* Accepts a connection on the DomainSocket.
* @return DomainSocket object representing the accepted connection or null if failed
*/","* Accept a new UNIX domain connection.
   *
   * This method can only be used on sockets that were bound with bind().
   *
   * @return                The new connection.
   * @throws IOException    If there was an I/O error performing the accept--
   *                        such as the socket being closed from under us.
   *                        Particularly when the accept is timed out, it throws
   *                        SocketTimeoutException.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,setAttribute,"org.apache.hadoop.net.unix.DomainSocket:setAttribute(int,int)",308,317,"/**
* Sets attribute of reference with specified type and size.
* @param type attribute type
* @param size attribute size
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,getAttribute,org.apache.hadoop.net.unix.DomainSocket:getAttribute(int),321,332,"/**
* Retrieves an attribute of a specified type.
* @param type the type of attribute to retrieve
* @return the retrieved attribute value or throws IOException if unsuccessful
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,shutdown,org.apache.hadoop.net.unix.DomainSocket:shutdown(),395,404,"/**
* Shuts down the resource associated with this object.
* @throws IOException if an I/O error occurs during shutdown
*/","* Call shutdown(SHUT_RDWR) on the UNIX domain socket.
   *
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,sendFileDescriptors,"org.apache.hadoop.net.unix.DomainSocket:sendFileDescriptors(java.io.FileDescriptor[],byte[],int,int)",421,431,"/**
* Sends file descriptors to the native layer.
* @param descriptors array of FileDescriptor objects
* @param jbuf buffer containing file data
* @param offset starting position in jbuf
* @param length number of bytes to send
*/","* Send some FileDescriptor objects to the process on the other side of this
   * socket.
   * 
   * @param descriptors       The file descriptors to send.
   * @param jbuf              Some bytes to send.  You must send at least
   *                          one byte.
   * @param offset            The offset in the jbuf array to start at.
   * @param length            Length of the jbuf array to use.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,recvFileInputStreams,"org.apache.hadoop.net.unix.DomainSocket:recvFileInputStreams(java.io.FileInputStream[],byte[],int,int)",448,487,"/**
* Receives file input stream descriptors from the server.
* @param streams array of FileInputStream objects to populate
* @param buf buffer for receiving data
* @param offset starting position in buffer
* @param length number of bytes to receive
* @return status code or throws IOException if an error occurs
*/","* Receive some FileDescriptor objects from the process on the other side of
   * this socket, and wrap them in FileInputStream objects.
   *
   * @param streams input stream.
   * @param buf input buf.
   * @param offset input offset.
   * @param length input length.
   * @return wrap them in FileInputStream objects.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextIntegration.java,createNewInstance,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:createNewInstance(java.lang.Long),102,107,"/**
* Creates a new IOStatisticsContext instance with the given ID.
* @param key unique identifier for this context
*/","* Creating a new IOStatisticsContext instance for a FS to be used.
   * @param key Thread ID that represents which thread the context belongs to.
   * @return an instance of IOStatisticsContext.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,verifyChunkedSums,"org.apache.hadoop.util.DataChecksum:verifyChunkedSums(java.nio.ByteBuffer,java.nio.ByteBuffer,java.lang.String,long)",401,427,"/**
* Verifies chunked sums for the given ByteBuffer data and checksums.
* @param data input ByteBuffer
* @param checksums ByteBuffer containing checksums
* @param fileName file name associated with the verification
* @param basePos starting position for verification
*/","* Verify that the given checksums match the given data.
   * 
   * The 'mark' of the ByteBuffer parameters may be modified by this function,.
   * but the position is maintained.
   *  
   * @param data the DirectByteBuffer pointing to the data to verify.
   * @param checksums the DirectByteBuffer pointing to a series of stored
   *                  checksums
   * @param fileName the name of the file being read, for error-reporting
   * @param basePos the file position to which the start of 'data' corresponds
   * @throws ChecksumException if the checksums do not match",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,afterDecryption,"org.apache.hadoop.crypto.CryptoInputStream:afterDecryption(org.apache.hadoop.crypto.Decryptor,java.nio.ByteBuffer,long,byte[])",271,286,"/**
* Updates decryptor context and returns padding byte if required.
* @param decryptor Decryptor instance
* @param inBuffer ByteBuffer containing encrypted data
* @param position Current position within the buffer
* @param iv Initialization vector for decryption
* @return Padding byte (0) or updated value if re-init required","* This method is executed immediately after decryption. Check whether 
   * decryptor should be updated and recalculate padding if needed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,resetStreamOffset,org.apache.hadoop.crypto.CryptoInputStream:resetStreamOffset(long),309,317,"/**
* Resets stream offset and initializes buffers for decryption.
* @param offset new stream offset
*/","* Reset the underlying stream offset; clear {@link #inBuffer} and 
   * {@link #outBuffer}. This Typically happens during {@link #seek(long)} 
   * or {@link #skip(long)}.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,write,"org.apache.hadoop.crypto.CryptoOutputStream:write(byte[],int,int)",151,172,"/**
* Writes a byte array to the underlying stream.
* @param b the byte array to write
* @param off starting offset within the array
* @param len number of bytes to write
*/","* Encryption is buffer based.
   * If there is enough room in {@link #inBuffer}, then write to this buffer.
   * If {@link #inBuffer} is full, then do encryption and write data to the
   * underlying stream.
   * @param b the data.
   * @param off the start offset in the data.
   * @param len the number of bytes to write.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,flush,org.apache.hadoop.crypto.CryptoOutputStream:flush(),262,269,"/**
* Flushes data to storage, encrypting it first.
* @throws IOException if an I/O error occurs during flushing.","* To flush, we need to encrypt the data in the buffer and write to the 
   * underlying stream, then do the flush.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobPattern.java,compile,org.apache.hadoop.fs.GlobPattern:compile(java.lang.String),57,59,"/**
* Compiles a shell glob pattern into a regular expression.
* @param globPattern glob pattern to be compiled
*/","* Compile glob pattern string
   * @param globPattern the glob pattern
   * @return the pattern object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,init,"org.apache.hadoop.fs.GlobFilter:init(java.lang.String,org.apache.hadoop.fs.PathFilter)",64,73,"/**
* Initializes the pattern and filter for user profile files.
* @param filePattern glob pattern to match profiles
* @param filter PathFilter object to validate profiles
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,prepare,org.apache.hadoop.fs.shell.find.Name:prepare(),73,80,"/**
* Initializes search pattern and glob pattern based on command-line argument.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingTar,"org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.InputStream,java.io.File,boolean)",1035,1051,"/**
* Uncompresses and extracts tar archive to specified directory.
* @param inputStream input stream containing tar archive
* @param untarDir target directory for extraction
* @param gzipped true if tar is gzip-compressed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,close,org.apache.hadoop.fs.sftp.SFTPFileSystem:close(),710,722,"/**
* Closes the resource, releasing any held connections and setting a flag to prevent further access.
* @throws IOException if an I/O error occurs during shutdown
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,create,"org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",723,729,"/**
* Creates a file output stream with specified permissions and settings.
* @param f the path to create the output stream for
* @param permission the file system permissions to apply
* @return FSDataOutputStream instance or null if failed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,resetChecksumBufSize,org.apache.hadoop.fs.FSOutputSummer:resetChecksumBufSize(),263,265,"/**
* Resets checksum buffer size based on chunking configuration.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getAllStatistics,org.apache.hadoop.fs.AbstractFileSystem:getAllStatistics(),234,244,"/**
* Retrieves a synchronized map of all statistics, creating copies for each URI.
* @return Map<URI, Statistics> containing cloned statistic objects
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,<init>,"org.apache.hadoop.fs.FileSystemStorageStatistics:<init>(java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)",118,125,"/**
* Initializes storage statistics with name and non-null data.
* @param name unique storage identifier
* @param stats non-null FileSystem.Statistics object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,getLongStatistics,org.apache.hadoop.fs.FileSystemStorageStatistics:getLongStatistics(),132,135,"/**
* Returns an iterator over long statistics data.
* @return Iterator of LongStatistic objects or null if empty
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,getLong,org.apache.hadoop.fs.FileSystemStorageStatistics:getLong(java.lang.String),137,140,"/**
* Retrieves long value from stats map by given key.
* @param key unique identifier of data to retrieve
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesReadByDistance,org.apache.hadoop.fs.FileSystem$Statistics:getBytesReadByDistance(int),4411,4430,"/**
* Calculates bytes read based on distance.
* @param distance proximity level (0-5)
* @return total bytes read
*/","* In the common network topology setup, distance value should be an even
     * number such as 0, 2, 4, 6. To make it more general, we group distance
     * by {1, 2}, {3, 4} and {5 and beyond} for accounting. So if the caller
     * ask for bytes read for distance 2, the function will return the value
     * for group {1, 2}.
     * @param distance the network distance
     * @return the total number of bytes read by the network distance",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,reset,org.apache.hadoop.fs.FileSystemStorageStatistics:reset(),157,160,"/**
 * Resets game statistics to their initial state.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,clearStatistics,org.apache.hadoop.fs.AbstractFileSystem:clearStatistics(),218,222,"/**
* Resets all statistics in the table to their default state.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,primitiveCreate,"org.apache.hadoop.fs.FileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",1334,1359,"/**
* Creates or appends to a file with specified permissions and flags.
* @param f file path
* @param absolutePermission file permissions
* @param flag create flags (APPEND, OVERWRITE, etc.)
* @return FSDataOutputStream instance or null if not created
*/","* This create has been added to support the FileContext that processes
   * the permission with umask before calling this method.
   * This a temporary method added to support the transition from FileSystem
   * to FileContext for user applications.
   *
   * @param f path.
   * @param absolutePermission permission.
   * @param flag create flag.
   * @param bufferSize buffer size.
   * @param replication replication.
   * @param blockSize block size.
   * @param progress progress.
   * @param checksumOpt check sum opt.
   * @return output stream.
   * @throws IOException IO failure",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,<init>,"org.apache.hadoop.fs.AbstractFileSystem:<init>(java.net.URI,java.lang.String,boolean,int)",278,283,"/**
* Initializes an AbstractFileSystem instance from a URI.
* @param uri the file system's URI
* @param supportedScheme supported scheme (e.g., ""file"", ""http"")
* @param authorityNeeded whether authority is required in the URI
* @param defaultPort default port number for the scheme
*/","* Constructor to be called by subclasses.
   * 
   * @param uri for this file system.
   * @param supportedScheme the scheme supported by the implementor
   * @param authorityNeeded if true then theURI must have authority, if false
   *          then the URI must have null authority.
   * @param defaultPort default port to use if port is not specified in the URI.
   * @throws URISyntaxException <code>uri</code> has syntax error",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,encode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(byte[][],byte[][])",117,127,"/**
* Encodes input byte arrays and stores output in separate byte arrays.
* @param inputs array of input byte arrays
* @param outputs array to store encoded output byte arrays
*/","* Encode with inputs and generates outputs. More see above.
   *
   * @param inputs input buffers to read data from
   * @param outputs output buffers to put the encoded data into, read to read
   *                after the call
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,encode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",68,99,"/**
* Encodes input buffers and stores output in corresponding buffers.
* @param inputs array of input ByteBuffers to encode
* @param outputs array of output ByteBuffers to store encoded data
*/","* Encode with inputs and generates outputs.
   *
   * Note, for both inputs and outputs, no mixing of on-heap buffers and direct
   * buffers are allowed.
   *
   * If the coder option ALLOW_CHANGE_INPUTS is set true (false by default), the
   * content of input buffers may change after the call, subject to concrete
   * implementation. Anyway the positions of input buffers will move forward.
   *
   * @param inputs input buffers to read data from. The buffers' remaining will
   *               be 0 after encoding
   * @param outputs output buffers to put the encoded data into, ready to read
   *                after the call
   * @throws IOException if the encoder is closed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable:<init>(java.lang.Object),122,124,"/**
* Initializes an ArrayPrimitiveWritable with the given primitive array value.
* @param value primitive array to be stored in this writable object
*/","* Wrap an existing array of primitives
   * @param value - array of primitives",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getCanonicalUri,org.apache.hadoop.fs.HarFileSystem:getCanonicalUri(),318,321,"/**
* Returns canonical URI based on file system configuration.
* @return Canonical URI object.","* Used for delegation token related functionality. Must delegate to
   * underlying file system.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getCanonicalUri,org.apache.hadoop.fs.FilterFileSystem:getCanonicalUri(),113,116,"/**
 * Returns the canonical URI as per file system conventions.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFsStatus,org.apache.hadoop.fs.DelegateToFileSystem:getFsStatus(),148,151,"/**
 * Retrieves file system status.
 * @return FsStatus object containing file system metadata
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,hasCapability,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:hasCapability(java.lang.String),687,693,"/**
* Checks if the object has a specific capability.
* @param capability unique identifier for the capability to check
* @return true if the object has the capability, false otherwise
*/","* Probe the inner stream for a capability.
     * Syncable operations are rejected before being passed down.
     * @param capability string to query the stream support for.
     * @return true if a capability is known to be supported.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,hasCapability,org.apache.hadoop.io.SequenceFile$Writer:hasCapability(java.lang.String),1410,1416,"/**
* Checks if the object has a specific capability.
* @param capability name of capability to check for
* @return true if capability exists, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,hasCapability,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:hasCapability(java.lang.String),484,487,"/**
* Checks if the user has a specific capability.
* @param capability name of the capability to check for
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,addToCacheAndRelease,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:addToCacheAndRelease(org.apache.hadoop.fs.impl.prefetch.BufferData,java.util.concurrent.Future,java.time.Instant)",484,552,"/**
* Adds data to cache and releases resources.
* @param data BufferData object containing block information
* @param blockFuture Future containing result of asynchronous operation
* @param taskQueuedStartTime Instant when task was queued
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,release,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData),215,226,"/**
* Releases block and buffers associated with the provided BufferData.
* @param data BufferData containing block number and other metadata
*/","* Releases resources allocated to the given block.
   *
   * @throws IllegalArgumentException if data is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,releaseDoneBlocks,org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseDoneBlocks(),192,198,"/**
* Releases done blocks from the buffer.
* Iterates over all BufferData objects and releases those with state DONE.
*/",* Releases resources for any blocks marked as 'done'.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getSummary,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getSummary(boolean),232,250,"/**
* Generates a summary string for all operations.
* @param showDebugInfo true to include debug info, false for summaries only
* @return Summary string or empty if no ops found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,<init>,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:<init>(org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters),113,137,"/**
* Initializes a CachingBlockManager instance with given parameters.
* @param blockManagerParameters configuration for caching and block management
*/","* Constructs an instance of a {@code CachingBlockManager}.
   *
   * @param blockManagerParameters params for block manager.
   * @throws IllegalArgumentException if bufferPoolSize is zero or negative.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,<init>,"org.apache.hadoop.util.BlockingThreadPoolExecutorService:<init>(int,java.util.concurrent.ThreadPoolExecutor)",104,108,"/**
* Initializes a custom thread pool executor service with specified permit count and shared event processing executor.
* @param permitCount maximum number of concurrent threads
* @param eventProcessingExecutor shared event processing executor instance
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,get,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:get(int,java.nio.ByteBuffer)",265,283,"/**
* Reads and validates a file entry by block number into the provided buffer.
* @param blockNumber unique block identifier
* @param buffer byte buffer to store the entry data
* @throws IOException if an I/O error occurs during reading or validation
*/","* Gets the block having the given {@code blockNumber}.
   *
   * @throws IllegalArgumentException if buffer is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,toString,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:toString(),635,647,"/**
* Returns a human-readable representation of the cache and pool.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,absolute,org.apache.hadoop.fs.impl.prefetch.FilePosition:absolute(),145,148,"/**
* Calculates the absolute offset from the start of the buffer.
* @return Absolute offset as a long value
*/","* Gets the current absolute position within this file.
   *
   * @return the current absolute position within this file.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,bufferFullyRead,org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferFullyRead(),241,246,"/**
* Checks if a buffer is fully read and valid.
* @return true if the buffer is fully consumed, false otherwise
*/","* Determines whether the current buffer has been fully read.
   *
   * @return true if the current buffer has been fully read, false otherwise.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,setAbsolute,org.apache.hadoop.fs.impl.prefetch.FilePosition:setAbsolute(long),157,165,"/**
* Sets absolute position in the buffer.
* @param pos new offset from buffer start
* @return true if successfully set, false otherwise
*/","* If the given {@code pos} lies within the current buffer, updates the current position to
   * the specified value and returns true; otherwise returns false without changing the position.
   *
   * @param pos the absolute position to change the current position to if possible.
   * @return true if the given current position was updated, false otherwise.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsContext.java,getCurrentIOStatisticsContext,org.apache.hadoop.fs.statistics.IOStatisticsContext:getCurrentIOStatisticsContext(),71,77,"/**
* Retrieves the current IO statistics context, throwing exception on null result.
*/","* Get the context's IOStatisticsContext.
   *
   * @return instance of IOStatisticsContext for the context.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsContext.java,setThreadIOStatisticsContext,org.apache.hadoop.fs.statistics.IOStatisticsContext:setThreadIOStatisticsContext(org.apache.hadoop.fs.statistics.IOStatisticsContext),84,88,"/**
* Sets thread I/O statistics context.
* @param statisticsContext context to be applied
*/","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getInstanceConfigs,org.apache.hadoop.metrics2.impl.MetricsConfig:getInstanceConfigs(java.lang.String),157,171,"/**
* Retrieves configuration for instances of a given type.
* @param type unique identifier for the type
*/","* Return sub configs for instance specified in the config.
   * Assuming format specified as follows:<pre>
   * [type].[instance].[option] = [value]</pre>
   * Note, '*' is a special default instance, which is excluded in the result.
   * @param type  of the instance
   * @return  a map with [instance] as key and config object as value",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,applyItem,org.apache.hadoop.fs.shell.find.Find:applyItem(org.apache.hadoop.fs.shell.PathData),412,419,"/**
* Applies a single item to the current path, stopping or adding it based on depth and root expression result.
* @param item PathData object to apply
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,processArguments,org.apache.hadoop.fs.shell.find.Find:processArguments(java.util.LinkedList),421,429,"/**
* Prepares the root expression and executes the processing of arguments.
* @param args list of PathData objects
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,processOptions,org.apache.hadoop.fs.shell.Test:processOptions(java.util.LinkedList),59,75,"/**
* Parses command-line flags and extracts the first specified flag.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Rm:processOptions(java.util.LinkedList),81,90,"/**
* Parses command-line options and populates flags.
* @param args list of command arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processOptions,org.apache.hadoop.fs.shell.Display$Checksum:processOptions(java.util.LinkedList),189,195,"/**
* Parses command line options and sets display block size.
* @param args list of command line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processOptions,org.apache.hadoop.fs.shell.FsUsage$Df:processOptions(java.util.LinkedList),85,92,"/**
* Parses command-line options and sets human-readable flag.
* @param args list of command arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,processOptions,org.apache.hadoop.fs.shell.Head:processOptions(java.util.LinkedList),50,54,"/**
* Parses command-line options from the argument list.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processOptions,org.apache.hadoop.fs.shell.Ls:processOptions(java.util.LinkedList),132,153,"/**
* Parses command-line options and initializes internal state.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,processOptions,org.apache.hadoop.fs.shell.Tail:processOptions(java.util.LinkedList),63,78,"/**
* Parses command-line options and sets up follow behavior.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Expunge:processOptions(java.util.LinkedList),235,242,"/**
* Parses command line options and sets up immediate flag and filesystem path.
* @param args list of command line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Rmdir:processOptions(java.util.LinkedList),197,203,"/**
* Parses command line arguments and extracts the -ignore-fail-on-non-empty flag.
* @param args list of command line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Get:processOptions(java.util.LinkedList),234,249,"/**
* Parses command line options and sets properties accordingly.
* @param args list of command line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,processOptions,org.apache.hadoop.fs.shell.Count:processOptions(java.util.LinkedList),124,178,"/**
* Parses command options and sets up display properties.
* @param args list of command arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,processOptions,org.apache.hadoop.fs.shell.Mkdir:processOptions(java.util.LinkedList),51,56,"/**
* Parses command-line arguments and extracts parent option value.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processOptions,org.apache.hadoop.fs.shell.TouchCommands$Touch:processOptions(java.util.LinkedList),134,146,"/**
* Parses command options and populates object properties.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Put:processOptions(java.util.LinkedList),276,292,"/**
* Parses command-line options and sets corresponding settings.
* @param args list of input arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Cp:processOptions(java.util.LinkedList),175,189,"/**
* Processes command-line options and sets configuration.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processOptions(java.util.LinkedList),377,390,"/**
* Processes command line options.
* @param args list of command arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processOptions,org.apache.hadoop.fs.shell.Display$Cat:processOptions(java.util.LinkedList),80,86,"/**
* Parses command-line arguments and updates checksum verification flag.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processOptions,org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processOptions(java.util.LinkedList),64,75,"/**
* Validates and processes command options.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Stat.java,processOptions,org.apache.hadoop.fs.shell.Stat:processOptions(java.util.LinkedList),82,89,"/**
* Parses command-line options and sets recursive flag.
* @param args list of arguments to parse
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processOptions,org.apache.hadoop.fs.shell.TouchCommands$Touchz:processOptions(java.util.LinkedList),61,65,"/**
* Parses command line arguments according to specified format.
* @param args list of input arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,parse,"org.apache.hadoop.fs.shell.CommandFormat:parse(java.lang.String[],int)",87,92,"/**
* Truncates and parses the given arguments list up to the specified position.
* @param args array of command-line arguments
* @param pos position from which to start parsing
* @return truncated list of parsed strings
*/","Parse parameters starting from the given position
   * Consider using the variant that directly takes a List
   * 
   * @param args an array of input arguments
   * @param pos the position at which starts to parse
   * @return a list of parameters",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processOptions,org.apache.hadoop.fs.shell.MoveCommands$Rename:processOptions(java.util.LinkedList),104,109,"/**
* Processes command-line options and retrieves remote destination.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,processOptions,org.apache.hadoop.fs.shell.Truncate:processOptions(java.util.LinkedList),51,66,"/**
* Parses command-line options and extracts user ID and new length.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,processOptions,org.apache.hadoop.fs.shell.SetReplication:processOptions(java.util.LinkedList),56,72,"/**
* Processes command-line options and validates replication value.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processOptions,org.apache.hadoop.fs.shell.FsUsage$Du:processOptions(java.util.LinkedList),183,192,"/**
 * Parses command-line options and sets corresponding properties.
 * @param args list of command-line arguments
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processOptions,org.apache.hadoop.fs.FsShellPermissions$Chown:processOptions(java.util.LinkedList),144,150,"/**
* Parses command-line options and sets up recursive processing.
* @param args list of command-line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,displayError,org.apache.hadoop.fs.shell.Command:displayError(java.lang.String),501,504,"/**
* Increments error count and displays warning with given message.
* @param message error message to be displayed
*/","* Display an error string prefaced with the command name.  Also increments
   * the error count for the command which will result in a non-zero exit
   * code.
   * @param message error message to display",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printInstanceUsage,"org.apache.hadoop.fs.FsShell:printInstanceUsage(java.io.PrintStream,org.apache.hadoop.fs.shell.Command)",249,251,"/**
* Prints command usage prefix and instance-specific usage.
* @param out output stream to write to
* @param instance Command object containing usage information
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsCollectorImpl.java,addRecord,org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:addRecord(java.lang.String),52,55,"/**
* Adds a new metrics record with specified name.
* @param name unique identifier for the record
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processArguments,org.apache.hadoop.fs.shell.FsUsage$Df:processArguments(java.util.LinkedList),94,105,"/**
* Processes command-line arguments and outputs filesystem usage table.
* @param args list of PathData objects
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processArguments,org.apache.hadoop.fs.shell.FsUsage$Du:processArguments(java.util.LinkedList),194,207,"/**
* Processes command-line arguments and prints usage table to output stream.
* @param args list of PathData objects
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createPathHandle,"org.apache.hadoop.fs.RawLocalFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",1148,1172,"/**
* Creates a path handle for a file, throwing exceptions if the path is not a regular file.
* @param stat FileStatus object containing file metadata
* @param opts Options with HandleOpt settings (data and location)
*/","* Hook to implement support for {@link PathHandle} operations.
   * @param stat Referent in the target FileSystem
   * @param opts Constraints that determine the validity of the
   *            {@link PathHandle} reference.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,exact,org.apache.hadoop.fs.Options$HandleOpt:exact(),384,386,"/**
 * Creates an array of HandleOpt objects representing unchanged and unmoved elements.
 */","* Handle is valid iff the referent is neither moved nor changed.
     * Equivalent to changed(false), moved(false).
     * @return Options requiring that the content and location of the entity
     * be unchanged between calls.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,content,org.apache.hadoop.fs.Options$HandleOpt:content(),394,396,"/**
 * Returns an array of HandleOptions with predefined settings.
 */","* Handle is valid iff the content of the referent is the same.
     * Equivalent to changed(false), moved(true).
     * @return Options requiring that the content of the entity is unchanged,
     * but it may be at a different location.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,path,org.apache.hadoop.fs.Options$HandleOpt:path(),404,406,"/**
* Returns array of handle options for path operations.
*/","* Handle is valid iff the referent is unmoved in the namespace.
     * Equivalent to changed(true), moved(false).
     * @return Options requiring that the referent exist in the same location,
     * but its content may have changed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,reference,org.apache.hadoop.fs.Options$HandleOpt:reference(),414,416,"/**
* Returns an array of HandleOpt instances with updated status.
* @return Array of HandleOpt objects
*/","* Handle is valid iff the referent exists in the namespace.
     * Equivalent to changed(true), moved(true).
     * @return Options requiring that the implementation resolve a reference
     * to this entity regardless of changes to content or location.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map,long,boolean)",1248,1259,"/**
* Initializes ShellCommandExecutor with command, working directory, environment,
* timeout, and inheritance settings.
* @param execString command to execute
* @param dir working directory for the command
* @param env environment variables for the process
* @param timeout maximum execution time in milliseconds
* @param inheritParentEnv whether to inherit parent's environment
*/","* Create a new instance of the ShellCommandExecutor to execute a command.
     *
     * @param execString The command to execute with arguments
     * @param dir If not-null, specifies the directory which should be set
     *            as the current working directory for the command.
     *            If null, the current working directory is not modified.
     * @param env If not-null, environment of the command will include the
     *            key-value pairs specified in the map. If null, the current
     *            environment is not modified.
     * @param timeout Specifies the time in milliseconds, after which the
     *                command will be killed and the status marked as timed-out.
     *                If 0, the command will not be timed out.
     * @param inheritParentEnv Indicates if the process should inherit the env
     *                         vars from the parent process or not.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,initRefreshThread,org.apache.hadoop.fs.CachingGetSpaceUsed:initRefreshThread(boolean),108,118,"/**
* Initializes and starts the refresh thread if enabled.
* @param runImmediately whether to run immediately or wait for refresh interval
*/","* RunImmediately should set true, if we skip the first refresh.
   * @param runImmediately The param default should be false.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,privateClone,org.apache.hadoop.security.token.Token:privateClone(org.apache.hadoop.io.Text),243,245,"/**
 * Creates a private clone of the current token instance with a new service.
 */","* Create a private clone of a public token.
   * @param newService the new service name
   * @return a private token",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAllStoragePolicies,org.apache.hadoop.fs.viewfs.ViewFileSystem:getAllStoragePolicies(),1125,1139,"/**
* Retrieves a collection of all block storage policies from child file systems.
* @return Collection of BlockStoragePolicySpi objects
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,initAsyncCall,"org.apache.hadoop.io.retry.AsyncCallHandler:initAsyncCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue)",333,353,"/**
* Initializes asynchronous call with given parameters.
* @param asyncCall asynchronous call object
* @param asyncCallReturn value to be returned from the call
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,process,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:process(java.nio.ByteBuffer,java.nio.ByteBuffer)",164,182,"/**
* Encrypts input data and writes result to output buffer, optionally resetting crypto context if all input consumed. 
* @param inBuffer input data to encrypt
* @param outBuffer output buffer for encrypted result
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,initializeInterceptors,org.apache.hadoop.fs.viewfs.RegexMountPoint:initializeInterceptors(),98,115,"/**
* Initializes interceptors from a comma-separated string of settings.
* @throws IOException if invalid settings are encountered
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,main,org.apache.hadoop.fs.DF:main(java.lang.String[]),216,223,"/**
* Initializes and prints a Distributed Filesystem (DFS) instance.
* @param args command-line arguments; first argument is the DFS root directory
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,initialize,"org.apache.hadoop.fs.Path:initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",258,266,"/**
* Initializes the URI with the specified scheme, authority, path, and fragment.
* @param scheme protocol scheme (e.g., http or https)
* @param authority network location (e.g., domain name or IP address)
* @param path resource path
* @param fragment fragment identifier (optional)
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,uriToString,"org.apache.hadoop.fs.shell.PathData:uriToString(java.net.URI,boolean)",466,487,"/**
* Converts URI to string representation.
* @param uri input URI object
* @param inferredSchemeFromPath whether scheme was inferred from path
* @return string representation of URI, or just the path if scheme was inferred
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,checkNotSchemeWithRelative,org.apache.hadoop.fs.Path:checkNotSchemeWithRelative(),85,90,"/**
* Validates that URI is absolute with no scheme.
*/","* Test whether this Path uses a scheme and is relative.
   * Pathnames with scheme and relative path are illegal.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isAbsoluteAndSchemeAuthorityNull,org.apache.hadoop.fs.Path:isAbsoluteAndSchemeAuthorityNull(),377,380,"/**
* Checks if URI path is absolute and scheme/authority are null.
*/","* Returns true if the path component (i.e. directory) of this URI is
   * absolute <strong>and</strong> the scheme is null, <b>and</b> the authority
   * is null.
   *
   * @return whether the path is absolute and the URI has no scheme nor
   * authority parts",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isAbsolute,org.apache.hadoop.fs.Path:isAbsolute(),399,401,"/**
* Checks if the URI path is absolute.
* @return true if absolute, false otherwise
*/","* Returns true if the path component (i.e. directory) of this URI is
   * absolute.  This method is a wrapper for {@link #isUriPathAbsolute()}.
   *
   * @return whether this URI's path is absolute",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,checkPath,org.apache.hadoop.fs.AbstractFileSystem:checkPath(org.apache.hadoop.fs.Path),369,413,"/**
* Validates the given file system path against the current instance's URI.
* @param path Path to validate
*/","* Check that a Path belongs to this FileSystem.
   * 
   * If the path is fully qualified URI, then its scheme and authority
   * matches that of this file system. Otherwise the path must be 
   * slash-relative name.
   * @param path the path.
   * @throws InvalidPathException if the path is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,write,org.apache.hadoop.fs.FileStatus:write(java.io.DataOutput),530,537,"/**
* Writes this object's data to the output stream.
* @throws IOException if an I/O error occurs
*/","* Write instance encoded as protobuf to stream.
   * @param out Output stream
   * @see PBHelper#convert(FileStatus)
   * @deprecated Use the {@link PBHelper} and protobuf serialization directly.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkPutArguments,"org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPutArguments(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long)",114,124,"/**
* Validates input arguments for uploading a file part.
* @param filePath the file path
* @param inputStream the input stream
* @param partNumber the part number (must be > 0)
* @param uploadId the upload handle (cannot be null)
* @param lengthInBytes the part length (must be >= 0)
*/","* Check all the arguments to the
   * {@link MultipartUploader#putPart(UploadHandle, int, Path, InputStream, long)}
   * operation.
   * @param filePath Target path for upload (as {@link #startUpload(Path)}).
   * @param inputStream Data for this part. Implementations MUST close this
   * stream after reading in the data.
   * @param partNumber Index of the part relative to others.
   * @param uploadId Identifier from {@link #startUpload(Path)}.
   * @param lengthInBytes Target length to read from the stream.
   * @throws IllegalArgumentException invalid argument",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,abortUploadsUnderPath,org.apache.hadoop.fs.impl.AbstractMultipartUploader:abortUploadsUnderPath(org.apache.hadoop.fs.Path),132,138,"/**
* Aborts all uploads under a given directory path.
* @param path directory path to cancel uploads for
*/","* {@inheritDoc}.
   * @param path path to abort uploads under.
   * @return a future to -1.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,append,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",1452,1456,"/**
* Throws an exception when attempting to append data to a read-only mount point.
* @param f the file path
* @param bufferSize buffer size for writing
* @param progress progress callback (ignored)
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path,boolean)",1498,1503,"/**
* Deletes a file or directory.
* @param f the path to delete
* @param recursive true for recursive deletion (default: false)
* @throws AccessControlException if access is denied
* @throws IOException on I/O error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,rename,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",1739,1745,"/**
* Renames a file or directory by updating its path.
* @param src source file/directory to be renamed
* @param dst new name for the file/directory
* @return true if rename operation is successful, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:truncate(org.apache.hadoop.fs.Path,long)",1747,1750,"/**
* Truncates a file to a specified length.
* @param f Path of the file to truncate
* @param newLength desired file size in bytes
* @throws IOException if truncation fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setOwner,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1752,1757,"/**
* Sets file owner to specified user and group.
* @param f Path object
* @param username new owner username
* @param groupname new owning group name
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setPermission,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1759,1764,"/**
* Sets file system permissions on a given path.
* @param f the path to set permissions for
* @param permission the new FsPermission to apply
* @throws AccessControlException if access control fails
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setReplication,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setReplication(org.apache.hadoop.fs.Path,short)",1766,1771,"/**
* Sets file system replication level.
* @param f Path to the file system
* @param replication desired replication level (short value)
* @throws AccessControlException if access is denied
* @throws IOException on I/O error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setTimes,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)",1773,1778,"/**
* Sets file times (modified and access) for the given path.
* @param f Path to set times for
* @param mtime Modified time in milliseconds
* @param atime Access time in milliseconds
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1800,1805,"/**
* Modifies ACL entries for the specified file/directory.
* @param path path to modify ACL entries for
* @param aclSpec list of updated ACL entries
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1807,1812,"/**
* Removes ACL entries from a file or directory.
* @param path the file system path
* @param aclSpec list of AclEntry objects to be removed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path),1814,1818,"/**
* Removes default ACL from the specified file or directory.
* @param path Path to the resource
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeAcl(org.apache.hadoop.fs.Path),1820,1824,"/**
* Removes ACL (Access Control List) from specified file or directory.
* @param path Path to the file or directory
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",1826,1830,"/**
* Sets ACL (Access Control List) for the specified file system path.
* @param path file system path
* @param aclSpec list of ACL entries to be set
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",1841,1846,"/**
* Sets an extended attribute on a file or directory.
* @param path the path to set the attribute for
* @param name the name of the attribute
* @param value the byte array value of the attribute
* @param flag the set flags for this operation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1869,1873,"/**
* Removes extended attribute from file or directory.
* @param path filesystem path to access
* @param name extended attribute name to remove
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1875,1880,"/**
* Creates a new snapshot of the specified file system at the given path.
* @param path file system to snapshot
* @param snapshotName name for the new snapshot
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1882,1887,"/**
* Renames a snapshot on the specified file system.
* @param path File system path
* @param snapshotOldName Current name of the snapshot
* @param snapshotNewName New name for the snapshot
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1889,1894,"/**
* Deletes a snapshot by name at the specified mount point.
* @param path the file system mount point
* @param snapshotName the name of the snapshot to delete
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),1901,1905,"/**
* Satisfies storage policy by checking path and throwing exception if read-only mount. 
* @param src Path to be checked
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",1907,1912,"/**
* Sets storage policy for a given path.
* @param src Path to be policy-set
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),1914,1918,"/**
* Unsets storage policy on a given path.
* @param src Path to unset storage policy for
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,delete,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path,boolean)",1044,1049,"/**
* Deletes a file or directory.
* @param f the file to be deleted
* @param recursive whether deletion should be recursive
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:truncate(org.apache.hadoop.fs.Path,long)",1308,1313,"/**
* Truncates a file to a specified length.
* @param f the file path to truncate
* @param newLength the desired length in bytes
* @return false (always throws an exception)
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",1315,1321,"/**
* Renames a file or directory within the internal filesystem.
* @param src original path
* @param dst new path
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSymlink,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1328,1332,"/**
* Creates symbolic link to target file/directory.
* @param target target file/directory path
* @param link symbolic link path
* @throws AccessControlException if access is denied
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setOwner,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1340,1345,"/**
* Sets owner for a file or directory.
* @param f file path
* @param username new owner username
* @param groupname new owner group name
* @throws AccessControlException if access control fails
* @throws IOException on I/O error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setPermission,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1347,1352,"/**
* Sets file system permissions on a specified path.
* @param f the file or directory path
* @param permission the desired file system permissions
* @throws AccessControlException if access control fails
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setReplication,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setReplication(org.apache.hadoop.fs.Path,short)",1354,1359,"/**
* Sets replication factor on a file system path.
* @param f file system path
* @param replication replication factor (short value)
* @throws AccessControlException if access is denied
* @throws IOException on I/O error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setTimes,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)",1361,1366,"/**
* Sets file timestamps (mtime and atime) for the given path.
* @param f Path to set timestamps for
* @param mtime Modified time in milliseconds
* @param atime Access time in milliseconds
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1374,1379,"/**
* Modifies ACL entries for the specified file/directory.
* @param path Path to modify ACL for
* @param aclSpec List of new ACL entries
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1381,1386,"/**
* Removes ACL entries from a file or directory.
* @param path the file or directory to modify
* @param aclSpec list of ACL entries to remove
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path),1388,1392,"/**
* Removes default ACL from specified path.
* @param path file system path to remove ACL from
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeAcl(org.apache.hadoop.fs.Path),1394,1398,"/**
* Removes ACL from specified file or directory.
* @param path Path to resource with ACL to be removed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",1400,1404,"/**
* Sets ACL (Access Control List) for the given file or directory.
* @param path file system path
* @param aclSpec list of ACL entries to apply
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",1415,1420,"/**
* Sets extended attribute on a file or directory.
* @param path Path to the file or directory
* @param name Attribute name
* @param value Attribute value bytes
* @param flag XAttrSetFlag enum specifying operation type
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1443,1447,"/**
* Removes extended attribute from file system path.
* @param path file system path to modify
* @param name name of the extended attribute to remove
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1449,1454,"/**
* Creates a read-only snapshot of a specified file system.
* @param path the file system to snapshot
* @param snapshotName name for the new snapshot
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1456,1461,"/**
* Renames a snapshot at the specified path.
* @param path file system path to the snapshot
* @param snapshotOldName current name of the snapshot
* @param snapshotNewName new desired name for the snapshot
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1463,1468,"/**
* Deletes a snapshot by name at the specified path.
* @param path file system path to snapshot location
* @param snapshotName unique identifier of snapshot to delete
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),1470,1473,"/**
* Applies storage policy to given file system path.
* @param path file system path to apply policy to
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",1475,1479,"/**
* Sets storage policy for given file system path.
* @param path file system path
* @param policyName name of the storage policy to apply
* @throws IOException if an I/O error occurs during policy application
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createInternal,"org.apache.hadoop.fs.viewfs.ViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",340,364,"/**
* Creates a FSDataOutputStream on the specified file with optional flags and permissions.
* @param f Path to the file
* @param flag Optional creation flags
* @param absolutePermission File system permissions
* @param bufferSize Buffer size for I/O operations
* @param replication Replication factor
* @param blockSize Block size for I/O operations
* @param progress Progress monitor
* @param checksumOpt Checksum option
* @param createParent Whether to create parent directories if they don't exist
* @return FSDataOutputStream object or throws an exception",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSymlink,"org.apache.hadoop.fs.viewfs.ViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",651,667,"/**
* Creates a symbolic link from the given target to the specified location.
* @param target path of the target file
* @param link path where the symbolic link will be created
* @param createParent whether to create parent directories if they don't exist
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:equals(java.lang.Object),3923,3929,"/**
* Checks for equality with another LinkedSegmentsDescriptor.
* @param o object to compare with
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,getCredentialEntry,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getCredentialEntry(java.lang.String),173,198,"/**
* Retrieves a credential entry by its alias.
* @param alias unique identifier for the credential
* @return CredentialEntry object or null if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,getAliases,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getAliases(),206,226,"/**
* Retrieves a list of aliases from the key store.
* @return List of String aliases
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,skip,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:skip(long),291,299,"/**
* Skips the specified number of bytes in the underlying stream.
* @param n number of bytes to skip
* @return the total number of bytes skipped, or 0 if already at EOF
*/","* Skips over and discards <code>n</code> bytes of data from the
     * input stream.
     *
     * The <code>skip</code> method skips over some smaller number of bytes
     * when reaching end of file before <code>n</code> bytes have been skipped.
     * The actual number of bytes skipped is returned.  If <code>n</code> is
     * negative, no bytes are skipped.
     *
     * @param      n   the number of bytes to be skipped.
     * @return     the actual number of bytes skipped.
     * @exception  IOException  if an I/O error occurs.
     *             ChecksumException if the chunk to skip to is corrupted",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getPermissions,org.apache.hadoop.fs.ftp.FTPFileSystem:getPermissions(org.apache.commons.net.ftp.FTPFile),455,461,"/**
* Retrieves file permissions from FTP server for the given file.
* @param ftpFile FTPFile object containing file information
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,applyUMask,org.apache.hadoop.fs.permission.FsPermission:applyUMask(org.apache.hadoop.fs.permission.FsPermission),297,301,"/**
* Applies a user mask to the current permissions.
* @param umask the user mask to apply
* @return updated FsPermission object
*/","* Apply a umask to this permission and return a new one.
   *
   * The umask is used by create, mkdir, and other Hadoop filesystem operations.
   * The mode argument for these operations is modified by removing the bits
   * which are set in the umask.  Thus, the umask limits the permissions which
   * newly created files and directories get.
   *
   * @param umask              The umask to use
   * 
   * @return                   The effective permission",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FSProtos$FsPermissionProto),38,41,"/**
* Converts FsPermissionProto to FsPermission object.
* @param proto permission protocol buffer
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getPermissions,org.apache.hadoop.fs.sftp.SFTPFileSystem:getPermissions(com.jcraft.jsch.ChannelSftp$LsEntry),305,307,"/**
* Retrieves file permissions from SFTP file attributes.
* @param sftpFile SFTP file entry containing attributes
* @return FsPermission object representing file permissions
*/","* Return file permission.
   *
   * @param sftpFile
   * @return file permission",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processPath,org.apache.hadoop.fs.FsShellPermissions$Chmod:processPath(org.apache.hadoop.fs.shell.PathData),98,110,"/**
* Updates file system permissions based on the given PathData.
* @param item PathData object containing file statistics
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission:<init>(int),127,129,"/**
* Constructs an FsPermission object from a Unix-style file mode.
* @param mode Unix-style file mode (permissions and type)
*/","* Construct by the given mode.
   *
   * octal mask is applied.
   *
   *<pre>
   *              before mask     after mask    file type   sticky bit
   *
   *    octal     100644            644         file          no
   *    decimal    33188            420
   *
   *    octal     101644           1644         file          yes
   *    decimal    33700           1420
   *
   *    octal      40644            644         directory     no
   *    decimal    16804            420
   *
   *    octal      41644           1644         directory     yes
   *    decimal    17316           1420
   *</pre>
   *
   * 100644 becomes 644 while 644 remains as 644
   *
   * @param mode Mode is supposed to come from the result of native stat() call.
   *             It contains complete permission information: rwxrwxrwx, sticky
   *             bit, whether it is a directory or a file, etc. Upon applying
   *             mask, only permission and sticky bit info will be kept because
   *             they are the only parts to be used for now.
   * @see #FsPermission(short mode)",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getDefault,org.apache.hadoop.fs.permission.FsPermission:getDefault(),416,418,"/**
* Returns default file system permissions with read/write access for owner and group.
* @return FsPermission object representing the default permissions
*/","* Get the default permission for directory and symlink.
   * In previous versions, this default permission was also used to
   * create files, so files created end up with ugo+x permission.
   * See HADOOP-9155 for detail. 
   * Two new methods are added to solve this, please use 
   * {@link FsPermission#getDirDefault()} for directory, and use
   * {@link FsPermission#getFileDefault()} for file.
   * This method is kept for compatibility.
   *
   * @return Default FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getDirDefault,org.apache.hadoop.fs.permission.FsPermission:getDirDefault(),425,427,"/**
* Returns default file system permissions for directories.
* @return FsPermission object with default directory access rights
*/","* Get the default permission for directory.
   *
   * @return DirDefault FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getFileDefault,org.apache.hadoop.fs.permission.FsPermission:getFileDefault(),434,436,"/**
* Creates default file permissions.
* @return FsPermission object with rwxr-x permissions
*/","* Get the default permission for file.
   *
   * @return FileDefault FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getCachePoolDefault,org.apache.hadoop.fs.permission.FsPermission:getCachePoolDefault(),443,445,"/**
* Returns default permissions for cache pool.
* @return FsPermission object with rwxr-x permissions
*/","* Get the default permission for cache pools.
   *
   * @return CachePoolDefault FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,valueOf,org.apache.hadoop.fs.permission.FsPermission:valueOf(java.lang.String),452,475,"/**
* Converts a Unix symbolic permission string to an FsPermission object.
* @param unixSymbolicPermission String representation of permissions (e.g., ""rwxr-x"")
* @return FsPermission object or null if input is invalid
*/","* Create a FsPermission from a Unix symbolic permission string
   * @param unixSymbolicPermission e.g. ""-rw-rw-rw-""
   * @return FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission:<init>(short),479,481,"/**
* Initializes an ImmutableFsPermission with the specified Unix-style file system permission. 
* @param permission Unix-style permission value (e.g., read/execute, write/exec)",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processOptions,org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processOptions(java.util.LinkedList),188,248,"/**
* Validates and processes command-line options.
* @param args list of arguments to parse
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,printAclEntriesForSingleScope,"org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:printAclEntriesForSingleScope(org.apache.hadoop.fs.permission.AclStatus,org.apache.hadoop.fs.permission.FsPermission,java.util.List)",112,126,"/**
* Prints ACL entries for a single scope.
* @param aclStatus Acl status
* @param fsPerm File system permissions
* @param entries List of AclEntries to print
*/","* Prints all the ACL entries in a single scope.
     * @param aclStatus AclStatus for the path
     * @param fsPerm FsPermission for the path
     * @param entries List<AclEntry> containing ACL entries of file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,aclSpecToString,org.apache.hadoop.fs.permission.AclEntry:aclSpecToString(java.util.List),330,337,"/**
* Converts ACL specification to a comma-separated string.
* @param aclSpec list of AclEntry objects
*/","* Convert a List of AclEntries into a string - the reverse of parseAclSpec.
   * @param aclSpec List of AclEntries to convert
   * @return String representation of aclSpec",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission:<init>(java.lang.String),148,150,"/**
* Constructs an FsPermission instance from a string representation.
* @param mode string specifying permissions (e.g. ""rwxr-x"")
*/","* Construct by given mode, either in octal or symbolic format.
   * @param mode mode as a string, either in octal or symbolic format
   * @throws IllegalArgumentException if <code>mode</code> is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processOptions,org.apache.hadoop.fs.FsShellPermissions$Chmod:processOptions(java.util.LinkedList),81,96,"/**
* Parses command line options and sets recursive flag.
*@param args list of command line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,read,org.apache.hadoop.fs.store.ByteBufferInputStream:read(),94,100,"/**
* Reads one byte from the underlying buffer.
* @return next available byte value or -1 if end of data reached
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,skip,org.apache.hadoop.fs.store.ByteBufferInputStream:skip(long),102,114,"/**
* Seeks to a specified offset in the underlying file.
* @param offset absolute offset position
* @return new position or throws IOException if invalid seek
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,mark,org.apache.hadoop.fs.store.ByteBufferInputStream:mark(int),140,145,"/**
* Marks current position in the byte buffer.
* @param readlimit number of bytes to be read before marking is invalidated
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,read,"org.apache.hadoop.fs.store.ByteBufferInputStream:read(byte[],int,int)",169,189,"/**
* Reads up to length bytes from the underlying buffer starting at offset.
* @param b destination byte array
* @param offset starting index within b
* @param length maximum number of bytes to read
* @return number of bytes actually read, or -1 if end-of-file reached","* Read in data.
   * @param b destination buffer.
   * @param offset offset within the buffer.
   * @param length length of bytes to read.
   * @throws EOFException if the position is negative
   * @throws IndexOutOfBoundsException if there isn't space for the
   * amount of data requested.
   * @throws IllegalArgumentException other arguments are invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:startUpload(),887,897,"/**
* Initiates block upload, flushing and closing output stream.
* @return BlockUploadData object with buffer file
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:startUpload(),593,600,"/**
* Starts block upload, returning data size and input stream.
* @return BlockUploadData object containing upload metadata
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:startUpload(),722,731,"/**
* Starts upload process and returns initial data.
* @return BlockUploadData object containing initial data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,close,org.apache.hadoop.fs.store.DataBlocks$DataBlock:close(),475,481,"/**
* Closes the resource and enters closed state.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,checkAndWriteSync,org.apache.hadoop.io.SequenceFile$Writer:checkAndWriteSync(),1445,1450,"/**
* Checks and writes synchronization data when required.
* @throws IOException on write failure
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressedSize,org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getCompressedSize(),170,173,"/**
* Calculates the compressed size by subtracting initial position from current file pointer. 
* @return Compressed file size in bytes 
*/","* Current size of compressed data.
       * 
       * @return
       * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getFileLength,org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:getFileLength(),507,512,"/**
* Retrieves the length of the file in bytes. 
* @return Length of the file or -1 if unknown
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processPathArgument,org.apache.hadoop.fs.shell.Ls:processPathArgument(org.apache.hadoop.fs.shell.PathData),232,246,"/**
* Processes a path argument, handling ECPolicy and recursion.
* @param item PathData object containing file system and path information
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,adjustColumnWidths,org.apache.hadoop.fs.shell.Ls:adjustColumnWidths(org.apache.hadoop.fs.shell.PathData[]),327,355,"/**
* Updates column widths for PathData items.
* @throws IOException if an I/O error occurs
*/","* Compute column widths and rebuild the format string
   * @param items to find the max field width for each column",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processPath,org.apache.hadoop.fs.shell.FsUsage$Du:processPath(org.apache.hadoop.fs.shell.PathData),219,230,"/**
* Updates usage table with file size and space consumption details.
* @param item PathData object containing file information
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getQuotaUsage,org.apache.hadoop.fs.FileSystem:getQuotaUsage(org.apache.hadoop.fs.Path),1952,1954,"/**
* Retrieves quota usage information for the specified file.
* @param f Path to the file
* @return QuotaUsage object containing usage details or null if not available
*/","Return the {@link QuotaUsage} of a given {@link Path}.
   * @param f path to use
   * @return the quota usage
   * @throws IOException IO failure",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getUsed,org.apache.hadoop.fs.FileSystem:getUsed(org.apache.hadoop.fs.Path),2730,2732,"/**
* Retrieves total used content length at specified file system Path.
* @param path file system location to fetch usage summary from
* @return total used content length in bytes or -1 if error occurs
*/","* Return the total size of all files from a specified path.
   * @param path the path.
   * @throws IOException IO failure
   * @return the number of path content summary.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/AbstractLaunchableService.java,<init>,org.apache.hadoop.service.launcher.AbstractLaunchableService:<init>(java.lang.String),48,50,"/**
* Initializes an AbstractLaunchableService instance with the given name.
* @param name unique identifier of the service
*/","* Construct an instance with the given name.
   *
   * @param name input name.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,<init>,org.apache.hadoop.service.CompositeService:<init>(java.lang.String),53,55,"/**
* Initializes a new instance of CompositeService with the specified name.
* @param name unique service identifier
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,<init>,org.apache.hadoop.util.JvmPauseMonitor:<init>(),70,72,"/**
* Initializes a new instance of JvmPauseMonitor with its class name.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,enterState,org.apache.hadoop.service.ServiceStateModel:enterState(org.apache.hadoop.service.Service$STATE),113,119,"/**
* Transitions service to a new state.
* @param proposed target state identifier
* @return previous state before transition
*/","* Enter a state -thread safe.
   *
   * @param proposed proposed new state
   * @return the original state
   * @throws ServiceStateException if the transition is not permitted",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,instantiateService,org.apache.hadoop.service.launcher.ServiceLauncher:instantiateService(org.apache.hadoop.conf.Configuration),658,694,"/**
* Instantiates and returns an instance of the specified service class.
* @param conf Configuration object
* @return Service instance or throws exception on failure
*/","* @return Instantiate the service defined in {@code serviceClassName}.
   *
   * Sets the {@code configuration} field
   * to the the value of {@code conf},
   * and the {@code service} field to the service created.
   *
   * @param conf configuration to use",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.Throwable,java.lang.String,java.lang.Object[])",1094,1098,"/**
* Constructs a KerberosDiagsFailure with specified category and details.
* @param category failure category
* @param throwable underlying exception
* @param message descriptive message
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,serviceStop,org.apache.hadoop.service.CompositeService:serviceStop(),128,136,"/**
* Stops all started services and calls the superclass's serviceStop method.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,run,org.apache.hadoop.service.CompositeService$CompositeServiceShutdownHook:run(),184,187,"/**
 * Stops the composite service instance quietly.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,progressable,org.apache.hadoop.io.MapFile$Writer:progressable(org.apache.hadoop.util.Progressable),308,310,"/**
* Returns progressable option for SequenceFile Writer.
* @param value Progressable object to track writing progress
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,valueClass,org.apache.hadoop.io.MapFile$Writer:valueClass(java.lang.Class),293,295,"/**
* Returns option to specify value class for sequence file writer.
* @param value the Class of the values to be written
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,equals,org.apache.hadoop.io.BinaryComparable:equals(java.lang.Object),74,82,"/**
* Compares this object with the given object for equality.
* @param other object to compare, must be of type BinaryComparable
*/",* Return true if bytes from {#getBytes()} match.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,hashCode,org.apache.hadoop.io.BytesWritable:hashCode(),207,210,"/**
* Returns the hash code of this object.
* Delegates to superclass implementation.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,hashCode,org.apache.hadoop.io.Text:hashCode(),422,425,"/**
* Returns hash code based on superclass implementation.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,hashCode,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:hashCode(),95,98,"/**
* Computes and returns hash code of this object based on its token.
* @return Hash code value of the associated token.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,set,"org.apache.hadoop.io.BytesWritable:set(byte[],int,int)",178,182,"/**
* Updates data in byte array.
* @param newData new data to replace
* @param offset starting index of replacement
* @param length number of bytes to update
*/","* Set the value to a copy of the given byte range.
   *
   * @param newData the new values to copy in
   * @param offset the offset in newData to start at
   * @param length the number of bytes to copy",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,readFields,org.apache.hadoop.io.BytesWritable:readFields(java.io.DataInput),184,189,"/**
* Reads serialized fields from input stream.
* @param in DataInput object containing serialized data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKey,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(org.apache.hadoop.io.BytesWritable),1690,1694,"/**
* Sets and retrieves key length from BytesWritable object.
* @param key BytesWritable object to modify
* @return key length (0 if not set)
*/","* Copy the key into BytesWritable. The input BytesWritable will be
         * automatically resized to the actual key size.
         * 
         * @param key
         *          BytesWritable to hold the key.
         * @throws IOException raised on errors performing I/O.
         * @return the key into BytesWritable.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,list,org.apache.hadoop.fs.FileUtil:list(java.io.File),1619,1630,"/**
* Lists files in a directory.
* @param dir the directory to list
* @return array of file names, or throws exception if access is denied or an error occurs.","* A wrapper for {@link File#list()}. This java.io API returns null
   * when a dir is not a directory or for any I/O error. Instead of having
   * null check everywhere File#list() is used, we will add utility API
   * to get around this problem. For the majority of cases where we prefer
   * an IOException to be thrown.
   * @param dir directory for which listing should be performed
   * @return list of file names or empty string list
   * @exception AccessDeniedException for unreadable directory
   * @exception IOException for invalid directory or for bad disk",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkAccessByFileMethods,org.apache.hadoop.util.DiskChecker:checkAccessByFileMethods(java.io.File),153,174,"/**
* Verifies directory access by checking its existence, readability, writability and executability.
* @param dir the directory to check
*/","* Checks that the current running process can read, write, and execute the
   * given directory by using methods of the File object.
   * 
   * @param dir File to check
   * @throws DiskErrorException if dir is not readable, not writable, or not
   *   executable",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,mlock,"org.apache.hadoop.io.nativeio.NativeIO$POSIX:mlock(java.nio.ByteBuffer,long)",470,477,"/**
* Locks the specified direct ByteBuffer in memory for exclusive access.
* @param buffer direct ByteBuffer to lock
* @param len length of data to lock
*/","* Locks the provided direct ByteBuffer into memory, preventing it from
     * swapping out. After a buffer is locked, future accesses will not incur
     * a page fault.
     * 
     * See the mlock(2) man page for more information.
     * 
     * @throws NativeIOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/SharedFileDescriptorFactory.java,create,"org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:create(java.lang.String,java.lang.String[])",74,100,"/**
* Creates a SharedFileDescriptorFactory instance using the provided prefix and paths.
* @param prefix unique identifier for the factory
* @param paths array of file descriptor locations
* @return SharedFileDescriptorFactory object or throws an IOException on failure
*/","* Create a new SharedFileDescriptorFactory.
   *
   * @param prefix       The prefix to prepend to all the file names created
   *                       by this factory.
   * @param paths        An array of paths to use.  We will try each path in 
   *                       succession, and return a factory using the first 
   *                       usable path.
   * @return             The factory.
   * @throws IOException If a factory could not be created for any reason.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getMemlockLimit,org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:getMemlockLimit(),285,287,"/**
* Retrieves the memlock limit in bytes.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,writeChecksumChunks,"org.apache.hadoop.fs.FSOutputSummer:writeChecksumChunks(byte[],int,int)",212,228,"/**
* Writes checksum chunks for the given byte array.
* @param b input data
* @param off starting offset in the byte array
* @param len length of the byte array to process
*/","Generate checksums for the given data chunks and output chunks & checksums
   * to the underlying output stream.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,calculateChunkedSums,"org.apache.hadoop.util.DataChecksum:calculateChunkedSums(java.nio.ByteBuffer,java.nio.ByteBuffer)",534,564,"/**
* Calculates chunked sums for the given data and checksum buffers.
* @param data ByteBuffer containing data to process
* @param checksums ByteBuffer for storing calculated checksums
*/","* Calculate checksums for the given data.
   * 
   * The 'mark' of the ByteBuffer parameters may be modified by this function,
   * but the position is maintained.
   * 
   * @param data the DirectByteBuffer pointing to the data to checksum.
   * @param checksums the DirectByteBuffer into which checksums will be
   *                  stored. Enough space must be available in this
   *                  buffer to put the checksums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,freeBuffers,org.apache.hadoop.crypto.CryptoInputStream:freeBuffers(),809,813,"/**
* Releases allocated buffers and cleans up buffer pool.
*/",Forcibly free the direct buffers.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,unbuffer,org.apache.hadoop.crypto.CryptoInputStream:unbuffer(),853,858,"/**
* Unbuffers streams and resources.
* Closes buffer pools and decryptors. 
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BoundedByteArrayOutputStream.java,<init>,org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(int),45,47,"/**
 * Initializes a new instance of the stream with the specified capacity.
 * @param capacity maximum size of the buffer in bytes
 */","* Create a BoundedByteArrayOutputStream with the specified
   * capacity
   * @param capacity The capacity of the underlying byte array",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,decodeFromUrlString,org.apache.hadoop.security.token.Token:decodeFromUrlString(java.lang.String),382,384,"/**
* Decodes data from URL string.
* @param newValue new string value to decode
*/","* Decode the given url safe string into this token.
   * @param newValue the encoded string
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getOutputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),89,107,"/**
* Retrieves erased blocks from the given ECBlockGroup.
* @param blockGroup ECBlockGroup to extract blocks from
* @return Array of erased ECBlock objects, or null if none found
*/","* Which blocks were erased ?
   * @param blockGroup blockGroup.
   * @return output blocks to recover",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",36,49,"/**
* Initializes ByteBufferDecodingState with decoder and input/output buffers.
* @param decoder RawErasureDecoder instance
* @param inputs array of input ByteBuffers
* @param erasedIndexes array of indices to be erased from inputs
* @param outputs array of output ByteBuffers
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,byte[][],int[],byte[][])",37,52,"/**
* Initializes ByteArrayDecodingState with provided decoder and input/output buffers.
* @param decoder RawErasureDecoder instance for decoding
* @param inputs byte[][] of input buffers
* @param erasedIndexes int[] of erased indexes
* @param outputs byte[][] of output buffers
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),38,41,"/**
* Creates a native XOR-based raw decoder instance.
* @param coderOptions configuration options for the decoder
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),38,41,"/**
* Creates a native RS raw decoder instance.
* @param coderOptions Erasure coder options
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),43,53,"/**
* Initializes legacy raw decoder with given ErasureCoderOptions.
* @param coderOptions configuration options for erasure coding
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,53,"/**
* Initializes RSLegacyRawEncoder with ErasureCoderOptions.
* @param coderOptions options for erasure coding
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),33,36,"/**
* Creates a native XOR encoder instance with specified options.
* @param coderOptions configuration settings for the encoder
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),33,36,"/**
* Creates a native RS encoder instance.
* @param coderOptions encoding options
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState),39,62,"/**
* Performs XOR-based decoding on input buffers and writes result to a single output buffer.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState),39,60,"/**
* Encodes input buffers into a single output buffer using XOR operation.
* @param encodingState state object containing input and output buffers
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState),167,217,"/**
* Adjusts input buffers for decoding and calls the implementation.
*@param decodingState state object containing input/output buffers
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState),55,88,"/**
* Performs encoding using the provided state.
* @param encodingState Encoding state containing outputs and inputs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState),63,68,"/**
* Encodes data using GF tables and outputs to provided buffers.
* @param gfTables table of Galois Field values
* @param encodingState state object containing input/output buffers and encode length
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),64,86,"/**
* Decrypts and combines input buffers into a single output buffer.
* @param decodingState decryption state object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),62,85,"/**
* Encodes input buffers using XOR operation.
* @param encodingState state object containing input and output buffers
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),111,165,"/**
* Decodes input data and adjusts output buffers for caller convenience.
* @param decodingState decoding state object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),90,128,"/**
* Performs the encode operation using a given encoding state.
* @param encodingState state object containing input/output buffers and offsets
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),70,79,"/**
* Encodes user data using RSUtil and resets output buffers.
* @param encodingState state object containing input/output buffers
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,40,"/**
* Creates a raw erasure decoder instance.
* @param coderOptions ErasureCoder options to configure the decoder
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,35,"/**
* Creates a raw encoder instance based on provided ErasureCoderOptions.
* @param coderOptions options for the erasure coder
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,processErasures,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:processErasures(int[]),117,140,"/**
* Initializes erasure state and generates decoding matrix.
* @param erasedIndexes array of unit indexes to erase
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,skipToNextMarker,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextMarker(long,int)",217,257,"/**
* Skips to the next marker of a specified length in the compressed stream.
* @param marker target marker value
* @param markerBitLength bit length of the marker
* @return true if the marker is found, false otherwise
*/","* This method tries to find the marker (passed to it as the first parameter)
   * in the stream. It can find bit patterns of length &lt;= 63 bits.
   * Specifically this method is used in CBZip2InputStream to find the end of
   * block (EOB) delimiter in the stream, starting from the current position
   * of the stream. If marker is found, the stream position will be at the
   * byte containing the starting bit of the marker.
   * @param marker The bit pattern to be found in the stream
   * @param markerBitLength No of bits in the marker
   * @return true if the marker was found otherwise false
   * @throws IOException raised on errors performing I/O.
   * @throws IllegalArgumentException if marketBitLength is greater than 63",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsGetUByte,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetUByte(),660,662,"/**
 * Reads and returns an unsigned byte value from the input stream.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsGetInt,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetInt(),664,666,"/**
* Reads and combines 4 bytes from the input stream into a single integer.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,getAndMoveToFrontDecode0,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getAndMoveToFrontDecode0(int),1006,1037,"/**
* Decodes and moves a value to front using bit-stream.
* @param groupNo selector index
* @return decoded integer value
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,recvDecodingTables,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:recvDecodingTables(),709,788,"/**
* Receives and decodes decoding tables from a binary stream.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,<init>,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:<init>(java.io.OutputStream,int)",627,643,"/**
* Initializes CBZip2OutputStream with specified block size.
* @param out OutputStream to write compressed data to
* @param blockSize compression block size (1-9)
*/","* Constructs a new <tt>CBZip2OutputStream</tt> with specified blocksize.
  *
  * <p>
  * <b>Attention: </b>The caller is resonsible to write the two BZip2 magic
  * bytes <tt>""BZ""</tt> to the specified stream prior to calling this
  * constructor.
  * </p>
  *
  *
  * @param out
  *            the destination stream.
  * @param blockSize
  *            the blockSize as 100k units.
  *
  * @throws IOException
  *             if an I/O error occurs in the specified stream.
  * @throws IllegalArgumentException
  *             if {@code (blockSize < 1) || (blockSize > 9)}
  * @throws NullPointerException
  *             if {@code out == null}.
  *
  * @see #MIN_BLOCKSIZE
  * @see #MAX_BLOCKSIZE",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,moveToFrontCodeAndSend,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:moveToFrontCodeAndSend(),1391,1395,"/**
* Moves code to front and sends MTF values.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,blockSort,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:blockSort(),1605,1629,"/**
* Performs block sort, attempting to find a sorted block and then sorting it.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockCompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",69,71,"/**
* Initializes BlockCompressorStream with default settings.
* @param out target output stream
* @param compressor compression algorithm instance
*/","* Create a {@link BlockCompressorStream} with given output-stream and 
   * compressor.
   * Use default of 512 as bufferSize and compressionOverhead of 
   * (1% of bufferSize + 12 bytes) =  18 bytes (zlib algorithm).
   * 
   * @param out stream
   * @param compressor compressor to be used",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,"org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",74,77,"/**
* Initializes DecompressorStream with input stream and decompressor.
* @param in input stream to read from
* @param decompressor decompression algorithm to apply
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",48,51,"/**
* Initializes a BlockDecompressorStream with input stream and decompressor.
* @param in input stream to decompress
* @param decompressor decompression algorithm instance
* @param bufferSize buffer size for decompression
*/","* Create a {@link BlockDecompressorStream}.
   * 
   * @param in input stream
   * @param decompressor decompressor to use
   * @param bufferSize size of buffer
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,createInputStream,"org.apache.hadoop.io.compress.PassthroughCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",138,142,"/**
* Creates a compression-aware input stream using provided decompressor.
* @param in original input stream
* @param decompressor decompression strategy to apply
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,read,"org.apache.hadoop.io.compress.DecompressorStream:read(byte[],int,int)",95,106,"/**
* Reads compressed data into the provided byte array.
* @param b target buffer
* @param off offset in bytes
* @param len number of bytes to read
* @return actual number of bytes read or -1 on error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,write,"org.apache.hadoop.io.compress.BlockCompressorStream:write(byte[],int,int)",81,134,"/**
* Compresses and writes data in chunks up to MAX_INPUT_SIZE.
* @param b input byte array
* @param off starting offset within b
* @param len length of chunk
*/","* Write the data provided to the compression codec, compressing no more
   * than the buffer size less the compression overhead as specified during
   * construction for each block.
   *
   * Each block contains the uncompressed length for the block, followed by
   * one or more length-prefixed blocks of compressed data.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<init>(),70,72,"/**
* Initializes decompressor with default stream size.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:<init>(int),298,300,"/**
* Initializes a new instance of the decompressor with the specified direct buffer size.
* @param directBufferSize the maximum size of the direct buffer
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,<init>,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(int,int)",90,92,"/**
* Constructs a ZStandard compressor with specified compression level and buffer sizes.
* @param level compression level (0-22)
* @param bufferSize input/output buffer size
*/","* Creates a new compressor with the default compression level.
   * Compressed data will be generated in ZStandard format.
   * @param level level.
   * @param bufferSize bufferSize.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getCompressor,org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec),167,169,"/**
* Returns a Compressor instance based on the provided CompressionCodec.
* @param codec compression algorithm to use
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getDecompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:getDecompressor(),319,343,"/**
* Retrieves a Decompressor instance from CodecPool or returns null if not available.
*@return Decompressor object or null
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodec.java,createOutputStreamWithCodecPool,"org.apache.hadoop.io.compress.CompressionCodec$Util:createOutputStreamWithCodecPool(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration,java.io.OutputStream)",128,143,"/**
* Creates a CompressionOutputStream with the given codec and compressor pool.
* @param codec compression codec
* @param conf configuration
* @param out output stream to compress
* @return CompressionOutputStream or null on failure
*/","* Create an output stream with a codec taken from the global CodecPool.
     *
     * @param codec       The codec to use to create the output stream.
     * @param conf        The configuration to use if we need to create a new codec.
     * @param out         The output stream to wrap.
     * @return            The new output stream
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionOutputStream.java,close,org.apache.hadoop.io.compress.CompressionOutputStream:close(),61,75,"/**
* Closes the output stream and returns a tracked compressor to the pool.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Writer:close(),1422,1443,"/**
* Closes resources and releases compressor and output stream.
*/",Close the file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,returnCompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:returnCompressor(org.apache.hadoop.io.compress.Compressor),310,317,"/**
* Returns the specified Compressor to the Codec Pool.
* @param compressor Compressor object to be returned
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodec.java,createInputStreamWithCodecPool,"org.apache.hadoop.io.compress.CompressionCodec$Util:createInputStreamWithCodecPool(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration,java.io.InputStream)",154,169,"/**
* Creates a compression input stream with a codec pool.
* @param codec CompressionCodec instance
* @param conf Configuration object (not used)
* @param in InputStream to compress
* @return CompressionInputStream or null if creation fails
*/","* Create an input stream with a codec taken from the global CodecPool.
     *
     * @param codec       The codec to use to create the input stream.
     * @param conf        The configuration to use if we need to create a new codec.
     * @param in          The input stream to wrap.
     * @return            The new input stream
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionInputStream.java,close,org.apache.hadoop.io.compress.CompressionInputStream:close(),65,75,"/**
* Closes the input stream and returns decompressors to pool.
* @throws IOException if closing or returning fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Reader:close(),2169,2188,"/**
* Releases resources and closes deserializers.
* @throws IOException on close operation failure
*/",Close the file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,returnDecompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:returnDecompressor(org.apache.hadoop.io.compress.Decompressor),345,352,"/**
* Returns a Decompressor to the Codec pool.
* @param decompressor Decompressor instance to return
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createCompressor,org.apache.hadoop.io.compress.GzipCodec:createCompressor(),62,67,"/**
* Creates a compressor instance based on native Zlib availability.
* @return Compressor object, either GzipZlib or BuiltInGzip
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibCompressor,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressor(org.apache.hadoop.conf.Configuration),109,113,"/**
* Returns a compressor instance based on zlib configuration.
* @param conf Hadoop Configuration object
*/","* Return the appropriate implementation of the zlib compressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the zlib compressor.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibDirectDecompressor,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDirectDecompressor(org.apache.hadoop.conf.Configuration),144,147,"/**
* Returns a native zlib direct decompressor instance if supported.
* @param conf configuration object
* @return DirectDecompressor object or null if not supported
*/","* Return the appropriate implementation of the zlib direct decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the zlib decompressor.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createDirectDecompressor,org.apache.hadoop.io.compress.GzipCodec:createDirectDecompressor(),109,114,"/**
* Creates a direct decompressor based on native zlib availability.
* @return DirectDecompressor instance or null if native zlib is not loaded
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibDecompressor,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressor(org.apache.hadoop.conf.Configuration),133,136,"/**
* Returns a zlib decompressor instance based on native support.
* @param conf configuration object indicating native zlib availability
*/","* Return the appropriate implementation of the zlib decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the zlib decompressor.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createDecompressor,org.apache.hadoop.io.compress.GzipCodec:createDecompressor(),95,100,"/**
* Creates a decompressor instance based on native Zlib availability.
* @return Decompressor object, either GzipZlib or BuiltInGzip implementation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,decompress,"org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:decompress(byte[],int,int)",189,243,"/**
* Decompresses gzip data into a byte array.
* @param b target buffer for decompressed data
* @param off offset in target buffer to write decompressed data
* @param len maximum length of decompressed data to write
* @return number of available bytes written to target buffer
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,write,org.apache.hadoop.io.SequenceFile$Metadata:write(java.io.DataOutput),759,769,"/**
* Writes metadata entries to output stream.
* @throws IOException on write failure
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,writeImpl,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:writeImpl(java.io.DataOutput),205,215,"/**
* Writes user profile data to output stream.
* @param out DataOutput stream to write to
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,write,org.apache.hadoop.security.token.Token:write(java.io.DataOutput),323,331,"/**
* Writes user credentials and service details to output stream.
* @throws IOException if write operation fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,storeDelegationKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),333,345,"/**
* Stores a DelegationKey instance, persisting it to SQL database and local cache.
* @param key the DelegationKey object to be stored
*/","* Persists a DelegationKey into the SQL database. The delegation keyId
   * is expected to be unique and any duplicate key attempts will result
   * in an IOException.
   * @param key DelegationKey to persist into the SQL database.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,updateDelegationKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),351,363,"/**
* Updates delegation key by writing it to output stream and persisting to database and cache.
* @param key DelegationKey object to be updated
*/","* Updates an existing DelegationKey in the SQL database.
   * @param key Updated DelegationKey.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,addOrUpdateDelegationKey,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean)",691,725,"/**
* Adds or updates a delegation key in ZooKeeper.
* @param key DelegationKey object to store
* @param isUpdate whether this is an update operation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VIntWritable.java,readFields,org.apache.hadoop.io.VIntWritable:readFields(java.io.DataInput),49,52,"/**
* Reads VInt value from input stream. 
* @throws IOException if reading fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readStringSafely,"org.apache.hadoop.io.WritableUtils:readStringSafely(java.io.DataInput,int)",484,497,"/**
* Safely reads a string from the input stream, enforcing maximum length.
* @param in DataInput to read from
* @param maxLength Maximum allowed string length
* @return String value or null if invalid length
*/","* Read a string, but check it for sanity. The format consists of a vint
   * followed by the given number of bytes.
   * @param in the stream to read from
   * @param maxLength the largest acceptable length of the encoded string
   * @return the bytes as a string
   * @throws IOException if reading from the DataInput fails
   * @throws IllegalArgumentException if the encoded byte size for string 
             is negative or larger than maxSize. Only the vint is read.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readFields,org.apache.hadoop.io.Text:readFields(java.io.DataInput),346,350,"/**
* Reads known-length fields from input stream.
* @throws IOException if reading fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readFields,"org.apache.hadoop.io.Text:readFields(java.io.DataInput,int)",352,362,"/**
* Reads fields from DataInput with known length.
* @param in input stream
* @param maxLength maximum allowed length
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,skip,org.apache.hadoop.io.Text:skip(java.io.DataInput),369,372,"/**
* Skips data of specified length from input stream.
* @param in DataInput stream to read from
*/","* Skips over one Text in the input.
   * @param in input in.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readBuffer,"org.apache.hadoop.io.SequenceFile$Reader:readBuffer(org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.compress.CompressionInputStream)",2274,2291,"/**
* Reads compressed data into a temporary buffer and resets the compression filter.
* @param buffer DataInputBuffer to receive decompressed data
* @param filter CompressionInputStream to be reset
*/",Read a compressed buffer,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,readFields,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:readFields(java.io.DataInput),749,758,"/**
* Reads user profile fields from the input stream.
* @param in DataInput stream containing user data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readString,"org.apache.hadoop.io.Text:readString(java.io.DataInput,int)",566,572,"/**
* Reads a string from the given DataInput with optional max length constraint.
* @param in input stream
* @param maxLength maximum allowed string length
* @return decoded string or null if read fails
*/","* @return Read a UTF8 encoded string with a maximum size.
   * @param in input datainput.
   * @param maxLength input maxLength.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,readFields,org.apache.hadoop.security.token.delegation.DelegationKey:readFields(java.io.DataInput),108,119,"/**
* Reads user data fields from input stream.
* @throws IOException on read errors
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,writeString,"org.apache.hadoop.io.file.tfile.Utils:writeString(java.io.DataOutput,java.lang.String)",256,266,"/**
* Writes a string to the output stream.
* @param out DataOutput stream
* @param s String data to be written (null indicates end of string)
*/","* Write a String as a VInt n, followed by n Bytes as in Text format.
   * 
   * @param out out.
   * @param s s.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,getCredentialEntry,org.apache.hadoop.security.alias.UserProvider:getCredentialEntry(java.lang.String),54,62,"/**
* Retrieves credential entry by alias.
* @param alias unique identifier for the credential
* @return CredentialEntry object or null if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,createCredentialEntry,"org.apache.hadoop.security.alias.UserProvider:createCredentialEntry(java.lang.String,char[])",64,75,"/**
* Creates a new CredentialEntry with the given name and credential.
* @param name unique identifier for the credential
* @param credential sensitive data to be stored
* @return newly created CredentialEntry object
* @throws IOException if credential already exists
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,deleteCredentialEntry,org.apache.hadoop.security.alias.UserProvider:deleteCredentialEntry(java.lang.String),77,87,"/**
* Removes a credential entry by its secret key.
* @param name unique credential identifier
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,buildTokenService,org.apache.hadoop.security.SecurityUtil:buildTokenService(java.net.InetSocketAddress),474,487,"/**
* Builds a token service string from an InetSocketAddress.
* @param addr socket address to extract host and port
* @return formatted token service string or null if invalid
*/","* Construct the service key for a token
   * @param addr InetSocketAddress of remote connection with a token
   * @return ""ip:port"" or ""host:port"" depending on the value of
   *          hadoop.security.token.service.use_ip",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.UserProvider:getKeyVersion(java.lang.String),58,66,"/**
* Retrieves a KeyVersion object by its name.
* @param versionName unique key version identifier
* @return KeyVersion object or null if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getMetadata,org.apache.hadoop.crypto.key.UserProvider:getMetadata(java.lang.String),68,80,"/**
* Retrieves metadata by name from cache or computes it using secret key.
* @param name unique metadata identifier
* @return Metadata object or null if not found or computed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,createKey,"org.apache.hadoop.crypto.key.UserProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",82,100,"/**
* Creates a new key with the given name and material.
* @param name unique key identifier
* @param material raw key bytes
* @param options key creation options (cipher, length, description)
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getDtService,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDtService(java.net.URI),430,441,"/**
* Constructs a Text representation of the DT Service URI.
* @param uri the DT Service URI
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,tokenFromProto,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:tokenFromProto(org.apache.hadoop.security.proto.SecurityProtos$TokenProto),123,131,"/**
* Creates a Token object from the given proto.
* @param tokenProto TokenProto object
* @return Token<? extends TokenIdentifier> object or null if invalid
*/","* Create a hadoop token from a protobuf token.
   * @param tokenProto token
   * @return a new token",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,find,org.apache.hadoop.io.Text:find(java.lang.String),171,173,"/**
* Finds index of specified string in array.
* @param what target string to search for
* @return index of found string or -1 if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeEnum,"org.apache.hadoop.io.WritableUtils:writeEnum(java.io.DataOutput,java.lang.Enum)",432,435,"/**
* Writes an enumeration value to the output stream.
* @param out DataOutput stream
* @param enumVal enumeration value to serialize
*/","* writes String value of enum to DataOutput. 
   * @param out Dataoutput stream
   * @param enumVal enum value
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,write,"org.apache.hadoop.fs.permission.PermissionStatus:write(java.io.DataOutput,java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",128,135,"/**
* Writes user and group information to the output stream.
* @param out DataOutput stream
* @param username user identifier string
* @param groupname group identifier string
* @param permission FsPermission object representing file permissions
*/","* Serialize a {@link PermissionStatus} from its base components.
   * @param out out.
   * @param username username.
   * @param groupname groupname.
   * @param permission FsPermission.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text:<init>(byte[]),112,114,"/**
 * Creates a new Text instance from UTF-8 encoded byte array.","* Construct from a byte array.
   *
   * @param utf8 input utf8.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text:<init>(org.apache.hadoop.io.Text),103,105,"/**
 * Constructs a new Text instance from UTF-8 encoded string.","* Construct from another text.
   * @param utf8 input utf8.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readLine,"org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text,int,int)",180,187,"/**
* Reads a line from the input Text stream.
* @param str input Text stream
* @param maxLineLength maximum allowed line length
* @param maxBytesToConsume maximum bytes to consume from the stream
* @return the length of the read line, or -1 if end-of-stream reached
*/","* Read one line from the InputStream into the given Text.
   *
   * @param str the object to store the given line (without newline)
   * @param maxLineLength the maximum number of bytes to store into str;
   *  the rest of the line is silently discarded.
   * @param maxBytesToConsume the maximum number of bytes to consume
   *  in this call.  This is only a hint, because if the line cross
   *  this threshold, we allow it to happen.  It can overshoot
   *  potentially by as much as one buffer length.
   *
   * @return the number of bytes read including the (longest) newline
   * found.
   *
   * @throws IOException if the underlying stream throws",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,getTextLength,org.apache.hadoop.io.Text:getTextLength(),146,151,"/**
* Returns the length of the text representation.
* @return Text length in characters
*/","* @return Returns the length of this text. The length is equal to the number of
   * Unicode code units in the text.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,toString,org.apache.hadoop.io.SequenceFile$Metadata:toString(),828,840,"/**
* Returns a human-readable string representation of metadata.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,setRenewer,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setRenewer(org.apache.hadoop.io.Text),105,116,"/**
* Sets the user renewer, handling null or empty input and converting to short Kerberos name.
* @param renewer user ID as Text object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,getAliases,org.apache.hadoop.security.alias.UserProvider:getAliases(),111,119,"/**
* Retrieves a list of secret key aliases from the credentials store.
*@return List of string aliases or empty list if none exist
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getCanonicalServiceName,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getCanonicalServiceName(),1020,1023,"/**
* Returns the canonical service name as a string.
* @return The canonical service name.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getCanonicalServiceName,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCanonicalServiceName(),246,249,"/**
* Returns the canonical service name as a string.
* @return Canonical service name representation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,<init>,org.apache.hadoop.io.SortedMapWritable:<init>(org.apache.hadoop.io.SortedMapWritable),55,58,"/**
* Copies contents from another SortedMapWritable instance.
* @param other map to copy from
*/","* Copy constructor.
   * 
   * @param other the map to copy from",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,<init>,org.apache.hadoop.io.MapWritable:<init>(org.apache.hadoop.io.MapWritable),53,56,"/**
* Copies properties from another MapWritable instance.
* @param other source MapWritable to copy from
*/","* Copy constructor.
   * 
   * @param other the map to copy from",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,compression,org.apache.hadoop.io.MapFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType),297,300,"/**
* Sets compression option for writing to a SequenceFile.
* @param type Compression type (e.g. NONE, RECORD, BLOCK)
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/JavaSerializationComparator.java,<init>,org.apache.hadoop.io.serializer.JavaSerializationComparator:<init>(),42,45,"/**
* Initializes private comparator with custom deserializer. 
* @throws IOException if serialization fails.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryUpToMaximumTimeWithFixedSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumTimeWithFixedSleep(long,long,java.util.concurrent.TimeUnit)",114,116,"/**
* Creates a retry policy that attempts retries up to the specified maximum time with fixed sleep intervals.
* @param maxTime maximum allowed execution time
* @param sleepTime interval between retries
* @param timeUnit unit of time for maxTime and sleepTime (e.g. seconds, milliseconds)","* <p>
   * Keep trying for a maximum time, waiting a fixed time between attempts,
   * and then fail by re-throwing the exception.
   * </p>
   *
   * @param timeUnit timeUnit.
   * @param sleepTime sleepTime.
   * @param maxTime maxTime.
   * @return RetryPolicy.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,run,org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable:run(),977,1053,"/**
* Runs the Kerberos renewal loop with exponential back-off.
* @throws InterruptedException if interrupted
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,"org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int)",205,208,"/**
* Returns a retry policy that fails over to the fallback policy after network exceptions.
* @param fallbackPolicy default retry policy
* @param maxFailovers maximum number of failovers allowed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,newCall,"org.apache.hadoop.io.retry.RetryInvocationHandler:newCall(java.lang.reflect.Method,java.lang.Object[],boolean,int)",349,356,"/**
* Creates a new Call instance based on asynchronous mode.
* @param method the invoked method
* @param args arguments passed to the method
* @param isRpc whether it's an RPC call
* @return a Call object or null if not applicable
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,<init>,"org.apache.hadoop.util.InstrumentedWriteLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long,org.apache.hadoop.util.Timer)",50,57,"/**
* Initializes InstrumentedWriteLock with specified parameters.
* @param name identifier for logging purposes
* @param logger instance of the Logger to use
* @param readWriteLock shared ReentrantReadWriteLock instance
* @param minLoggingGapMs minimum gap between writes in milliseconds
* @param lockWarningThresholdMs threshold for lock warnings in milliseconds
* @param clock Timer instance for timing-related operations
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,<init>,"org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.Lock,long,long)",81,85,"/**
* Constructs an instrumented lock instance.
* @param name lock identifier
* @param logger logging utility for lock events
* @param lock underlying synchronization lock
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,<init>,"org.apache.hadoop.util.InstrumentedReadLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long,org.apache.hadoop.util.Timer)",56,63,"/**
* Initializes an instrumented read lock instance.
* @param name lock identifier
* @param logger logging utility
* @param readWriteLock shared lock object
* @param minLoggingGapMs minimum gap between log messages (ms)
* @param lockWarningThresholdMs threshold for displaying lock warnings (ms)
* @param clock timer utility
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,tryLock,org.apache.hadoop.util.InstrumentedLock:tryLock(),117,124,"/**
* Tries to acquire an exclusive lock; returns true on success, false otherwise.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",58,65,"/**
* Creates a proxy instance for the given interface with failover and retry capabilities.
* @param iface target interface
* @param proxyProvider provider of failover proxies
* @param retryPolicy policy for retrying failed operations
*/","* Create a proxy for an interface of implementations of that interface using
   * the given {@link FailoverProxyProvider} and the same retry policy for each
   * method in the interface.
   * 
   * @param iface the interface that the retry will implement
   * @param proxyProvider provides implementation instances whose methods should be retried
   * @param retryPolicy the policy for retrying or failing over method call failures
   * @param <T> T.
   * @return the retry proxy",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/LossyRetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.LossyRetryInvocationHandler:<init>(int,org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",35,39,"/**
* Constructs a lossy retry invocation handler with specified drop count.
* @param numToDrop number of failed invocations to ignore before retrying
* @param proxyProvider provider for failover proxies
* @param retryPolicy policy for retrying failed invocations
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,java.lang.Object,java.util.Map)",79,85,"/**
* Creates a proxy instance for the given interface and implementation.
* @param iface interface to proxy
* @param implementation concrete implementation of the interface
*/","* Create a proxy for an interface of an implementation class
   * using the a set of retry policies specified by method name.
   * If no retry policy is defined for a method then a default of
   * {@link RetryPolicies#TRY_ONCE_THEN_FAIL} is used.
   * 
   * @param iface the interface that the retry will implement
   * @param <T> T.
   * @param implementation the instance whose methods should be retried
   * @param methodNameToPolicyMap a map of method names to retry policies
   * @return the retry proxy",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,processRetryInfo,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processRetryInfo(),151,159,"/**
* Processes retry information and updates counters.
* @param none
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,handleException,"org.apache.hadoop.io.retry.RetryInvocationHandler:handleException(java.lang.reflect.Method,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,long,java.lang.Exception)",376,396,"/**
* Handles exceptions by determining if a retry is warranted.
* @param method the failed method invocation
* @param callId unique call identifier
* @param policy RetryPolicy instance
* @param counters execution metrics
* @param expectFailoverCount expected failover count
* @return RetryInfo object describing retry outcome
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,write,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(int),327,340,"/**
* Writes a single byte to all output streams in the set.
* @throws IOException if any write operation fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,write,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(byte[],int,int)",348,361,"/**
* Writes bytes to multiple output streams concurrently.
* @param bytes data to be written
* @param offset starting position in the byte array
* @param len number of bytes to write
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,flush,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:flush(),363,376,"/**
* Flushes all output streams associated with the operation set.
* @throws IOException if any stream flush fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProxyCombiner.java,close,org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:close(),133,149,"/**
* Closes the proxies and throws a MultipleIOException if any fail.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Writer:<init>(org.apache.hadoop.fs.FSDataOutputStream,java.lang.String,org.apache.hadoop.conf.Configuration)",285,297,"/**
* Initializes Writer object with output stream, compression name, and configuration.
* @param fout FSDataOutputStream for writing data
* @param compressionName type of compression used
* @param conf Hadoop Configuration object
*/","* Constructor
     * 
     * @param fout
     *          FS output stream.
     * @param compressionName
     *          Name of the compression algorithm, which will be used for all
     *          data blocks.
     * @throws IOException
     * @see Compression#getSupportedAlgorithms",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:<init>(java.io.DataInput),809,822,"/**
* Reads and parses a MetaIndexEntry from the given input stream.
* @param in input stream containing the index entry
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:<init>(java.io.DataInput),2060,2068,"/**
* Parses TFile metadata from input stream.
* @param in DataInput containing TFile metadata
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$DataIndex:<init>(java.io.DataInput),864,875,"/**
* Initializes DataIndex object from input stream.
* @param in DataInput stream containing data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$TFileIndex:<init>(int,java.io.DataInput,org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator)",2145,2179,"/**
* Creates a TFileIndex from the input stream and comparator.
* @param entryCount number of index entries
* @param in DataInput to read from
* @param comparator BytesComparator for key comparison
*/","* For reading from file.
     * 
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,checkEOF,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:checkEOF(),119,126,"/**
* Checks for end-of-file condition.
* @return true if at EOF, false otherwise
*/","* Check whether we reach the end of the stream.
     * 
     * @return false if the chunk encoded stream has more data to read (in which
     *         case available() will be greater than 0); true otherwise.
     * @throws java.io.IOException
     *           on I/O errors.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,flushBuffer,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:flushBuffer(),296,301,"/**
* Flushes the buffer by writing any pending data and resetting the counter.
*/","* Flush the internal buffer.
     * 
     * Is this the last call to flushBuffer?
     * 
     * @throws java.io.IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,close,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:close(),338,348,"/**
* Closes the output stream and writes any pending data.
* @throws IOException if an I/O error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,"org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(byte[],int,int)",316,330,"/**
* Writes input data to buffer or directly if buffer is full.
* @param b input byte array
* @param off offset in the array
* @param len length of data to write
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,write,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:write(java.io.DataOutput),2272,2290,"/**
* Writes TFileIndex entries to DataOutput stream.
* @throws IOException if write operation fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(org.apache.hadoop.io.file.tfile.RawComparable),1957,1961,"/**
* Compares this RawComparable with the given key.
* @param key RawComparable object to compare with
*/","* Compare an entry with a RawComparable object. This is useful when
         * Entries are stored in a collection, and we want to compare a user
         * supplied key.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Writer$KeyRegister:close(),432,473,"/**
* Commits the current key and updates internal state.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,<init>,org.apache.hadoop.io.UTF8:<init>(java.lang.String),70,72,"/**
* Constructs a new UTF8 instance from the specified string.
* @param string input string to be converted
*/","* Construct from a given string.
   * @param string input string.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,getBytes,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getBytes(),80,83,"/**
* Converts this object to a byte array.
* @return byte representation of the object or null if not applicable
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(java.lang.String),186,188,"/**
* Generates an MD5 hash from the given string.
* @param string input string to be hashed
*/","* Construct a hash value for a String.
   * @param string string.
   * @return MD5Hash.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sync,org.apache.hadoop.io.SequenceFile$BlockCompressWriter:sync(),1638,1665,"/**
* Synchronizes buffered records and writes them to the output stream.
* @throws IOException if an I/O error occurs
*/",Compress and flush contents to dfs,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/TokenIdentifier.java,getTrackingId,org.apache.hadoop.security.token.TokenIdentifier:getTrackingId(),78,83,"/**
* Generates and returns MD5 hash of bytes as tracking ID.
* @return Unique tracking identifier as a hexadecimal string
*/","* Returns a tracking identifier that can be used to associate usages of a
   * token across multiple client sessions.
   *
   * Currently, this function just returns an MD5 of {{@link #getBytes()}.
   *
   * @return tracking identifier",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,encodeToUrlString,org.apache.hadoop.security.token.Token:encodeToUrlString(),373,375,"/**
* Encodes current object to a URL-safe string.
* @return encoded string representation or null if failed
*/","* Encode this token as a url safe string.
   * @return the encoded string
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,cloneWritableInto,"org.apache.hadoop.util.ReflectionUtils:cloneWritableInto(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",363,371,"/**
* Copies data from one Writable object to another.
* @param dst destination writable object
* @param src source writable object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,hashCode,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:hashCode(),162,166,"/**
* Returns the hash code as implemented by superclass.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,add,org.apache.hadoop.net.NetworkTopology:add(org.apache.hadoop.net.Node),133,169,"/**
* Adds a new Node to the topology, enforcing leaf node depth consistency.
* @param node Node object to be added
*/","Add a leaf node
   * Update node counter &amp; rack counter if necessary
   * @param node node to be added; can be null
   * @exception IllegalArgumentException if add a node to a leave 
                                         or node to be added is not a leaf",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,"org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.lang.String,java.util.Collection)",497,554,"/**
* Chooses a random datanode within a scope, excluding nodes from another scope if specified.
* @param scope current scope
* @param excludedScope scope to exclude nodes from (null for no exclusion)
* @param excludedNodes collection of nodes to exclude
* @return chosen Node object or null if not found",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,remove,org.apache.hadoop.net.NetworkTopology:remove(org.apache.hadoop.net.Node),221,241,"/**
* Removes a node from the network topology.
* @param node Node to be removed
*/","Remove a node
   * Update node counter and rack counter if necessary
   * @param node node to be removed; can be null",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,decommissionNode,org.apache.hadoop.net.NetworkTopology:decommissionNode(org.apache.hadoop.net.Node),1061,1076,"/**
* Decommissions a node, adding it to the list of nodes to be removed.
* @param node Node object to be decommissioned
*/","* Update empty rack number when remove a node like decommission.
   * @param node node to be added; can be null",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistance,"org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer)",912,915,"/**
* Sorts array of nodes based on distance from the given reader node.
* @param reader reference node for distance calculation
* @param nodes array of Node objects to sort
*/","* Sort nodes array by network distance to <i>reader</i> with secondary sort.
   * <p>
   * In a three-level topology, a node can be either local, on the same rack,
   * or on a different rack from the reader. Sorting the nodes based on network
   * distance from the reader reduces network traffic and improves
   * performance.
   * </p>
   * As an additional twist, we also randomize the nodes at each network
   * distance. This helps with load balancing when there is data skew.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array
   * @param secondarySort a secondary sorting strategy which can inject into
   *     that point from outside to help sort the same distance.
   * @param <T> Generics Type T",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistanceUsingNetworkLocation,"org.apache.hadoop.net.NetworkTopology:sortByDistanceUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer)",953,956,"/**
* Sorts an array of nodes by distance using network location.
* @param reader node used to calculate distances
* @param nodes array of nodes to sort
* @param activeLen number of active nodes
* @param secondarySort callback for secondary sorting (if needed)
*/","* Sort nodes array by network distance to <i>reader</i>.
   * <p> using network location. This is used when the reader
   * is not a datanode. Sorting the nodes based on network distance
   * from the reader reduces network traffic and improves
   * performance.
   * </p>
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array
   * @param secondarySort a secondary sorting strategy which can inject into
   *     that point from outside to help sort the same distance.
   * @param <T> Generics Type T.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,"org.apache.hadoop.net.SocketInputStream:<init>(java.nio.channels.ReadableByteChannel,long)",72,76,"/**
* Initializes SocketInputStream with a readable byte channel and timeout.
* @param channel readable byte channel for socket I/O
* @param timeout connection timeout in milliseconds
*/","* Create a new input stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @param channel 
   *        Channel for reading, should also be a {@link SelectableChannel}.
   *        The channel will be configured to be non-blocking.
   * @param timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,<init>,"org.apache.hadoop.net.SocketOutputStream:<init>(java.nio.channels.WritableByteChannel,long)",77,81,"/**
* Initializes a SocketOutputStream with a WritableByteChannel and timeout.
* @param channel the underlying writeable channel
* @param timeout connection timeout in milliseconds
*/","* Create a new ouput stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @param channel 
   *        Channel for writing, should also be a {@link SelectableChannel}.  
   *        The channel will be configured to be non-blocking.
   * @param timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,createSocket,org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:createSocket(),184,196,"/**
* Creates a socket instance with specified server host and port.
* @throws IOException if socket creation fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getRpcResponse,"org.apache.hadoop.ipc.Client:getRpcResponse(org.apache.hadoop.ipc.Client$Call,org.apache.hadoop.ipc.Client$Connection,long,java.util.concurrent.TimeUnit)",1566,1598,"/**
* Retrieves RPC response from a Call object, handling timeouts and exceptions.
* @param call the Call object containing the RPC request
* @param connection the Connection used for the RPC call
* @param timeout maximum time to wait for a response
* @param unit TimeUnit for the timeout; e.g., SECONDS or MILLISECONDS
* @return Writable RPC response, or null if timed out
*/","@return the rpc response or, in case of timeout, null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping:<init>(),87,89,"/**
* Initializes ScriptBasedMapping with default configuration.
* @param mapping Raw script-based mapping object. 
*/","* Create an instance with the default configuration.
   * <p>
   * Calling {@link #setConf(Configuration)} will trigger a
   * re-evaluation of the configuration settings and so be used to
   * set up the mapping script.
   *",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,<init>,org.apache.hadoop.net.ScriptBasedMappingWithDependency:<init>(),59,61,"/**
 * Initializes a ScriptBasedMappingWithDependency instance.
 * @param no parameters are used by this constructor
 */","* Create an instance with the default configuration.
   * <p>
   * Calling {@link #setConf(Configuration)} will trigger a
   * re-evaluation of the configuration settings and so be used to
   * set up the mapping script.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,newInnerNode,org.apache.hadoop.net.InnerNodeImpl$Factory:newInnerNode(java.lang.String),32,35,"/**
* Creates a new InnerNodeImpl instance with the specified path.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,<init>,org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:<init>(java.lang.String),306,308,"/**
* Constructs an InnerNode with a NodeGroup at the specified path.
* @param path unique node identifier
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:<init>(org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode),127,129,"/**
 * Constructs an instance of MRNflyNode from an existing NflyNode.
 * @param n existing NflyNode object to copy properties from
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,createParentNode,org.apache.hadoop.net.InnerNodeImpl:createParentNode(java.lang.String),184,187,"/**
* Creates an inner node with the specified parent name and increments level.
* @param parentName name of the parent node
*/","* Creates a parent node to be added to the list of children.
   * Creates a node using the InnerNode four argument constructor specifying
   * the name, location, parent, and level of this node.
   *
   * <p>To be overridden in subclasses for specific InnerNode implementations,
   * as alternative to overriding the full {@link #add(Node)} method.
   *
   * @param parentName The name of the parent node
   * @return A new inner node
   * @see InnerNodeImpl(String, String, InnerNode, int)",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,add,"org.apache.hadoop.net.unix.DomainSocketWatcher:add(org.apache.hadoop.net.unix.DomainSocket,org.apache.hadoop.net.unix.DomainSocketWatcher$Handler)",304,332,"/**
* Adds a DomainSocket to the queue for processing.
* @param sock DomainSocket instance to be added
* @param handler Handler to invoke on socket completion
*/","* Add a socket.
   *
   * @param sock     The socket to add.  It is an error to re-add a socket that
   *                   we are already watching.
   * @param handler  The handler to associate with this socket.  This may be
   *                   called any time after this function is called.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,remove,org.apache.hadoop.net.unix.DomainSocketWatcher:remove(org.apache.hadoop.net.unix.DomainSocket),339,354,"/**
* Removes a DomainSocket from the domain socket pool.
* @param sock DomainSocket object to be removed
*/","* Remove a socket.  Its handler will be called.
   *
   * @param sock     The socket to remove.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,<init>,"org.apache.hadoop.net.unix.DomainSocketWatcher:<init>(int,java.lang.String)",241,259,"/**
* Initializes DomainSocketWatcher with interrupt check period and source.
* @param interruptCheckPeriodMs interval to check for interruptions
* @param src source identifier for thread naming
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,select,"org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:select(java.nio.channels.SelectableChannel,int,long)",321,376,"/**
* Selects IO operations on a channel with specified options and timeout.
* @param channel channel to select IO on
* @param ops IO options to select for
* @param timeout total time in milliseconds to wait for IO
* @return number of ready channels or 0 if timed out
*/","* Waits on the channel with the given timeout using one of the 
     * cached selectors. It also removes any cached selectors that are
     * idle for a few seconds.
     * 
     * @param channel
     * @param ops
     * @param timeout
     * @return
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultIP,org.apache.hadoop.net.DNS:getDefaultIP(java.lang.String),224,228,"/**
* Returns the default IP address from a given network interface.
* @param strInterface name of the network interface
*/","* Returns the first available IP address associated with the provided
   * network interface or the local host IP if ""default"" is given.
   *
   * @param strInterface
   *            The name of the network interface or subinterface to query
   *             (e.g. eth0 or eth0:0) or the string ""default""
   * @return The IP address in text form, the local host IP is returned
   *         if the interface name ""default"" is specified
   * @throws UnknownHostException
   *             If the given interface is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getHosts,org.apache.hadoop.net.DNS:getHosts(java.lang.String),340,343,"/**
* Retrieves host names associated with a given network interface.
* @param strInterface name of the network interface
*/","* Returns all the host names associated by the default nameserver with the
   * address bound to the specified network interface
   * 
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0)
   * @return The list of host names associated with IPs bound to the network
   *         interface
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface
   *",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultHost,"org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String,java.lang.String,boolean)",360,374,"/**
* Retrieves the default host based on interface and DNS server.
* @param strInterface network interface or ""default"" for cached value
* @param nameserver DNS server address or ""default"" to disable
* @param tryfallbackResolution whether to attempt fallback resolution
* @return the first resolved host or null if not found
*/","* Returns the default (first) host name associated by the provided
   * nameserver with the address bound to the specified network interface
   * 
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0)
   * @param nameserver
   *            The DNS host name
   * @param tryfallbackResolution
   *            Input tryfallbackResolution.
   * @return The default host names associated with IPs bound to the network
   *         interface
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream),134,136,"/**
* Prints application usage to the specified output stream.
* @param pStr output stream to write to
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkParameterValidity,"org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[],java.util.Map)",363,385,"/**
* Validates input parameters and usage.
* @param argv array of command-line arguments
* @param helpEntries map of available commands and their descriptions
* @return true if valid, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,help,"org.apache.hadoop.ha.HAAdmin:help(java.lang.String[],java.util.Map)",512,537,"/**
* Prints command usage or detailed help for a specific command.
* @param argv array of command-line arguments
* @return 0 on success, -1 on error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addContext,"org.apache.hadoop.http.HttpServer2:addContext(org.eclipse.jetty.servlet.ServletContextHandler,boolean)",997,1001,"/**
* Adds context to handler collection and sets caching behavior.
* @param ctxt ServletContextHandler instance
* @param isFiltered true for filtered context, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getPlugin,org.apache.hadoop.metrics2.impl.MetricsConfig:getPlugin(java.lang.String),202,216,"/**
* Retrieves a plugin instance by name.
* @param name unique plugin identifier
* @return Plugin instance of type T or null if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,loadFirst,"org.apache.hadoop.metrics2.impl.MetricsConfig:loadFirst(java.lang.String,java.lang.String[])",113,142,"/**
* Loads the first found metrics configuration from a list of files.
* @param prefix configuration prefix
* @param fileNames comma-separated file names to load
* @return loaded MetricsConfig object or default config if not found
*/","* Load configuration from a list of files until the first successful load
   * @param conf  the configuration object
   * @param files the list of filenames to try
   * @return  the configuration object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,toString,org.apache.hadoop.metrics2.impl.MetricsConfig:toString(),282,285,"/**
* Returns a string representation of this object.
* @return human-readable description of the current object state.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,putMetrics,org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),108,193,"/**
* Puts a MetricsRecord into Ganglia, handling both dense and sparse publish modes.
* @param record the MetricsRecord to be published
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,putMetrics,org.apache.hadoop.metrics2.sink.GraphiteSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),70,108,"/**
* Sends metrics data to Graphite.
* @param record MetricsRecord object containing data to be sent
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/PrometheusServlet.java,doGet,"org.apache.hadoop.http.PrometheusServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",40,46,"/**
* Publishes metrics to Prometheus sink.
* @throws ServletException if an error occurs
* @throws IOException on write operation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,publishMetricsFromQueue,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:publishMetricsFromQueue(),128,166,"/**
* Periodically publishes metrics from the queue, handling retries and errors.
* @throws InterruptedException if interrupted while waiting for retry
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,incrCacheClearedCounter,org.apache.hadoop.ipc.RetryCache:incrCacheClearedCounter(),221,223,"/**
* Increments cache cleared counter.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,requeueCall,org.apache.hadoop.ipc.Server$Handler:requeueCall(org.apache.hadoop.ipc.Server$Call),3232,3240,"/**
* Requeues a call with the specified parameters.
* @param call Call object to requeue
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsCollectorImpl.java,getRecords,org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:getRecords(),57,66,"/**
* Retrieves a list of metrics records.
* @return List of MetricsRecordImpl objects
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,int)",62,65,"/**
* Records integer metric value with given MetricsInfo.
* @param info metrics information object
* @param value integer value to record
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,long)",67,70,"/**
* Records metric value as Long attribute.
* @param info MetricsInfo object
* @param value numeric value to record
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,float)",72,75,"/**
* Records a floating-point metric value.
* @param info MetricsInfo object containing metric details
* @param value the numeric value to record
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,double)",77,80,"/**
* Records a gauge metric with specified value.
* @param info MetricsInfo object
* @param value double value to record
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,counter,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:counter(org.apache.hadoop.metrics2.MetricsInfo,int)",82,85,"/**
* Increments or decrements metrics counter.
* @param info MetricsInfo object containing metric details
* @param value integer value to increment/decrement by
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,counter,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:counter(org.apache.hadoop.metrics2.MetricsInfo,long)",87,90,"/**
* Updates counter metrics with provided value.
* @param info MetricsInfo object containing metric details
* @param value numerical value to update the counter with
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,updateInfoCache,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateInfoCache(java.lang.Iterable),243,248,"/**
* Updates the info cache with the provided metrics records.
* @param lastRecs iterable of MetricsRecordImpl objects
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,newMBeanName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newMBeanName(java.lang.String),108,111,"/**
 * Creates a new MBean object with the given name.
 * @param name unique name of the MBean
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,sourceName,"org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:sourceName(java.lang.String,boolean)",123,126,"/**
* Generates a new source name based on the provided name and duplication policy.
* @param name the original source name
* @param dupOK whether to allow duplicate names
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,load,org.apache.hadoop.security.Groups$GroupCacheLoader:load(java.lang.String),342,370,"/**
* Loads user groups by ID.
* @param user unique user identifier
* @return Set of String group IDs or throws exception if not found
*/","* This method will block if a cache entry doesn't exist, and
     * any subsequent requests for the same user will wait on this
     * request to return. If a user already exists in the cache,
     * and when the key expires, the first call to reload the key
     * will block, but subsequent requests will return the old
     * value until the blocking thread returns.
     * If reloadGroupsInBackground is true, then the thread that
     * needs to refresh an expired key will not block either. Instead
     * it will return the old cache value and schedule a background
     * refresh
     * @param user key of cache
     * @return List of groups belonging to user
     * @throws IOException to prevent caching negative entries",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,shutdownSingleton,org.apache.hadoop.metrics2.source.JvmMetrics:shutdownSingleton(),139,141,"/**
 * Shuts down the singleton instance.
 */","* Shutdown the JvmMetrics singleton. This is not necessary if the JVM itself
   * is shutdown, but may be necessary for scenarios where JvmMetrics instance
   * needs to be re-created while the JVM is still around. One such scenario
   * is unit-testing.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reattach,org.apache.hadoop.security.UserGroupInformation$UgiMetrics:reattach(),151,153,"/**
* Reinitializes and attaches new metrics instance.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,stop,org.apache.hadoop.ipc.Server:stop(),3696,3719,"/**
* Stops the server, interrupting all listeners and handlers.
* @throws InterruptedException if interrupted while stopping
*/",Stops the service.  No new calls will be handled after this is called.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,stopMBeans,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:stopMBeans(),226,231,"/**
* Unregisters and cleans up MBean registration.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,unregisterSource,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:unregisterSource(java.lang.String),896,902,"/**
* Unregisters DecayRpcSchedulerMetrics2 source for the given namespace and associated metrics bean.
* @param namespace unique identifier for the source to unregister
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2Metrics.java,create,"org.apache.hadoop.http.HttpServer2Metrics:create(org.eclipse.jetty.server.handler.StatisticsHandler,int)",151,159,"/**
* Creates and registers HTTP server metrics with the given handler and port.
* @param handler StatisticsHandler instance
* @param port unique port number for the HTTP server
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,stop,org.apache.hadoop.http.HttpServer2:stop(),1559,1609,"/**
* Stops the web app context and related resources.
* @throws Exception on any failure
*/","* stop the server.
   *
   * @throws Exception exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,getMetrics,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",569,581,"/**
* Collects and records metrics into the provided builder.
* @param builder MetricsCollector instance to populate
* @param all whether to include inactive sources/sinks in metrics
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,<init>,"org.apache.hadoop.metrics2.lib.MutableQuantiles:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",87,106,"/**
* Initializes a MutableQuantiles instance with specified name, description,
* sample and value names, and quantile interval.
* @param name unique identifier
* @param description descriptive text
* @param sampleName sample name
* @param valueName value name
* @param interval quantile calculation interval in seconds
*/","* Instantiates a new {@link MutableQuantiles} for a metric that rolls itself
   * over on the specified time interval.
   * 
   * @param name
   *          of the metric
   * @param description
   *          long-form textual description of the metric
   * @param sampleName
   *          type of items in the stream (e.g., ""Ops"")
   * @param valueName
   *          type of the values
   * @param interval
   *          rollover interval (in seconds) of the estimator",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:<init>(),993,1000,"/**
* Initializes metrics for DelegationTokenSecretManager. 
* @param none 
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,<init>,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:<init>(org.apache.hadoop.ipc.RetryCache),42,48,"/**
* Initializes metrics for the given RetryCache instance.
* @param retryCache RetryCache object to initialize metrics for
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.reflect.Field)",129,131,"/**
* Retrieves metrics info based on annotation and field name.
* @param annotation Metric annotation
* @param fieldName Name of the field to retrieve metrics for
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.reflect.Method)",137,139,"/**
* Retrieves metrics information based on the given metric annotation and method.
* @param annotation Metric annotation object
* @param method Method object from which to extract name
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newStat,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newStat(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)",262,269,"/**
* Creates a new mutable stat with given details and stores it in the metrics map.
* @param name unique metric identifier
* @param desc metric description
* @param sampleName sampling name
* @param valueName value name
* @param extended whether to use extended mode
* @return MutableStat object or null if creation fails
*/","* Create a mutable metric with stats
   * @param name  of the metric
   * @param desc  metric description
   * @param sampleName  of the metric (e.g., ""Ops"")
   * @param valueName   of the metric (e.g., ""Time"" or ""Latency"")
   * @param extended    produce extended stat (stdev, min/max etc.) if true.
   * @return a new mutable stat metric object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,<init>,"org.apache.hadoop.metrics2.lib.MutableStat:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",94,97,"/**
* Constructs a new MutableStat instance with default flags.
* @param name name of the stat
* @param description description of the stat
* @param sampleName name of the sample associated with the stat
* @param valueName name of the value associated with the stat
*/","* Construct a snapshot stat metric with extended stat off by default
   * @param name        of the metric
   * @param description of the metric
   * @param sampleName  of the metric (e.g. ""Ops"")
   * @param valueName   of the metric (e.g. ""Time"", ""Latency"")",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRate.java,<init>,"org.apache.hadoop.metrics2.lib.MutableRate:<init>(java.lang.String,java.lang.String,boolean)",31,33,"/**
* Initializes a mutable rate object with given parameters.
* @param name unique identifier
* @param description descriptive text
* @param extended flag indicating extended properties
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getGcUsage,org.apache.hadoop.metrics2.source.JvmMetrics:getGcUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder),180,207,"/**
* Collects and records garbage collection metrics.
* @param rb MetricsRecordBuilder to populate with GC data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,setContext,org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:setContext(java.lang.String),147,150,"/**
* Sets context metadata in metrics record.
* @param value context string to be set
* @return this instance (for chaining)
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,setContext,org.apache.hadoop.metrics2.lib.MetricsRegistry:setContext(java.lang.String),380,382,"/**
* Sets context for metrics registry.
* @param name unique context identifier
* @return MetricsRegistry instance with updated context
*/","* Set the metrics context tag
   * @param name of the context
   * @return the registry itself as a convenience",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(java.lang.String,java.lang.String,java.lang.String,boolean)",403,406,"/**
* Creates a metrics registry entry with specified name, description, and value.
* @param name unique metric identifier
* @param description human-readable metric description
* @param value metric value to record
* @param override whether to override existing metric with same name
*/","* Add a tag to the metrics
   * @param name  of the tag
   * @param description of the tag
   * @param value of the tag
   * @param override  existing tag if true
   * @return the registry (for keep adding tags)",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",422,424,"/**
* Tags metrics with a given string value.
* @param info MetricsInfo object
* @param value string to be used as metric tag
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,add,"org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:add(java.lang.String,long)",100,114,"/**
* Adds elapsed time to user's thread-safe sample statistics.
* @param name unique user identifier
* @param elapsed elapsed time in milliseconds
*/","* Add a rate sample for a rate metric.
   * @param name of the rate metric
   * @param elapsed time",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,publishMetrics,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:publishMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,boolean)",435,449,"/**
* Publishes metrics to all registered sinks.
* @param buffer Metrics data to be published
* @param immediate Whether to use immediate publishing mode
*/","* Publish a metrics snapshot to all the sinks
   * @param buffer  the metrics snapshot to publish
   * @param immediate  indicates that we should publish metrics immediately
   *                   instead of using a separate thread.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,copyTo,org.apache.hadoop.metrics2.util.SampleStat:copyTo(org.apache.hadoop.metrics2.util.SampleStat),59,61,"/**
* Copies statistical data to another SampleStat object.
* @param other destination SampleStat instance
*/","* Copy the values to other (saves object creation and gc.)
   * @param other the destination to hold our values",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,<init>,"org.apache.hadoop.metrics2.lib.MethodMetric:<init>(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.MetricsInfo,org.apache.hadoop.metrics2.annotation.Metric$Type)",46,53,"/**
* Initializes a MethodMetric object for the given method and metric type.
* @param obj the object being measured
* @param method the method to be evaluated
* @param info metrics information
* @param type metric type (e.g. execution time, memory usage) 
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,toString,org.apache.hadoop.metrics2.lib.MutableStat:toString(),187,190,"/**
* Returns a string representation of the last statistical update.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,logSlowRpcCalls,"org.apache.hadoop.ipc.Server:logSlowRpcCalls(java.lang.String,org.apache.hadoop.ipc.Server$Call,org.apache.hadoop.ipc.ProcessingDetails)",593,615,"/**
* Logs slow RPC calls based on processing time and sample size.
* @param methodName name of the method
* @param call RPC call object
* @param details Processing details with timing information
*/","* Logs a Slow RPC Request.
   *
   * @param methodName - RPC Request method name
   * @param details - Processing Detail.
   *
   * If a request took significant more time than other requests,
   * and its processing time is at least `logSlowRPCThresholdMs` we consider that as a slow RPC.
   *
   * The definition rules for calculating whether the current request took too much time
   * compared to other requests are as follows:
   * 3 is a magic number that comes from 3 sigma deviation.
   * A very simple explanation can be found by searching for 68-95-99.7 rule.
   * We flag an RPC as slow RPC if and only if it falls above 99.7% of requests.
   * We start this logic only once we have enough sample size.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,toString,org.apache.hadoop.metrics2.util.SampleQuantiles:toString(),280,288,"/**
* Returns a human-readable string representation of this object.
* @return Stringified map of quantiles to sample counts, or ""[no samples]"" if none
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addTopNCallerSummary,org.apache.hadoop.ipc.DecayRpcScheduler:addTopNCallerSummary(org.apache.hadoop.metrics2.MetricsRecordBuilder),1079,1096,"/**
* Adds top N caller summary to metrics record.
* @param rb MetricsRecordBuilder instance
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java,cacheGroupsRefresh,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh(),78,83,"/**
* Refreshes cached groups by clearing and re-caching.
*/",* Refresh the netgroup cache,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroupsSet,org.apache.hadoop.security.UserGroupInformation$TestingGroups:getGroupsSet(java.lang.String),1585,1592,"/**
* Retrieves groups set for a given user, using cached mapping or underlying implementation.
* @param user unique user identifier
* @return Set of group names or empty set if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,endln,org.apache.hadoop.security.KDiag:endln(),875,878,"/**
* Prints separator line to console.
*/",* Print something at the end of a section,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,title,"org.apache.hadoop.security.KDiag:title(java.lang.String,java.lang.Object[])",886,891,"/**
* Formats and prints a centered title in console.
* @param format print format string
* @param args variable arguments for format string
*/","* Print a title entry.
   *
   * @param format format string
   * @param args any arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,fail,"org.apache.hadoop.security.KDiag:fail(java.lang.String,java.lang.String,java.lang.Object[])",942,946,"/**
* Throws a KerberosDiagsFailure exception with the specified details.
* @param category diagnostic category
* @param message failure message
* @param args optional arguments for the message
*/","* Format and raise a failure.
   *
   * @param category category for exception
   * @param message string formatting message
   * @param args any arguments for the formatting
   * @throws KerberosDiagsFailure containing the formatted text",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,loadFullMaps,org.apache.hadoop.security.ShellBasedIdMapping:loadFullMaps(),386,389,"/**
* Loads full maps of users and groups.
* Synchronized to ensure thread safety.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createRemoteUser,"org.apache.hadoop.security.UserGroupInformation:createRemoteUser(java.lang.String,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",1451,1462,"/**
* Creates a remote user with specified authentication method.
* @param user unique username
* @param authMethod authentication method (e.g. KERBEROS, SIMPLE)
* @return UserGroupInformation object representing the created user
*/","* Create a user from a login name. It is intended to be used for remote
   * users in RPC, since it won't have any credentials.
   * @param user the full user principal name, must not be empty or null
   * @param authMethod authMethod.
   * @return the UserGroupInformation for the remote user.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getDefault,org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:getDefault(),1052,1067,"/**
* Returns the default SSL socket factory instance.
* @return LdapSslSocketFactory object or null if initialization fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,retrievePassword,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),548,552,"/**
* Retrieves password from token.
* @param identifier unique TokenIdent object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,startThreads,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:startThreads(),169,177,"/**
* Starts the thread for removing expired tokens.
* @throws IOException if an I/O error occurs
*/","* should be called before this object is used.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,rollMasterKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:rollMasterKey(),463,476,"/**
* Updates master key by rolling the current key and updating expiration dates.
*/","* Update the current master key for generating delegation tokens 
   * It should be called only by tokenRemoverThread.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(),150,152,"/**
* Initializes an authenticated URL with no token.
* @param delegationToken unused parameter
* @param serviceName unused parameter
*/","* Creates an <code>DelegationTokenAuthenticatedURL</code>.
   * <p>
   * An instance of the default {@link DelegationTokenAuthenticator} will be
   * used.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator),160,163,"/**
 * Constructs an authenticated URL with a delegationToken authenticator.
 * @param authenticator DelegationTokenAuthenticator instance
 */","* Creates an <code>DelegationTokenAuthenticatedURL</code>.
   *
   * @param authenticator the {@link DelegationTokenAuthenticator} instance to
   * use, if <code>null</code> the default one will be used.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.authentication.client.ConnectionConfigurator),171,174,"/**
* Constructs an authenticated URL with delegated token using the given configurator.
* @param connConfigurator configuration for establishing connection
*/","* Creates an <code>DelegationTokenAuthenticatedURL</code> using the default
   * {@link DelegationTokenAuthenticator} class.
   *
   * @param connConfigurator a connection configurator.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,org.apache.hadoop.security.SaslInputStream:read(byte[]),225,228,"/**
* Reads data into byte array and returns length.","* Reads up to <code>b.length</code> bytes of data from this input stream into
   * an array of bytes.
   * <p>
   * The <code>read</code> method of <code>InputStream</code> calls the
   * <code>read</code> method of three arguments with the arguments
   * <code>b</code>, <code>0</code>, and <code>b.length</code>.
   * 
   * @param b
   *          the buffer into which the data is read.
   * @return the total number of bytes read into the buffer, or <code>-1</code>
   *         is there is no more data because the end of the stream has been
   *         reached.
   * @exception IOException
   *              if an I/O error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isFromKeytab,org.apache.hadoop.security.UserGroupInformation:isFromKeytab(),834,838,"/**
* Determines whether credentials are from a Keytab file.
* @return true if using Kerberos with Hadoop login and valid Keytab, false otherwise
*/","* Is this user logged in from a keytab file managed by the UGI?
   * @return true if the credentials are from a keytab file.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isFromTicket,org.apache.hadoop.security.UserGroupInformation:isFromTicket(),844,846,"/**
* Checks if credentials are from ticket-based authentication.
* @return true if credentials are from ticket, false otherwise
*/","*  Is this user logged in from a ticket (but no keytab) managed by the UGI?
   * @return true if the credentials are from a ticket cache.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,shouldRelogin,org.apache.hadoop.security.UserGroupInformation:shouldRelogin(),869,873,"/**
* Determines if re-login is required based on Kerberos credentials and Hadoop login status.
* @return true if re-login is necessary, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,toString,org.apache.hadoop.fs.FileSystem$Cache$Key:toString(),3915,3918,"/**
* Generates a string representation of this URI object.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,toString,org.apache.hadoop.security.UserGroupInformation$RealUser:toString(),497,500,"/**
* Returns string representation of associated user profile.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,closeIdle,org.apache.hadoop.ipc.Server$ConnectionManager:closeIdle(boolean),4130,4149,"/**
* Closes idle connections, optionally scanning all connections.
* @param scanAll true to scan and close all connections
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,closeAll,org.apache.hadoop.ipc.Server$ConnectionManager:closeAll(),4151,4157,"/**
* Closes all database connections.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,closeConnection,org.apache.hadoop.ipc.Server:closeConnection(org.apache.hadoop.ipc.Server$Connection),3493,3495,"/**
 * Closes the specified database connection using the connection manager.
 * @param connection the Connection object to be closed
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,javax.net.ssl.SSLSocket)",275,278,"/**
* Checks SSL socket connection for the given host.
* @param host hostname or IP address to verify
* @param ssl SSL socket object to inspect
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,initializeDefaultFactory,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeDefaultFactory(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode),103,108,"/**
* Initializes and sets a default SSLSocketFactory instance.
* @param preferredMode desired SSL channel mode
*/","* Initialize a singleton SSL socket factory.
   *
   * @param preferredMode applicable only if the instance is not initialized.
   * @throws IOException if an error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,<init>,org.apache.hadoop.fs.shell.FsCommand:<init>(org.apache.hadoop.conf.Configuration),78,80,"/**
 * Initializes FS command with configuration.
 * @param conf Hadoop configuration object.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,<init>,org.apache.hadoop.fs.shell.CommandFactory:<init>(),45,47,"/**
 * Default constructor to initialize the factory with no command.
 */",Factory constructor for commands,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,org.apache.hadoop.fs.HarFileSystem:<init>(),83,85,"/**
* Initializes the underlying file system.
*/",* public construction of harfilesystem,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,org.apache.hadoop.fs.HarFileSystem:<init>(org.apache.hadoop.fs.FileSystem),103,106,"/**
 * Initializes HarFileSystem with provided FileSystem instance.
 * @param fs the underlying file system to interact with
 */","* Constructor to create a HarFileSystem with an
   * underlying filesystem.
   * @param fs underlying file system",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,<init>,org.apache.hadoop.fs.FilterFileSystem:<init>(),71,72,"/**
 * Initializes a new instance of the FilterFileSystem class. 
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,<init>,org.apache.hadoop.fs.FilterFileSystem:<init>(org.apache.hadoop.fs.FileSystem),74,77,"/**
 * Initializes a new FilterFileSystem instance with an underlying FileSystem.
 * @param fs underlying file system to filter
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,<init>,org.apache.hadoop.fs.FsShell:<init>(),66,68,"/**
 * Initializes FsShell with an optional shell path.
 */","* Default ctor with no configuration.  Be sure to invoke
   * {@link #setConf(Configuration)} with a valid configuration prior
   * to running commands.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,<init>,org.apache.hadoop.tools.GetGroupsBase:<init>(org.apache.hadoop.conf.Configuration),43,45,"/**
* Initializes groups base with given configuration.
* @param conf Configuration object containing group settings
*/","* Create an instance of this tool using the given configuration.
   * @param conf configuration.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,<init>,org.apache.hadoop.fs.shell.Command:<init>(),76,79,"/**
* Initializes output and error streams to standard console.",Constructor,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),39,43,"/**
* Initializes ErasureEncoder with provided configuration.
* @param options ErasureCoderOptions containing encoding parameters
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),38,42,"/**
* Initializes an instance of ErasureDecoder with given configuration.
* @param options ErasureCoderOptions containing data and parity units settings
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/WritableSerialization.java,<init>,"org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)",48,51,"/**
* Initializes deserializer with configuration and target class.
* @param conf Hadoop Configuration object
* @param c target class for deserialization
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,<init>,org.apache.hadoop.log.LogLevel$CLI:<init>(org.apache.hadoop.conf.Configuration),105,107,"/**
 * Initializes CLI configuration from given Configuration object.
 * @param conf Configuration instance to initialize CLI with.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,org.apache.hadoop.security.KDiag:<init>(),186,187,"/**
 * Initializes a new instance of KDiag.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,<init>,org.apache.hadoop.ha.HAAdmin:<init>(),99,101,"/**
* Initializes a new instance of HAAdmin.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByName,org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByName(java.lang.String),647,685,"/**
* Resolves host name to InetAddress.
* @param host hostname or IP address
* @return InetAddress object or throws UnknownHostException on failure","* Create an InetAddress with a fully qualified hostname of the given
     * hostname.  InetAddress does not qualify an incomplete hostname that
     * is resolved via the domain search list.
     * {@link InetAddress#getCanonicalHostName()} will fully qualify the
     * hostname, but it always return the A record whereas the given hostname
     * may be a CNAME.
     * 
     * @param host a hostname or ip address
     * @return InetAddress with the fully qualified hostname or ip
     * @throws UnknownHostException if host does not exist",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateStaticMapping,org.apache.hadoop.security.ShellBasedIdMapping:updateStaticMapping(),304,336,"/**
* Reloads or initializes the static UID/GID mapping from file.
* @throws IOException if file access fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,write,org.apache.hadoop.security.authorize.AccessControlList:write(java.io.DataOutput),317,321,"/**
* Writes ACL string to output stream.
* @throws IOException if I/O error occurs
*/",* Serializes the AccessControlList object,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,createZooKeeper,org.apache.hadoop.ha.ActiveStandbyElector:createZooKeeper(),752,762,"/**
* Creates a ZooKeeper instance with optional SSL configuration.
* @throws IOException if SSL configuration is invalid
*/","* Get a new zookeeper client instance. protected so that test class can
   * inherit and pass in a mock object for zookeeper
   *
   * @return new zookeeper client instance
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,relogin,"org.apache.hadoop.security.UserGroupInformation:relogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean)",1334,1345,"/**
* Atomically attempts to relogin with Hadoop login context.
* @param login HadoopLoginContext instance
* @param ignoreLastLoginTime whether to ignore last login time
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],boolean)",34,40,"/**
* Initializes a CryptoFSDataOutputStream with the given parameters.
* @param out underlying FSDataOutputStream
* @param codec encryption codec to use
* @param bufferSize buffer size for encryption/decryption
* @param key encryption key
* @param iv initialization vector
* @param closeOutputStream whether to close the underlying stream
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long)",91,95,"/**
* Constructs a cryptographic output stream with given parameters.
* @param out the underlying output stream
* @param codec the crypto codec to use
* @param bufferSize the buffer size for encryption
* @param key the encryption key
* @param iv the initialization vector
* @param streamOffset the stream offset (in bytes)
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,<init>,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:<init>(int,org.apache.hadoop.crypto.CipherSuite,java.lang.String)",124,128,"/**
* Initializes OpenSSL CTR cipher with specified parameters.
* @param mode encryption mode
* @param suite chosen cipher suite
* @param engineId identifier for the underlying crypto engine
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,getInstance,org.apache.hadoop.crypto.OpensslCipher:getInstance(java.lang.String),111,114,"/**
* Returns an instance of OpenSSL Cipher based on the provided transformation.
* @param transformation cipher algorithm and mode (e.g., AES/ECB/PKCS5Padding) 
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,<init>,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:<init>(),39,48,"/**
* Initializes OpensslSm4CtrCryptoCodec instance, checking for loading and SM4 CTR support failures.
* @throws RuntimeException if initialization fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONEncKeyVersion,"org.apache.hadoop.util.KMSUtil:parseJSONEncKeyVersion(java.lang.String,java.util.Map)",157,183,"/**
* Parses JSON to create an EncryptedKeyVersion object.
* @param keyName unique key name
* @param valueMap map containing encrypted key version data
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$CreateCommand:execute(),456,474,"/**
* Creates a key with specified name and options.
* @throws IOException if I/O error occurs
* @throws NoSuchAlgorithmException if algorithm is unknown
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,createKey,"org.apache.hadoop.crypto.key.KeyProviderExtension:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",71,75,"/**
* Creates a new KeyVersion object using the provided name and options.
* @param name identifier for the new key
* @param options configuration settings for the key creation process
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String),148,154,"/**
* Generates and returns a new version of a given key.
* @param name unique identifier for the key
* @return KeyVersion object or null if error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,rollNewVersion,org.apache.hadoop.crypto.key.KeyProviderExtension:rollNewVersion(java.lang.String),77,81,"/**
* Rolls a new version of the cryptographic key.
* @param name unique identifier for the key
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$RollCommand:execute(),308,328,"/**
* Rolls a new version of the specified key using the provided KeyProvider.
* @throws NoSuchAlgorithmException if rolling fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getSize,org.apache.hadoop.crypto.key.kms.ValueQueue:getSize(java.lang.String),323,340,"/**
* Retrieves size of queue associated with given key.
* @param keyName unique identifier for the queue
* @return size of queue, or 0 if not found
*/","* Get size of the Queue for keyName. This is only used in unit tests.
   * @param keyName the key name
   * @return int queue size. Zero means the queue is empty or the key does not exist.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getAtMost,"org.apache.hadoop.crypto.key.kms.ValueQueue:getAtMost(java.lang.String,int)",354,399,"/**
* Fetches at most 'num' values for the given key.
* @param keyName unique key identifier
* @param num maximum number of values to fetch
* @return List of retrieved values or null if not found
*/","* This removes the ""num"" values currently at the head of the Queue for the
   * provided key. Will immediately fire the Queue filler function if key
   * does not exist
   * How many values are actually returned is governed by the
   * <code>SyncGenerationPolicy</code> specified by the user.
   * @param keyName String key name
   * @param num Minimum number of values to return.
   * @return {@literal List<E>} values returned
   * @throws IOException raised on errors performing I/O.
   * @throws ExecutionException execution exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,drain,org.apache.hadoop.crypto.key.kms.ValueQueue:drain(java.lang.String),302,316,"/**
* Removes and processes queued items for the given name.
* @param keyName unique identifier to drain queue for
*/","* Drains the Queue for the provided key.
   *
   * @param keyName the key to drain the Queue for",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,refresh,"org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:refresh(java.lang.String,java.lang.String[])",57,68,"/**
* Executes a refresh operation with the given identifier and arguments.
* @param identifier unique identifier for the refresh operation
* @param args array of additional arguments for the refresh operation
* @return Collection of RefreshResponse objects or throws IOException if an error occurs.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,newEntry,"org.apache.hadoop.ipc.RetryCache:newEntry(java.lang.Object,long,byte[],int)",341,345,"/**
* Creates a new cache entry with the given payload and expiration time.
* @param payload object to be cached
* @param expirationTime time in nanoseconds until entry expires
* @param clientId unique client identifier
* @param callId unique call identifier
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:<init>(byte[],int,java.lang.Object,long,boolean)",161,165,"/**
* Creates a cache entry with associated payload.
* @param clientId unique client identifier
* @param callId unique call ID
* @param payload associated data object
* @param expirationTime time until cache entry expires
* @param success flag indicating successful operation
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,put,org.apache.hadoop.ipc.CallQueueManager:put(java.lang.Object),289,299,"/**
* Puts an element into the map with optional client-side backoff.
* @param e the element to put
* @throws InterruptedException if interrupted during backoff
*/","* Insert e into the backing queue or block until we can.  If client
   * backoff is enabled this method behaves like add which throws if
   * the queue overflows.
   * If we block and the queue changes on us, we will insert while the
   * queue is drained.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,add,org.apache.hadoop.ipc.CallQueueManager:add(java.lang.Object),301,304,"/**
* Adds an element to the collection.
* @param e element to be added
* @return true if successful, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,internalQueueCall,"org.apache.hadoop.ipc.Server:internalQueueCall(org.apache.hadoop.ipc.Server$Call,boolean)",3111,3141,"/**
* Enqueues a call to the internal queue with optional blocking.
* @param call Call object to enqueue
* @param blocking whether to block if queue is full
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,ensureInitialized,org.apache.hadoop.ipc.WritableRpcEngine:ensureInitialized(),71,75,"/**
* Ensures initialization has occurred.
* @throws IllegalStateException if called concurrently with initialization
*/",* Initialize this class if it isn't already.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,get,org.apache.hadoop.util.LightWeightCache:get(java.lang.Object),186,199,"/**
* Retrieves an entry by key and updates its expiration if enabled.
* @param key unique identifier
* @return Entry object or null if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,close,org.apache.hadoop.util.StopWatch:close(),116,121,"/**
* Shuts down the service instance when closed.
* @implSpec If started, stops the running process. 
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,now,org.apache.hadoop.util.StopWatch:now(java.util.concurrent.TimeUnit),97,100,"/**
* Converts current timestamp to specified unit.
* @param timeUnit target time unit
*/","* now.
   *
   * @param timeUnit timeUnit.
   * @return current elapsed time in specified timeunit.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,toString,org.apache.hadoop.util.StopWatch:toString(),111,114,"/**
* Returns the current date and time as a string.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Invocation:<init>(java.lang.reflect.Method,java.lang.Object[])",104,120,"/**
* Initializes invocation with method and parameters.
* @param method target method to invoke
* @param parameters input arguments for the method
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(int,long,java.lang.Class)",210,223,"/**
* Retrieves protocol signature based on client methods hash code and server version.
* @param clientMethodsHashCode unique identifier for client-side protocol
* @param serverVersion server's version information
* @param protocol protocol class to verify against
* @return ProtocolSignature object or null if match is found
*/","* Get a server protocol's signature
   * 
   * @param clientMethodsHashCode client protocol methods hashcode
   * @param serverVersion server protocol version
   * @param protocol protocol
   * @return the server's protocol signature",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(java.lang.String,long)",225,229,"/**
* Retrieves the protocol signature based on name and version.
* @param protocolName unique protocol identifier
* @param version protocol version number
* @return ProtocolSignature object or throws exception if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:shouldRetry(java.lang.Exception,int,int,boolean)",697,750,"/**
* Determines retry decision based on exception type and failure counts.
* @param e the thrown exception
* @param retries current retry count
* @param failovers current failover count
* @param isIdempotentOrAtMostOnce whether method is idempotent or at-most-once
* @return RetryAction indicating whether to retry, failover and retry, or fail.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,shouldRetry,"org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:shouldRetry(java.lang.Exception,int,int,boolean)",98,129,"/**
* Determines the retry policy based on exception type and applies it.
* @param e Exception to determine retry policy for
* @return RetryAction instance based on determined policy
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,doHealthChecks,org.apache.hadoop.ha.HealthMonitor:doHealthChecks(),195,227,"/**
* Continuously monitors service health and updates state accordingly.
* @throws InterruptedException if thread is interrupted during monitoring
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,org.apache.hadoop.ipc.Server$Call:<init>(),980,983,"/**
* Initializes an invalid call with default parameters.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,addResponseTime,"org.apache.hadoop.ipc.CallQueueManager:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",256,258,"/**
* Adds response time to scheduler.
* @param name identifier for scheduling entry
* @param e schedulable entity (e.g. task or process)
* @param details processing details (e.g. start/end times) 
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doAccept,org.apache.hadoop.ipc.Server$Listener:doAccept(java.nio.channels.SelectionKey),1616,1639,"/**
* Accepts incoming connections and registers them with the connection manager.
* @param key SelectionKey representing the server socket channel
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,add,org.apache.hadoop.ipc.FairCallQueue:add(org.apache.hadoop.ipc.Schedulable),194,213,"/**
* Adds an element to the call queue, handling overflow and failover scenarios.
* @param e Element to add
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,put,org.apache.hadoop.ipc.FairCallQueue:put(org.apache.hadoop.ipc.Schedulable),215,222,"/**
* Offers or puts the given element into a priority queue based on its priority level.
* @param e element to be offered or put
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$RpcCall:run(),1234,1272,"/**
* Executes a remote procedure call (RPC) and sends the response.
* @throws Exception if an error occurs
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setDeferredError,org.apache.hadoop.ipc.Server$RpcCall:setDeferredError(java.lang.Throwable),1367,1391,"/**
* Sets a deferred error response with the specified exception.
* @param t the Throwable object representing the error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.java,getProtocolVersions,"org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersions(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto)",44,68,"/**
* Retrieves protocol versions by RPC kind and protocol.
* @param controller RpcController instance
* @param request GetProtocolVersionsRequestProto object
* @return GetProtocolVersionsResponseProto object or null on error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolProxy.java,fetchServerMethods,org.apache.hadoop.ipc.ProtocolProxy:fetchServerMethods(java.lang.reflect.Method),57,78,"/**
* Fetches server methods and version, verifying protocol compatibility.
* @throws IOException if communication with the server fails
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProtocolImpl,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:getProtocolImpl(org.apache.hadoop.ipc.RPC$Server,java.lang.String,long)",510,527,"/**
* Retrieves Protocol Implementation for given server, Proto Name and Client Version.
* @param server RPC Server instance
* @param protoName protocol name to fetch
* @param clientVersion desired protocol version from client
* @return ProtoClassProtoImpl object or throws exception if not found
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$FatalRpcServerException:<init>(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,java.lang.String)",1990,1992,"/**
* Constructs a FatalRpcServerException with an error code and custom message.
* @param errCode error code
* @param message custom error message",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,<init>,org.apache.hadoop.ipc.ResponseBuffer:<init>(int),37,39,"/**
 * Initializes a new ResponseBuffer instance with specified capacity.
 * @param capacity maximum size of buffer in bytes
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,run,org.apache.hadoop.ipc.Client$Connection$RpcRequestSender:run(),1115,1150,"/**
* Continuously sends RPC requests from the queue and handles any exceptions.
*@throws InterruptedException if interrupted while waiting for a request
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,decayCurrentCosts,org.apache.hadoop.ipc.DecayRpcScheduler:decayCurrentCosts(),479,540,"/**
* Decays current costs by applying the decay factor to each user's cost.
* Updates total decayed and raw call costs, as well as service user costs.
*/","* Decay the stored costs for each user and clean as necessary.
   * This method should be called periodically in order to keep
   * costs current.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getPriorityLevel,org.apache.hadoop.ipc.DecayRpcScheduler:getPriorityLevel(org.apache.hadoop.ipc.Schedulable),677,684,"/**
* Retrieves the priority level of the given schedulable object.
* @param obj Schedulable object to evaluate
* @return Priority level (non-negative integer) or 0 for highest priority users 
*/","* Compute the appropriate priority for a schedulable based on past requests.
   * @param obj the schedulable obj to query and remember
   * @return the level index which we recommend scheduling in",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getPriorityLevel,org.apache.hadoop.ipc.DecayRpcScheduler:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation),686,691,"/**
* Retrieves the priority level for a given UserGroupInformation instance.
* @param ugi UserGroupInformation object
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setPriorityLevel,"org.apache.hadoop.ipc.Server:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",732,735,"/**
* Sets priority level in the call queue.
* @param ugi User information; ignored by this method
* @param priority new priority level to set
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invoke,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke(),161,163,"/**
* Invokes method and returns CallReturn object.
* @throws Throwable on invocation error.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,getValue,org.apache.hadoop.ipc.RpcWritable$Buffer:getValue(java.lang.Object),190,192,"/**
* Reads RPC-writable data from input stream and returns deserialized object.
* @param bb InputStream to read from
* @return Deserialized object of type T or null on IO error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,setResponse,org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:setResponse(com.google.protobuf.Message),405,410,"/**
* Updates response with given message and logs processing time.
* @param message Message object to set as response
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,setResponse,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:setResponse(org.apache.hadoop.thirdparty.protobuf.Message),437,442,"/**
* Updates response with given Message and updates metrics.
* @param message the Message to set as response
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponseForWritable,"org.apache.hadoop.ipc.Server:setupResponseForWritable(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",3564,3580,"/**
* Assembles writable response by combining header and data, 
* then returns the response as a byte array. 
* @param header RpcResponseHeaderProto object
* @param rv Writable object to be written into response
* @return byte[] containing assembled response or null on error
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,removeNextElement,org.apache.hadoop.ipc.FairCallQueue:removeNextElement(),165,178,"/**
* Retrieves and removes the next element from the queue based on priority.
* @return the removed element or null if none available
*/","* Returns an element first non-empty queue equal to the priority returned
   * by the multiplexer or scans from highest to lowest priority queue.
   *
   * Caller must always acquire a semaphore permit before invoking.
   *
   * @return the first non-empty queue with less priority, or null if
   * everything was empty",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,close,org.apache.hadoop.ipc.Client$Connection:close(),1266,1304,"/**
* Closes the IPC connection, removing it from the pool and cleaning up resources.
*/",Close the connection.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRunLoop,org.apache.hadoop.ipc.Server$Responder:doRunLoop(),1727,1796,"/**
* Runs the event loop, processing and purging async writes.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,sendResponse,org.apache.hadoop.ipc.Server$Connection:sendResponse(org.apache.hadoop.ipc.Server$RpcCall),3062,3064,"/**
* Sends response to RPC call.
* @param call RpcCall object containing request details
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",56,63,"/**
* Initializes Globber with file system, pattern and optional filter.
* @param fs FileSystem instance
* @param pathPattern globbing pattern for files
* @param filter PathFilter to apply on matched files
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter,boolean)",81,91,"/**
* Initializes a Globber instance to match files against a pattern.
* @param fs FileSystem instance
* @param pathPattern file system path pattern
* @param filter optional PathFilter for filtering results
* @param resolveSymlinks whether to follow symbolic links
*/","* Filesystem constructor for use by {@link GlobBuilder}.
   * @param fs filesystem
   * @param pathPattern path pattern
   * @param filter optional filter
   * @param resolveSymlinks should symlinks be resolved.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,org.apache.hadoop.util.MachineList:<init>(java.lang.String),73,75,"/**
* Constructs a MachineList instance from host entries string.
* @param hostEntries string containing machine information
*/","* 
   * @param hostEntries comma separated ip/cidr/host addresses",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FileBasedIPList.java,<init>,org.apache.hadoop.util.FileBasedIPList:<init>(java.lang.String),52,65,"/**
* Initializes FileBasedIPList with file name.
* @param fileName path to IP list file
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfo.java,newInstance,org.apache.hadoop.util.SysInfo:newInstance(),36,44,"/**
* Creates a platform-specific system information instance.
* @return SysInfo object for the current operating system
*/","* Return default OS instance.
   * @throws UnsupportedOperationException If cannot determine OS.
   * @return Default instance for the detected OS.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getPhysicalMemorySize,org.apache.hadoop.util.SysInfoLinux:getPhysicalMemorySize(),594,600,"/**
* Calculates total physical memory size in bytes.
* @return Physical memory size in bytes, or -1 if failed to read proc mem info file
*/",{@inheritDoc},,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getAvailableVirtualMemorySize,org.apache.hadoop.util.SysInfoLinux:getAvailableVirtualMemorySize(),619,622,"/**
* Calculates total available virtual memory size.
* @return Total available virtual memory in bytes
*/",{@inheritDoc},,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,<init>,org.apache.hadoop.fs.FSDataInputStream:<init>(java.io.InputStream),58,64,"/**
* Validates and initializes the input stream for fetching file data.
* @param in InputStream to be validated
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,read,"org.apache.hadoop.fs.FSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)",196,212,"/**
* Reads data from input stream using specified options and buffer pool.
* @param bufferPool ByteBuffer pool to use for reading
* @param maxLength maximum length of read data
* @param opts Read options (e.g. timeouts, flags)
* @return ByteBuffer containing read data or null if failed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,put,org.apache.hadoop.util.LightWeightResizableGSet:put(java.lang.Object),91,96,"/**
* Inserts the given element into the map and expands the underlying data structure if necessary.
* @param element the element to be inserted
* @return the previously associated value or null if none existed.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,remove,org.apache.hadoop.util.LightWeightGSet$SetIterator:remove(),346,357,"/**
* Removes the current element from the set.
* @throws IllegalStateException if there's no current element
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,remove,org.apache.hadoop.util.LightWeightResizableGSet:remove(java.lang.Object),103,106,"/**
 * Removes an entry from the map using the provided key.
 * @param key unique identifier of the entry to be removed
 * @return value associated with the removed key, or null if not found
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,evict,org.apache.hadoop.util.LightWeightCache:evict(),155,161,"/**
* Evicts and returns the oldest element from the queue.
* @return The evicted element, or null if queue is empty
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,transform,"org.apache.hadoop.util.XMLUtils:transform(java.io.InputStream,java.io.InputStream,java.io.Writer)",79,95,"/**
* Transforms XML data using a stylesheet, writing output to a Writer.
* @param styleSheet InputStream containing XSLT stylesheet
* @param xml InputStream containing XML data
* @param out Writer to write transformed output to
*/","* Transform input xml given a stylesheet.
   * 
   * @param styleSheet the style-sheet
   * @param xml input xml data
   * @param out output
   * @throws TransformerConfigurationException synopsis signals a problem
   *         creating a transformer object.
   * @throws TransformerException this is used for throwing processor
   *          exceptions before the processing has started.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean,boolean,java.util.List)",446,469,"/**
* Formats storage summary based on options and types.
* @param qOption true to include quota usage
* @param hOption true for human-readable size formatting
* @param tOption true to show type-specific data
* @param xOption true to exclude snapshots from count
* @param types list of StorageType objects
* @return formatted storage summary string","Return the string representation of the object in the output format.
   * if qOption is false, output directory count, file count, and content size;
   * if qOption is true, output quota and remaining quota as well.
   * if hOption is false, file sizes are returned in bytes
   * if hOption is true, file sizes are returned in human readable
   * if tOption is true, display the quota by storage types
   * if tOption is false, same logic with #toString(boolean,boolean)
   * if xOption is false, output includes the calculation from snapshots
   * if xOption is true, output excludes the calculation from snapshots
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output is to be used
   * @param tOption a flag indicating if display quota by storage types
   * @param xOption a flag indicating if calculation from snapshots is to be
   *                included in the output
   * @param types Storage types to display
   * @return the string representation of the object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toSnapshot,org.apache.hadoop.fs.ContentSummary:toSnapshot(boolean),498,503,"/**
* Formats snapshot statistics into a human-readable string.
* @param hOption true to use human-readable units (e.g. ""GB"") instead of raw values
* @return formatted string with space consumed and file/directory counts
*/","* Return the string representation of the snapshot counts in the output
   * format.
   * @param hOption flag indicating human readable or not
   * @return String representation of the snapshot counts",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,getQuotaUsage,org.apache.hadoop.fs.QuotaUsage:getQuotaUsage(boolean),329,346,"/**
* Formats quota usage as human-readable strings.
* @param hOption true for human-readable format, false otherwise
* @return Quota usage string or QUOTA_NONE if not applicable
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,getTypesQuotaUsage,"org.apache.hadoop.fs.QuotaUsage:getTypesQuotaUsage(boolean,java.util.List)",348,366,"/**
* Calculates types quota usage summary for each storage type.
* @param hOption indicates whether to use human-readable format
* @param types list of StorageType objects
* @return formatted string with quota usage details or empty string if none
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,computeCapacity,"org.apache.hadoop.util.LightWeightGSet:computeCapacity(double,java.lang.String)",374,377,"/**
* Calculates available memory capacity as a percentage.
* @param percentage desired capacity percentage
* @param mapName name of the memory map (currently unused)
*/","* Let t = percentage of max memory.
   * Let e = round(log_2 t).
   * Then, we choose capacity = 2^e/(size of reference),
   * unless it is outside the close interval [1, 2^30].
   *
   * @param mapName mapName.
   * @param percentage percentage.
   * @return compute capacity.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,fill,org.apache.hadoop.fs.FSInputChecker:fill(),217,222,"/**
* Fills the internal buffer with a checksum chunk from the input stream.
* @throws IOException on I/O errors
*/","* Fills the buffer with a chunk data. 
   * No mark is supported.
   * This method assumes that all data in the buffer has already been read in,
   * hence pos > count.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,readAndDiscard,org.apache.hadoop.fs.FSInputChecker:readAndDiscard(int),231,245,"/**
* Reads and discards a specified number of bytes from the underlying stream.
* @param len desired number of bytes to discard
* @return actual number of bytes discarded
*/","* Like read(byte[], int, int), but does not provide a dest buffer,
   * so the read data is discarded.
   * @param      len maximum number of bytes to read.
   * @return     the number of bytes read.
   * @throws     IOException  if an I/O error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,toString,org.apache.hadoop.io.UTF8:toString(),163,175,"/**
* Converts the character sequence to a human-readable string.
* @return A string representation of the characters or null if an error occurs
*/",Convert to a String.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,toStringChecked,org.apache.hadoop.io.UTF8:toStringChecked(),183,190,"/**
* Returns a string representation of the data.
* @throws IOException if an I/O error occurs
*/","* Convert to a string, checking for valid UTF8.
   * @return the converted string
   * @throws UTFDataFormatException if the underlying bytes contain invalid
   * UTF8 data.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,fromBytes,org.apache.hadoop.io.UTF8:fromBytes(byte[]),257,263,"/**
* Converts byte array to string representation.
* @param bytes input byte array
*/","* @return Convert a UTF-8 encoded byte array back into a string.
   *
   * @param bytes input bytes.
   * @throws IOException if the byte array is invalid UTF8",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,readString,org.apache.hadoop.io.UTF8:readString(java.io.DataInput),272,277,"/**
* Reads a string from the input stream.
* @param in DataInput stream containing the string
*/","* @return Read a UTF-8 encoded string.
   *
   * @see DataInput#readUTF()
   * @param in DataInput.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,checkResponse,org.apache.hadoop.ipc.Client:checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto),252,267,"/**
* Validates RPC response header.
* @param header Response header from server
*/",Check the rpc response header.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,toString,org.apache.hadoop.ipc.Client:toString(),1353,1357,"/**
* Returns a string representation of this object in the format ""<classSimpleName>-<clientId>"".
* @return unique identifier as a hexadecimal string.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteToHexString,org.apache.hadoop.util.StringUtils:byteToHexString(byte),210,212,"/**
* Converts single byte to its hexadecimal string representation.
* @param b single byte value
*/","* Convert a byte to a hex string.
   * @see #byteToHexString(byte[])
   * @see #byteToHexString(byte[], int, int)
   * @param b byte
   * @return byte's hex value as a String",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,toString,org.apache.hadoop.ha.ActiveStandbyElector:toString(),1274,1280,"/**
* Returns a string representation of this object, including its unique ID and App Data.
* @return Unique identifier and App Data as hexadecimal string
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,uncaughtException,"org.apache.hadoop.service.launcher.ServiceLauncher:uncaughtException(java.lang.Thread,java.lang.Throwable)",779,783,"/**
* Handles and logs uncaught exceptions.
* @param thread the Thread instance that threw the exception
* @param exception the actual Throwable exception
*/","* Handler for uncaught exceptions: terminate the service.
   * @param thread thread
   * @param exception exception",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exitWithUsageMessage,org.apache.hadoop.service.launcher.ServiceLauncher:exitWithUsageMessage(),1033,1035,"/**
 * Exits the program with usage message.
 */","* Exit with the usage exit code {@link #EXIT_USAGE}
   * and message {@link #USAGE_MESSAGE}.
   * @throws ExitUtil.ExitException if exceptions are disabled",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/HadoopUncaughtExceptionHandler.java,uncaughtException,"org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable)",83,127,"/**
* Handles uncaught exceptions in threads.
* @param thread Thread that threw the exception
* @param exception Throwable exception object
*/","* Uncaught exception handler.
   * If an error is raised: shutdown
   * The state of the system is unknown at this point -attempting
   * a clean shutdown is dangerous. Instead: exit
   * @param thread thread that failed
   * @param exception the raised exception",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exit,"org.apache.hadoop.service.launcher.ServiceLauncher:exit(int,java.lang.String)",856,858,"/**
* Terminates application with specified exit code and error message.
* @param exitCode process termination status
* @param message brief description of termination reason
*/","* Exit the JVM.
   *
   * This is method can be overridden for testing, throwing an 
   * exception instead. Any subclassed method MUST raise an 
   * {@code ExitException} instance/subclass.
   * The service launcher code assumes that after this method is invoked,
   * no other code in the same method is called.
   * @param exitCode code to exit
   * @param message input message.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,org.apache.hadoop.util.ExitUtil:terminate(int),368,370,"/**
* Terminates the program with the specified exit status and message.
* @param status exit status code
*/","* Like {@link #terminate(int, Throwable)} without a message.
   *
   * @param status exit code
   * @throws ExitException if {@link System#exit(int)} is disabled.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Classpath.java,terminate,"org.apache.hadoop.util.Classpath:terminate(int,java.lang.String)",121,124,"/**
* Terminates program with specified exit status and error message.
* @param status exit code
* @param msg error message to print before termination
*/","* Prints a message to stderr and exits with a status code.
   *
   * @param status exit code
   * @param msg message",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,interrupted,org.apache.hadoop.service.launcher.InterruptEscalator:interrupted(org.apache.hadoop.service.launcher.IrqHandler$InterruptData),103,135,"/**
* Handles service interruption by logging warnings and initiating forced shutdown.
* @param interruptData Interrupt data from the IrqHandler
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,org.apache.hadoop.util.ExitUtil:halt(int),389,391,"/**
* Halts the program with the specified exit status.
* @param status exit code to be returned
*/","* Forcibly terminates the currently running Java virtual machine.
   * @param status status code
   * @throws HaltException if {@link Runtime#halt(int)} is disabled.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/QuickSort.java,sort,"org.apache.hadoop.util.QuickSort:sort(org.apache.hadoop.util.IndexedSortable,int,int)",58,61,"/**
* Sorts an IndexedSortable object in-place using QuickSort algorithm.
* @param s IndexedSortable object to be sorted
* @param p Leftmost index of the subarray to sort
* @param r Rightmost index of the subarray to sort
*/","* Sort the given range of items using quick sort.
   * {@inheritDoc} If the recursion depth falls below {@link #getMaxDepth},
   * then switch to {@link HeapSort}.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclStatus.java,<init>,"org.apache.hadoop.fs.permission.AclStatus:<init>(java.lang.String,java.lang.String,boolean,java.lang.Iterable,org.apache.hadoop.fs.permission.FsPermission)",216,223,"/**
* Constructs an AclStatus object with specified file attributes.
* @param owner file owner
* @param group file group
* @param stickyBit indicates presence of sticky bit
* @param entries list of ACL entries
* @param permission FsPermission value for the file
*/","* Private constructor.
   *
   * @param file Path file associated to this ACL
   * @param owner String file owner
   * @param group String file group
   * @param stickyBit the sticky bit
   * @param entries the ACL entries
   * @param permission permission of the path",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,parseACLs,org.apache.hadoop.util.ZKUtil:parseACLs(java.lang.String),95,122,"/**
* Parses ACL string into a list of ACL objects.
* @param aclString comma-separated ACLs in scheme:id:perm format
* @return List of ACL objects or empty list if input is null
*/","* Parse comma separated list of ACL entries to secure generated nodes, e.g.
   * <code>sasl:hdfs/host1@MY.DOMAIN:cdrwa,sasl:hdfs/host2@MY.DOMAIN:cdrwa</code>
   *
   * @param aclString aclString.
   * @return ACL list
   * @throws BadAclFormatException if an ACL is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,parseAuth,org.apache.hadoop.util.ZKUtil:parseAuth(java.lang.String),133,154,"/**
* Parses a comma-separated authentication string into a list of ZKAuthInfo objects.
* @param authString comma-separated scheme:auth strings
* @return List of ZKAuthInfo objects or empty list if input is null
*/","* Parse a comma-separated list of authentication mechanisms. Each
   * such mechanism should be of the form 'scheme:auth' -- the same
   * syntax used for the 'addAuth' command in the ZK CLI.
   * 
   * @param authString the comma-separated auth mechanisms
   * @return a list of parsed authentications
   * @throws BadAuthFormatException if the auth format is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,preserveAttributes,"org.apache.hadoop.fs.shell.CommandWithDestination:preserveAttributes(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData,boolean)",445,490,"/**
* Preserves attributes from source to target PathData.
* @param src source data
* @param target target data
* @param preserveRawXAttrs whether to preserve raw xattrs
*/","* Preserve the attributes of the source to the target.
   * The method calls {@link #shouldPreserve(FileAttribute)} to check what
   * attribute to preserve.
   * @param src source to preserve
   * @param target where to preserve attributes
   * @param preserveRawXAttrs true if raw.* xattrs should be preserved
   * @throws IOException if fails to preserve attributes",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,add,org.apache.hadoop.util.ChunkedArrayList:add(java.lang.Object),132,146,"/**
* Adds an element to the list, potentially resizing chunks as needed.
* @param e element to be added
* @return true if addition was successful, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclUtil.java,getMinimalAcl,org.apache.hadoop.fs.permission.AclUtil:getMinimalAcl(org.apache.hadoop.fs.permission.FsPermission),99,116,"/**
* Creates a list of minimal ACL entries for the given permissions.
* @param perm FsPermission object defining user, group and other actions
* @return List of AclEntry objects with ACCESS scope and USER, GROUP, OTHER types
*/","* Translates the given permission bits to the equivalent minimal ACL.
   *
   * @param perm FsPermission to translate
   * @return List&lt;AclEntry&gt; containing exactly 3 entries representing the
   *         owner, group and other permissions",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,trackDuration,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:trackDuration(java.lang.String,long)",461,468,"/**
* Creates a duration tracker based on the provided key and count.
* @param key unique identifier for tracking
* @param count initial value for tracking
* @return DurationTracker instance or stub if key is unknown
*/","* If the store is tracking the given key, return the
   * duration tracker for it. If not tracked, return the
   * stub tracker.
   * @param key statistic key prefix
   * @param count  #of times to increment the matching counter in this
   * operation.
   * @return a tracker.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,<init>,"org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String)",58,62,"/**
* Initializes duration tracker with given statistics store and key.
* @param iostats IO statistics store
* @param key unique identifier for tracking
*/","* Constructor -increments the counter by 1.
   * @param iostats statistics to update
   * @param key prefix of values.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DurationInfo.java,<init>,"org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,java.lang.String,java.lang.Object[])",57,59,"/**
* Constructs DurationInfo object with logging and formatting.
* @param log Logger instance for logging
* @param format string format for duration display
*/","* Create the duration text from a {@code String.format()} code call;
   * log output at info level.
   * @param log log to write to
   * @param format format string
   * @param args list of arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,waitForCompletion,org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.concurrent.CompletableFuture),116,126,"/**
* Waits for the asynchronous operation to complete and throws an exception if cancelled or completed with an error.
* @throws IOException if the operation is cancelled or completes with an error
*/","* Wait for a single of future to complete, extracting IOEs afterwards.
   *
   * @param <T> Generics Type T.
   * @param future future to wait for.
   * @throws IOException      if one of the called futures raised an IOE.
   * @throws RuntimeException if one of the futures raised one.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,waitForCompletionIgnoringExceptions,org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletionIgnoringExceptions(java.util.concurrent.CompletableFuture),133,143,"/**
* Waits for CompletableFuture to complete, ignoring any exceptions.
* @param future CompletableFuture object to wait on
*/","* Wait for a single of future to complete, ignoring exceptions raised.
   * @param future future to wait for.
   * @param <T> Generics Type T.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,toString,org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:toString(),107,112,"/**
* Returns a human-readable string representation of this duration statistic.
* @return formatted string containing duration and statistic type
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DurationInfo.java,toString,org.apache.hadoop.util.DurationInfo:toString(),89,92,"/**
* Returns a string representation of this object.
* Combines formatted text and parent class details with durations.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,newCrcComposer,"org.apache.hadoop.util.CrcComposer:newCrcComposer(org.apache.hadoop.util.DataChecksum$Type,long)",60,64,"/**
* Creates a new CRC composer instance with specified parameters.
* @param type checksum type to use
* @param bytesPerCrcHint hint for optimal CRC byte size
*/","* Returns a CrcComposer which will collapse all ingested CRCs into a single
   * value.
   *
   * @param type type.
   * @param bytesPerCrcHint bytesPerCrcHint.
   * @throws IOException raised on errors performing I/O.
   * @return a CrcComposer which will collapse all ingested CRCs into a single value.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,update,"org.apache.hadoop.util.CrcComposer:update(int,long)",167,192,"/**
* Updates the composite CRC and advances stripe position by bytesPerCrc.
* @param crcB new CRC value
* @param bytesPerCrc number of bytes to advance stripe position by
*/","* Updates with a single additional CRC which corresponds to an underlying
   * data size of {@code bytesPerCrc}.
   *
   * @param crcB crcB.
   * @param bytesPerCrc bytesPerCrc.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,lock,org.apache.hadoop.util.InstrumentedLock:lock(),101,107,"/**
* Acquires the lock and starts timing its usage.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,lockInterruptibly,org.apache.hadoop.util.InstrumentedLock:lockInterruptibly(),109,115,"/**
 * Acquires a lock that can be interrupted and starts timing its acquisition. 
 * @throws InterruptedException if interruption is detected while waiting for the lock
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,tryLock,"org.apache.hadoop.util.InstrumentedLock:tryLock(long,java.util.concurrent.TimeUnit)",126,136,"/**
* Tries to acquire lock for a specified duration.
* @param time maximum wait time
* @param unit wait time unit (e.g. seconds)
* @return true if lock is acquired, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,unlock,org.apache.hadoop.util.InstrumentedLock:unlock(),138,144,"/**
* Unlocks the lock and performs a check on its usage.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getFormattedTimeWithDiff,"org.apache.hadoop.util.StringUtils:getFormattedTimeWithDiff(org.apache.commons.lang3.time.FastDateFormat,long,long)",375,379,"/**
* Formats time difference between two timestamps.
* @param dateFormat date format to apply
* @param finishTime end timestamp
* @param startTime start timestamp
*/","* Formats time in ms and appends difference (finishTime - startTime)
   * as returned by formatTimeDiff().
   * If finish time is 0, empty string is returned, if start time is 0
   * then difference is not appended to return value.
   *
   * @param dateFormat date format to use
   * @param finishTime finish time
   * @param startTime  start time
   * @return formatted value.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,escapeString,org.apache.hadoop.util.StringUtils:escapeString(java.lang.String),665,667,"/**
* Escapes special characters in a string.
* @param str input string to be escaped
*/","* Escape commas in the string using the default escape char
   * @param str a string
   * @return an escaped string",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,unEscapeString,org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String),723,725,"/**
* Unescapes special characters in a string.
* @param str input string to process
*/","* Unescape commas in the string using the default escape char
   * @param str a string
   * @return an unescaped string",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,startupShutdownMessage,"org.apache.hadoop.service.launcher.ServiceLauncher:startupShutdownMessage(java.lang.String,java.util.List)",1010,1016,"/**
* Creates a startup/shutdown message for the given class and command-line arguments.
* @param classname name of the class
* @param args command-line arguments
* @return formatted message or null if unsuccessful
*/","* @return Build a log message for starting up and shutting down.
   * @param classname the class of the server
   * @param args arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,sourceNext,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceNext(),489,499,"/**
* Advances to the next source element and returns it.
* @throws IOException if an I/O error occurs
*/","* Get the next source value.
     * This calls {@link #sourceHasNext()} first to verify
     * that there is data.
     * @return the next value
     * @throws IOException failure
     * @throws NoSuchElementException no more data",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,sourceHasNext,org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:sourceHasNext(),790,793,"/**
* Determines whether there is another item in the source.
* @return true if next item exists, false otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,awaitFuture,org.apache.hadoop.fs.impl.FutureIOSupport:awaitFuture(java.util.concurrent.Future),65,69,"/**
* Awaits completion of a future, rethrowing any IO exceptions.
* @param future the future to await
* @return the result or null if cancelled 
*/","* Given a future, evaluate it. Raised exceptions are
   * extracted and handled.
   * See {@link FutureIO#awaitFuture(Future, long, TimeUnit)}.
   * @param future future to evaluate
   * @param <T> type of the result.
   * @return the result, if all went well.
   * @throws InterruptedIOException future was interrupted
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitAllFutures,org.apache.hadoop.util.functional.FutureIO:awaitAllFutures(java.util.Collection),162,169,"/**
* Asynchronously retrieves results from a collection of futures and returns them as a list.
* @param collection Collection of futures to await completion on
* @return List of results or null if any future failed
*/","* Evaluates a collection of futures and returns their results as a list.
   * <p>
   * This method blocks until all futures in the collection have completed.
   * If any future throws an exception during its execution, this method
   * extracts and rethrows that exception.
   * </p>
   * @param collection collection of futures to be evaluated
   * @param <T> type of the result.
   * @return the list of future's result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,awaitFuture,"org.apache.hadoop.fs.impl.FutureIOSupport:awaitFuture(java.util.concurrent.Future,long,java.util.concurrent.TimeUnit)",86,93,"/**
* Awaits a future result with specified timeout.
* @param future the future to await
* @param timeout maximum wait time in given unit
* @param unit time unit for timeout (e.g., milliseconds, seconds)
* @return the awaited result or throws exception if timed out
*/","* Given a future, evaluate it. Raised exceptions are
   * extracted and handled.
   * See {@link FutureIO#awaitFuture(Future, long, TimeUnit)}.
   * @param future future to evaluate
   * @param <T> type of the result.
   * @param timeout timeout.
   * @param unit unit.
   * @return the result, if all went well.
   * @throws InterruptedIOException future was interrupted
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown
   * @throws TimeoutException the future timed out.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitAllFutures,"org.apache.hadoop.util.functional.FutureIO:awaitAllFutures(java.util.Collection,java.time.Duration)",188,197,"/**
* Asynchronously fetches results from all futures in the specified collection within the given time duration.
* @param collection Collection of Future objects to await
* @param duration Time duration to wait for results
* @return List of results or null if any future timed out
*/","* Evaluates a collection of futures and returns their results as a list,
   * but only waits up to the specified timeout for each future to complete.
   * <p>
   * This method blocks until all futures in the collection have completed or
   * the timeout expires, whichever happens first. If any future throws an
   * exception during its execution, this method extracts and rethrows that exception.
   * @param collection collection of futures to be evaluated
   * @param duration timeout duration
   * @param <T> type of the result.
   * @return the list of future's result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown
   * @throws TimeoutException the future timed out.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,cancelAllFuturesAndAwaitCompletion,"org.apache.hadoop.util.functional.FutureIO:cancelAllFuturesAndAwaitCompletion(java.util.Collection,boolean,java.time.Duration)",211,239,"/**
* Cancels all futures in the collection and awaits their completion within a specified duration.
* @param collection Collection of futures to cancel and await
* @param interruptIfRunning Whether to interrupt running tasks
* @param duration Duration to wait for task completion
* @return List of results from awaited tasks or null if not found
*/","* Cancels a collection of futures and awaits the specified duration for their completion.
   * <p>
   * This method blocks until all futures in the collection have completed or
   * the timeout expires, whichever happens first.
   * All exceptions thrown by the futures are ignored. as is any TimeoutException.
   * @param collection collection of futures to be evaluated
   * @param interruptIfRunning should the cancel interrupt any active futures?
   * @param duration total timeout duration
   * @param <T> type of the result.
   * @return all futures which completed successfully.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,newInstance,"org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration,java.lang.Class[],java.lang.Object[])",138,161,"/**
* Creates a new instance of the specified class with given arguments.
* @param theClass class to instantiate
* @param conf configuration object for initialization
* @param argTypes argument types matching constructor requirements
* @param values actual constructor arguments
* @return instantiated object or null on failure
*/","Create an object for the given class and initialize it from conf
   *
   * @param theClass class of which an object is created
   * @param conf Configuration
   * @param argTypes the types of the arguments
   * @param values the values of the arguments
   * @param <T> Generics Type.
   * @return a new object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getKeyClass,org.apache.hadoop.io.SequenceFile$Reader:getKeyClass(),2196,2205,"/**
* Retrieves the class associated with a given key name.
* @return Class<?> object or null if not found
*/",@return Returns the class of keys in this file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getValueClass,org.apache.hadoop.io.SequenceFile$Reader:getValueClass(),2213,2222,"/**
* Retrieves the class associated with the value, loading it if necessary.
* @return Class<?> object or null if not loaded
*/",@return Returns the class of values in this file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,readFields,org.apache.hadoop.io.EnumSetWritable:readFields(java.io.DataInput),117,133,"/**
* Reads EnumSet data from DataInput stream.
* @param in input stream
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,loadClass,org.apache.hadoop.util.FindClass:loadClass(java.lang.String),244,259,"/**
* Loads a class by name and executes the post-load hook.
* @param name class name to load
* @return status code indicating success or failure
*/","* Loads the class of the given name
   * @param name classname
   * @return outcome code",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,createClassInstance,org.apache.hadoop.util.FindClass:createClassInstance(java.lang.String),278,302,"/**
* Creates an instance of a specified class.
* @param name Class name to instantiate
* @return int result code: SUCCESS or error code (E_NOT_FOUND, E_CREATE_FAILED)
*/","* Create an instance of a class
   * @param name classname
   * @return the outcome",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,<init>,org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(org.apache.hadoop.fs.Path),115,117,"/**
 * Initializes an instance of FSBuilder with a specified file system location.
 * @param path absolute file system path to initialize from.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,<init>,org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(org.apache.hadoop.fs.PathHandle),119,121,"/**
* Initializes FS builder with given file system handle.
* @param pathHandle unique file system identifier
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandler.java,<init>,org.apache.hadoop.fs.FsUrlStreamHandler:<init>(),42,44,"/**
* Initializes FsUrlStreamHandler with default configuration. 
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,createConfiguration,org.apache.hadoop.service.launcher.ServiceLauncher:createConfiguration(),399,401,"/**
 * Creates and returns a new instance of the Configuration class.
 */","* Override point: create the base configuration for the service.
   *
   * Subclasses can override to create HDFS/YARN configurations etc.
   * @return the configuration to use as the service initializer.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,loadConf,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:loadConf(),437,449,"/**
* Loads and returns configuration object based on provided or default settings.
*/","* Return the supplied configuration for testing or otherwise load a new
   * configuration.
   *
   * @return the configuration to use",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,<init>,org.apache.hadoop.util.FindClass:<init>(),123,125,"/**
 * Initializes the FindClass instance with default configuration.
 */","* Empty constructor; passes a new Configuration
   * object instance to its superclass's constructor",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,<init>,org.apache.hadoop.conf.ReconfigurableBase:<init>(),75,77,"/**
 * Initializes a new instance of the ReconfigurableBase class with default configuration.",* Construct a ReconfigurableBase.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,<init>,org.apache.hadoop.conf.ReconfigurableBase:<init>(org.apache.hadoop.conf.Configuration),84,86,"/**
* Initializes a ReconfigurableBase instance with the provided configuration.
* @param conf optional Configuration object to initialize from
*/","* Construct a ReconfigurableBase with the {@link Configuration}
   * conf.
   * @param conf configuration.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newDataChecksum,"org.apache.hadoop.util.DataChecksum:newDataChecksum(org.apache.hadoop.util.DataChecksum$Type,int)",135,150,"/**
* Creates a new data checksum instance based on the specified type and byte count.
* @param type Type of checksum (NULL, CRC32, or CRC32C)
* @param bytesPerChecksum Number of bytes to include in each checksum
* @return DataChecksum object for the specified type and byte count, or null if invalid
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getQualifiedBinPath,org.apache.hadoop.util.Shell:getQualifiedBinPath(java.lang.String),701,704,"/**
* Returns absolute path to executable with trailing slash.
* @param executable name of executable (e.g. ""myapp"")
*/","*  Fully qualify the path to a binary that should be in a known hadoop
   *  bin location. This is primarily useful for disambiguating call-outs
   *  to executable sub-components of Hadoop to avoid clashes with other
   *  executables that may be in the path.  Caveat:  this call doesn't
   *  just format the path to the bin directory.  It also checks for file
   *  existence of the composed path. The output of this call should be
   *  cached by callers.
   *
   * @param executable executable
   * @return executable file reference
   * @throws FileNotFoundException if the path does not exist
   * @throws IOException on path canonicalization failures",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,runCommand,org.apache.hadoop.util.Shell:runCommand(),967,1098,"/**
* Runs a command in a child shell process.
*/","* Run the command.
   *
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,org.apache.hadoop.util.Progress:addPhase(java.lang.String),61,65,"/**
* Adds a new phase to the progress with specified status.
* @param status current status of the phase
*/","* Adds a named node to the tree.
   * @param status status.
   * @return Progress.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,createRootDirRecursively,org.apache.hadoop.util.curator.ZKCuratorManager:createRootDirRecursively(java.lang.String),361,363,"/**
* Creates root directory recursively at specified path.
* @param path absolute directory path
*/","* Utility function to ensure that the configured base znode exists.
   * This recursively creates the znode as well as all of its parents.
   * @param path Path of the znode to create.
   * @throws Exception If it cannot create the file.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,invoke,"org.apache.hadoop.util.dynamic.DynConstructors$Ctor:invoke(java.lang.Object,java.lang.Object[])",77,82,"/**
* Invokes the constructor of a class with given arguments.
* @param target  always null for constructors
* @param args    variable number of constructor arguments
* @return        instance of the constructed class or its subclass, cast to type R
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,pathCapabilities_hasPathCapability,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:pathCapabilities_hasPathCapability(java.lang.Object,org.apache.hadoop.fs.Path,java.lang.String)",349,356,"/**
* Checks if a file system has a specific capability for the given path.
* @param fs the file system object
* @param path the path to check
* @param capability the capability name to look for
* @return true if the capability exists, false otherwise
*/","* Does a path have a given capability?
   * Calls {@code PathCapabilities#hasPathCapability(Path, String)},
   * mapping IOExceptions to false.
   * @param fs filesystem
   * @param path path to query the capability of.
   * @param capability non-null, non-empty string to query the path for support.
   * @return true if the capability is supported
   * under that part of the FS
   * false if the method is not loaded or the path lacks the capability.
   * @throws IllegalArgumentException invalid arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,streamCapabilities_hasCapability,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:streamCapabilities_hasCapability(java.lang.Object,java.lang.String)",367,372,"/**
* Checks if an object has a specific capability.
* @param object the object to check
* @param capability the capability to look for
* @return true if the object has the capability, false otherwise
*/","* Does an object implement {@code StreamCapabilities} and, if so,
   * what is the result of the probe for the capability?
   * Calls {@code StreamCapabilities#hasCapability(String)},
   * @param object object to probe
   * @param capability capability string
   * @return true iff the object implements StreamCapabilities and the capability is
   * declared available.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_counters,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_counters(java.io.Serializable),611,614,"/**
* Invokes iostatisticsCountersMethod with source object to retrieve counters.
* @param source object containing statistics data
*/","* Get the counters of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of counters.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_gauges,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_gauges(java.io.Serializable),621,625,"/**
* Invokes IO statistics gauges method with provided source.
* @param source data source for IO statistics
*/","* Get the gauges of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of gauges.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_minimums,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_minimums(java.io.Serializable),632,635,"/**
* Invokes the minimum statistics method on the given source object.
* @param source source object to invoke the method on
* @return a map of minimum statistics or null if not available
*/","* Get the minimums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of minimums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_maximums,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_maximums(java.io.Serializable),642,645,"/**
* Invokes iostatisticsMaximumsMethod to retrieve maximum statistics.
* @param source data source object
*/","* Get the maximums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of maximums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_means,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_means(java.io.Serializable),654,657,"/**
* Invokes method to calculate means of IO statistics.
* @param source data source object
*/","* Get the means of an IOStatisticsSnapshot.
   * Each value in the map is the (sample, sum) tuple of the values;
   * the mean is then calculated by dividing sum/sample wherever sample is non-zero.
   * @param source source of statistics.
   * @return a map of mean key to (sample, sum) tuples.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invoke,org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:invoke(java.lang.Object[]),219,221,"/**
* Invokes a Method with variable arguments.
* @param args array of objects to pass as method arguments
* @return result of method invocation or null if method is void
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeStatic,org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invokeStatic(java.lang.Object[]),106,109,"/**
* Invokes a static method with given arguments.
* @param args variable number of method arguments
* @return result of the invoked method or null if invalid
*/","* Invoke a static method.
     * @param args arguments.
     * @return result.
     * @param <R> type of result.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invoke,org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:invoke(java.lang.Object[]),202,204,"/**
* Invokes a method on an object with variable arguments.
* @param args zero or more arguments to pass to the method
* @return result of the invoked method
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.String,java.lang.Class[])",308,311,"/**
* Implements builder with specified class name and argument classes.
* @param className name of the class
* @param argClasses classes of arguments to be used in building process
*/","* Checks for an implementation, first finding the given class by name.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param className name of a class
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.String,java.lang.Class[])",411,414,"/**
* Constructs builder instance with specified class name and argument classes.
* @param className class name
* @param argClasses variable number of argument classes
* @return Builder instance for method chaining
*/","* Checks for an implementation, first finding the given class by name.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param className name of a class
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,loadStaticMethod,"org.apache.hadoop.util.dynamic.BindingUtils:loadStaticMethod(java.lang.Class,java.lang.Class,java.lang.String,java.lang.Class[])",139,149,"/**
* Loads a static method from the given class, ensuring it's available and static.
* @param source the class containing the method
* @param returnType the expected return type of the method
* @param name the method name
* @param parameterTypes the parameter types (varargs)
* @return an UnboundMethod object or null if not found
*/","* Load a static method from the source class, which will be a noop() if
   * the class is null or the method isn't found.
   * If the class and method are not found, then an {@code IllegalStateException}
   * is raised on the basis that this means that the binding class is broken,
   * rather than missing/out of date.
   *
   * @param <T> return type
   * @param source source. If null, the method is a no-op.
   * @param returnType return type class (unused)
   * @param name method name
   * @param parameterTypes parameters
   *
   * @return the method or a no-op.
   * @throws IllegalStateException if the method is not static.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,isIOStatisticsSource,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatisticsSource(java.lang.Object),394,397,"/**
* Checks if provided object is an IO statistics source.
* @param object the object to check
*/","* Probe for an object being an instance of {@code IOStatisticsSource}.
   * @param object object to probe
   * @return true if the object is the right type, false if the classes
   * were not found or the object is null/of a different type",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,isIOStatistics,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatistics(java.lang.Object),405,408,"/**
* Checks if IO statistics are available and applicable to an object.
* @param object the object to check
* @return true if IO statistics are available for this object, false otherwise
*/","* Probe for an object being an instance of {@code IOStatisticsSource}.
   * @param object object to probe
   * @return true if the object is the right type, false if the classes
   * were not found or the object is null/of a different type",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,isIOStatisticsSnapshot,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatisticsSnapshot(java.io.Serializable),416,419,"/**
* Verifies if the provided object represents an IO statistics snapshot.
* @param object Serializable object to verify
* @return true if it's a valid snapshot, false otherwise
*/","* Probe for an object being an instance of {@code IOStatisticsSnapshot}.
   * @param object object to probe
   * @return true if the object is the right type, false if the classes
   * were not found or the object is null/of a different type",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_enabled,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_enabled(),427,430,"/**
* Verifies whether IO statistics context is enabled.
* @return true if enabled, false otherwise
*/","* Probe to check if the thread-level IO statistics enabled.
   * If the relevant classes and methods were not found, returns false
   * @return true if the IOStatisticsContext API was found
   * and is enabled.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,toString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:toString(),671,677,"/**
* Returns a string representation of the DynamicWrappedStatistics object.
* Includes availability status of IO statistics and context.",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,bulkDelete_pageSize,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_pageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",263,268,"/**
* Deletes bulk page data in the specified file system with given path.
* @param fileSystem target file system
* @param path directory to delete from
* @return number of pages deleted
*/","* Get the maximum number of objects/files to delete in a single request.
   * @param fileSystem filesystem
   * @param path path to delete under.
   * @return a number greater than or equal to zero.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws IllegalArgumentException path not valid.
   * @throws IOException problems resolving paths
   * @throws RuntimeException invocation failure.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,bulkDelete_delete,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_delete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Collection)",293,299,"/**
* Deletes a collection of files and directories by path.
* @param fs FileSystem object
* @param base Base directory for deletion
* @param paths Collection of file/directory paths to delete
*/","* Delete a list of files/objects.
   * <ul>
   *   <li>Files must be under the path provided in {@code base}.</li>
   *   <li>The size of the list must be equal to or less than the page size.</li>
   *   <li>Directories are not supported; the outcome of attempting to delete
   *       directories is undefined (ignored; undetected, listed as failures...).</li>
   *   <li>The operation is not atomic.</li>
   *   <li>The operation is treated as idempotent: network failures may
   *        trigger resubmission of the request -any new objects created under a
   *        path in the list may then be deleted.</li>
   *    <li>There is no guarantee that any parent directories exist after this call.
   *    </li>
   * </ul>
   * @param fs filesystem
   * @param base path to delete under.
   * @param paths list of paths which must be absolute and under the base path.
   * @return a list of all the paths which couldn't be deleted for a reason other than
   *          ""not found"" and any associated error message.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws IllegalArgumentException if a path argument is invalid.
   * @throws IOException IO problems including networking, authentication and more.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,fileSystem_openFile,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:fileSystem_openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.FileStatus,java.lang.Long,java.util.Map)",323,335,"/**
* Opens a file in the specified file system with optional parameters.
* @param fs target file system
* @param path file path to open
* @param policy access control policy
* @param status cached file status (null for fresh status)
* @param length file size in bytes (null for unknown)
* @param options additional file opening options
* @return FSDataInputStream object or null on failure","* OpenFile assistant, easy reflection-based access to
   * {@code FileSystem#openFile(Path)} and blocks
   * awaiting the operation completion.
   * @param fs filesystem
   * @param path path
   * @param policy read policy
   * @param status optional file status
   * @param length optional file length
   * @param options nullable map of other options
   * @return stream of the opened file
   * @throws IOException if the operation was attempted and failed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,byteBufferPositionedReadable_readFully,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_readFully(java.io.InputStream,long,java.nio.ByteBuffer)",412,419,"/**
* Reads input stream into buffer starting at specified position. 
* @param in InputStream to read from
* @param position Starting position in the buffer
* @param buf ByteBuffer to fill with data
*/","* Delegate to {@code ByteBufferPositionedReadable#read(long, ByteBuffer)}.
   * @param in input stream
   * @param position position within file
   * @param buf the ByteBuffer to receive the results of the read operation.
   * @throws UnsupportedOperationException if the input doesn't implement
   * the interface or, if when invoked, it is raised.
   * Note: that is the default behaviour of {@code FSDataInputStream#readFully(long, ByteBuffer)}.
   * @throws IOException if the operation was attempted and failed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,checkIoStatisticsAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:checkIoStatisticsAvailable(),376,378,"/**
* Checks availability of I/O statistics.
*/","* Require a IOStatistics to be available.
   * @throws UnsupportedOperationException if the method was not found.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,checkIoStatisticsContextAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:checkIoStatisticsContextAvailable(),384,386,"/**
* Checks availability of IO statistics context.
* @throws RuntimeException if unavailable.","* Require IOStatisticsContext methods to be available.
   * @throws UnsupportedOperationException if the classes/methods were not found",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,<init>,org.apache.hadoop.util.ComparableVersion:<init>(java.lang.String),355,358,"/**
* Initializes a new ComparableVersion instance from a given string representation.
* @param version string in format ""major.minor.patch"" or similar variations
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,<init>,"org.apache.hadoop.util.LightWeightCache:<init>(int,int,long,long)",112,118,"/**
* Initializes a LightWeightCache instance with specified parameters.
* @param recommendedLength recommended cache length in units
* @param sizeLimit maximum allowed cache size in bytes
* @param creationExpirationPeriod expiration period for created items in seconds
* @param accessExpirationPeriod expiration period for accessed items in seconds
*/","* @param recommendedLength Recommended size of the internal array.
   * @param sizeLimit the limit of the size of the cache.
   *            The limit is disabled if it is &lt;= 0.
   * @param creationExpirationPeriod the time period C &gt; 0 in nanoseconds
   *            that the creation of an entry is expired if it is added to the
   *            cache longer than C.
   * @param accessExpirationPeriod the time period A &gt;= 0 in nanoseconds that
   *            the access of an entry is expired if it is not accessed
   *            longer than A.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,contains,org.apache.hadoop.util.LightWeightGSet$Values:contains(java.lang.Object),250,254,"/**
* Checks if this set contains the specified object.
* @param o object to search for, casted to key type K
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readFileToMap,"org.apache.hadoop.util.HostsFileReader:readFileToMap(java.lang.String,java.lang.String,java.util.Map)",123,128,"/**
* Reads file content into a HashMap.
* @param type file content type
* @param filename file to read from
* @param map target map to store file content
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refresh,"org.apache.hadoop.util.HostsFileReader:refresh(java.io.InputStream,java.io.InputStream)",236,259,"/**
 * Updates hosts list by reading from input streams and updating internal state.
 * @param inFileInputStream InputStream for included host file
 * @param exFileInputStream InputStream for excluded host file
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Filter.java,<init>,"org.apache.hadoop.util.bloom.Filter:<init>(int,int,int)",102,107,"/**
* Initializes a Filter object with specified parameters.
* @param vectorSize size of the data vector
* @param nbHash number of hash functions to use
* @param hashType type of hash function (implementation-dependent)
*/","* Constructor.
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash functions to consider.
   * @param hashType type of the hashing function (see {@link Hash}).",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Filter.java,readFields,org.apache.hadoop.util.bloom.Filter:readFields(java.io.DataInput),204,218,"/**
* Reads and initializes object fields from the given DataInput stream.
* @throws IOException if an I/O error occurs or unsupported version is encountered
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,delete,org.apache.hadoop.util.bloom.CountingBloomFilter:delete(org.apache.hadoop.util.bloom.Key),135,160,"/**
* Deletes a member with the given Key.
* @param key unique identifier of the member to delete
*/","* Removes a specified key from <i>this</i> counting Bloom filter.
   * <p>
   * <b>Invariant</b>: nothing happens if the specified key does not belong to <i>this</i> counter Bloom filter.
   * @param key The key to remove.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,membershipTest,org.apache.hadoop.util.bloom.DynamicBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key),175,188,"/**
* Performs membership test on the key against a matrix of sub-keys.
* @param key Key to be tested
* @return True if key is found in any sub-key, False otherwise
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(java.util.Collection),156,164,"/**
* Recursively adds false positives to the index by iterating over a collection of keys.
* @param coll Collection of Key objects to process
*/","* Adds a collection of false positive information to <i>this</i> retouched Bloom filter.
   * @param coll The collection of false positive.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(java.util.List),170,178,"/**
* Recursively adds false positives to the system.
* @param keys list of Key objects to process
*/","* Adds a list of false positive information to <i>this</i> retouched Bloom filter.
   * @param keys The list of false positive.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(org.apache.hadoop.util.bloom.Key[]),184,192,"/**
* Adds false positives to the system by processing an array of keys.
* @param keys array of unique identifiers
*/","* Adds an array of false positive information to <i>this</i> retouched Bloom filter.
   * @param keys The array of false positive.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,clearBit,org.apache.hadoop.util.bloom.RetouchedBloomFilter:clearBit(int),313,344,"/**
* Clears the specified index in the key and false positive vectors, 
* and resets the corresponding ratio value.
* @param index position to clear in the vector
*/","* Clears a specified bit in the bit vector and keeps up-to-date the KeyList vectors.
   * @param index The position of the bit to clear.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,ratioRemove,org.apache.hadoop.util.bloom.RetouchedBloomFilter:ratioRemove(int[]),294,307,"/**
* Finds the index of the hash with the minimum ratio.
* @return Index of the smallest ratio or a default value if none found
*/","* Chooses the bit position that minimizes the number of false negative generated while maximizing.
   * the number of false positive removed.
   * @param h The different bit positions.
   * @return The position that minimizes the number of false negative generated while maximizing.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,driver,org.apache.hadoop.util.ProgramDriver:driver(java.lang.String[]),155,159,"/**
* Exits application with error code if run() fails.
* @param argv array of command-line arguments
*/","* API compatible with Hadoop 1.x.
   *
   * @param argv argv.
   * @throws Throwable Anything thrown
   *                   by the example program's main",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String),130,132,"/**
* Adds a field to the builder with default justification and visibility.
* @param title field name
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,"org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,org.apache.hadoop.tools.TableListing$Justification)",134,136,"/**
 * Adds a field to the builder with the given title and justification.
 * @param title the field's title
 * @param justification the reason for adding this field
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,"org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,boolean)",138,140,"/**
* Specifies a field to be added to the builder with left justification.
* @param title field title
* @param wrap whether to wrap long titles
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getCredentialEntry,"org.apache.hadoop.conf.Configuration:getCredentialEntry(org.apache.hadoop.security.alias.CredentialProvider,java.lang.String)",2439,2469,"/**
* Retrieves CredentialEntry from the specified provider by name.
* @param provider credential provider
* @param name unique credential identifier
* @return CredentialEntry object or null if not found
*/","* Get the credential entry by name from a credential provider.
   *
   * Handle key deprecation.
   *
   * @param provider a credential provider
   * @param name alias of the credential
   * @return the credential entry or null if not found",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadResource,"org.apache.hadoop.conf.Configuration:loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)",3102,3147,"/**
* Loads resource by ID from properties and XML.
* @param properties Properties object to overlay with loaded values
* @param wrapper Resource wrapper object containing resource data
* @param quiet Whether to suppress errors or not
* @return Loaded Resource object or null if failed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String[],java.lang.String)",594,600,"/**
* Adds a single deprecation with the given key and message.
* @param key unique identifier for the deprecated item
* @param newKeys array of replacement keys
* @param customMessage custom deprecation message (optional)
*/","* Adds the deprecated key to the global deprecation map.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If a key is deprecated in favor of multiple keys, they are all treated as 
   * aliases of each other, and setting any one of them resets all the others 
   * to the new value.
   *
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   * 
   * @param key to be deprecated
   * @param newKeys list of keys that take up the values of deprecated key
   * @param customMessage depcrication message
   * @deprecated use {@link #addDeprecation(String key, String newKey,
      String customMessage)} instead",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,parseNext,org.apache.hadoop.conf.Configuration$Parser:parseNext(),3448,3466,"/**
* Advances through the XML stream and handles elements or tokens accordingly.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,openListeners,org.apache.hadoop.http.HttpServer2:openListeners(),1537,1552,"/**
* Binds and starts server listeners.
* @throws Exception on binding or starting failure
*/","* Open the main listener for the server
   * @throws Exception",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,checkArgs,org.apache.hadoop.ha.SshFenceByTcpPort:checkArgs(java.lang.String),73,78,"/**
* Validates fencing configuration string.
* @param argStr configuration string to parse or null for no validation
*/","* Verify that the argument, if given, in the conf is parseable.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,parseOpts,"org.apache.hadoop.ha.HAAdmin:parseOpts(java.lang.String,org.apache.commons.cli.Options,java.lang.String[])",505,507,"/**
* Parses command line arguments.
* @param cmdName name of the command
* @param opts options for parsing
* @param argv array of command line arguments
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,clearParentZNode,org.apache.hadoop.ha.ActiveStandbyElector:clearParentZNode(),407,427,"/**
* Recursively deletes the working directory's parent node in ZooKeeper.
* @throws IOException if deletion fails
*/","* Clear all of the state held within the parent ZNode.
   * This recursively deletes everything within the znode as well as the
   * parent znode itself. It should only be used when it's certain that
   * no electors are currently participating in the election.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException interrupted exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,fenceOldActive,org.apache.hadoop.ha.ActiveStandbyElector:fenceOldActive(),1016,1047,"/**
* Fetches and fences old active ZNode if necessary.
* @return Stat object or null if no old node found
*/","* If there is a breadcrumb node indicating that another node may need
   * fencing, try to fence that node.
   * @return the Stat of the breadcrumb node that was read, or null
   * if no breadcrumb node existed",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,createWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:createWithRetries(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode)",1082,1091,"/**
* Creates a ZooKeeper node with retries.
* @param path node path to create
* @param data byte array of node contents
* @param acl ACL list for node permissions
* @param mode creation mode (e.g. EPHEMERAL)
* @return created node path or null on failure
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,getDataWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:getDataWithRetries(java.lang.String,boolean,org.apache.zookeeper.data.Stat)",1093,1101,"/**
* Retrieves data from ZooKeeper with retries.
* @param path ZooKeeper node path
* @param watch whether to watch for changes
* @param stat statistics about the node
* @return byte array containing the node's data or null on failure
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,setDataWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:setDataWithRetries(java.lang.String,byte[],int)",1103,1111,"/**
* Sets data with retries on a ZooKeeper node.
* @param path node path
* @param data byte array to set
* @param version version of the data
* @return Stat object describing the update or null if failed
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,deleteWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:deleteWithRetries(java.lang.String,int)",1113,1122,"/**
* Deletes ZooKeeper node with specified path and version.
* @param path node path to delete
* @param version node version to delete
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readRangeFrom,"org.apache.hadoop.fs.VectoredReadUtils:readRangeFrom(org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.FileRange,java.util.function.IntFunction)",117,142,"/**
* Reads a file range from the given stream and returns the data as a ByteBuffer.
* @param stream input stream
* @param range FileRange object specifying the requested range
* @param allocate function to allocate a ByteBuffer with the correct size
* @return CompletableFuture containing the read ByteBuffer or null if an error occurs
*/","* Synchronously reads a range from the stream dealing with the combinations
   * of ByteBuffers buffers and PositionedReadable streams.
   * @param stream the stream to read from
   * @param range the range to read
   * @param allocate the function to allocate ByteBuffers
   * @return the CompletableFuture that contains the read data or an exception.
   * @throws IllegalArgumentException the range is invalid other than by offset or being null.
   * @throws EOFException the range offset is negative
   * @throws NullPointerException if the range is null.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,requestCaching,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestCaching(org.apache.hadoop.fs.impl.prefetch.BufferData),435,482,"/**
* Requests caching of a buffer by its block number.
* @param data BufferData object containing the block to cache
*/","* Requests that the given block should be copied to the local cache.
   * The block must not be accessed by the caller after calling this method
   * because it will released asynchronously relative to the caller.
   *
   * @throws IllegalArgumentException if data is null.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setPrefetch,org.apache.hadoop.fs.impl.prefetch.BufferData:setPrefetch(java.util.concurrent.Future),181,186,"/**
* Sets prefetch state with a future action.
* @param actionFuture Future object representing the prefetch action
*/","* Indicates that a prefetch operation is in progress.
   *
   * @param actionFuture the {@code Future} of a prefetch action.
   *
   * @throws IllegalArgumentException if actionFuture is null.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setReady,org.apache.hadoop.fs.impl.prefetch.BufferData:setReady(org.apache.hadoop.fs.impl.prefetch.BufferData$State[]),209,218,"/**
* Sets the state to READY and updates checksum.
* @param expectedCurrentState varargs of expected current states
*/","* Marks the completion of reading data into the buffer.
   * The buffer cannot be modified once in this state.
   *
   * @param expectedCurrentState the collection of states from which transition to READY is allowed.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getSize,org.apache.hadoop.fs.impl.prefetch.BlockData:getSize(int),154,164,"/**
* Calculates the size of a file block based on its number.
* @param blockNumber unique block identifier
* @return Size of the block in bytes or 0 if fileSize is zero
*/","* Gets the size of the given block.
   * @param blockNumber the id of the desired block.
   * @return the size of the given block.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getRelativeOffset,"org.apache.hadoop.fs.impl.prefetch.BlockData:getRelativeOffset(int,long)",194,198,"/**
* Calculates relative offset within a block.
* @param blockNumber unique block identifier
* @param offset absolute position to find relative offset for
* @return Relative offset as an integer, or throws exception if invalid offset
*/","* Gets the relative offset corresponding to the given block and the absolute offset.
   * @param blockNumber the id of the given block.
   * @param offset absolute offset in the file.
   * @return the relative offset corresponding to the given block and the absolute offset.
   * @throws IllegalArgumentException if either blockNumber or offset is invalid.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getStateString,org.apache.hadoop.fs.impl.prefetch.BlockData:getStateString(),225,241,"/**
* Formats a string representing the state sequence.
* @return A formatted string with each contiguous state group.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,<init>,"org.apache.hadoop.fs.impl.prefetch.BlockData:<init>(long,int)",75,95,"/**
* Initializes BlockData with fileSize and blockSize.
* @param fileSize total file size
* @param blockSize fixed block size
*/","* Constructs an instance of {@link BlockData}.
   * @param fileSize the size of a file.
   * @param blockSize the file is divided into blocks of this size.
   * @throws IllegalArgumentException if fileSize is negative.
   * @throws IllegalArgumentException if blockSize is negative.
   * @throws IllegalArgumentException if blockSize is zero or negative.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,blockNumber,org.apache.hadoop.fs.impl.prefetch.FilePosition:blockNumber(),194,197,"/**
 * Retrieves the current block number from the buffer.
 */","* Gets the id of the current block.
   *
   * @return the id of the current block.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,run,org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer:run(),3834,3841,"/**
* Closes all cache entries and logs any exceptions encountered.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem$Cache:closeAll(),3790,3792,"/**
* Closes all connections and resources.
* @throws IOException if an I/O error occurs
*/","* Close all FileSystems in the cache, whether they are marked for
     * automatic closing or not.
     * @throws IOException a problem arose closing one or more FileSystem.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAllForUGI,org.apache.hadoop.fs.FileSystem:closeAllForUGI(org.apache.hadoop.security.UserGroupInformation),653,657,"/**
* Closes all file system resources for the given User Group Information.
* @param ugi user group information
*/","* Close all cached FileSystem instances for a given UGI.
   * Be sure those filesystems are not used anymore.
   * @param ugi user group info to close
   * @throws IOException a problem arose closing one or more filesystem.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,readFully,"org.apache.hadoop.fs.FSInputStream:readFully(long,byte[])",135,139,"/**
* Reads fully into provided buffer starting from specified position.
* @param position offset in stream to begin reading from
* @param buffer destination array to store read data
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,readFully,"org.apache.hadoop.fs.BufferedFSInputStream:readFully(long,byte[],int,int)",120,123,"/**
* Reads fully from file stream into provided buffer.
* @param position current stream position
* @param buffer data storage array
* @param offset starting index in buffer
* @param length number of bytes to read
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence)",2063,2066,"/**
* Writes character sequence to a file at specified path.
* @param fileContext context for the file operation
* @param path file location
* @param charseq data to be written
*/","* Write a line of text to a file. Characters are encoded into bytes using
   * UTF-8. This utility method opens the file for writing, creating the file if
   * it does not exist, or overwrites an existing file.
   *
   * @param fileContext the files system with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,createFile,org.apache.hadoop.fs.HarFileSystem:createFile(org.apache.hadoop.fs.Path),1305,1308,"/**
* Creates a file output stream builder at the specified path.
* @param path file system path to create file at
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,byte[])",1851,1862,"/**
* Writes byte array to a file in the specified file system.
* @param fs target file system
* @param path destination file path
* @param bytes data to be written
* @return the modified file system
*/","* Writes bytes to a file. This utility method opens the file for writing,
   * creating the file if it does not exist, or overwrites an existing file. All
   * bytes in the byte array are written to the file.
   *
   * @param fs the file system with which to create the file
   * @param path the path to the file
   * @param bytes the byte array with the bytes to write
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)",1910,1928,"/**
* Writes a list of lines to a file on the specified file system.
* @param fs target file system
* @param path file location
* @param lines iterable of line data
* @param cs character set for encoding
* @return the modified file system
*/","* Write lines of text to a file. Each line is a char sequence and is written
   * to the file in sequence with each line terminated by the platform's line
   * separator, as defined by the system property {@code
   * line.separator}. Characters are encoded into bytes using the specified
   * charset. This utility method opens the file for writing, creating the file
   * if it does not exist, or overwrites an existing file.
   *
   * @param fs the file system with which to create the file
   * @param path the path to the file
   * @param lines a Collection to iterate over the char sequences
   * @param cs the charset to use for encoding
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence,java.nio.charset.Charset)",1983,1997,"/**
* Writes a CharSequence to a file in the given FileSystem.
* @param fs target FileSystem
* @param path file location
* @param charseq data to write
* @param cs character set for encoding
* @return the modified FileSystem
*/","* Write a line of text to a file. Characters are encoded into bytes using the
   * specified charset. This utility method opens the file for writing, creating
   * the file if it does not exist, or overwrites an existing file.
   *
   * @param fs the file system with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   * @param cs the charset to use for encoding
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createFile,org.apache.hadoop.fs.FilterFileSystem:createFile(org.apache.hadoop.fs.Path),699,702,"/**
 * Creates an output stream builder for a new file at the specified path.
 * @param path file system path to create the file at
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,appendFile,org.apache.hadoop.fs.HarFileSystem:appendFile(org.apache.hadoop.fs.Path),1310,1313,"/**
* Creates an output stream builder to append data to a file.
* @param path file system path to the target file
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,appendFile,org.apache.hadoop.fs.FilterFileSystem:appendFile(org.apache.hadoop.fs.Path),704,707,"/**
 * Creates an output stream builder to append data to a file at the specified path.
 * @param path location of the file to be appended to
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],long,long,boolean)",150,153,"/**
* Initializes a BlockLocation object with given parameters.
* @param names array of block names
* @param hosts array of host addresses
* @param topologyPaths array of topology paths (optional)
* @param offset offset value
* @param length length value
* @param corrupt indicates if the block is corrupted
*/","* Constructor with host, name, network topology, offset, length 
   * and corrupt flag.
   * @param names names.
   * @param hosts hosts.
   * @param topologyPaths topologyPaths.
   * @param offset offset.
   * @param length length.
   * @param corrupt corrupt.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpFs.java,getServerDefaults,org.apache.hadoop.fs.ftp.FtpFs:getServerDefaults(),60,64,"/**
* Retrieves server defaults from FTP configuration.
* @return FsServerDefaults object or null if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpFs.java,getServerDefaults,org.apache.hadoop.fs.ftp.FtpFs:getServerDefaults(org.apache.hadoop.fs.Path),66,69,"/**
* Retrieves server defaults from configuration.
* @return FsServerDefaults object; throws IOException if error occurs.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,getServerDefaults,org.apache.hadoop.fs.local.RawLocalFs:getServerDefaults(org.apache.hadoop.fs.Path),66,70,"/**
* Retrieves server defaults from LocalConfigKeys.
* @param f file path (not used in this implementation)
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,getServerDefaults,org.apache.hadoop.fs.local.RawLocalFs:getServerDefaults(),72,76,"/**
* Retrieves server defaults using LocalConfigKeys.
* @return FsServerDefaults object or null if unavailable
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getServerDefaults(),1135,1139,"/**
* Retrieves server defaults from local configuration.
* @return FsServerDefaults object or null if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getServerDefaults(org.apache.hadoop.fs.Path),1141,1144,"/**
* Returns server defaults based on local configuration.
* @param f ignored file path parameter
* @return FsServerDefaults object representing default settings
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults(),292,296,"/**
* Retrieves server defaults.
* @return FsServerDefaults object or null if not available
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults(org.apache.hadoop.fs.Path),298,307,"/**
* Retrieves FsServerDefaults for the given file path.
* @param f Path to fetch server defaults for
* @return FsServerDefaults object or default values if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(java.lang.String,java.lang.String,int)",93,95,"/**
* Creates a new mutable counter with initial integer value.
* @param name counter name
* @param desc counter description
* @param iVal initial integer value
*/","* Create a mutable integer counter
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new counter object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(java.lang.String,java.lang.String,long)",117,119,"/**
* Creates a new mutable counter with initial value and description.
* @param name unique identifier for the counter
* @param desc human-readable description of the counter
* @param iVal initial value of the counter
*/","* Create a mutable long integer counter
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new counter object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,long)",166,168,"/**
* Creates a new mutable gauge with initial value and description.
* @param name gauge identifier
* @param desc gauge description
* @param iVal initial gauge value
*/","* Create a mutable long integer gauge
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new gauge object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,float)",190,192,"/**
* Creates a new gauge with specified name and description.
* @param name unique identifier for the gauge
* @param desc human-readable description of the gauge
* @param iVal initial value of the gauge
*/","* Create a mutable float gauge
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new gauge object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,int)",142,144,"/**
* Creates a new gauge with specified initial value and description.
* @param name Gauge display name
* @param desc Gauge description
* @param iVal Initial gauge value
*/","* Create a mutable integer gauge
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new gauge object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,checkCalls,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:checkCalls(),126,143,"/**
* Finds the minimum wait time among pending asynchronous calls.
* @return minimum wait time in milliseconds or -1 if none found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeCompressedStringArray,"org.apache.hadoop.io.WritableUtils:writeCompressedStringArray(java.io.DataOutput,java.lang.String[])",149,158,"/**
* Writes a compressed array of strings to the output stream.
* @param out DataOutput stream
* @param s array of strings to compress and write
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,copy,org.apache.hadoop.fs.statistics.MeanStatistic:copy(),281,283,"/**
* Creates a deep copy of this MeanStatistic instance.
* @return A new MeanStatistic object with identical properties
*/","* Create a copy of this instance.
   * @return copy.
   *",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,toString,org.apache.hadoop.fs.statistics.IOStatisticsLogging$SourceToString:toString(),297,302,"/**
* Returns string representation of IO statistics source.
* @return null if source is null, otherwise string value from source.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,logIOStatisticsAtDebug,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(org.slf4j.Logger,java.lang.String,java.lang.Object)",227,238,"/**
* Logs IO statistics at debug level if enabled.
* @param log Logger instance for logging
* @param message Customizable log message prefix
* @param source Object containing IO statistics to be logged
*/","* Extract any statistics from the source and log at debug, if
   * the log is set to log at debug.
   * No-op if logging is not at debug or the source is null/of
   * the wrong type/doesn't provide statistics.
   * @param log log to log to
   * @param message message for log -this must contain ""{}"" for the
   * statistics report to actually get logged.
   * @param source source object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,toString,org.apache.hadoop.fs.FSInputStream:toString(),148,158,"/**
* Returns a string representation of the object, including IO statistics.
* @return A formatted string containing superclass and IO stats details
*/","* toString method returns the superclass toString, but if the subclass
   * implements {@link IOStatisticsSource} then those statistics are
   * extracted and included in the output.
   * That is: statistics of subclasses are automatically reported.
   * @return a string value.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_toPrettyString,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_toPrettyString(java.lang.Object),324,328,"/**
* Converts IOStatistics object to pretty string representation.
* @param statistics IOStatistics object or null for empty string
*/","* Convert IOStatistics to a string form, with all the metrics sorted
   * and empty value stripped.
   * @param statistics A statistics instance; may be null
   * @return string value or the empty string if null",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,measureDurationOfInvocation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:measureDurationOfInvocation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)",484,507,"/**
* Measures invocation duration using a DurationTracker.
* @param factory tracker factory for creating trackers
* @param statistic metric to track (e.g., ""read"", ""write"")
* @return measured duration in the format returned by TrackerFactory
*/","* Given an IOException raising callable/lambda expression,
   * execute it and update the relevant statistic,
   * returning the measured duration.
   *
   * {@link #trackDurationOfInvocation(DurationTrackerFactory, String, InvocationRaisingIOE)}
   * with the duration returned for logging etc.; added as a new
   * method to avoid linking problems with any code calling the existing
   * method.
   *
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @return the duration of the operation, as measured by the duration tracker.
   * @throws IOException IO failure.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,trackDurationOfSupplier,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfSupplier(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,java.util.function.Supplier)",642,663,"/**
* Tracks execution duration of a supplier function.
* @param factory DurationTrackerFactory instance (optional)
* @param statistic tracking statistic to record
* @return input function's return value or null if fails
*/","* Given a Java supplier, evaluate it while
   * tracking the duration of the operation and success/failure.
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @param <B> return type.
   * @return the output of the supplier.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,addToLinkedListAndEvictIfRequired,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToLinkedListAndEvictIfRequired(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry),421,439,"/**
* Adds entry to linked list and evicts oldest block if required.
* @param entry Entry object to add
*/","* Add the given entry to the head of the linked list and if the LRU cache size
   * exceeds the max limit, evict tail of the LRU linked list.
   *
   * @param entry Block entry to add.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,<init>,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:<init>(java.util.List,java.util.List,java.util.List,java.util.List,java.util.List)",92,140,"/**
* Initializes IO statistics store with dynamic counters and gauges.
* @param counters list of counter names
* @param gauges list of gauge names
* @param minimums list of minimum values
* @param maximums list of maximum values
* @param meanStatistics list of mean statistic functions
*/","* Constructor invoked via the builder.
   * @param counters keys to use for the counter statistics.
   * @param gauges names of gauges
   * @param minimums names of minimums
   * @param maximums names of maximums
   * @param meanStatistics names of mean statistics.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(byte[],int,int)",162,220,"/**
* Reads up to len bytes from the underlying stream into the given array.
* @param b byte array to read into
* @param off offset in b to start reading at
* @param len maximum number of bytes to read
* @return actual number of bytes read, or -1 on end-of-file
*/","* Decryption is buffer based.
   * If there is data in {@link #outBuffer}, then read it out of this buffer.
   * If there is no data in {@link #outBuffer}, then read more from the 
   * underlying stream and do the decryption.
   * @param b the buffer into which the decrypted data is read.
   * @param off the buffer offset.
   * @param len the maximum number of decrypted data bytes to read.
   * @return int the total number of decrypted data bytes read into the buffer.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(long,byte[],int,int)",395,425,"/**
* Decrypts data in-place within the provided buffer.
* @param position starting position for decryption
* @param buffer input/output data to be decrypted
* @param offset starting offset within the buffer
* @param length number of bytes to decrypt
*/","* Decrypt length bytes in buffer starting at offset. Output is also put 
   * into buffer starting at offset. It is thread-safe.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(long,java.nio.ByteBuffer,int,int)",456,499,"/**
* Decrypts data from a ByteBuffer using a provided Decryptor.
* @param filePosition position to start decryption
* @param buf input/output ByteBuffer
* @param length total bytes to decrypt
* @param start starting byte offset in the buffer
*/","* Decrypts the given {@link ByteBuffer} in place. {@code length} bytes are
   * decrypted from {@code buf} starting at {@code start}.
   * {@code buf.position()} and {@code buf.limit()} are unchanged after this
   * method returns. This method is thread-safe.
   *
   * <p>
   *   This method decrypts the input buf chunk-by-chunk and writes the
   *   decrypted output back into the input buf. It uses two local buffers
   *   taken from the {@link #bufferPool} to assist in this process: one is
   *   designated as the input buffer and it stores a single chunk of the
   *   given buf, the other is designated as the output buffer, which stores
   *   the output of decrypting the input buffer. Both buffers are of size
   *   {@link #bufferSize}.
   * </p>
   *
   * <p>
   *   Decryption is done by using a {@link Decryptor} and the
   *   {@link #decrypt(Decryptor, ByteBuffer, ByteBuffer, byte)} method. Once
   *   the decrypted data is written into the output buffer, is is copied back
   *   into buf. Both buffers are returned back into the pool once the entire
   *   buf is decrypted.
   * </p>
   *
   * @param filePosition the current position of the file being read
   * @param buf the {@link ByteBuffer} to decrypt
   * @param length the number of bytes in {@code buf} to decrypt
   * @param start the position in {@code buf} to start decrypting data from",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(java.nio.ByteBuffer,int,int)",649,670,"/**
* Decrypts a specified portion of the input buffer.
* @param buf ByteBuffer containing encrypted data
* @param length total number of bytes to decrypt
* @param start starting position within the ByteBuffer
*/","* Decrypts the given {@link ByteBuffer} in place. {@code length} bytes are
   * decrypted from {@code buf} starting at {@code start}.
   * {@code buf.position()} and {@code buf.limit()} are unchanged after this
   * method returns.
   *
   * @see #decrypt(long, ByteBuffer, int, int)",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,<init>,"org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long)",124,140,"/**
* Initializes a crypto input stream with specified codec, key, and IV.
* @param in underlying input stream
* @param codec encryption/decryption algorithm
* @param bufferSize buffer size for reading/writing data
* @param key encryption key
* @param iv initialization vector
* @param streamOffset offset into the encrypted stream
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,seek,org.apache.hadoop.crypto.CryptoInputStream:seek(long),523,546,"/**
* Seeks to a specific position in the stream.
* @param pos target position (must be non-negative)
* @throws IOException if an I/O error occurs
*/",Seek to a position.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,skip,org.apache.hadoop.crypto.CryptoInputStream:skip(long),549,577,"/**
* Skips specified number of bytes in the underlying stream.
* @param n positive skip length
* @return actual number of skipped bytes from user's point of view
*/",Skip n bytes,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,seekToNewSource,org.apache.hadoop.crypto.CryptoInputStream:seekToNewSource(long),693,705,"/**
* Seeks to a new source position if supported by the underlying stream.
* @param targetPos the desired offset
* @return true if successful, false otherwise
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,write,org.apache.hadoop.crypto.CryptoOutputStream:write(int),271,275,"/**
* Writes a single byte to the underlying output stream.
* @param b the byte value to be written
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,close,org.apache.hadoop.crypto.CryptoOutputStream:close(),238,256,"/**
* Closes the stream, releasing resources and flushing buffered data.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,hflush,org.apache.hadoop.crypto.CryptoOutputStream:hflush(),294,300,"/**
* Flushes output and syncs if out is a Syncable. 
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,hsync,org.apache.hadoop.crypto.CryptoOutputStream:hsync(),302,308,"/**
* Synchronizes output and calls HSYNC on underlying device, if applicable. 
* @throws IOException if synchronization fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/GlobFilter.java,compile,org.apache.hadoop.metrics2.filter.GlobFilter:compile(java.lang.String),36,39,"/**
* Compiles glob pattern from string.
* @param s glob pattern string to compile
* @return compiled Pattern object or null if invalid
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,<init>,org.apache.hadoop.fs.GlobFilter:<init>(java.lang.String),49,51,"/**
* Initializes GlobFilter with specified file pattern.
* @param filePattern glob pattern to filter files
*/","* Creates a glob filter with the specified file pattern.
   *
   * @param filePattern the file pattern.
   * @throws IOException thrown if the file pattern is incorrect.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,<init>,"org.apache.hadoop.fs.GlobFilter:<init>(java.lang.String,org.apache.hadoop.fs.PathFilter)",60,62,"/**
* Initializes a new GlobFilter instance with the specified file pattern and filter.
* @param filePattern glob pattern to match files
* @param filter additional filtering criteria
*/","* Creates a glob filter with the specified file pattern and an user filter.
   *
   * @param filePattern the file pattern.
   * @param filter user filter in addition to the glob pattern.
   * @throws IOException thrown if the file pattern is incorrect.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTar,"org.apache.hadoop.fs.FileUtil:unTar(java.io.InputStream,java.io.File,boolean)",985,1003,"/**
* Extracts a tar archive from an InputStream to a specified directory.
* @param inputStream input stream containing the tar archive
* @param untarDir target directory for extraction
* @param gzipped true if the archive is gzip-compressed
*/","* Given a Tar File as input it will untar the file in a the untar directory
   * passed as the second parameter
   *
   * This utility will untar "".tar"" files and "".tar.gz"",""tgz"" files.
   *
   * @param inputStream The tar file as input.
   * @param untarDir The untar directory where to untar the tar file.
   * @param gzipped The input stream is gzipped
   *                TODO Use magic number and PusbackInputStream to identify
   * @throws IOException an exception occurred
   * @throws InterruptedException command interrupted
   * @throws ExecutionException task submit failed",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAllStatistics,org.apache.hadoop.fs.FileContext:getAllStatistics(),2420,2422,"/**
* Retrieves all system-wide statistics as a map of URI to Statistics objects.
* @return A map of file system URIs to their respective statistics.","* @return Map of uri and statistics for each filesystem instantiated. The uri
   *         consists of scheme and authority for the filesystem.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,clearStatistics,org.apache.hadoop.fs.FileContext:clearStatistics(),2404,2406,"/**
* Clears statistics from the file system.
*/","* Clears all the statistics stored in AbstractFileSystem, for all the file
   * systems.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,primitiveCreate,"org.apache.hadoop.fs.FilterFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",551,559,"/**
* Delegates primitive file creation to the underlying FS implementation.
* @param f target file path
* @param ... remaining arguments passed through to FS implementation
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:<init>(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir,long,org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.fs.viewfs.InodeTree,org.apache.hadoop.conf.Configuration)",977,988,"/**
* Initializes an InternalDirOfViewFs object with provided parameters.
* @param dir directory in the inode tree
* @param cTime creation time of the view FS
* @param ugi user/group information
* @param uri URI for the view FS
* @param fsState current state of the file system
* @param conf configuration settings
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,<init>,org.apache.hadoop.fs.FilterFs:<init>(org.apache.hadoop.fs.AbstractFileSystem),63,66,"/**
* Initializes filter file system with provided abstract file system.
* @param fs AbstractFileSystem instance to wrap
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java,getPiggyBacksFromInput,"org.apache.hadoop.io.erasurecode.coder.util.HHUtil:getPiggyBacksFromInput(java.nio.ByteBuffer[],int[],int,int,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",64,120,"/**
* Retrieves piggyback data from input buffers using the provided encoder.
* @param inputs array of input ByteBuffer objects
* @param piggyBackIndex array of indices for piggyback creation
* @param numParityUnits number of parity units to process
* @param pgIndex index of parity unit to retrieve (0-based)
* @param encoder RawErasureEncoder instance used for encoding
* @return array of ByteBuffer objects containing retrieved piggybacks
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,encode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",146,150,"/**
* Encodes input and output chunks using provided encoding method.
* @param inputs array of ECChunks to be encoded
* @param outputs array of ECChunks to store encoded result
*/","* Encode with inputs and generates outputs. More see above.
   *
   * @param inputs input buffers to read data from
   * @param outputs output buffers to put the encoded data into, read to read
   *                after the call
   * @throws IOException if the encoder is closed.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable$Internal:<init>(java.lang.Object),163,165,"/**
* Constructs an internal object with the specified value.
* @param value the object to be constructed
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,acquireHelper,"org.apache.hadoop.fs.impl.prefetch.BufferPool:acquireHelper(int,boolean)",161,187,"/**
* Acquires or creates BufferData for the specified block number.
* @param blockNumber unique block identifier
* @param canBlock whether to block if not available
* @return BufferData object or null on failure
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,numAvailable,org.apache.hadoop.fs.impl.prefetch.BufferPool:numAvailable(),300,303,"/**
* Returns the number of available blocks in the pool.
* @return count of unused blocks
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,newInstance,"org.apache.hadoop.util.BlockingThreadPoolExecutorService:newInstance(int,int,long,java.util.concurrent.TimeUnit,java.lang.String)",122,148,"/**
* Creates a new instance of a blocking thread pool with customizable parameters.
* @param activeTasks number of active threads
* @param waitingTasks initial queue size
* @param keepAliveTime time to keep idle threads alive
* @param unit TimeUnit for keepAliveTime
* @param prefixName identifier for daemon threads
*/","* A thread pool that that blocks clients submitting additional tasks if
   * there are already {@code activeTasks} running threads and {@code
   * waitingTasks} tasks waiting in its queue.
   *
   * @param activeTasks maximum number of active tasks
   * @param waitingTasks maximum number of waiting tasks
   * @param keepAliveTime time until threads are cleaned up in {@code unit}
   * @param unit time unit
   * @param prefixName prefix of name for threads
   * @return BlockingThreadPoolExecutorService.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,setData,"org.apache.hadoop.fs.impl.prefetch.FilePosition:setData(org.apache.hadoop.fs.impl.prefetch.BufferData,long,long)",109,128,"/**
* Initializes data and buffer with provided values.
* @param bufferData BufferData object to initialize from
* @param startOffset starting offset for buffer data
* @param readOffset starting offset for reading data
*/","* Associates a buffer with this file.
   *
   * @param bufferData the buffer associated with this file.
   * @param startOffset Start offset of the buffer relative to the start of a file.
   * @param readOffset Offset where reading starts relative to the start of a file.
   *
   * @throws IllegalArgumentException if bufferData is null.
   * @throws IllegalArgumentException if startOffset is negative.
   * @throws IllegalArgumentException if readOffset is negative.
   * @throws IllegalArgumentException if readOffset is outside the range [startOffset, buffer end].",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_getCurrent,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_getCurrent(),261,263,"/**
* Retrieves current IO statistics context.
* @return The current IO statistics context object or null if unavailable
*/","* Get the context's {@link IOStatisticsContext} which
   * implements {@link IOStatisticsSource}.
   * This is either a thread-local value or a global empty context.
   * @return instance of {@link IOStatisticsContext}.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_reset,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_reset(),287,289,"/**
* Resets IO statistics context.
*/","* Reset the context's IOStatistics.
   * {@link IOStatisticsContext#reset()}",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_snapshot,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_snapshot(),296,298,"/**
 * Returns a snapshot of current IO statistics context.
 */","* Take a snapshot of the context IOStatistics.
   * {@link IOStatisticsContext#snapshot()}
   * @return an instance of {@link IOStatisticsSnapshot}.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_aggregate,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_aggregate(java.lang.Object),308,316,"/**
* Aggregates IO statistics from the given source into the current context.
* @param source object containing IO statistics to aggregate
* @return true if aggregation was successful, false otherwise
*/","* Aggregate into the IOStatistics context the statistics passed in via
   * IOStatistics/source parameter.
   * <p>
   * Returns false if the source is null or does not contain any statistics.
   * @param source implementation of {@link IOStatisticsSource} or {@link IOStatistics}
   * @return true if the the source object was aggregated.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,runParallel,org.apache.hadoop.util.functional.TaskPool$Builder:runParallel(org.apache.hadoop.util.functional.TaskPool$Task),385,519,"/**
* Runs a task in parallel for each item, handling failures and reverts.
* @throws E exception thrown by the task or its revert
* @throws IOException iterator failure
*/","* Parallel execution.
     * All tasks run within the same IOStatisticsContext as the
     * thread calling this method.
     * @param task task to execute
     * @param <E> exception which may be raised in execution.
     * @return true if the operation executed successfully
     * @throws E any exception raised.
     * @throws IOException IOExceptions raised by remote iterator or in execution.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_setThreadIOStatisticsContext,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_setThreadIOStatisticsContext(java.lang.Object),270,273,"/**
* Sets thread IO statistics context.
* @param statisticsContext IOStatisticsContext object or null to clear
*/","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,setStatisticsContext,org.apache.hadoop.util.functional.TaskPool$Builder:setStatisticsContext(),524,528,"/**
* Sets thread-level IO statistics context.
* @param ioStatisticsContext Thread-specific IO stats context
*/",* Set the statistics context for this thread.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,resetStatisticsContext,org.apache.hadoop.util.functional.TaskPool$Builder:resetStatisticsContext(),535,539,"/**
* Resets and clears thread IO statistics context. 
* @param none
*/","* Reset the statistics context if it was set earlier.
     * This unbinds the current thread from any statistics
     * context.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,processPath,org.apache.hadoop.fs.shell.find.Find:processPath(org.apache.hadoop.fs.shell.PathData),394,401,"/**
* Processes a path item, applying it immediately unless in depth-first mode.
* @param item PathData object to process
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,postProcessPath,org.apache.hadoop.fs.shell.find.Find:postProcessPath(org.apache.hadoop.fs.shell.PathData),403,410,"/**
* Posts process path data, applying it when in depth-first traversal mode.
* @param item the PathData to post-process
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Rmr:processOptions(java.util.LinkedList),174,178,"/**
* Adds a required option and passes remaining arguments to superclass. 
* @param args list of command-line options (modified in-place)
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processOptions,org.apache.hadoop.fs.shell.Ls$Lsr:processOptions(java.util.LinkedList),411,416,"/**
* Adds -R option and passes to superclass.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processOptions,org.apache.hadoop.fs.shell.FsUsage$Dus:processOptions(java.util.LinkedList),236,240,"/**
* Prepends '-s' flag to options and calls parent processOptions.
* @param args list of command-line arguments
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,displayError,org.apache.hadoop.fs.shell.Command:displayError(java.lang.Exception),474,493,"/**
* Handles and displays an error with logging and recursive display of the error message.
* @param e Exception object to be handled
*/","* Display an exception prefaced with the command name.  Also increments
   * the error count for the command which will result in a non-zero exit
   * code.
   * @param e exception to display",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getPathHandle,"org.apache.hadoop.fs.FileSystem:getPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",1050,1057,"/**
* Creates a path handle based on file status and optional parameters.
* @param stat FileStatus object
* @param opt HandleOpt array (optional)
* @return PathHandle object
*/","* Create a durable, serializable handle to the referent of the given
   * entity.
   * @param stat Referent in the target FileSystem
   * @param opt If absent, assume {@link HandleOpt#path()}.
   * @throws IllegalArgumentException If the FileStatus does not belong to
   *         this FileSystem
   * @throws UnsupportedOperationException If {@link #createPathHandle}
   *         not overridden by subclass.
   * @throws UnsupportedOperationException If this FileSystem cannot enforce
   *         the specified constraints.
   * @return path handle.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map,long)",1227,1230,"/**
* Constructs a ShellCommandExecutor with default logging enabled.
* @param execString command execution string
* @param dir working directory for the executor
* @param env environment variables to be used in the command execution
* @param timeout maximum allowed execution time in milliseconds
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,init,org.apache.hadoop.fs.CachingGetSpaceUsed:init(),90,102,"/**
* Initializes the system by potentially refreshing data and spawning a refresh thread.
* @param none
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,addToken,"org.apache.hadoop.security.Credentials:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)",121,137,"/**
* Adds a token to the map for a given Text alias.
* @param alias unique text identifier
* @param t Token object to add (or its private clone if existing token is private)
*/","* Add a token in the storage (in memory).
   * @param alias the alias for the key
   * @param t the token object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,encrypt,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:encrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",148,152,"/**
* Encrypts input data and writes result to output buffer.
* @param inBuffer input data to be encrypted (ByteBuffer)
* @param outBuffer output buffer for encrypted data (ByteBuffer) 
* @throws IOException if encryption fails
*/","* AES-CTR will consume all of the input data. It requires enough space in
     * the destination buffer to encrypt entire input buffer.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,decrypt,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:decrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",158,162,"/**
* Decrypts input data to output buffer.
* @param inBuffer encrypted data to decrypt
* @param outBuffer decrypted data to store
*/","*  AES-CTR will consume all of the input data. It requires enough space in
     * the destination buffer to decrypt entire input buffer.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,initialize,org.apache.hadoop.fs.viewfs.RegexMountPoint:initialize(),86,96,"/**
* Initializes configuration for mounting points by compiling the source pattern and populating variable-in-destination-path map.
* @throws IOException if an error occurs during initialization
*/","* Initialize regex mount point.
   *
   * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",149,164,"/**
* Resolves a child path against its parent, adding a trailing slash if necessary.
* @param parent the parent path
* @param child the child path to resolve
*/","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,org.apache.hadoop.fs.Path:<init>(java.lang.String),184,223,"/**
* Initializes a Path object from a string representation.
* @param pathString string containing path information
*/","* Construct a path from a String.  Path strings are URIs, but with
   * unescaped elements and some additional normalization.
   *
   * @param pathString the path string",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String,java.lang.String)",241,256,"/**
* Initializes a Path object from its components.
* @param scheme URI scheme
* @param authority URI authority
* @param path URI path
*/","* Construct a Path from components.
   *
   * @param scheme the scheme
   * @param authority the authority
   * @param path the path",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,toString,org.apache.hadoop.fs.shell.PathData:toString(),461,464,"/**
* Converts URI to string representation.
* @return stringified URI or null if invalid
*/","* Returns the printable version of the path that is either the path
   * as given on the commandline, or the full path
   * @return String of the path",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,checkNotRelative,org.apache.hadoop.fs.Path:checkNotRelative(),92,96,"/**
* Validates that the path is not relative.
* @throws HadoopIllegalArgumentException if path is relative
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getUriPath,org.apache.hadoop.fs.AbstractFileSystem:getUriPath(org.apache.hadoop.fs.Path),423,431,"/**
* Extracts the file system path of a given Path object.
* @param p input Path to extract path from
* @return the extracted path or throws InvalidPathException if invalid
*/","* Get the path-part of a pathname. Checks that URI matches this file system
   * and that the path-part is a valid name.
   * 
   * @param p path
   * 
   * @return path-part of the Path p",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,resolvePath,org.apache.hadoop.fs.AbstractFileSystem:resolvePath(org.apache.hadoop.fs.Path),506,510,"/**
* Resolves a given file system path.
* @param p the input path to resolve
* @return the resolved path or throws an exception if unresolved
*/","* Return the fully-qualified path of path f resolving the path
   * through any internal symlinks or mount point
   * @param p path to be resolved
   * @return fully qualified path 
   * @throws FileNotFoundException when file not find throw.
   * @throws AccessControlException when accees control error throw.
   * @throws IOException raised on errors performing I/O.
   * @throws UnresolvedLinkException if symbolic link on path cannot be
   * resolved internally",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,create,"org.apache.hadoop.fs.AbstractFileSystem:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",530,640,"/**
* Creates an FSDataOutputStream with specified options.
* @param f the path to create
* @param createFlag flags for creating file
* @param opts optional creation options (e.g. block size, buffer size)
* @return FSDataOutputStream instance
*/","* The specification of this method matches that of
   * {@link FileContext#create(Path, EnumSet, Options.CreateOpts...)} except
   * that the Path f must be fully qualified and the permission is absolute
   * (i.e. umask has been applied).
   *
   * @param f the path.
   * @param createFlag create_flag.
   * @param opts create ops.
   * @throws AccessControlException access controll exception.
   * @throws FileAlreadyExistsException file already exception.
   * @throws FileNotFoundException file not found exception.
   * @throws ParentNotDirectoryException parent not dir exception.
   * @throws UnsupportedFileSystemException unsupported file system exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.
   * @return output stream.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,checkPath,org.apache.hadoop.fs.FilterFs:checkPath(org.apache.hadoop.fs.Path),184,187,"/**
* Verifies existence of file system path.
* @param path Path object to be checked
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path),1505,1510,"/**
* Deletes a file by ID.
* @param f the file path to delete
* @return true if deletion was successful, false otherwise
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,applyUMask,"org.apache.hadoop.fs.permission.FsCreateModes:applyUMask(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",43,49,"/**
* Applies a user mask to the given file system permission.
* @param mode original FS permission
* @param umask user mask to apply
*/","* Create from unmasked mode and umask.
   *
   * @param mode mode.
   * @param umask umask.
   * @return If the mode is already
   * an FsCreateModes object, return it.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,loadPermissionInfoByNativeIO,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfoByNativeIO(),1063,1085,"/**
* Loads permission info from native I/O.
* @throws IOException if file access fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,mkdirs,org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.Path),2495,2497,"/**
* Creates a directory with default permissions.
* @param f Path to create
*/","* Call {@link #mkdirs(Path, FsPermission)} with default permission.
   * @param f path
   * @return true if the directory was created
   * @throws IOException IO failure",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set)",160,190,"/**
* Constructs FileStatus object from provided attributes.
* @param length total file size in bytes
* @param isdir true for directories, false for files or symlinks
* @param block_replication block replication count
* @param blocksize block size in bytes
* @param modification_time last modification timestamp
* @param access_time last access timestamp
* @param permission file permissions (null implies default)
* @param owner file owner name (null implies empty string)
* @param group file group name (null implies empty string)
* @param symlink path to symbolic link target (null implies no symlink)
* @param path file system path (required for directories and files)
* @param attr additional file attributes
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,setPermission,org.apache.hadoop.fs.FileStatus:setPermission(org.apache.hadoop.fs.permission.FsPermission),367,370,"/**
* Sets the file system permission.
* @param permission FsPermission object or null to reset to default
*/","* Sets permission.
   * @param permission if permission is null, default value is set",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,getPermission,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getPermission(),111,116,"/**
* Returns the file system permissions for this object.
* @return FsPermission object or default permissions if not set
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createNonRecursive,"org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1432,1438,"/**
* Creates a non-recursive FSDataOutputStream on the specified file.
* @param f target file path
*/","* Opens an FSDataOutputStream at the indicated Path with write-progress
   * reporting. Same as create(), except fails if parent directory doesn't
   * already exist.
   * @param f the file name to open
   * @param overwrite if a file with this name already exists, then if true,
   * the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize block size
   * @param progress the progress reporter
   * @throws IOException IO failure
   * @see #setPermission(Path, FsPermission)
   * @return output stream.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,getPermission,org.apache.hadoop.fs.FSDataOutputStreamBuilder:getPermission(),146,151,"/**
* Returns default file permissions.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,createImmutable,org.apache.hadoop.fs.permission.FsPermission:createImmutable(short),64,66,"/**
* Creates an immutable file system permission with the specified value.
* @param permission short value representing the permission
*/","* Create an immutable {@link FsPermission} object.
   * @param permission permission.
   * @return FsPermission.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processPath,org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processPath(org.apache.hadoop.fs.shell.PathData),77,104,"/**
* Prints file metadata and ACL information.
* @param item PathData object containing file stats
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,createPermissions,org.apache.hadoop.security.alias.KeyStoreProvider:createPermissions(java.lang.String),68,71,"/**
 * Creates file system permissions from a string representation.
 * @param perms string of permissions (e.g. ""rwxr-x"") 
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$Writer:append(java.lang.Object,java.lang.Object)",1469,1502,"/**
* Appends a serialized key-value pair to the output stream.
* @param key unique identifier
* @param val associated value
* @throws IOException on serialization or write errors
*/","* Append a key/value pair.
     * @param key input Object key.
     * @param val input Object val.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,appendRaw,"org.apache.hadoop.io.SequenceFile$Writer:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)",1504,1517,"/**
* Appends raw data to the output stream with a key and compressed value.
* @param keyData byte array containing the key data
* @param keyOffset offset into keyData for writing
* @param keyLength length of key in bytes
* @param val ValueBytes object representing the compressed value
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressedSize,org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:getCompressedSize(),242,244,"/**
* Returns the compressed size of the block state.
*/","* Get the compressed size of the block in progress.
       * 
       * @return the number of compressed bytes written to the underlying FS
       *         file. The size may be smaller than actual need to compress the
       *         all data written due to internal buffering inside the
       *         compressor.
       * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,skip,org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:skip(long),528,536,"/**
* Skips a specified number of bytes in the underlying stream.
* @param n number of bytes to skip
* @return actual number of bytes skipped
*/","* Skips over and discards <code>n</code> bytes of data from the
     * input stream.
     *
     *The <code>skip</code> method skips over some smaller number of bytes
     * when reaching end of file before <code>n</code> bytes have been skipped.
     * The actual number of bytes skipped is returned.  If <code>n</code> is
     * negative, no bytes are skipped.
     *
     * @param      n   the number of bytes to be skipped.
     * @return     the actual number of bytes skipped.
     * @exception  IOException  if an I/O error occurs.
     *             ChecksumException if the chunk to skip to is corrupted",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,seek,org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:seek(long),550,556,"/**
* Seeks to a specified position in the file.
* @param pos target position in bytes
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processPaths,"org.apache.hadoop.fs.shell.Ls:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])",270,283,"/**
* Processes path data for display.
* @param parent parent PathData object
* @param items array of child PathData objects to process
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getUsed,org.apache.hadoop.fs.HarFileSystem:getUsed(org.apache.hadoop.fs.Path),1277,1280,"/**
 * Retrieves the used disk space in bytes for the given file system path.
 * @param path the file system path to query
 * @return the amount of used disk space in bytes, or -1 if an error occurs
 */",Return the total size of all files from a specified path.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getUsed,org.apache.hadoop.fs.FilterFileSystem:getUsed(org.apache.hadoop.fs.Path),421,424,"/**
 * Retrieves the used disk space in bytes for the specified file system location.
 * @param path file system path to query
 */",Return the total size of all files from a specified path.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,main,org.apache.hadoop.util.JvmPauseMonitor:main(java.lang.String[]),221,231,"/**
* Continuously adds integers to a list, simulating an infinite loop.
*/","* Simple 'main' to facilitate manual testing of the pause monitor.
   * 
   * This main function just leaks memory into a list. Running this class
   * with a 1GB heap will very quickly go into ""GC hell"" and result in
   * log messages about the GC pauses.
   *
   * @param args args.
   * @throws Exception Exception.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,start,org.apache.hadoop.service.AbstractService:start(),185,208,"/**
* Starts the service if not already running.
*/","* {@inheritDoc}
   * @throws ServiceStateException if the current service state does not permit
   * this action",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,enterState,org.apache.hadoop.service.AbstractService:enterState(org.apache.hadoop.service.Service$STATE),440,449,"/**
* Enters a new state, recording the transition and updating service state.
* @param newState new state to enter
*/","* Enter a state; record this via {@link #recordLifecycleEvent}
   * and log at the info level.
   * @param newState the proposed new state
   * @return the original state
   * it wasn't already in that state, and the state model permits state re-entrancy.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printDefaultRealm,org.apache.hadoop.security.KDiag:printDefaultRealm(),488,512,"/**
* Retrieves and prints the default Kerberos realm.
* @throws KerberosDiagsFailure if getDefaultRealm() fails
*/","* Get the default realm.
   * <p>
   * Not having a default realm may be harmless, so is noted at info.
   * All other invocation failures are downgraded to warn, as
   * follow-on actions may still work.
   * Failure to invoke the method via introspection is considered a failure,
   * as it's a sign of JVM compatibility issues that may have other 
   * consequences",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,equals,org.apache.hadoop.io.BytesWritable:equals(java.lang.Object),200,205,"/**
* Checks equality with another object.
* @param right_obj object to compare with
* @return true if equal, false otherwise
*/",* Are the two byte sequences equal?,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,equals,org.apache.hadoop.io.Text:equals(java.lang.Object),415,420,"/**
* Checks if this object is equal to another Text object.
* @param o the object to compare with (null or not an instance of Text returns false) 
*/","* Returns true iff <code>o</code> is a Text with the same length and same
   * contents.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,hashCode,org.apache.hadoop.security.token.Token$PrivateToken:hashCode(),299,304,"/**
* Calculates hash code by combining superclass and public service hash codes.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,set,org.apache.hadoop.io.BytesWritable:set(org.apache.hadoop.io.BytesWritable),167,169,"/**
* Sets user profile data from BytesWritable object.
* @param newData new profile data to be set
*/","* Set the BytesWritable to the contents of the given newData.
   *
   * @param newData the value to set this BytesWritable to.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirInternal,org.apache.hadoop.util.DiskChecker:checkDirInternal(java.io.File),94,101,"/**
* Checks internal directory existence and access.
* @param dir File object representing the directory to check
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,mlock,"org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:mlock(java.lang.String,java.nio.ByteBuffer,long)",280,283,"/**
* Locks memory region into RAM using POSIX mlock.
* @param identifier unique identifier (not used in this implementation)
* @param buffer ByteBuffer to lock
* @param len length of the memory block to lock
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,flushBuffer,"org.apache.hadoop.fs.FSOutputSummer:flushBuffer(boolean,boolean)",159,176,"/**
* Flushes buffer and updates count based on flush options.
* @param keep whether to retain partial data in buffer
* @param flushPartial whether to flush entire buffer or only full checksum chunks
* @return number of unflushed bytes left in buffer
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,close,org.apache.hadoop.crypto.CryptoInputStream:close(),319,329,"/**
* Closes this object, releasing any underlying resources.
* Ensures that all buffers and codecs are properly released.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",85,116,"/**
* Decodes input byte buffers and writes output to corresponding outputs.
* @param inputs array of ByteBuffer objects to decode from
* @param erasedIndexes array of indices for erasure in outputs
* @param outputs array of ByteBuffer objects to write decoded data to
*/","* Decode with inputs and erasedIndexes, generates outputs.
   * How to prepare for inputs:
   * 1. Create an array containing data units + parity units. Please note the
   *    data units should be first or before the parity units.
   * 2. Set null in the array locations specified via erasedIndexes to indicate
   *    they're erased and no data are to read from;
   * 3. Set null in the array locations for extra redundant items, as they're
   *    not necessary to read when decoding. For example in RS-6-3, if only 1
   *    unit is really erased, then we have 2 extra items as redundant. They can
   *    be set as null to indicate no data will be used from them.
   *
   * For an example using RS (6, 3), assuming sources (d0, d1, d2, d3, d4, d5)
   * and parities (p0, p1, p2), d2 being erased. We can and may want to use only
   * 6 units like (d1, d3, d4, d5, p0, p2) to recover d2. We will have:
   *     inputs = [null(d0), d1, null(d2), d3, d4, d5, p0, null(p1), p2]
   *     erasedIndexes = [2] // index of d2 into inputs array
   *     outputs = [a-writable-buffer]
   *
   * Note, for both inputs and outputs, no mixing of on-heap buffers and direct
   * buffers are allowed.
   *
   * If the coder option ALLOW_CHANGE_INPUTS is set true (false by default), the
   * content of input buffers may change after the call, subject to concrete
   * implementation.
   *
   * @param inputs input buffers to read data from. The buffers' remaining will
   *               be 0 after decoding
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to put decoded data into according to
   *                erasedIndexes, ready for read after the call
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(byte[][],int[],byte[][])",135,145,"/**
* Decodes input byte arrays using provided decoding state.
* @param inputs input byte arrays to decode
* @param erasedIndexes array of indexes that were erased during encoding
* @param outputs decoded output byte arrays
*/","* Decode with inputs and erasedIndexes, generates outputs. More see above.
   *
   * @param inputs input buffers to read data from
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to put decoded data into according to
   *                erasedIndexes, ready for read after the call
   * @throws IOException if the decoder is closed.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,40,"/**
* Creates an instance of RawErasureDecoder using provided ErasureCoderOptions.
* @param coderOptions configuration for the decoder
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,35,"/**
* Creates a legacy raw encoder instance with specified options.
* @param coderOptions Erasure Coder configuration settings
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,prepareDecoding,"org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:prepareDecoding(java.lang.Object[],int[])",103,115,"/**
* Updates decoding state with new input erasures.
* @param inputs array of encoded values to decode
* @param erasedIndexes indices of erased values in inputs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,skipToNextBlockMarker,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextBlockMarker(),328,332,"/**
* Skips to next block marker in input stream.
* @throws IOException if reading fails
*/","* Skips bytes in the stream until the start marker of a block is reached
   * or end of stream is reached. Used for testing purposes to identify the
   * start offsets of blocks.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,complete,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:complete(),591,599,"/**
* Completes parsing and verifies CRC, transitioning to EOF state.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,getAndMoveToFrontDecode,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getAndMoveToFrontDecode(),821,1004,"/**
* Processes the decompression tables and reads data from the input stream.
* @throws IOException if an error occurs during processing
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,<init>,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:<init>(java.io.OutputStream),598,600,"/**
* Initializes the CBZip2OutputStream with the specified output stream and compression block size.
* @param out the output stream to use
*/","* Constructs a new <tt>CBZip2OutputStream</tt> with a blocksize of 900k.
  *
  * <p>
  * <b>Attention: </b>The caller is resonsible to write the two BZip2 magic
  * bytes <tt>""BZ""</tt> to the specified stream prior to calling this
  * constructor.
  * </p>
  *
  * @param out *
  *            the destination stream.
  *
  * @throws IOException
  *             if an I/O error occurs in the specified stream.
  * @throws NullPointerException
  *             if <code>out == null</code>.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,endBlock,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:endBlock(),788,831,"/**
* Finalizes the current block with CRC and marker.
* @throws IOException if output operation fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",60,62,"/**
 * Initializes a block decompressor stream with input and decompression logic.
 * @param in input stream to read from
 * @param decompressor decompression strategy to use
 */","* Create a {@link BlockDecompressorStream}.
   * 
   * @param in input stream
   * @param decompressor decompressor to use
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,createInputStream,org.apache.hadoop.io.compress.PassthroughCodec:createInputStream(java.io.InputStream),132,136,"/**
 * Creates input stream with default compression settings.
 * @param in input stream to compress
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,read,org.apache.hadoop.io.compress.DecompressorStream:read(),89,93,"/**
* Reads a single byte from the stream.
* @return The byte value or -1 if end-of-stream reached
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,skip,org.apache.hadoop.io.compress.DecompressorStream:skip(long),193,213,"/**
* Skips 'n' bytes from the stream.
* @param n number of bytes to skip
* @return actual number of bytes skipped
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(),78,82,"/**
* Initializes ZStandard compressor with default settings.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:getCompressor(),285,308,"/**
* Retrieves a Compressor instance from CodecPool, handling finished compressors and logging warnings/errors as necessary.
* @return a valid Compressor object or null if unavailable
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createOutputStream,org.apache.hadoop.io.compress.DefaultCodec:createOutputStream(java.io.OutputStream),53,58,"/**
* Creates an instance of CompressionOutputStream with codec pool.
* @param out OutputStream to compress
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createOutputStream,org.apache.hadoop.io.compress.Lz4Codec:createOutputStream(java.io.OutputStream),66,71,"/**
* Creates a compression output stream based on codec pool configuration. 
* @param out target output stream
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out the location for the final output stream
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createOutputStream,org.apache.hadoop.io.compress.BZip2Codec:createOutputStream(java.io.OutputStream),105,110,"/**
* Creates a compression output stream using the codec pool. 
* @param out target output stream","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out        the location for the final output stream
   * @return a stream the user can write uncompressed data to, to have it 
   *         compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createOutputStream,org.apache.hadoop.io.compress.ZStandardCodec:createOutputStream(java.io.OutputStream),121,126,"/**
* Creates an output stream for compression with codec pool.
* @param out target output stream
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out the location for the final output stream
   * @return a stream the user can write uncompressed data to have compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createOutputStream,org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream),44,49,"/**
* Creates a compression-enabled output stream.
* @param out underlying OutputStream to wrap
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createOutputStream,org.apache.hadoop.io.compress.SnappyCodec:createOutputStream(java.io.OutputStream),66,71,"/**
* Creates a compression output stream with codec pool.
* @param out target output stream
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out the location for the final output stream
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,close,org.apache.hadoop.io.compress.CompressorStream:close(),102,112,"/**
* Closes this resource and marks it as closed.
* Ensures superclass resources are properly closed before marking this one as closed.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,close,org.apache.hadoop.io.MapFile$Writer:close(),385,389,"/**
* Closes and releases all underlying resources (data and index).
* @throws IOException if an I/O error occurs during closure
*/",Close the map.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,finish,org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:finish(),178,188,"/**
* Closes output stream and returns compressor resources.
* @throws IOException on write or close errors
*/",* Finishing up the current block.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createInputStream,org.apache.hadoop.io.compress.DefaultCodec:createInputStream(java.io.InputStream),79,84,"/**
* Creates a compression-aware input stream from the given InputStream.
* @param in underlying input stream to be compressed
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createInputStream,org.apache.hadoop.io.compress.Lz4Codec:createInputStream(java.io.InputStream),130,135,"/**
* Creates an input stream with compression support.
* @param in underlying input stream
*/","* Create a {@link CompressionInputStream} that will read from the given
   * input stream.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createInputStream,org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream),160,165,"/**
* Creates an input stream for compression using codec pool.
* @param in input stream to compress
* @return CompressionInputStream object or null if failed
*/","* Create a {@link CompressionInputStream} that will read from the given
   * input stream and return a stream for uncompressed data.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createInputStream,org.apache.hadoop.io.compress.ZStandardCodec:createInputStream(java.io.InputStream),178,183,"/**
* Creates a compression-aware input stream from the given InputStream.
* @param in target InputStream to compress
*/","* Create a {@link CompressionInputStream} that will read from the given
   * input stream.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createInputStream,org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream),76,81,"/**
* Creates a compression-aware input stream from the given InputStream.
* @param in the InputStream to compress
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createInputStream,org.apache.hadoop.io.compress.SnappyCodec:createInputStream(java.io.InputStream),127,132,"/**
* Creates an input stream with compression codec pool.
* @param in InputStream to compress
*/","* Create a {@link CompressionInputStream} that will read from the given
   * input stream.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,close,org.apache.hadoop.io.compress.DecompressorStream:close(),221,230,"/**
* Closes this resource and releases any system resources associated with it.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,close,org.apache.hadoop.fs.shell.Display$TextRecordInputStream:close(),259,263,"/**
 * Closes the underlying reader and superclass resources.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:close(),3181,3191,"/**
* Closes input and output streams, releasing system resources.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,close,org.apache.hadoop.io.MapFile$Reader:close(),881,887,"/**
* Closes the underlying index and data streams.
* @throws IOException if an I/O error occurs during closing
*/","* Close the map.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:close(),3877,3880,"/**
 * Closes input stream and releases resources.
 */",closes the underlying reader,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,finish,org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:finish(),532,539,"/**
* Closes input stream and releases decompression resources.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createCompressor,org.apache.hadoop.io.compress.DefaultCodec:createCompressor(),74,77,"/**
 * Creates and returns an instance of Zlib-based compressor.
 * @return Compressor object or null if creation fails.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createDirectDecompressor,org.apache.hadoop.io.compress.DefaultCodec:createDirectDecompressor(),108,111,"/**
* Creates and returns an instance of DirectDecompressor.
* @return DirectDecompressor object initialized with configuration from conf.",* {@inheritDoc},,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createDecompressor,org.apache.hadoop.io.compress.DefaultCodec:createDecompressor(),100,103,"/**
 * Creates and returns an instance of Decompressor using ZlibFactory.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeFileHeader,org.apache.hadoop.io.SequenceFile$Writer:writeFileHeader(),1274,1289,"/**
* Writes file header to output stream.
* @throws IOException if I/O error occurs during writing
*/",Write and flush the file header.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,write,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:write(java.io.DataOutput),217,229,"/**
* Writes object data to output stream, enforcing string length limits.
* @throws IOException if owner, renewer or realuser exceeds max length
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,write,org.apache.hadoop.security.Credentials:write(java.io.DataOutput),354,371,"/**
* Writes token and secret key data to output stream.
* @throws IOException on write failure
*/","* Stores all the keys to DataOutput.
   * @param out DataOutput.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,storeDelegationKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),681,684,"/**
* Stores delegation key in storage.
* @param key DelegationKey object to be stored
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,updateDelegationKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),686,689,"/**
* Updates the delegation key with the provided value.
* @param key DelegationKey object to be updated
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readFields,org.apache.hadoop.io.SequenceFile$Metadata:readFields(java.io.DataInput),771,783,"/**
* Reads file metadata from input stream into a TreeMap.
* @throws IOException if invalid size or I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,readFields,org.apache.hadoop.security.token.Token:readFields(java.io.DataInput),307,321,"/**
* Reads fields from DataInput stream.
* @throws IOException on read failures
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,readFields,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:readFields(java.io.DataInput),189,203,"/**
* Reads delegation token fields from DataInput stream.
* @throws IOException if unknown version or read failure
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readBlock,org.apache.hadoop.io.SequenceFile$Reader:readBlock(),2294,2330,"/**
* Reads a block of records from the input stream.
* @throws IOException if file is corrupt or reading fails
*/",Read the next 'compressed' block,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,seekToCurrentValue,org.apache.hadoop.io.SequenceFile$Reader:seekToCurrentValue(),2336,2369,"/**
* Skips to the current value in compressed or uncompressed block.
* @throws IOException if seeking fails
*/","* Position valLenIn/valIn to the 'value' 
     * corresponding to the 'current' key",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,createTokenInfo,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:createTokenInfo(byte[]),262,271,"/**
* Creates DelegationTokenInformation object from serialized bytes.
* @param tokenInfoBytes serialized token information
* @return DelegationTokenInformation object or null on parsing failure
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,readFields,org.apache.hadoop.fs.permission.PermissionStatus:readFields(java.io.DataInput),96,101,"/**
* Reads user credentials from DataInput stream.
* @throws IOException if I/O operation fails
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readString,org.apache.hadoop.io.Text:readString(java.io.DataInput),556,558,"/**
* Reads a string from the specified DataInput up to a maximum length.
* @param in input stream
*/","* @return Read a UTF8 encoded string from in.
   * @param in input in.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,getDelegationKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getDelegationKey(int),385,412,"/**
* Retrieves DelegationKey by ID from local cache or SQL database.
* @param keyId unique identifier for the DelegationKey
* @return DelegationKey object or null if not found
*/","* Obtains the DelegationKey from the SQL database.
   * @param keyId KeyId of the DelegationKey to obtain.
   * @return DelegationKey that matches the given keyId or null
   *         if it doesn't exist in the database.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,processKeyAddOrUpdate,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processKeyAddOrUpdate(byte[]),391,397,"/**
* Reads and processes a delegation key from the provided byte array.
* @param data raw key data
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getKeyFromZK,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getKeyFromZK(int),579,598,"/**
* Retrieves a DelegationKey object from ZooKeeper by ID.
* @param keyId unique identifier for the DelegationKey
* @return DelegationKey object or null if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:write(java.io.DataOutput),843,848,"/**
* Writes compressed data to output stream.
* @param out DataOutput instance
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,write,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:write(java.io.DataOutput),2098,2102,"/**
* Writes API version and metadata to output stream.
* @param out DataOutput stream
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$DataIndex:write(java.io.DataOutput),897,905,"/**
* Writes block regions to output stream.
* @param out DataOutput stream
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,selectDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:selectDelegationToken(java.net.URL,org.apache.hadoop.security.Credentials)",344,354,"/**
* Retrieves a delegation token for the specified URL and credentials.
* @param url target URL
* @param creds user credentials
* @return Delegation token or null if not available
*/","* Select a delegation token from all tokens in credentials, based on url.
   *
   * @param url url.
   * @param creds credentials.
   * @return token.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getServerToken,org.apache.hadoop.security.SaslRpcClient:getServerToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),279,293,"/**
* Retrieves a server token based on the provided authentication type.
* @param authType SaslAuth type to use for token selection
* @return Token object or null if protocol does not support tokens
*/","* Try to locate the required token for the server.
   * 
   * @param authType of the SASL client
   * @return Token for server, or null if no token available
   * @throws IOException - token selector cannot be instantiated",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setTokenService,"org.apache.hadoop.security.SecurityUtil:setTokenService(org.apache.hadoop.security.token.Token,java.net.InetSocketAddress)",456,466,"/**
* Sets the token service for a given token, or logs a warning if token is null.
* @param token The token to set service for (may be null)
* @param addr The InetSocketAddress of the token service
*/","* Set the given token's service to the format expected by the RPC client 
   * @param token a delegation token
   * @param addr the socket for the rpc connection",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,deleteKey,org.apache.hadoop.crypto.key.UserProvider:deleteKey(java.lang.String),102,113,"/**
* Deletes a key by ID with all versions.
* @param name unique key identifier
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.UserProvider:rollNewVersion(java.lang.String,byte[])",115,131,"/**
* Generates a new key version and adds it to the credentials store.
* @param name unique key identifier
* @param material byte array representing the key
* @return KeyVersion object containing the new key information
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.UserProvider:getKeyVersions(java.lang.String),167,181,"/**
* Retrieves a list of key versions for the given name.
* @param name unique identifier
* @return List of KeyVersion objects or empty list if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,tokenFromProto,org.apache.hadoop.ipc.ProtobufHelper:tokenFromProto(org.apache.hadoop.security.proto.SecurityProtos$TokenProto),114,117,"/**
* Converts TokenProto to Token using Protobuf helper.
* @param tokenProto TokenProto object
* @return Token object or null if conversion fails
*/","* Get a token from a TokenProto payload.
   * @param tokenProto marshalled token
   * @return the token.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getKeys,org.apache.hadoop.crypto.key.UserProvider:getKeys(),155,165,"/**
* Retrieves a list of secret key identifiers without '@' symbol.
* @return List of String key identifiers or empty list if none found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,write,org.apache.hadoop.fs.FsServerDefaults:write(java.io.DataOutput),163,173,"/**
* Serializes block configuration settings to output stream.
* @param out DataOutput stream for writing
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,write,org.apache.hadoop.fs.permission.PermissionStatus:write(java.io.DataOutput),103,106,"/**
* Writes user credentials to output stream.
* @param out DataOutput stream to serialize data to
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,getFixedByteString,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getFixedByteString(org.apache.hadoop.io.Text),82,89,"/**
* Retrieves a fixed byte string from cache or creates it if not found.
* @param key unique text identifier
* @return ByteString object representing the fixed byte string
*/","* Get the ByteString for frequently used fixed and small set strings.
   * @param key Hadoop Writable Text string
   * @return the ByteString for frequently used fixed and small set strings.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,org.apache.hadoop.security.token.Token:<init>(org.apache.hadoop.security.token.Token),107,112,"/**
* Copies token properties from another token object.
* @param other the token to copy from
*/","* Clone a token.
   * @param other the token to clone",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readLine,"org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text,int)",380,382,"/**
* Reads a line from the text stream with a maximum length.
* @param str Text stream to read from
* @param maxLineLength Maximum allowed line length
* @return Length of the read line or -1 on error
*/","* Read from the InputStream into the given Text.
   * @param str the object to store the given line
   * @param maxLineLength the maximum number of bytes to store into str.
   * @return the number of bytes read including the newline
   * @throws IOException if the underlying stream throws",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readLine,org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text),390,392,"/**
* Reads a line from the given text, up to max length.
* @param str input Text object
* @return number of characters read or -1 on error
*/","* Read from the InputStream into the given Text.
   * @param str the object to store the given line
   * @return the number of bytes read including the newline
   * @throws IOException if the underlying stream throws",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)",166,188,"/**
* Creates a delegationToken with given UGI and service info.
* @param ugi UserGroupInformation object
* @param renewer user or principal that can renew the token, defaults to UGI short username if null
* @param service service name, optional
* @return Token object or null on failure
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,<init>,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",55,61,"/**
* Initializes a new delegationToken identifier with specified owner, 
* renewer and real user. Issue date and maximum date are initialized to 0.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(int),201,203,"/**
* Returns a retry policy that fails over after network exceptions.
* @param maxFailovers maximum number of attempts before failing
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,<init>,"org.apache.hadoop.util.InstrumentedWriteLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long)",43,48,"/**
* Initializes an InstrumentedWriteLock instance with the specified parameters.
* @param name unique identifier for the lock
* @param logger logging utility to report lock events
* @param readWriteLock underlying locking mechanism
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,<init>,"org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,long,long)",75,79,"/**
* Constructs an instrumented lock with specified parameters.
* @param name unique lock identifier
* @param logger logging instance for lock events
* @param minLoggingGapMs minimum time gap between log messages (ms)
* @param lockWarningThresholdMs threshold for warning on excessive locking (ms)
*/","* Create a instrumented lock instance which logs a warning message
   * when lock held time is above given threshold.
   *
   * @param name the identifier of the lock object
   * @param logger this class does not have its own logger, will log to the
   *               given logger instead
   * @param minLoggingGapMs  the minimum time gap between two log messages,
   *                         this is to avoid spamming to many logs
   * @param lockWarningThresholdMs the time threshold to view lock held
   *                               time as being ""too long""",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,<init>,"org.apache.hadoop.util.InstrumentedReadLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long)",49,54,"/**
* Constructs an instrumented read lock with specified parameters.
* @param name lock name
* @param logger logging instance
* @param readWriteLock underlying read-write lock
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,java.lang.Object,org.apache.hadoop.io.retry.RetryPolicy)",40,45,"/**
* Creates a proxy instance for the given interface and implementation.
* @param iface target interface
* @param implementation concrete implementation of the interface
* @param retryPolicy retry policy configuration
*/","* <p>
   * Create a proxy for an interface of an implementation class
   * using the same retry policy for each method in the interface. 
   * </p>
   * @param iface the interface that the retry will implement
   * @param implementation the instance whose methods should be retried
   * @param retryPolicy the policy for retrying method call failures
   * @param <T> T.
   * @return the retry proxy",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,processWaitTimeAndRetryInfo,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processWaitTimeAndRetryInfo(),129,149,"/**
* Waits and retries based on wait time and retry info.
* @throws InterruptedIOException if retry is interrupted
*/","* It first processes the wait time, if there is any,
     * and then invokes {@link #processRetryInfo()}.
     *
     * If the wait time is positive, it either sleeps for synchronous calls
     * or immediately returns for asynchronous calls.
     *
     * @return {@link CallReturn#RETRY} if the retryInfo is processed;
     *         otherwise, return {@link CallReturn#WAIT_RETRY}.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Writer:<init>(org.apache.hadoop.fs.FSDataOutputStream,int,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)",281,292,"/**
* Initializes a Writer object with file system output stream and configuration.
* @param fsdos FSDataOutputStream to write to
* @param minBlockSize minimum block size for writing
* @param compressName compression algorithm name
* @param comparator comparison function name
* @param conf Hadoop Configuration object
*/","* Constructor
     * 
     * @param fsdos
     *          output stream for writing. Must be at position 0.
     * @param minBlockSize
     *          Minimum compressed block size in bytes. A compression block will
     *          not be closed until it reaches this size except for the last
     *          block.
     * @param compressName
     *          Name of the compression algorithm. Must be one of the strings
     *          returned by {@link TFile#getSupportedCompressionAlgorithms()}.
     * @param comparator
     *          Leave comparator as null or empty string if TFile is not sorted.
     *          Otherwise, provide the string name for the comparison algorithm
     *          for keys. Two kinds of comparators are supported.
     *          <ul>
     *          <li>Algorithmic comparator: binary comparators that is language
     *          independent. Currently, only ""memcmp"" is supported.
     *          <li>Language-specific comparator: binary comparators that can
     *          only be constructed in specific language. For Java, the syntax
     *          is ""jclass:"", followed by the class name of the RawComparator.
     *          Currently, we only support RawComparators that can be
     *          constructed through the default constructor (with no
     *          parameters). Parameterized RawComparators such as
     *          {@link WritableComparator} or
     *          {@link JavaSerializationComparator} may not be directly used.
     *          One should write a wrapper class that inherits from such classes
     *          and use its default constructor to perform proper
     *          initialization.
     *          </ul>
     * @param conf
     *          The configuration object.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:<init>(java.io.DataInput),772,780,"/**
* Builds a meta-index from input stream.
* @param in DataInput source
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,isLastChunk,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:isLastChunk(),80,83,"/**
* Checks if current chunk is the last one in the sequence.
* @return true if this is the final chunk, false otherwise
*/","* Have we reached the last chunk.
     * 
     * @return true if we have reached the last chunk.
     * @throws java.io.IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,getRemain,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:getRemain(),91,94,"/**
* Returns the remaining bytes in the input stream.
* @throws IOException if end of file has been reached
*/","* How many bytes remain in the current chunk?
     * 
     * @return remaining bytes left in the current chunk.
     * @throws java.io.IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,read,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(),137,144,"/**
* Reads a single character from the input stream.
* @return integer representation of the read character or -1 on EOF
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,read,"org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[],int,int)",151,165,"/**
* Reads a specified number of bytes from the input stream into the provided byte array.
* @param b the target byte array
* @param off starting index in the array
* @param len number of bytes to read
* @return actual number of bytes read or -1 if end-of-file reached
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,skip,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:skip(long),167,175,"/**
* Skips the specified number of bytes or until EOS is reached.
* @param n number of bytes to skip
* @return actual number of bytes skipped (or 0 on EOF)
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(int),303,309,"/**
* Writes a single byte to the output buffer.
* @param b the byte to be written
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,flush,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:flush(),332,336,"/**
* Flushes buffered output and underlying writer.
* @throws IOException if an I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(byte[]),311,314,"/**
 * Writes an array of bytes to the underlying output stream, using default offset and length.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[],int,int)",1948,1950,"/**
 * Compares this byte array to another using lexicographical order.
 * @param buf source byte array
 * @param offset starting index in buf
 * @param length number of bytes to compare
 */","* Compare the entry key to another key. Synonymous to compareTo(new
         * ByteArray(buf, offset, length)
         * 
         * @param buf
         *          The key buffer
         * @param offset
         *          offset into the key buffer.
         * @param length
         *          the length of the key.
         * @return comparison result between the entry key with the input key.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayWritable.java,<init>,org.apache.hadoop.io.ArrayWritable:<init>(java.lang.String[]),62,67,"/**
* Initializes an ArrayWritable with a specified array of string values.
* @param strings array of strings to be written
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$BlockCompressWriter:close(),1668,1674,"/**
* Closes the output stream and synchronizes any pending operations.
* @throws IOException if an I/O error occurs during closing
*/",Close the file.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$BlockCompressWriter:append(java.lang.Object,java.lang.Object)",1677,1707,"/**
* Appends a key-value pair to the underlying storage.
* @param key object with class matching keyClass
* @param val object with class matching valClass
*/",Append a key/value pair.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,appendRaw,"org.apache.hadoop.io.SequenceFile$BlockCompressWriter:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)",1710,1733,"/**
* Appends a raw key-value pair, compressing and flushing when buffer size exceeds threshold.
* @param keyData the key data to append
* @param keyOffset offset into keyData for appending
* @param keyLength length of key data to append
* @param val ValueBytes object containing value data
*/",Append a key/value pair.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,delegationTokenToJSON,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:delegationTokenToJSON(org.apache.hadoop.security.token.Token),357,365,"/**
* Converts a Token object to a JSON-formatted map.
* @param token the input token to convert
* @return a map containing the token's JSON representation or null if an error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,doDelegationTokenOperation,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:doDelegationTokenOperation(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation,java.lang.String,org.apache.hadoop.security.token.Token,boolean,java.lang.String)",288,356,"/**
* Performs a delegation token operation on the specified URL.
* @param url URL to perform operation on
* @param token authentication token
* @param operation type of operation to perform
* @param renewer optional renewer identifier
* @param dToken optional delegation token
* @param hasResponse whether response is expected
* @param doAsUser user to impersonate (optional)
* @return result of operation or null if not successful
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,cloneInto,"org.apache.hadoop.io.WritableUtils:cloneInto(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",236,239,"/**
* Clones one Writable object into another.
* @param dst destination Writable object
* @param src source Writable object to be cloned
*/","* Make a copy of the writable object using serialization to a buffer.
   * @param dst the object to copy from
   * @param src the object to copy into, which is destroyed
   * @throws IOException raised on errors performing I/O.
   * @deprecated use ReflectionUtils.cloneInto instead.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,"org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.util.Collection)",483,495,"/**
* Chooses a random Node within the specified scope, excluding provided nodes.
* @param scope unique identifier for the node hierarchy
* @param excludedNodes collection of nodes to exclude from selection
*/","* Randomly choose one node from <i>scope</i>.
   *
   * If scope starts with ~, choose one from the all nodes except for the
   * ones in <i>scope</i>; otherwise, choose one from <i>scope</i>.
   * If excludedNodes is given, choose a node that's not in excludedNodes.
   *
   * @param scope range of nodes from which a node will be chosen
   * @param excludedNodes nodes to be excluded from
   * @return the chosen node",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistance,"org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",886,892,"/**
* Sorts an array of nodes by distance from a given node.
* @param reader reference node
* @param nodes array of nodes to sort
* @param activeLen length of active nodes
*/","* Sort nodes array by network distance to <i>reader</i>.
   * <p>
   * In a three-level topology, a node can be either local, on the same rack,
   * or on a different rack from the reader. Sorting the nodes based on network
   * distance from the reader reduces network traffic and improves
   * performance.
   * <p>
   * As an additional twist, we also randomize the nodes at each network
   * distance. This helps with load balancing when there is data skew.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistanceUsingNetworkLocation,"org.apache.hadoop.net.NetworkTopology:sortByDistanceUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",929,936,"/**
* Sorts nodes by distance from the reader's network location.
* @param reader node to calculate distances from
* @param nodes array of nodes to sort
*/","* Sort nodes array by network distance to <i>reader</i> with secondary sort.
   * <p> using network location. This is used when the reader
   * is not a datanode. Sorting the nodes based on network distance
   * from the reader reduces network traffic and improves
   * performance.
   * </p>
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,"org.apache.hadoop.net.SocketInputStream:<init>(java.net.Socket,long)",91,94,"/**
 * Creates a new SocketInputStream instance.
 * @param socket underlying socket channel
 * @param timeout read timeout in milliseconds
 */","* Same as SocketInputStream(socket.getChannel(), timeout): <br><br>
   * 
   * Create a new input stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @see SocketInputStream#SocketInputStream(ReadableByteChannel, long)
   *  
   * @param socket should have a channel associated with it.
   * @param timeout timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,org.apache.hadoop.net.SocketInputStream:<init>(java.net.Socket),108,110,"/**
 * Initializes SocketInputStream with a given Socket instance.
 * @param socket active network connection
 */","* Same as SocketInputStream(socket.getChannel(), socket.getSoTimeout())
   * :<br><br>
   * 
   * Create a new input stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * @see SocketInputStream#SocketInputStream(ReadableByteChannel, long)
   *  
   * @param socket should have a channel associated with it.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,<init>,"org.apache.hadoop.net.SocketOutputStream:<init>(java.net.Socket,long)",96,99,"/**
* Creates a SocketOutputStream with specified timeout.
* @param socket underlying socket channel
* @param timeout socket operation timeout in milliseconds
*/","* Same as SocketOutputStream(socket.getChannel(), timeout):<br><br>
   * 
   * Create a new ouput stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @see SocketOutputStream#SocketOutputStream(WritableByteChannel, long)
   *  
   * @param socket should have a channel associated with it.
   * @param timeout timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,write,org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:write(java.lang.String),198,205,"/**
* Sends a message over the established socket connection.
* @param msg the message to send
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,<init>,org.apache.hadoop.net.NetworkTopology:<init>(),122,125,"/**
* Initializes network topology with root node.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getNodeForNetworkLocation,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getNodeForNetworkLocation(org.apache.hadoop.net.Node),42,55,"/**
* Retrieves a node for the given network location, potentially resolving default rack and nodegroup info.
*@param node input node to resolve
*@return resolved Node object or null if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,add,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:add(org.apache.hadoop.net.Node),176,222,"/**
* Adds a new node to the topology, updating rack info and cluster map.
* @param node Node object to add
*/","Add a leaf node
   * Update node counter &amp; rack counter if necessary
   * @param node node to be added; can be null
   * @exception IllegalArgumentException if add a node to a leave 
   *                                     or node to be added is not a leaf",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,add,org.apache.hadoop.net.InnerNodeImpl:add(org.apache.hadoop.net.Node),129,170,"/**
* Adds a Node to this tree, ensuring it is a descendant.
* @param n Node to add
* @return true on successful addition, false otherwise
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,doIO,"org.apache.hadoop.net.SocketIOWithTimeout:doIO(java.nio.ByteBuffer,int)",124,170,"/**
* Performs I/O operations on a ByteBuffer until it's empty or an error occurs.
* @param buf ByteBuffer to read from/write to
* @param ops IO operation flags (e.g. read, write)
* @return number of bytes processed (-1 if closed, 0 if successful)","* Performs one IO and returns number of bytes read or written.
   * It waits up to the specified timeout. If the channel is 
   * not read before the timeout, SocketTimeoutException is thrown.
   * 
   * @param buf buffer for IO
   * @param ops Selection Ops used for waiting. Suggested values: 
   *        SelectionKey.OP_READ while reading and SelectionKey.OP_WRITE while
   *        writing. 
   *        
   * @return number of bytes read or written. negative implies end of stream.
   * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,connect,"org.apache.hadoop.net.SocketIOWithTimeout:connect(java.nio.channels.SocketChannel,java.net.SocketAddress,int)",182,228,"/**
* Establishes a connection to the specified endpoint with the given timeout.
* @param channel SocketChannel instance
* @param endpoint SocketAddress of the remote server
* @param timeout Connection timeout in milliseconds
*/","* The contract is similar to {@link SocketChannel#connect(SocketAddress)} 
   * with a timeout.
   * 
   * @see SocketChannel#connect(SocketAddress)
   * 
   * @param channel - this should be a {@link SelectableChannel}
   * @param endpoint
   * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,waitForIO,org.apache.hadoop.net.SocketIOWithTimeout:waitForIO(int),242,248,"/**
* Waits for specified number of I/O operations on the channel.
* @param ops number of I/O operations to wait for
*/","* This is similar to {@link #doIO(ByteBuffer, int)} except that it
   * does not perform any I/O. It just waits for the channel to be ready
   * for I/O as specified in ops.
   * 
   * @param ops Selection Ops used for waiting
   * 
   * @throws SocketTimeoutException 
   *         if select on the channel times out.
   * @throws IOException
   *         if any other I/O error occurs.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultHost,org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String),388,391,"/**
* Returns default host name based on given interface.
* @param strInterface network interface (optional) 
*/","* Returns the default (first) host name associated by the default
   * nameserver with the address bound to the specified network interface
   * 
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0).
   *            Must not be null.
   * @return The default host name associated with IPs bound to the network
   *         interface
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultHost,"org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String,java.lang.String)",404,408,"/**
* Returns default host with optional interface and nameserver.
* @param strInterface interface to use (optional)
* @param nameserver nameserver to use (optional)
*/","* @return Returns the default (first) host name associated by the provided
   * nameserver with the address bound to the specified network interface.
   *
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0)
   * @param nameserver
   *            The DNS host name
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkParameterValidity,org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[]),387,389,"/**
* Validates command-line parameters based on provided usage.
* @param argv array of command-line arguments
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,help,org.apache.hadoop.ha.HAAdmin:help(java.lang.String[]),508,510,"/**
 * Displays usage message and returns exit status.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getFilter,org.apache.hadoop.metrics2.impl.MetricsConfig:getFilter(java.lang.String),266,280,"/**
* Retrieves a metrics filter instance based on the provided prefix,
* either from configured plugins or creates a glob filter with options.
* @param prefix metric prefix to determine filter type
* @return MetricsFilter object or null if no applicable filters found",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,create,org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String),98,101,"/**
* Creates MetricsConfig instance with given prefix.
* @param prefix unique identifier for metrics configuration
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,create,"org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String,java.lang.String[])",103,105,"/**
 * Creates and returns the first instance of MetricsConfig with the given prefix and file names.
 * @param prefix configuration prefix
 * @param fileNames variable number of file names
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,clear,org.apache.hadoop.ipc.RetryCache:clear(org.apache.hadoop.ipc.RetryCache),398,403,"/**
* Clears the retry cache.
* @param cache RetryCache instance to be cleared
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getMetrics,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMetrics(org.apache.hadoop.metrics2.impl.MetricsCollectorImpl,boolean)",196,210,"/**
* Retrieves metrics records using the provided collector and filter settings.
* @param builder MetricsCollector to configure record collection
* @param all whether to collect all available metrics or just filtered ones
* @return Iterable of collected MetricsRecord objects
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,getMBeanName,"org.apache.hadoop.metrics2.util.MBeans:getMBeanName(java.lang.String,java.lang.String,java.util.Map)",151,169,"/**
* Creates an MBean name for a service with specified parameters.
* @param serviceName unique service identifier
* @param nameName human-readable service name
* @param additionalParameters optional key-value pairs to include in the name
* @return ObjectName instance or null on error
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reattachMetrics,org.apache.hadoop.security.UserGroupInformation:reattachMetrics(),259,261,"/**
* Reattaches metrics to the current metrics registry.
*/",* Reattach the class's metrics to a new metric system.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stopMetricsMBeans,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopMetricsMBeans(),341,346,"/**
* Stops all metrics MBeans associated with registered sources.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,stop,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:stop(),212,214,"/**
 * Stops all associated MBean services.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,stop,org.apache.hadoop.ipc.DecayRpcScheduler:stop(),1156,1161,"/**
* Shuts down and unregisters all metrics-related resources.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableInverseQuantiles.java,<init>,"org.apache.hadoop.metrics2.lib.MutableInverseQuantiles:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",61,64,"/**
* Creates a new MutableInverseQuantiles object with specified parameters.
* @param name unique identifier for the quantile metric
* @param description human-readable description of the metric
* @param sampleName name of the data source being quantified
* @param valueName name of the data values being processed
* @param intervalSecs time interval in seconds between quantile calculations
*/","* Instantiates a new {@link MutableInverseQuantiles} for a metric that rolls itself
   * over on the specified time interval.
   *
   * @param name          of the metric
   * @param description   long-form textual description of the metric
   * @param sampleName    type of items in the stream (e.g., ""Ops"")
   * @param valueName     type of the values
   * @param intervalSecs  rollover interval (in seconds) of the estimator",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newQuantiles,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",217,228,"/**
* Creates a new quantiles metric with the specified name and description.
* @param name metric name
* @param desc metric description
* @param sampleName sample name
* @param valueName value name
* @param interval positive interval for quantile calculation
* @return newly created MutableQuantiles object
*/","* Create a mutable metric that estimates quantiles of a stream of values
   * @param name of the metric
   * @param desc metric description
   * @param sampleName of the metric (e.g., ""Ops"")
   * @param valueName of the metric (e.g., ""Time"" or ""Latency"")
   * @param interval rollover interval of estimator in seconds
   * @return a new quantile estimator object
   * @throws MetricsException if interval is not a positive integer",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,create,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:create(),989,991,"/**
* Creates and registers metrics instance for delegated token secret manager.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,create,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:create(org.apache.hadoop.ipc.RetryCache),52,55,"/**
* Creates and registers metrics for the given retry cache.
* @param cache RetryCache instance to generate metrics for
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newStat,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newStat(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",279,282,"/**
* Creates a new mutable statistic with given name and description.
* @param name      unique statistic name
* @param desc      descriptive text for the statistic
* @param sampleName name of the sample associated with this statistic
* @param valueName  name of the value associated with this statistic
*/","* Create a mutable metric with stats
   * @param name  of the metric
   * @param desc  metric description
   * @param sampleName  of the metric (e.g., ""Ops"")
   * @param valueName   of the metric (e.g., ""Time"" or ""Latency"")
   * @return a new mutable metric object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,addMetricIfNotExists,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:addMetricIfNotExists(java.lang.String),163,172,"/**
* Retrieves or creates a MutableRate metric by name.
* @param name unique metric identifier
* @return MutableRate object or null if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String,boolean,boolean)",314,329,"/**
* Creates or returns existing MutableRate instance.
* @param name unique metric identifier
* @return MutableRate object
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getMetrics,"org.apache.hadoop.metrics2.source.JvmMetrics:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",143,155,"/**
* Collects JVM metrics and adds them to the provided MetricsCollector.
* @param collector Metrics collection object
* @param all whether to collect all metrics or only current ones
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,initRegistry,org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:initRegistry(java.lang.Object),98,127,"/**
* Initializes the metrics registry for the given object.
* @param source Object to retrieve or create registry for
* @return Existing or newly created MetricsRegistry instance
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(java.lang.String,java.lang.String,java.lang.String)",391,393,"/**
 * Tags a metric with given name, description and value.
 * @param name unique metric identifier
 * @param description human-readable metric description
 * @param value current metric value as string
 */","* Add a tag to the metrics
   * @param name  of the tag
   * @param description of the tag
   * @param value of the tag
   * @return the registry (for keep adding tags)",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,add,"org.apache.hadoop.metrics2.lib.MutableRollingAverages:add(java.lang.String,long)",212,214,"/**
* Adds a new metric with specified name and value.
* @param name unique metric identifier
* @param value numeric value associated with the metric
*/","* @param name
   *          name of metric
   * @param value
   *          value of metric",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,addQueueTime,"org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:addQueueTime(int,long)",88,90,"/**
* Adds time spent in queue to its corresponding rate.
* @param priority queue level (0-9)
* @param queueTime time spent in queue
*/","* Instrument a Call queue time based on its priority.
   *
   * @param priority of the RPC call
   * @param queueTime of the RPC call in the queue of the priority",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,addProcessingTime,"org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:addProcessingTime(int,long)",98,100,"/**
* Adds processing time to the RPC processing rates map.
* @param priority integer representing message priority level
* @param processingTime duration of processing in milliseconds 
*/","* Instrument a Call processing time based on its priority.
   *
   * @param priority of the RPC call
   * @param processingTime of the RPC call in the queue of the priority",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,addProcessingTime,"org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addProcessingTime(java.lang.String,long)",87,89,"/**
* Adds RPC call processing time to the rate map.
* @param rpcCallName name of the RPC call
* @param processingTime duration in milliseconds
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,addDeferredProcessingTime,"org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addDeferredProcessingTime(java.lang.String,long)",91,93,"/**
* Adds deferred RPC processing time with specified name.
* @param name unique identifier of RPC operation
* @param processingTime total processing time in milliseconds
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,addOverallProcessingTime,"org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addOverallProcessingTime(java.lang.String,long)",100,102,"/**
* Adds RPC call processing time to the overall rates map.
* @param rpcCallName name of the RPC call
* @param overallProcessingTime total processing time for the call
*/","* Add an overall RPC processing time sample.
   * @param rpcCallName of the RPC call
   * @param overallProcessingTime  the overall RPC processing time",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableStat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",138,163,"/**
* Updates metrics snapshot for the given builder.
* @param builder MetricsRecordBuilder to update
* @param all whether to include all statistics or only changed ones
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,newForMethod,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForMethod(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)",94,105,"/**
* Creates a mutable metric for the given method and adds it to the registry.
* @param source object associated with the metric
* @param method target method
* @param annotation metric annotation
* @param registry metrics registry
* @return MutableMetric instance or null if creation failed
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getMetrics,"org.apache.hadoop.ipc.DecayRpcScheduler:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",1009,1027,"/**
* Collects and adds metrics to the MetricsCollector.
* @param collector Metrics collector instance
* @param all whether to collect all or only current metrics
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroups,org.apache.hadoop.security.UserGroupInformation$TestingGroups:getGroups(java.lang.String),1580,1583,"/**
* Retrieves list of group names associated with a user.
* @param user username to fetch groups for
* @return list of group names or empty list if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateKrb5File,org.apache.hadoop.security.KDiag:validateKrb5File(),561,588,"/**
* Validates and locates Kerberos configuration file.
* @throws IOException if file cannot be accessed
*/","* Locate the {@code krb5.conf} file and dump it.
   *
   * No-op on windows.
   * @throws IOException problems reading the file.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,verify,"org.apache.hadoop.security.KDiag:verify(boolean,java.lang.String,java.lang.String,java.lang.Object[])",963,981,"/**
* Verifies a conditional statement and handles failure or reporting.
* @param condition boolean condition to evaluate
* @param category diagnostic category for messages
* @param message error message template
* @param args variable arguments for formatted message
* @return true if condition is met, false otherwise
*/","* Assert that a condition must hold.
   *
   * If not, an exception is raised, or, if {@link #nofail} is set,
   * an error will be logged and the method return false.
   *
   * @param condition condition which must hold
   * @param category category for exception
   * @param message string formatting message
   * @param args any arguments for the formatting
   * @return true if the verification succeeded, false if it failed but
   * an exception was not raised.
   * @throws KerberosDiagsFailure containing the formatted text
   *         if the condition was met",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,failif,"org.apache.hadoop.security.KDiag:failif(boolean,java.lang.String,java.lang.String,java.lang.Object[])",1034,1042,"/**
* Fails with a diagnostics error if the given condition is true.
* @param condition boolean condition to evaluate
* @param category diagnostic category for the failure
* @param message failure message string
* @param args variable arguments for message formatting
*/","* Conditional failure with string formatted arguments.
   * There is no chek for the {@link #nofail} value.
   * @param condition failure condition
   * @param category category for exception
   * @param message string formatting message
   * @param args any arguments for the formatting
   * @throws KerberosDiagsFailure containing the formatted text
   *         if the condition was met",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createRemoteUser,org.apache.hadoop.security.UserGroupInformation:createRemoteUser(java.lang.String),1438,1442,"/**
* Creates remote user with specified username and default authentication method.
* @param user name of the remote user to be created
*/","* Create a user from a login name. It is intended to be used for remote
   * users in RPC, since it won't have any credentials.
   * @param user the full user principal name, must not be empty or null
   * @return the UserGroupInformation for the remote user.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuthorizedUgi,org.apache.hadoop.ipc.Server$Connection:getAuthorizedUgi(java.lang.String),2161,2176,"/**
* Retrieves user identity based on authorized ID using specified authentication method.
* @param authorizedId unique identifier for authentication
* @return UserGroupInformation object with authenticated credentials
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,verifyToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])",575,582,"/**
* Verifies a token by comparing its stored and provided passwords.
* @param identifier unique token identifier
* @param password the user-provided password to verify against stored value
*/","* Verifies that the given identifier and password are valid and match.
   * @param identifier Token identifier.
   * @param password Password in the token.
   * @throws InvalidToken InvalidToken.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,init,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:init(),143,152,"/**
* Initializes the managed secret manager, starting its threads if enabled.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,startThreads,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:startThreads(),243,348,"/**
* Initializes and starts ZK threads, including sequence counters and caches.
* @throws IOException if an error occurs during thread initialization
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,run,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover:run(),829,859,"/**
* Periodically removes expired tokens and updates the master key.
* @throws InterruptedException if interrupted while sleeping
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,org.apache.hadoop.security.SaslInputStream:read(java.nio.ByteBuffer),367,384,"/**
* Reads data from this stream into the provided ByteBuffer.
* @param dst ByteBuffer to store the read data
* @return number of bytes read or -1 if end-of-stream reached
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,spawnAutoRenewalThreadForKeytab,org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForKeytab(),905,918,"/**
* Spawns a thread to auto-renew keytabs based on Kerberos ticket.
* @param none
*/","* Spawn a thread to do periodic renewals of kerberos credentials from a
   * keytab file.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,<init>,org.apache.hadoop.fs.shell.Ls:<init>(org.apache.hadoop.conf.Configuration),122,124,"/**
 * Initializes an instance of Ls with the given configuration.
 * @param conf Hadoop Configuration object
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,<init>,"org.apache.hadoop.fs.shell.Count:<init>(java.lang.String[],int,org.apache.hadoop.conf.Configuration)",118,122,"/**
* Creates a new Count object from an array of command-line arguments.
*@param cmd array of command-line arguments
*@param pos starting index for argument copying
*@param conf configuration object passed to superclass constructor
*/","Constructor
   * @deprecated invoke via {@link FsShell}
   * @param cmd the count command
   * @param pos the starting index of the arguments
   * @param conf configuration",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,<init>,org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:<init>(org.apache.hadoop.fs.FileSystem),495,497,"/**
* Initializes TargetFileSystem with an underlying FileSystem.
* @param fs the target file system instance
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,org.apache.hadoop.fs.ChecksumFileSystem:<init>(org.apache.hadoop.fs.FileSystem),79,81,"/**
 * Initializes a new ChecksumFileSystem instance with the given FileSystem.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,newShellInstance,org.apache.hadoop.fs.FsShell:newShellInstance(),398,400,"/**
* Creates a new instance of FsShell.
* @return FsShell object 
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,<init>,org.apache.hadoop.fs.shell.FsCommand:<init>(),76,76,"/**
 * Constructor to initialize FsCommand instance. 
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,39,"/**
* Initializes ErasureEncoder with provided configuration.
* @param options ErasureCoderOptions to configure encoder
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),44,46,"/**
 * Initializes an instance of the ErasureEncoder with given configuration.
 * @param options ErasureCoderOptions object containing encoding settings
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Initializes this encoder with given ErasureCoderOptions.
 * @param options configuration settings for erasure encoding.",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,38,"/**
 * Initializes an instance of XORErasureEncoder with specified encoding options.
 * @param options ErasureCoderOptions object containing encoding settings
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
* Initializes a new instance of DummyErasureDecoder with specified options.
* @param options ErasureCoderOptions to configure decoder behavior
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,38,"/**
 * Initializes an instance of XORErasureDecoder with provided options.
 * @param options ErasureCoderOptions object containing configuration settings
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,39,"/**
* Initializes ErasureDecoder with given configuration options.
* @param options ErasureCoderOptions object containing decoder settings
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),45,47,"/**
* Initializes ErasureDecoder with provided configuration.
* @param options ErasureCoderOptions object containing decoder settings
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/WritableSerialization.java,getDeserializer,org.apache.hadoop.io.serializer.WritableSerialization:getDeserializer(java.lang.Class),120,124,"/**
* Returns a deserializer instance for the given writable class.
* @param c Class of the Writable object to deserialize
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMaps,org.apache.hadoop.security.ShellBasedIdMapping:updateMaps(),343,357,"/**
* Updates internal map representations.
* @throws IOException on I/O error
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMapIncr,"org.apache.hadoop.security.ShellBasedIdMapping:updateMapIncr(java.lang.String,boolean)",465,504,"/**
* Updates internal mapping for user/group ID synchronization.
* @param name unique identifier name
* @param isGrp true for group, false for user
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMapIncr,"org.apache.hadoop.security.ShellBasedIdMapping:updateMapIncr(int,boolean)",506,540,"/**
* Updates the map with user/group information incrementally.
* @param id unique ID
* @param isGrp true for group, false for user
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,connectToZooKeeper,org.apache.hadoop.ha.ActiveStandbyElector:connectToZooKeeper(),723,743,"/**
* Establishes a ZooKeeper connection and waits for the asynchronous success/failure.
* @throws IOException if ZooKeeper connection cannot be established
* @throws KeeperException on ZooKeeper operation failure
*/","* Get a new zookeeper client instance. protected so that test class can
   * inherit and mock out the zookeeper instance
   * 
   * @return new zookeeper client instance
   * @throws IOException raised on errors performing I/O.
   * @throws KeeperException zookeeper connectionloss exception",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromKeytab,"org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(boolean,boolean)",1272,1289,"/**
* Performs re-login from keytab if necessary and possible.
* @param checkTGT whether to verify TGT
* @param ignoreLastLoginTime whether to ignore last login time
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromTicketCache,org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache(boolean),1322,1332,"/**
* Relogs from ticket cache if necessary and not expired.
* @param ignoreLastLoginTime whether to ignore last login time
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",29,32,"/**
* Constructs a CryptoFSDataOutputStream instance.
* @param out underlying FS data output stream
* @param codec crypto codec to use
* @param bufferSize buffer size for encryption/decryption
* @param key encryption key
* @param iv initialization vector
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",86,89,"/**
* Initializes CryptoOutputStream with specified parameters.
* @param out OutputStream to encrypt
* @param codec CryptoCodec instance for encryption
* @param bufferSize buffer size for encryption
* @param key cryptographic key for encryption
* @param iv initialization vector for encryption
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:createEncryptor(),72,76,"/**
* Creates an OpenSSL-based encryptor instance.
* @return Encryptor object for encryption operations
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:createDecryptor(),78,82,"/**
* Creates an OpenSSL-based decryptor instance.
* @return Decryptor object for decryption operations
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,<init>,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:<init>(int,org.apache.hadoop.crypto.CipherSuite)",130,134,"/**
* Initializes OpenSSLCTR cipher with specified mode and suite.
* @param mode encryption mode
* @param suite selected cipher suite
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONEncKeyVersions,"org.apache.hadoop.util.KMSUtil:parseJSONEncKeyVersions(java.lang.String,java.util.List)",143,155,"/**
* Parses JSON encrypted key versions from a list of maps.
* @param keyName the key name
* @param valueList list of map representations of encrypted key versions
* @return List<EncryptedKeyVersion> of parsed key versions or empty list if input is invalid
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getEncKeyQueueSize,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getEncKeyQueueSize(java.lang.String),966,969,"/**
* Retrieves size of encryption key queue by name.
* @param keyName unique key identifier
* @return queue size or -1 if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getNext,org.apache.hadoop.crypto.key.kms.ValueQueue:getNext(java.lang.String),292,295,"/**
* Retrieves the next element of type E associated with the given key.
* @param keyName name of the key to look up
* @return the next available element or null if not found
*/","* This removes the value currently at the head of the Queue for the
   * provided key. Will immediately fire the Queue filler function if key
   * does not exist.
   * If Queue exists but all values are drained, It will ask the generator
   * function to add 1 value to Queue and then drain it.
   * @param keyName String key name
   * @return E the next value in the Queue
   * @throws IOException raised on errors performing I/O.
   * @throws ExecutionException executionException.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,drain,org.apache.hadoop.crypto.key.kms.KMSClientProvider:drain(java.lang.String),961,964,"/**
* Drains encryption key versions from queue by name.
* @param keyName unique key identifier
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,internalQueueCall,org.apache.hadoop.ipc.Server:internalQueueCall(org.apache.hadoop.ipc.Server$Call),3106,3109,"/**
* Wraps an external queue call with default parameters.
* @param call Call object to be processed
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByName,org.apache.hadoop.security.SecurityUtil:getByName(java.lang.String),569,588,"/**
* Resolves hostname to InetAddress. Times slow lookups and logs results.
* @param hostname the hostname to resolve
* @return InetAddress object or null if not found
*/","* Resolves a host subject to the security requirements determined by
   * hadoop.security.token.service.use_ip. Optionally logs slow resolutions.
   * 
   * @param hostname host or ip to resolve
   * @return a resolved host
   * @throws UnknownHostException if the host doesn't exist",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,run,org.apache.hadoop.util.JvmPauseMonitor$Monitor:run(),181,208,"/**
* Monitors JVM pauses and logs threshold-exceeded events.
*@throws InterruptedException if interrupted while sleeping
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(org.apache.hadoop.ipc.VersionedProtocol,java.lang.String,long,int)",241,254,"/**
* Retrieves protocol signature based on server and client versions.
* @param server VersionedProtocol instance
* @param protocol string identifier for target protocol class
* @param clientVersion client version number
* @param clientMethodsHash hash of client methods
* @return ProtocolSignature object or throws IOException if error occurs","* Get a server protocol's signature
   *
   * @param server server implementation
   * @param protocol server protocol
   * @param clientVersion client's version
   * @param clientMethodsHash client's protocol's hash code
   * @return the server protocol's signature
   * @throws IOException if any error occurs",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolServerSideTranslatorPB.java,getProtocolSignature,"org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:getProtocolSignature(java.lang.String,long,int)",74,86,"/**
* Retrieves the protocol signature based on the provided client version and hash.
* @param protocol requested protocol name
* @param clientVersion client software version
* @param clientMethodsHash client method hash
* @return ProtocolSignature object or throws IOException if unknown protocol
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,getProtocolSignature,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getProtocolSignature(java.lang.String,long,int)",186,198,"/**
* Retrieves the protocol signature for a given client version and hash.
* @param protocol requested protocol name
* @param clientVersion client software version
* @param clientMethodsHash client method hash value
* @return ProtocolSignature object or throws IOException if invalid protocol
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolSignature(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto)",70,104,"/**
* Retrieves protocol signature response based on request parameters.
* @param controller Rpc controller
* @param request GetProtocolSignatureRequestProto object with protocol and rpcKind
* @return GetProtocolSignatureResponseProto or null if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ExternalCall.java,<init>,org.apache.hadoop.ipc.ExternalCall:<init>(java.security.PrivilegedExceptionAction),36,38,"/**
* Constructs an ExternalCall instance with the given PrivilegedExceptionAction.
* @param action the action to perform on the external resource
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Listener:run(),1551,1600,"/**
* Runs the server, handling incoming connections and IO operations until stopped.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,add,org.apache.hadoop.ipc.FairCallQueue:add(java.lang.Object),194,213,"/**
* Adds an element to the call queue, attempting to offer it to all queues.
* @param e element to add
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,put,org.apache.hadoop.ipc.FairCallQueue:put(java.lang.Object),215,222,"/**
* Adds an element to the priority queues based on its priority level.
* @param e the element to add
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolProxy.java,isMethodSupported,"org.apache.hadoop.ipc.ProtocolProxy:isMethodSupported(java.lang.String,java.lang.Class[])",95,117,"/**
* Checks if a method is supported by the server.
* @param methodName name of the method to check
* @param parameterTypes method parameters (varargs)
* @return true if method is supported, false otherwise
*/","* Check if a method is supported by the server or not.
   * 
   * @param methodName a method's name in String format
   * @param parameterTypes a method's parameter types
   * @return true if the method is supported by the server
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,checkRpcHeaders,org.apache.hadoop.ipc.Server$Connection:checkRpcHeaders(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto),2830,2852,"/**
* Validates RPC request headers for correctness and expected operation.
* @param header RpcRequestHeaderProto object containing header data
*/","* Verify RPC header is valid
     * @param header - RPC request header
     * @throws RpcServerException - header contains invalid values",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,<init>,org.apache.hadoop.ipc.ResponseBuffer:<init>(),33,35,"/**
* Initializes a new ResponseBuffer instance with default capacity.
* @param initialCapacity buffer size in bytes (default is 1024) 
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,forceDecay,org.apache.hadoop.ipc.DecayRpcScheduler:forceDecay(),822,825,"/**
 * Forces immediate cost decay of current costs.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,getPriorityLevel,org.apache.hadoop.ipc.CallQueueManager:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation),265,270,"/**
* Returns priority level based on user's group information.
* @param user UserGroupInformation object
* @return priority level or 0 if not applicable
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getReturnMessage,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)",301,323,"/**
* Retrieves a message for the given RPC method and buffer.
* @param method RPC method
* @param buf RpcWritable.Buffer object
* @return Message object or null if not found, throws ServiceException on error
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,newInstance,"org.apache.hadoop.ipc.RpcWritable$Buffer:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",175,188,"/**
* Creates a new instance of the specified class, optionally configuring it.
* @param valueClass Class to instantiate
* @param conf configuration object
* @return New instance or null if instantiation failed
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getMessage,"org.apache.hadoop.ipc.Server$Connection:getMessage(org.apache.hadoop.thirdparty.protobuf.Message,org.apache.hadoop.ipc.RpcWritable$Buffer)",3046,3057,"/**
* Retrieves a Message object of type T from the provided buffer.
* @param message the message to retrieve
* @param buffer the buffer containing the message data
* @return the retrieved Message object or throws an exception if failed
*/","* Decode the a protobuf from the given input stream 
     * @return Message - decoded protobuf
     * @throws RpcServerException - deserialization failed",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getReturnMessage,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)",311,333,"/**
* Retrieves a response message from the RPC buffer.
* @param method Method object
* @param buf RPC buffer containing the response
* @return Message object or null if not found
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponse,"org.apache.hadoop.ipc.Server:setupResponse(org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",3549,3562,"/**
* Sets up RPC response based on response type and writes it to the Writable.
* @param call RpcCall object
* @param header Response header
* @param rv Writable object containing response data
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,take,org.apache.hadoop.ipc.FairCallQueue:take(),292,296,"/**
* Retrieves an element from the collection using a semaphore.
* @throws InterruptedException if interrupted while waiting for the semaphore
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,poll,"org.apache.hadoop.ipc.FairCallQueue:poll(long,java.util.concurrent.TimeUnit)",298,301,"/**
* Polls and removes the next element from this queue with a specified time limit.
* @param timeout maximum wait time in the specified time unit
* @param unit time unit for the timeout (e.g. milliseconds)
* @return next element or null if timed out or queue is empty",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,poll,org.apache.hadoop.ipc.FairCallQueue:poll(),307,310,"/**
* Retrieves and removes the next element from this queue.
* @return element of type E or null if queue is empty
*/","* poll() provides no strict consistency: it is possible for poll to return
   * null even though an element is in the queue.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Responder:run(),1711,1725,"/**
* Executes the server run loop, initializing and shutting down resources.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doSaslReply,org.apache.hadoop.ipc.Server$Connection:doSaslReply(org.apache.hadoop.thirdparty.protobuf.Message),2420,2426,"/**
* Sends SASL reply to the client.
* @param message incoming message from client
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doSaslReply,org.apache.hadoop.ipc.Server$Connection:doSaslReply(java.lang.Exception),2428,2433,"/**
* Handles SASL authentication failure by sending a fatal response.
* @param ioe exception thrown during authentication
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupBadVersionResponse,org.apache.hadoop.ipc.Server$Connection:setupBadVersionResponse(int),2636,2665,"/**
* Handles bad version responses based on client version.
* @param clientVersion client's protocol version
*/","* Try to set up the response to indicate that the client version
     * is incompatible with the server. This can contain special-case
     * code to speak enough of past IPC protocols to pass back
     * an exception to the caller.
     * @param clientVersion the version the caller is using 
     * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupHttpRequestOnIpcPortResponse,org.apache.hadoop.ipc.Server$Connection:setupHttpRequestOnIpcPortResponse(),2667,2672,"/**
* Sends a mock HTTP response on IPC port.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CombinedIPWhiteList.java,<init>,"org.apache.hadoop.util.CombinedIPWhiteList:<init>(java.lang.String,java.lang.String,long)",31,43,"/**
* Initializes CombinedIPWhiteList with fixed and optional variable white lists.
* @param fixedWhiteListFile file path for fixed whitelist
* @param variableWhiteListFile file path for variable whitelist (null if unused)
* @param cacheExpiryInSeconds time to live for variable whitelist in seconds
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CombinedIPList.java,<init>,"org.apache.hadoop.util.CombinedIPList:<init>(java.lang.String,java.lang.String,long)",33,44,"/**
* Combines fixed and variable blacklists into a single list.
* @param fixedBlackListFile path to file-based blacklist
* @param variableBlackListFile optional path to cacheable blacklist (null if not used)
* @param cacheExpiryInSeconds time-to-live for cached blacklist in seconds
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FileBasedIPList.java,reload,org.apache.hadoop.util.FileBasedIPList:reload(),67,69,"/**
* Reloads and returns a new instance of file-based IP list.
* @return A new FileBasedIPList object with updated data
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getVirtualMemorySize,org.apache.hadoop.util.SysInfoLinux:getVirtualMemorySize(),603,606,"/**
* Calculates total virtual memory size by adding physical and swap sizes.
* @return Total virtual memory size in bytes.",{@inheritDoc},,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.InputStream)",495,499,"/**
* Initializes a bounded input stream from a file.
* @param fs  the underlying file system
* @param file  the path to the file
* @param in  the original input stream
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,open,"org.apache.hadoop.fs.http.AbstractHttpFileSystem:open(org.apache.hadoop.fs.Path,int)",61,67,"/**
* Opens a remote resource at the specified path with given buffer size.
* @param path URI of the resource to open
* @param bufferSize data transfer buffer size
* @return FSDataInputStream for reading from the resource or null on failure
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long,long,int)",1109,1112,"/**
* Constructs a new HarFSDataInputStream instance for the specified file.
* @param fs FileSystem instance
* @param p Path to the file
* @param start starting byte offset
* @param length total bytes to read
* @param bufsize buffer size in bytes
*/","* constructors for har input stream.
     * @param fs the underlying filesystem
     * @param p The path in the underlying filesystem
     * @param start the start position in the part file
     * @param length the length of valid data in the part file
     * @param bufsize the buffer size
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,read,"org.apache.hadoop.fs.FSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int)",217,220,"/**
* Reads data into a buffer from storage.
* @param bufferPool pool of buffers to use
* @param maxLength maximum length of data to read
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,evictExpiredEntries,org.apache.hadoop.util.LightWeightCache:evictExpiredEntries(),164,175,"/**
* Regularly removes expired entries from the queue.
* @throws AssertionError if eviction fails to match the expected entry
*/",Evict expired entries.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,evictEntries,org.apache.hadoop.util.LightWeightCache:evictEntries(),178,184,"/**
* Evicts excess cache entries to maintain a maximum size limit.
* @param none
*/",Evict entries in order to enforce the size limit of the cache.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean)",408,410,"/**
* Constructs a string representation with query and headers options.
* @param qOption whether to include query option
* @param hOption whether to include header option
* @param xOption whether to include extra option (not used in this method)
*/","Return the string representation of the object in the output format.
   * For description of the options,
   * @see #toString(boolean, boolean, boolean, boolean, List)
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output is to be used
   * @param xOption a flag indicating if calculation from snapshots is to be
   *                included in the output
   * @return the string representation of the object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean,java.util.List)",423,426,"/**
* Constructs string representation with default filter option.
* @param qOption query option flag
* @param hOption hide option flag
* @param tOption type option flag
* @param types list of storage types
*/","* Return the string representation of the object in the output format.
   * For description of the options,
   * @see #toString(boolean, boolean, boolean, boolean, List)
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output if to be used
   * @param tOption a flag indicating if display quota by storage types
   * @param types Storage types to display
   * @return the string representation of the object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,toString,"org.apache.hadoop.fs.QuotaUsage:toString(boolean,boolean,java.util.List)",321,327,"/**
* Generates string representation based on type usage or quota.
* @param hOption high-level option
* @param tOption show type usage option
* @param types list of storage types
*/","* Return the string representation of the object in the output format.
   * if hOption is false file sizes are returned in bytes
   * if hOption is true file sizes are returned in human readable
   *
   * @param hOption a flag indicating if human readable output if to be used
   * @param tOption type option.
   * @param types storage types.
   * @return the string representation of the object.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,read,org.apache.hadoop.fs.FSInputChecker:read(),150,159,"/**
* Reads the next byte from the buffer, filling it if necessary.
* @return the read byte value or -1 on EOF
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,read1,"org.apache.hadoop.fs.FSInputChecker:read1(byte[],int,int)",251,275,"/**
* Reads a chunk from the internal buffer into the provided byte array.
* @param b user buffer
* @param off starting offset in user buffer
* @param len length of data to read
* @return number of bytes read or -1 if EOF reached
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,readFields,org.apache.hadoop.io.ObjectWritable$NullInstance:readFields(java.io.DataInput),114,125,"/**
* Reads class name and type from DataInput stream.
* @throws IOException on I/O error
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,readFields,org.apache.hadoop.io.ArrayPrimitiveWritable:readFields(java.io.DataInput),205,251,"/**
* Deserializes a primitive array from the given DataInput stream.
* @param in input stream containing the encoded array data
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,waitForCompletion,org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.List),98,106,"/**
* Awaits completion of all provided futures.
* @param futures list of completable futures to wait for
*/","* Wait for a list of futures to complete. If the list is empty,
   * return immediately.
   *
   * @param futures list of futures.
   * @param <T> Generics Type T.
   * @throws IOException      if one of the called futures raised an IOE.
   * @throws RuntimeException if one of the futures raised one.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,maybeAwaitCompletion,org.apache.hadoop.util.functional.CommonCallableSupplier:maybeAwaitCompletion(java.util.concurrent.CompletableFuture),152,157,"/**
* Waits for asynchronous operation completion if provided.
* @param future nullable CompletableFuture instance to wait on
*/","* Block awaiting completion for any non-null future passed in;
   * No-op if a null arg was supplied.
   * @param future future
   * @throws IOException      if one of the called futures raised an IOE.
   * @throws RuntimeException if one of the futures raised one.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,update,"org.apache.hadoop.util.CrcComposer:update(byte[],int,int,long)",123,138,"/**
* Updates CRC values from byte array.
* @param crcBuffer input byte array
* @param offset starting position in array
* @param length number of CRCs to update
* @param bytesPerCrc CRC size in bytes
*/","* Composes length / CRC_SIZE_IN_BYTES more CRCs from crcBuffer, with
   * each CRC expected to correspond to exactly {@code bytesPerCrc} underlying
   * data bytes.
   *
   * @param crcBuffer crcBuffer.
   * @param offset offset.
   * @param length must be a multiple of the expected byte-size of a CRC.
   * @param bytesPerCrc bytesPerCrc.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,update,"org.apache.hadoop.util.CrcComposer:update(java.io.DataInputStream,long,long)",150,157,"/**
* Updates CRC values from input stream.
* @param checksumIn DataInputStream containing CRC values
* @param numChecksumsToRead number of CRC values to read
* @param bytesPerCrc size of each CRC value in bytes
*/","* Composes {@code numChecksumsToRead} additional CRCs into the current digest
   * out of {@code checksumIn}, with each CRC expected to correspond to exactly
   * {@code bytesPerCrc} underlying data bytes.
   *
   * @param checksumIn checksumIn.
   * @param numChecksumsToRead numChecksumsToRead.
   * @param bytesPerCrc bytesPerCrc.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,hasNext,org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:hasNext(),780,783,"/**
* Checks if there is another item available from data source.
* @throws IOException if an I/O error occurs while checking
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,newInstance,"org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",124,127,"/**
* Creates a new instance of the specified class.
* @param theClass Class to instantiate
*/","Create an object for the given class and initialize it from conf
   * 
   * @param theClass class of which an object is created
   * @param conf Configuration
   * @param <T> Generics Type T.
   * @return a new object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getKeyClass,org.apache.hadoop.io.MapFile$Reader:getKeyClass(),465,465,"/**
* Retrieves the key class associated with the data.
*/","* Returns the class of keys in this file.
     *
     * @return keyClass.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getValueClass,org.apache.hadoop.io.MapFile$Reader:getValueClass(),472,472,"/**
* Returns the class type of the value associated with the data.
*/","* Returns the class of values in this file.
     *
     * @return Value Class.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,run,org.apache.hadoop.util.FindClass:run(java.lang.String[]),310,335,"/**
* Executes a command with two arguments: action and name.
* @param args array of command-line arguments
* @return integer status code (SUCCESS or ERROR)
*/","* Run the class/resource find or load operation
   * @param args command specific arguments.
   * @return the outcome
   * @throws Exception if something went very wrong",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",78,84,"/**
* Initializes a new FutureDataInputStreamBuilderImpl instance with the specified file context and path.
* @param fc non-null FileContext object
* @param path non-null Path to initialize the builder with
*/","* Construct from a {@link FileContext}.
   *
   * @param fc FileContext
   * @param path path.
   * @throws IOException failure",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newDataChecksum,"org.apache.hadoop.util.DataChecksum:newDataChecksum(byte[],int)",160,181,"/**
* Creates a new DataChecksum object from the given byte array, starting at offset.
* @param bytes input byte array
* @param offset starting position in the byte array
* @return DataChecksum object or throws InvalidChecksumSizeException if invalid
*/","* Creates a DataChecksum from HEADER_LEN bytes from arr[offset].
   *
   * @param bytes bytes.
   * @param offset offset.
   * @return DataChecksum of the type in the array or null in case of an error.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newDataChecksum,org.apache.hadoop.util.DataChecksum:newDataChecksum(java.io.DataInputStream),192,202,"/**
* Creates a new DataChecksum instance from an input stream.
* @param in Input stream containing checksum parameters
* @return DataChecksum object or null if invalid parameters are encountered
*/","* This constructs a DataChecksum by reading HEADER_LEN bytes from input
   * stream <i>in</i>.
   *
   * @param in data input stream.
   * @throws IOException raised on errors performing I/O.
   * @return DataChecksum by reading HEADER_LEN
   *         bytes from input stream.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,run,org.apache.hadoop.util.Shell:run(),951,960,"/**
* Executes the command and resets exit code.
* @throws IOException on execution error
*/","* Check to see if a command needs to be executed and execute if needed.
   *
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:<init>(java.lang.String),148,210,"/**
* Initializes DynamicWrappedIO instance with specified class name.
* @param classname unique class identifier
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:<init>(java.lang.String),228,346,"/**
* Initializes a DynamicWrappedStatistics object for the given class.
* @param classname name of the class to wrap
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_aggregate,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_aggregate(java.io.Serializable,java.lang.Object)",502,507,"/**
* Aggregates I/O statistics from a snapshot.
* @param snapshot IoStatistics snapshot object
* @param statistics Additional statistics to aggregate (may be null)
*/","* Aggregate an existing {@code IOStatisticsSnapshot} with
   * the supplied statistics.
   * @param snapshot snapshot to update
   * @param statistics IOStatistics to add
   * @return true if the snapshot was updated.
   * @throws IllegalArgumentException if the {@code statistics} argument is not
   * null but not an instance of IOStatistics, or if  {@code snapshot} is invalid.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_create(),514,518,"/**
* Creates an IO statistics snapshot.
* @throws UnsupportedOperationException if not supported
*/","* Create a new {@code IOStatisticsSnapshot} instance.
   * @return an empty IOStatisticsSnapshot.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_create(java.lang.Object),527,532,"/**
* Creates I/O statistics snapshot with optional source object.
* @param source optional source object (can be null)","* Create a new {@code IOStatisticsSnapshot} instance.
   * @param source optional source statistics
   * @return an IOStatisticsSnapshot.
   * @throws ClassCastException if the {@code source} is not valid.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_toJsonString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_toJsonString(java.io.Serializable),541,545,"/**
* Converts IO statistics snapshot to JSON string.
* @param snapshot IO statistics snapshot object
*/","* Save IOStatisticsSnapshot to a JSON string.
   * @param snapshot statistics; may be null or of an incompatible type
   * @return JSON string value or null if source is not an IOStatisticsSnapshot
   * @throws UncheckedIOException Any IO/jackson exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_fromJsonString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_fromJsonString(java.lang.String),554,558,"/**
* Creates iostatistics snapshot from JSON string.
* @param json input JSON data
*/","* Load IOStatisticsSnapshot from a JSON string.
   * @param json JSON string value.
   * @return deserialized snapshot.
   * @throws UncheckedIOException Any IO/jackson exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_load,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",568,573,"/**
* Loads IO statistics snapshot from specified file system and path.
* @param fs the file system to load from
* @param path the path to the snapshot file
*/","* Load IOStatisticsSnapshot from a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @return the loaded snapshot
   * @throws UncheckedIOException Any IO exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_retrieve,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_retrieve(java.lang.Object),581,585,"/**
* Retrieves IO statistics snapshot using the invoke method.
* @param source can be null
*/","* Extract the IOStatistics from an object in a serializable form.
   * @param source source object, may be null/not a statistics source/instance
   * @return {@code IOStatisticsSnapshot} or null if the object is null/doesn't have statistics
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_save,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_save(java.io.Serializable,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",596,604,"/**
* Saves IO statistics snapshot to file system.
* @param snapshot snapshot data
* @param fs file system for saving
* @param path file path for saving
* @param overwrite whether to overwrite existing file
*/","* Save IOStatisticsSnapshot to a Hadoop filesystem as a JSON file.
   * @param snapshot statistics
   * @param fs filesystem
   * @param path path
   * @param overwrite should any existing file be overwritten?
   * @throws UncheckedIOException Any IO exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_toPrettyString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_toPrettyString(java.lang.Object),666,669,"/**
* Converts IO statistics to a pretty string representation.
* @param statistics IO statistics object
*/","* Convert IOStatistics to a string form, with all the metrics sorted
   * and empty value stripped.
   * @param statistics A statistics instance.
   * @return string value or the empty string if null
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_getCurrent,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_getCurrent(),439,443,"/**
* Retrieves current I/O statistics context.
*/","* Get the context's {@code IOStatisticsContext} which
   * implements {@code IOStatisticsSource}.
   * This is either a thread-local value or a global empty context.
   * @return instance of {@code IOStatisticsContext}.
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_setThreadIOStatisticsContext,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_setThreadIOStatisticsContext(java.lang.Object),451,455,"/**
* Sets thread IO statistics context.
* @param statisticsContext optional IO statistics context object
*/","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_reset,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_reset(),462,466,"/**
* Resets IO statistics context.
*/","* Reset the context's IOStatistics.
   * {@code IOStatisticsContext#reset()}
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_snapshot,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_snapshot(),474,478,"/**
* Retrieves IO statistics context snapshot.
* @throws UnsupportedOperationException if not available
*/","* Take a snapshot of the context IOStatistics.
   * {@code IOStatisticsContext#snapshot()}
   * @return an instance of {@code IOStatisticsSnapshot}.
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_aggregate,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_aggregate(java.lang.Object),487,490,"/**
* Aggregates IO statistics context with given data.
* @param source data to aggregate
* @return true if aggregation successful, false otherwise
*/","* Aggregate into the IOStatistics context the statistics passed in via
   * IOStatistics/source parameter.
   * <p>
   * Returns false if the source is null or does not contain any statistics.
   * @param source implementation of {@link IOStatisticsSource} or {@link IOStatistics}
   * @return true if the the source object was aggregated.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionUtil.java,compareVersions,"org.apache.hadoop.util.VersionUtil:compareVersions(java.lang.String,java.lang.String)",39,43,"/**
* Compares two software versions as strings.
* @param version1 first version to compare
* @param version2 second version to compare
* @return negative/zero/positive if version1 is less than/equal to/greater than version2
*/","* Compares two version name strings using maven's ComparableVersion class.
   *
   * @param version1
   *          the first version to compare
   * @param version2
   *          the second version to compare
   * @return a negative integer if version1 precedes version2, a positive
   *         integer if version2 precedes version1, and 0 if and only if the two
   *         versions are equal.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refreshInternal,"org.apache.hadoop.util.HostsFileReader:refreshInternal(java.lang.String,java.lang.String,boolean)",200,225,"/**
* Updates hosts list by reading from files and setting as current or lazy-loaded.
* @param includesFile file containing included host list
* @param excludesFile file containing excluded host list
* @param lazy whether to use lazy loading (true) or update current state (false)
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,<init>,"org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.io.InputStream,java.lang.String,java.io.InputStream)",67,75,"/**
* Initializes HostsFileReader with included and excluded files.
* @param includesFile path to include file
* @param inFileInputStream input stream for include file
* @param excludesFile path to exclude file
* @param exFileInputStream input stream for exclude file
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,<init>,"org.apache.hadoop.util.bloom.CountingBloomFilter:<init>(int,int,int)",93,96,"/**
* Initializes Counting Bloom Filter with specified parameters.
* @param vectorSize size of the filter's underlying vector
* @param nbHash number of hash functions to use
* @param hashType type of hash function (not further explained in this comment)
*/","* Constructor
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,<init>,"org.apache.hadoop.util.bloom.BloomFilter:<init>(int,int,int)",110,114,"/**
* Initializes a Bloom filter with specified parameters.
* @param vectorSize size of the filter vector
* @param nbHash number of hash functions to use
* @param hashType type of hashing algorithm to employ
*/","* Constructor
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,readFields,org.apache.hadoop.util.bloom.CountingBloomFilter:readFields(java.io.DataInput),301,309,"/**
* Reads bucket values from data stream.
* @throws IOException if I/O error occurs
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,readFields,org.apache.hadoop.util.bloom.BloomFilter:readFields(java.io.DataInput),218,233,"/**
* Reads vector data from input stream and populates BitSet.
* @param in input stream to read from
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,probablyHasKey,org.apache.hadoop.io.BloomMapFile$Reader:probablyHasKey(org.apache.hadoop.io.WritableComparable),264,272,"/**
* Checks if the Bloom filter probably contains a specified key.
* @param key WritableComparable to test membership
*/","* Checks if this MapFile has the indicated key. The membership test is
     * performed using a Bloom filter, so the result has always non-zero
     * probability of false positives.
     * @param key key to check
     * @return  false iff key doesn't exist, true if key probably exists.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,selectiveClearing,"org.apache.hadoop.util.bloom.RetouchedBloomFilter:selectiveClearing(org.apache.hadoop.util.bloom.Key,short)",199,235,"/**
* Performs selective clearing based on the provided key and scheme.
* @param k Key to be processed
* @param scheme Selective clearing scheme (RANDOM, MINIMUM_FN, MAXIMUM_FP, RATIO)
*/","* Performs the selective clearing for a given key.
   * @param k The false positive key to remove from <i>this</i> retouched Bloom filter.
   * @param scheme The selective clearing scheme to apply.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,createOptionTableListing,org.apache.hadoop.fs.FsShell:createOptionTableListing(),292,295,"/**
* Creates an option table listing with default field and width settings.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadResources,"org.apache.hadoop.conf.Configuration:loadResources(java.util.Properties,java.util.ArrayList,int,boolean,boolean)",3082,3100,"/**
* Loads and updates resources from properties.
* @param properties configuration properties
* @param resources list of resources to update
* @param startIdx starting index for resource loading
* @param fullReload force reload all default resources
* @param quiet suppress output if true
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String,java.lang.String)",616,619,"/**
* Adds deprecation for a single key.
* @param key deprecated key
* @param newKey replacement key
* @param customMessage custom deprecation message
*/","* Adds the deprecated key to the global deprecation map.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   *
   * @param key to be deprecated
   * @param newKey key that take up the values of deprecated key
   * @param customMessage deprecation message",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String[])",640,643,"/**
* Adds deprecation information with default configuration.
* @param key unique identifier for deprecation
* @param newKeys list of added keys or null to use defaults
*/","* Adds the deprecated key to the global deprecation map when no custom
   * message is provided.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If a key is deprecated in favor of multiple keys, they are all treated as 
   * aliases of each other, and setting any one of them resets all the others 
   * to the new value.
   * 
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   *
   * @param key Key that is to be deprecated
   * @param newKeys list of keys that take up the values of deprecated key
   * @deprecated use {@link #addDeprecation(String key, String newKey)} instead",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String)",659,661,"/**
* Adds deprecation notice for the specified key.
* @param key old key to be deprecated
* @param newKey replacement key (may be null if no replacement)
*/","* Adds the deprecated key to the global deprecation map when no custom
   * message is provided.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   *
   * @param key Key that is to be deprecated
   * @param newKey key that takes up the value of deprecated key",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,start,org.apache.hadoop.http.HttpServer2:start(),1382,1434,"/**
* Initializes and starts the web server, registering metrics as needed.
*/","* Start the server. Does not wait for the server to start.
   *
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,writeBreadCrumbNode,org.apache.hadoop.ha.ActiveStandbyElector:writeBreadCrumbNode(org.apache.zookeeper.data.Stat),963,977,"/**
* Writes or updates ZNode to indicate current local node status.
* @param oldBreadcrumbStat previous breadcrumb stat (null if new)
*/","* Write the ""ActiveBreadCrumb"" node, indicating that this node may need
   * to be fenced on failover.
   * @param oldBreadcrumbStat",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,tryDeleteOwnBreadCrumbNode,org.apache.hadoop.ha.ActiveStandbyElector:tryDeleteOwnBreadCrumbNode(),985,1008,"/**
* Deletes this node's breadcrumb from ZooKeeper if still active.
* @throws IllegalStateException if data mismatch or deletion fails
*/","* Try to delete the ""ActiveBreadCrumb"" node when gracefully giving up
   * active status.
   * If this fails, it will simply warn, since the graceful release behavior
   * is only an optimization.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readVectored,"org.apache.hadoop.fs.VectoredReadUtils:readVectored(org.apache.hadoop.fs.PositionedReadable,java.util.List,java.util.function.IntFunction)",98,104,"/**
* Reads vectored data from the stream into specified file ranges.
* @param stream PositionedReadable stream to read from
* @param ranges List of FileRange objects to populate with data
* @param allocate IntFunction to allocate ByteBuffer for each range
*/","* This is the default implementation which iterates through the ranges
   * to read each synchronously, but the intent is that subclasses
   * can make more efficient readers.
   * The data or exceptions are pushed into {@link FileRange#getData()}.
   * @param stream the stream to read the data from
   * @param ranges the byte ranges to read
   * @param allocate the byte buffer allocation
   * @throws IllegalArgumentException if there are overlapping ranges or a range is invalid
   * @throws EOFException the range offset is negative",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,cancelPrefetches,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:cancelPrefetches(),294,306,"/**
* Cancels pending prefetches and adds prefetched blocks to local cache.
*/",* Requests cancellation of any previously issued prefetch requests.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,get,org.apache.hadoop.fs.impl.prefetch.BlockManager:get(int),77,86,"/**
* Retrieves data for a specified block number.
* @param blockNumber unique block identifier
* @return BufferData object containing block data or null on failure
*/","* Gets the block having the given {@code blockNumber}.
   *
   * The entire block is read into memory and returned as a {@code BufferData}.
   * The blocks are treated as a limited resource and must be released when
   * one is done reading them.
   *
   * @param blockNumber the number of the block to be read and returned.
   * @return {@code BufferData} having data from the given block.
   *
   * @throws IOException if there an error reading the given block.
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,readBlock,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:readBlock(org.apache.hadoop.fs.impl.prefetch.BufferData,boolean,org.apache.hadoop.fs.impl.prefetch.BufferData$State[])",331,395,"/**
* Reads a block from cache or network, handling prefetch and caching logic.
* @param data BufferData object containing block information
* @param isPrefetch true to perform prefetch operation, false for read
* @param expectedState one of DONE, READY, or other states to verify against
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,<init>,"org.apache.hadoop.fs.impl.prefetch.FilePosition:<init>(long,int)",83,95,"/**
* Initializes FilePosition object with fileSize and blockSize parameters.
* @param fileSize total size of the file
* @param blockSize size of each block in the file
*/","* Constructs an instance of {@link FilePosition}.
   *
   * @param fileSize size of the associated file.
   * @param blockSize size of each block within the file.
   *
   * @throws IllegalArgumentException if fileSize is negative.
   * @throws IllegalArgumentException if blockSize is zero or negative.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,isLastBlock,org.apache.hadoop.fs.impl.prefetch.FilePosition:isLastBlock(),204,206,"/**
* Determines whether the current block is the last one.
* @return true if this is the last block, false otherwise
*/","* Determines whether the current block is the last block in this file.
   *
   * @return true if the current block is the last block in this file, false otherwise.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,toString,org.apache.hadoop.fs.impl.prefetch.FilePosition:toString(),275,296,"/**
* Returns a human-readable string representation of the object.
* @return formatted string with block number, position, and current buffer state
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem:closeAll(),642,645,"/**
* Closes all open resources and cache.
* @throws IOException if an I/O error occurs during closure
*/","* Close all cached FileSystem instances. After this operation, they
   * may not be used in any operations.
   *
   * @throws IOException a problem arose closing one or more filesystem.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,readFully,"org.apache.hadoop.fs.BufferedFSInputStream:readFully(long,byte[])",125,128,"/**
* Reads fully from input stream at specified position into given buffer.
* @param position current file position to start reading from
* @param buffer array to store read data
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence)",2044,2047,"/**
* Writes a CharSequence to a file at the specified Path using UTF-8 encoding.
* @param fs target file system
* @param path file location
* @param charseq text content
*/","* Write a line of text to a file. Characters are encoded into bytes using
   * UTF-8. This utility method opens the file for writing, creating the file if
   * it does not exist, or overwrites an existing file.
   *
   * @param fs the files system with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],long,long,boolean)",122,125,"/**
* Constructs a BlockLocation with default metadata.
* @param names array of block names
* @param hosts array of hostnames
* @param offset starting byte offset within the block
* @param length total bytes occupied by the block
* @param corrupt flag indicating if the block is corrupted
*/","* Constructor with host, name, offset, length and corrupt flag.
   * @param names names.
   * @param hosts hosts.
   * @param offset offset.
   * @param length length.
   * @param corrupt corrupt.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],long,long)",135,138,"/**
* Constructs a BlockLocation object with specified parameters.
* @param names array of block names
* @param hosts array of host identifiers
* @param topologyPaths array of topology paths
* @param offset starting position in bytes
* @param length total size in bytes
*/","* Constructor with host, name, network topology, offset and length.
   * @param names names.
   * @param hosts hosts.
   * @param topologyPaths topologyPaths.
   * @param offset offset.
   * @param length length.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,clone,org.apache.hadoop.fs.statistics.MeanStatistic:clone(),271,274,"/**
* Creates a deep copy of this MeanStatistic instance. 
* @return a cloned MeanStatistic object with same values as this instance.",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,<init>,org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:<init>(),58,59,"/**
 * Initializes dynamic I/O statistics. 
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,aggregateMeanStatistics,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMeanStatistics(org.apache.hadoop.fs.statistics.MeanStatistic,org.apache.hadoop.fs.statistics.MeanStatistic)",312,317,"/**
* Merges two mean statistics into one.
* @param l left statistic
* @param r right statistic
* @return combined MeanStatistic object
*/","* Aggregate the mean statistics.
   * This returns a new instance.
   * @param l left value
   * @param r right value
   * @return aggregate value",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,snapshot,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:snapshot(org.apache.hadoop.fs.statistics.IOStatistics),160,168,"/**
* Synchronizes all counters and statistics with the provided source.
* @param source IOStatistics object containing data to synchronize
*/","* Take a snapshot.
   *
   * This completely overwrites the map data with the statistics
   * from the source.
   * @param source statistics source.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,logIOStatisticsAtDebug,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(java.lang.String,java.lang.Object)",250,254,"/**
* Logs IO statistics at debug level.
* @param message custom log message
* @param source object providing context for logging
*/","* Extract any statistics from the source and log to
   * this class's log at debug, if
   * the log is set to log at debug.
   * No-op if logging is not at debug or the source is null/of
   * the wrong type/doesn't provide statistics.
   * @param message message for log -this must contain ""{}"" for the
   * statistics report to actually get logged.
   * @param source source object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,logIOStatisticsAtLevel,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtLevel(org.slf4j.Logger,java.lang.String,java.lang.Object)",263,281,"/**
* Logs IO statistics at specified level.
* @param log Logger instance
* @param level logging level (INFO, ERROR, WARN)
* @param source Object providing IO statistics
*/","* A method to log IOStatistics from a source at different levels.
   *
   * @param log    Logger for logging.
   * @param level  LOG level.
   * @param source Source to LOG.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,cleanupRemoteIterator,org.apache.hadoop.util.functional.RemoteIterators:cleanupRemoteIterator(org.apache.hadoop.fs.RemoteIterator),297,304,"/**
* Closes and logs remote iterator resources.
* @param source RemoteIterator instance to cleanup
*/","* Clean up after an iteration.
   * If the log is at debug, calculate and log the IOStatistics.
   * If the iterator is closeable, cast and then cleanup the iterator
   * @param source iterator source
   * @param <T> type of source",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,trackDurationOfInvocation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfInvocation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)",460,466,"/**
* Tracks invocation duration using provided factory and measures it.
* @param factory factory for tracking durations
* @param statistic type of statistic to track
* @throws IOException if an I/O error occurs during measurement
*/","* Given an IOException raising callable/lambda expression,
   * execute it and update the relevant statistic.
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @throws IOException IO failure.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreBuilderImpl.java,build,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:build(),107,111,"/**
* Builds and returns an instance of IOStatisticsStore.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,org.apache.hadoop.crypto.CryptoInputStream:read(),779,782,"/**
* Reads a single byte from the underlying stream.
* @return The read byte value or -1 on end-of-stream
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(long,byte[],int,int)",332,348,"/**
* Reads data from a positioned stream, decrypting it as needed.
* @param position current read position
* @param buffer storage for read data
* @param offset starting index in the buffer to write data
* @param length number of bytes to read and decrypt
* @return actual number of bytes read and decrypted
*/",Positioned read. It is thread-safe,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFully,"org.apache.hadoop.crypto.CryptoInputStream:readFully(long,byte[],int,int)",502,515,"/**
* Reads fully from a positioned stream.
* @param position current file position
* @param buffer data buffer to read into
* @param offset starting offset in the buffer
* @param length number of bytes to read
*/",Positioned read fully. It is thread-safe,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(long,java.nio.ByteBuffer)",353,369,"/**
* Reads data from stream into a ByteBuffer at specified position.
* @param position stream position to read from
* @param buf target ByteBuffer for read data
* @return number of bytes read or -1 if end of file reached
*/",* Positioned read using {@link ByteBuffer}s. This method is thread-safe.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFully,"org.apache.hadoop.crypto.CryptoInputStream:readFully(long,java.nio.ByteBuffer)",374,389,"/**
* Reads fully from stream at specified position into ByteBuffer.
* @param position target position in stream
* @param buf buffer to read into
*/",* Positioned readFully using {@link ByteBuffer}s. This method is thread-safe.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,org.apache.hadoop.crypto.CryptoInputStream:read(java.nio.ByteBuffer),588,639,"/**
* Reads data from a ByteBuffer or ReadableByteChannel into the output buffer.
* @param buf input buffer
* @return number of bytes read (or -1 on error)
*/",ByteBuffer read.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)",707,736,"/**
* Reads data from the input stream into a ByteBuffer.
* @param bufferPool Buffer pool to use for allocation
* @param maxLength Maximum length of data to read
* @param opts Read options (e.g. encryption)
* @return Decrypted ByteBuffer or null if no data available
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,<init>,"org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",118,122,"/**
* Initializes a secure input stream with the provided parameters.
* @param in underlying input stream
* @param codec cryptographic codec to use
* @param bufferSize buffer size for encryption/decryption
* @param key encryption key
* @param iv initialization vector for encryption
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,doEncode,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:doEncode(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][])",101,118,"/**
* RS encodes input byte-stripe and adds parity data with optional piggybacks.
* @param inputs[][] input byte-stripe arrays
* @param outputs[][] output byte-stripe arrays
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",50,54,"/**
* Performs encoding on input chunks and stores result in output chunks.
* @param inputChunks array of chunks to encode
* @param outputChunks array of chunks to store encoded result
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,writeObject,"org.apache.hadoop.io.ObjectWritable:writeObject(java.io.DataOutput,java.lang.Object,java.lang.Class,org.apache.hadoop.conf.Configuration,boolean)",165,234,"/**
* Writes an object to a DataOutput stream, handling various types and compact arrays.
* @param out the output stream
* @param instance the object to be written
* @param declaredClass the class of the object being written
* @param conf configuration (not used in this method)
* @param allowCompactArrays whether to enable compact array writing
*/","* Write a {@link Writable}, {@link String}, primitive type, or an array of
     * the preceding.  
     * 
     * @param allowCompactArrays - set true for RPC and internal or intra-cluster
     * usages.  Set false for inter-cluster, File, and other persisted output 
     * usages, to preserve the ability to interchange files with other clusters 
     * that may not be running the same version of software.  Sometime in ~2013 
     * we can consider removing this parameter and always using the compact format.
     *
     * @param conf configuration.
     * @param out dataoutput.
     * @param declaredClass declaredClass.
     * @param instance instance.
     * @throws IOException raised on errors performing I/O.
     *",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,tryAcquire,org.apache.hadoop.fs.impl.prefetch.BufferPool:tryAcquire(int),157,159,"/**
* Tries to acquire buffer data by block number.
* @param blockNumber unique block identifier
* @return BufferData object or null if not acquired
*/","* Acquires a buffer if one is immediately available. Otherwise returns null.
   * @param blockNumber the id of the block to try acquire.
   * @return the acquired block's {@code BufferData} or null.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,numAvailable,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numAvailable(),600,602,"/**
* Returns the number of available buffers in the pool.
* @return Number of available buffers
*/","* Number of ByteBuffers available to be acquired.
   *
   * @return the number of available buffers.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,run,org.apache.hadoop.util.functional.TaskPool$Builder:run(org.apache.hadoop.util.functional.TaskPool$Task),268,282,"/**
* Runs a task with optional exception handling and parallel or single-threaded execution based on service availability.
* @param task Task to be executed (includes input and potential exceptions)
*/","* Execute the task across the data.
     * @param task task to execute
     * @param <E> exception which may be raised in execution.
     * @return true if the operation executed successfully
     * @throws E any exception raised.
     * @throws IOException IOExceptions raised by remote iterator or in execution.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPaths,"org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])",342,351,"/**
* Processes multiple paths and their children.
* @param parent parent path data
* @param items child path data to process
*/","*  Iterates over the given expanded paths and invokes
   *  {@link #processPath(PathData)} on each element.  If ""recursive"" is true,
   *  will do a post-visit DFS on directories.
   *  @param parent if called via a recurse, will be the parent dir, else null
   *  @param items a list of {@link PathData} objects to process
   *  @throws IOException if anything goes wrong...",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,getPathHandle,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:getPathHandle(org.apache.hadoop.fs.Path),163,166,"/**
* Returns the file system's path handle for the given file path.
* @param filePath file path to retrieve handle for
* @return PathHandle object or null if not available
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,resolve,"org.apache.hadoop.fs.Options$HandleOpt:resolve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Options$HandleOpt[])",359,362,"/**
* Resolves FileStatus to PathHandle using provided FileSystem and options.
* @param fs file system instance
* @param opt handle options (variable arguments)
*/","* Utility function for mapping {@link FileSystem#getPathHandle} to a
     * fixed set of handle options.
     * @param fs Target filesystem
     * @param opt Options to bind in partially evaluated function
     * @return Function reference with options fixed",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createPathHandle,"org.apache.hadoop.fs.FilterFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",177,180,"/**
* Creates a PathHandle instance based on file status and options.
* @param stat FileStatus object describing the file
* @param opts Options for handling the path (varargs)
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,createGroupExecutor,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:createGroupExecutor(java.lang.String),132,135,"/**
* Creates a shell command executor for group operations.
* @param userName user name to fetch groups for
*/","* Create a ShellCommandExecutor object using the user's name.
   *
   * @param userName user's name
   * @return a ShellCommandExecutor object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,createGroupIDExecutor,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:createGroupIDExecutor(java.lang.String),153,156,"/**
* Creates an executor to fetch group IDs for the specified user.
* @param userName name of the user
*/","* Create a ShellCommandExecutor object for fetch a user's group id list.
   *
   * @param userName the user's name
   * @return a ShellCommandExecutor object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map)",1222,1225,"/**
* Constructs a new ShellCommandExecutor instance.
* @param execString array of command strings to execute
* @param dir directory where the command will be executed from
* @param env environment variables to pass to the command
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execCommand,"org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[],long)",1373,1379,"/**
* Executes a shell command with environment variables.
* @param env map of environment variables
* @param cmd array of command arguments
* @param timeout execution timeout in milliseconds
* @return output of the executed command as a string, or null if failed
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readProto,org.apache.hadoop.security.Credentials:readProto(java.io.DataInput),403,413,"/**
* Reads credentials from a delimited protobuf stream.
* @param in input stream containing credentials data
*/","* Populates keys/values from proto buffer storage.
   * @param in - stream ready to read a serialized proto buffer message",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,addAll,"org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials,boolean)",463,476,"/**
* Merges credentials from another instance into this one.
* @param other Credentials to merge
* @param overwrite Whether to overwrite existing keys or tokens
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DelegationTokenIssuer.java,collectDelegationTokens,"org.apache.hadoop.security.token.DelegationTokenIssuer:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)",98,138,"/**
* Collects delegation tokens from the given issuer and its children.
* @param issuer DelegationTokenIssuer object
* @param renewer string identifier for token renewal
* @param credentials Credentials object containing existing tokens
* @throws IOException if an I/O error occurs
*/","* NEVER call this method directly.
   *
   * @param issuer issuer.
   * @param renewer renewer.
   * @param credentials cache in which to add new delegation tokens.
   * @param tokens list of new delegation tokens.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addToken,"org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)",1712,1717,"/**
* Adds a new token to the credentials with the given alias.
* @param alias Text alias for the token
* @param token Token object to add
* @return True if added successfully; otherwise false
*/","* Add a named token to this UGI
   * 
   * @param alias Name of the token
   * @param token Token to be added
   * @return true on successful add of new token",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,addRegexMountEntry,org.apache.hadoop.fs.viewfs.InodeTree:addRegexMountEntry(org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry),807,816,"/**
* Adds a regular expression mount point entry with specified settings.
* @param le LinkEntry containing mount point details
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,makeAbsolute,"org.apache.hadoop.fs.sftp.SFTPFileSystem:makeAbsolute(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",176,181,"/**
* Converts a relative path to an absolute one
* @param workDir base directory for the conversion
* @param path relative path to convert
* @return absolute Path object or original if already absolute
*/","* Resolve against given working directory.
   *
   * @param workDir
   * @param path
   * @return absolute path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,makeAbsolute,"org.apache.hadoop.fs.ftp.FTPFileSystem:makeAbsolute(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",269,274,"/**
* Converts relative path to absolute using provided working directory.
* @param workDir the base directory
* @param path the relative path
* @return absolute Path object or original if already absolute
*/","* Resolve against given working directory. *
   * 
   * @param workDir
   * @param path
   * @return",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,makeAbsolute,org.apache.hadoop.fs.RawLocalFileSystem:makeAbsolute(org.apache.hadoop.fs.Path),105,111,"/**
* Converts a relative path to an absolute one by adding the working directory.
* @param f input path to convert
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,pathToFile,org.apache.hadoop.fs.RawLocalFileSystem:pathToFile(org.apache.hadoop.fs.Path),119,125,"/**
* Resolves a file path from a given Path object, handling relative paths.
* @param path the input Path to resolve
* @return a File representing the resolved path or null if invalid
*/","* Convert a path to a File.
   *
   * @param path the path.
   * @return file.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,fixRelativePart,org.apache.hadoop.fs.FileSystem:fixRelativePart(org.apache.hadoop.fs.Path),2884,2890,"/**
* Resolves relative paths to absolute paths.
* @param p input path
*/","* See {@link FileContext#fixRelativePart}.
   * @param p the path.
   * @return relative part.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,makeAbsolute,org.apache.hadoop.fs.viewfs.ViewFileSystem:makeAbsolute(org.apache.hadoop.fs.Path),269,271,"/**
* Converts relative path to absolute path if necessary.
* @param f input file path
* @return absolute file path
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),188,191,"/**
* Sets the working directory to the specified path.
* If the input is not absolute, it's resolved relative to the current working directory. 
* @param new_dir new working directory as a Path object
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,makeQualified,"org.apache.hadoop.fs.Path:makeQualified(java.net.URI,org.apache.hadoop.fs.Path)",562,598,"/**
* Makes a qualified path from the current path and a default URI.
* @param defaultUri default URI to use for qualification
* @param workingDir directory to resolve relative paths against
* @return Qualified Path object or null if not possible
*/","* Returns a qualified path object.
   *
   * @param defaultUri if this path is missing the scheme or authority
   * components, borrow them from this URI
   * @param workingDir if this path isn't absolute, treat it as relative to this
   * working directory
   * @return this path if it contains a scheme and authority and is absolute, or
   * a new path that includes a path and authority and is fully qualified",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,fixRelativePart,org.apache.hadoop.fs.FileContext:fixRelativePart(org.apache.hadoop.fs.Path),284,291,"/**
* Normalizes the path's relative part to absolute by prepending the working directory.
* @param p input path to normalize
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.HarFileSystem:getWorkingDirectory(),273,276,"/**
* Returns a Path representing the working directory.
* @return Path object containing URI string representation
*/",* return the top level archive.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.HarFileSystem:getHomeDirectory(),809,812,"/**
 * Returns the home directory path based on the provided URI.
 * @return Path object representing the home directory or null if not found
 */",* return the top level archive path.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getHomeDirectory(com.jcraft.jsch.ChannelSftp),680,686,"/**
* Retrieves the home directory path from an SFTP channel.
* @param channel active SFTP connection
* @return Path to home directory or null on failure
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,org.apache.hadoop.fs.ChecksumFs:<init>(org.apache.hadoop.fs.AbstractFileSystem),58,63,"/**
* Initializes a ChecksumFs instance with the given underlying file system.
* @param theFs AbstractFileSystem to use for checksum operations
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.RawLocalFileSystem:getHomeDirectory(),842,845,"/**
* Returns the qualified home directory path.
* @return absolute path to user's home directory or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.RawLocalFileSystem:getInitialWorkingDirectory(),861,864,"/**
* Returns the initial working directory as a qualified file path.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,abort,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:abort(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path)",245,261,"/**
* Aborts an upload by deleting the associated file.
* @param uploadId unique identifier of the upload to abort
* @param filePath path to the file being uploaded
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,lookupStat,"org.apache.hadoop.fs.shell.PathData:lookupStat(org.apache.hadoop.fs.FileSystem,java.lang.String,boolean)",175,186,"/**
 * Retrieves the file status for a given path, handling FileNotFoundException.
 * @param fs Hadoop FileSystem instance
 * @param pathString the path to look up
 * @param ignoreFNF whether to throw an exception on FileNotFound or return null
 * @return FileStatus object if found, or null if ignored
 */","* Get the FileStatus info
   * @param ignoreFNF if true, stat will be null if the path doesn't exist
   * @return FileStatus for the given path
   * @throws IOException if anything goes wrong",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,getPath,org.apache.hadoop.fs.PathIOException:getPath(),107,107,"/**
 * Returns a new Path instance based on the internal path representation.
 */",@return Path that generated the exception,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,getTargetPath,org.apache.hadoop.fs.PathIOException:getTargetPath(),110,112,"/**
* Returns the target path as a Path object or null if not set.
*/","@return Path if the operation involved copying or moving, else null",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getUsed,org.apache.hadoop.fs.FileSystem:getUsed(),2719,2722,"/**
* Retrieves total used space in bytes for the specified file system.
* @throws IOException if an I/O error occurs
*/","* Return the total size of all files in the filesystem.
   * @throws IOException IO failure
   * @return the number of path used.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.viewfs.ViewFileSystem:getHomeDirectory(),422,434,"/**
* Returns the user's home directory path.
* @return Path object representing the user's home directory
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getMountPoints,org.apache.hadoop.fs.viewfs.ViewFileSystem:getMountPoints(),1060,1070,"/**
* Retrieves an array of available mount points.
* @return Array of MountPoint objects
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>(org.apache.hadoop.fs.FileSystem,java.net.URI)",105,116,"/**
* Initializes a ChRootedFileSystem with a given file system and URI.
* @param fs underlying file system
* @param uri URI specifying the root directory path
*/","* Constructor
   * @param fs base file system
   * @param uri base uri
   * @throws IOException",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getResolvedQualifiedPath,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getResolvedQualifiedPath(org.apache.hadoop.fs.Path),177,181,"/**
* Resolves and qualifies an input path by prepending to the root part.
* @param f input path to be resolved
* @return qualified path or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getHomeDirectory,org.apache.hadoop.fs.viewfs.ViewFs:getHomeDirectory(),314,326,"/**
* Returns the user's home directory path.
* @return Path to user's home directory or null if not initialized
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getMountPoints,org.apache.hadoop.fs.viewfs.ViewFs:getMountPoints(),720,730,"/**
* Retrieves an array of file system mount points.
* @return Array of MountPoint objects representing mounted directories
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,getRemainingPathStr,"org.apache.hadoop.fs.viewfs.RegexMountPoint:getRemainingPathStr(java.lang.String,java.lang.String)",207,215,"/**
* Extracts and returns the remaining path from a source path.
* @param srcPath original path string
* @param resolvedPathStr resolved path string to truncate from
* @return Path object with the remaining path or null if invalid
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,createLink,"org.apache.hadoop.fs.viewfs.InodeTree:createLink(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$LinkType,java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration)",423,505,"/**
* Creates a symbolic link to the target at the specified source path.
* @param src absolute path of the link
* @param target URI or file system path of the target
* @param linkType type of link (SINGLE, SINGLE_FALLBACK, MERGE, NFLY)
* @param settings configuration for the link
* @param aUgi user information for the operation
* @param config configuration for the file system
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRemainingPath,"org.apache.hadoop.fs.viewfs.InodeTree:getRemainingPath(java.lang.String[],int)",996,1008,"/**
* Creates a Path object from the given array starting at the specified index.
* @param path array of path segments
* @param startIndex starting index in the array (inclusive)
* @return Path object or SlashPath if start index exceeds array length
*/","* Return remaining path from specified index to the end of the path array.
   * @param path An array of path components split by slash
   * @param startIndex the specified start index of the path array
   * @return remaining path.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getTargetLink,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:getTargetLink(),370,377,"/**
* Concatenates and returns a single path string from the list of merged URIs.
* @return merged URI as a Path object
*/","* Get the target of the link. If a merge link then it returned
     * as "","" separated URI list.
     *
     * @return the path.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)",119,121,"/**
* Creates a composite path from two component paths.
* @param parent the parent directory in the path
* @param child the child directory in the path
*/","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)",129,131,"/**
* Creates a nested path with the given child path.
* @param parent the parent path of this nested path
* @param child the child path to be nested under parent
*/","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(java.lang.String,org.apache.hadoop.fs.Path)",139,141,"/**
* Creates a new Path instance by copying from another Path.
* @param parent parent directory path
* @param child child directory path to copy attributes from
*/","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,rename,"org.apache.hadoop.io.MapFile:rename(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.String)",898,905,"/**
* Renames a directory in the specified file system.
* @param fs target file system
* @param oldName current directory name
* @param newName new directory name
* @throws IOException if renaming fails
*/","* Renames an existing map directory.
   * @param fs fs.
   * @param oldName oldName.
   * @param newName newName.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,insecureCreateForWrite,"org.apache.hadoop.io.SecureIOUtils:insecureCreateForWrite(java.io.File,int)",243,262,"/**
* Creates FileOutputStream for writing with specified permissions.
* @param f file to write to
* @param permissions OS-specific file permissions
* @return FileOutputStream instance or null on failure
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,fileToPath,org.apache.hadoop.security.token.DtFileOperations:fileToPath(java.io.File),93,95,"/**
* Converts a File to a Hadoop Path.
* @param f input File object
*/",Add the service prefix for a local filesystem.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,unnestUri,org.apache.hadoop.security.ProviderUtils:unnestUri(java.net.URI),80,101,"/**
* Unnests a URI into its constituent parts.
* @param nestedUri URI to process
* @return Path object representing the unnested URI or null if invalid
*/","* Convert a nested URI to decode the underlying path. The translation takes
   * the authority and parses it into the underlying scheme and authority.
   * For example, ""myscheme://hdfs@nn/my/path"" is converted to
   * ""hdfs://nn/my/path"".
   * @param nestedUri the URI from the nested URI
   * @return the unnested path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,constructNewPath,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:constructNewPath(org.apache.hadoop.fs.Path),300,302,"/**
* Constructs a new unique file path by appending '_NEW' to the input path. 
* @param path existing file path
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,constructOldPath,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:constructOldPath(org.apache.hadoop.fs.Path),304,306,"/**
* Constructs an old version of a given file path.
* @param path original file path
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,stringToPath,org.apache.hadoop.util.StringUtils:stringToPath(java.lang.String[]),273,282,"/**
* Converts an array of string path components to an array of Path objects.
* @param str array of path strings
* @return array of Path objects or null if input is null
*/","* stringToPath.
   * @param str str.
   * @return path array.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,makeQualified,org.apache.hadoop.fs.HarFileSystem:makeQualified(org.apache.hadoop.fs.Path),400,412,"/**
* Converts an unqualified file system path to a qualified HAR (HTTP Archive) path.
* @param path the original file system path
* @return a new qualified HAR path object
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getPathWithoutSchemeAndAuthority,org.apache.hadoop.fs.Path:getPathWithoutSchemeAndAuthority(org.apache.hadoop.fs.Path),104,111,"/**
* Removes scheme and authority from a given file path.
* @param path input file path
* @return simplified file path without scheme and authority
*/","* Return a version of the given Path without the scheme information.
   *
   * @param path the source Path
   * @return a copy of this Path without the scheme information",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,mergePaths,"org.apache.hadoop.fs.Path:mergePaths(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",277,286,"/**
* Merges two Paths into a single Path, preserving scheme and authority.
* @param path1 initial Path
* @param path2 subsequent Path to append
* @return merged Path object
*/","* Merge 2 paths such that the second path is appended relative to the first.
   * The returned path has the scheme and authority of the first path.  On
   * Windows, the drive specification in the second path is discarded.
   * 
   * @param path1 the first path
   * @param path2 the second path, to be appended relative to path1
   * @return the merged path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getParentUtil,org.apache.hadoop.fs.Path:getParentUtil(),444,459,"/**
* Calculates the parent directory of a URI.
* @return Parent Path object or null for root or empty path
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,apply,"org.apache.hadoop.fs.shell.find.Print:apply(org.apache.hadoop.fs.shell.PathData,int)",59,63,"/**
* Applies transformation to PathData object and prints result.
* @param item PathData object to process
* @param depth processing depth (not used in this implementation)
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processPath,org.apache.hadoop.fs.shell.Display$Checksum:processPath(org.apache.hadoop.fs.shell.PathData),197,214,"/**
* Processes a path data item, displaying its checksum and block size if applicable.
* @param item PathData object containing the path to process
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,checkIfExists,org.apache.hadoop.fs.shell.PathData:checkIfExists(org.apache.hadoop.fs.shell.PathData$FileTypeRequirement),220,233,"/**
* Validates file type requirements against actual path state.
* @param typeRequirement expected file type requirement
*/","* Ensure that the file exists and if it is or is not a directory
   * @param typeRequirement Set it to the desired requirement.
   * @throws PathIOException if file doesn't exist or the type does not match
   * what was specified in typeRequirement.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getStringForChildPath,org.apache.hadoop.fs.shell.PathData:getStringForChildPath(org.apache.hadoop.fs.Path),319,328,"/**
* Constructs a string representation for a child directory path.
* @param childPath relative path to the child directory
*/","* Given a child of this directory, use the directory's path and the child's
   * basename to construct the string to the child.  This preserves relative
   * paths since Path will fully qualify.
   * @param childPath a path contained within this directory
   * @return String of the path relative to this directory",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processPath,org.apache.hadoop.fs.shell.Ls:processPath(org.apache.hadoop.fs.shell.PathData),285,321,"/**
* Formats and prints file status information.
* @param item PathData object containing file metadata
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,rename,"org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:rename(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",546,563,"/**
* Renames a file or directory, deleting the target if it exists.
* @param src source PathData object
* @param target target PathData object
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processPath,org.apache.hadoop.fs.shell.Delete$Rmdir:processPath(org.apache.hadoop.fs.shell.PathData),205,217,"/**
* Deletes a non-directory path if it's empty or deletes the directory itself.
* @param item PathData object containing file/directory information
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,processPath,org.apache.hadoop.fs.shell.Mkdir:processPath(org.apache.hadoop.fs.shell.PathData),58,67,"/**
* Validates directory existence based on createParents flag.
* @param item PathData object to validate
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processPath,org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processPath(org.apache.hadoop.fs.shell.PathData),143,148,"/**
* Verifies that a given path is a directory.
* @param item PathData object containing the path to verify
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processNonexistentPath,org.apache.hadoop.fs.shell.Command:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),330,332,"/**
* Throws exception when encountering nonexistent file path.
* @param item PathData object containing nonexistent path information
*/","*  Provides a hook for handling paths that don't exist.  By default it
   *  will throw an exception.  Primarily overriden by commands that create
   *  paths such as mkdir or touch.
   *  @param item the {@link PathData} that doesn't exist
   *  @throws FileNotFoundException if arg is a path and it doesn't exist
   *  @throws IOException if anything else goes wrong...",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processPath,org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processPath(org.apache.hadoop.fs.shell.PathData),57,62,"/**
* Validates directory existence in path data.
* @param item PathData object containing file or directory information
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processPath,"org.apache.hadoop.fs.shell.MoveCommands$Rename:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",111,128,"/**
* Renames a file in the source filesystem to the target location.
* @param src source file path
* @param target target file path
* @throws IOException on rename failure or invalid filesystem
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processPath,org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processPath(org.apache.hadoop.fs.shell.PathData),102,107,"/**
* Validates that the provided path is a directory.
* @param item PathData object containing file/directory information
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,processPath,org.apache.hadoop.fs.shell.Truncate:processPath(org.apache.hadoop.fs.shell.PathData),75,97,"/**
* Truncates a file to the specified length.
* @param item PathData object containing file information
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,processPath,org.apache.hadoop.fs.shell.SetReplication:processPath(org.apache.hadoop.fs.shell.PathData),81,103,"/**
* Processes file path data, setting replication if necessary and adding to wait list.
* @param item PathData object containing file information
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processPath,"org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",61,68,"/**
* Processes a file path for moving operation.
* @param src source path data
* @param target target path data
* @throws IOException if an I/O error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,postProcessPath,org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:postProcessPath(org.apache.hadoop.fs.shell.PathData),70,78,"/**
* Deletes file system path.
* @param src PathData object containing path and fs context
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFSofPath,org.apache.hadoop.fs.FileContext:getFSofPath(org.apache.hadoop.fs.Path),325,337,"/**
* Retrieves the AbstractFileSystem associated with a given file path.
* @param absOrFqPath absolute or fully qualified file path to resolve
*/","* Get the file system of supplied path.
   * 
   * @param absOrFqPath - absolute or fully qualified path
   * @return the file system of the path
   * 
   * @throws UnsupportedFileSystemException If the file system for
   *           <code>absOrFqPath</code> is not supported.
   * @throws IOException If the file system for <code>absOrFqPath</code> could
   *         not be instantiated.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,<init>,"org.apache.hadoop.fs.viewfs.ChRootedFs:<init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)",102,122,"/**
* Creates a ChRootedFs instance for the given file system and root path.
* @param fs AbstractFileSystem instance
* @param theRoot Path to the chrooted directory
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getUriPath,org.apache.hadoop.fs.FilterFs:getUriPath(org.apache.hadoop.fs.Path),189,192,"/**
* Retrieves URI path from the file system.
* @param p file system object containing path information
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,resolvePath,org.apache.hadoop.fs.viewfs.ViewFs:resolvePath(org.apache.hadoop.fs.Path),328,338,"/**
* Resolves a file system path to its absolute form.
* @param f the input Path to resolve
* @return the resolved Path or original Path if already internal directory
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,resolvePath,org.apache.hadoop.fs.FilterFs:resolvePath(org.apache.hadoop.fs.Path),168,172,"/**
* Resolves a file path to an absolute path.
* @param p input file path
* @return resolved Path object or null if unresolved
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createInternal,"org.apache.hadoop.fs.FilterFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",88,97,"/**
* Creates an internal FSDataOutputStream using the underlying file system.
* @param f Path to the file
* @throws IOException if I/O error occurs
* @throws UnresolvedLinkException if path is a symbolic link
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,delete,"org.apache.hadoop.fs.FilterFs:delete(org.apache.hadoop.fs.Path,boolean)",99,104,"/**
* Deletes a file or directory.
* @param f Path to delete
* @param recursive whether to delete recursively
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileBlockLocations,"org.apache.hadoop.fs.FilterFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",106,111,"/**
* Retrieves file block locations for the given file.
* @param f Path to the file
* @param start starting offset
* @param len length of the range
* @return array of BlockLocation objects or empty array if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileChecksum,org.apache.hadoop.fs.FilterFs:getFileChecksum(org.apache.hadoop.fs.Path),113,118,"/**
* Retrieves file checksum using external file system.
* @param f Path to file for which checksum is requested
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileStatus,org.apache.hadoop.fs.FilterFs:getFileStatus(org.apache.hadoop.fs.Path),120,125,"/**
* Retrieves file status by path.
* @param f Path to file
* @return FileStatus object or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileLinkStatus,org.apache.hadoop.fs.FilterFs:getFileLinkStatus(org.apache.hadoop.fs.Path),139,144,"/**
* Retrieves file link status for the specified path.
* @param f the path to query (must be checked by checkPath())
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listStatus,org.apache.hadoop.fs.FilterFs:listStatus(org.apache.hadoop.fs.Path),194,199,"/**
* Lists file statuses for a given path.
* @param f Path to list file statuses from
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listLocatedStatus,org.apache.hadoop.fs.FilterFs:listLocatedStatus(org.apache.hadoop.fs.Path),201,207,"/**
* Lists located status for a given file system path.
* @param f the file system path to list
* @return iterator over LocatedFileStatus objects or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,mkdir,"org.apache.hadoop.fs.FilterFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",215,221,"/**
* Creates a new directory with specified permissions and behavior.
* @param dir the path to the directory
* @param permission file system permissions for the directory
* @param createParent whether to create parent directories if they don't exist
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,open,org.apache.hadoop.fs.FilterFs:open(org.apache.hadoop.fs.Path),223,228,"/**
* Opens and returns an input stream to the specified file.
* @param f the path to the file
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,open,"org.apache.hadoop.fs.FilterFs:open(org.apache.hadoop.fs.Path,int)",230,235,"/**
* Opens a file stream with specified buffer size.
* @param f the path to the file
* @param bufferSize the buffer size for reading
* @return FSDataInputStream object or throws IOException/UnresolvedLinkException",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,truncate,"org.apache.hadoop.fs.FilterFs:truncate(org.apache.hadoop.fs.Path,long)",237,243,"/**
* Truncates a file to the specified length.
* @param f Path to the file
* @param newLength desired file size in bytes
* @return true if truncation succeeded, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setOwner,"org.apache.hadoop.fs.FilterFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",261,267,"/**
* Sets file owner to specified user and group.
* @param f Path object of the file
* @param username new owner's username
* @param groupname new owner's group name
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setPermission,"org.apache.hadoop.fs.FilterFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",269,274,"/**
* Sets file system permissions on a given path.
* @param f Path to update permissions for
* @param permission New fs permissions to apply
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setReplication,"org.apache.hadoop.fs.FilterFs:setReplication(org.apache.hadoop.fs.Path,short)",276,281,"/**
* Sets file replication level.
* @param f Path to the file
* @param replication desired replication value (short)
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setTimes,"org.apache.hadoop.fs.FilterFs:setTimes(org.apache.hadoop.fs.Path,long,long)",283,288,"/**
* Sets file times (mtime and atime) for the given file.
* @param f Path to the file
* @param mtime last modification time in milliseconds
* @param atime last access time in milliseconds
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,mkdirs,"org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",764,771,"/**
* Creates a directory with specified permissions.
* @param fs FileSystem object
* @param dir Path to create directory at
* @param permission FsPermission to apply to new directory
* @return true if directory creation was successful
*/","* Create a directory with the provided permission.
   * The permission of the directory is set to be the provided permission as in
   * setPermission, not permission{@literal &~}umask
   *
   * @see #create(FileSystem, Path, FsPermission)
   *
   * @param fs FileSystem handle
   * @param dir the name of the directory to be created
   * @param permission the permission of the directory
   * @return true if the directory creation succeeds; false otherwise
   * @throws IOException A problem creating the directories.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,mkdirs,org.apache.hadoop.fs.ChecksumFileSystem:mkdirs(org.apache.hadoop.fs.Path),986,989,"/**
* Creates directory at specified path.
* @param f Path to create directory at
* @return true if successful, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,mkdirs,org.apache.hadoop.fs.FilterFileSystem:mkdirs(org.apache.hadoop.fs.Path),339,342,"/**
* Creates directories recursively.
* @param f Path to directory
* @return true if successful, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FSProtos$FileStatusProto),49,106,"/**
* Converts a FileStatusProto object to a FileStatus.
* @param proto the input FileStatusProto
* @return FileStatus representation of the input
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean)",151,158,"/**
* Initializes a FileStatus object with basic file metadata.
* @param length total file size
* @param isdir true if directory, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set,org.apache.hadoop.fs.BlockLocation[])",142,149,"/**
* Initializes LocatedFileStatus with file attributes and block locations.
* @param locations array of BlockLocation objects
*/","* Constructor.
   *
   * @param length a file's length
   * @param isdir if the path is a directory
   * @param block_replication the file's replication factor
   * @param blocksize a file's block size
   * @param modification_time a file's modification time
   * @param access_time a file's access time
   * @param permission a file's permission
   * @param owner a file's owner
   * @param group a file's group
   * @param symlink symlink if the path is a symbolic link
   * @param path the path's qualified name
   * @param attr Attribute flags (See {@link FileStatus.AttrFlags}).
   * @param locations a file's block locations",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getPermission,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getPermission(),59,62,"/**
* Returns file system permission based on superclass implementation.
* @return FsPermission object representing current file system permission.",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$Writer:append(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",1458,1461,"/**
* Appends a writable key-value pair to the underlying storage.
* @param key writable key object
* @param val writable value object
*/","* Append a key/value pair.
     * @param key input Writable key.
     * @param val input Writable val.
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeFile,"org.apache.hadoop.io.SequenceFile$Sorter:writeFile(org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator,org.apache.hadoop.io.SequenceFile$Writer)",3431,3438,"/**
* Writes raw key-value pairs to a file.
* @param records iterator of key-value pairs
* @param writer output writer for the file
*/","* Writes records from RawKeyValueIterator into a file represented by the 
     * passed writer.
     * @param records the RawKeyValueIterator
     * @param writer the Writer created earlier 
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,init,org.apache.hadoop.service.AbstractService:init(org.apache.hadoop.conf.Configuration),152,178,"/**
* Initializes service instance with provided configuration.
* @param conf service configuration
*/","* {@inheritDoc}
   * This invokes {@link #serviceInit}
   * @param conf the configuration of the service. This must not be null
   * @throws ServiceStateException if the configuration was null,
   * the state change not permitted, or something else went wrong",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,stop,org.apache.hadoop.service.AbstractService:stop(),213,240,"/**
* Stops the service, transitioning from running state.
* @throws ServiceStateException if stopping fails
*/",* {@inheritDoc},,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Metadata:equals(org.apache.hadoop.io.SequenceFile$Metadata),797,820,"/**
* Compares this metadata object with another by key-value pairs.
* @param other Metadata object to compare with
* @return true if equal, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,handleKind,org.apache.hadoop.security.token.Token$TrivialRenewer:handleKind(org.apache.hadoop.io.Text),528,531,"/**
* Verifies whether the provided Text kind matches the current kind.
* @param kind the text kind to verify
* @return true if matching, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSelector.java,selectToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)",46,60,"/**
* Selects a Token with matching kind and service from the given collection.
* @param service Text service to match
* @param tokens Collection of Tokens to search
* @return Matching Token or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,isPrivateCloneOf,org.apache.hadoop.security.token.Token$PrivateToken:isPrivateCloneOf(org.apache.hadoop.io.Text),279,282,"/**
* Checks if the given text service is a private clone of this service.
* @param thePublicService Text service to compare with
* @return true if identical, false otherwise
*/","* Whether this is a private clone of a public token.
     * @param thePublicService the public service name
     * @return true when the public service is the same as specified",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,equals,org.apache.hadoop.security.token.Token:equals(java.lang.Object),386,400,"/**
* Compares this token instance with another for equality.
* @param right the object to compare with
* @return true if equal, false otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,matchAlias,"org.apache.hadoop.security.token.DtFileOperations:matchAlias(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",73,75,"/**
* Checks if token service matches given alias.
* @param token Token object
* @param alias Alias text to compare with
*/",Match token service field to alias text.  True if alias is null.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,matchService,"org.apache.hadoop.security.token.DtFileOperations:matchService(org.apache.hadoop.security.token.DtFetcher,org.apache.hadoop.io.Text,java.lang.String)",78,83,"/**
* Verifies if a service matches the given URL or name.
* @param fetcher DtFetcher instance
* @param service Text representation of the service (null for URL-based match)
* @param url Service URL to compare with
* @return true if the service matches, false otherwise
*/",Match fetcher's service name to the service text and/or url prefix.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,selectDelegationToken,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text)",1008,1018,"/**
* Selects a delegationToken for the given service and credentials.
* @param creds Hadoop Credentials object
* @param service Text representation of the service name
* @return Token object or null if selection fails
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,handleKind,org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:handleKind(org.apache.hadoop.io.Text),179,182,"/**
* Verifies whether the given text matches the expected token kind. 
* @param kind input text to be compared with TOKEN_KIND 
* @return true if match, false otherwise 
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDir,org.apache.hadoop.util.DiskChecker:checkDir(java.io.File),76,78,"/**
* Verifies the existence and accessibility of a directory.
* @param dir the directory to be checked
*/","* Create the directory if it doesn't exist and check that dir is readable,
   * writable and executable
   *  
   * @param dir dir.
   * @throws DiskErrorException disk problem.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirWithDiskIo,org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(java.io.File),88,92,"/**
* Verifies directory exists and performs disk I/O operation.
* @param dir File object representing the directory to verify
*/","* Create the directory if it doesn't exist and check that dir is
   * readable, writable and executable. Perform some disk IO to
   * ensure that the disk is usable for writes.
   *
   * @param dir dir.
   * @throws DiskErrorException disk problem.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,flushBuffer,org.apache.hadoop.fs.FSOutputSummer:flushBuffer(),145,147,"/**
* Flushes the buffer to disk.
* @throws IOException on write error
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,flush,org.apache.hadoop.fs.FSOutputSummer:flush(),183,185,"/**
* Flushes output buffer to underlying device.
*/","* Checksums all complete data chunks and flushes them to the underlying
   * stream. If there is a trailing partial chunk, it is not flushed and is
   * maintained in the buffer.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,doDecodeSingle,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeSingle(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int,int,boolean)",123,222,"/**
* Decodes data by handling erased locations.
* @param inputs input ByteBuffer arrays
* @param outputs output ByteBuffer array
* @param erasedLocationToFix location to fix in the first subPacket
* @param bufSize size of the buffer
* @param isDirect whether the input and output buffers are direct or not
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,doDecodeMultiAndParity,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeMultiAndParity(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int[],int)",265,351,"/**
* Decodes multi-parity and fixes erased locations in Reed-Solomon encoded data.
* @param inputs first sub-stripe and parity input buffers
* @param outputs first sub-stripe output buffer and second sub-stripe output buffer
* @param erasedLocationToFix array of indices to fix in the output buffers
* @param bufSize size of each input buffer
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DecodingValidator.java,validate,"org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",73,128,"/**
* Validates input buffers by decoding and comparing with expected output.
* @param inputs array of ByteBuffer inputs
* @param erasedIndexes array of indexes to erase from inputs
* @param outputs array of ByteBuffer outputs
*/","* Validate outputs decoded from inputs, by decoding an input back from
   * the outputs and comparing it with the original one.
   *
   * For instance, in RS (6, 3), let (d0, d1, d2, d3, d4, d5) be sources
   * and (p0, p1, p2) be parities, and assume
   *  inputs = [d0, null (d1), d2, d3, d4, d5, null (p0), p1, null (p2)];
   *  erasedIndexes = [1, 6];
   *  outputs = [d1, p0].
   * Then
   *  1. Create new inputs, erasedIndexes and outputs for validation so that
   *     the inputs could contain the decoded outputs, and decode them:
   *      newInputs = [d1, d2, d3, d4, d5, p0]
   *      newErasedIndexes = [0]
   *      newOutputs = [d0']
   *  2. Compare d0 and d0'. The comparison will fail with high probability
   *     when the initial outputs are wrong.
   *
   * Note that the input buffers' positions must be the ones where data are
   * read: If the input buffers have been processed by a decoder, the buffers'
   * positions must be reset before being passed into this method.
   *
   * This method does not change outputs and erasedIndexes.
   *
   * @param inputs input buffers used for decoding. The buffers' position
   *               are moved to the end after this method.
   * @param erasedIndexes indexes of erased units used for decoding
   * @param outputs decoded output buffers, which are ready to be read after
   *                the call
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])",168,173,"/**
* Decodes EC-encoded chunks from inputs to outputs.
* @param inputs encoded input chunks
* @param erasedIndexes indexes of erased input chunks
* @param outputs output chunk buffer
*/","* Decode with inputs and erasedIndexes, generates outputs. More see above.
   *
   * Note, for both input and output ECChunks, no mixing of on-heap buffers and
   * direct buffers are allowed.
   *
   * @param inputs input buffers to read data from
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to put decoded data into according to
   *                erasedIndexes, ready for read after the call
   * @throws IOException if the decoder is closed",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",55,68,"/**
* Decodes input data while adjusting output order and handling erased indexes.
* @param inputs     input ByteBuffer array
* @param erasedIndexes array of indexes to be ignored during decoding
* @param outputs    output ByteBuffer array
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(byte[][],int[],byte[][])",70,83,"/**
* Decodes input data while adjusting its order and erasures.
* @param inputs byte arrays to decode
* @param erasedIndexes indexes of erased bytes in inputs
* @param outputs output byte arrays after decoding
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState),73,84,"/**
* Performs data decoding and encoding using GF tables.
* @param decodingState state containing input/output buffers and indices
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),86,101,"/**
* Performs decoding using provided state.
* @param decodingState state containing input/output buffers and offsets
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,initBlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:initBlock(),511,570,"/**
* Initialises block state, allocating data if necessary.
* @throws IOException on invalid block header or end of file
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,internalReset,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:internalReset(),274,280,"/**
* Resets internal state and reinitializes output stream.
* @throws IOException on write error
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,writeRun,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:writeRun(),654,706,"/**
* Writes a run of consecutive characters to the output stream.
* @throws IOException if I/O error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,close,org.apache.hadoop.io.MapFile$Merger:close(),1148,1157,"/**
* Closes all input readers and the output writer, releasing resources.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,close,org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:close(),256,271,"/**
* Closes the resource, performing cleanup and finalizing state.
* @throws IOException if an I/O error occurs during closing
*/","* Signaling the end of write to the block. The block register will be
       * called for registering the finished block.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,cleanup,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:cleanup(),3887,3892,"/**
* Cleans up resources after use.
* Closes underlying connections and deletes temporary files when not preserved. 
* @throws IOException if file system operation fails
*/","* The default cleanup. Subclasses can override this with a custom
       * cleanup.
       * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,close,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:close(),557,569,"/**
* Closes the resource, ensuring it's no longer usable.
* @throws IOException if an I/O error occurs during closure
*/",* Finishing reading the block. Release all resources.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeWritableOutputStream,org.apache.hadoop.security.Credentials:writeWritableOutputStream(java.io.DataOutputStream),321,326,"/**
* Writes header to writable output stream.
* @param os DataOutputStream instance
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readFields,org.apache.hadoop.security.Credentials:readFields(java.io.DataInput),420,443,"/**
* Reads serialized data from input stream and populates token and secret key maps.
* @throws IOException if serialization fails
*/","* Loads all the keys.
   * @param in DataInput.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,createTokenIdent,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:createTokenIdent(byte[]),253,260,"/**
* Creates a TokenIdent object from the provided byte array.
* @param tokenIdentBytes input data as a byte array
* @return TokenIdent object or throws IOException if read fails
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,processTokenAddOrUpdate,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processTokenAddOrUpdate(byte[]),411,427,"/**
* Processes an add or update token request.
* @param data byte array containing token information
* @return TokenIdent object if successful, null otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,processTokenRemoved,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processTokenRemoved(org.apache.curator.framework.recipes.cache.ChildData),429,435,"/**
* Removes token from internal state upon user removal.
* @param data ChildData object containing removed token's binary data
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfoFromZK,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(java.lang.String,boolean)",652,679,"/**
* Retrieves DelegationTokenInformation from ZooKeeper node.
* @param nodePath path to the token info node
* @param quiet whether to suppress error messages if node is missing
* @return DelegationTokenInformation object or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawKey,org.apache.hadoop.io.SequenceFile$Reader:nextRawKey(org.apache.hadoop.io.DataOutputBuffer),2662,2697,"/**
* Retrieves the next raw key from input stream.
* @param key output buffer to store the key
* @return key length or -1 on error
*/","* Read 'raw' keys.
     * @param key - The buffer into which the key is read
     * @return Returns the key length or -1 for end of file
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getCurrentValue,org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(org.apache.hadoop.io.Writable),2376,2408,"/**
* Reads current value from stream and populates Writable object.
* @param val Writable object to populate with current value
*/","* Get the 'value' corresponding to the last read 'key'.
     * @param val : The 'value' to be read.
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getCurrentValue,org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(java.lang.Object),2415,2448,"/**
* Retrieves the current configurable object value.
* @param val initializable object to set configuration on
* @return initialized Configurable object or null if not found
*/","* @return Get the 'value' corresponding to the last read 'key'.
     * @param val : The 'value' to be read.
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRaw,"org.apache.hadoop.io.SequenceFile$Reader:nextRaw(org.apache.hadoop.io.DataOutputBuffer,org.apache.hadoop.io.SequenceFile$ValueBytes)",2603,2654,"/**
* Fetches next compressed or uncompressed record from storage.
* @param key DataOutputBuffer for storing key data
* @param val ValueBytes object to store value data
* @return raw length of the record, -1 if not found or EOF reached
*/","* Read 'raw' records.
     * @param key - The buffer into which the key is read
     * @param val - The 'raw' value
     * @return Returns the total record length or -1 for end of file
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawValue,org.apache.hadoop.io.SequenceFile$Reader:nextRawValue(org.apache.hadoop.io.SequenceFile$ValueBytes),2765,2790,"/**
* Reads and decompresses the next value from the compressed stream.
* @param val ValueBytes object containing compressed or uncompressed data
* @return length of the decompressed value
*/","* Read 'raw' values.
     * @param val - The 'raw' value
     * @return Returns the value length
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,getTokenInfoFromSQL,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getTokenInfoFromSQL(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),238,251,"/**
* Retrieves DelegationTokenInformation from SQL secret manager.
* @param ident TokenIdent object containing sequence number and bytes
* @throws NoSuchElementException if token not found in SQL secret manager
*/","* Obtains the DelegationTokenInformation associated with the given
   * TokenIdentifier in the SQL database.
   * @param ident Existing TokenIdentifier in the SQL database.
   * @return DelegationTokenInformation that matches the given TokenIdentifier or
   *         null if it doesn't exist in the database.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,read,org.apache.hadoop.fs.permission.PermissionStatus:read(java.io.DataInput),114,118,"/**
* Reads and returns a PermissionStatus object from the given DataInput stream.
* @param in DataInput stream containing serialized PermissionStatus data
*/","* Create and initialize a {@link PermissionStatus} from {@link DataInput}.
   * @param in data input.
   * @throws IOException raised on errors performing I/O.
   * @return PermissionStatus.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readEnum,"org.apache.hadoop.io.WritableUtils:readEnum(java.io.DataInput,java.lang.Class)",422,425,"/**
* Reads an enumeration value from the input stream. 
* @param in DataInput stream containing the enumeration name
* @param enumType Enumerated class to read from
* @return Enum instance or null if not found
*/","* Read an Enum value from DataInput, Enums are read and written 
   * using String values. 
   * @param <T> Enum type
   * @param in DataInput to read from 
   * @param enumType Class type of Enum
   * @return Enum represented by String read from DataInput
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,readFields,org.apache.hadoop.security.authorize.AccessControlList:readFields(java.io.DataInput),326,330,"/**
* Reads ACL data from input stream and builds ACL object.
*/",* Deserializes the AccessControlList object,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getDelegationKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getDelegationKey(int),561,577,"/**
* Retrieves DelegationKey by ID from cache or ZooKeeper.
* @param keyId unique identifier for the key
* @return DelegationKey object if found, null otherwise
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:write(java.io.DataOutput),790,796,"/**
* Writes this MetaIndex to the given DataOutput.
* @param out output stream
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getFixedByteString,org.apache.hadoop.ipc.ProtobufHelper:getFixedByteString(org.apache.hadoop.io.Text),85,87,"/**
* Retrieves fixed byte string representation from Text object.
* @param key input Text value
*/","* Get the ByteString for frequently used fixed and small set strings.
   * @param key string
   * @return the ByteString for frequently used fixed and small set strings.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,protoFromToken,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:protoFromToken(org.apache.hadoop.security.token.Token),142,149,"/**
* Converts a Token object to a TokenProto message.
* @param tok the input token object
*/","* Create a {@code TokenProto} instance
   * from a hadoop token.
   * This builds and caches the fields
   * (identifier, password, kind, service) but not
   * renewer or any payload.
   * @param tok token
   * @return a marshallable protobuf class.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,copyToken,org.apache.hadoop.security.token.Token:copyToken(),114,116,"/**
* Creates a deep copy of this token instance.
* @return A new token with the same type T and identical properties
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",160,164,"/**
* Creates a token using the provided UGI and renewer.
* @param ugi UserGroupInformation object
* @param renewer string identifier of the renewer
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenIdentifier.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",49,53,"/**
* Constructs a delegationToken identifier with specified properties.
* @param kind token kind
* @param owner token owner
* @param renewer token renewer
* @param realUser underlying user identity
*/","* Create a new delegation token identifier
   *
   * @param kind token kind
   * @param owner the effective username of the token owner
   * @param renewer the username of the renewer
   * @param realUser the real username of the token owner",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,<init>,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:<init>(),51,53,"/**
* Initializes an empty AbstractDelegationTokenIdentifier.
* @param masterKeyId null (not applicable)
* @param renewUntil null (not applicable)
* @param sequenceNumber null (not applicable)
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadWriteLock.java,<init>,"org.apache.hadoop.util.InstrumentedReadWriteLock:<init>(boolean,java.lang.String,org.slf4j.Logger,long,long)",40,47,"/**
* Initializes an instrumented read-write lock with specified settings.
* @param fair true for starvation-free behavior
* @param name lock identifier for logging purposes
* @param logger instance of Logger for logging events
* @param minLoggingGapMs minimum gap between log messages in milliseconds
* @param lockWarningThresholdMs threshold value for lock warnings in milliseconds
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invokeOnce,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce(),89,117,"/**
* Invokes the method once, handling retries and exceptions.
* @return CallReturn object with result or exception
*/",Invoke the call once without retrying.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,checkKey,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:checkKey(),1596,1611,"/**
* Reads and parses key-value data from input stream.
* @throws IOException on read errors
*/","* check whether we have already successfully obtained the key. It also
       * initializes the valueInputStream.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getValue,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(org.apache.hadoop.io.BytesWritable),1706,1720,"/**
* Reads data into BytesWritable object.
* @param value BytesWritable object to fill
* @return length of filled BytesWritable object
*/","* Copy the value into BytesWritable. The input BytesWritable will be
         * automatically resized to the actual value size. The implementation
         * directly uses the buffer inside BytesWritable for storing the value.
         * The call does not require the value length to be known.
         * 
         * @param value value.
         * @throws IOException raised on errors performing I/O.
         * @return long value.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,writeValue,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:writeValue(java.io.OutputStream),1747,1763,"/**
* Writes serialized value to output stream.
* @param out target output stream
* @return total written value size in bytes
*/","* Writing the value to the output stream. This method avoids copying
         * value data from Scanner into user buffer, then writing to the output
         * stream. It does not require the value length to be known.
         * 
         * @param out
         *          The output stream
         * @return the length of the value
         * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,read,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[]),146,149,"/**
 * Reads data into the provided byte array. 
 * @param b the buffer to store the read data
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,close,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:close(),186,197,"/**
* Closes the resource, ensuring all data is read and the underlying stream is released.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[]),1932,1934,"/**
* Compares this byte array with another.
* @param buf the byte array to compare with
*/","* Compare the entry key to another key. Synonymous to compareTo(key, 0,
         * key.length).
         * 
         * @param buf
         *          The key buffer.
         * @return comparison result between the entry key with the input key.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,equals,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:equals(java.lang.Object),1966,1971,"/**
* Compares this entry with another object for equality.
* @param other the object to compare with
* @return true if the objects are equal, false otherwise
*/",* Compare whether this and other points to the same key value.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String,java.lang.String)",188,203,"/**
* Retrieves a delegation token from the specified URL.
* @param url URL of the token service
* @param token existing token for authentication
* @param renewer user or service that issued the token
* @param doAsUser user on whose behalf the operation is performed
* @return Token<AbstractDelegationTokenIdentifier> object, or null if failed
*/","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return abstract delegation token identifier.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)",237,245,"/**
* RENEWS a delegated token.
* @param url URL to perform operation on
* @param token Authenticated token for the operation
* @param dToken Delegated token to renew
* @param doAsUser User ID to impersonate
* @return new token expiration time in milliseconds or 0 if not renewed
*/","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @param doAsUser the user to do as, which will be the token owner.
   * @param dToken abstract delegation token identifier.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)",275,286,"/**
* Cancels a delegated token.
* @param url URL to perform operation on
* @param token authenticated token for the operation
* @param dToken delegationToken to cancel
* @param doAsUser user performing the cancellation
*/","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @param dToken abstract delegation token identifier.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String),468,470,"/**
* Selects a random node within the specified scope.
* @param scope unique identifier for the scope
*/","* Randomly choose a node.
   *
   * @param scope range of nodes from which a node will be chosen
   * @return the chosen node
   *
   * @see #chooseRandom(String, Collection)",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,sortByDistance,"org.apache.hadoop.net.NetworkTopologyWithNodeGroup:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",285,300,"/**
* Sorts nodes by distance from a given reader node.
* @param reader Node to sort from
* @param nodes Array of nodes to sort
* @param activeLen Length of active section (ignored)
*/","* Sort nodes array by their distances to <i>reader</i>.
   * <p>
   * This is the same as {@link NetworkTopology#sortByDistance(Node, Node[],
   * int)} except with a four-level network topology which contains the
   * additional network distance of a ""node group"" which is between local and
   * same rack.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getInputStream,"org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket,long)",496,503,"/**
* Creates a SocketInputWrapper instance with specified timeout.
* @param socket the underlying socket
* @param timeout milliseconds before timeout occurs
* @return SocketInputWrapper instance or throws IOException if error occurs
*/","* Return a {@link SocketInputWrapper} for the socket and set the given
   * timeout. If the socket does not have an associated channel, then its socket
   * timeout will be set to the specified value. Otherwise, a
   * {@link SocketInputStream} will be created which reads with the configured
   * timeout.
   * 
   * Any socket created using socket factories returned by {@link #NetUtils},
   * must use this interface instead of {@link Socket#getInputStream()}.
   * 
   * In general, this should be called only once on each socket: see the note
   * in {@link SocketInputWrapper#setTimeout(long)} for more information.
   *
   * @see Socket#getChannel()
   * 
   * @param socket socket.
   * @param timeout timeout in milliseconds. zero for waiting as
   *                long as necessary.
   * @return SocketInputWrapper for reading from the socket.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getOutputStream,"org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket,long)",550,554,"/**
* Returns an output stream to write data to a given socket.
* @param socket the socket to get the output stream for
* @param timeout timeout value for the socket (milliseconds)
* @return OutputStream object or null if not applicable
*/","* Returns OutputStream for the socket. If the socket has an associated
   * SocketChannel then it returns a 
   * {@link SocketOutputStream} with the given timeout. If the socket does not
   * have a channel, {@link Socket#getOutputStream()} is returned. In the later
   * case, the timeout argument is ignored and the write will wait until 
   * data is available.<br><br>
   * 
   * Any socket created using socket factories returned by {@link NetUtils},
   * must use this interface instead of {@link Socket#getOutputStream()}.
   * 
   * @see Socket#getChannel()
   * 
   * @param socket socket.
   * @param timeout timeout in milliseconds. This may not always apply. zero
   *        for waiting as long as necessary.
   * @return OutputStream for writing to the socket.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,writeMetric,org.apache.hadoop.metrics2.sink.StatsDSink:writeMetric(java.lang.String),150,157,"/**
* Writes a single metric to StatsD.
* @param line the metric string to write
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,<init>,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:<init>(),38,40,"/**
 * Initializes network topology with default node group.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,connect,"org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,java.net.SocketAddress,int)",590,636,"/**
* Establishes a socket connection to a remote endpoint.
* @param socket the socket object
* @param endpoint the remote address to connect to
* @param localAddr the local address to bind (optional)
* @param timeout the connection timeout in milliseconds
* @throws IOException if an I/O error occurs
*/","* Like {@link NetUtils#connect(Socket, SocketAddress, int)} but
   * also takes a local address and port to bind the socket to. 
   * 
   * @param socket socket.
   * @param endpoint the remote address
   * @param localAddr the local address to bind the socket to
   * @param timeout timeout in milliseconds
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource,java.lang.Iterable,long,org.apache.hadoop.metrics2.impl.MetricsConfig)",90,98,"/**
* Creates a MetricsSourceAdapter instance with specified configuration.
* @param prefix metric prefix
* @param name metric name
* @param description metric description
* @param source underlying metrics source
* @param injectedTags additional tags to be added
* @param period data aggregation interval
* @param conf metrics configuration object
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,snapshotMetrics,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:snapshotMetrics(org.apache.hadoop.metrics2.impl.MetricsSourceAdapter,org.apache.hadoop.metrics2.impl.MetricsBufferBuilder)",420,427,"/**
 * Snaps a metrics collection from the given MetricsSourceAdapter.
 * @param sa adapter to the metrics source
 * @param bufferBuilder builder for the metrics buffer
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,updateJmxCache,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateJmxCache(),160,194,"/**
* Updates JMX cache by refreshing metrics and attributes.
* @param none
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,register,"org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.util.Map,java.lang.Object)",89,114,"/**
* Registers an MBean with the given properties and object.
* @param serviceName service identifier
* @param nameName unique name for the MBean
* @param properties bean registration properties
* @param theMbean MBean object to register
* @return registered ObjectName or null on failure
*/","* Register the MBean using our standard MBeanName format
   * ""hadoop:service={@literal <serviceName>,name=<nameName>}""
   * Where the {@literal <serviceName> and <nameName>} are the supplied
   * parameters.
   *
   * @param serviceName serviceName.
   * @param nameName nameName.
   * @param properties - Key value pairs to define additional JMX ObjectName
   *                     properties.
   * @param theMbean    - the MBean to register
   * @return the named used to register the MBean",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,unregisterSource,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:unregisterSource(java.lang.String),245,258,"/**
* Unregisters a source by name, stopping and removing associated metrics and callbacks.
* @param name unique identifier for the source to unregister
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stopSources,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSources(),460,469,"/**
* Stops all registered metrics sources and the system metrics source.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newInverseQuantiles,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newInverseQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",240,251,"/**
* Creates a synchronized mutable quantiles object with inverse functionality.
* @param name metric name
* @param desc metric description
* @param sampleName sample name
* @param valueName value name
* @param interval positive interval for quantiles calculation
* @return MutableQuantiles instance or null if invalid input","* Create a mutable inverse metric that estimates inverse quantiles of a stream of values
   * @param name of the metric
   * @param desc metric description
   * @param sampleName of the metric (e.g., ""Ops"")
   * @param valueName of the metric (e.g., ""Rate"")
   * @param interval rollover interval of estimator in seconds
   * @return a new inverse quantile estimator object
   * @throws MetricsException if interval is not a positive integer",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,<init>,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:<init>(),53,71,"/**
 * Initializes disk validator metrics for read and write operations.
 * @param quantileIntervals array of time intervals (e.g. [10, 30]) 
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache:<init>(java.lang.String,double,long)",196,204,"/**
* Initializes a retry cache with specified parameters.
* @param cacheName unique cache identifier
* @param percentage initial cache capacity percentage
* @param expirationTime maximum item lifetime in milliseconds
*/","* Constructor
   * @param cacheName name to identify the cache by
   * @param percentage percentage of total java heap space used by this cache
   * @param expirationTime time for an entry to expire in nanoseconds",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,init,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class),71,81,"/**
* Initializes the metrics cache with a given protocol class, skipping duplicates.
* @param protocol Class<?> to initialize from
*/","* Initialize the registry with all the methods in a protocol
   * so they all show up in the first snapshot.
   * Convenient for JMX implementations.
   * @param protocol the protocol class",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,init,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.String[]),89,93,"/**
* Initializes metrics by creating them if they do not exist.
* @param names array of metric names to initialize
*/","* Initialize the registry with all rate names passed in.
   * This is an alternative to the above init function since this metric
   * can be used more than just for rpc name.
   * @param names the array of all rate names",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,aggregateLocalStatesToGlobalMetrics,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:aggregateLocalStatesToGlobalMetrics(java.util.concurrent.ConcurrentMap),149,157,"/**
* Aggregates local state statistics into global metrics.
* @param localStats map of local thread-safe statistics
*/","* Aggregates the thread's local samples into the global metrics. The caller
   * should ensure its thread safety.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String,boolean)",310,312,"/**
* Creates a new rate with the given attributes.
* @param name rate name
* @param desc rate description
* @param extended whether to create an extended rate
*/","* Create a mutable rate metric (for throughput measurement).
   * @param name  of the metric
   * @param desc  description
   * @param extended  produce extended stat (stdev/min/max etc.) if true
   * @return a new mutable rate metric object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,init,org.apache.hadoop.metrics2.lib.MutableRates:init(java.lang.Class),58,69,"/**
* Initializes protocol with given class, registering methods as rate metrics.
* @param protocol Class containing methods to be registered
*/","* Initialize the registry with all the methods in a protocol
   * so they all show up in the first snapshot.
   * Convenient for JMX implementations.
   * @param protocol the protocol class",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,<init>,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:<init>(java.lang.String),53,58,"/**
* Registers detailed metrics for RPC scheduler with specified namespace.
* @param ns namespace string
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,<init>,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:<init>(int),57,62,"/**
* Initializes RPC detailed metrics for the specified port.
* @param port the port number (0-65535) to register metrics for
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addResponseTime,"org.apache.hadoop.ipc.DecayRpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",747,770,"/**
* Adds response time metrics for a schedulable with given details.
* @param callName name of the call
* @param schedulable Schedulable object to track metrics for
* @param details ProcessingDetails containing timing information
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,updateDeferredMetrics,"org.apache.hadoop.ipc.Server:updateDeferredMetrics(java.lang.String,long)",670,673,"/**
* Updates deferred RPC metrics with given processing time.
* @param name RPC operation name
* @param processingTime duration of processing time in milliseconds
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,updateMetrics,"org.apache.hadoop.ipc.Server:updateMetrics(org.apache.hadoop.ipc.Server$Call,long,boolean)",617,668,"/**
* Updates metrics for a given RPC request.
* @param call the RPC call object
* @param processingStartTimeNanos start time of processing in nanoseconds
* @param connDropped whether connection was dropped during processing
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,add,"org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:add(java.lang.Object,java.lang.reflect.Method)",162,170,"/**
* Adds metric annotations for a given method.
* @param source object to generate metrics for
* @param method the target method with metric annotations
*/",Add {@link MutableMetric} for a method annotated with {@link Metric},,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getMetrics,"org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",963,969,"/**
* Retrieves metrics from the decay RPC scheduler.
* @param collector MetricsCollector instance
* @param all whether to retrieve all metrics or only current ones
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateKeyLength,org.apache.hadoop.security.KDiag:validateKeyLength(),442,450,"/**
* Validates the maximum AES encryption key length.
* @throws NoSuchAlgorithmException if Java Cryptography Extensions are missing
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateUGI,"org.apache.hadoop.security.KDiag:validateUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",697,706,"/**
* Validates a UserGroupInformation object using Kerberos authentication.
* @param messagePrefix prefix for error messages
* @param user UserGroupInformation to validate
*/","* Validate the UGI: verify it is kerberized.
   * @param messagePrefix message in exceptions
   * @param user user to validate",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,verifyFileIsValid,"org.apache.hadoop.security.KDiag:verifyFileIsValid(java.io.File,java.lang.String,java.lang.String)",795,805,"/**
* Verifies a given file's existence and integrity.
* @param file the File object to validate
* @param category descriptive category for validation results
* @param text user-provided text associated with the file
* @return true if the file is valid; false otherwise
*/","* Verify that a file is valid: it is a file, non-empty and readable.
   * @param file file
   * @param category category for exceptions
   * @param text text message
   * @return true if the validation held; false if it did not <i>and</i>
   * {@link #nofail} has disabled raising exceptions.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateShortName,org.apache.hadoop.security.KDiag:validateShortName(),459,476,"/**
* Validates the short name of a Kerberos principal.
* @throws KerberosDiagsFailure if validation fails
*/","* Verify whether auth_to_local rules transform a principal name
   * <p>
   * Having a local user name ""bar@foo.com"" may be harmless, so it is noted at
   * info. However if what was intended is a transformation to ""bar""
   * it can be difficult to debug, hence this check.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,getUser,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getUser(),71,87,"/**
* Retrieves the user's UGI, creating a proxy if necessary.
* @return UserGroupInformation object or null if invalid owner
*/","* Get the username encoded in the token identifier
   * 
   * @return the username or owner",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,getUgi,org.apache.hadoop.util.ProtoUtil:getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto),133,150,"/**
* Creates a UserGroupInformation object from the provided user information.
* @param userInfo UserInformationProto containing effective and real users
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,<init>,org.apache.hadoop.fs.LocalFileSystem:<init>(org.apache.hadoop.fs.FileSystem),70,72,"/**
* Initializes a LocalFileSystem instance from a raw file system.
* @param rawLocalFileSystem underlying file system object
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,<init>,org.apache.hadoop.fs.shell.find.Find:<init>(),162,164,"/**
* Enables recursive search mode.",Default constructor for the Find command.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,<init>,org.apache.hadoop.fs.shell.Ls:<init>(),120,120,"/**
* Initializes an empty LightStream (Ls) instance.",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,<init>,org.apache.hadoop.fs.shell.Count:<init>(),110,110,"/**
* Initializes an empty Count object.",Constructor,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/RSErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:createEncoder(),38,41,"/**
* Creates an instance of RSErasureEncoder with coder options.",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/HHXORErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:createEncoder(),38,41,"/**
* Creates an encoder instance with default coder options.
* @return ErasureEncoder object
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/DummyErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:createEncoder(),36,39,"/**
* Creates an instance of ErasureEncoder with default options. 
* @return ErasureEncoder object initialized with coder options
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/XORErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:createEncoder(),39,42,"/**
* Creates an XOR-based erasure encoder.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/DummyErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:createDecoder(),41,44,"/**
* Creates and returns an instance of DummyErasureDecoder.
* @return DummyErasureDecoder object with coder options.",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/XORErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:createDecoder(),44,47,"/**
* Creates and returns an instance of XORErasureDecoder with coder options.",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/RSErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:createDecoder(),43,46,"/**
 * Creates an instance of RSErasureDecoder with configured coder options. 
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/HHXORErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:createDecoder(),43,46,"/**
* Creates and returns an instance of ErasureDecoder with specified options.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,checkAndUpdateMaps,org.apache.hadoop.security.ShellBasedIdMapping:checkAndUpdateMaps(),166,176,"/**
* Updates internal map data if it has expired.
* @throws IOException if unable to update maps
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,createConnection,org.apache.hadoop.ha.ActiveStandbyElector:createConnection(),894,909,"/**
* Reestablishes a ZooKeeper client connection.
* @throws IOException on connection errors
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,forceReloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:forceReloginFromKeytab(),1262,1266,"/**
* Forces user to relog in using keytab authentication.
*/","* Force re-Login a user in from a keytab file irrespective of the last login
   * time. Loads a user identity from a keytab file and logs them in. They
   * become the currently logged-in user. This method assumes that
   * {@link #loginUserFromKeytab(String, String)} had happened already. The
   * Subject field of this UserGroupInformation object is updated to have the
   * new credentials.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException on a failure",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(boolean),1268,1270,"/**
* Relogs user from keytab with optional TGT check.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,forceReloginFromTicketCache,org.apache.hadoop.security.UserGroupInformation:forceReloginFromTicketCache(),1302,1306,"/**
* Forces user to relog in from ticket cache.
*/","* Force re-Login a user in from the ticket cache irrespective of the last
   * login time. This method assumes that login had happened already. The
   * Subject field of this UserGroupInformation object is updated to have the
   * new credentials.
   *
   * @throws IOException
   *           raised on errors performing I/O.
   * @throws KerberosAuthException
   *           on a failure",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromTicketCache,org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache(),1316,1320,"/**
* Relogs user from ticket cache.
*/","* Re-Login a user in from the ticket cache.  This
   * method assumes that login had happened already.
   * The Subject field of this UserGroupInformation object is updated to have
   * the new credentials.
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException on a failure",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:createEncryptor(),58,62,"/**
* Creates an instance of the OpenSSL-based encryptor.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:createDecryptor(),64,68,"/**
* Creates an OpenSSL-based decryptor instance.
* @return Decryptor object for decryption operations
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,fillQueueForKey,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$EncryptedQueueRefiller:fillQueueForKey(java.lang.String,java.util.Queue,int)",145,161,"/**
* Fetches and adds a specified number of Encrypted Key Versions to the queue.
* @param keyName unique key identifier
* @param numEKVs number of EKVs to generate
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,generateEncryptedKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:generateEncryptedKey(java.lang.String),788,799,"/**
* Retrieves an Encrypted Key Version from the queue.
* @param encryptionKeyName unique name for the key
* @return EncryptedKeyVersion object or throws exception if failed",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,drain,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:drain(java.lang.String),311,316,"/**
* Drains encryption keys from all KMS client providers.
* @param keyName name of the encryption key to drain 
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,queueCall,org.apache.hadoop.ipc.Server:queueCall(org.apache.hadoop.ipc.Server$Call),3097,3104,"/**
* Queues a remote procedure call to be executed asynchronously.
* @param call RPC invocation details
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddrForHost,"org.apache.hadoop.net.NetUtils:createSocketAddrForHost(java.lang.String,int)",308,325,"/**
* Creates an InetSocketAddress for the given host and port.
* @param host hostname or IP address
* @param port port number
* @return InetSocketAddress object or unresolved address if host lookup fails
*/","* Create a socket address with the given host and port.  The hostname
   * might be replaced with another host that was set via
   * {@link #addStaticResolution(String, String)}.  The value of
   * hadoop.security.token.service.use_ip will determine whether the
   * standard java host resolver is used, or if the fully qualified resolver
   * is used.
   * @param host the hostname or IP use to instantiate the object
   * @param port the port number
   * @return InetSocketAddress",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,canonicalizeHost,org.apache.hadoop.net.NetUtils:canonicalizeHost(java.lang.String),362,377,"/**
* Normalizes the hostname by resolving it to its FQDN.
* @param host hostname to be canonicalized
* @return fully qualified hostname or original hostname if resolution failed
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getLocalInetAddress,org.apache.hadoop.net.NetUtils:getLocalInetAddress(java.lang.String),798,811,"/**
* Retrieves the InetAddress of the specified host, filtering out non-local addresses.
* @param host hostname or IP address to resolve
* @return InetAddress object for the host if it's local; null otherwise
*/","* Checks if {@code host} is a local host name and return {@link InetAddress}
   * corresponding to that address.
   * 
   * @param host the specified host
   * @return a valid local {@link InetAddress} or null
   * @throws SocketException if an I/O error occurs",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,sendSaslMessage,"org.apache.hadoop.security.SaslRpcClient:sendSaslMessage(java.io.OutputStream,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)",457,469,"/**
* Sends SASL message to output stream.
* @param out OutputStream to write to
* @param message RpcSaslProto message to send
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,writeConnectionContext,"org.apache.hadoop.ipc.Client$Connection:writeConnectionContext(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",1007,1028,"/**
* Writes connection context to remote server.
* @param remoteId unique ID of the remote connection
* @param authMethod authentication method used for this connection
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,sendRpcRequest,org.apache.hadoop.ipc.Client$Connection:sendRpcRequest(org.apache.hadoop.ipc.Client$Call),1159,1191,"/**
* Sends an RPC request to the connection.
* @param call The RPC call to send
*/","Initiates a rpc call by sending the rpc request to the remote server.
     * Note: this is not called from the current thread, but by another
     * thread, so that if the current thread is interrupted that the socket
     * state isn't corrupted with a partially written message.
     * @param call - the rpc request",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getPriorityLevel,org.apache.hadoop.ipc.Server:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation),727,730,"/**
* Retrieves priority level from call queue.
* @param ugi UserGroupInformation object
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,receiveRpcResponse,org.apache.hadoop.ipc.Client$Connection:receiveRpcResponse(),1196,1249,"/**
* Processes RPC response from server.
* @throws IOException on read error or fatal status
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processRpcRequest,"org.apache.hadoop.ipc.Server$Connection:processRpcRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)",2869,2970,"/**
* Processes an RPC request from a client.
* @param header RpcRequestHeaderProto containing request metadata
* @param buffer RpcWritable.Buffer containing serialized request data
*/","* Process an RPC Request 
     *   - the connection headers and context must have been already read.
     *   - Based on the rpcKind, decode the rpcRequest.
     *   - A successfully decoded RpcCall will be deposited in RPC-Q and
     *     its response will be sent later when the request is processed.
     * @param header - RPC request header
     * @param buffer - stream to request payload
     * @throws RpcServerException - generally due to fatal rpc layer issues
     *   such as invalid header or deserialization error.  The call queue
     *   may also throw a fatal or non-fatal exception on overflow.
     * @throws IOException - fatal internal error that should/could not
     *   be sent to client.
     * @throws InterruptedException",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponse,"org.apache.hadoop.ipc.Server:setupResponse(org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)",3507,3547,"/**
* Sets up the response to an RPC call.
* @param call RPC call context
* @param status RPC status (FATAL, SUCCESS, or ERROR)
* @param erCode error code for failure responses
* @param rv response value (optional)
* @param errorClass and error detail for failure responses
*/","* Setup response for the IPC Call.
   * 
   * @param call {@link Call} to which we are setting up the response
   * @param status of the IPC call
   * @param rv return value for the IPC Call, if the call was successful
   * @param errorClass error class, if the the call failed
   * @param error error message, if the call failed
   * @throws IOException",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,wrapWithSasl,org.apache.hadoop.ipc.Server:wrapWithSasl(org.apache.hadoop.ipc.Server$RpcCall),3641,3661,"/**
* Wraps RPC call response with SASL token.
* @param call the RPC call to wrap
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,initializeAuthContext,org.apache.hadoop.ipc.Server$Connection:initializeAuthContext(int),2567,2593,"/**
* Initializes and returns the AuthProtocol instance based on the provided type.
* @param authType integer identifier for the authentication protocol
* @return AuthProtocol instance or throws exception if unknown protocol is specified
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CacheableIPList.java,reset,org.apache.hadoop.util.CacheableIPList:reset(),42,45,"/**
* Resets and updates internal state by reloading IP list and updating cache expiry time.",* Reloads the ip list,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,main,org.apache.hadoop.util.SysInfoLinux:main(java.lang.String[]),705,734,"/**
* Prints system information on Linux.
* @param none
*/","* Test the {@link SysInfoLinux}.
   *
   * @param args - arguments to this calculator test",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,open,"org.apache.hadoop.fs.http.HttpsFileSystem:open(org.apache.hadoop.fs.Path,int)",61,67,"/**
* Opens a file stream from an HTTP connection.
* @param path URI of the remote resource
* @param bufferSize buffer size for data transfer
* @return FSDataInputStream object or throws IOException on failure
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,open,"org.apache.hadoop.fs.http.HttpFileSystem:open(org.apache.hadoop.fs.Path,int)",61,67,"/**
* Opens a file stream for the given path with specified buffer size.
* @param path file system path
* @param bufferSize data read/write buffer size
* @return FSDataInputStream object or throws IOException on error
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,remove,org.apache.hadoop.util.LightWeightCache:remove(java.lang.Object),223,232,"/**
* Removes entry by key and updates associated queue.
* @param key unique identifier of entry to remove
* @return removed entry or null if not found
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,put,org.apache.hadoop.util.LightWeightCache:put(java.lang.Object),201,221,"/**
* Inserts a user entry into the cache and updates expiration time.
* @param entry Entry object to insert
* @return previously cached entry or null if new
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean)",394,396,"/**
* Constructs string representation with query and help options.
* @param qOption include query option
* @param hOption include help option
*/","Return the string representation of the object in the output format.
   * For description of the options,
   * @see #toString(boolean, boolean, boolean, boolean, List)
   * 
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output if to be used
   * @return the string representation of the object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,processPath,org.apache.hadoop.fs.shell.Count:processPath(org.apache.hadoop.fs.shell.PathData),195,217,"/**
* Processes the path by generating a string representation of the content and quotas.
* @param src PathData object containing the file system data
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,toString,org.apache.hadoop.fs.QuotaUsage:toString(boolean),307,309,"/**
* Returns a string representation of the object with header option.
* @param hOption true to include header in the output
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,read,"org.apache.hadoop.fs.FSInputChecker:read(byte[],int,int)",191,209,"/**
* Reads bytes from the underlying stream into the given byte array.
* @param b byte array to fill
* @param off starting offset in the array
* @param len number of bytes to read
* @return actual number of bytes read, or -1 on end-of-stream
*/","* Read checksum verified bytes from this byte-input stream into 
   * the specified byte array, starting at the given offset.
   *
   * <p> This method implements the general contract of the corresponding
   * <code>{@link InputStream#read(byte[], int, int) read}</code> method of
   * the <code>{@link InputStream}</code> class.  As an additional
   * convenience, it attempts to read as many bytes as possible by repeatedly
   * invoking the <code>read</code> method of the underlying stream.  This
   * iterated <code>read</code> continues until one of the following
   * conditions becomes true: <ul>
   *
   *   <li> The specified number of bytes have been read,
   *
   *   <li> The <code>read</code> method of the underlying stream returns
   *   <code>-1</code>, indicating end-of-file.
   *
   * </ul> If the first <code>read</code> on the underlying stream returns
   * <code>-1</code> to indicate end-of-file then this method returns
   * <code>-1</code>.  Otherwise this method returns the number of bytes
   * actually read.
   *
   * @param      b     destination buffer.
   * @param      off   offset at which to start storing bytes.
   * @param      len   maximum number of bytes to read.
   * @return     the number of bytes read, or <code>-1</code> if the end of
   *             the stream has been reached.
   * @exception  IOException  if an I/O error occurs.
   *             ChecksumException if any checksum error occurs",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,createExpression,"org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.Class,org.apache.hadoop.conf.Configuration)",127,134,"/**
* Creates a new instance of the specified expression class.
* @param expressionClass Class to instantiate
* @param conf Configuration object for instantiation
* @return Newly created Expression instance or null if invalid class
*/","* Creates an instance of the requested {@link Expression} class.
   *
   * @param expressionClass
   *          {@link Expression} class to be instantiated
   * @param conf
   *          the Hadoop configuration
   * @return a new instance of the requested {@link Expression} class",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,getInstance,"org.apache.hadoop.fs.shell.CommandFactory:getInstance(java.lang.String,org.apache.hadoop.conf.Configuration)",118,131,"/**
* Retrieves a Command instance by name from the object map or dynamically instantiates it based on the provided configuration.
* @param cmdName unique command identifier
* @param conf configuration settings
* @return Command object or null if not found
*/","* Get an instance of the requested command
   * @param cmdName name of the command to lookup
   * @param conf the hadoop configuration
   * @return the {@link Command} or null if the command is unknown",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,newKey,org.apache.hadoop.io.WritableComparator:newKey(),166,168,"/**
* Creates and returns a new instance of the key class.
* @return an instance of the key class
*/","* Construct a new {@link WritableComparable} instance.
   * @return WritableComparable.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,readFields,org.apache.hadoop.io.SortedMapWritable:readFields(java.io.DataInput),156,180,"/**
* Reads map entries from DataInput stream.
* @throws IOException if read operation fails
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/GenericWritable.java,readFields,org.apache.hadoop.io.GenericWritable:readFields(java.io.DataInput),124,135,"/**
* Reads and initializes a Writable object from the given DataInput stream.
* @throws IOException if initialization fails
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,add,"org.apache.hadoop.io.serializer.SerializationFactory:add(org.apache.hadoop.conf.Configuration,java.lang.String)",69,79,"/**
* Adds a serialization class to the list of available serializations.
* @param conf configuration object
* @param serializationName name of the serialization class to add
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/WritableSerialization.java,deserialize,org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:deserialize(org.apache.hadoop.io.Writable),62,73,"/**
* Deserializes a Writable object from the given input.
* @param w Writable object to deserialize
* @return deserialized Writable object or null if not provided
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,readFields,org.apache.hadoop.io.MapWritable:readFields(java.io.DataInput),165,191,"/**
* Reads map entries from DataInput stream.
* @throws IOException if I/O error occurs
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableFactories.java,newInstance,"org.apache.hadoop.io.WritableFactories:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",63,74,"/**
* Creates a new instance of the given Writable class.
* @param c Class<? extends Writable> to instantiate
* @param conf Configuration object for configuration
* @return Instantiated Writable object or null if creation fails
*/","* Create a new instance of a class with a defined factory.
   *
   * @param c input c.
   * @param conf input configuration.
   * @return a new instance of a class with a defined factory.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getSocketFactoryFromProperty,"org.apache.hadoop.net.NetUtils:getSocketFactoryFromProperty(org.apache.hadoop.conf.Configuration,java.lang.String)",142,152,"/**
* Retrieves SocketFactory instance from configuration property.
* @param conf Configuration object
* @param propValue Property value containing factory class name
* @return SocketFactory instance or throws exception if not found
*/","* Get the socket factory corresponding to the given proxy URI. If the
   * given proxy URI corresponds to an absence of configuration parameter,
   * returns null. If the URI is malformed raises an exception.
   *
   * @param conf configuration.
   * @param propValue the property which is the class name of the
   *        SocketFactory to instantiate; assumed non null and non empty.
   * @return a socket factory as defined in the property value.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,decodeIdentifier,org.apache.hadoop.security.token.Token:decodeIdentifier(),164,176,"/**
* Decodes identifier into object of type T.
* @return instance of T or null if decoding fails
*/","* Get the token identifier object, or null if it could not be constructed
   * (because the class could not be loaded, for example).
   * @return the token identifier, or null if there was no class found for it
   * @throws IOException failure to unmarshall the data
   * @throws RuntimeException if the token class could not be instantiated.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskValidatorFactory.java,getInstance,org.apache.hadoop.util.DiskValidatorFactory:getInstance(java.lang.Class),45,62,"/**
* Returns a singleton instance of the specified DiskValidator class.
* @param clazz the type of validator to retrieve
* @return DiskValidator instance or null if not created
*/","* Returns a {@link DiskValidator} instance corresponding to the passed clazz.
   * @param clazz a class extends {@link DiskValidator}
   * @return disk validator.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,createFenceMethod,"org.apache.hadoop.ha.NodeFencer:createFenceMethod(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",167,195,"/**
* Creates a FenceMethodWithArg instance by instantiating and configuring the specified fencing method.
* @param conf configuration object
* @param clazzName name of the fencing method class (e.g. ""ClassName"")
* @param arg argument to pass to the fencing method
* @return FenceMethodWithArg instance or throws BadFencingConfigurationException if creation fails",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:<init>(),144,146,"/**
* Initializes DynamicWrappedIO with default class name.
*/
 
or 
 
/**
* Constructs DynamicWrappedIO instance with default wrapped IO class name.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:<init>(),224,226,"/**
 * Initializes wrapped statistics with default class name.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refresh,"org.apache.hadoop.util.HostsFileReader:refresh(java.lang.String,java.lang.String)",190,193,"/**
* Refreshes project configuration by reading from specified files.
* @param includesFile file containing include patterns
* @param excludesFile file containing exclude patterns
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,lazyRefresh,"org.apache.hadoop.util.HostsFileReader:lazyRefresh(java.lang.String,java.lang.String)",195,198,"/**
* Performs lazy refresh on project files.
* @param includesFile file to include in refresh
* @param excludesFile file to exclude from refresh
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,<init>,"org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>(int,int,int,int)",126,134,"/**
* Initializes a DynamicBloomFilter instance with specified parameters.
* @param vectorSize filter size
* @param nbHash number of hash functions
* @param hashType type of hash function
* @param nr additional parameter for internal use
*/","* Constructor.
   * <p>
   * Builds an empty Dynamic Bloom filter.
   * @param vectorSize The number of bits in the vector.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).
   * @param nr The threshold for the maximum number of keys to record in a
   * dynamic Bloom filter row.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,addRow,org.apache.hadoop.util.bloom.DynamicBloomFilter:addRow(),275,285,"/**
* Adds a new row to the matrix by appending an empty BloomFilter.
*/",* Adds a new row to <i>this</i> dynamic Bloom filter.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,<init>,"org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>(int,int,int)",111,116,"/**
* Initializes a new RetouchedBloomFilter instance with specified parameters.
* @param vectorSize size of the filter's vector
* @param nbHash number of hash functions to use
* @param hashType type of hash function to employ
*/","* Constructor
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,readFields,org.apache.hadoop.util.bloom.DynamicBloomFilter:readFields(java.io.DataInput),259,270,"/**
* Reads fields from input stream to populate object state.
* @throws IOException if I/O operation fails
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,readFields,org.apache.hadoop.util.bloom.RetouchedBloomFilter:readFields(java.io.DataInput),429,454,"/**
* Reads data from DataInput stream and populates object fields.
* @throws IOException if read operation fails
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printInstanceHelp,"org.apache.hadoop.fs.FsShell:printInstanceHelp(java.io.PrintStream,org.apache.hadoop.fs.shell.Command)",253,288,"/**
* Prints detailed help information about a command instance.
* @param out output stream to print to
* @param instance Command object containing usage and description
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadProps,"org.apache.hadoop.conf.Configuration:loadProps(java.util.Properties,int,boolean)",2961,2981,"/**
* Loads properties and resources synchronously.
* @param props Properties object to update
* @param startIdx Starting index for resource loading
* @param fullReload Whether to reload all resources (true) or not (false)
*/","* Loads the resource at a given index into the properties.
   * @param props the object containing the loaded properties.
   * @param startIdx the index where the new resource has been added.
   * @param fullReload flag whether we do complete reload of the conf instead
   *                   of just loading the new resource.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,becomeActive,org.apache.hadoop.ha.ActiveStandbyElector:becomeActive(),936,956,"/**
* Tries to become active by writing a breadcrumb node and making an API call.
* @throws Exception if API call fails
* @return true on success, false on failure (caller handles quit/rejoin)
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,quitElection,org.apache.hadoop.ha.ActiveStandbyElector:quitElection(boolean),443,452,"/**
* Quits the election process; removes persistent ZNode if not fenced and active.
* @param needFence whether to fence or not
*/","* Any service instance can drop out of the election by calling quitElection. 
   * <br>
   * This will lose any leader status, if held, and stop monitoring of the lock
   * node. <br>
   * If the instance wants to participate in election again, then it needs to
   * call joinElection(). <br>
   * This allows service instances to take themselves out of rotation for known
   * impending unavailable states (e.g. long GC pause or software upgrade).
   * 
   * @param needFence true if the underlying daemon may need to be fenced
   * if a failover occurs due to dropping out of the election.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PositionedReadable.java,readVectored,"org.apache.hadoop.fs.PositionedReadable:readVectored(java.util.List,java.util.function.IntFunction)",132,135,"/**
* Reads vectored file ranges.
* @param ranges list of file ranges to read
* @param allocate callback for allocating buffer space
*/","* Read fully a list of file ranges asynchronously from this file.
   * The default iterates through the ranges to read each synchronously, but
   * the intent is that FSDataInputStream subclasses can make more efficient
   * readers.
   * As a result of the call, each range will have FileRange.setData(CompletableFuture)
   * called with a future that when complete will have a ByteBuffer with the
   * data from the file's range.
   * <p>
   *   The position returned by getPos() after readVectored() is undefined.
   * </p>
   * <p>
   *   If a file is changed while the readVectored() operation is in progress, the output is
   *   undefined. Some ranges may have old data, some may have new and some may have both.
   * </p>
   * <p>
   *   While a readVectored() operation is in progress, normal read api calls may block.
   * </p>
   * @param ranges the byte ranges to read
   * @param allocate the function to allocate ByteBuffer
   * @throws IOException any IOE.
   * @throws IllegalArgumentException if the any of ranges are invalid, or they overlap.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,close,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:close(),228,248,"/**
* Closes the operations and cleans up resources.
* @throws No such operation as close() is not applicable here. This method simply ensures that all resources are released.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,read,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:read(org.apache.hadoop.fs.impl.prefetch.BufferData),308,317,"/**
* Reads a buffer block from the provided data.
* @param data BufferData object containing the block to read
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,prefetch,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:prefetch(org.apache.hadoop.fs.impl.prefetch.BufferData,java.time.Instant)",319,329,"/**
* Prefetches block for given BufferData instance.
* @param data BufferData object to prefetch
* @param taskQueuedStartTime Instant when task was queued
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],long,long)",109,112,"/**
* Constructor to initialize BlockLocation with given parameters.
* @param names array of block names
* @param hosts array of hostnames or IP addresses
* @param offset starting position in bytes
* @param length total size in bytes
*/","* Constructor with host, name, offset and length.
   * @param names names array.
   * @param hosts host array.
   * @param offset offset.
   * @param length length.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationStatisticSummary.java,<init>,"org.apache.hadoop.fs.statistics.DurationStatisticSummary:<init>(java.lang.String,boolean,long,long,long,org.apache.hadoop.fs.statistics.MeanStatistic)",71,83,"/**
* Initializes a DurationStatisticSummary object with provided values.
* @param key summary key
* @param success true if successful, false otherwise
* @param count total duration count
* @param max maximum duration value
* @param min minimum duration value
* @param mean optional MeanStatistic object to clone (or null)","* Constructor.
   * @param key Statistic key.
   * @param success Are these success or failure statistics.
   * @param count Count of operation invocations.
   * @param max Max duration; -1 if unknown.
   * @param min Min duration; -1 if unknown.
   * @param mean Mean duration -may be null. (will be cloned)",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,aggregate,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:aggregate(org.apache.hadoop.fs.statistics.IOStatistics),178,199,"/**
* Aggregates statistics from the provided source.
* @param source IOStatistics object to merge with existing data
* @return true if aggregation was successful, false otherwise
*/","* Aggregate the current statistics with the
   * source reference passed in.
   *
   * The operation is synchronized.
   * @param source source; may be null
   * @return true if a merge took place.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,<init>,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:<init>(org.apache.hadoop.fs.statistics.IOStatistics),123,129,"/**
* Initializes statistics snapshot from given source or creates default maps.
* @param source IOStatistics object to snapshot, or null for defaults
*/","* Construct, taking a snapshot of the source statistics data
   * if the source is non-null.
   * If the source is null, the empty maps are created
   * @param source statistics source. Nullable.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,foreach,"org.apache.hadoop.util.functional.RemoteIterators:foreach(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.ConsumerRaisingIOE)",273,288,"/**
* Iterates over remote data source and applies consumer function.
* @param source RemoteIterator to read from
* @param consumer Consumer function to apply to each item
* @return Item count, or -1 on error
*/","* Apply an operation to all values of a RemoteIterator.
   *
   * If the iterator is an IOStatisticsSource returning a non-null
   * set of statistics, <i>and</i> this classes log is set to DEBUG,
   * then the statistics of the operation are evaluated and logged at
   * debug.
   * <p>
   * The number of entries processed is returned, as it is useful to
   * know this, especially during tests or when reporting values
   * to users.
   * </p>
   * This does not close the iterator afterwards.
   * @param source iterator source
   * @param consumer consumer of the values.
   * @return the number of elements processed
   * @param <T> type of source
   * @throws IOException if the source RemoteIterator or the consumer raise one.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackInvocation,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackInvocation(org.apache.hadoop.util.functional.InvocationRaisingIOE,java.lang.String,org.apache.hadoop.metrics2.lib.MutableRate)",1014,1024,"/**
* Tracks the duration of an invocation and updates a rate metric.
* @param invocation IOE event to track
* @param statistic identifier for tracking data
* @param metric MutableRate object to update
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFully,"org.apache.hadoop.crypto.CryptoInputStream:readFully(long,byte[])",517,520,"/**
* Reads data fully into provided buffer starting at specified position.
* @param position offset to start reading from
* @param buffer array to store the read data in
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataInputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",28,31,"/**
* Initializes a CryptoFSDataInputStream with the given parameters.
* @param in underlying FSDataInputStream
* @param codec encryption codec to use
* @param bufferSize buffer size for decryption
* @param key encryption key
* @param iv initialization vector for encryption
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:performCoding(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",67,99,"/**
* Performs encoding on input data and parity units.
* @param inputs input data buffers
* @param outputs output parity buffers
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,writeObject,"org.apache.hadoop.io.ObjectWritable:writeObject(java.io.DataOutput,java.lang.Object,java.lang.Class,org.apache.hadoop.conf.Configuration)",142,146,"/**
* Writes object to output stream using specified configuration.
* @param out DataOutput stream
* @param instance Object to serialize
* @param declaredClass Class of the object
* @param conf Configuration for serialization
*/","* Write a {@link Writable}, {@link String}, primitive type, or an array of
   * the preceding.
   *
   * @param out DataOutput.
   * @param instance instance.
   * @param conf Configuration.
   * @param declaredClass declaredClass.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,write,org.apache.hadoop.ipc.WritableRpcEngine$Invocation:write(java.io.DataOutput),166,179,"/**
* Writes a serialized RPC request to the output stream.
* @param out DataOutput stream
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,requestPrefetch,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestPrefetch(int),256,289,"/**
* Initiates a prefetch operation for the specified block number.
* @param blockNumber unique block identifier
*/","* Requests optional prefetching of the given block.
   * The block is prefetched only if we can acquire a free buffer.
   *
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,getData,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getData(int),631,633,"/**
* Attempts to retrieve data from the buffer pool at the specified block number.
* @param blockNumber unique identifier for the requested block
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,acquire,org.apache.hadoop.fs.impl.prefetch.BufferPool:acquire(int),125,150,"/**
* Attempts to acquire BufferData with specified block number, retrying on failure.
* @param blockNumber unique block identifier
* @return BufferData object if acquired successfully, or null otherwise
*/","* Acquires a {@code ByteBuffer}; blocking if necessary until one becomes available.
   * @param blockNumber the id of the block to acquire.
   * @return the acquired block's {@code BufferData}.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPathArgument,org.apache.hadoop.fs.shell.Command:processPathArgument(org.apache.hadoop.fs.shell.PathData),315,320,"/**
* Initializes path processing with a single item and resets depth.
* @param item PathData object to start processing from
*/","*  This is the last chance to modify an argument before going into the
   *  (possibly) recursive {@link #processPaths(PathData, PathData...)}
   *  {@literal ->} {@link #processPath(PathData)} loop.  Ex.  ls and du use
   *  this to expand out directories.
   *  @param item a {@link PathData} representing a path which exists
   *  @throws IOException if anything goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPaths,"org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)",361,380,"/**
* Processes file paths in batches based on the listing group size.
* @param parent parent path data
* @param itemsIterator iterator over child path data
*/","* Iterates over the given expanded paths and invokes
   * {@link #processPath(PathData)} on each element. If ""recursive"" is true,
   * will do a post-visit DFS on directories.
   * @param parent if called via a recurse, will be the parent dir, else null
   * @param itemsIterator a iterator of {@link PathData} objects to process
   * @throws IOException if anything goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,resolvePartialGroupNames,"org.apache.hadoop.security.ShellBasedUnixGroupsMapping:resolvePartialGroupNames(java.lang.String,java.lang.String,java.lang.String)",280,319,"/**
* Resolves partial group names for a given user.
* @param userName unique user identifier
* @param errMessage error message to include in exceptions
* @return set of resolved group names or throws exception if failed
*/","* Attempt to partially resolve group names.
   *
   * @param userName the user's name
   * @param errMessage error message from the shell command
   * @param groupNames the incomplete list of group names
   * @return a set of resolved group names
   * @throws PartialGroupNameException if the resolution fails or times out",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File)",1218,1220,"/**
* Constructs a new ShellCommandExecutor instance with default configuration.
* @param execString array of command strings to execute
* @param dir directory path for execution context
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execCommand,org.apache.hadoop.util.Shell:execCommand(java.lang.String[]),1358,1360,"/**
* Executes a command in shell and returns output as string.
* @param cmd command to execute (can contain multiple arguments)
*/","* Static method to execute a shell command.
   * Covers most of the simple cases without requiring the user to implement
   * the <code>Shell</code> interface.
   * @param cmd shell command to execute.
   * @return the output of the executed command.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execCommand,"org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[])",1390,1393,"/**
* Executes a shell command with optional environment variables.
* @param env map of environment variables (key-value pairs)
* @param cmd command to execute as vararg array
*/","* Static method to execute a shell command.
   * Covers most of the simple cases without requiring the user to implement
   * the <code>Shell</code> interface.
   * @param env the map of environment key=value
   * @param cmd shell command to execute.
   * @return the output of the executed command.
   * @throws IOException on any problem.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,addAll,org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials),450,452,"/**
* Adds all credentials from another Credentials object.
* @param other Credentials to merge with current instance
*/","* Copy all of the credentials from one credential object into another.
   * Existing secrets and tokens are overwritten.
   * @param other the credentials to copy",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,mergeAll,org.apache.hadoop.security.Credentials:mergeAll(org.apache.hadoop.security.Credentials),459,461,"/**
* Merges all credentials from another user into this instance.
* @param other Credentials object to merge from
*/","* Copy all of the credentials from one credential object into another.
   * Existing secrets and tokens are not overwritten.
   * @param other the credentials to copy",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DelegationTokenIssuer.java,addDelegationTokens,"org.apache.hadoop.security.token.DelegationTokenIssuer:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)",79,87,"/**
* Adds delegation tokens for the given renewer.
* @param renewer unique identifier of the token issuer
* @param credentials authentication details for token issuance
* @return array of Token objects or empty array if none found
*/","* Given a renewer, add delegation tokens for issuer and it's child issuers
   * to the <code>Credentials</code> object if it is not already present.
   *<p>
   * Note: This method is not intended to be overridden.  Issuers should
   * implement getCanonicalService and getDelegationToken to ensure
   * consistent token acquisition behavior.
   *
   * @param renewer the user allowed to renew the delegation tokens
   * @param credentials cache in which to add new delegation tokens
   * @return list of new delegation tokens
   * @throws IOException thrown if IOException if an IO error occurs.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addToken,org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.security.token.Token),1701,1703,"/**
* Adds a new token to the system.
* @param token Token object with service and identifier
*/","* Add a token to this UGI
   * 
   * @param token Token to be added
   * @return true on successful add of new token",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.RawLocalFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),850,854,"/**
* Sets the working directory to an absolute path.
* @param newDir new working directory as a Path object
*/",* Set the working directory to the given directory.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,exists,org.apache.hadoop.fs.RawLocalFileSystem:exists(org.apache.hadoop.fs.Path),768,771,"/**
* Checks if file exists at specified path.
* @param f Path to check
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getStatus,org.apache.hadoop.fs.RawLocalFileSystem:getStatus(org.apache.hadoop.fs.Path),866,874,"/**
* Retrieves file system status for a given path.
* @param p Path object representing the file system location
* @return FsStatus object containing total, used, and free space
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setTimes,"org.apache.hadoop.fs.RawLocalFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",1129,1140,"/**
* Sets file timestamps (modification and access times).
* @param p Path to the file
* @param mtime Modification time in milliseconds or -1 for default
* @param atime Access time in milliseconds or -1 for default
*/","* Sets the {@link Path}'s last modified time and last access time to
   * the given valid times.
   *
   * @param mtime the modification time to set (only if no less than zero).
   * @param atime the access time to set (only if no less than zero).
   * @throws IOException if setting the times fails.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,pathToFile,org.apache.hadoop.fs.LocalFileSystem:pathToFile(org.apache.hadoop.fs.Path),79,81,"/**
 * Converts a Path to a File using the underlying RawLocalFileSystem.
 * @param path input file system path
 */","* Convert a path to a File.
   * @param path the path.
   * @return file.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getUriPath,org.apache.hadoop.fs.viewfs.ViewFileSystem:getUriPath(org.apache.hadoop.fs.Path),264,267,"/**
* Converts a Path to its absolute URI path.
* @param p input file system path
*/","* Make the path Absolute and get the path-part of a pathname.
   * Checks that URI matches this file system
   * and that the path-part is a valid name.
   *
   * @param p path
   * @return path-part of the Path p",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,setWorkingDirectory,org.apache.hadoop.fs.viewfs.NflyFSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),854,859,"/**
* Updates working directory for all NflyNodes.
* @param newDir new file system path to set as working directory
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Stat.java,<init>,"org.apache.hadoop.fs.Stat:<init>(org.apache.hadoop.fs.Path,long,boolean,org.apache.hadoop.fs.FileSystem)",50,68,"/**
* Initializes Stat object with file system and block size.
* @param path original file path
* @param blockSize block size in bytes
* @param deref whether to dereference symlinks
* @param fs FileSystem instance
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,makeQualified,org.apache.hadoop.fs.Path:makeQualified(org.apache.hadoop.fs.FileSystem),547,550,"/**
* Creates a qualified path by wrapping an existing file system URI and working directory.
* @param fs FileSystem object containing URI and working directory
*/","* Returns a qualified path object for the {@link FileSystem}'s working
   * directory.
   *  
   * @param fs the target FileSystem
   * @return a qualified path object for the FileSystem's working directory
   * @deprecated use {@link #makeQualified(URI, Path)}",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,makeQualified,org.apache.hadoop.fs.FileContext:makeQualified(org.apache.hadoop.fs.Path),629,631,"/**
* Makes the given file system path qualified with default FS and working directory.
* @param path the input file system path
*/","* Make the path fully qualified if it is isn't. 
   * A Fully-qualified path has scheme and authority specified and an absolute
   * path.
   * Use the default file system and working dir in this FileContext to qualify.
   * @param path the path.
   * @return qualified path",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,makeQualified,org.apache.hadoop.fs.AbstractFileSystem:makeQualified(org.apache.hadoop.fs.Path),438,441,"/**
* Makes a qualified file system path by wrapping it with a URI.
* @param path the input file system path
*/","* Make the path fully qualified to this file system
   * @param path the path.
   * @return the qualified path",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path),1926,1937,"/**
* Resolves file status for the given path.
* @param f input path
* @return array of FileStatus objects or null if not found
*/","* List the statuses of the files/directories in the given path 
     * if the path is a directory.
     * 
     * @param f is the path
     *
     * @return an array that contains statuses of the files/directories 
     *         in the given path
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If <code>f</code> does not exist
     * @throws UnsupportedFileSystemException If file system for <code>f</code> is
     *           not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,fixRelativePart,org.apache.hadoop.fs.Globber:fixRelativePart(org.apache.hadoop.fs.Path),138,144,"/**
* Resolves relative paths using either Filesystem or FileContext.
* @param path the input path to resolve
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,delete,"org.apache.hadoop.fs.FileContext:delete(org.apache.hadoop.fs.Path,boolean)",842,853,"/**
* Deletes a file or directory with optional recursion.
* @param f file/directory to delete
* @param recursive true for recursive deletion
* @return true if successful, false otherwise
*/","* Delete a file.
   * @param f the path to delete.
   * @param recursive if path is a directory and set to 
   * true, the directory is deleted else throws an exception. In
   * case of a file the recursive can be set to either true or false.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is invalid
   *
   * @return if delete success true, not false.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,open,org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path),873,883,"/**
* Opens a file stream for the given path.
* @param f Path to the file
* @return FSDataInputStream object or null if not resolved
*/","* Opens an FSDataInputStream at the indicated Path using
   * default buffersize.
   * @param f the file name to open
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code>
   *         is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * @return input stream.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,open,"org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path,int)",904,915,"/**
* Opens a file stream for the given path with specified buffer size.
* @param f file path
* @param bufferSize input/output buffer size
* @return FSDataInputStream object or throws an exception if failed
*/","* Opens an FSDataInputStream at the indicated Path.
   * 
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * @return output stream.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,truncate,"org.apache.hadoop.fs.FileContext:truncate(org.apache.hadoop.fs.Path,long)",947,958,"/**
* Truncates a file to a specified length.
* @param f file path
* @param newLength desired file size in bytes
* @return true if truncation is successful, false otherwise
*/","* Truncate the file in the indicated path to the indicated size.
   * <ul>
   * <li>Fails if path is a directory.
   * <li>Fails if path does not exist.
   * <li>Fails if path is not closed.
   * <li>Fails if new size is greater than current size.
   * </ul>
   * @param f The path to the file to be truncated
   * @param newLength The size the file is to be truncated to
   *
   * @return <code>true</code> if the file has been truncated to the desired
   * <code>newLength</code> and is immediately available to be reused for
   * write operations such as <code>append</code>, or
   * <code>false</code> if a background process of adjusting the length of
   * the last block has been started, and clients should wait for it to
   * complete before proceeding with further file updates.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   *
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setReplication,"org.apache.hadoop.fs.FileContext:setReplication(org.apache.hadoop.fs.Path,short)",978,989,"/**
* Sets file system replication factor for a given path.
* @param f file path
* @param replication desired replication factor (short value)
*/","* Set replication for an existing file.
   * 
   * @param f file name
   * @param replication new replication
   *
   * @return true if successful
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setPermission,"org.apache.hadoop.fs.FileContext:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1079,1091,"/**
* Sets file system permissions for a specified path.
* @param f the file path
* @param permission FsPermission to set
* @throws various exceptions on permission or I/O errors
*/","* Set permission of a path.
   * @param f the path.
   * @param permission - the new absolute permission (umask is not applied)
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code>
   *         is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setOwner,"org.apache.hadoop.fs.FileContext:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1117,1134,"/**
* Sets owner on a file system entity.
* @param f file path
* @param username user name (or null for no change)
* @param groupname group name (or null for no change)
*/","* Set owner of a path (i.e. a file or a directory). The parameters username
   * and groupname cannot both be null.
   * 
   * @param f The path
   * @param username If it is null, the original username remains unchanged.
   * @param groupname If it is null, the original groupname remains unchanged.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws HadoopIllegalArgumentException If <code>username</code> or
   *           <code>groupname</code> is invalid.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setTimes,"org.apache.hadoop.fs.FileContext:setTimes(org.apache.hadoop.fs.Path,long,long)",1158,1170,"/**
* Sets file times (mtime and atime) for a given file.
* @param f Path to the file
* @param mtime last modification time in milliseconds
* @param atime last access time in milliseconds
*/","* Set access time of a file.
   * @param f The path
   * @param mtime Set the modification time of this file.
   *        The number of milliseconds since epoch (Jan 1, 1970). 
   *        A value of -1 means that this call should not set modification time.
   * @param atime Set the access time of this file.
   *        The number of milliseconds since Jan 1, 1970. 
   *        A value of -1 means that this call should not set access time.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileChecksum,org.apache.hadoop.fs.FileContext:getFileChecksum(org.apache.hadoop.fs.Path),1191,1202,"/**
* Resolves file checksum for the given file path.
* @param f absolute or relative file path
* @return FileChecksum object or null if unresolved
*/","* Get the checksum of a file.
   *
   * @param f file path
   *
   * @return The file checksum.  The default return value is null,
   *  which indicates that no checksum algorithm is implemented
   *  in the corresponding FileSystem.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileStatus,org.apache.hadoop.fs.FileContext:getFileStatus(org.apache.hadoop.fs.Path),1248,1258,"/**
* Resolves file status for a given path.
* @param f input path
* @return FileStatus object or null if not found
*/","* Return a file status object that represents the path.
   * @param f The path we want information from
   *
   * @return a FileStatus object
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,access,"org.apache.hadoop.fs.FileContext:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",1305,1318,"/**
* Resolves access permissions for a given file path.
* @param path file system path
* @param mode desired action (read/write/delete)
*/","* Checks if the user can access a path.  The mode specifies which access
   * checks to perform.  If the requested permissions are granted, then the
   * method returns normally.  If access is denied, then the method throws an
   * {@link AccessControlException}.
   * <p>
   * The default implementation of this method calls {@link #getFileStatus(Path)}
   * and checks the returned permissions against the requested permissions.
   * Note that the getFileStatus call will be subject to authorization checks.
   * Typically, this requires search (execute) permissions on each directory in
   * the path's prefix, but this is implementation-defined.  Any file system
   * that provides a richer authorization model (such as ACLs) may override the
   * default implementation so that it checks against that model instead.
   * <p>
   * In general, applications should avoid using this method, due to the risk of
   * time-of-check/time-of-use race conditions.  The permissions on a file may
   * change immediately after the access call returns.  Most applications should
   * prefer running specific file system actions as the desired user represented
   * by a {@link UserGroupInformation}.
   *
   * @param path Path to check
   * @param mode type of access to check
   * @throws AccessControlException if access is denied
   * @throws FileNotFoundException if the path does not exist
   * @throws UnsupportedFileSystemException if file system for <code>path</code>
   *   is not supported
   * @throws IOException see specific implementation
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileLinkStatus,org.apache.hadoop.fs.FileContext:getFileLinkStatus(org.apache.hadoop.fs.Path),1334,1350,"/**
* Resolves file link status for the given path.
* @param f input path to resolve
*/","* Return a file status object that represents the path. If the path 
   * refers to a symlink then the FileStatus of the symlink is returned.
   * The behavior is equivalent to #getFileStatus() if the underlying
   * file system does not support symbolic links.
   * @param  f The path we want information from.
   * @return A FileStatus object
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getLinkTarget,org.apache.hadoop.fs.FileContext:getLinkTarget(org.apache.hadoop.fs.Path),1367,1378,"/**
* Resolves the target of a symbolic link.
* @param f file path to resolve
* @return resolved Path or throws exception if failed
*/","* Returns the target of the given symbolic link as it was specified
   * when the link was created.  Links in the path leading up to the
   * final path component are resolved transparently.
   *
   * @param f the path to return the target of
   * @return The un-interpreted target of the symbolic link.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If path <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If the given path does not refer to a symlink
   *           or an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileBlockLocations,"org.apache.hadoop.fs.FileContext:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",1437,1450,"/**
* Resolves file block locations for a given path and range.
* @param f the input path
* @param start starting offset
* @param len length of the range
* @return array of BlockLocation objects or null if not found
*/","* Return blockLocation of the given file for the given offset and len.
   *  For a nonexistent file or regions, null will be returned.
   *
   * This call is most helpful with DFS, where it returns 
   * hostnames of machines that contain the given file.
   *
   * In HDFS, if file is three-replicated, the returned array contains
   * elements like:
   * <pre>
   * BlockLocation(offset: 0, length: BLOCK_SIZE,
   *   hosts: {""host1:9866"", ""host2:9866, host3:9866""})
   * BlockLocation(offset: BLOCK_SIZE, length: BLOCK_SIZE,
   *   hosts: {""host2:9866"", ""host3:9866, host4:9866""})
   * </pre>
   *
   * And if a file is erasure-coded, the returned BlockLocation are logical
   * block groups.
   *
   * Suppose we have a RS_3_2 coded file (3 data units and 2 parity units).
   * 1. If the file size is less than one stripe size, say 2 * CELL_SIZE, then
   * there will be one BlockLocation returned, with 0 offset, actual file size
   * and 4 hosts (2 data blocks and 2 parity blocks) hosting the actual blocks.
   * 3. If the file size is less than one group size but greater than one
   * stripe size, then there will be one BlockLocation returned, with 0 offset,
   * actual file size with 5 hosts (3 data blocks and 2 parity blocks) hosting
   * the actual blocks.
   * 4. If the file size is greater than one group size, 3 * BLOCK_SIZE + 123
   * for example, then the result will be like:
   * <pre>
   * BlockLocation(offset: 0, length: 3 * BLOCK_SIZE, hosts: {""host1:9866"",
   *   ""host2:9866"",""host3:9866"",""host4:9866"",""host5:9866""})
   * BlockLocation(offset: 3 * BLOCK_SIZE, length: 123, hosts: {""host1:9866"",
   *   ""host4:9866"", ""host5:9866""})
   * </pre>
   *
   * @param f - get blocklocations of this file
   * @param start position (byte offset)
   * @param len (in bytes)
   *
   * @return block locations for given file at specified offset of len
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is invalid",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFsStatus,org.apache.hadoop.fs.FileContext:getFsStatus(org.apache.hadoop.fs.Path),1476,1489,"/**
* Resolves file system status for the given path.
* @param f file system path
* @return FsStatus object or default FS status if null input
*/","* Returns a status object describing the use and capacity of the
   * file system denoted by the Parh argument p.
   * If the file system has multiple partitions, the
   * use and capacity of the partition pointed to by the specified
   * path is reflected.
   * 
   * @param f Path for which status should be obtained. null means the
   * root partition of the default file system. 
   *
   * @return a FsStatus object
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createSymlink,"org.apache.hadoop.fs.FileContext:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1570,1588,"/**
* Creates a symbolic link to the target file.
* @param target target file path
* @param link symbolic link path
* @param createParent whether to create parent directory if needed
*/","* Creates a symbolic link to an existing file. An exception is thrown if 
   * the symlink exits, the user does not have permission to create symlink,
   * or the underlying file system does not support symlinks.
   * 
   * Symlink permissions are ignored, access to a symlink is determined by
   * the permissions of the symlink target.
   * 
   * Symlinks in paths leading up to the final path component are resolved 
   * transparently. If the final path component refers to a symlink some 
   * functions operate on the symlink itself, these are:
   * - delete(f) and deleteOnExit(f) - Deletes the symlink.
   * - rename(src, dst) - If src refers to a symlink, the symlink is 
   *   renamed. If dst refers to a symlink, the symlink is over-written.
   * - getLinkTarget(f) - Returns the target of the symlink. 
   * - getFileLinkStatus(f) - Returns a FileStatus object describing
   *   the symlink.
   * Some functions, create() and mkdir(), expect the final path component
   * does not exist. If they are given a path that refers to a symlink that 
   * does exist they behave as if the path referred to an existing file or 
   * directory. All other functions fully resolve, ie follow, the symlink. 
   * These are: open, setReplication, setOwner, setTimes, setWorkingDirectory,
   * setPermission, getFileChecksum, setVerifyChecksum, getFileBlockLocations,
   * getFsStatus, getFileStatus, exists, and listStatus.
   * 
   * Symlink targets are stored as given to createSymlink, assuming the 
   * underlying file system is capable of storing a fully qualified URI.
   * Dangling symlinks are permitted. FileContext supports four types of 
   * symlink targets, and resolves them as follows
   * <pre>
   * Given a path referring to a symlink of form:
   * 
   *   {@literal <---}X{@literal --->}
   *   fs://host/A/B/link 
   *   {@literal <-----}Y{@literal ----->}
   * 
   * In this path X is the scheme and authority that identify the file system,
   * and Y is the path leading up to the final path component ""link"". If Y is
   * a symlink  itself then let Y' be the target of Y and X' be the scheme and
   * authority of Y'. Symlink targets may:
   * 
   * 1. Fully qualified URIs
   * 
   * fs://hostX/A/B/file  Resolved according to the target file system.
   * 
   * 2. Partially qualified URIs (eg scheme but no host)
   * 
   * fs:///A/B/file  Resolved according to the target file system. Eg resolving
   *                 a symlink to hdfs:///A results in an exception because
   *                 HDFS URIs must be fully qualified, while a symlink to 
   *                 file:///A will not since Hadoop's local file systems 
   *                 require partially qualified URIs.
   * 
   * 3. Relative paths
   * 
   * path  Resolves to [Y'][path]. Eg if Y resolves to hdfs://host/A and path 
   *       is ""../B/file"" then [Y'][path] is hdfs://host/B/file
   * 
   * 4. Absolute paths
   * 
   * path  Resolves to [X'][path]. Eg if Y resolves hdfs://host/A/B and path
   *       is ""/file"" then [X][path] is hdfs://host/file
   * </pre>
   * 
   * @param target the target of the symbolic link
   * @param link the path to be created that points to target
   * @param createParent if true then missing parent dirs are created if 
   *                     false then parent must exist
   *
   *
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If file <code>link</code> already exists
   * @throws FileNotFoundException If <code>target</code> does not exist
   * @throws ParentNotDirectoryException If parent of <code>link</code> is not a
   *           directory.
   * @throws UnsupportedFileSystemException If file system for 
   *           <code>target</code> or <code>link</code> is not supported
   * @throws IOException If an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,org.apache.hadoop.fs.FileContext:listStatus(org.apache.hadoop.fs.Path),1611,1623,"/**
* Resolves file status iterator for the given path.
* @param f input path
*/","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   * 
   * @param f is the path
   *
   * @return an iterator that traverses statuses of the files/directories 
   *         in the given path
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listCorruptFileBlocks,org.apache.hadoop.fs.FileContext:listCorruptFileBlocks(org.apache.hadoop.fs.Path),1633,1644,"/**
* Resolves and lists corrupt file blocks for a given path.
* @param path filesystem path
* @return iterator over corrupt file block paths or null if not found
*/","* List CorruptFile Blocks.
   *
   * @param path the path.
   * @return an iterator over the corrupt files under the given path
   * (may contain duplicates if a file has more than one corrupt block)
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listLocatedStatus,org.apache.hadoop.fs.FileContext:listLocatedStatus(org.apache.hadoop.fs.Path),1673,1686,"/**
* Resolves located file status iterator for the given file.
* @param f file path
* @return RemoteIterator of LocatedFileStatus objects or null if not found
*/","* List the statuses of the files/directories in the given path if the path is
   * a directory. 
   * Return the file's status and block locations If the path is a file.
   * 
   * If a returned status is a file, it contains the file's block locations.
   *
   * @param f is the path
   *
   * @return an iterator that traverses statuses of the files/directories 
   *         in the given path
   * If any IO exception (for example the input directory gets deleted while
   * listing is being executed), next() or hasNext() of the returned iterator
   * may throw a RuntimeException with the io exception as the cause.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,resolveAbstractFileSystems,org.apache.hadoop.fs.FileContext:resolveAbstractFileSystems(org.apache.hadoop.fs.Path),2372,2386,"/**
* Resolves abstract file systems for a given path.
* @param f input path
* @return set of resolved AbstractFileSystem instances or empty set if none found
*/","* Returns the list of AbstractFileSystems accessed in the path. The list may
   * contain more than one AbstractFileSystems objects in case of symlinks.
   * 
   * @param f
   *          Path which needs to be resolved
   * @return List of AbstractFileSystems accessed in the path
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,modifyAclEntries,"org.apache.hadoop.fs.FileContext:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",2456,2467,"/**
* Modifies ACL entries for the specified file system path.
* @param path absolute file system path
* @param aclSpec list of ACL entries to apply
*/","* Modifies ACL entries of files and directories.  This method can add new ACL
   * entries or modify the permissions on existing ACL entries.  All existing
   * ACL entries that are not specified in this call are retained without
   * changes.  (Modifications are merged into the current ACL.)
   *
   * @param path Path to modify
   * @param aclSpec List{@literal <}AclEntry{@literal >} describing
   * modifications
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeAclEntries,"org.apache.hadoop.fs.FileContext:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",2478,2489,"/**
* Removes ACL entries from the specified file system path.
* @param path absolute file system path
* @param aclSpec list of ACL entries to remove
*/","* Removes ACL entries from files and directories.  Other ACL entries are
   * retained.
   *
   * @param path Path to modify
   * @param aclSpec List{@literal <}AclEntry{@literal >} describing entries
   * to remove
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeDefaultAcl,org.apache.hadoop.fs.FileContext:removeDefaultAcl(org.apache.hadoop.fs.Path),2497,2508,"/**
* Removes default ACLs from the specified file system path.
* @param path absolute or relative file system path
*/","* Removes all default ACL entries from files and directories.
   *
   * @param path Path to modify
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeAcl,org.apache.hadoop.fs.FileContext:removeAcl(org.apache.hadoop.fs.Path),2518,2528,"/**
* Removes ACLs from the specified file system path.
* @param path absolute or relative file system path
*/","* Removes all but the base ACL entries of files and directories.  The entries
   * for user, group, and others are retained for compatibility with permission
   * bits.
   *
   * @param path Path to modify
   * @throws IOException if an ACL could not be removed",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setAcl,"org.apache.hadoop.fs.FileContext:setAcl(org.apache.hadoop.fs.Path,java.util.List)",2540,2551,"/**
* Sets ACL entries for a file system path.
* @param path absolute file system path
* @param aclSpec list of ACL entries to apply
*/","* Fully replaces ACL of files and directories, discarding all existing
   * entries.
   *
   * @param path Path to modify
   * @param aclSpec List{@literal <}AclEntry{@literal >} describing
   * modifications, must include entries for user, group, and others for
   * compatibility with permission bits.
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAclStatus,org.apache.hadoop.fs.FileContext:getAclStatus(org.apache.hadoop.fs.Path),2561,2570,"/**
* Resolves ACL status for a given file path.
* @param path absolute or relative file path
* @return AclStatus object representing the access control list status
*/","* Gets the ACLs of files and directories.
   *
   * @param path Path to get
   * @return RemoteIterator{@literal <}AclStatus{@literal >} which returns
   *         each AclStatus
   * @throws IOException if an ACL could not be read",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setXAttr,"org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",2603,2614,"/**
* Sets an extended attribute on a file or directory.
* @param path filesystem path
* @param name attribute name
* @param value attribute value bytes
* @param flag set flags (e.g. XATTR_CREATE, XATTR_REPLACE)
*/","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @param flag xattr set flag
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getXAttr,"org.apache.hadoop.fs.FileContext:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",2628,2637,"/**
* Retrieves extended attribute value for the given path and name.
* @param path file system path
* @param name name of the extended attribute
* @return byte array containing the extended attribute value or null if not found
*/","* Get an xattr for a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attribute
   * @param name xattr name.
   * @return byte[] xattr value.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getXAttrs,org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path),2651,2660,"/**
* Resolves and fetches extended attributes for a given file path.
* @param path the file system path
*/","* Get all of the xattrs for a file or directory.
   * Only those xattrs for which the logged-in user has permissions to view
   * are returned.
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attributes
   * @return Map{@literal <}String, byte[]{@literal >} describing the XAttrs
   * of the file or directory
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getXAttrs,"org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",2675,2685,"/**
* Retrieves extended attributes for a file or directory.
* @param path absolute file system path
* @param names list of attribute names to fetch
* @return Map of attribute name to byte array value, or null if not found
*/","* Get all of the xattrs for a file or directory.
   * Only those xattrs for which the logged-in user has permissions to view
   * are returned.
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attributes
   * @param names XAttr names.
   * @return Map{@literal <}String, byte[]{@literal >} describing the XAttrs
   * of the file or directory
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeXAttr,"org.apache.hadoop.fs.FileContext:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",2698,2708,"/**
* Removes extended attribute from the given file system path.
* @param path file system path
* @param name name of the extended attribute to remove
*/","* Remove an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to remove extended attribute
   * @param name xattr name
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listXAttrs,org.apache.hadoop.fs.FileContext:listXAttrs(org.apache.hadoop.fs.Path),2722,2731,"/**
* Lists extended attributes for a given file system path.
* @param path file system path to list XAttrs from
* @return list of attribute names or empty list if none found
*/","* Get all of the xattr names for a file or directory.
   * Only those xattr names which the logged-in user has permissions to view
   * are returned.
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attributes
   * @return List{@literal <}String{@literal >} of the XAttr names of the
   * file or directory
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createSnapshot,"org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",2766,2777,"/**
* Creates a snapshot of the given file system location.
* @param path absolute file system location
* @param snapshotName name for the new snapshot
* @return Path to the created snapshot or null if failed
*/","* Create a snapshot.
   *
   * @param path The directory where snapshots will be taken.
   * @param snapshotName The name of the snapshot
   * @return the snapshot path.
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,renameSnapshot,"org.apache.hadoop.fs.FileContext:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",2794,2805,"/**
* Renames a Hadoop snapshot by ID.
* @param path absolute file system path
* @param snapshotOldName original name of the snapshot
* @param snapshotNewName new name for the snapshot
*/","* Rename a snapshot.
   *
   * @param path The directory path where the snapshot was taken
   * @param snapshotOldName Old name of the snapshot
   * @param snapshotNewName New name of the snapshot
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,deleteSnapshot,"org.apache.hadoop.fs.FileContext:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",2821,2832,"/**
* Deletes a named snapshot from the file system.
* @param path absolute file system path
* @param snapshotName name of the snapshot to delete
*/","* Delete a snapshot of a directory.
   *
   * @param path The directory that the to-be-deleted snapshot belongs to
   * @param snapshotName The name of the snapshot
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,satisfyStoragePolicy,org.apache.hadoop.fs.FileContext:satisfyStoragePolicy(org.apache.hadoop.fs.Path),2839,2850,"/**
* Applies storage policy to the specified file system path.
* @param path absolute or relative path to apply policy to
*/","* Set the source path to satisfy storage policy.
   * @param path The source path referring to either a directory or a file.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setStoragePolicy,"org.apache.hadoop.fs.FileContext:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",2861,2872,"/**
* Sets storage policy for a file system location.
* @param path absolute file system path
* @param policyName name of the storage policy to set
*/","* Set the storage policy for a given file or directory.
   *
   * @param path file or directory path.
   * @param policyName the name of the target storage policy. The list
   *                   of supported Storage policies can be retrieved
   *                   via {@link #getAllStoragePolicies}.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,unsetStoragePolicy,org.apache.hadoop.fs.FileContext:unsetStoragePolicy(org.apache.hadoop.fs.Path),2879,2889,"/**
* Unsets storage policy for the given file system path.
* @param src file system path to unset policy from
*/","* Unset the storage policy set for a given file or directory.
   * @param src file or directory path.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getStoragePolicy,org.apache.hadoop.fs.FileContext:getStoragePolicy(org.apache.hadoop.fs.Path),2898,2908,"/**
* Resolves BlockStoragePolicySpi instance for a given file system path.
* @param path absolute file system path
* @return BlockStoragePolicySpi object or null if not found
*/","* Query the effective storage policy ID for the given file or directory.
   *
   * @param path file or directory path.
   * @return storage policy for give file.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,hasPathCapability,"org.apache.hadoop.fs.FileContext:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",3001,3007,"/**
* Checks if a file system path has a specific capability.
* @param path the file system path to check
* @param capability the capability to look for
* @return true if the path has the capability, false otherwise
*/","* Return the path capabilities of the bonded {@code AbstractFileSystem}.
   * @param path path to query the capability of.
   * @param capability string to query the stream support for.
   * @return true iff the capability is supported under that FS.
   * @throws IOException path resolution or other IO failure
   * @throws IllegalArgumentException invalid arguments",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getServerDefaults,org.apache.hadoop.fs.FileContext:getServerDefaults(org.apache.hadoop.fs.Path),3015,3020,"/**
* Resolves server defaults for a given file system location.
* @param path absolute or relative file system path
* @return FsServerDefaults object or null if not found
*/","* Return a set of server default configuration values based on path.
   * @param path path to fetch server defaults
   * @return server default configuration values for path
   * @throws IOException an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createMultipartUploader,org.apache.hadoop.fs.FileContext:createMultipartUploader(org.apache.hadoop.fs.Path),3029,3035,"/**
* Creates a multipart uploader with the specified base path.
* @param basePath absolute file system path for uploading
*/","* Create a multipart uploader.
   * @param basePath file path under which all files are uploaded
   * @return a MultipartUploaderBuilder object to build the uploader
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.HarFileSystem:getInitialWorkingDirectory(),278,281,"/**
* Returns the initial working directory based on the current working directory.
* @return Path object representing the initial working directory
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getWorkingDirectory(com.jcraft.jsch.ChannelSftp),652,655,"/**
* Returns the user's home directory as the working directory.
* @param client SFTP channel instance
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,<init>,org.apache.hadoop.fs.RawLocalFileSystem:<init>(),101,103,"/**
* Initializes RawLocalFileSystem with initial working directory.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,refreshStatus,org.apache.hadoop.fs.shell.PathData:refreshStatus(),198,208,"/**
* Retrieves file status by performing a lookup on the filesystem.
* @return FileStatus object representing the current state of the file; null if not found
*/","* Updates the paths's file status
   * @return the updated FileStatus
   * @throws IOException if anything goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getUsed,org.apache.hadoop.fs.HarFileSystem:getUsed(),1271,1274,"/**
* Retrieves the used file system space.
* @return Used file system space in bytes
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getUsed,org.apache.hadoop.fs.FilterFileSystem:getUsed(),415,418,"/**
* Retrieves the used disk space from the file system.
* @return Used disk space in bytes or -1 if an error occurs
*/",Return the total size of all files in the filesystem.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,resolve,"org.apache.hadoop.fs.viewfs.RegexMountPoint:resolve(java.lang.String,boolean)",168,205,"/**
* Resolves a source path using regex-based mount points.
* @param srcPath source path to resolve
* @param resolveLastComponent whether to resolve the last component
*/","* Get resolved path from regex mount points.
   *  E.g. link: ^/user/(?<username>\\w+) => s3://$user.apache.com/_${user}
   *  srcPath: is /user/hadoop/dir1
   *  resolveLastComponent: true
   *  then return value is s3://hadoop.apache.com/_hadoop
   * @param srcPath - the src path to resolve
   * @param resolveLastComponent - whether resolve the path after last `/`
   * @return mapped path of the mount point.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,checkDest,"org.apache.hadoop.fs.FileUtil:checkDest(java.lang.String,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",607,630,"/**
* Verifies destination path for file copy or move operation.
* @param srcName source filename (null for directory copy)
* @param dstFS destination file system
* @param dst target path
* @param overwrite whether to overwrite existing files
* @return verified Path object
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,advance,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:advance(),561,569,"/**
* Advances through directory paths, returning the first existing one.
* @throws IOException on file system access errors
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,ifExists,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)",615,636,"/**
* Checks if a file or directory exists at the specified path.
* @param pathStr absolute file/directory path
* @param conf Configuration object
* @return true if found, false otherwise
*/","We search through all the configured dirs for the file's existence
     *  and return true when we find one",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,delete,"org.apache.hadoop.io.MapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)",913,921,"/**
* Deletes a file system directory and its contents.
* @param fs FileSystem object to delete from
* @param name unique directory identifier
*/","* Deletes the named map file.
   * @param fs input fs.
   * @param name input name.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,delete,"org.apache.hadoop.io.BloomMapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)",59,69,"/**
* Deletes a user's data and index files from the file system.
* @param fs FileSystem object
* @param name unique user identifier
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,findCurrentDirectory,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:findCurrentDirectory(java.util.Date),551,558,"/**
* Calculates the current directory based on the last flush time and roll interval.
* @param now current date
*/","* Use the given time to determine the current directory. The current
   * directory will be based on the {@link #rollIntervalMinutes}.
   *
   * @param now the current time
   * @return the current directory",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,createForWrite,"org.apache.hadoop.io.SecureIOUtils:createForWrite(java.io.File,int)",273,280,"/**
* Creates a file output stream for writing with specified permissions.
* @param f the target file
* @param permissions OS-specific file creation flags
* @return FileOutputStream object or null if security check fails
*/","* Open the specified File for write access, ensuring that it does not exist.
   * @param f the file that we want to create
   * @param permissions we want to have on the file (if security is enabled)
   *
   * @throws AlreadyExistsException if the file already exists
   * @throws IOException if any other error occurred
   * @return createForWrite FileOutputStream.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,initFileSystem,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:initFileSystem(java.net.URI),165,171,"/**
* Initializes the file system with a provided key store URI.
* @param keystoreUri unique identifier for the key store location
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,extractKMSPath,org.apache.hadoop.crypto.key.kms.KMSClientProvider:extractKMSPath(java.net.URI),443,445,"/**
* Extracts KMS path from URI.
* @param uri input URI
* @return extracted KMS path or null if not found
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,checkPathsForReservedRaw,"org.apache.hadoop.fs.shell.CommandWithDestination:checkPathsForReservedRaw(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",385,406,"/**
* Checks whether source and target paths are in the reserved raw file system.
* @param src source path
* @param target target path
* @return true if both paths are in reserved raw, false otherwise
*/","* Check the source and target paths to ensure that they are either both in
   * /.reserved/raw or neither in /.reserved/raw. If neither src nor target are
   * in /.reserved/raw, then return false, indicating not to preserve raw.*
   * xattrs. If both src/target are in /.reserved/raw, then return true,
   * indicating raw.* xattrs should be preserved. If only one of src/target is
   * in /.reserved/raw then throw an exception.
   *
   * @param src The source path to check. This should be a fully-qualified
   *            path, not relative.
   * @param target The target path to check. This should be a fully-qualified
   *               path, not relative.
   * @return true if raw.* xattrs should be preserved.
   * @throws PathOperationException is only one of src/target are in
   * /.reserved/raw.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createInternal,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",997,1042,"/**
* Creates a new internal file or directory.
* @param f the path to create
* @param flag creation flags
* @param absolutePermission permissions for the created item
* @throws various exceptions on failure
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",1051,1071,"/**
* Resolves block locations for a file within the filesystem.
* @param f Path to the file
* @param start starting offset
* @param len length of data to retrieve
* @return BlockLocation array or throws exceptions if invalid
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,mkdir,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",1267,1299,"/**
* Creates a directory with specified permissions and creates parent if necessary.
* @param dir Path to the directory
* @param permission FsPermission for the new directory
* @param createParent whether to create parent directories if they do not exist
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,create,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1458,1496,"/**
* Creates a new FSDataOutputStream for the specified file.
* @param f Path to the file
* @return FSDataOutputStream object or throws an exception if creation fails
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listStatusForFallbackLink,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatusForFallbackLink(),1638,1657,"/**
* Lists file statuses for the fallback link's target filesystem.
* @return array of FileStatus objects or empty array if not found
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1696,1730,"/**
* Creates a directory with specified permissions or throws an exception if it already exists.
* @param dir Path to the directory to create
* @param permission FsPermission for the new directory
* @return true if creation was successful, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,makeTrashRelativePath,"org.apache.hadoop.fs.TrashPolicyDefault:makeTrashRelativePath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",120,122,"/**
* Merges base path with trash file path to create relative path.
* @param basePath absolute directory where trash is located
* @param rmFilePath path of the file in the trash
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,innerPutPart,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerPutPart(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long)",124,153,"/**
* Creates a new part in the upload with specified metadata and data.
* @param filePath path to the collector directory
* @param inputStream stream containing part data
* @param partNumber unique part identifier
* @param uploadId unique upload handle
* @param lengthInBytes total size of the part in bytes
* @return PartHandle object representing the created part
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getParent,org.apache.hadoop.fs.Path:getParent(),429,431,"/**
* Returns the parent directory of this file.
*/","* Returns the parent of a path or null if at root. Better alternative is
   * {@link #getOptionalParentPath()} to handle nullable value for root path.
   *
   * @return the parent of a path or null if at root",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getOptionalParentPath,org.apache.hadoop.fs.Path:getOptionalParentPath(),440,442,"/**
* Returns the parent directory as an Optional Path.
*/","* Returns the parent of a path as {@link Optional} or
   * {@link Optional#empty()} i.e an empty Optional if at root.
   *
   * @return Parent of path wrappen in {@link Optional}.
   * {@link Optional#empty()} i.e an empty Optional if at root.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getDirectoryContentsIterator,org.apache.hadoop.fs.shell.PathData:getDirectoryContentsIterator(),293,299,"/**
* Returns an iterator over directory contents.
* @return Iterator of PathData objects for each item in the directory
*/","* Returns a RemoteIterator for PathData objects of the items contained in the
   * given directory.
   * @return remote iterator of PathData objects for its children
   * @throws IOException if anything else goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,schemeFromPath,org.apache.hadoop.fs.Globber:schemeFromPath(org.apache.hadoop.fs.Path),171,182,"/**
* Extracts the URI scheme from a given file system path.
* @param path filesystem path
* @return URI scheme (e.g., ""file"", ""http"") or null if unknown
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,authorityFromPath,org.apache.hadoop.fs.Globber:authorityFromPath(org.apache.hadoop.fs.Path),184,195,"/**
* Extracts the authority from a file path.
* @param path file path
* @return authority string or null if not found
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,<init>,"org.apache.hadoop.fs.FSDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",111,122,"/**
* Initializes FSDataOutputStreamBuilder with file context and path.
* @param fc FileContext instance
* @param p Path to the file
*/","* Construct from a {@link FileContext}.
   *
   * @param fc FileContext
   * @param p path.
   * @throws IOException failure",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setVerifyChecksum,"org.apache.hadoop.fs.FileContext:setVerifyChecksum(boolean,org.apache.hadoop.fs.Path)",1223,1228,"/**
* Sets verification of checksum for a file.
* @param verifyChecksum true to enable checksum verification
* @param f file path to configure checksum verification for
*/","* Set the verify checksum flag for the  file system denoted by the path.
   * This is only applicable if the 
   * corresponding FileSystem supports checksum. By default doesn't do anything.
   * @param verifyChecksum verify check sum.
   * @param f set the verifyChecksum for the Filesystem containing this path
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,readFields,org.apache.hadoop.fs.FileStatus:readFields(java.io.DataInput),496,522,"/**
* Reads FileStatusProto from input stream and populates this object's fields.
* @throws IOException if invalid size or data is encountered
*/","* Read instance encoded as protobuf from stream.
   * @param in Input stream
   * @see PBHelper#convert(FileStatus)
   * @deprecated Use the {@link PBHelper} and protobuf serialization directly.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",140,149,"/**
* Constructs a FileStatus object with the given attributes.
* @param length file size in bytes
* @param isdir true if directory, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean,org.apache.hadoop.fs.BlockLocation[])",113,123,"/**
* Constructs a LocatedFileStatus object from its components.
* @param length file length
* @param isdir whether the file is a directory
* @param block_replication replication factor for blocks
* @param blocksize size of each block
* @param modification_time last modification time
* @param access_time last access time
* @param permission file permissions
* @param owner file owner
* @param group file group
* @param symlink symbolic link target path
* @param path actual file path
* @param hasAcl whether ACLs are present
* @param isEncrypted whether the file is encrypted
* @param isErasureCoded whether the file uses erasure coding
* @param locations block locations","* Constructor.
   *
   * @param length a file's length
   * @param isdir if the path is a directory
   * @param block_replication the file's replication factor
   * @param blocksize a file's block size
   * @param modification_time a file's modification time
   * @param access_time a file's access time
   * @param permission a file's permission
   * @param owner a file's owner
   * @param group a file's group
   * @param symlink symlink if the path is a symbolic link
   * @param path the path's qualified name
   * @param hasAcl entity has associated ACLs
   * @param isEncrypted entity is encrypted
   * @param isErasureCoded entity is erasure coded
   * @param locations a file's block locations",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,<init>,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:<init>(org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder,org.apache.hadoop.fs.FileSystem)",87,96,"/**
* Initializes a new instance of the uploader with specified settings.
* @param builder configuration builder for file system multipart uploader
* @param fs underlying file system instance
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,append,"org.apache.hadoop.io.MapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",399,416,"/**
* Appends a key-value pair to the underlying data store.
* @param key unique key for the value
* @param val associated value
*/","* Append a key/value pair to the map.  The key must be greater or equal
     * to the previous key added to the map.
     *
     * @param key key.
     * @param val value.
     * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,close,org.apache.hadoop.service.AbstractService:close(),246,249,"/**
 * Closes the resource by stopping its underlying operation.
 */","* Relay to {@link #stop()}
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Metadata:equals(java.lang.Object),785,795,"/**
* Compares two Metadata objects for equality.
* @param other the object to compare with
* @return true if equal, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,equals,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:equals(java.lang.Object),100,108,"/**
* Compares this RenewAction instance with another object for equality.
* @param that the object to compare with
* @return true if objects are equal, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,equals,org.apache.hadoop.security.token.Token$PrivateToken:equals(java.lang.Object),284,297,"/**
* Compares this PrivateToken object with another for equality.
* @param o object to compare with
* @return true if objects have same public service, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,selectDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials),998,1006,"/**
* Selects a delegationToken for the given credentials.
* @param creds Credentials object
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BasicDiskValidator.java,checkStatus,org.apache.hadoop.util.BasicDiskValidator:checkStatus(java.io.File),30,33,"/**
 * Verifies the status of a directory using a disk checker.
 * @param dir the directory to check (must not be null)
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,write,org.apache.hadoop.fs.FSOutputSummer:write(int),76,82,"/**
* Writes a single byte to the output buffer.
* @throws IOException on write failure
*/",Write one byte,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,write1,"org.apache.hadoop.fs.FSOutputSummer:write1(byte[],int,int)",120,140,"/**
* Writes user data to the local buffer and underlying stream.
* @param b user data byte array
* @param off offset of user data in the array
* @param len length of user data
* @return number of bytes written
*/","* Write a portion of an array, flushing to the underlying
   * stream at most once if necessary.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:performCoding(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",79,121,"/**
* Reconstructs corrupted data by performing coding on input and output buffers.
* @param inputs array of ByteBuffer objects representing input data
* @param outputs array of ByteBuffer objects representing output data
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DecodingValidator.java,validate,"org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])",138,143,"/**
* Validates input and output arrays of ECChunks.
* @param inputs array of encoded chunks to be validated
* @param erasedIndexes array of indexes that have been erased
* @param outputs array of expected output chunks
*/","*  Validate outputs decoded from inputs, by decoding an input back from
   *  those outputs and comparing it with the original one.
   * @param inputs input buffers used for decoding
   * @param erasedIndexes indexes of erased units used for decoding
   * @param outputs decoded output buffers
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",54,58,"/**
* Performs coding operation using raw decoder.
* @param inputChunks array of input chunks to process
* @param outputChunks array to store processed output chunks
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupRandPartA,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartA(),1077,1104,"/**
* Processes the current block and transitions to Rand Part B state.
* @throws IOException if an I/O error occurs
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupNoRandPartA,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartA(),1106,1126,"/**
* Updates state and CRC for No-Rand Part A.
* @throws IOException on I/O error
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,finish,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:finish(),718,732,"/**
* Closes and writes any remaining compressed data to the output stream.
* @throws IOException if an I/O error occurs during closing or writing
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,write0,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write0(int),881,900,"/**
* Writes a single byte to the output stream.
* @param b single byte value
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,finishDataBlock,org.apache.hadoop.io.file.tfile.TFile$Writer:finishDataBlock(boolean),653,671,"/**
* Closes and compresses a data block if it exceeds size threshold or forceFinish is true.
* @param bForceFinish flag to force closing the block regardless of size
*/","* Close the current data block if necessary.
     * 
     * @param bForceFinish
     *          Force the closure regardless of the block size.
     * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:close(),3549,3556,"/**
* Closes and cleans up all input segments.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,parkCursorAtEnd,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:parkCursorAtEnd(),1561,1571,"/**
* Resets cursor position to the end of the data and closes the block reader. 
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readTokenStorageStream,org.apache.hadoop.security.Credentials:readTokenStorageStream(java.io.DataInputStream),273,295,"/**
* Reads token storage stream, validating header and parsing data.
*@param in DataInputStream containing the stream to parse
*/","* Convenience method for reading a token from a DataInputStream.
   *
   * @param in DataInputStream.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,getCandidateTokensForCleanup,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getCandidateTokensForCleanup(),176,197,"/**
* Retrieves tokens for cleanup based on the last token renewal period.
* @return Map of TokenIdent to DelegationTokenInformation, or an empty map if failed
*/","* Obtain a list of tokens that will be considered for cleanup, based on the last
   * time the token was updated in SQL. This list may include tokens that are not
   * expired and should not be deleted (e.g. if the token was last renewed using a
   * higher renewal interval).
   * The number of results is limited to reduce performance impact. Some level of
   * contention is expected when multiple routers run cleanup simultaneously.
   * @return Map of tokens that have not been updated in SQL after the token renewal
   *         period.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfoFromZK,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)",644,650,"/**
* Retrieves DelegationTokenInformation from ZooKeeper by token ID.
* @param ident unique token identifier
* @param quiet whether to suppress output on failure
* @return DelegationTokenInformation object or null if not found
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawValue,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawValue(org.apache.hadoop.io.SequenceFile$ValueBytes),3866,3869,"/**
* Reads and returns the length of the next raw value.
* @param rawValue ValueBytes object to store the result
* @return Length of the next raw value, or -1 if an error occurs
*/","* Fills up the passed rawValue with the value corresponding to the key
       * read earlier.
       * @param rawValue input ValueBytes rawValue.
       * @return the length of the value
       * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,removeExpiredStoredToken,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeExpiredStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),213,229,"/**
* Removes expired stored token.
* @param ident Token identifier
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,readFields,org.apache.hadoop.fs.FsServerDefaults:readFields(java.io.DataInput),175,185,"/**
* Reads and populates an object's fields from a binary input stream.
* @param in the DataInput stream containing the serialized data
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeProto,org.apache.hadoop.security.Credentials:writeProto(java.io.DataOutput),378,397,"/**
* Serializes credentials data to a Protobuf stream.
* @param out output stream
*/","* Write contents of this instance as CredentialsProto message to DataOutput.
   * @param out
   * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,protoFromToken,org.apache.hadoop.ipc.ProtobufHelper:protoFromToken(org.apache.hadoop.security.token.Token),128,130,"/**
* Converts a token to protobuf representation.
* @param tok token object to convert
*/","* Create a {@code TokenProto} instance
   * from a hadoop token.
   * This builds and caches the fields
   * (identifier, password, kind, service) but not
   * renewer or any payload.
   * @param tok token
   * @return a marshallable protobuf class.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenIdentifier.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text),37,39,"/**
* Creates a new DelegationTokenIdentifier instance with specified token kind. 
* @param kind the type of token being delegated (e.g., Text)",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invoke,"org.apache.hadoop.io.retry.RetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",358,374,"/**
* Invokes the given method on the proxy object, handling RPC and synchronous calls.
* @param proxy the proxy object to invoke on
* @param method the method to invoke
* @param args the arguments for the invocation
* @return the return value of the invocation or null for async calls
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,entry,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:entry(),1619,1622,"/**
* Returns an instance of Entry.
* 
* @throws IOException if key is invalid
*/","* Get an entry to access the key and value.
       * 
       * @return The Entry object to access the key and value.
       * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareCursorKeyTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:compareCursorKeyTo(org.apache.hadoop.io.file.tfile.RawComparable),1642,1646,"/**
* Compares cursor key with another RawComparable object.
* @param other the object to compare with
* @return a negative/positive value if the keys differ or 0 if they are equal
*/","* Internal API. Comparing the key at cursor to user-specified key.
       * 
       * @param other
       *          user-specified key.
       * @return negative if key at cursor is smaller than user key; 0 if equal;
       *         and positive if key at cursor greater than user key.
       * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,get,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:get(org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable)",1675,1679,"/**
* Retrieves and sets key and value writable objects.
* @param key BytesWritable object to store key
* @param value BytesWritable object to store value
*/","* Copy the key and value in one shot into BytesWritables. This is
         * equivalent to getKey(key); getValue(value);
         * 
         * @param key
         *          BytesWritable to hold key.
         * @param value
         *          BytesWritable to hold value
         * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,inBlockAdvance,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(long),1986,1995,"/**
* Advances through a block of records by the specified count.
* @param n number of records to advance
*/","* Advance cursor by n positions within the block.
       * 
       * @param n
       *          Number of key-value pairs to skip in block.
       * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String)",168,172,"/**
* Retrieves delegationToken using the provided parameters.
* @param url URL for authentication
* @param token existing token to reuse
* @param renewer entity that can renew expired tokens
*/","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return abstract delegation token identifier.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)",217,222,"/**
* Renews a delegation token.
* @param url URL to the authentication service
* @param token Authenticated token to renew
* @param dToken Delegation token identifier
*/","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @param dToken abstract delegation token identifier.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)",257,262,"/**
* Cancels a delegated token.
* @param url URL to cancel from
* @param token authenticated token to cancel
* @param dToken delegationToken identifier
*/","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @param dToken abstract delegation token identifier.
   * @throws IOException if an IO error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getInputStream,org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket),470,473,"/**
* Returns a SocketInputWrapper object from the given socket.
* @param socket active network socket
*/","* Same as <code>getInputStream(socket, socket.getSoTimeout()).</code>
   *
   * @param socket socket.
   * @throws IOException raised on errors performing I/O.
   * @return SocketInputWrapper for reading from the socket.
   * @see #getInputStream(Socket, long)",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getOutputStream,org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket),526,529,"/**
* Returns an OutputStream associated with the given Socket.
* @param socket the client socket
*/","* Same as getOutputStream(socket, 0). Timeout of zero implies write will
   * wait until data is available.<br><br>
   * 
   * From documentation for {@link #getOutputStream(Socket, long)} : <br>
   * Returns OutputStream for the socket. If the socket has an associated
   * SocketChannel then it returns a 
   * {@link SocketOutputStream} with the given timeout. If the socket does not
   * have a channel, {@link Socket#getOutputStream()} is returned. In the later
   * case, the timeout argument is ignored and the write will wait until 
   * data is available.<br><br>
   * 
   * Any socket created using socket factories returned by {@link NetUtils},
   * must use this interface instead of {@link Socket#getOutputStream()}.
   * 
   * @see #getOutputStream(Socket, long)
   * 
   * @param socket socket.
   * @return OutputStream for writing to the socket.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,putMetrics,org.apache.hadoop.metrics2.sink.StatsDSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),97,148,"/**
* Processes a MetricsRecord, overriding host name and service name as needed,
* then writes metrics to the underlying system.
* @param record MetricsRecord object containing data to process
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,connect,"org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,int)",574,578,"/**
* Establishes a connection to a server using the given socket and address.
* @param socket client socket
* @param address remote server address
* @param timeout connection timeout in milliseconds
*/","* This is a drop-in replacement for 
   * {@link Socket#connect(SocketAddress, int)}.
   * In the case of normal sockets that don't have associated channels, this 
   * just invokes <code>socket.connect(endpoint, timeout)</code>. If 
   * <code>socket.getChannel()</code> returns a non-null channel,
   * connect is implemented using Hadoop's selectors. This is done mainly
   * to avoid Sun's connect implementation from creating thread-local 
   * selectors, since Hadoop does not have control on when these are closed
   * and could end up taking all the available file descriptors.
   * 
   * @see java.net.Socket#connect(java.net.SocketAddress, int)
   * 
   * @param socket socket.
   * @param address the remote address
   * @param timeout timeout in milliseconds
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,sampleMetrics,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:sampleMetrics(),403,418,"/**
* Creates a sampled metrics buffer by aggregating data from individual sources.
* @return MetricsBuffer object containing aggregated metric data
*/","* Sample all the sources for a snapshot of metrics/tags
   * @return  the metrics buffer containing the snapshot",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getAttribute,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttribute(java.lang.String),104,118,"/**
 * Retrieves the value of a specific attribute by its name.
 * @param attribute unique identifier for the requested attribute
 * @return The attribute's value or null if not found; throws exceptions otherwise
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getAttributes,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttributes(java.lang.String[]),127,141,"/**
* Retrieves a list of attributes by name.
* @param attributes array of attribute names
* @return AttributeList object containing retrieved attributes
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getMBeanInfo,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMBeanInfo(),154,158,"/**
* Retrieves and returns cached JMX information.
* @return MBeanInfo object representing JMX data
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,register,"org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.lang.Object)",71,74,"/**
* Registers an MBean with empty attributes.","* Register the MBean using our standard MBeanName format
   * ""hadoop:service={@literal <serviceName>,name=<nameName>}""
   * Where the {@literal <serviceName> and <nameName>} are the supplied
   * parameters.
   *
   * @param serviceName serviceName.
   * @param nameName nameName.
   * @param theMbean - the MBean to register
   * @return the named used to register the MBean",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stop,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stop(),196,219,"/**
* Stops the metrics system, notifying registered callbacks and performing cleanup.
* @throws MetricsException if stop is called prematurely
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,getMetric,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:getMetric(java.lang.String),86,103,"/**
* Retrieves or initializes ReadWriteDiskValidatorMetrics for a given directory name.
* @param dirName unique directory identifier
* @return Metrics object associated with the directory or null if not found
*/","* Get a metric by given directory name.
   *
   * @param dirName directory name
   * @return the metric",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,init,"org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class,java.lang.String)",190,193,"/**
* Initializes the protocol with the given type prefix and class.
* @param protocol Class of the protocol to initialize
* @param prefix Type prefix for the protocol
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,init,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:init(int),70,80,"/**
* Initializes RPC stats for a specified number of priority levels.
* @param numLevels the number of priority levels to initialize
*/","* Initialize the metrics for JMX with priority levels.
   * @param numLevels input numLevels.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",116,132,"/**
* Aggregates local thread metrics and updates global metrics snapshot.
* @param rb MetricsRecordBuilder to update with aggregated data
* @param all true to include all global metrics in the snapshot
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,collectThreadLocalStates,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:collectThreadLocalStates(),137,143,"/**
* Collects and aggregates thread-local states into global metrics.
*/","* Collects states maintained in {@link ThreadLocal}, if any.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:<init>(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink,java.lang.String,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,int,int,int,float,int)",62,94,"/**
* Initializes a MetricsSinkAdapter with specified configuration.
* @param name unique adapter identifier
* @param description adapter description
* @param sink underlying metrics sink object
* @param context adapter context
* @param sourceFilter filter for source metrics
* @param recordFilter filter for records
* @param metricFilter filter for metrics
* @param periodMs interval between data collection periods
* @param queueCapacity maximum number of metrics in queue
* @param retryDelay initial delay before retrying failed writes
* @param retryBackoff exponential backoff factor for retries
* @param retryCount maximum number of retries",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String),289,291,"/**
* Creates a new rate with the given name.
* @param name unique identifier and display name of the rate
*/","* Create a mutable rate metric
   * @param name  of the metric
   * @return a new mutable metric object",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String)",299,301,"/**
* Creates a new rate with the given name and description.
* @param name rate name
* @param description rate description
*/","* Create a mutable rate metric
   * @param name  of the metric
   * @param description of the metric
   * @return a new mutable rate metric object",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,create,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:create(java.lang.String),60,64,"/**
* Creates and registers a DecayRpcSchedulerDetailedMetrics instance for the specified namespace.
* @param ns namespace name
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,create,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:create(int),66,69,"/**
* Creates and registers an RPC detailed metrics instance on a specified port.
* @param port the port to run the metrics service on
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Handler:run(),3151,3230,"/**
* Runs the server loop, processing calls from the queue and handling exceptions.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dumpKeytab,org.apache.hadoop.security.KDiag:dumpKeytab(java.io.File),596,620,"/**
* Dumps keytab contents, including principal names and entries.
* @param keytabFile file containing keytab data
*/","* Dump a keytab: list all principals.
   *
   * @param keytabFile the keytab file
   * @throws IOException IO problems",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateJAAS,org.apache.hadoop.security.KDiag:validateJAAS(boolean),755,771,"/**
* Validates and displays JAAS configuration details if required.
* @param jaasRequired true to validate JAAS config, false otherwise
*/","* Validate any JAAS entry referenced in the {@link #SUN_SECURITY_JAAS_FILE}
   * property.
   * @param jaasRequired is JAAS required",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateNTPConf,org.apache.hadoop.security.KDiag:validateNTPConf(),773,784,"/**
* Validates and displays NTP configuration on non-Windows platforms.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,getTokenRealOwner,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenRealOwner(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),907,917,"/**
* Retrieves the token's real owner, either from the TokenIdent or its User.
* @param id TokenIdent with potential proxy information
* @return The actual user owning the token as a string
*/","* Return the real owner for a token. If this is a token from a proxy user,
   * the real/effective user will be returned.
   *
   * @param id
   * @return real owner",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,getUgi,org.apache.hadoop.util.ProtoUtil:getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto),124,131,"/**
* Retrieves the UserGroupInformation based on the provided IpcConnectionContextProto.
* @param context Ipc connection context proto
* @return UserGroupInformation object or null if not found
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getUid,org.apache.hadoop.security.ShellBasedIdMapping:getUid(java.lang.String),632,644,"/**
* Retrieves a unique user ID by name.
* @param user username to look up
* @return numeric user ID or throws IOException on failure
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getGid,org.apache.hadoop.security.ShellBasedIdMapping:getGid(java.lang.String),646,658,"/**
* Retrieves the unique Group ID for a given group name.
* @param group group identifier as string
* @return gid integer value or throws IOException if not found
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getUserName,"org.apache.hadoop.security.ShellBasedIdMapping:getUserName(int,java.lang.String)",660,676,"/**
* Retrieves the user name by ID, updating internal maps if necessary.
* @param uid unique user identifier
* @param unknown default user name to use if not found
* @return User name or null if not found (or default name)",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getGroupName,"org.apache.hadoop.security.ShellBasedIdMapping:getGroupName(int,java.lang.String)",678,694,"/**
* Retrieves the group name associated with a given ID.
* @param gid unique group identifier
* @param unknown fallback group name if not found
* @return group name or null if not found (with optional fallback)
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,ensureParentZNode,org.apache.hadoop.ha.ActiveStandbyElector:ensureParentZNode(),360,396,"/**
* Ensures the existence of parent ZNode(s) for working directory.
* @throws IOException if unable to set ACLs or create node
*/","* Utility function to ensure that the configured base znode exists.
   * This recursively creates the znode as well as all of its parents.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException interrupted exception.
   * @throws KeeperException other zookeeper operation errors.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,getActiveData,org.apache.hadoop.ha.ActiveStandbyElector:getActiveData(),474,491,"/**
* Retrieves active data from ZooKeeper, handling connection creation and exceptions.
* @throws ActiveNotFoundException if node does not exist
* @throws KeeperException|InterruptedException|IOException on zk operation failure
*/","* get data set by the active leader
   * 
   * @return data set by the active instance
   * @throws ActiveNotFoundException
   *           when there is no active leader
   * @throws KeeperException
   *           other zookeeper operation errors
   * @throws InterruptedException
   *           interrupted exception.
   * @throws IOException
   *           when ZooKeeper connection could not be established",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reEstablishSession,org.apache.hadoop.ha.ActiveStandbyElector:reEstablishSession(),872,892,"/**
* Re-establishes session connection with Zookeeper, retrying up to maxRetryNum times.
* @return true if successful, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,checkTGTAndReloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:checkTGTAndReloginFromKeytab(),1197,1199,"/**
* Checks TGT and relogs from keytab if necessary.
*/","* Re-login a user from keytab if TGT is expired or is close to expiry.
   * 
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if it's a kerberos login exception.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(),1245,1249,"/**
* Initiates re-login from keytab.
*/","* Re-Login a user in from a keytab file. Loads a user identity from a keytab
   * file and logs them in. They become the currently logged-in user. This
   * method assumes that {@link #loginUserFromKeytab(String, String)} had
   * happened already.
   * The Subject field of this UserGroupInformation object is updated to have
   * the new credentials.
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException on a failure",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String,boolean,boolean)",229,261,"/**
* Creates an InetSocketAddress from the given target address.
* @param target address to parse (e.g. ""example.com:8080"" or ""http://example.com"")
* @param defaultPort fallback port if not specified in target
* @param configName configuration property name for help text (optional)
* @param useCacheIfPresent whether to reuse cached URI (optional)
* @param isResolved whether the host has been resolved (optional)
* @return InetSocketAddress object or null if invalid
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getConnectAddress,org.apache.hadoop.net.NetUtils:getConnectAddress(java.net.InetSocketAddress),450,460,"/**
* Resolves unresolved InetSocketAddress to a local address.
* @param addr InetSocketAddress to be resolved
* @return Resolved InetSocketAddress or original if already valid
*/","* Returns an InetSocketAddress that a client can use to connect to the
   * given listening address.
   * 
   * @param addr of a listener
   * @return socket address that a client can use to connect to the server.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,updateAddress,org.apache.hadoop.ipc.Client$Connection:updateAddress(),588,606,"/**
* Updates the client's address if a change is detected.
* @return true if the address was updated, false otherwise
*/","* Update the server address if the address corresponding to the host
     * name has changed.
     *
     * @return true if an addr change was detected.
     * @throws IOException when the hostname cannot be resolved.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getCanonicalUri,"org.apache.hadoop.net.NetUtils:getCanonicalUri(java.net.URI,int)",333,354,"/**
* Canonicalizes a URI by updating its host and/or port.
* @param uri input URI
* @param defaultPort default port to use if not specified in the input URI
* @return canonicalized URI or original URI if no changes were needed
*/","* Resolve the uri's hostname and add the default port if not in the uri
   * @param uri to resolve
   * @param defaultPort if none is given
   * @return URI",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",1467,1531,"/**
* Initiates and sends RPC request to server.
* @param rpcKind type of RPC
* @param rpcRequest request data
* @param remoteId ID of remote endpoint
* @param serviceClass class of RPC service
* @param fallbackToSimpleAuth whether to fallback to simple auth
* @return null if in asynchronous mode, otherwise response data
*/","* Make a call, passing <code>rpcRequest</code>, to the IPC server defined by
   * <code>remoteId</code>, returning the rpc response.
   *
   * @param rpcKind
   * @param rpcRequest -  contains serialized method and method parameters
   * @param remoteId - the target rpc server
   * @param serviceClass - service class for RPC
   * @param fallbackToSimpleAuth - set to true or false during this method to
   *   indicate if a secure client falls back to simple auth
   * @param alignmentContext - state alignment context
   * @return the rpc response
   * Throws exceptions if there are network problems or if the remote code
   * threw an exception.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,run,org.apache.hadoop.ipc.Client$Connection:run(),1082,1109,"/**
* Runs the thread, starting RPC parameter sending and waiting for work.
* @throws Throwable on unexpected errors
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CacheableIPList.java,isIn,org.apache.hadoop.util.CacheableIPList:isIn(java.lang.String),62,75,"/**
* Checks whether IP address is within the allowed list.
* @param ipAddress IP address to check
* @return true if IP is in whitelist, false otherwise
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,waitForCompletion,org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache$CacheEntry),259,300,"/**
* Waits for a cached RPC request to complete and returns its state.
* @param newEntry newly added cache entry
*/","* This method handles the following conditions:
   * <ul>
   * <li>If retry is not to be processed, return null</li>
   * <li>If there is no cache entry, add a new entry {@code newEntry} and return
   * it.</li>
   * <li>If there is an existing entry, wait for its completion. If the
   * completion state is {@link CacheEntry#FAILED}, the expectation is that the
   * thread that waited for completion, retries the request. the
   * {@link CacheEntry} state is set to {@link CacheEntry#INPROGRESS} again.
   * <li>If the completion state is {@link CacheEntry#SUCCESS}, the entry is
   * returned so that the thread that waits for it can can return previous
   * response.</li>
   * <ul>
   * 
   * @return {@link CacheEntry}.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,addCacheEntry,"org.apache.hadoop.ipc.RetryCache:addCacheEntry(byte[],int)",309,319,"/**
* Adds a cache entry for the given client ID and call ID.
* @param clientId unique client identifier
* @param callId call identifier
*/","* Add a new cache entry into the retry cache. The cache entry consists of 
   * clientId and callId extracted from editlog.
   *
   * @param clientId input clientId.
   * @param callId input callId.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,addCacheEntryWithPayload,"org.apache.hadoop.ipc.RetryCache:addCacheEntryWithPayload(byte[],int,java.lang.Object)",321,333,"/**
* Adds a cache entry with associated payload.
* @param clientId unique client identifier
* @param callId call identification number
* @param payload arbitrary data object to store with the entry
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,org.apache.hadoop.fs.ContentSummary:toString(boolean),381,384,"/**
* Returns string representation with query option flag.
* @param qOption boolean indicating whether to include query option in output
*/","Return the string representation of the object in the output format.
   * if qOption is false, output directory count, file count, and content size;
   * if qOption is true, output quota and remaining quota as well.
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @return the string representation of the object",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,toString,org.apache.hadoop.fs.QuotaUsage:toString(),302,305,"/**
* Returns a human-readable string representation of this object.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,getExpression,"org.apache.hadoop.fs.shell.find.ExpressionFactory:getExpression(java.lang.String,org.apache.hadoop.conf.Configuration)",108,116,"/**
* Retrieves an instance of a registered Expression class by name.
* @param expressionName unique identifier for the expression type
* @param conf configuration object used to instantiate the expression
* @return created Expression instance or null if not found
*/","* Get an instance of the requested expression
   *
   * @param expressionName
   *          name of the command to lookup
   * @param conf
   *          the Hadoop configuration
   * @return the {@link Expression} or null if the expression is unknown",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,createExpression,"org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.String,org.apache.hadoop.conf.Configuration)",145,155,"/**
* Creates an instance of a specific Expression subclass.
* @param expressionClassname fully qualified name of the Expression class
* @param conf configuration object
*/","* Creates an instance of the requested {@link Expression} class.
   *
   * @param expressionClassname
   *          name of the {@link Expression} class to be instantiated
   * @param conf
   *          the Hadoop configuration
   * @return a new instance of the requested {@link Expression} class",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,buildDescription,org.apache.hadoop.fs.shell.find.Find:buildDescription(org.apache.hadoop.fs.shell.find.ExpressionFactory),109,159,"/**
* Generates a description of recognised expressions and operators.
* @param factory ExpressionFactory used to create expressions
*/",Build the description used by the help command.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,getExpression,org.apache.hadoop.fs.shell.find.Find:getExpression(java.lang.Class),438,442,"/**
* Creates an instance of the specified Expression subclass.
* @param expressionClass class type of the desired Expression
*/",Gets an instance of an expression from the factory.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,getInstance,org.apache.hadoop.fs.shell.CommandFactory:getInstance(java.lang.String),108,110,"/**
 * Retrieves a Command instance based on the given command string.
 * @param cmd command string to look up
 */","* Returns an instance of the class implementing the given command.  The
   * class must have been registered via
   * {@link #addClass(Class, String...)}
   * @param cmd name of the command
   * @return instance of the requested command",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,"org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration,boolean)",141,154,"/**
* Initializes a writable comparator with the specified configuration.
* @param keyClass class of the WritableComparable to be compared
* @param conf Hadoop Configuration object, or null for default config
* @param createInstances whether to create instance keys and buffer
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,readObject,"org.apache.hadoop.io.ObjectWritable:readObject(java.io.DataInput,org.apache.hadoop.io.ObjectWritable,org.apache.hadoop.conf.Configuration)",261,340,"/**
* Reads an object from a DataInput stream, resolving the class and instantiating it.
* @param in input data to read from
* @param objectWritable optional ObjectWritable to store values in
* @param conf Hadoop Configuration for loading classes
* @return instantiated object or null if not found
*/","* Read a {@link Writable}, {@link String}, primitive type, or an array of
   * the preceding.
   *
   * @param in DataInput.
   * @param objectWritable objectWritable.
   * @param conf configuration.
   * @return Object.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableFactories.java,newInstance,org.apache.hadoop.io.WritableFactories:newInstance(java.lang.Class),81,83,"/**
* Creates a new instance of the specified Writable class.
* @param c Class of the Writable object to instantiate
*/","* Create a new instance of a class with a defined factory.
   * @param c input c.
   * @return a new instance of a class with a defined factory.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,decodeTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token),870,872,"/**
* Decodes the token identifier from the given token.
* @param token input token containing encoded identifier
*/","* Decode the token identifier. The subclass can customize the way to decode
   * the token identifier.
   * 
   * @param token the token where to extract the identifier
   * @return the delegation token identifier
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,identifierToString,org.apache.hadoop.security.token.Token:identifierToString(java.lang.StringBuilder),422,435,"/**
* Converts identifier to string representation and appends to StringBuilder.
* @param buffer StringBuilder instance
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,printCredentials,"org.apache.hadoop.security.token.DtFileOperations:printCredentials(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text,java.io.PrintStream)",137,163,"/**
* Prints user credentials to the specified output stream.
*@param creds Credentials object containing tokens
*@param alias Text alias for filtering tokens
*@param out Output stream for printing credentials
*/","Print out a Credentials object.
   *  @param creds the Credentials object to be printed out.
   *  @param alias print only tokens matching alias (null matches all).
   *  @param out print to this stream.
   *  @throws IOException failure to unmarshall a token identifier.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskValidatorFactory.java,getInstance,org.apache.hadoop.util.DiskValidatorFactory:getInstance(java.lang.String),72,92,"/**
* Retrieves a DiskValidator instance based on the specified type.
* @param diskValidator name of the validator (Basic, ReadWrite, or custom class)
* @return DiskValidator instance
* @throws DiskErrorException if invalid or missing validator class
*/","* Returns {@link DiskValidator} instance corresponding to its name.
   * The diskValidator parameter can be ""basic"" for {@link BasicDiskValidator}
   * or ""read-write"" for {@link ReadWriteDiskValidator}.
   * @param diskValidator canonical class name, for example, ""basic""
   * @throws DiskErrorException if the class cannot be located
   * @return disk validator.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,parseMethod,"org.apache.hadoop.ha.NodeFencer:parseMethod(org.apache.hadoop.conf.Configuration,java.lang.String)",151,165,"/**
* Parses a method definition from the given configuration line.
* @param conf Configuration object
* @param line Method definition string
* @throws BadFencingConfigurationException if parsing fails
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,<init>,"org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.lang.String)",58,65,"/**
* Initializes the hosts file reader with input and exceptions files.
* @param inFile path to main hosts file
* @param exFile path to exception hosts file
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refresh,org.apache.hadoop.util.HostsFileReader:refresh(),118,121,"/**
* Refreshes the current host details by reading new include and exclude files.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,add,org.apache.hadoop.util.bloom.DynamicBloomFilter:add(org.apache.hadoop.util.bloom.Key),136,153,"/**
* Adds a key to the Bloom filter, potentially creating a new row if necessary.
* @param key unique identifier to add
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResourceObject,org.apache.hadoop.conf.Configuration:addResourceObject(org.apache.hadoop.conf.Configuration$Resource),1034,1038,"/**
* Adds a resource object and updates system properties.
* @param resource Resource object to be added
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getProps,org.apache.hadoop.conf.Configuration:getProps(),2946,2952,"/**
* Retrieves and synchronizes Properties object.
* If not initialized, loads default settings.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,readVectored,"org.apache.hadoop.fs.BufferedFSInputStream:readVectored(java.util.List,java.util.function.IntFunction)",179,183,"/**
* Reads vectored data from input stream.
* @param ranges list of file ranges to read
* @param allocate callback for allocating byte buffers
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,readVectored,"org.apache.hadoop.fs.FSDataInputStream:readVectored(java.util.List,java.util.function.IntFunction)",304,308,"/**
* Reads vectored data from input stream.
* @param ranges list of file ranges to read
* @param allocate function to allocate buffer for reading
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,getInternal,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getInternal(org.apache.hadoop.fs.impl.prefetch.BufferData),177,208,"/**
* Retrieves internal buffer data by executing prefetch and caching operations.
* @param data BufferData object to process
* @return true if successful, false otherwise
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,org.apache.hadoop.fs.BlockLocation:<init>(),82,84,"/**
* Constructs an empty block location with default coordinates.
*/",* Default Constructor.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",883,901,"/**
* Retrieves block locations for a file segment.
* @param file FileStatus object
* @param start start offset of the segment
* @param len length of the segment
* @return array of BlockLocation objects or empty if no blocks exist
*/","* Return an array containing hostnames, offset and size of
   * portions of the given file.  For nonexistent
   * file or regions, {@code null} is returned.
   *
   * <pre>
   *   if f == null :
   *     result = null
   *   elif f.getLen() {@literal <=} start:
   *     result = []
   *   else result = [ locations(FS, b) for b in blocks(FS, p, s, s+l)]
   * </pre>
   * This call is most helpful with and distributed filesystem
   * where the hostnames of machines that contain blocks of the given file
   * can be determined.
   *
   * The default implementation returns an array containing one element:
   * <pre>
   * BlockLocation( { ""localhost:9866"" },  { ""localhost"" }, 0, file.getLen())
   * </pre>
   *
   * In HDFS, if file is three-replicated, the returned array contains
   * elements like:
   * <pre>
   * BlockLocation(offset: 0, length: BLOCK_SIZE,
   *   hosts: {""host1:9866"", ""host2:9866, host3:9866""})
   * BlockLocation(offset: BLOCK_SIZE, length: BLOCK_SIZE,
   *   hosts: {""host2:9866"", ""host3:9866, host4:9866""})
   * </pre>
   *
   * And if a file is erasure-coded, the returned BlockLocation are logical
   * block groups.
   *
   * Suppose we have a RS_3_2 coded file (3 data units and 2 parity units).
   * 1. If the file size is less than one stripe size, say 2 * CELL_SIZE, then
   * there will be one BlockLocation returned, with 0 offset, actual file size
   * and 4 hosts (2 data blocks and 2 parity blocks) hosting the actual blocks.
   * 3. If the file size is less than one group size but greater than one
   * stripe size, then there will be one BlockLocation returned, with 0 offset,
   * actual file size with 5 hosts (3 data blocks and 2 parity blocks) hosting
   * the actual blocks.
   * 4. If the file size is greater than one group size, 3 * BLOCK_SIZE + 123
   * for example, then the result will be like:
   * <pre>
   * BlockLocation(offset: 0, length: 3 * BLOCK_SIZE, hosts: {""host1:9866"",
   *   ""host2:9866"",""host3:9866"",""host4:9866"",""host5:9866""})
   * BlockLocation(offset: 3 * BLOCK_SIZE, length: 123, hosts: {""host1:9866"",
   *   ""host4:9866"", ""host5:9866""})
   * </pre>
   *
   * @param file FilesStatus to get data from
   * @param start offset into the given file
   * @param len length for which to get locations for
   * @throws IOException IO failure
   * @return block location array.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationStatisticSummary.java,fetchDurationSummary,"org.apache.hadoop.fs.statistics.DurationStatisticSummary:fetchDurationSummary(org.apache.hadoop.fs.statistics.IOStatistics,java.lang.String,boolean)",129,140,"/**
* Fetches duration statistic summary by key and success status.
* @param source IO statistics data
* @param key unique identifier for the metric
* @param success true if fetching successful duration, false otherwise
* @return DurationStatisticSummary object or null if not found
*/","* Fetch the duration timing summary of success or failure operations
   * from an IO Statistics source.
   * If the duration key is unknown, the summary will be incomplete.
   * @param source source of data
   * @param key duration statistic key
   * @param success fetch success statistics, or if false, failure stats.
   * @return a summary of the statistics.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextImpl.java,snapshot,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:snapshot(),92,96,"/**
* Returns an IOStatistics snapshot object representing current state.
*/","* Returns a snapshot of the current thread's IOStatistics.
   *
   * @return IOStatisticsSnapshot of the context.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSupport.java,snapshotIOStatistics,org.apache.hadoop.fs.statistics.IOStatisticsSupport:snapshotIOStatistics(org.apache.hadoop.fs.statistics.IOStatistics),46,50,"/**
* Creates an IOStatisticsSnapshot object from given IOStatistics.
* @param statistics IOStatistics instance to be converted
* @return IOStatisticsSnapshot object or null if invalid input
*/","* Take a snapshot of the current statistics state.
   * <p>
   * This is not an atomic option.
   * <p>
   * The instance can be serialized, and its
   * {@code toString()} method lists all the values.
   * @param statistics statistics
   * @return a snapshot of the current values.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_create(java.lang.Object),123,125,"/**
* Creates an IO statistics snapshot object from the given source.
* @param source IO statistics object or null to create a default snapshot
*/","* Create a new {@link IOStatisticsSnapshot} instance.
   * @param source optional source statistics
   * @return an IOStatisticsSnapshot.
   * @throws ClassCastException if the {@code source} is not null and not an IOStatistics instance",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,toList,org.apache.hadoop.util.functional.RemoteIterators:toList(org.apache.hadoop.fs.RemoteIterator),232,237,"/**
* Converts RemoteIterator to a list.
* @param source iterator over elements of type T
* @return List of elements or empty list if iterator is exhausted
*/","* Build a list from a RemoteIterator.
   * @param source source iterator
   * @param <T> type
   * @return a list of the values.
   * @throws IOException if the source RemoteIterator raises it.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackStoreToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackStoreToken(org.apache.hadoop.util.functional.InvocationRaisingIOE),1002,1004,"/**
* Tracks an IO exception while storing token.
* @param invocation InvocationRaisingIOE object containing error details
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackUpdateToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackUpdateToken(org.apache.hadoop.util.functional.InvocationRaisingIOE),1006,1008,"/**
* Tracks an update token invocation.
* @param invocation InvocationRaisingIOE object
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackRemoveToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackRemoveToken(org.apache.hadoop.util.functional.InvocationRaisingIOE),1010,1012,"/**
* Tracks removal of access token.
* @param invocation IO exception event to log
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",59,65,"/**
* Performs coding operation on input chunks and stores results in output chunks.
* @param inputChunks array of input data chunks
* @param outputChunks array to store coded result chunks
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,write,org.apache.hadoop.io.EnumSetWritable:write(java.io.DataOutput),135,154,"/**
* Serializes EnumSet value to output stream.
* @param out DataOutput stream to write to
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,write,org.apache.hadoop.io.ObjectWritable:write(java.io.DataOutput),89,92,"/**
* Writes this object to the output stream.
* @throws IOException if I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processArgument,org.apache.hadoop.fs.shell.Command:processArgument(org.apache.hadoop.fs.shell.PathData),299,305,"/**
* Processes path data based on existence.
* @param item PathData object to process
*/","* Processes a {@link PathData} item, calling
   * {@link #processPathArgument(PathData)} or
   * {@link #processNonexistentPath(PathData)} on each item.
   * @param item {@link PathData} item to process
   * @throws IOException if anything goes wrong...",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getUnixGroups,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getUnixGroups(java.lang.String),203,231,"/**
* Retrieves Unix groups for a given user.
* @param user username to fetch group information for
* @return Set of Unix group names or EMPTY_GROUPS_SET if not found
*/","* Get the current user's group list from Unix by running the command 'groups'
   * NOTE. For non-existing user it will return EMPTY list.
   *
   * @param user get groups for this user
   * @return the groups list that the <code>user</code> belongs to. The primary
   *         group is returned first.
   * @throws IOException if encounter any error when running the command",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,runResolveCommand,"org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:runResolveCommand(java.util.List,java.lang.String)",222,262,"/**
* Runs a command with variable arguments, logging output.
* @param args list of arguments to pass
* @return concatenated output as string or null on failure
*/","* Build and execute the resolution command. The command is
     * executed in the directory specified by the system property
     * ""user.dir"" if set; otherwise the current working directory is used.
     * @param args a list of arguments
     * @param commandScriptName input commandScriptName.
     * @return null if the number of arguments is out of range,
     * or the output of the command.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[]),1214,1216,"/**
 * Constructs a ShellCommandExecutor instance with an array of execution strings.
 * @param execString array of shell command execution strings
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,readLink,org.apache.hadoop.fs.FileUtil:readLink(java.io.File),213,230,"/**
* Reads the target of a symbolic link.
* @param f File object to read
* @return Target path as a string or empty if failed
*/","* Returns the target of the given symlink. Returns the empty string if
   * the given path does not refer to a symlink or there is an error
   * accessing the symlink.
   * @param f File representing the symbolic link.
   * @return The target of the symbolic link, empty string on error or if not
   *         a symlink.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,execCommand,"org.apache.hadoop.fs.FileUtil:execCommand(java.io.File,java.lang.String[])",1531,1537,"/**
* Executes a system command with the specified file as an argument.
* @param f the file to pass as a command-line argument
* @param cmd the command to execute (varargs)
* @return command output or null on failure
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setPermission,"org.apache.hadoop.fs.RawLocalFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1108,1119,"/**
* Sets file permissions using POSIX or shell commands.
* @param p the file path
* @param permission the desired FsPermission
*/",* Use the command chmod to set permission.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,execShellGetUserForNetgroup,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:execShellGetUserForNetgroup(java.lang.String),134,146,"/**
* Retrieves user list for a given network group.
* @param netgroup unique network group identifier
* @return string representation of the user list or an empty string if not found
*/","* Calls shell to get users for a netgroup by calling getent
   * netgroup, this is a low level function that just returns string
   * that 
   *
   * @param netgroup get users for this netgroup
   * @return string of users for a given netgroup in getent netgroups format
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,relogin,org.apache.hadoop.security.UserGroupInformation$TicketCacheRenewalRunnable:relogin(),1073,1078,"/**
* Relogs in by renewing Kerberos ticket and re-logging from cache.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,stashOriginalFilePermissions,org.apache.hadoop.security.alias.LocalKeyStoreProvider:stashOriginalFilePermissions(),92,114,"/**
* Saves original file permissions to be restored later if needed. 
* @throws IOException on file access errors
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,<init>,org.apache.hadoop.security.Credentials:<init>(org.apache.hadoop.security.Credentials),103,105,"/**
* Copies all credentials from the specified instance.
* @param credentials source credentials to copy from
*/","* Create a copy of the given credentials.
   * @param credentials to copy",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addCredentials,org.apache.hadoop.security.UserGroupInformation:addCredentials(org.apache.hadoop.security.Credentials),1753,1757,"/**
* Adds credentials to the subject's existing credentials.
* @param credentials Credentials object to be added
*/","* Add the given Credentials to this user.
   * @param credentials of tokens and secrets",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.RawLocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",580,590,"/**
* Creates a non-recursive output stream for the specified file.
* @param f path to the file
* @return FSDataOutputStream instance or throws exception if file exists and overwrite is not allowed. 
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,toFile,org.apache.hadoop.fs.shell.PathData:toFile(),494,499,"/**
* Converts file system path to a File object.
* @throws IllegalArgumentException if not a local path
*/","* Get the path to a local file
   * @return File representing the local path
   * @throws IllegalArgumentException if this.fs is not the LocalFileSystem",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,mkdirsWithExistsAndPermissionCheck,"org.apache.hadoop.util.DiskChecker:mkdirsWithExistsAndPermissionCheck(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",224,235,"/**
* Ensures directory exists and has specified permissions.
* @param dir directory path
* @param expected desired file system permissions
*/","* Create the directory or check permissions if it already exists.
   *
   * The semantics of mkdirsWithExistsAndPermissionCheck method is different
   * from the mkdirs method provided in the Sun's java.io.File class in the
   * following way:
   * While creating the non-existent parent directories, this method checks for
   * the existence of those directories if the mkdir fails at any point (since
   * that directory might have just been created by some other process).
   * If both mkdir() and the exists() check fails for any seemingly
   * non-existent directory, then we signal an error; Sun's mkdir would signal
   * an error (return false) if a directory it is attempting to create already
   * exists or the mkdir fails.
   *
   * @param localFS local filesystem
   * @param dir directory to be created or checked
   * @param expected expected permission
   * @throws IOException",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.viewfs.ViewFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),441,445,"/**
* Sets the working directory to the specified absolute path.
* @param new_dir absolute path of the new working directory
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getNativeFileLinkStatus,"org.apache.hadoop.fs.RawLocalFileSystem:getNativeFileLinkStatus(org.apache.hadoop.fs.Path,boolean)",1300,1306,"/**
* Retrieves native file link status.
* @param f the path to query
* @param dereference whether to follow symbolic links
* @return FileStatus object or null if not found
*/","* Calls out to platform's native stat(1) implementation to get file metadata
   * (permissions, user, group, atime, mtime, etc). This works around the lack
   * of lstat(2) in Java 6.
   * 
   *  Currently, the {@link Stat} class used to do this only supports Linux
   *  and FreeBSD, so the old {@link #deprecatedGetFileLinkStatusInternal(Path)}
   *  implementation (deprecated) remains further OS support is added.
   *
   * @param f File to stat
   * @param dereference whether to dereference symlinks
   * @return FileStatus of f
   * @throws IOException",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getResolvedQualifiedPath,org.apache.hadoop.fs.viewfs.ChRootedFs:getResolvedQualifiedPath(org.apache.hadoop.fs.Path),167,171,"/**
* Resolves and qualifies the given file path.
* @param f input file path
* @return a qualified file path object or null if not found
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.AbstractFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1614,1625,"/**
* Checks if a path has a specific capability.
* @param path file system path
* @param capability feature to check for (e.g. FS_SYMLINKS)
* @return true if the path supports the specified capability, false otherwise
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.AbstractFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),1652,1657,"/**
* Returns the enclosing root directory of a given file system path.
* @param path input file system path
*/","* Return path of the enclosing root for a given path
   * The enclosing root path is a common ancestor that should be used for temp and staging dirs
   * as well as within encryption zones and other restricted directories.
   *
   * Call makeQualified on the param path to ensure its part of the correct filesystem
   *
   * @param path file path to find the enclosing root path for
   * @return a path to the enclosing root
   * @throws IOException early checks like failure to resolve path cause IO failures",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,makeQualified,org.apache.hadoop.fs.FilterFs:makeQualified(org.apache.hadoop.fs.Path),73,76,"/**
 * Makes the given file system path qualified with schema and authority.
 * @param path file system path to qualify
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,"org.apache.hadoop.fs.FileContext$Util:listStatus(java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",1892,1903,"/**
* Filters and adds file statuses to the result list based on a given path filter.
* @param results list of filtered file statuses
* @param f source path to list from
* @param filter criteria for inclusion in the results
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,listStatus,org.apache.hadoop.fs.Globber:listStatus(org.apache.hadoop.fs.Path),125,136,"/**
* Retrieves file status information for a given path.
* @param path the file system path to retrieve status for
* @return an array of FileStatus objects or empty array if not found
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,processDeleteOnExit,org.apache.hadoop.fs.FileContext:processDeleteOnExit(),296,312,"/**
* Removes files with deleteOnExit flag from disk.
* @throws IOException if an error occurs during deletion
*/",* Delete all the paths that were marked as delete-on-exit.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,exists,org.apache.hadoop.fs.FileContext$Util:exists(org.apache.hadoop.fs.Path),1757,1766,"/**
* Checks if a file exists at the specified path.
* @param f Path to the file
* @return True if the file exists, False otherwise
*/","* Does the file exist?
     * Note: Avoid using this method if you already have FileStatus in hand.
     * Instead reuse the FileStatus 
     * @param f the  file or dir to be checked
     *
     * @throws AccessControlException If access is denied
     * @throws IOException If an I/O error occurred
     * @throws UnsupportedFileSystemException If file system for <code>f</code> is
     *           not supported
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server
     * @return if f exists true, not false.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,getFileStatus,org.apache.hadoop.fs.Globber:getFileStatus(org.apache.hadoop.fs.Path),112,123,"/**
* Retrieves file status for the given path.
* @param path file system path
* @return FileStatus object or null if not found
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setWorkingDirectory,org.apache.hadoop.fs.FileContext:setWorkingDirectory(org.apache.hadoop.fs.Path),542,554,"/**
* Sets the working directory to a specified absolute path.
* @param newWDir absolute directory path
*/","* Set the working directory for wd-relative names (such a ""foo/bar""). Working
   * directory feature is provided by simply prefixing relative names with the
   * working dir. Note this is different from Unix where the wd is actually set
   * to the inode. Hence setWorkingDir does not follow symlinks etc. This works
   * better in a distributed environment that has multiple independent roots.
   * {@link #getWorkingDirectory()} should return what setWorkingDir() set.
   * 
   * @param newWDir new working directory
   * @throws IOException 
   * <br>
   *           NewWdir can be one of:
   *           <ul>
   *           <li>relative path: ""foo/bar"";</li>
   *           <li>absolute without scheme: ""/foo/bar""</li>
   *           <li>fully qualified with scheme: ""xx://auth/foo/bar""</li>
   *           </ul>
   * <br>
   *           Illegal WDs:
   *           <ul>
   *           <li>relative with scheme: ""xx:foo/bar""</li>
   *           <li>non existent directory</li>
   *           </ul>",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,checkDest,"org.apache.hadoop.fs.FileContext:checkDest(java.lang.String,org.apache.hadoop.fs.Path,boolean)",2261,2278,"/**
* Verifies destination for file copy operation.
* @param srcName source name (optional if dest is directory)
* @param dst target path
* @param overwrite whether to overwrite existing files
*/","* Check if copying srcName to dst would overwrite an existing 
   * file or directory.
   * @param srcName File or directory to be copied.
   * @param dst Destination to copy srcName to.
   * @param overwrite Whether it's ok to overwrite an existing file. 
   * @throws AccessControlException If access is denied.
   * @throws IOException If dst is an existing directory, or dst is an 
   * existing file and the overwrite option is not passed.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getContentSummary,org.apache.hadoop.fs.FileContext$Util:getContentSummary(org.apache.hadoop.fs.Path),1786,1812,"/**
* Calculates content summary for a given path.
* @param f the path to calculate summary for
* @return ContentSummary object containing total length, files and directories count
*/","* Return the {@link ContentSummary} of path f.
     * @param f path
     *
     * @return the {@link ContentSummary} of path f.
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If <code>f</code> does not exist
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>f</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getDelegationTokens,"org.apache.hadoop.fs.FileContext:getDelegationTokens(org.apache.hadoop.fs.Path,java.lang.String)",2432,2443,"/**
* Retrieves a list of delegation tokens from the given path and renewer.
* @param p Path to retrieve tokens for
* @param renewer Entity responsible for token renewal
* @return List of Token objects or empty list if not found
*/","* Get delegation tokens for the file systems accessed for a given
   * path.
   * @param p Path for which delegations tokens are requested.
   * @param renewer the account name that is allowed to renew the token.
   * @return List of delegation tokens.
   * @throws IOException If an I/O error occurred.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setXAttr,"org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",2584,2588,"/**
* Sets an extended attribute on a file.
* @param path the file to modify
* @param name the attribute's name
* @param value the attribute's value
*/","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @throws IOException If an I/O error occurred.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createSnapshot,org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path),2747,2749,"/**
* Creates a snapshot of the given file system location.
* @param path file system location to snapshot
*/","* Create a snapshot with a default name.
   *
   * @param path The directory where snapshots will be taken.
   * @return the snapshot path.
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",77,87,"/**
* Initializes MultipartUploaderBuilderImpl with file context and path.
* @param fc FileContext instance
* @param p Path to upload
*/","* Construct from a {@link FileContext}.
   *
   * @param fc FileContext
   * @param p path.
   * @throws IOException failure",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,<init>,org.apache.hadoop.fs.LocalFileSystem:<init>(),40,42,"/**
* Initializes a LocalFileSystem instance with a RawLocalFileSystem provider.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,hasMoreThanOneSourcePaths,org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:hasMoreThanOneSourcePaths(java.util.LinkedList),105,118,"/**
* Checks if there's more than one source path provided.
* @param args list of PathData objects representing paths
* @return true if multiple sources, false otherwise
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,waitForRecovery,org.apache.hadoop.fs.shell.Truncate:waitForRecovery(),102,116,"/**
* Waits for and truncates all items in the wait list to a specified length.
*/",* Wait for all files in waitList to have length equal to newLength.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,tryResolveInRegexMountpoint,"org.apache.hadoop.fs.viewfs.InodeTree:tryResolveInRegexMountpoint(java.lang.String,boolean)",1022,1032,"/**
* Resolves the given source path in a list of regex mount points.
* @param srcPath path to resolve
* @param resolveLastComponent whether to resolve last component only
* @return ResolveResult object or null if not resolved
*/","* Walk through all regex mount points to see
   * whether the path match any regex expressions.
   *  E.g. link: ^/user/(?&lt;username&gt;\\w+) =&gt; s3://$user.apache.com/_${user}
   *  srcPath: is /user/hadoop/dir1
   *  resolveLastComponent: true
   *  then return value is s3://hadoop.apache.com/_hadoop
   *
   * @param srcPath srcPath.
   * @param resolveLastComponent resolveLastComponent.
   * @return ResolveResult.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,<init>,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.Path[])",548,554,"/**
* Initializes a PathIterator with the given file system, path string, and root directories.
* @param fs file system instance
* @param pathStr path string to iterate over
* @param rootDirs array of root directories for iteration
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,next,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:next(),571,583,"/**
* Advances to the next item and returns its Path.
* @throws RuntimeException on IO error or NoSuchElementException if exhausted
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,ifExists,"org.apache.hadoop.fs.LocalDirAllocator:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)",248,251,"/**
* Checks if a given path exists in HDFS or other storage system.
* @param pathStr the path to check
* @param conf configuration object for file system settings
*/","*  We search through all the configured dirs for the file's existence
   *  and return true when we find.
   *  @param pathStr the requested file (this will be searched)
   *  @param conf the Configuration object
   *  @return true if files exist. false otherwise",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,initFileSystem,org.apache.hadoop.security.alias.LocalKeyStoreProvider:initFileSystem(java.net.URI),116,141,"/**
* Initializes the file system with a URI.
* @param uri unique identifier for the file system
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,archivePath,org.apache.hadoop.fs.HarFileSystem:archivePath(org.apache.hadoop.fs.Path),200,211,"/**
* Finds the archive path within a directory hierarchy.
* @param p Path to search in
* @return Archive path (ending with .har) or null if not found
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getPathInHar,org.apache.hadoop.fs.HarFileSystem:getPathInHar(org.apache.hadoop.fs.Path),356,373,"/**
* Resolves the path in Hadoop Archive to its corresponding directory within the archive.
*@param path input path to resolve
*/","* this method returns the path 
   * inside the har filesystem.
   * this is relative path inside 
   * the har filesystem.
   * @param path the fully qualified path in the har filesystem.
   * @return relative path in the filesystem.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,makeRelative,"org.apache.hadoop.fs.HarFileSystem:makeRelative(java.lang.String,org.apache.hadoop.fs.Path)",379,393,"/**
* Converts an absolute path to a relative path based on the given initial directory.
* @param initial Initial directory
* @param p Absolute path
* @return Relative path or null if not applicable
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getChecksumFile,org.apache.hadoop.fs.ChecksumFs:getChecksumFile(org.apache.hadoop.fs.Path),88,90,"/**
* Returns the checksum file path for the given input file.
* @param file input file
*/","* Return the name of the checksum file associated with a file.
   *
   * @param file the file path.
   * @return the checksum file associated with a file.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,createCollectorPath,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:createCollectorPath(org.apache.hadoop.fs.Path),155,161,"/**
* Creates a collector path by merging file name, random UUID and separator.
* @param filePath input file path
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,parentExists,org.apache.hadoop.fs.shell.PathData:parentExists(),250,253,"/**
* Checks if a directory's parent exists.
* @return true if parent exists, false otherwise
*/","* Test if the parent directory exists
   * @return boolean indicating parent exists
   * @throws IOException upon unexpected error",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,processNonexistentPath,org.apache.hadoop.fs.shell.Mkdir:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),69,90,"/**
* Processes a nonexistent path by checking parent existence and creating the directory.
* @param item PathData object containing the non-existent path
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,primitiveMkdir,"org.apache.hadoop.fs.FileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",1392,1415,"/**
* Creates a directory with specified permissions; throws exception if parent does not exist when createParent is false.
* @param f the directory path
* @param absolutePermission file system permission
* @param createParent whether to throw exception for missing parent
*/","* This version of the mkdirs method assumes that the permission is absolute.
   * It has been added to support the FileContext that processes the permission
   * with umask before calling this method.
   * This a temporary method added to support the transition from FileSystem
   * to FileContext for user applications.
   *
   * @param f the path.
   * @param absolutePermission permission.
   * @param createParent create parent.
   * @throws IOException IO failure.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,rename,"org.apache.hadoop.fs.FileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",1669,1726,"/**
* Renames or moves a file/directory.
* @param src source Path
* @param dst destination Path
* @param options optional Rename options (e.g. OVERWRITE)
* @throws IOException on error or when overwriting existing non-empty directory
*/","* Renames Path src to Path dst
   * <ul>
   *   <li>Fails if src is a file and dst is a directory.</li>
   *   <li>Fails if src is a directory and dst is a file.</li>
   *   <li>Fails if the parent of dst does not exist or is a file.</li>
   * </ul>
   * <p>
   * If OVERWRITE option is not passed as an argument, rename fails
   * if the dst already exists.
   * </p>
   * <p>
   * If OVERWRITE option is passed as an argument, rename overwrites
   * the dst if it is a file or an empty directory. Rename fails if dst is
   * a non-empty directory.
   * </p>
   * Note that atomicity of rename is dependent on the file system
   * implementation. Please refer to the file system documentation for
   * details. This default implementation is non atomic.
   * <p>
   * This method is deprecated since it is a temporary method added to
   * support the transition from FileSystem to FileContext for user
   * applications.
   * </p>
   *
   * @param src path to be renamed
   * @param dst new path after rename
   * @param options rename options.
   * @throws FileNotFoundException src path does not exist, or the parent
   * path of dst does not exist.
   * @throws FileAlreadyExistsException dest path exists and is a file
   * @throws ParentNotDirectoryException if the parent path of dest is not
   * a directory
   * @throws IOException on failure",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getNflyTmpPath,org.apache.hadoop.fs.viewfs.NflyFSystem:getNflyTmpPath(org.apache.hadoop.fs.Path),447,449,"/**
* Creates a temporary path by appending NFLY_TMP_PREFIX to the parent and name of the given file.
* @param f input file path
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getChecksumFile,org.apache.hadoop.fs.ChecksumFileSystem:getChecksumFile(org.apache.hadoop.fs.Path),120,122,"/**
* Returns the checksum file path based on the input file.
* @param file input file to calculate checksum for
*/","* Return the name of the checksum file associated with a file.
   *
   * @param file the file path.
   * @return name of the checksum file associated with a file.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BulkDeleteUtils.java,validatePathIsUnderParent,"org.apache.hadoop.fs.BulkDeleteUtils:validatePathIsUnderParent(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",56,64,"/**
* Checks if a file path is under its parent directory.
* @param p the file path to check
* @param basePath the base directory path
* @return true if p is under basePath, false otherwise
*/","* Check if a given path is the base path or under the base path.
   * @param p path to check.
   * @param basePath base path.
   * @return true if the given path is the base path or under the base path.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isRoot,org.apache.hadoop.fs.Path:isRoot(),408,410,"/**
 * Checks whether this node is the root by verifying its parent.
 */","* Returns true if and only if this path represents the root of a file system.
   *
   * @return true if and only if this path represents the root of a file system",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,suffix,org.apache.hadoop.fs.Path:suffix(java.lang.String),467,474,"/**
* Constructs a new path with the given suffix appended to the current path.
* @param suffix string to append to the path
*/","* Adds a suffix to the final name in the path.
   *
   * @param suffix the suffix to add
   * @return a new path with the suffix added",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSLinkResolver.java,qualifySymlinkTarget,"org.apache.hadoop.fs.FSLinkResolver:qualifySymlinkTarget(java.net.URI,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",46,55,"/**
* Qualifies the target of a symbolic link with its effective URI.
* @param pathURI base URI for qualification
* @param pathWithLink path containing the symlink
* @param target path to be qualified
* @return Path object with resolved symlink target
*/","* Return a fully-qualified version of the given symlink target if it
   * has no scheme and authority. Partially and fully-qualified paths
   * are returned unmodified.
   * @param pathURI URI of the filesystem of pathWithLink
   * @param pathWithLink Path that contains the symlink
   * @param target The symlink's absolute target
   * @return Fully qualified version of the target.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,renameInternal,"org.apache.hadoop.fs.AbstractFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",848,897,"/**
* Renames an internal file or directory from one path to another.
* @param src original path
* @param dst new path
* @param overwrite whether to overwrite the destination if it exists
*/","* The specification of this method matches that of
   * {@link FileContext#rename(Path, Path, Options.Rename...)} except that Path
   * f must be for this file system.
   *
   * @param src src.
   * @param dst dst.
   * @param overwrite overwrite flag.
   * @throws AccessControlException access control exception.
   * @throws FileAlreadyExistsException file already exists exception.
   * @throws FileNotFoundException file not found exception.
   * @throws ParentNotDirectoryException parent not directory exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,createPath,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createPath(org.apache.hadoop.fs.Path,java.lang.String,boolean)",362,377,"/**
* Creates a Path object for the specified directory and path.
* @param dir base directory
* @param path relative path
* @param checkWrite whether to validate write permissions on the parent directory
* @return the created Path object or null if validation fails
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,createInternal,"org.apache.hadoop.fs.DelegateToFileSystem:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",79,109,"/**
* Creates an FSDataOutputStream on the specified Path with given parameters.
* @param f Path to create output stream for
* @param flag Create flags (e.g. overwrite, append)
* @return FSDataOutputStream object or null if creation fails
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,<init>,"org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",716,721,"/**
* Constructs an FCDataOutputStreamBuilder instance with a specified FileContext and Path.
* @param fc FileContext instance
* @param p Path to the data stream
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)",131,138,"/**
* Constructs a FileStatus object with the given parameters.
* @param length file length
* @param isdir  true if directory, false otherwise
* @param block_replication replication factor for blocks
* @param blocksize size of each block
* @param modification_time last modified time
* @param access_time last accessed time
* @param permission file permissions
* @param owner    file owner
* @param group    file group
* @param path     file path
*/","* Constructor for file systems on which symbolic links are not supported
   *
   * @param length length.
   * @param isdir isdir.
   * @param block_replication block replication.
   * @param blocksize block size.
   * @param modification_time modification time.
   * @param access_time access_time.
   * @param permission permission.
   * @param owner owner.
   * @param group group.
   * @param path the path.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,org.apache.hadoop.fs.FileStatus:<init>(org.apache.hadoop.fs.FileStatus),198,206,"/**
* Copies a FileStatus object, allowing for subclassing and overriding getters.
* @param other the FileStatus to copy
*/","* Copy constructor.
   *
   * @param other FileStatus to copy
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Stat.java,parseExecResult,org.apache.hadoop.fs.Stat:parseExecResult(java.io.BufferedReader),110,170,"/**
* Parses the output of 'stat' command and populates a FileStatus object.
* @param lines BufferedReader containing the stat output
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,cloneStatus,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:cloneStatus(),172,182,"/**
* Creates a shallow clone of the FileStatus object.
* @return cloned FileStatus object
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.BlockLocation[])",49,62,"/**
* Constructs LocatedFileStatus object from FileStatus and BlockLocation array.
* @param stat FileStatus containing file metadata
* @param locations array of BlockLocations describing storage layout
*/","* Constructor 
   * @param stat a file status
   * @param locations a file's block locations",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.BlockLocation[])",80,92,"/**
* Constructs a LocatedFileStatus object from various file attributes.
* @param length total file size
* @param isdir true if directory, false otherwise
* @param block_replication replication factor for blocks
* @param blocksize size of each block
* @param modification_time timestamp of last modification
* @param access_time timestamp of last access
* @param permission FsPermission object (may be null)
* @param owner file owner name
* @param group file group name
* @param symlink symbolic link target path (null if not a link)
* @param path file path
* @param locations block locations for erasure-coded files
*/","* Constructor
   * 
   * @param length a file's length
   * @param isdir if the path is a directory
   * @param block_replication the file's replication factor
   * @param blocksize a file's block size
   * @param modification_time a file's modification time
   * @param access_time a file's access time
   * @param permission a file's permission
   * @param owner a file's owner
   * @param group a file's group
   * @param symlink symlink if the path is a symbolic link
   * @param path the path's qualified name
   * @param locations a file's block locations",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,build,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:build(),48,52,"/**
* Builds and returns a FileSystemMultipartUploader instance.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,append,org.apache.hadoop.io.ArrayFile$Writer:append(org.apache.hadoop.io.Writable),84,87,"/**
* Appends a Writable value to the collection and increments a counter.
* @param value item to be added
*/","* Append a value to the file.
     * @param value value.
     * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,selectDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials),147,165,"/**
* Selects a delegation token from the configured KMS providers.
* @param creds credentials for authentication
* @return Token object or null if not found
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,write,"org.apache.hadoop.fs.FSOutputSummer:write(byte[],int,int)",102,114,"/**
* Writes a portion of the given byte array to this output stream.
* @param b the byte array
* @param off the starting offset in the array
* @param len the number of bytes to write
*/","* Writes <code>len</code> bytes from the specified byte array 
   * starting at offset <code>off</code> and generate a checksum for
   * each data chunk.
   *
   * <p> This method stores bytes from the given array into this
   * stream's buffer before it gets checksumed. The buffer gets checksumed 
   * and flushed to the underlying output stream when all data 
   * in a checksum chunk are in the buffer.  If the buffer is empty and
   * requested length is at least as large as the size of next checksum chunk
   * size, this method will checksum and write the chunk directly 
   * to the underlying output stream.  Thus it avoids unnecessary data copy.
   *
   * @param      b     the data.
   * @param      off   the start offset in the data.
   * @param      len   the number of bytes to write.
   * @exception  IOException  if an I/O error occurs.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",67,77,"/**
* Performs coding operation on input and output chunks.
* @param inputChunks array of input chunks to process
* @param outputChunks array of output chunks to store result in
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupRandPartC,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartC(),1156,1167,"/**
* Advances to part C of the randomization process.
* @throws IOException if an I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupBlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupBlock(),1039,1075,"/**
* Initializes block processing state based on user data and origPtr position.
* @throws IOException if stream is corrupted
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupNoRandPartC,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartC(),1183,1195,"/**
* Advances to No Rand Part C state if su_j2 < su_z, otherwise transitions to No Rand Part A. 
* @throws IOException on input/output error
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,finalize,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:finalize(),711,715,"/**
* Performs cleanup and finalization tasks.
* Calls finish() to release resources before superclass finalize().
*/",* Overriden to close the stream.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,close,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:close(),734,746,"/**
* Closes the underlying output stream, ensuring any pending data is flushed.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,finish,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:finish(),263,272,"/**
* Finalizes output and resets state if necessary. 
* @throws IOException on write error
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,write,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write(int),645,652,"/**
* Writes single byte to output stream.
* @throws IOException if stream is closed or error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,write,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write(byte[],int,int)",859,879,"/**
* Writes specified bytes to the output stream.
* @param buf byte array to write
* @param offs starting offset in buffer
* @param len number of bytes to write
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Writer$ValueRegister:close(),492,510,"/**
* Closes this data block, updating counts and error tracking.
* @throws IOException if closing operation fails
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekToEnd,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekToEnd(),1447,1449,"/**
* Seeks to the end of the current file.
*/","* Seek to the end of the scanner. The entry returned by the previous
       * entry() call will be invalid.
       * 
       * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:close(),1578,1581,"/**
 * Closes this cursor by parking it at the current position.","* Close the scanner. Release all resources. The behavior of using the
       * scanner after calling close is not defined. The entry returned by the
       * previous entry() call will be invalid.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readTokenStorageFile,"org.apache.hadoop.security.Credentials:readTokenStorageFile(java.io.File,org.apache.hadoop.conf.Configuration)",250,265,"/**
* Reads user credentials from a file and returns as Credentials object.
* @param filename path to the token storage file
* @param conf configuration (not used)
*/","* Convenience method for reading a token storage file and loading its Tokens.
   * @param filename filename.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return Token.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfoFromZK,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),639,642,"/**
* Retrieves token information from ZooKeeper.
* @param ident unique token identifier
* @return TokenDelegationTokenInformation object or null if not found
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,removeStoredToken,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)",791,831,"/**
* Removes stored token from ZooKeeper.
* @param ident unique token identifier
* @param checkAgainstZkBeforeDeletion flag to check against zk before deletion
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeProtobufOutputStream,org.apache.hadoop.security.Credentials:writeProtobufOutputStream(java.io.DataOutputStream),328,333,"/**
* Writes protocol buffer data to output stream.
* @param os DataOutputStream instance
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,decodeToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:decodeToken(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",223,232,"/**
* Decodes a delegation token into its identifier.
* @param token Token object containing the encoded data
* @param tokenKind Type of token (e.g. HDFS, etc.)
* @return DelegationTokenIdentifier object or null on error
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createIdentifier,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:createIdentifier(),80,83,"/**
* Creates a new delegationToken identifier with specified token kind.
* @return DelegationTokenIdentifier object representing the created token.",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createIdentifier,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:createIdentifier(),103,106,"/**
* Creates a new DelegationTokenIdentifier instance with specified token kind.",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSDelegationToken.java,<init>,org.apache.hadoop.crypto.key.kms.KMSDelegationToken$KMSDelegationTokenIdentifier:<init>(),43,45,"/**
* Constructs a new KMSDelegationTokenIdentifier instance.",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/LossyRetryInvocationHandler.java,invoke,"org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",41,46,"/**
* Invokes the target method with retry mechanism.
* @param proxy A dynamic proxy instance
* @param method The method to be invoked
* @param args Method arguments
* @return Result of method invocation or null if failed
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,inBlockAdvance,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",2008,2028,"/**
* Searches for a key within the current block.
* @param key RawComparable to search for
* @param greater whether to return true if key is greater than found key
* @return true if key matches or is less than the first key, false otherwise
*/","* Advance cursor in block until we find a key that is greater than or
       * equal to the input key.
       * 
       * @param key
       *          Key to compare.
       * @param greater
       *          advance until we find a key greater than the input key.
       * @return true if we find a equal key.
       * @throws IOException",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client$IpcStreams:<init>(java.net.Socket,int)",1897,1903,"/**
* Initializes IPC streams for the given socket with specified maximum response length.
* @param socket network socket to establish connection
* @param maxResponseLength maximum allowed length of incoming responses
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,onTimerEvent,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:onTimerEvent(),382,387,"/**
* Updates internal time and triggers metrics publication.
* @param none
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,publishMetricsNow,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:publishMetricsNow(),392,397,"/**
* Publishes current metrics to all configured sinks.
*/",* Requests an immediate publish of all metrics from sources to sinks.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,initSystemMBean,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initSystemMBean(),583,588,"/**
* Initializes system MBean with given prefix.
* @param prefix unique identifier for the MBean
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,startMBeans,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:startMBeans(),216,224,"/**
* Registers and starts the MBean with the given prefix and name.
* @param none
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,<init>,"org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:<init>(java.lang.String,int,org.apache.hadoop.ipc.DecayRpcScheduler)",859,867,"/**
* Initializes MetricsProxy with namespace, number of levels, and DecayRpcScheduler.
* @param namespace unique identifier for metrics
* @param numLevels number of metric levels to track
* @param drs DecayRpcScheduler instance to delegate to
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:<init>(java.lang.String),403,408,"/**
* Initializes a MetricsProxy instance with the given namespace and registers it with JMX.
* @param namespace unique identifier for the metrics proxy
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,shutdown,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:shutdown(),590,614,"/**
* Shuts down the metrics system.
* @return true if successfully shut down, false otherwise
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidator.java,checkStatus,org.apache.hadoop.util.ReadWriteDiskValidator:checkStatus(java.io.File),42,94,"/**
* Performs disk check and validation on the specified directory.
* @param dir directory to be checked
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,init,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:init(java.lang.Class),75,79,"/**
* Initializes rate-related components with the specified protocol.","* Initialize the metrics for JMX with protocol methods
   * @param protocol the protocol class",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,run,org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller:run(),223,240,"/**
* Collects metrics, rolls over averages, and updates the current snapshot.
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,collectThreadLocalStates,org.apache.hadoop.metrics2.lib.MutableRollingAverages:collectThreadLocalStates(),202,204,"/**
 * Collects and aggregates thread-local states into InnerMetrics. 
 */","* Collects states maintained in {@link ThreadLocal}, if any.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,newSink,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:newSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink,org.apache.hadoop.metrics2.impl.MetricsConfig)",521,532,"/**
* Creates a new MetricsSinkAdapter instance with specified configuration.
* @param name adapter name
* @param desc adapter description
* @param sink underlying metrics sink
* @param conf metrics configuration
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,add,"org.apache.hadoop.metrics2.lib.MetricsRegistry:add(java.lang.String,long)",358,373,"/**
* Adds a value to the specified metric.
* @param name unique metric identifier
* @param value value to add
*/","* Add sample to a stat metric by name.
   * @param name  of the metric
   * @param value of the snapshot to add",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,addTokenForOwnerStats,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addTokenForOwnerStats(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),924,928,"/**
* Increments owner stats token count for given ID.
* @param id unique token identifier
*/","* Add token stats to the owner to token count mapping.
   *
   * @param id token id.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeTokenForOwnerStats,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeTokenForOwnerStats(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),935,945,"/**
* Decrements or removes owner stats for a given token.
* @param id Token identifier
*/","* Remove token stats to the owner to token count mapping.
   *
   * @param id",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getUidAllowingUnknown,org.apache.hadoop.security.ShellBasedIdMapping:getUidAllowingUnknown(java.lang.String),697,707,"/**
* Returns the user ID, either from the mapping or as a hash code if not found.
* @param user username to look up
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getGidAllowingUnknown,org.apache.hadoop.security.ShellBasedIdMapping:getGidAllowingUnknown(java.lang.String),710,720,"/**
* Returns the GID for a given group, either by mapping it or using its hash code if mapping fails.
* @param group name of the group to get GID for
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,getCurrentActive,org.apache.hadoop.ha.ZKFailoverController:getCurrentActive(),784,802,"/**
* Retrieves the current active HA service target from ZooKeeper.
* @return HAServiceTarget object or null if not found
*/","* @return an {@link HAServiceTarget} for the current active node
   * in the cluster, or null if no node is active.
   * @throws IOException if a ZK-related issue occurs
   * @throws InterruptedException if thread is interrupted",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,<init>,"org.apache.hadoop.ha.ActiveStandbyElector:<init>(java.lang.String,int,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback,int,boolean,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",273,299,"/**
* Initializes ActiveStandbyElector with configuration and establishes ZooKeeper connection.
* @param zookeeperHostPorts comma-separated host:port of ZK servers
* @param zookeeperSessionTimeout session timeout in milliseconds
* @param parentZnodeName working directory for Znode operations
* @param acl list of access control lists for Znode security
* @param authInfo list of ZooKeeper authentication information
* @param app callback to notify on elector events
* @param maxRetryNum maximum retry count for connection establishment
* @param failFast whether to fail-fast or re-establish session on failure
* @param truststoreKeystore keystore and truststore configuration
*/","* Create a new ActiveStandbyElector object <br>
   * The elector is created by providing to it the Zookeeper configuration, the
   * parent znode under which to create the znode and a reference to the
   * callback interface. <br>
   * The parent znode name must be the same for all service instances and
   * different across services. <br>
   * After the leader has been lost, a new leader will be elected after the
   * session timeout expires. Hence, the app must set this parameter based on
   * its needs for failure response time. The session timeout must be greater
   * than the Zookeeper disconnect timeout and is recommended to be 3X that
   * value to enable Zookeeper to retry transient disconnections. Setting a very
   * short session timeout may result in frequent transitions between active and
   * standby states during issues like network outages/GS pauses.
   * 
   * @param zookeeperHostPorts
   *          ZooKeeper hostPort for all ZooKeeper servers
   * @param zookeeperSessionTimeout
   *          ZooKeeper session timeout
   * @param parentZnodeName
   *          znode under which to create the lock
   * @param acl
   *          ZooKeeper ACL's
   * @param authInfo a list of authentication credentials to add to the
   *                 ZK connection
   * @param app
   *          reference to callback interface object
   * @param failFast
   *          whether need to add the retry when establishing ZK connection.
   * @param maxRetryNum max Retry Num
   * @param truststoreKeystore truststore keystore, that we will use for ZK if SSL/TLS is enabled
   * @throws IOException
   *          raised on errors performing I/O.
   * @throws HadoopIllegalArgumentException
   *          if valid data is not supplied.
   * @throws KeeperException
   *          other zookeeper operation errors.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,joinElectionInternal,org.apache.hadoop.ha.ActiveStandbyElector:joinElectionInternal(),783,796,"/**
* Initiates joining an election process, re-establishing ZooKeeper session if necessary.
* @throws FatalError if connection with ZooKeeper cannot be re-established
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,relogin,org.apache.hadoop.security.UserGroupInformation$KeytabRenewalRunnable:relogin(),1094,1097,"/**
 * Initiates a new login using keytab credentials.",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddrUnresolved,org.apache.hadoop.net.NetUtils:createSocketAddrUnresolved(java.lang.String),166,168,"/**
* Creates an unresolved InetSocketAddress with the specified target host.
* @param target hostname or IP address
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String,boolean)",222,227,"/**
* Creates an InetSocketAddress instance with the specified target and port.
* @param target network address (e.g., hostname or IP address)
* @param defaultPort fallback port number
*/","* Create an InetSocketAddress from the given target string and
   * default port. If the string cannot be parsed correctly, the
   * <code>configName</code> parameter is used as part of the
   * exception message, allowing the user to better diagnose
   * the misconfiguration.
   *
   * @param target a string of either ""host"" or ""host:port""
   * @param defaultPort the default port if <code>target</code> does not
   *                    include a port number
   * @param configName the name of the configuration from which
   *                   <code>target</code> was loaded. This is used in the
   *                   exception message in the case that parsing fails.
   * @param useCacheIfPresent Whether use cache when create URI
   * @return  socket addr",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getConnectAddress,org.apache.hadoop.net.NetUtils:getConnectAddress(org.apache.hadoop.ipc.Server),439,441,"/**
* Retrieves the connect address for a given Server instance.
* @param server Server object with listener address
*/","* Returns InetSocketAddress that a client can use to 
   * connect to the server. Server.getListenerAddress() is not correct when
   * the server binds to ""0.0.0.0"". This returns ""hostname:port"" of the server,
   * or ""127.0.0.1:port"" when the getListenerAddress() returns ""0.0.0.0:port"".
   * 
   * @param server server.
   * @return socket address that a client can use to connect to the server.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setupConnection,org.apache.hadoop.ipc.Client$Connection:setupConnection(org.apache.hadoop.security.UserGroupInformation),608,695,"/**
* Establishes a connection to the server.
* @param ticket UserGroupInformation object
* @throws IOException if an I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean)",1415,1420,"/**
* Calls an RPC service with the given parameters and default authentication.
* @param rpcKind type of RPC service to call
* @param rpcRequest payload for the RPC request
* @param remoteId ID of the remote connection
* @param fallbackToSimpleAuth flag for falling back to simple authentication
*/","* Make a call, passing <code>rpcRequest</code>, to the IPC server defined by
   * <code>remoteId</code>, returning the rpc respond.
   *
   * @param rpcKind - input rpcKind.
   * @param rpcRequest -  contains serialized method and method parameters
   * @param remoteId - the target rpc server
   * @param fallbackToSimpleAuth - set to true or false during this method to
   *   indicate if a secure client falls back to simple auth
   * @return the rpc response
   * Throws exceptions if there are network problems or if the remote code
   * threw an exception.
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",1422,1428,"/**
* Wraps the main call method to facilitate simpler usage.
* @param rpcKind type of RPC request
* @param rpcRequest RPC data payload
* @param remoteId ID of the remote connection
* @param fallbackToSimpleAuth flag for authentication fallback
* @param alignmentContext context for alignment operations
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean)",1444,1450,"/**
* Wraps the core RPC call with a default authentication strategy.
* @param rpcKind type of RPC request
* @return result of the wrapped RPC call
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,waitForCompletion,"org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,byte[],int)",354,362,"/**
* Waits for cache entry completion in retry cache.
* @param cache RetryCache instance or null if not available
* @param clientId unique client identifier
* @param callId unique call identifier
* @return CacheEntry object on completion, or null otherwise
*/","* Static method that provides null check for retryCache.
   * @param cache input Cache.
   * @param clientId client id of this request
   * @param callId client call id of this request
   * @return CacheEntry.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,waitForCompletion,"org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,java.lang.Object,byte[],int)",372,380,"/**
* Waits for cache entry completion and returns the result.
* @param cache RetryCache instance
* @param payload associated payload object
* @param clientId unique client ID
* @param callId unique call identifier
* @return CacheEntryWithPayload object or null if skipped
*/","* Static method that provides null check for retryCache.
   * @param cache input cache.
   * @param payload input payload.
   * @param clientId client id of this request
   * @param callId client call id of this request
   * @return CacheEntryWithPayload.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,org.apache.hadoop.fs.ContentSummary:toString(),369,372,"/**
* Returns a string representation of this object.
* @return human-readable description of the object's state
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,getExpression,org.apache.hadoop.fs.shell.find.Find:getExpression(java.lang.String),432,435,"/**
* Retrieves an expression object from the factory based on its name.
* @param expressionName unique identifier of the expression to fetch
*/",Gets a named expression from the factory.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printInfo,"org.apache.hadoop.fs.FsShell:printInfo(java.io.PrintStream,java.lang.String,boolean)",212,247,"/**
* Prints help or usage information for a single command or all commands.
* @param out output stream to write to
* @param cmd name of the command (null for all commands)
* @param showHelp whether to display detailed help instead of usage
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,displayError,"org.apache.hadoop.fs.FsShell:displayError(java.lang.String,java.lang.String)",353,365,"/**
* Displays error message for the specified command.
* @param cmd command identifier
* @param message error details
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,get,"org.apache.hadoop.io.WritableComparator:get(java.lang.Class,org.apache.hadoop.conf.Configuration)",65,81,"/**
* Retrieves a WritableComparator instance for the specified class and configuration.
* @param c Class of the WritableComparable to get the comparator for
* @param conf Hadoop Configuration object
* @return WritableComparator instance or null if not found
*/","* Get a comparator for a {@link WritableComparable} implementation.
   * @param c class.
   * @param conf configuration.
   * @return WritableComparator.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class),132,134,"/**
* Initializes a new instance of WritableComparator with the specified key class.
* @param keyClass the type of comparable objects to be written
*/","* Construct for a {@link WritableComparable} implementation.
   * @param keyClass WritableComparable Class.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,"org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,boolean)",136,139,"/**
* Initializes writable comparator with specified key class.
* @param keyClass Class of WritableComparable to use
* @param createInstances Flag to create instances for comparison
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,readFields,org.apache.hadoop.io.ObjectWritable:readFields(java.io.DataInput),84,87,"/**
 * Reads serialized data from input stream and populates object fields.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,readFields,org.apache.hadoop.ipc.WritableRpcEngine$Invocation:readFields(java.io.DataInput),148,164,"/**
* Reads RPC method fields from DataInput stream.
* @param in input data stream
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayWritable.java,readFields,org.apache.hadoop.io.ArrayWritable:readFields(java.io.DataInput),93,101,"/**
* Reads and populates an array of writable objects from input stream.
* @param in DataInput stream containing object data
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,verifyToken,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:verifyToken(org.apache.hadoop.security.token.Token),208,215,"/**
* Verifies a delegation token and returns the user associated with it.
* @param token Token to verify
* @return User associated with the token or null if invalid
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,toString,org.apache.hadoop.security.token.Token:toString(),437,447,"/**
* Returns a human-readable string representation of the object.
* @return A formatted string containing kind, service, and ident values
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,<init>,org.apache.hadoop.fs.LocalDirAllocator:<init>(java.lang.String),85,93,"/**
* Initializes LocalDirAllocator with the given configuration item name.
* @param contextCfgItemName unique identifier for disk validation
*/","* Create an allocator object.
   * @param contextCfgItemName contextCfgItemName.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,parseMethods,"org.apache.hadoop.ha.NodeFencer:parseMethods(org.apache.hadoop.conf.Configuration,java.lang.String)",134,149,"/**
* Parses fencing specification into a list of methods with arguments.
*@param conf configuration object
*@param spec fencing specification string
*@return List of FenceMethodWithArg objects or empty list if invalid
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,append,"org.apache.hadoop.io.BloomMapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",182,190,"/**
* Appends a key-value pair to the bloom filter and updates the buffer.
* @param key WritableComparable object
* @param val Writable object
* @throws IOException if an I/O error occurs
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.lang.String,boolean)",927,929,"/**
* Adds a new resource with specified name and parser restriction.
* @param name unique resource identifier
* @param restrictedParser flag indicating parser access restrictions
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.net.URL,boolean)",945,947,"/**
 * Adds a new resource to the system with the specified URL and parser restriction.
 * @param url URL of the resource
 * @param restrictedParser whether the resource requires restricted parsing
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.fs.Path,boolean)",963,965,"/**
* Adds a resource to the system with optional parser restriction. 
* @param file file path to add as resource
* @param restrictedParser true if parser should be restricted for this resource
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,boolean)",984,986,"/**
* Adds resource to system with optional parser restriction. 
* @param in InputStream object containing resource data
* @param restrictedParser flag indicating if parser is restricted
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,java.lang.String,boolean)",1002,1005,"/**
* Adds a new resource to the system.
* @param in input stream containing resource data
* @param name unique identifier for the resource
* @param restrictedParser flag indicating parser restrictions
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setDeprecatedProperties,org.apache.hadoop.conf.Configuration:setDeprecatedProperties(),688,706,"/**
* Migrates deprecated properties to new keys in the properties file.
* @param deprecations DeprecationContext containing key mapping information
*/","* Sets all deprecated properties that are not currently set but have a
   * corresponding new property that is set. Useful for iterating the
   * properties when all deprecated properties for currently set properties
   * need to be present.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,updatePropertiesWithDeprecatedKeys,"org.apache.hadoop.conf.Configuration:updatePropertiesWithDeprecatedKeys(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String[])",764,775,"/**
* Updates properties with deprecated keys.
* @param deprecations Deprecation context
* @param newNames Array of new key names to update
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration:<init>(org.apache.hadoop.conf.Configuration),843,875,"/**
* Deep copies configuration from another instance.
* @param other Configuration object to copy properties and resources from
*/","* A new configuration with the same settings cloned from another.
   * 
   * @param other the configuration from which to clone settings.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.conf.Configuration),1015,1017,"/**
* Adds a resource to the configuration.
* @param conf configuration object containing properties and restrictions
*/","* Add a configuration resource.
   *
   * The properties of this resource will override properties of previously
   * added resources, unless they were marked <a href=""#Final"">final</a>.
   *
   * @param conf Configuration object from which to load properties",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getAlternativeNames,org.apache.hadoop.conf.Configuration:getAlternativeNames(java.lang.String),1372,1393,"/**
* Retrieves alternative names for the given name, considering deprecation context.
* @param name input name to fetch alternatives for
* @return array of alternative names or null if not found","* Returns alternative names (non-deprecated keys or previously-set deprecated keys)
   * for a given non-deprecated key.
   * If the given key is deprecated, return null.
   *
   * @param name property name.
   * @return alternative names.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPropertySources,org.apache.hadoop.conf.Configuration:getPropertySources(java.lang.String),2109,2129,"/**
* Retrieves property sources for the given resource name.
* @param name unique resource identifier
* @return array of property values or null if not found or loading
*/","* Gets information about why a property was set.  Typically this is the 
   * path to the resource objects (file, URL, etc.) the property came from, but
   * it can also indicate that it was set programmatically, or because of the
   * command line.
   *
   * @param name - The property name to get the source of.
   * @return null - If the property or its source wasn't found. Otherwise, 
   * returns a list of the sources of the resource.  The older sources are
   * the first ones in the list.  So for example if a configuration is set from
   * the command line, and then written out to a file that is read back in the
   * first entry would indicate that it was set from the command line, while
   * the second one would indicate the file that the new configuration was read
   * in from.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,size,org.apache.hadoop.conf.Configuration:size(),2988,2990,"/**
* Returns the number of properties in the collection.
*/","* Return the number of keys in the configuration.
   *
   * @return number of keys in the configuration.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,clear,org.apache.hadoop.conf.Configuration:clear(),2995,2998,"/**
* Clears all properties and overlay data.
*/",* Clears all keys from the configuration.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,iterator,org.apache.hadoop.conf.Configuration:iterator(),3006,3022,"/**
* Returns an iterator over string-to-string property pairs.
* @return Iterator over Map.Entry<String, String> objects
*/","* Get an {@link Iterator} to go through the list of <code>String</code> 
   * key-value pairs in the configuration.
   * 
   * @return an iterator over the entries.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,write,org.apache.hadoop.conf.Configuration:write(java.io.DataOutput),3965,3975,"/**
* Writes properties and associated data to output stream.
* @throws IOException if write operation fails
*/",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getValByRegex,org.apache.hadoop.conf.Configuration:getValByRegex(java.lang.String),3982,4001,"/**
* Fetches key-value pairs by regex pattern.
* @param regex regular expression to match keys
* @return Map of matching keys and their corresponding values
*/","* get keys matching the the regex.
   * @param regex the regex to match against.
   * @return {@literal Map<String,String>} with matching keys",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,readVectored,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readVectored(java.util.List,java.util.function.IntFunction)",438,482,"/**
* Reads vectored data from the stream into user-specified ranges.
* @param ranges List of FileRange objects
* @param allocate Function to allocate ByteBuffer for each range
*/","* Vectored read.
     * If the file has no checksums: delegate to the underlying stream.
     * If the file is checksummed: calculate the checksum ranges as
     * well as the data ranges, read both, and validate the checksums
     * as well as returning the data.
     * @param ranges the byte ranges to read
     * @param allocate the function to allocate ByteBuffer
     * @throws IOException",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,get,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:get(int),144,175,"/**
* Retrieves BufferData for a given block number.
* @param blockNumber unique identifier
* @return BufferData object or throws exception on failure
*/","* Gets the block having the given {@code blockNumber}.
   *
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,waitForReplication,org.apache.hadoop.fs.shell.SetReplication:waitForReplication(),108,141,"/**
* Waits for replication of files in the waitList to reach a consistent state.
* @throws IOException if an I/O error occurs during file system operations
*/",* Wait for all files in waitList to have replication number equal to rep.,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",924,931,"/**
* Retrieves block locations for a file within a specified range.
* @param p file path
* @param start starting offset in bytes
* @param len length of the range in bytes
* @return array of BlockLocation objects or null if not found
*/","* Return an array containing hostnames, offset and size of
   * portions of the given file.  For a nonexistent
   * file or regions, {@code null} is returned.
   *
   * This call is most helpful with location-aware distributed
   * filesystems, where it returns hostnames of machines that
   * contain the given file.
   *
   * A FileSystem will normally return the equivalent result
   * of passing the {@code FileStatus} of the path to
   * {@link #getFileBlockLocations(FileStatus, long, long)}
   *
   * @param p path is used to identify an FS since an FS could have
   *          another FS that it could be delegating the call to
   * @param start offset into the given file
   * @param len length for which to get locations for
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException IO failure
   * @return block location array.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.FilterFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",151,155,"/**
* Retrieves block locations for a given file status.
* @param file FileStatus object representing the file
* @param start starting offset in bytes
* @param len length of data to retrieve
* @return array of BlockLocation objects or null if not found
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationStatisticSummary.java,fetchSuccessSummary,"org.apache.hadoop.fs.statistics.DurationStatisticSummary:fetchSuccessSummary(org.apache.hadoop.fs.statistics.IOStatistics,java.lang.String)",149,153,"/**
* Fetches success summary statistics.
* @param source IOStatistics object
* @param key unique identifier for the statistics
*/","* Fetch the duration timing summary from an IOStatistics source.
   * If the duration key is unknown, the summary will be incomplete.
   * @param source source of data
   * @param key duration statistic key
   * @return a summary of the statistics.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_create(),113,115,"/**
* Creates an empty iostatistics snapshot.
* @return ISnapshot object or null if creation fails
*/","* Create a new {@link IOStatisticsSnapshot} instance.
   * @return an empty IOStatisticsSnapshot.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_retrieve,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_retrieve(java.lang.Object),146,152,"/**
* Retrieves an IO statistics snapshot from the specified source.
* @param source object containing IO statistics data
* @return Serializable snapshot of IO statistics or null if not found
*/","* Extract the IOStatistics from an object in a serializable form.
   * @param source source object, may be null/not a statistics source/instance
   * @return {@link IOStatisticsSnapshot} or null if the object is null/doesn't have statistics",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,toArray,"org.apache.hadoop.util.functional.RemoteIterators:toArray(org.apache.hadoop.fs.RemoteIterator,java.lang.Object[])",248,252,"/**
* Converts a RemoteIterator to an array, reusing the provided array if possible.
* @param source iterator of elements
* @param a array to reuse (optional)
* @return the converted array
*/","* Build an array from a RemoteIterator.
   * @param source source iterator
   * @param a destination array; if too small a new array
   * of the same type is created
   * @param <T> type
   * @return an array of the values.
   * @throws IOException if the source RemoteIterator raises it.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,createPassword,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),493,515,"/**
* Generates a password for the given TokenIdent based on current time and key.
* @param identifier unique token identifier
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,renewToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)",592,643,"/**
* Renews a delegated token.
* @param token Token to be renewed
* @param renewer entity performing the renewal
* @return new expiration time for the renewed token
*/","* Renew a delegation token.
   * @param token the token to renew
   * @param renewer the full principal name of the user doing the renewal
   * @return the new expiration time
   * @throws InvalidToken if the token is invalid
   * @throws AccessControlException if the user can't renew token",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",654,685,"/**
* Cancels a token with the given identifier and canceller.
* @param token Token object to cancel
* @param canceller String representation of the user cancelling the token
* @return cancelled TokenIdent object
*/","* Cancel a token by removing it from cache.
   *
   * @param token token.
   * @param canceller canceller.
   * @return Identifier of the canceled token
   * @throws InvalidToken for invalid token
   * @throws AccessControlException if the user isn't allowed to cancel",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processArguments,org.apache.hadoop.fs.shell.Command:processArguments(java.util.LinkedList),281,290,"/**
* Processes a list of arguments, executing each one and handling errors.
* @param args list of PathData objects to process
*/","*  Processes the command's list of expanded arguments.
   *  {@link #processArgument(PathData)} will be invoked with each item
   *  in the list.  The loop catches IOExceptions, increments the error
   *  count, and displays the exception.
   *  @param args a list of {@link PathData} to process
   *  @throws IOException if anything goes wrong...",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroups,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroups(java.lang.String),98,101,"/**
* Retrieves Unix groups associated with a user.
* @param userName unique username to fetch groups for
* @return list of group names or empty list if not found
*/","* Returns list of groups for a user
   *
   * @param userName get groups for this user
   * @return list of groups for a given user",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsSet(java.lang.String),121,124,"/**
* Retrieves unique Unix groups associated with the given user name.
* @param userName the user name to fetch group information for
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,resolve,org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:resolve(java.util.List),174,211,"/**
* Resolves a list of names by running a command and parsing the output.
* @param names list of names to resolve
* @return list of resolved names or null on error
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HardLink.java,getLinkCount,org.apache.hadoop.fs.HardLink:getLinkCount(java.io.File),214,255,"/**
* Retrieves the hard link count for a given file.
* @param fileName the file to query
* @return the number of hard links or -1 on error
* @throws IOException if an I/O error occurs
*/","* Retrieves the number of links to the specified file.
    *
    * @param fileName file name.
    * @throws IOException raised on errors performing I/O.
    * @return link count.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingTar,"org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.File,java.io.File,boolean)",1053,1084,"/**
* Untars a file to a specified directory, optionally gzipped.
* @param inFile input file to unarchive
* @param untarDir target directory for extracted files
* @param gzipped true if the file is gzip-compressed
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,symLink,"org.apache.hadoop.fs.FileUtil:symLink(java.lang.String,java.lang.String)",1226,1279,"/**
* Creates a symbolic link (symlink) with the specified name pointing to the target file.
* @param target absolute path of the target file
* @param linkname desired name of the symlink
* @return exit code from shell command execution, or 1 if invalid input
*/","* Create a soft link between a src and destination
   * only on a local disk. HDFS does not support this.
   * On Windows, when symlink creation fails due to security
   * setting, we will log a warning. The return code in this
   * case is 2.
   *
   * @param target the target for symlink
   * @param linkname the symlink
   * @return 0 on success
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,chmod,"org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String,boolean)",1303,1319,"/**
* Changes file/directory permissions using the shell command.
* @param filename name of file or directory to modify
* @param perm new permission value (e.g., '755')
* @param recursive whether to apply changes recursively
* @return exit code from shell command execution
*/","* Change the permissions on a file / directory, recursively, if
   * needed.
   * @param filename name of the file whose permissions are to change
   * @param perm permission string
   * @param recursive true, if permissions should be changed recursively
   * @return the exit code from the command.
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getConf,org.apache.hadoop.util.SysInfoLinux:getConf(java.lang.String),158,170,"/**
* Retrieves Linux system configuration value by attribute.
* @param attr configuration attribute name
* @return long value or -1 if not found or error occurs
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getSystemInfoInfoFromShell,org.apache.hadoop.util.SysInfoWindows:getSystemInfoInfoFromShell(),81,92,"/**
* Retrieves system information from the Windows command line.
* @return system information as a string, or null on error
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,checkIsBashSupported,org.apache.hadoop.util.Shell:checkIsBashSupported(),810,834,"/**
* Checks if Bash shell is supported on the current OS.
* @throws InterruptedIOException if interrupted during check
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,isSetsidSupported,org.apache.hadoop.util.Shell:isSetsidSupported(),845,879,"/**
* Checks if setsid command is supported on the system.
* @return true if setsid is available, false otherwise
*/","* Look for <code>setsid</code>.
   * @return true if <code>setsid</code> was present",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,loadPermissionInfoByNonNativeIO,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfoByNonNativeIO(),998,1045,"/**
* Loads file permission information using non-native IO.
* @throws RuntimeException on execution error
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setOwner,"org.apache.hadoop.fs.FileUtil:setOwner(java.io.File,java.lang.String,java.lang.String)",1329,1338,"/**
* Sets file owner using specified username and/or group name.
* @param file File object to set owner for
* @param username desired user name (optional)
* @param groupname desired group name (optional)
*/","* Set the ownership on a file / directory. User name and group name
   * cannot both be null.
   * @param file the file to change
   * @param username the new user owner name
   * @param groupname the new group owner name
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,execSetPermission,"org.apache.hadoop.fs.FileUtil:execSetPermission(java.io.File,org.apache.hadoop.fs.permission.FsPermission)",1520,1529,"/**
* Executes set permission command on a file.
* @param f target file
* @param permission FsPermission to apply
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,getUsersForNetgroup,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getUsersForNetgroup(java.lang.String),97,123,"/**
* Retrieves a list of user IDs from the specified netgroup.
* @param netgroup unique netgroup identifier
* @return List of user IDs or empty list if not found
*/","* Gets users for a netgroup
   *
   * @param netgroup return users for this netgroup
   * @return list of users for a given netgroup
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getCredentials,org.apache.hadoop.security.UserGroupInformation:getCredentials(),1736,1747,"/**
* Retrieves credentials, removing private tokens.
*/","* Obtain the tokens in credentials form associated with this user.
   * 
   * @return Credentials of tokens associated with this user",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,flush,org.apache.hadoop.security.alias.UserProvider:flush(),94,97,"/**
* Flushes credentials to the user's profile.
* @throws no exceptions are explicitly declared in this method",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,flush,org.apache.hadoop.crypto.key.UserProvider:flush(),138,141,"/**
* Flushes cached credentials to user profile.
* @throws (no parameters)
* @return (no return value)
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirInternal,"org.apache.hadoop.util.DiskChecker:checkDirInternal(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",138,143,"/**
* Verifies directory existence and permissions.
* @param localFS Local file system instance
* @param dir Directory path to verify
* @param expected Expected FsPermission value
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,hasPathCapability,"org.apache.hadoop.fs.FilterFs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",451,455,"/**
* Checks if a file system path has a specific capability.
* @param path file system path to check
* @param capability capability name to verify
* @return true if the path has the capability, false otherwise
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getEnclosingRoot,org.apache.hadoop.fs.FilterFs:getEnclosingRoot(org.apache.hadoop.fs.Path),463,466,"/**
* Retrieves the enclosing root directory for the given file system path.
* @param path file system path to resolve the root for
* @return Path object representing the enclosing root directory, or null if not found
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,"org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",1850,1856,"/**
* Retrieves an array of file statuses for the given path and filter.
* @param f Path to query
* @param filter Optional filter for file selection
* @return Array of FileStatus objects or null if failed
*/","* Filter files/directories in the given path using the user-supplied path
     * filter.
     * 
     * @param f is the path name
     * @param filter is the user-supplied path filter
     *
     * @return an array of FileStatus objects for the files under the given path
     *         after applying the filter
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If <code>f</code> does not exist
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>pathPattern</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,"org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)",1879,1886,"/**
* Lists status of multiple file paths and filters them based on the provided filter.
* @param files array of file paths to retrieve status for
* @param filter optional filter to apply to the result
* @return array of FileStatus objects or null if an error occurs
*/","* Filter files/directories in the given list of paths using user-supplied
     * path filter.
     * 
     * @param files is a list of paths
     * @param filter is the filter
     *
     * @return a list of statuses for the files under the given paths after
     *         applying the filter
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If a file in <code>files</code> does not 
     *           exist
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,run,org.apache.hadoop.fs.FileContext$FileContextFinalizer:run(),2318,2321,"/**
 * Starts the deletion process on exit. 
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,isMultiThreadNecessary,org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:isMultiThreadNecessary(java.util.LinkedList),98,102,"/**
* Determines if multi-threading is necessary based on thread count and number of source paths.
* @param args list of path data
* @return true if multi-threading is required, false otherwise
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,processArguments,org.apache.hadoop.fs.shell.Truncate:processArguments(java.util.LinkedList),68,73,"/**
* Processes command line arguments and waits for recovery if required.
* @param args list of PathData objects to be processed
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,resolve,"org.apache.hadoop.fs.viewfs.InodeTree:resolve(java.lang.String,boolean)",893,988,"/**
* Resolves a path to an INode.
* @param p the path to resolve
* @param resolveLastComponent whether to resolve the last component of the path
* @return ResolveResult object or null if not found
*/","* Resolve the pathname p relative to root InodeDir.
   * @param p - input path
   * @param resolveLastComponent resolveLastComponent.
   * @return ResolveResult which allows further resolution of the remaining path
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getFileHarStatus,org.apache.hadoop.fs.HarFileSystem:getFileHarStatus(org.apache.hadoop.fs.Path),646,659,"/**
* Retrieves HAR status for a given file.
* @param f Path to the file
* @return HarStatus object or throws IOException if invalid
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,int)",153,178,"/**
* Verifies and initializes ChecksumFSInputChecker with a checksum file.
* @param fs ChecksumFs instance
* @param file Path to the data file
* @param bufferSize Buffer size for reading data
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,setReplication,"org.apache.hadoop.fs.ChecksumFs:setReplication(org.apache.hadoop.fs.Path,short)",463,475,"/**
* Sets file replication and propagates to checksum file.
* @param src source file path
* @param replication new replication setting (short value)
*/","* Set replication for an existing file.
   * Implement the abstract <tt>setReplication</tt> of <tt>FileSystem</tt>
   * @param src file name
   * @param replication new replication
   * @throws IOException if an I/O error occurs.
   * @return true if successful;
   *         false if file does not exist or is a directory",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,delete,"org.apache.hadoop.fs.ChecksumFs:delete(org.apache.hadoop.fs.Path,boolean)",529,550,"/**
* Deletes a file or directory.
* @param f the file path to delete
* @param recursive whether to delete recursively
* @return true if deleted successfully, false otherwise
*/","* Implement the delete(Path, boolean) in checksum
   * file system.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",363,389,"/**
* Creates a ChecksumFSOutputSummer for the given file with specified parameters.
* @param fs ChecksumFs instance
* @param file Path to the file
* @param createFlag Create flags (e.g. CREATE, OVERWRITE)
* @param absolutePermission File permissions
* @param bufferSize Buffer size for data writing
* @param replication Replication factor
* @param blockSize Block size for data writing
* @param progress Progressable instance for tracking progress
* @param checksumOpt Checksum options
* @param createParent Flag to create parent directory if needed
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processArguments,org.apache.hadoop.fs.shell.CommandWithDestination:processArguments(java.util.LinkedList),223,245,"/**
* Validates and processes command-line arguments.
* @param args list of PathData objects
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,mkdir,"org.apache.hadoop.fs.DelegateToFileSystem:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",185,192,"/**
* Creates a new directory with specified permissions and behavior.
* @param dir the directory to be created
* @param permission FsPermission for the newly created directory
* @param createParent whether to create parent directories if they don't exist
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,rename,"org.apache.hadoop.fs.FileUtil:rename(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",2068,2082,"/**
* Renames a file in the specified filesystem with provided options.
* @param srcFs FileSystem instance
* @param src source path to rename
* @param dst destination path to rename to
* @param options Rename options (e.g. overwrite)",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,renameInternal,"org.apache.hadoop.fs.DelegateToFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",206,212,"/**
* Renames a file internally using the underlying filesystem implementation.
* @param src original file path
* @param dst new file name
* @throws IOException on rename failure
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,rename,"org.apache.hadoop.fs.FilterFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",254,258,"/**
* Renames a file or directory from source to destination path.
* @param src original file/directory path
* @param dst new file/directory path
* @param options rename operation options (currently not used)",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/InternalOperations.java,rename,"org.apache.hadoop.fs.InternalOperations:rename(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",35,39,"/**
* Renames a file or directory in the given file system.
* @param fs target file system
* @param src original path to rename
* @param dst new path for the renamed item
* @param options optional rename options (e.g. overwrite)",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,run,org.apache.hadoop.fs.ChecksumFileSystem$FsOperation:run(org.apache.hadoop.fs.Path),771,780,"/**
* Applies a command to the specified file path and optionally re-runs it on the checksum file.
* @param p the file path to process
* @return true if the operation was successful, false otherwise
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,int)",187,214,"/**
* Initializes a ChecksumFSInputChecker instance for the given file.
* @param fs Checksum-enabled Filesystem
* @param file Path to the file being checked
* @param bufferSize Buffer size for reading the file and checksum
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,rename,"org.apache.hadoop.fs.ChecksumFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",890,915,"/**
* Renames a file or directory and updates associated checksum files.
* @param src source path to rename
* @param dst destination path for renaming
* @return true if successful, false otherwise
*/",* Rename files/dirs,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,delete,"org.apache.hadoop.fs.ChecksumFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",921,941,"/**
* Deletes a file or directory from the filesystem.
* @param f path to the file/directory
* @param recursive whether to delete recursively
* @return true if deletion was successful, false otherwise
*/","* Implement the delete(Path, boolean) in checksum
   * file system.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.permission.FsPermission)",621,642,"/**
* Initializes output summer for checksum file creation.
* @param fs ChecksumFileSystem instance
* @param file Path to create checksums for
* @param overwrite whether to overwrite existing file
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,isAncestor,"org.apache.hadoop.fs.shell.find.Find:isAncestor(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",335,343,"/**
* Checks if source path is an ancestor of target path.
* @param source PathData object representing the potential ancestor
* @param target PathData object representing the descendant
* @return true if source is an ancestor, false otherwise
*/",Returns true if the target is an ancestor of the source.,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,fullPath,org.apache.hadoop.fs.viewfs.ChRootedFs:fullPath(org.apache.hadoop.fs.Path),91,95,"/**
* Returns the full path by combining the root part and the given path.
* @param path input path to combine with the root part
*/","* 
   * @param path
   * @return return full path including the chroot",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,stripOutRoot,org.apache.hadoop.fs.viewfs.ChRootedFs:stripOutRoot(org.apache.hadoop.fs.Path),137,148,"/**
* Strips the root path component from a given Path object.
* @param p input Path to process
*/","*  
   * Strip out the root from the path.
   * 
   * @param p - fully qualified path p
   * @return -  the remaining path  without the beginning /",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,stripOutRoot,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:stripOutRoot(org.apache.hadoop.fs.Path),153,163,"/**
* Strips out the root directory from a given file system path.
* @param p Path object to process
* @return String representation of the path with root removed, or empty string if entire path is root
*/","* Strip out the root from the path.
   * @param p - fully qualified path p
   * @return -  the remaining path  without the beginning /
   * @throws IOException if the p is not prefixed with root",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,createCheckpoint,"org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(org.apache.hadoop.fs.Path,java.util.Date)",335,359,"/**
* Creates a checkpoint of the current trash data by renaming the existing CURRENT directory.
* @param trashRoot root path of the trash directory
* @param date timestamp for the checkpoint file name
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSLinkResolver.java,resolve,"org.apache.hadoop.fs.FSLinkResolver:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",79,112,"/**
* Resolves a path to its final file or directory.
* @param fc FileContext for the resolution
* @param path Path to resolve
* @return resolved object of type T, or null if not found
*/","* Performs the operation specified by the next function, calling it
   * repeatedly until all symlinks in the given path are resolved.
   * @param fc FileContext used to access file systems.
   * @param path The path to resolve symlinks on.
   * @return Generic type determined by the implementation of next.
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,rename,"org.apache.hadoop.fs.AbstractFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",795,808,"/**
* Renames a file or directory from src to dst with optional overwrite behavior.
* @param src original path
* @param dst new path
* @param options rename options (e.g. OVERWRITE)
*/","* The specification of this method matches that of
   * {@link FileContext#rename(Path, Path, Options.Rename...)} except that Path
   * f must be for this file system.
   *
   * @param src src.
   * @param dst dst.
   * @param options options.
   * @throws AccessControlException access control exception.
   * @throws FileAlreadyExistsException file already exists exception.
   * @throws FileNotFoundException file not found exception.
   * @throws ParentNotDirectoryException parent not directory exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,renameInternal,"org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",253,259,"/**
* Renames a file or directory within the internal file system.
* @param src original path to rename
* @param dst new path for the renamed item
* @param overwrite whether to overwrite existing destination
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,toFileStatus,org.apache.hadoop.fs.HarFileSystem:toFileStatus(org.apache.hadoop.fs.HarFileSystem$HarStatus),530,553,"/**
* Converts HarStatus to FileStatus object.
* @param h HarStatus object
* @return FileStatus object or null if not found
*/","* Combine the status stored in the index and the underlying status. 
   * @param h status stored in the index
   * @return the combined file status
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,<init>,"org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:<init>(java.io.File,long,org.apache.hadoop.fs.FileSystem)",942,949,"/**
* Initializes deprecated raw local file status with given parameters.
* @param f the File object
* @param defaultBlockSize default block size
* @param fs FileSystem instance
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,org.apache.hadoop.fs.FileStatus:<init>(),107,107,"/**
* Constructs an empty FileStatus object with default values.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,org.apache.hadoop.fs.Path)",110,115,"/**
* Initializes a FileStatus object with the given parameters.
* @param length file size in bytes
* @param isdir whether the file is a directory
* @param block_replication replication factor for blocks
* @param blocksize block size in bytes
* @param modification_time last modified timestamp
* @param path file system path",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.commons.net.ftp.FTPFile,org.apache.hadoop.fs.Path)",557,572,"/**
* Retrieves file status for an FTP file.
* @param ftpFile FTP file object
* @param parentPath parent directory path
* @return FileStatus object containing file metadata
*/","* Convert the file information in FTPFile to a {@link FileStatus} object. *
   * 
   * @param ftpFile
   * @param parentPath
   * @return FileStatus",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupRandPartB,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartB(),1128,1154,"/**
* Updates the random part B state based on the su_ch2 value.
* @throws IOException if an I/O error occurs
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,changeStateToProcessABlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:changeStateToProcessABlock(),355,362,"/**
* Changes state to process a block, either initializing and setting up the block if skip result is false.
* Otherwise, sets the current state to EOF. 
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,init,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:init(),492,509,"/**
* Initializes BZip2 stream by validating magic bytes and block size.
* @throws IOException if stream is not properly formatted
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupNoRandPartB,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartB(),1169,1181,"/**
* Sets up part B of the no-rand process, handling different conditions based on user input.
* @throws IOException if an I/O error occurs
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,close,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:close(),302,308,"/**
 * Closes resources and ensures output stream is properly closed.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,write,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:write(int),288,293,"/**
* Writes a single byte to output stream.
* @param b the byte to be written
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,write,"org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:write(byte[],int,int)",295,300,"/**
* Writes bytes to output stream, resetting if necessary.
* @param b byte array to write
* @param off starting offset in the array
* @param len number of bytes to write
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,verify,"org.apache.hadoop.security.KDiag:verify(java.io.File,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",990,1003,"/**
* Verifies token file integrity and configuration.
* @param tokenFile storage location of token
* @param conf system configuration
* @param category logging category
* @param message error message
* @return true if verification successful, false otherwise
*/","* Verify that tokenFile contains valid Credentials.
   *
   * If not, an exception is raised, or, if {@link #nofail} is set,
   * an error will be logged and the method return false.
   *",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,printTokenFile,"org.apache.hadoop.security.token.DtFileOperations:printTokenFile(java.io.File,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration,java.io.PrintStream)",123,129,"/**
* Prints token file contents to the specified output stream.
* @param tokenFile file containing credentials
* @param alias user-friendly identifier for credentials
* @param conf configuration settings
* @param out target output stream
*/","Print out a Credentials file from the local filesystem.
   *  @param tokenFile a local File object.
   *  @param alias print only tokens matching alias (null matches all).
   *  @param conf Configuration object passed along.
   *  @param out print to this stream.
   *  @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfo,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),600,617,"/**
* Retrieves DelegationTokenInformation for the specified TokenIdent.
* @param ident unique token identifier
* @return DelegationTokenInformation object or null if not found
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,syncLocalCacheWithZk,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:syncLocalCacheWithZk(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),625,637,"/**
* Updates local cache with ZK data for a given token identifier.
* @param ident TokenIdent object containing sequence number
*/","* This method synchronizes the state of a delegation token information in
   * local cache with its actual value in Zookeeper.
   *
   * @param ident Identifier of the token",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,removeStoredToken,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),785,789,"/**
* Removes stored token by ID.
* @param ident unique TokenIdent identifier
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageToStream,"org.apache.hadoop.security.Credentials:writeTokenStorageToStream(java.io.DataOutputStream,org.apache.hadoop.security.Credentials$SerializedFormat)",306,319,"/**
* Writes token storage data to a stream based on the specified format.
* @param os Data output stream
* @param format Serialized format (WRITABLE, PROTOBUF)
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,<init>,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<init>(java.lang.String),118,134,"/**
* Initializes the metrics system with a specified prefix.
* @param prefix unique identifier for this metrics instance
*/","* Construct the metrics system
   * @param prefix  for the system",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,startMetricsMBeans,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:startMetricsMBeans(),334,339,"/**
* Starts metrics MBeans for all registered sources.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,start,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:start(),100,102,"/**
* Starts MBean initialization if enabled.
* @param startMBeans flag indicating whether to initialize MBeans
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getInstance,"org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getInstance(java.lang.String,int,org.apache.hadoop.ipc.DecayRpcScheduler)",869,881,"/**
* Retrieves or creates a MetricsProxy instance for the given namespace.
* @param namespace unique metrics identifier
* @param numLevels number of levels to consider
* @param drs DecayRpcScheduler instance
* @return MetricsProxy instance or null if creation fails
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getInstance,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getInstance(java.lang.String),410,418,"/**
* Returns a singleton instance of MetricsProxy for the given namespace.
* @param namespace unique identifier of metrics scope
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,call,"org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)",546,642,"/**
* Handles RPC calls for WritableRpc protocol.
* @param server the RPC server instance
* @param protocolName name of the protocol to handle
* @param rpcRequest RPC request object
* @return result of the RPC call or null if not found
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,processCall,"org.apache.hadoop.ipc.ProtobufRpcEngine$Server:processCall(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)",463,504,"/**
* Calls a blocking method on the server and returns the result.
* @throws RpcNoSuchMethodException if method is unknown
* @return RpcWritable object containing the response or null if deferred
*/","* This implementation is same as
     * ProtobufRpcEngine2.Server.ProtobufInvoker#call(..)
     * except this implementation uses non-shaded protobuf classes from legacy
     * protobuf version (default 2.5.0).",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,call,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)",599,641,"/**
* Calls a blocking method on the server with detailed metrics and potential deferred response.
* @param connectionProtocolName protocol name
* @param request RPC request message
* @param methodName method to call
* @param protocolImpl protocol implementation
* @return RPC response message or null if deferred
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,<init>,org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller:<init>(org.apache.hadoop.metrics2.lib.MutableRollingAverages),219,221,"/**
* Initializes RatesRoller with a parent RollingAverages instance. 
* @param parent reference to the parent rolling averages object
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,registerSink,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)",296,306,"/**
* Registers a metrics sink by name and configuration.
* @param name unique sink identifier
* @param desc sink description
* @param sink MetricsSink object to be registered
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,newSink,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:newSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.impl.MetricsConfig)",534,537,"/**
* Creates a new metrics sink adapter instance.
* @param name        name of the sink
* @param desc        description of the sink
* @param conf        configuration object containing plugin and other settings
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,add,"org.apache.hadoop.metrics2.lib.MutableRates:add(java.lang.String,long)",76,78,"/**
* Adds timing data to the registry with specified name and elapsed time. 
* @param name unique event identifier
* @param elapsed accumulated time in milliseconds
*/","* Add a rate sample for a rate metric
   * @param name of the rate metric
   * @param elapsed time",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,addPersistedDelegationToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)",404,433,"/**
* Adds a persisted delegation token to the SecretManager.
* @param identifier TokenIdent object
* @param renewDate expiration date of the token in milliseconds
* @throws IOException if SecretManager is running or token already exists
*/","* This method is intended to be used for recovering persisted delegation
   * tokens. Tokens that have an unknown <code>DelegationKey</code> are
   * marked as expired and automatically cleaned up.
   * This method must be called before this secret manager is activated (before
   * startThreads() is called)
   * @param identifier identifier read from persistent storage
   * @param renewDate token renew time
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,syncTokenOwnerStats,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:syncTokenOwnerStats(),952,957,"/**
* Updates token owner statistics by clearing and re-populating with current tokens.
*/","* This method syncs token information from currentTokens to tokenOwnerStats.
   * It is used when the currentTokens is initialized or refreshed. This is
   * called from a single thread thus no synchronization is needed.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeExpiredToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredToken(),762,780,"/**
* Removes expired tokens, updating owner statistics and logging the removal.
* @throws IOException if an error occurs during token cleanup
*/",Remove expired delegation tokens from cache,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,<init>,"org.apache.hadoop.ha.ActiveStandbyElector:<init>(java.lang.String,int,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback,int,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",226,233,"/**
* Initializes ActiveStandbyElector with ZooKeeper connection details.
* @param zookeeperHostPorts ZooKeeper host and port string
* @param zookeeperSessionTimeout session timeout in milliseconds
* @param parentZnodeName parent node name for election
* @param acl access control list for ZooKeeper operations
* @param authInfo authentication information for ZooKeeper connections
* @param app callback object for ActiveStandbyElector events
* @param maxRetryNum maximum retry count for ZooKeeper operations
* @param truststoreKeystore truststore and keystore configuration
*/","* Create a new ActiveStandbyElector object <br>
   * The elector is created by providing to it the Zookeeper configuration, the
   * parent znode under which to create the znode and a reference to the
   * callback interface. <br>
   * The parent znode name must be the same for all service instances and
   * different across services. <br>
   * After the leader has been lost, a new leader will be elected after the
   * session timeout expires. Hence, the app must set this parameter based on
   * its needs for failure response time. The session timeout must be greater
   * than the Zookeeper disconnect timeout and is recommended to be 3X that
   * value to enable Zookeeper to retry transient disconnections. Setting a very
   * short session timeout may result in frequent transitions between active and
   * standby states during issues like network outages/GS pauses.
   * 
   * @param zookeeperHostPorts
   *          ZooKeeper hostPort for all ZooKeeper servers
   * @param zookeeperSessionTimeout
   *          ZooKeeper session timeout
   * @param parentZnodeName
   *          znode under which to create the lock
   * @param acl
   *          ZooKeeper ACL's
   * @param authInfo a list of authentication credentials to add to the
   *                 ZK connection
   * @param app
   *          reference to callback interface object
   * @param maxRetryNum maxRetryNum.
   * @param truststoreKeystore truststore keystore, that we will use for ZK if SSL/TLS is enabled
   * @throws IOException raised on errors performing I/O.
   * @throws HadoopIllegalArgumentException
   *         if valid data is not supplied.
   * @throws KeeperException
   *         other zookeeper operation errors.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,joinElection,org.apache.hadoop.ha.ActiveStandbyElector:joinElection(byte[]),315,334,"/**
* Joins an election process with the provided data.
* @param data byte array containing election data
*/","* To participate in election, the app will call joinElection. The result will
   * be notified by a callback on either the becomeActive or becomeStandby app
   * interfaces. <br>
   * After this the elector will automatically monitor the leader status and
   * perform re-election if necessary<br>
   * The app could potentially start off in standby mode and ignore the
   * becomeStandby call.
   * 
   * @param data
   *          to be set by the app. non-null data must be set.
   * @throws HadoopIllegalArgumentException
   *           if valid data is not supplied",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reJoinElection,org.apache.hadoop.ha.ActiveStandbyElector:reJoinElection(int),798,823,"/**
* Re-establishes ZK session and attempts to join election after a specified sleep time.
* @param sleepTime delay before joining election
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String)",200,204,"/**
* Creates an InetSocketAddress from given target and port.
* @param target host or IP address
* @param defaultPort default port number
*/","* Create an InetSocketAddress from the given target string and
   * default port. If the string cannot be parsed correctly, the
   * <code>configName</code> parameter is used as part of the
   * exception message, allowing the user to better diagnose
   * the misconfiguration.
   *
   * @param target a string of either ""host"" or ""host:port""
   * @param defaultPort the default port if <code>target</code> does not
   *                    include a port number
   * @param configName the name of the configuration from which
   *                   <code>target</code> was loaded. This is used in the
   *                   exception message in the case that parsing fails.
   * @return socket addr.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,invoke,"org.apache.hadoop.ipc.WritableRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",232,261,"/**
* Invokes a remote procedure on the client.
* @param proxy unused
* @param method method to invoke
* @param args invocation arguments
* @return result of RPC or null if not found
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,invoke,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",212,294,"/**
* Invokes a remote method using RPC protocol.
* @param proxy ignored
* @param method the Method to invoke
* @param args array containing RpcController and Message
* @return Message response from server or null in asynchronous mode
*/","* This is the client side invoker of RPC method. It only throws
     * ServiceException, since the invocation proxy expects only
     * ServiceException to be thrown by the method in case protobuf service.
     * 
     * ServiceException has the following causes:
     * <ol>
     * <li>Exceptions encountered on the client side in this method are 
     * set as cause in ServiceException as is.</li>
     * <li>Exceptions from the server are wrapped in RemoteException and are
     * set as cause in ServiceException</li>
     * </ol>
     * 
     * Note that the client calling protobuf RPC methods, must handle
     * ServiceException by getting the cause from the ServiceException. If the
     * cause is RemoteException, then unwrap it to get the exception thrown by
     * the server.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,invoke,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",220,304,"/**
* Invokes a remote method and returns the result.
* @param proxy an object representing the service
* @param method the method to invoke
* @param args parameters for the method invocation
* @return the method's return value or null if asynchronous
*/","* This is the client side invoker of RPC method. It only throws
     * ServiceException, since the invocation proxy expects only
     * ServiceException to be thrown by the method in case protobuf service.
     *
     * ServiceException has the following causes:
     * <ol>
     * <li>Exceptions encountered on the client side in this method are
     * set as cause in ServiceException as is.</li>
     * <li>Exceptions from the server are wrapped in RemoteException and are
     * set as cause in ServiceException</li>
     * </ol>
     *
     * Note that the client calling protobuf RPC methods, must handle
     * ServiceException by getting the cause from the ServiceException. If the
     * cause is RemoteException, then unwrap it to get the exception thrown by
     * the server.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,parseExpression,org.apache.hadoop.fs.shell.find.Find:parseExpression(java.util.Deque),272,332,"/**
* Parses an expression from a deque of tokens.
* @param args deque of tokens representing the expression
* @return parsed Expression object or null if invalid input
*/","* Parse a list of arguments to to extract the {@link Expression} elements.
   * The input Deque will be modified to remove the used elements.
   * 
   * @param args arguments to be parsed
   * @return list of {@link Expression} elements applicable to this command
   * @throws IOException if list can not be parsed",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printUsage,org.apache.hadoop.fs.FsShell:printUsage(java.io.PrintStream),193,195,"/**
 * Prints usage information to the specified output stream.
 * @param out target output stream (e.g. System.out)
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printUsage,"org.apache.hadoop.fs.FsShell:printUsage(java.io.PrintStream,java.lang.String)",198,200,"/**
 * Prints usage information to the specified output stream.
 * @param out target output stream
 * @param cmd command whose usage is being printed
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printHelp,org.apache.hadoop.fs.FsShell:printHelp(java.io.PrintStream),203,205,"/**
* Prints help information to the specified output stream.
* @param out target output stream
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printHelp,"org.apache.hadoop.fs.FsShell:printHelp(java.io.PrintStream,java.lang.String)",208,210,"/**
* Prints help information to the specified output stream.
* @param out PrintStream to write to
* @param cmd Command name (not used in this implementation)
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,get,org.apache.hadoop.io.WritableComparator:get(java.lang.Class),55,57,"/**
* Returns a WritableComparator instance for the given class.
* @param c Class of the WritableComparable to be compared
*/","* For backwards compatibility.
   *
   * @param c WritableComparable Type.
   * @return WritableComparator.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ByteWritable.java,<init>,org.apache.hadoop.io.ByteWritable$Comparator:<init>(),88,90,"/**
 * Initializes the comparator with ByteWritable class as its type.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IntWritable.java,<init>,org.apache.hadoop.io.IntWritable$Comparator:<init>(),90,92,"/**
* Initializes a Comparator instance with IntWritable class as the key type.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,org.apache.hadoop.io.WritableComparator:<init>(),124,126,"/**
 * Initializes a new instance of WritableComparator with default settings.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text$Comparator:<init>(),429,431,"/**
* Initializes comparator with Text class as the comparison type.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/NullWritable.java,<init>,org.apache.hadoop.io.NullWritable$Comparator:<init>(),62,64,"/**
 * Initializes the comparator with NullWritable as its key.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/LongWritable.java,<init>,org.apache.hadoop.io.LongWritable$Comparator:<init>(),90,92,"/**
* Initializes comparator with LongWritable class as key type.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DoubleWritable.java,<init>,org.apache.hadoop.io.DoubleWritable$Comparator:<init>(),88,90,"/**
* Constructor to initialize comparator with DoubleWritable class.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,<init>,org.apache.hadoop.io.MD5Hash$Comparator:<init>(),249,251,"/**
* Initializes MD5 hash comparator.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ShortWritable.java,<init>,org.apache.hadoop.io.ShortWritable$Comparator:<init>(),98,100,"/**
* Initializes custom comparator with ShortWritable class.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/FloatWritable.java,<init>,org.apache.hadoop.io.FloatWritable$Comparator:<init>(),85,87,"/**
* Initializes comparator with custom comparison type (FloatWritable).",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,<init>,org.apache.hadoop.io.BytesWritable$Comparator:<init>(),224,226,"/**
* Constructs a comparator for BytesWritable objects.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BooleanWritable.java,<init>,org.apache.hadoop.io.BooleanWritable$Comparator:<init>(),111,113,"/**
 * Initializes comparator with BooleanWritable as its target class.",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,<init>,org.apache.hadoop.io.UTF8$Comparator:<init>(),212,214,"/**
 * Initializes comparator with UTF-8 encoding. 
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,authenticate,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",380,410,"/**
* Authenticates user with delegation token or falls back to configured handler.
* @param request HttpServletRequest object
* @param response HttpServletResponse object
* @return AuthenticationToken object or null on failure
*/","* Authenticates a request looking for the <code>delegation</code>
   * query-string parameter and verifying it is a valid token. If there is not
   * <code>delegation</code> query-string parameter, it delegates the
   * authentication to the {@link KerberosAuthenticationHandler} unless it is
   * disabled.
   *
   * @param request the HTTP client request.
   * @param response the HTTP client response.
   * @return the authentication token for the authenticated request.
   * @throws IOException thrown if an IO error occurred.
   * @throws AuthenticationException thrown if the authentication failed.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,<init>,"org.apache.hadoop.ha.NodeFencer:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)",78,81,"/**
* Initializes NodeFencer with configuration and specification.
* @param conf Configuration object
* @param spec Specification string
* @throws BadFencingConfigurationException if initialization fails",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleDeprecation,"org.apache.hadoop.conf.Configuration:handleDeprecation(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String)",724,762,"/**
* Resolves deprecation of a given name and updates properties accordingly.
* @param deprecations context for handling deprecation
* @param name the name to resolve deprecation for
* @return array of resolved names or null if not found
*/","* Checks for the presence of the property <code>name</code> in the
   * deprecation map. Returns the first of the list of new keys if present
   * in the deprecation map or the <code>name</code> itself. If the property
   * is not presently set but the property map contains an entry for the
   * deprecated key, the value of the deprecated key is set as the value for
   * the provided property name.
   *
   * Also updates properties and overlays with deprecated keys, if the new
   * key does not already exist.
   *
   * @param deprecations deprecation context
   * @param name the property name
   * @return the first property in the list of properties mapping
   *         the <code>name</code> or the <code>name</code> itself.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,setConfAsEnvVars,org.apache.hadoop.ha.ShellCommandFencer:setConfAsEnvVars(java.util.Map),207,211,"/**
* Converts configuration as key-value pairs to environment variables.
* @param env map of environment variable names and values
*/","* Set the environment of the subprocess to be the Configuration,
   * with '.'s replaced by '_'s.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,setHeaders,org.apache.hadoop.http.HttpServer2:setHeaders(org.apache.hadoop.conf.Configuration),1958,1970,"/**
* Configures HTTP headers based on provided configuration.
* @param conf Configuration object
* @return Map of header name-value pairs
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HttpCrossOriginFilterInitializer.java,getFilterParameters,"org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getFilterParameters(org.apache.hadoop.conf.Configuration,java.lang.String)",54,65,"/**
* Extracts configuration parameters starting with the given prefix.
* @param conf Hadoop Configuration object
* @param prefix key prefix to filter by
* @return Map of filtered parameter names and values
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,processArguments,org.apache.hadoop.fs.shell.SetReplication:processArguments(java.util.LinkedList),74,79,"/**
* Processes command line arguments and waits for replication if specified.
* @param args list of PathData objects representing input files
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.DelegateToFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",117,122,"/**
* Retrieves file block locations.
* @param f Path to the file
* @param start starting offset in bytes
* @param len length of the range
* @return array of BlockLocation objects or empty array if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",1512,1535,"/**
* Resolves file block locations for the given FileStatus and offset range.
* @param fs FileStatus object
* @param start starting offset
* @param len length of offset range
* @return BlockLocation array or throws FileNotFoundException if invalid path
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,renewToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)",190,196,"/**
* Renew a delegationToken using the specified renewer.
* @param token Token object to be renewed
* @param renewer identifier of the entity performing renewal
* @return new token value or throws IOException if failed
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",198,206,"/**
* Cancels a token with the given canceler.
* @param token Token object to be cancelled
* @param canceler User ID or name of the user cancelling the token
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",154,164,"/**
* Cancels a token with the given identifier and canceller.
* @param token Token object containing unique identifier
* @param canceller Cancellation reason string
* @return cancelled TokenIdent object or null if failed
*/","* Cancels a token by removing it from the SQL database. This will
   * call the corresponding method in {@link AbstractDelegationTokenSecretManager}
   * to perform validation and remove the token from the cache.
   * @return Identifier of the canceled token",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,getGroups,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String),52,58,"/**
* Retrieves Unix groups for a given user.
* @param user unique user identifier
* @return list of group names or empty list if not found
*/","* Get unix groups (parent) and netgroups for given user
   *
   * @param user get groups and netgroups for this user
   * @return groups and netgroups for user",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTar,"org.apache.hadoop.fs.FileUtil:unTar(java.io.File,java.io.File)",1015,1033,"/**
* Extracts contents of a tar archive (gzipped or not) into the specified directory.
* @param inFile path to the tar archive file
* @param untarDir target directory for extraction
*/","* Given a Tar File as input it will untar the file in a the untar directory
   * passed as the second parameter
   *
   * This utility will untar "".tar"" files and "".tar.gz"",""tgz"" files.
   *
   * @param inFile The tar file as input.
   * @param untarDir The untar directory where to untar the tar file.
   * @throws IOException an exception occurred.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,chmod,"org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String)",1289,1292,"/**
* Sets file permissions using Linux's chmod command.
* @param filename path to the file
* @param perm desired permissions as a string (e.g. ""755"")
*/","* Change the permissions on a filename.
   * @param filename the name of the file to change
   * @param perm the permission string
   * @return the exit code from the command
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException command interrupted.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setReadable,"org.apache.hadoop.fs.FileUtil:setReadable(java.io.File,boolean)",1347,1359,"/**
* Sets the readibility of a file.
* @param f File object
* @param readable True for readable, False otherwise
* @return true if operation succeeded, false on failure
*/","* Platform independent implementation for {@link File#setReadable(boolean)}
   * File#setReadable does not work as expected on Windows.
   * @param f input file
   * @param readable readable.
   * @return true on success, false otherwise",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setWritable,"org.apache.hadoop.fs.FileUtil:setWritable(java.io.File,boolean)",1368,1380,"/**
* Sets file writability on Windows or uses native method on other platforms.
* @param f target File object
* @param writable true for write permission, false otherwise
* @return true if operation was successful, false otherwise
*/","* Platform independent implementation for {@link File#setWritable(boolean)}
   * File#setWritable does not work as expected on Windows.
   * @param f input file
   * @param writable writable.
   * @return true on success, false otherwise",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setExecutable,"org.apache.hadoop.fs.FileUtil:setExecutable(java.io.File,boolean)",1392,1404,"/**
* Sets file executability based on platform.
* @param f File object to modify
* @param executable true for executable, false otherwise
* @return true if successful, false otherwise
*/","* Platform independent implementation for {@link File#setExecutable(boolean)}
   * File#setExecutable does not work as expected on Windows.
   * Note: revoking execute permission on folders does not have the same
   * behavior on Windows as on Unix platforms. Creating, deleting or renaming
   * a file within that folder will still succeed on Windows.
   * @param f input file
   * @param executable executable.
   * @return true on success, false otherwise",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,refreshIfNeeded,org.apache.hadoop.util.SysInfoWindows:refreshIfNeeded(),94,141,"/**
* Refreshes system info if the refresh interval has passed.
* @throws Exception if unable to parse system info
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,loadPermissionInfo,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfo(),983,995,"/**
* Loads permission info using native or non-native IO, 
* depending on availability and previous loading status.
*/","* Load file permission information (UNIX symbol rwxrwxrwx, sticky bit info).
     *
     * To improve peformance, give priority to native stat() call. First try get
     * permission information by using native JNI call then fall back to use non
     * native (ProcessBuilder) call in case native lib is not loaded or native
     * call is not successful",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setOwner,"org.apache.hadoop.fs.RawLocalFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1099,1103,"/**
* Sets file owner and group using specified Path and usernames.
* @param p the Path to set ownership for
* @param username the user name to set as owner
* @param groupname the group name to set as group owner
*/",* Use the command chown to set owner.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setPermission,"org.apache.hadoop.fs.FileUtil:setPermission(java.io.File,org.apache.hadoop.fs.permission.FsPermission)",1470,1508,"/**
* Sets file permissions based on provided FsPermission object.
* @param f the File to set permissions for
* @param permission FsPermission object containing user/group/other actions
*/","* Set permissions to the required value. Uses the java primitives instead
   * of forking if group == other.
   * @param f the file to change
   * @param permission the new permissions
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,cacheGroupsAdd,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsAdd(java.util.List),75,88,"/**
* Adds groups to cache, handling both netgroups and Unix groups.
* @param groups list of group names to add
*/","* Add a group to cache, only netgroups are cached
   *
   * @param groups list of group names to add to cache",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dumpTokens,org.apache.hadoop.security.KDiag:dumpTokens(org.apache.hadoop.security.UserGroupInformation),811,819,"/**
* Dumps user's authentication tokens.
* @param ugi UserGroupInformation object
*/","* Dump all tokens of a UGI.
   * @param ugi UGI to examine",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logUserInfo,"org.apache.hadoop.security.UserGroupInformation:logUserInfo(org.slf4j.Logger,java.lang.String,org.apache.hadoop.security.UserGroupInformation)",1980,1991,"/**
* Logs user information with credentials.
* @param log logging instance
* @param caption descriptive message prefix
* @param ugi UserGroupInformation object containing credentials
*/","* Log current UGI and token information into specified log.
   * @param ugi - UGI
   * @param log log.
   * @param caption caption.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,containsKmsDt,org.apache.hadoop.crypto.key.kms.KMSClientProvider:containsKmsDt(org.apache.hadoop.security.UserGroupInformation),1155,1165,"/**
* Checks if the provided UGI contains a valid KMS delegation token.
* @param ugi UserGroupInformation instance to search
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDir,"org.apache.hadoop.util.DiskChecker:checkDir(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",113,117,"/**
* Verifies directory existence and permissions.
* @param localFS Local file system instance
* @param dir Directory path to verify
* @param expected Expected permissions for the directory
*/","* Create the local directory if necessary, check permissions and also ensure
   * it can be read from and written into.
   *
   * @param localFS local filesystem
   * @param dir directory
   * @param expected permission
   * @throws DiskErrorException disk problem.
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirWithDiskIo,"org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",131,136,"/**
* Verifies directory existence and permissions with disk I/O.
* @param localFS Local file system instance
* @param dir Path to the directory to verify
* @param expected Expected FsPermission value
*/","* Create the local directory if necessary, also ensure permissions
   * allow it to be read from and written into. Perform some diskIO
   * to ensure that the disk is usable for writes. 
   *
   * @param localFS local filesystem
   * @param dir directory
   * @param expected permission
   * @throws DiskErrorException disk problem.
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[]),1823,1826,"/**
* Lists status of multiple files.
* @param files array of file paths
*/","* See {@link #listStatus(Path[], PathFilter)}
     *
     * @param files files.
     * @throws AccessControlException If access is denied.
     * @throws FileNotFoundException If <code>files</code> does not exist.
     * @throws IOException If an I/O error occurred.
     * @return file status array.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getEnclosingRoot,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getEnclosingRoot(org.apache.hadoop.fs.Path),1481,1496,"/**
* Resolves the enclosing root of a given path.
* @param path input path to resolve
* @return Enclosing root path or original path if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,append,"org.apache.hadoop.fs.viewfs.ViewFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",447,453,"/**
* Appends data to a file in the specified FSDataOutputStream.
* @param f Path to the target file
* @param bufferSize buffer size for I/O operations
* @param progress Progressable object for tracking progress
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.viewfs.ViewFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",455,468,"/**
* Creates a non-recursive FSDataOutputStream for the given file.
* @param f Path to the file
* @return FSDataOutputStream instance or throws IOException if creation fails
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,create,"org.apache.hadoop.fs.viewfs.ViewFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",470,483,"/**
* Creates a new output stream for the given path with specified FS permissions.
* @param f Path to create output stream for
* @param permission File system permissions
* @param overwrite Whether to overwrite existing file
* @param bufferSize Buffer size for I/O operations
* @param replication Data replication factor
* @param blockSize Block size for data storage
* @param progress Progressable object for tracking progress
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,"org.apache.hadoop.fs.viewfs.ViewFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",486,496,"/**
* Deletes a file or directory.
* @param f the Path to delete
* @param recursive whether to delete recursively
* @return true if deletion is successful, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),514,521,"/**
* Retrieves file checksum for the given path.
* @param f Path to file
* @return FileChecksum object or throws exception if access fails
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileChecksum,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",523,530,"/**
* Retrieves file checksum by path and length.
* @param f Path to the file
* @param length File length in bytes
* @return FileChecksum object or throws exception if error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listLocatedStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",630,655,"/**
* Lists located file statuses for a given path and filter.
* @param f path to list located statuses from
* @param filter optional filter to apply to results
* @return iterator over LocatedFileStatus objects or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,mkdirs,org.apache.hadoop.fs.viewfs.ViewFileSystem:mkdirs(org.apache.hadoop.fs.Path),670,675,"/**
* Creates a directory at the specified path.
* @param dir directory to create
* @return true if successful, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.ViewFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",677,683,"/**
* Creates a directory with specified permissions.
* @param dir path to create
* @param permission FsPermission object for access control
* @return true if created, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,open,"org.apache.hadoop.fs.viewfs.ViewFileSystem:open(org.apache.hadoop.fs.Path,int)",685,691,"/**
* Opens a FSDataInputStream for the given file path with specified buffer size.
* @param f Path to the file
* @param bufferSize Size of the input stream buffer
* @return Opened FSDataInputStream or throws exception if access is denied or file not found.",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFileSystem:truncate(org.apache.hadoop.fs.Path,long)",800,806,"/**
* Truncates a file to the specified length.
* @param f Path to truncate
* @param newLength desired file size in bytes
* @return true if truncation was successful, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setOwner,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",808,815,"/**
* Sets owner and group for the specified file path.
* @param f target file path
* @param username new owner name
* @param groupname new group name
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setPermission,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",817,823,"/**
* Sets file system permissions for a given path.
* @param f the target file or directory
* @param permission desired file system permissions
* @throws AccessControlException if access denied
* @throws FileNotFoundException if path not found
* @throws IOException on I/O error
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setReplication,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",825,831,"/**
* Sets the replication factor for a file.
* @param f Path to the file
* @param replication new replication value (short)
* @return true if successful, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setTimes,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",833,839,"/**
* Sets file timestamps for the specified path.
* @param f the file to update
* @param mtime modification time in milliseconds
* @param atime access time in milliseconds
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",841,847,"/**
* Modifies ACL entries for a file system path.
* @param path the file system path
* @param aclSpec list of ACL entries to modify
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",849,855,"/**
* Removes ACL entries for the specified path.
* @param path file system path
* @param aclSpec list of ACL entries to remove
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path),857,863,"/**
* Removes default access control list (ACL) from the specified file system location.
* @param path file system path to remove ACL from
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem:removeAcl(org.apache.hadoop.fs.Path),865,871,"/**
* Removes ACL from the specified file system path.
* @param path file system path
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",873,878,"/**
* Sets ACL for a file system path.
* @param path file system path
* @param aclSpec list of ACL entries to apply
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getAclStatus(org.apache.hadoop.fs.Path),880,885,"/**
* Retrieves ACL status for the given file system path.
* @param path filesystem path to retrieve ACL status for
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",887,893,"/**
* Sets an extended attribute on a file system path.
* @param path file system path
* @param name attribute name
* @param value attribute value as byte array
* @param flag XAttrSetFlag indicating operation flags
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",895,900,"/**
* Retrieves the XATTR (extended attribute) with the given name from the file system.
* @param path file or directory to access
* @param name name of the XATTR to retrieve
* @return byte array containing the XATTR value or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttrs(org.apache.hadoop.fs.Path),902,907,"/**
* Retrieves extended attributes for a file at the specified path.
* @param path file system path
* @return Map of attribute names to values or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",909,915,"/**
* Retrieves extended attributes for the given file path.
* @param path file system path
* @param names list of attribute names to fetch
* @return map of attribute names to their corresponding byte arrays
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem:listXAttrs(org.apache.hadoop.fs.Path),917,922,"/**
* Lists extended attributes for the given file system path.
* @param path file system path to list attributes for
* @return List of attribute names or empty list if none exist
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",924,929,"/**
* Removes extended attribute with specified name from given file system location.
* @param path file system location
* @param name name of the extended attribute to remove
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),990,1002,"/**
* Retrieves default replication setting from the target file system.
* @param f Path to the file for which to retrieve the replication setting
* @return Default replication value (short)
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getContentSummary,org.apache.hadoop.fs.viewfs.ViewFileSystem:getContentSummary(org.apache.hadoop.fs.Path),1015,1020,"/**
* Retrieves content summary for a given file path.
* @param f the file path to fetch summary for
* @return ContentSummary object or throws IOException if an error occurs",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getQuotaUsage,org.apache.hadoop.fs.viewfs.ViewFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path),1022,1027,"/**
* Retrieves quota usage for the given file path.
* @param f Path to query quota usage for
* @return QuotaUsage object or throws IOException if error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1072,1078,"/**
* Creates a file system snapshot at the given path with the specified name.
* @param path Path to create the snapshot for
* @param snapshotName Name of the new snapshot
* @return The created snapshot path
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1080,1087,"/**
* Renames a snapshot on the file system.
* @param path directory containing the snapshot
* @param snapshotOldName current name of the snapshot
* @param snapshotNewName new name for the snapshot
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1089,1095,"/**
* Deletes a snapshot by name in the target file system.
* @param path directory containing the snapshot
* @param snapshotName name of the snapshot to delete
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path),1097,1102,"/**
* Resolves storage policy for the given file path.
* @param src Path to resolve
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",1104,1109,"/**
* Sets storage policy for the specified file or directory.
* @param src path to file or directory
* @param policyName name of storage policy to apply
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path),1111,1116,"/**
* Removes storage policy from the specified file or directory.
* @param src Path to the target resource
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path),1118,1123,"/**
* Retrieves the storage policy for a given file path.
* @param src file system path
* @return BlockStoragePolicySpi instance or throws IOException if failed
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getStatus(org.apache.hadoop.fs.Path),1304,1312,"/**
* Retrieves the status of a file system at the specified path.
* @param p Path to the file system
* @return FsStatus object or throws IOException if an error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getUsed,org.apache.hadoop.fs.viewfs.ViewFileSystem:getUsed(),1321,1330,"/**
* Retrieves the used space for the file system.
* @throws IOException if an I/O error occurs
*/","* Return the total size of all files under ""/"", if {@link
   * Constants#CONFIG_VIEWFS_LINK_MERGE_SLASH} is supported and is a valid
   * mount point. Else, throw NotInMountpointException.
   *
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ViewFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),1332,1341,"/**
* Resolves link target for the given path.
* @param path Path object to resolve
* @return Path to the linked file or directory
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.HarFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",468,481,"/**
* Retrieves block locations for a file range.
* @param file the original file
* @param start the starting offset of the requested range
* @param len the length of the requested range
* @return an array of BlockLocation objects or null if not found
*/","* Get block locations from the underlying fs and fix their
   * offsets and lengths.
   * @param file the input file status to get block locations
   * @param start the start of the desired range in the contained file
   * @param len the length of the desired range
   * @return block locations for this segment of file
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,open,"org.apache.hadoop.fs.HarFileSystem:open(org.apache.hadoop.fs.Path,int)",679,690,"/**
* Opens the underlying file for reading.
* @param f path to the file
* @param bufferSize buffer size for data read operations
* @return FSDataInputStream object or throws IOException/FNFE if invalid
*/","* Returns a har input stream which fakes end of 
   * file. It reads the index files to get the part 
   * file name and the size and start of the file.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path)",148,151,"/**
* Initializes checker with checksum FS and file path.
* @param fs Checksum FS instance
* @param file File to check
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,open,"org.apache.hadoop.fs.ChecksumFs:open(org.apache.hadoop.fs.Path,int)",334,339,"/**
* Opens a file stream for reading with specified buffer size.
* @param f Path to the file
* @param bufferSize Size of the read buffer
* @return FSDataInputStream object or throws IOException/UnresolvedLinkException if failed
*/","* Opens an FSDataInputStream at the indicated Path.
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,createInternal,"org.apache.hadoop.fs.ChecksumFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",418,428,"/**
* Creates a file output stream with specified permissions and settings.
* @param f the Path to the file
* @param createFlag flags for creating the file
* @return FSDataOutputStream instance or throws IOException on failure
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,processArguments,org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:processArguments(java.util.LinkedList),81,94,"/**
* Initializes and processes command-line arguments, potentially creating a thread pool.
* @param args list of PathData objects representing input files
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,open,"org.apache.hadoop.fs.ChecksumFileSystem:open(org.apache.hadoop.fs.Path,int)",566,578,"/**
* Opens a file stream with optional checksum verification.
* @param f the Path to open
* @param bufferSize the buffer size for reading
* @return an FSDataInputStream object or null if not found
*/","* Opens an FSDataInputStream at the indicated Path.
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.
   * @throws IOException if an I/O error occurs.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,create,"org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable)",704,734,"/**
* Creates a new FSDataOutputStream for writing to the specified file,
* handling parent directory creation and permissions as needed.
* @param f path to the output file
* @param permission file system permissions
* @param overwrite whether to overwrite existing files
* @param createParent whether to create non-existent parent directories
* @param bufferSize I/O buffer size
* @param replication data replication factor (if writing checksums)
* @param blockSize block size for checksums (if writing checksums)
* @param progress progress monitor (if writing checksums)
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,isValidName,org.apache.hadoop.fs.viewfs.ChRootedFs:isValidName(java.lang.String),97,100,"/**
* Validates user name by checking its format against file system rules.
* @param src user-provided name as string
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,createInternal,"org.apache.hadoop.fs.viewfs.ChRootedFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",173,182,"/**
* Wraps FSDataOutputStream creation with additional path processing.
* @param f file path
* @param flag CreateFlag enum value
* @throws IOException I/O exception
* @throws UnresolvedLinkException unresolved link exception
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,delete,"org.apache.hadoop.fs.viewfs.ChRootedFs:delete(org.apache.hadoop.fs.Path,boolean)",184,188,"/**
* Deletes a file or directory by path.
* @param f file or directory to delete
* @param recursive whether to delete recursively
* @return true if deleted successfully, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ChRootedFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",190,194,"/**
* Retrieves file block locations for the given path and size range.
* @param f file path
* @param start starting offset in bytes
* @param len length of data to retrieve in bytes
* @return array of BlockLocation objects or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ChRootedFs:getFileChecksum(org.apache.hadoop.fs.Path),196,200,"/**
* Retrieves file checksum by path.
* @param f file path to check
* @return FileChecksum object or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:getFileStatus(org.apache.hadoop.fs.Path),202,206,"/**
* Retrieves file status information from HDFS.
* @param path file path to retrieve status for
* @return FileStatus object or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileLinkStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:getFileLinkStatus(org.apache.hadoop.fs.Path),213,217,"/**
* Retrieves file link status by resolving path to full path.
* @param f the input file path
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ChRootedFs:getServerDefaults(org.apache.hadoop.fs.Path),230,233,"/**
* Retrieves server defaults from file system.
* @param f path to file containing server defaults
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:listStatus(org.apache.hadoop.fs.Path),240,244,"/**
* Lists file statuses for a given path.
* @param f input path to list statuses for
* @return array of FileStatus objects or null if an error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listStatusIterator,org.apache.hadoop.fs.viewfs.ChRootedFs:listStatusIterator(org.apache.hadoop.fs.Path),246,250,"/**
* Returns an iterator over file status objects for the given path.
* @param f file system path to iterate over
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:listLocatedStatus(org.apache.hadoop.fs.Path),252,256,"/**
* Lists located file statuses in the given directory.
* @param f directory path to list located statuses for
* @return iterator of LocatedFileStatus objects or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,mkdir,"org.apache.hadoop.fs.viewfs.ChRootedFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",258,263,"/**
* Creates a new directory with specified permissions.
* @param dir directory path
* @param permission file system permissions
* @param createParent whether to create parent directories if needed
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,open,"org.apache.hadoop.fs.viewfs.ChRootedFs:open(org.apache.hadoop.fs.Path,int)",265,269,"/**
* Opens a file stream from the given path with specified buffer size.
* @param f file path to open
* @param bufferSize size of the read/write buffer
* @return FSDataInputStream object for reading the file
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,truncate,"org.apache.hadoop.fs.viewfs.ChRootedFs:truncate(org.apache.hadoop.fs.Path,long)",271,275,"/**
* Truncates a file to a specified length.
* @param f the file path
* @param newLength the desired file size in bytes
* @return true if truncation was successful, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",277,283,"/**
* Renames an internal file or directory within the file system.
* @param src original path to be renamed
* @param dst new path for the renamed item
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",285,292,"/**
* Renames an internal file or directory, overwriting if specified.
* @param src source path to rename
* @param dst destination path
* @param overwrite whether to overwrite existing destination
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setOwner,"org.apache.hadoop.fs.viewfs.ChRootedFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",294,300,"/**
* Sets owner of a file or directory.
* @param f file or directory path
* @param username new owner's username
* @param groupname new owner's group name
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setPermission,"org.apache.hadoop.fs.viewfs.ChRootedFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",302,306,"/**
* Sets file system permissions on a given file.
* @param f file path to modify
* @param permission FS permissions to apply
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setReplication,"org.apache.hadoop.fs.viewfs.ChRootedFs:setReplication(org.apache.hadoop.fs.Path,short)",308,312,"/**
* Sets file replication on the file system.
* @param f file path
* @param replication new replication value (short)
* @return true if successful, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setTimes,"org.apache.hadoop.fs.viewfs.ChRootedFs:setTimes(org.apache.hadoop.fs.Path,long,long)",314,318,"/**
* Sets file timestamps (mtime and atime).
* @param f Path to the file
* @param mtime modification time in milliseconds
* @param atime access time in milliseconds
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",320,324,"/**
* Modifies ACL entries on a file system resource.
* @param path file system path to modify
* @param aclSpec list of AclEntry modifications
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",326,330,"/**
* Removes ACL entries from the file system at the specified path.
* @param path File system path
* @param aclSpec list of AclEntry objects to remove
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ChRootedFs:removeDefaultAcl(org.apache.hadoop.fs.Path),332,335,"/**
* Removes default ACL from specified file or directory.
* @param path Path to remove ACL from
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeAcl,org.apache.hadoop.fs.viewfs.ChRootedFs:removeAcl(org.apache.hadoop.fs.Path),337,340,"/**
 * Removes Access Control List (ACL) from the specified file.
 * @param path path to the file
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setAcl,"org.apache.hadoop.fs.viewfs.ChRootedFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",342,345,"/**
* Sets ACL (Access Control List) on a file system object.
* @param path the file system object to modify
* @param aclSpec list of ACL entries to apply
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getAclStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:getAclStatus(org.apache.hadoop.fs.Path),347,350,"/**
* Retrieves ACL status for a given file system path.
* @param path file system path to check
* @return AclStatus object or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",352,356,"/**
* Sets extended attribute on a file.
* @param path file path
* @param name attribute name
* @param value attribute value
* @param flag set flags for XATTR access
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",358,361,"/**
* Retrieves extended attribute by name from the file system.
* @param path file system path
* @param name extended attribute name
* @return extended attribute value as a byte array or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttrs(org.apache.hadoop.fs.Path),363,366,"/**
* Retrieves extended attributes from a file system location.
* @param path Path to the file system location
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",368,372,"/**
* Retrieves extended attributes by name from the file system.
* @param path file path to query
* @param names list of attribute names to retrieve
* @return map of attribute names to their corresponding byte values or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFs:listXAttrs(org.apache.hadoop.fs.Path),374,377,"/**
* Lists extended attributes of a file or directory.
* @param path Path object representing the target resource
* @return List of attribute names or empty list if none found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",379,382,"/**
* Removes extended attribute by name from file system object.
* @param path file system object path
* @param name extended attribute name to delete
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",384,387,"/**
* Creates a snapshot of the specified file system location.
* @param path existing file system path to snapshot
* @param name unique name for the snapshot
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",389,393,"/**
* Renames a snapshot on the file system.
* @param path directory containing the snapshot
* @param snapshotOldName original name of the snapshot
* @param snapshotNewName new name for the snapshot
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",395,399,"/**
* Deletes a Hadoop snapshot by path and name.
* @param snapshotDir directory containing the snapshot
* @param snapshotName unique identifier of the snapshot to delete
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ChRootedFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",406,410,"/**
* Sets storage policy for specified directory.
* @param path directory path
* @param policyName policy name to apply
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),412,416,"/**
* Unsets storage policy for the specified file system path.
* @param src the file system path to unset storage policy from
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,createSymlink,"org.apache.hadoop.fs.viewfs.ChRootedFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",441,451,"/**
* Creates a symbolic link from target to link with specified parent directory creation behavior.
*@param target path of the file or directory being linked
*@param link path for the new symbolic link
*@param createParent whether to create parent directories if missing
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ChRootedFs:getLinkTarget(org.apache.hadoop.fs.Path),453,456,"/**
* Retrieves the target of a symbolic link.
* @param f the link to resolve
* @return the resolved file path or null if not found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",568,636,"/**
 * Renames a file or directory from one internal path to another within the same mount table.
 * @param src source path
 * @param dst destination path
 * @param overwrite whether to overwrite existing files at destination
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,next,org.apache.hadoop.fs.viewfs.ViewFs$WrappingRemoteIterator:next(),945,952,"/**
* Returns the next file status in the iteration sequence.
* @return File status object or throws IOException if an error occurs.",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getChrootedPath,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getChrootedPath(org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",657,668,"/**
* Returns a chrooted path by prepending the resolved target file system's root.
* @param res ResolveResult containing target file system
* @param status FileStatus object
* @param f original file path
* @return qualified Path object or null if an error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,renameInternal,"org.apache.hadoop.fs.ChecksumFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",480,497,"/**
* Renames a file or directory internally.
* @param src original path
* @param dst new path
*/",* Rename files/dirs.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,renameInternal,"org.apache.hadoop.fs.ChecksumFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",499,523,"/**
* Renames a file or directory and its associated checksum file (if exists).
* @param src source path
* @param dst destination path
* @param overwrite whether to overwrite existing files
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,rename,"org.apache.hadoop.fs.FileContext:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",1031,1060,"/**
* Renames a file or directory to a new location within the same AbstractFileSystem.
* @param src path of the source file/directory
* @param dst path of the destination file/directory
* @param options rename options (e.g., overwrite)
*/","* Renames Path src to Path dst
   * <ul>
   * <li>Fails if src is a file and dst is a directory.
   * <li>Fails if src is a directory and dst is a file.
   * <li>Fails if the parent of dst does not exist or is a file.
   * </ul>
   * <p>
   * If OVERWRITE option is not passed as an argument, rename fails if the dst
   * already exists.
   * <p>
   * If OVERWRITE option is passed as an argument, rename overwrites the dst if
   * it is a file or an empty directory. Rename fails if dst is a non-empty
   * directory.
   * <p>
   * Note that atomicity of rename is dependent on the file system
   * implementation. Please refer to the file system documentation for details
   * <p>
   * 
   * @param src path to be renamed
   * @param dst new path after rename
   * @param options rename options.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If <code>dst</code> already exists and
   *           <code>options</code> has {@link Options.Rename#OVERWRITE}
   *           option false.
   * @throws FileNotFoundException If <code>src</code> does not exist
   * @throws ParentNotDirectoryException If parent of <code>dst</code> is not a
   *           directory
   * @throws UnsupportedFileSystemException If file system for <code>src</code>
   *           and <code>dst</code> is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,renameInternal,"org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",245,251,"/**
* Renames an internal file or directory.
* @param src source path to rename
* @param dst new name for the resource
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,fileStatusesInIndex,"org.apache.hadoop.fs.HarFileSystem:fileStatusesInIndex(org.apache.hadoop.fs.HarFileSystem$HarStatus,java.util.List)",511,522,"/**
* Fetches file statuses within the specified indexed directory.
* @param parent Indexed directory to search
* @param statuses list of FileStatus objects to populate
*/","* Get filestatuses of all the children of a given directory. This just reads
   * through index file and reads line by line to get all statuses for children
   * of a directory. Its a brute force way of getting all such filestatuses
   * 
   * @param parent
   *          the parent path directory
   * @param statuses
   *          the list to add the children filestatuses to",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getFileStatus,org.apache.hadoop.fs.HarFileSystem:getFileStatus(org.apache.hadoop.fs.Path),640,644,"/**
* Retrieves file status by converting HDFS har status.
* @param f Path object representing the file
* @return FileStatus object or null if not found
*/","* return the filestatus of files in har archive.
   * The permission returned are that of the archive
   * index files. The permissions are not persisted 
   * while creating a hadoop archive.
   * @param f the path in har filesystem
   * @return filestatus.
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,deprecatedGetFileStatus,org.apache.hadoop.fs.RawLocalFileSystem:deprecatedGetFileStatus(org.apache.hadoop.fs.Path),910,919,"/**
* Retrieves file status for a given path.
* @param f the file path to check
* @return FileStatus object or throws IOException if file is missing
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,org.apache.hadoop.fs.LocatedFileStatus:<init>(),40,42,"/**
* Initializes a new instance of the LocatedFileStatus class.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:<init>(org.apache.hadoop.fs.viewfs.ChRootedFileSystem,org.apache.hadoop.fs.FileStatus)",464,468,"/**
* Constructs an NflyStatus object from a ChRootedFileSystem and FileStatus.
* @param realFs the underlying file system
* @param realStatus the original file status
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFsFileStatus:<init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",35,38,"/**
 * Updates file system and path information for file status.
 * @param fs FileStatus object to update
 * @param newPath new path associated with the file status
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",205,250,"/**
* Retrieves the status of a file on an SFTP server.
* @param client SFTP connection
* @param file Path to the file
* @return FileStatus object or null if not found
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getFileStatus,org.apache.hadoop.fs.http.AbstractHttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path),113,116,"/**
* Returns file status information for the specified path.
* @param path file system path to query
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,doGlob,org.apache.hadoop.fs.Globber:doGlob(),208,396,"/**
* Performs a glob-style pattern matching on the given filesystem path.
* @return An array of matching FileStatus objects, or null if no matches are found
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,notFoundStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:notFoundStatus(org.apache.hadoop.fs.Path),623,625,"/**
* Returns a FileStatus object indicating a file is not found.
* @param f Path to the non-existent file
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",516,548,"/**
* Retrieves the status of a file on an FTP server.
* @param client FTPClient instance
* @param file Path to the file
* @return FileStatus object or throws IOException if not found
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,<init>,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE,boolean)",299,321,"/**
* Initializes CBZip2InputStream with the given InputStream and read mode.
* @param in input stream to read from
* @param readMode continuous or by-block read mode
* @param skipDecompression whether to skip decompression step
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,read0,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read0(),450,490,"/**
* Returns the next character from the input stream, handling various state transitions.
* @throws IOException if an I/O error occurs
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Print:execute(),193,198,"/**
* Prints all token files to the output stream.
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",833,843,"/**
* Cancels a token and updates local cache with ZooKeeper data.
* @param token unique token identifier
* @param canceller cancellation reason
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageToStream,org.apache.hadoop.security.Credentials:writeTokenStorageToStream(java.io.DataOutputStream),300,304,"/**
* Writes token storage to the provided output stream in the oldest supported format.
* @param os DataOutputStream to write to
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,<init>,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<init>(),139,141,"/**
* Initializes MetricsSystemImpl with default configuration (null metrics system). 
* @param metricsSystem null by default",* Construct the system but not initializing (read config etc.) it.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,registerSource,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)",260,270,"/**
* Registers a metrics source with the specified name and description.
* @param name unique identifier for the source
* @param desc human-readable description of the source
* @param source MetricsSource instance to register
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,call,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,long,java.lang.String,java.lang.String,long)",577,597,"/**
* Processes RPC call for given method and protocol.
* @param server RPC server instance
* @param connectionProtocolName name of the connection protocol
* @param request RPC request data
* @return Writable result object or null on error
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,<init>,org.apache.hadoop.metrics2.lib.MutableRollingAverages:<init>(java.lang.String),144,155,"/**
* Initializes rolling averages with custom metric value name.
* @param metricValueName unique identifier for the average calculation
*/","* Constructor for {@link MutableRollingAverages}.
   * @param metricValueName input metricValueName.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,replaceScheduledTask,"org.apache.hadoop.metrics2.lib.MutableRollingAverages:replaceScheduledTask(int,long,java.util.concurrent.TimeUnit)",160,167,"/**
* Replaces the scheduled task with a new one.
* @param windows number of rate changes
* @param interval fixed rate interval
* @param timeUnit unit of measurement for interval
*/",* This method is for testing only to replace the scheduledTask.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)",272,294,"/**
* Registers a metrics sink with specified name and description.
* @param name unique identifier for the sink
* @param description descriptive text for the sink
* @param sink MetricsSink object to be registered
* @return the registered sink instance
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configureSinks,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSinks(),492,519,"/**
* Configures metrics sinks from configuration instances.
* @throws Exception on sink creation errors
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,recheckElectability,org.apache.hadoop.ha.ZKFailoverController:recheckElectability(),808,860,"/**
* Rechecks electability based on current health state and delay until joining election.
*/","* Check the current state of the service, and join the election
   * if it should be in the election.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reJoinElectionAfterFailureToBecomeActive,org.apache.hadoop.ha.ActiveStandbyElector:reJoinElectionAfterFailureToBecomeActive(),627,629,"/**
* Re-joins election after failing to become active node.
*/","* We failed to become active. Re-join the election, but
   * sleep for a few seconds after terminating our existing
   * session, so that other nodes have a chance to become active.
   * The failure to become active is already logged inside
   * becomeActive().",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,processWatchEvent,"org.apache.hadoop.ha.ActiveStandbyElector:processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent)",635,713,"/**
* Handles a WatchEvent from the ZooKeeper, updating internal state and
* triggering actions as needed.
* @param zk active ZooKeeper connection
* @param event WatchEvent object containing details of the change
*/","* interface implementation of Zookeeper watch events (connection and node),
   * proxied by {@link WatcherWithClientRef}.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int)",180,183,"/**
* Creates an InetSocketAddress instance from the given target hostname or IP address and port.
* @param target hostname or IP address
* @param defaultPort default port number (optional)
*/","* Util method to build socket addr from either.
   *   {@literal <host>}
   *   {@literal <host>:<port>}
   *   {@literal <fs>://<host>:<port>/<path>}
   *
   * @param target target.
   * @param defaultPort default port.
   * @return socket addr.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,processOptions,org.apache.hadoop.fs.shell.find.Find:processOptions(java.util.LinkedList),166,212,"/**
* Processes command-line arguments, configuring options and parsing expressions.
*@param args list of command-line arguments
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/MultiSchemeDelegationTokenAuthenticationHandler.java,authenticate,"org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",154,182,"/**
* Authenticates a user based on the Authorization header in the request.
* @throws AuthenticationException if authentication fails
*/","* This method is overridden to restrict HTTP authentication schemes
   * available for delegation token management functionality. The
   * authentication schemes to be used for delegation token management are
   * configured using {@link DELEGATION_TOKEN_SCHEMES_PROPERTY}
   *
   * The basic logic here is to check if the current request is for delegation
   * token management. If yes then check if the request contains an
   * ""Authorization"" header. If it is missing, then return the HTTP 401
   * response with WWW-Authenticate header for each scheme configured for
   * delegation token management.
   *
   * It is also possible for a client to preemptively send Authorization header
   * for a scheme not configured for delegation token management. We detect
   * this case and return the HTTP 401 response with WWW-Authenticate header
   * for each scheme configured for delegation token management.
   *
   * If a client has sent a request with ""Authorization"" header for a scheme
   * configured for delegation token management, then it is forwarded to
   * underlying {@link MultiSchemeAuthenticationHandler} for actual
   * authentication.
   *
   * Finally all other requests (excluding delegation token management) are
   * forwarded to underlying {@link MultiSchemeAuthenticationHandler} for
   * actual authentication.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleDeprecation,org.apache.hadoop.conf.Configuration:handleDeprecation(),777,786,"/**
* Iterates over all properties in the config and handles deprecation.
* @param none
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,onlyKeyExists,org.apache.hadoop.conf.Configuration:onlyKeyExists(java.lang.String),1295,1305,"/**
* Checks if property exists with given key.
* @param name unique property identifier
* @return true if property is present, false otherwise
*/","* Return existence of the <code>name</code> property, but only for
   * names which have no valid value, usually non-existent or commented
   * out in XML.
   *
   * @param name the property name
   * @return true if the property <code>name</code> exists without value",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getRaw,org.apache.hadoop.conf.Configuration:getRaw(java.lang.String),1355,1362,"/**
* Retrieves the raw value of a property by its name.
* @param name property name
* @return raw property value or null if not found
*/","* Get the value of the <code>name</code> property, without doing
   * <a href=""#VariableExpansion"">variable expansion</a>.If the key is 
   * deprecated, it returns the value of the first key which replaces 
   * the deprecated key and is not null.
   * 
   * @param name the property name.
   * @return the value of the <code>name</code> property or 
   *         its replacing property and null if no such property exists.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,set,"org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String,java.lang.String)",1420,1458,"/**
* Sets property with specified name, value and source.
* @param name property name
* @param value property value
* @param source source of the change (or null for programmatically) 
*/","* Set the <code>value</code> of the <code>name</code> property. If 
   * <code>name</code> is deprecated, it also sets the <code>value</code> to
   * the keys that replace the deprecated key. Name will be trimmed before put
   * into configuration.
   *
   * @param name property name.
   * @param value property value.
   * @param source the place that this configuration value came from 
   * (For debugging).
   * @throws IllegalArgumentException when the value or name is null.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,unset,org.apache.hadoop.conf.Configuration:unset(java.lang.String),1476,1492,"/**
* Removes overlay and properties associated with a given name.
* @param name unique identifier to remove
*/","* Unset a previously set property.
   * @param name the property name",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,tryFence,"org.apache.hadoop.ha.ShellCommandFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",81,135,"/**
* Tries to fence the HA service target using a shell command.
* @param target HAServiceTarget instance
* @param args additional command arguments
* @return true if fencing is successful, false otherwise
*/",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,grantPermissions,org.apache.hadoop.fs.FileUtil:grantPermissions(java.io.File),235,239,"/**
* Grants read, write, and execute permissions to a file.
* @param f the file object
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getVirtualMemorySize,org.apache.hadoop.util.SysInfoWindows:getVirtualMemorySize(),144,148,"/**
* Retrieves the total virtual memory size.
* @return Total virtual memory size in bytes.",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getPhysicalMemorySize,org.apache.hadoop.util.SysInfoWindows:getPhysicalMemorySize(),151,155,"/**
* Retrieves physical memory size.
* @return total physical memory size in bytes
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getAvailableVirtualMemorySize,org.apache.hadoop.util.SysInfoWindows:getAvailableVirtualMemorySize(),158,162,"/**
* Retrieves available virtual memory size.
* @return total available virtual memory in bytes
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getAvailablePhysicalMemorySize,org.apache.hadoop.util.SysInfoWindows:getAvailablePhysicalMemorySize(),165,169,"/**
* Retrieves available physical memory size in bytes.
* @return Available physical memory size or -1 if unknown.",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNumProcessors,org.apache.hadoop.util.SysInfoWindows:getNumProcessors(),172,176,"/**
* Returns the number of processors available.
* @return The current processor count or -1 if unavailable
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getCpuFrequency,org.apache.hadoop.util.SysInfoWindows:getCpuFrequency(),185,189,"/**
* Retrieves CPU frequency in kHz.
* @return current CPU frequency or last known value if not refreshed.",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getCumulativeCpuTime,org.apache.hadoop.util.SysInfoWindows:getCumulativeCpuTime(),192,196,"/**
* Retrieves the cumulative CPU time in milliseconds. 
* @return total CPU time spent by this process since startup or last reset.",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getCpuUsagePercentage,org.apache.hadoop.util.SysInfoWindows:getCpuUsagePercentage(),199,207,"/**
* Retrieves CPU usage percentage as a floating-point value.
* @return CPU usage percentage or -1.0 if unavailable
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNumVCoresUsed,org.apache.hadoop.util.SysInfoWindows:getNumVCoresUsed(),210,218,"/**
* Calculates and returns the number of virtual CPU cores used, as a percentage.
* @return CPU usage percentage or -1.0 if unavailable
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNetworkBytesRead,org.apache.hadoop.util.SysInfoWindows:getNetworkBytesRead(),221,225,"/**
* Retrieves the total number of bytes read from the network.
* @return the current network bytes read value
*/",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNetworkBytesWritten,org.apache.hadoop.util.SysInfoWindows:getNetworkBytesWritten(),228,232,"/**
* Retrieves the total bytes written to the network.
* @return The cumulative number of bytes sent over the network.",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getStorageBytesRead,org.apache.hadoop.util.SysInfoWindows:getStorageBytesRead(),234,238,"/**
* Retrieves total bytes read from storage.
* @return current storage bytes read count
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getStorageBytesWritten,org.apache.hadoop.util.SysInfoWindows:getStorageBytesWritten(),240,244,"/**
* Retrieves total bytes written to storage.
* @return Total bytes written in disk space.",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getPermission,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getPermission(),951,957,"/**
* Returns file system permission, loading from cache if necessary.
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getOwner,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getOwner(),959,965,"/**
* Retrieves owner information from permission data.
* @return Owner string or null if permissions are not loaded.",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getGroup,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getGroup(),967,973,"/**
* Returns the group associated with this object, loading permission info if necessary.
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,write,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:write(java.io.DataOutput),1087,1093,"/**
* Writes this object to the output stream, loading permissions if necessary.
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,flush,org.apache.hadoop.security.alias.LocalKeyStoreProvider:flush(),143,160,"/**
* Flushes file permissions to disk.
* @throws IOException if an I/O error occurs
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,cacheGroupsRefresh,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh(),63,68,"/**
* Refreshes and caches user groups.
*/",* Refresh the netgroup cache,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,org.apache.hadoop.fs.viewfs.ViewFileSystem:delete(org.apache.hadoop.fs.Path),498,503,"/**
* Deletes a file by path.
* @param f file to be deleted
* @return true if deletion is successful, false otherwise
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getStatus(),1299,1302,"/**
* Returns file system status.
* @throws IOException if an I/O error occurs
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.java,updateMountPointFsStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:updateMountPointFsStatus(org.apache.hadoop.fs.viewfs.ViewFileSystem,java.util.Map,org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint,org.apache.hadoop.fs.Path)",169,175,"/**
* Updates the file system status for a given mount point.
* @param viewFileSystem View of the file system
* @param mountPointMap Map of mount points to their statuses
* @param mountPoint Mount point identifier
* @param path File system path to fetch status from
*/","* Update FsStatus for the given the mount point.
   *
   * @param viewFileSystem
   * @param mountPointMap
   * @param mountPoint
   * @param path",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,read,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:read(long,byte[],int,int)",194,210,"/**
* Reads data from a file at specified position into a byte array buffer.
* @param position offset to read from
* @param b buffer to store the read data
* @param off offset within the buffer to start writing data
* @param len maximum number of bytes to read
* @return actual number of bytes read, or 0 if no data was read
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processArguments,org.apache.hadoop.fs.shell.CopyCommands$Put:processArguments(java.util.LinkedList),306,315,"/**
* Processes command-line arguments and copies input stream to target if only '-' argument is provided.
* @param args list of PathData objects containing command-line options and values
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,create,"org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",696,702,"/**
* Wraps the main create method for FSDataOutputStream creation.
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",736,742,"/**
* Creates a non-recursive FSDataOutputStream.
* @param f file path
* @param permission file permissions
* @param overwrite whether to overwrite existing files
* @return FSDataOutputStream object
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",757,768,"/**
* Creates a non-recursive FSDataOutputStream.
* @param f the file path
* @param permission the file permissions
* @param flags create flag options
* @param bufferSize buffer size
* @param replication replication factor
* @param blockSize block size
* @param progress progress monitor
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",638,644,"/**
* Renames a file or directory from one internal path to another.
* @param src source path
* @param dst destination path
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,listStatus,org.apache.hadoop.fs.HarFileSystem:listStatus(org.apache.hadoop.fs.Path),784,804,"/**
* Retrieves a list of file statuses for the given path.
* @param f Path to query
* @return Array of FileStatus objects or null if not found
*/","* liststatus returns the children of a directory 
   * after looking up the index files.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getFileLinkStatusInternal,"org.apache.hadoop.fs.RawLocalFileSystem:getFileLinkStatusInternal(org.apache.hadoop.fs.Path,boolean)",1233,1242,"/**
* Resolves file link status for the given path.
* @param f Path to resolve
* @param dereference whether to follow symbolic links or not
* @return FileStatus object or null if not found
*/","* Public {@link FileStatus} methods delegate to this function, which in turn
   * either call the new {@link Stat} based implementation or the deprecated
   * methods based on platform support.
   * 
   * @param f Path to stat
   * @param dereference whether to dereference the final path component if a
   *          symlink
   * @return FileStatus of f
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:<init>(org.apache.hadoop.fs.LocatedFileStatus,org.apache.hadoop.fs.Path)",32,35,"/**
* Initializes file status with Fs metadata and path.
* @param locatedFileStatus File status with Fs metadata
* @param path Filesystem path to associate with file status
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",505,512,"/**
* Retrieves file block locations for the specified FileStatus.
* @param fs FileStatus object
* @param start starting position in bytes
* @param len length of data to retrieve
* @return array of BlockLocation objects or null if not found
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFs:getFileStatus(org.apache.hadoop.fs.Path),407,426,"/**
* Resolves file system path and returns corresponding FileStatus object.
* @param f input path to resolve
*/","* {@inheritDoc}
   *
   * If the given path is a symlink(mount link), the path will be resolved to a
   * target path and it will get the resolved path's FileStatus object. It will
   * not be represented as a symlink and isDirectory API returns true if the
   * resolved path is a directory, false otherwise.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFs:listStatus(org.apache.hadoop.fs.Path),518,538,"/**
* Retrieves file statuses for the given path.
* @param f target filesystem path
* @return array of FileStatus objects or null if not found
*/","* {@inheritDoc}
   *
   * Note: listStatus considers listing from fallbackLink if available. If the
   * same directory path is present in configured mount path as well as in
   * fallback fs, then only the fallback path will be listed in the returned
   * result except for link.
   *
   * If any of the the immediate children of the given path f is a symlink(mount
   * link), the returned FileStatus object of that children would be represented
   * as a symlink. It will not be resolved to the target path and will not get
   * the target path FileStatus object. The target path will be available via
   * getSymlink on that children's FileStatus object. Since it represents as
   * symlink, isDirectory on that children's FileStatus will return false.
   * This behavior can be changed by setting an advanced configuration
   * fs.viewfs.mount.links.as.symlinks to false. In this case, mount points will
   * be represented as non-symlinks and all the file/directory attributes like
   * permissions, isDirectory etc will be assigned from it's resolved target
   * directory/file.
   *
   * If you want to get the FileStatus of target path for that children, you may
   * want to use GetFileStatus API with that children's symlink path. Please see
   * {@link ViewFs#getFileStatus(Path f)}
   *
   * Note: In ViewFs, by default the mount links are represented as symlinks.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,exists,"org.apache.hadoop.fs.sftp.SFTPFileSystem:exists(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",189,198,"/**
* Checks if a remote SFTP file exists.
* @param channel active SFTP channel
* @param file Path to the file to check
* @return true if the file exists, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(com.jcraft.jsch.ChannelSftp,com.jcraft.jsch.ChannelSftp$LsEntry,org.apache.hadoop.fs.Path)",260,297,"/**
* Returns file status for the given SFTP file.
* @param channel SFTP channel to use
* @param sftpFile SFTP file entry to fetch status for
* @param parentPath parent directory path
* @return FileStatus object or null if not found
*/","* Convert the file information in LsEntry to a {@link FileStatus} object. *
   *
   * @param sftpFile
   * @param parentPath
   * @return file status
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,isFile,"org.apache.hadoop.fs.sftp.SFTPFileSystem:isFile(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",355,363,"/**
* Checks if a given SFTP file exists and is not a directory.
* @param channel active SFTP connection
* @param file Path to the file to check
* @return true if file exists, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getFileStatus,org.apache.hadoop.fs.http.HttpsFileSystem:getFileStatus(org.apache.hadoop.fs.Path),113,116,"/**
* Returns a FileStatus object representing the file at the specified Path.
* @param path location of the file to query
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getFileStatus,org.apache.hadoop.fs.http.HttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path),113,116,"/**
* Returns file status information for the specified path.
* @param path file system path to retrieve status for
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,glob,org.apache.hadoop.fs.Globber:glob(),197,206,"/**
* Performs globbing on the specified pattern, returning an array of FileStatus objects.
* @throws IOException if an I/O error occurs during globbing
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,exists,"org.apache.hadoop.fs.ftp.FTPFileSystem:exists(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",391,398,"/**
* Checks if a file exists on an FTP server.
* @param client active FTP connection
* @param file Path to the file to check
* @return true if the file exists, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * @throws IOException on IO problems other than FileNotFoundException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,listStatus,"org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",484,498,"/**
* Lists status of files and subdirectories in specified path.
* @param client FTPClient instance
* @param file Path to list directory/file for
* @return Array of FileStatus objects or null if not found
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,isFile,"org.apache.hadoop.fs.ftp.FTPFileSystem:isFile(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",617,625,"/**
* Checks if a remote file exists and is a regular file.
* @param client active FTP connection
* @param file local file path to verify
* @return true if the file exists and is a regular file, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,<init>,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",294,297,"/**
* Constructs a CBZip2InputStream from an existing stream.
* @param in input stream to wrap
* @param readMode mode of compression reading (e.g. READ_MODE) 
*/","* Constructs a new CBZip2InputStream which decompresses bytes read from the
  * specified stream.
  *
  * <p>
  * Although BZip2 headers are marked with the magic <tt>""Bz""</tt> this
  * constructor expects the next byte in the stream to be the first one after
  * the magic. Thus callers have to skip the first two bytes. Otherwise this
  * constructor will throw an exception.
  * </p>
  * @param in in.
  * @param readMode READ_MODE.
  * @throws IOException
  *             if the stream content is malformed or an I/O error occurs.
  * @throws NullPointerException
  *             if <tt>in == null</tt>",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,numberOfBytesTillNextMarker,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:numberOfBytesTillNextMarker(java.io.InputStream),346,349,"/**
* Calculates the number of bytes until the next marker.
*/","* Returns the number of bytes between the current stream position
   * and the immediate next BZip2 block marker.
   *
   * @param in
   *             The InputStream
   *
   * @return long Number of bytes between current stream position and the
   * next BZip2 block start marker.
 * @throws IOException raised on errors performing I/O.
   *",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,read,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read(byte[],int,int)",400,448,"/**
* Reads data into the provided byte array.
* @param dest destination buffer
* @param offs offset in the buffer to start writing at
* @param len number of bytes to read
* @return number of bytes written or -1 if end of stream is reached
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,call,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)",529,575,"/**
* Handles RPC server-side method invocation with protobuf response.
* @param server RPC server instance
* @param connectionProtocolName protocol name for connection
* @param writableRequest RPC request object
* @param receiveTime time of request reception
* @return Writable response or throws Exception on failure",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newMutableRollingAverages,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newMutableRollingAverages(java.lang.String,java.lang.String)",339,346,"/**
* Creates a new synchronized mutable rolling averages for the given metric.
* @param name unique metric identifier
* @param valueName name of the values to be averaged
* @return MutableRollingAverages instance or null if already created
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,verifyChangedServiceState,org.apache.hadoop.ha.ZKFailoverController:verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState),884,922,"/**
* Verifies the changed service state and updates election status accordingly.
* @param changedState new service state
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,processResult,"org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,java.lang.String)",496,551,"/**
* Processes ZooKeeper create node result.
* @param rc creation result code
* @param path znode path
* @param ctx connection context
* @param name lock name
*/",* interface implementation of Zookeeper callback for create,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,processResult,"org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat)",556,613,"/**
* Processes StatNode result, determining leader/standby status and handling errors.
* @param rc Zookeeper response code
* @param path monitored lock znode path
* @param ctx client context
* @param stat StatNode result object
*/",* interface implementation of Zookeeper callback for monitor (exists),,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,process,org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:process(org.apache.zookeeper.WatchedEvent),1233,1247,"/**
* Processes a watched event, potentially triggering leader election.
* @param event WatchedEvent object
*/",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String),162,164,"/**
* Creates an InetSocketAddress from a hostname or IP address.
* @param target network location (e.g. hostname or IP address)
*/","* Util method to build socket addr from either.
   *   {@literal <host>:<port>}
   *   {@literal <fs>://<host>:<port>/<path>}
   *
   * @param target target.
   * @return socket addr.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/Servers.java,parse,"org.apache.hadoop.metrics2.util.Servers:parse(java.lang.String,int)",50,62,"/**
* Parses string specifications into a list of InetSocketAddress objects.
* @param specs comma-separated host:port strings or null for localhost
* @return List of InetSocketAddress objects or default localhost address if specs is null
*/","* Parses a space and/or comma separated sequence of server specifications
   * of the form <i>hostname</i> or <i>hostname:port</i>.  If
   * the specs string is null, defaults to localhost:defaultPort.
   *
   * @param specs   server specs (see description)
   * @param defaultPort the default port if not specified
   * @return a list of InetSocketAddress objects.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,buildDTServiceName,"org.apache.hadoop.security.SecurityUtil:buildDTServiceName(java.net.URI,int)",338,345,"/**
* Builds the DT service name from a URI and default port.
* @param uri URI with authority to extract
* @param defPort default port number to use if not specified in URI
* @return String representation of the built token service or null if no authority found
*/","* create the service name for a Delegation token
   * @param uri of the service
   * @param defPort is used if the uri lacks a port
   * @return the token service, or null if no authority
   * @see #buildTokenService(InetSocketAddress)",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,asXmlDocument,"org.apache.hadoop.conf.Configuration:asXmlDocument(java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",3650,3685,"/**
* Creates an XML document representing configuration data.
* @param propertyName optional property name to include (null for all properties)
* @param redactor ConfigRedactor instance to apply formatting
* @return Document object containing configuration data or null on failure
*/",* Return the XML DOM corresponding to this Configuration.,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,substituteVars,org.apache.hadoop.conf.Configuration:substituteVars(java.lang.String),1150,1218,"/**
* Substitutes variables in the given expression by their values.
* @param expr input expression to substitute variables in
*/","* Attempts to repeatedly expand the value {@code expr} by replacing the
   * left-most substring of the form ""${var}"" in the following precedence order
   * <ol>
   *   <li>by the value of the environment variable ""var"" if defined</li>
   *   <li>by the value of the Java system property ""var"" if defined</li>
   *   <li>by the value of the configuration key ""var"" if defined</li>
   * </ol>
   *
   * If var is unbounded the current state of expansion ""prefix${var}suffix"" is
   * returned.
   * <p>
   * This function also detects self-referential substitutions, i.e.
   * <pre>
   *   {@code
   *   foo.bar = ${foo.bar}
   *   }
   * </pre>
   * If a cycle is detected then the original expr is returned. Loops
   * involving multiple substitutions are not detected.
   *
   * In order not to introduce breaking changes (as Oozie for example contains a method with the
   * same name and same signature) do not make this method public, use substituteCommonVariables
   * in this case.
   *
   * @param expr the literal value of a config key
   * @return null if expr is null, otherwise the value resulting from expanding
   * expr using the algorithm above.
   * @throws IllegalArgumentException when more than
   * {@link Configuration#MAX_SUBST} replacements are required",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,applyChanges,"org.apache.hadoop.conf.ReconfigurationServlet:applyChanges(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable,javax.servlet.http.HttpServletRequest)",140,197,"/**
* Applies configuration changes based on user input.
* @param out PrintWriter for output
* @param reconf Reconfigurable object for property updates
* @param req HttpServletRequest containing form data
*/",* Apply configuratio changes after admin has approved them.,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,set,"org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String)",1404,1406,"/**
* Sets a property with the given name and value.
* @param name property key
* @param value property value
*/","* Set the <code>value</code> of the <code>name</code> property. If 
   * <code>name</code> is deprecated or there is a deprecated name associated to it,
   * it sets the value to both names. Name will be trimmed before put into
   * configuration.
   * 
   * @param name property name.
   * @param value property value.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,set,"org.apache.hadoop.conf.ConfigurationWithLogging:set(java.lang.String,java.lang.String,java.lang.String)",107,112,"/**
* Sets a property with optional source information.
* @param name property name
* @param value property value
* @param source optional source of the value (may be null)
*/","* See {@link Configuration#set(String, String, String)}.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDelete,"org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File,boolean)",187,203,"/**
* Deletes a directory and its contents recursively.
* @param dir directory to be deleted
* @param tryGrantPermissions whether to grant write permissions before deletion
* @return true if the directory is fully deleted, false otherwise
*/","* Delete a directory and all its contents.  If
   * we return false, the directory may be partially-deleted.
   * (1) If dir is symlink to a file, the symlink is deleted. The file pointed
   *     to by the symlink is not deleted.
   * (2) If dir is symlink to a directory, symlink is deleted. The directory
   *     pointed to by symlink is not deleted.
   * (3) If dir is a normal file, it is deleted.
   * (4) If dir is a normal directory, then dir and all its contents recursively
   *     are deleted.
   * @param dir the file or directory to be deleted
   * @param tryGrantPermissions true if permissions should be modified to delete a file.
   * @return true on success false on failure.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNumCores,org.apache.hadoop.util.SysInfoWindows:getNumCores(),179,182,"/**
* Returns the number of CPU cores. 
* Alias for getNumProcessors(). 
*/",{@inheritDoc},,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.java,getStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:getStatus(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",106,159,"/**
* Retrieves the file system status for each mount point that the given path intersects with.
* @param fileSystem ViewFileSystem instance
* @param path Path to evaluate
* @return Map of MountPoints to FsStatus objects, or throws exceptions if not supported
*/","* Get FsStatus for all ViewFsMountPoints matching path for the given
   * ViewFileSystem.
   *
   * Say ViewFileSystem has following mount points configured
   *  (1) hdfs://NN0_host:port/sales mounted on /dept/sales
   *  (2) hdfs://NN1_host:port/marketing mounted on /dept/marketing
   *  (3) hdfs://NN2_host:port/eng_usa mounted on /dept/eng/usa
   *  (4) hdfs://NN3_host:port/eng_asia mounted on /dept/eng/asia
   *
   * For the above config, here is a sample list of paths and their matching
   * mount points while getting FsStatus
   *
   *  Path                  Description                      Matching MountPoint
   *
   *  ""/""                   Root ViewFileSystem lists all    (1), (2), (3), (4)
   *                         mount points.
   *
   *  ""/dept""               Not a mount point, but a valid   (1), (2), (3), (4)
   *                         internal dir in the mount tree
   *                         and resolved down to ""/"" path.
   *
   *  ""/dept/sales""         Matches a mount point            (1)
   *
   *  ""/dept/sales/india""   Path is over a valid mount point (1)
   *                         and resolved down to
   *                         ""/dept/sales""
   *
   *  ""/dept/eng""           Not a mount point, but a valid   (1), (2), (3), (4)
   *                         internal dir in the mount tree
   *                         and resolved down to ""/"" path.
   *
   *  ""/erp""                Doesn't match or leads to or
   *                         over any valid mount points     None
   *
   *
   * @param fileSystem - ViewFileSystem on which mount point exists
   * @param path - URI for which FsStatus is requested
   * @return Map of ViewFsMountPoint and FsStatus
   * @throws IOException raised on errors performing I/O.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,create,"org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",744,755,"/**
* Creates a new FSDataOutputStream with specified options.
* @param f file path
* @param permission file permissions
* @param flags creation flags (e.g. OVERWRITE)
* @param bufferSize buffer size
* @param replication replication factor
* @param blockSize block size
* @param progress progress tracker
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getFileStatus,org.apache.hadoop.fs.RawLocalFileSystem:getFileStatus(org.apache.hadoop.fs.Path),905,908,"/**
 * Retrieves file status information.
 * @param f file path to query
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.RawLocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),1209,1220,"/**
* Retrieves file status with qualified symlink targets.
* @param f the path to query
* @return FileStatus object or null if not found
*/","* Return a FileStatus representing the given path. If the path refers
   * to a symlink return a FileStatus representing the link rather than
   * the object the link refers to.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getLinkTarget,org.apache.hadoop.fs.RawLocalFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),1308,1313,"/**
* Returns the unqualified link target for a given file path.
* @param f input file path
* @return Path object representing the linked target or null if not found
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,wrapLocalFileStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystem:wrapLocalFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",552,557,"/**
* Wraps a local file status with the provided qualified path.
* @param orig original FileStatus object
* @param qualified Path to the underlying file system
* @return wrapped FileStatus object or null if orig is invalid
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,rename,"org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",464,491,"/**
* Renames a file on an SFTP server.
* @param channel SFTP connection
* @param src original file path
* @param dst new file path
* @return true if rename was successful, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   *
   * @param channel
   * @param src
   * @param dst
   * @return rename successful?
   * @throws IOException",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,listStatus,"org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",421,451,"/**
* Retrieves the status of a directory or single file on an SFTP server.
* @param client SFTP client connection
* @param file Path to the file/directory to list status for
* @return Array of FileStatus objects representing the files/directories in the specified path
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",314,347,"/**
* Creates directory recursively.
* @param client SFTP channel
* @param file Path to create directory for
* @param permission FsPermission to set on created directory
* @return true if directory was successfully created; false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,globStatus,org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path),2122,2126,"/**
* Returns an array of file statuses matching the specified pattern.
* @param pathPattern a file system path pattern
*/","* <p>Return all the files that match filePattern and are not checksum
     * files. Results are sorted by their names.
     * 
     * <p>
     * A filename pattern is composed of <i>regular</i> characters and
     * <i>special pattern matching</i> characters, which are:
     *
     * <dl>
     *  <dd>
     *   <dl>
     *    <dt> <tt> ? </tt>
     *    <dd> Matches any single character.
     *
     *    <dt> <tt> * </tt>
     *    <dd> Matches zero or more characters.
     *
     *    <dt> <tt> [<i>abc</i>] </tt>
     *    <dd> Matches a single character from character set
     *     <tt>{<i>a,b,c</i>}</tt>.
     *
     *    <dt> <tt> [<i>a</i>-<i>b</i>] </tt>
     *    <dd> Matches a single character from the character range
     *     <tt>{<i>a...b</i>}</tt>. Note: character <tt><i>a</i></tt> must be
     *     lexicographically less than or equal to character <tt><i>b</i></tt>.
     *
     *    <dt> <tt> [^<i>a</i>] </tt>
     *    <dd> Matches a single char that is not from character set or range
     *     <tt>{<i>a</i>}</tt>.  Note that the <tt>^</tt> character must occur
     *     immediately to the right of the opening bracket.
     *
     *    <dt> <tt> \<i>c</i> </tt>
     *    <dd> Removes (escapes) any special meaning of character <i>c</i>.
     *
     *    <dt> <tt> {ab,cd} </tt>
     *    <dd> Matches a string from the string set <tt>{<i>ab, cd</i>} </tt>
     *
     *    <dt> <tt> {ab,c{de,fh}} </tt>
     *    <dd> Matches a string from string set <tt>{<i>ab, cde, cfh</i>}</tt>
     *
     *   </dl>
     *  </dd>
     * </dl>
     *
     * @param pathPattern a glob specifying a path pattern
     *
     * @return an array of paths that match the path pattern
     *
     * @throws AccessControlException If access is denied
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>pathPattern</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,globStatus,"org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2151,2155,"/**
* Returns an array of file statuses matching the given pattern and filter.
* @param pathPattern glob pattern to match
* @param filter optional filter for matched files
* @return FileStatus array or null if no matches found
*/","* Return an array of FileStatus objects whose path names match pathPattern
     * and is accepted by the user-supplied path filter. Results are sorted by
     * their path names.
     * Return null if pathPattern has no glob and the path does not exist.
     * Return an empty array if pathPattern has a glob and no path matches it. 
     * 
     * @param pathPattern glob specifying the path pattern
     * @param filter user-supplied path filter
     *
     * @return an array of FileStatus objects
     *
     * @throws AccessControlException If access is denied
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>pathPattern</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,globStatus,org.apache.hadoop.fs.FileSystem:globStatus(org.apache.hadoop.fs.Path),2219,2226,"/**
* Returns an array of FileStatus objects matching the given path pattern.
* @param pathPattern globbing pattern to match
*/","* <p>Return all the files that match filePattern and are not checksum
   * files. Results are sorted by their names.
   *
   * <p>
   * A filename pattern is composed of <i>regular</i> characters and
   * <i>special pattern matching</i> characters, which are:
   *
   * <dl>
   *  <dd>
   *   <dl>
   *    <dt> <tt> ? </tt>
   *    <dd> Matches any single character.
   *
   *    <dt> <tt> * </tt>
   *    <dd> Matches zero or more characters.
   *
   *    <dt> <tt> [<i>abc</i>] </tt>
   *    <dd> Matches a single character from character set
   *     <tt>{<i>a,b,c</i>}</tt>.
   *
   *    <dt> <tt> [<i>a</i>-<i>b</i>] </tt>
   *    <dd> Matches a single character from the character range
   *     <tt>{<i>a...b</i>}</tt>.  Note that character <tt><i>a</i></tt> must be
   *     lexicographically less than or equal to character <tt><i>b</i></tt>.
   *
   *    <dt> <tt> [^<i>a</i>] </tt>
   *    <dd> Matches a single character that is not from character set or range
   *     <tt>{<i>a</i>}</tt>.  Note that the <tt>^</tt> character must occur
   *     immediately to the right of the opening bracket.
   *
   *    <dt> <tt> \<i>c</i> </tt>
   *    <dd> Removes (escapes) any special meaning of character <i>c</i>.
   *
   *    <dt> <tt> {ab,cd} </tt>
   *    <dd> Matches a string from the string set <tt>{<i>ab, cd</i>} </tt>
   *
   *    <dt> <tt> {ab,c{de,fh}} </tt>
   *    <dd> Matches a string from the string set <tt>{<i>ab, cde, cfh</i>}</tt>
   *
   *   </dl>
   *  </dd>
   * </dl>
   *
   * @param pathPattern a glob specifying a path pattern

   * @return an array of paths that match the path pattern
   * @throws IOException IO failure",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,globStatus,"org.apache.hadoop.fs.FileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2241,2244,"/**
* Returns an array of file status objects matching the specified path pattern and filtering criteria.
* @param pathPattern glob pattern for paths to match
* @param filter optional filter to apply to matched files
*/","* Return an array of {@link FileStatus} objects whose path names match
   * {@code pathPattern} and is accepted by the user-supplied path filter.
   * Results are sorted by their path names.
   *
   * @param pathPattern a glob specifying the path pattern
   * @param filter a user-supplied path filter
   * @return null if {@code pathPattern} has no glob and the path does not exist
   *         an empty array if {@code pathPattern} has a glob and no path
   *         matches it else an array of {@link FileStatus} objects matching the
   *         pattern
   * @throws IOException if any I/O error occurs when fetching file status",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,rename,"org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",670,705,"/**
* Renames a file or directory on an FTP server.
* @param client FTPClient instance
* @param src source path
* @param dst destination path
* @return true if rename operation is successful, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * 
   * @param client
   * @param src
   * @param dst
   * @return
   * @throws IOException",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,delete,"org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,boolean)",416,438,"/**
* Deletes a file or directory on an FTP server.
* @param client FTPClient instance
* @param file remote file path
* @param recursive whether to remove directory contents recursively (default: false)
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",590,610,"/**
* Creates the specified directory and all parent directories recursively.
* @param client FTPClient instance
* @param file Path to create directory for
* @param permission FsPermission for new directory
* @return true if directory was successfully created, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,<init>,"org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:<init>(java.io.InputStream,long,long,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",358,407,"/**
* Initializes BZip2CompressionInputStream with InputStream, start and end positions,
* and read mode. Strips headers and updates reported byte count if necessary.
* @param in input stream
* @param start starting position
* @param end ending position
* @param readMode compression read mode
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,internalReset,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:internalReset(),530,537,"/**
* Resets internal state and reinitializes stream reader.
* @throws IOException if I/O error occurs
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,<init>,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream),351,353,"/**
* Initializes a new CBZip2InputStream from an existing InputStream.
* @param in input stream containing compressed data
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,read,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read(),365,376,"/**
* Reads a single byte from the underlying stream.
* @throws IOException if the stream is closed
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,newForField,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForField(java.lang.reflect.Field,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)",40,92,"/**
* Creates a mutable metric for the given field and registry.
* @param field the field to create a metric for
* @param annotation metrics configuration annotation
* @param registry metrics registry to add the metric to
* @return MutableMetric object or null if unsupported
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,reportServiceStatus,org.apache.hadoop.ha.ZKFailoverController$ServiceStateCallBacks:reportServiceStatus(org.apache.hadoop.ha.HAServiceStatus),999,1002,"/**
* Reports service status update.
* @param status HAServiceStatus object containing updated state information
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,normalizeIP2HostName,org.apache.hadoop.net.NetUtils:normalizeIP2HostName(java.lang.String),731,738,"/**
* Converts an IP:port string to a host name.
* @param ipPort IP address and port number (e.g., ""192.168.1.100:8080"")
* @return Host name or original input if invalid
*/","* Attempt to normalize the given string to ""host:port""
   * if it like ""ip:port"".
   *
   * @param ipPort maybe lik ip:port or host:port.
   * @return host:port",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getTokenServiceAddr,org.apache.hadoop.security.SecurityUtil:getTokenServiceAddr(org.apache.hadoop.security.token.Token),447,449,"/**
* Resolves service address for given token.
* @param token Token object containing service details
*/","* Decode the given token's service field into an InetAddress
   * @param token from which to obtain the service
   * @return InetAddress for the service",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,buildTokenService,org.apache.hadoop.security.SecurityUtil:buildTokenService(java.net.URI),495,497,"/**
* Builds a token service instance with the specified URI authority.","* Construct the service key for a token
   * @param uri of remote connection with a token
   * @return ""ip:port"" or ""host:port"" depending on the value of
   *          hadoop.security.token.service.use_ip",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java,init,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:init(org.apache.commons.configuration2.SubsetConfiguration),120,172,"/**
* Initializes the GangliaSink with configuration.
* @param conf SubsetConfiguration object
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getCanonicalServiceName,org.apache.hadoop.fs.FileSystem:getCanonicalServiceName(),453,460,"/**
* Returns canonical service name or null if child file systems exist.
*/","* Get a canonical service name for this FileSystem.
   * The token cache is the only user of the canonical service name,
   * and uses it to lookup this FileSystem's service tokens.
   * If the file system provides a token of its own then it must have a
   * canonical name, otherwise the canonical name can be null.
   *
   * Default implementation: If the FileSystem has child file systems
   * (such as an embedded file system) then it is assumed that the FS has no
   * tokens of its own and hence returns a null name; otherwise a service
   * name is built using Uri and port.
   *
   * @return a service string that uniquely identifies this file system, null
   *         if the filesystem does not implement tokens
   * @see SecurityUtil#buildDTServiceName(URI, int)",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getCanonicalServiceName,org.apache.hadoop.fs.AbstractFileSystem:getCanonicalServiceName(),1243,1245,"/**
* Returns canonical service name based on URI and default port. 
* @return canonicalized service name as string
*/","* Get a canonical name for this file system.
   * @return a URI string that uniquely identifies this file system",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,substituteCommonVariables,org.apache.hadoop.conf.Configuration:substituteCommonVariables(java.lang.String),1115,1117,"/**
* Substitutes common variables in a given expression.
* @param expr input mathematical expression
* @return modified expression with substituted variables
*/","* Provides a public wrapper over substituteVars in order to avoid compatibility issues.
   * See HADOOP-18021 for further details.
   *
   * @param expr the literal value of a config key
   * @return null if expr is null, otherwise the value resulting from expanding
   * expr using the algorithm above.
   * @throws IllegalArgumentException when more than
   * {@link Configuration#MAX_SUBST} replacements are required",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,get,org.apache.hadoop.conf.Configuration:get(java.lang.String),1263,1270,"/**
* Retrieves the value of a property by its name.
* @param name unique property identifier
* @return The resolved property value, or null if not found
*/","* Get the value of the <code>name</code> property, <code>null</code> if
   * no such property exists. If the key is deprecated, it returns the value of
   * the first key which replaces the deprecated key and is not null.
   * 
   * Values are processed for <a href=""#VariableExpansion"">variable expansion</a> 
   * before being returned.
   *
   * As a side effect get loads the properties from the sources if called for
   * the first time as a lazy init.
   * 
   * @param name the property name, will be trimmed before get value.
   * @return the value of the <code>name</code> or its replacing property, 
   *         or null if no such property exists.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,get,"org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)",1524,1531,"/**
* Retrieves a property value by name or deprecation context.
* @param name the property name to retrieve
* @param defaultValue default value if property not found
* @return The retrieved property value or default value
*/","* Get the value of the <code>name</code>. If the key is deprecated,
   * it returns the value of the first key which replaces the deprecated key
   * and is not null.
   * If no such property exists,
   * then <code>defaultValue</code> is returned.
   * 
   * @param name property name, will be trimmed before get value.
   * @param defaultValue default value.
   * @return property value, or <code>defaultValue</code> if the property 
   *         doesn't exist.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,doPost,"org.apache.hadoop.conf.ReconfigurationServlet:doPost(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",214,236,"/**
* Handles POST requests and applies reconfiguration changes.
* @param req HttpServletRequest object
* @param resp HttpServletResponse object
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,java.lang.String)",169,175,"/**
* Sets an option with a given key-value pair.
* @param key unique option identifier
* @param value option value
* @return this builder instance for chaining
*/",* Set optional Builder parameter.,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,java.lang.String)",256,261,"/**
* Adds a mandatory option with the given key and value.
* @param key unique identifier for the option
* @param value value associated with the key
*/","* Set mandatory option to the Builder.
   *
   * If the option is not supported or unavailable on the {@link FileSystem},
   * the client should expect {@link #build()} throws IllegalArgumentException.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setDefaultUri,"org.apache.hadoop.fs.FileSystem:setDefaultUri(org.apache.hadoop.conf.Configuration,java.net.URI)",311,313,"/**
 * Sets the default file system URI in the Configuration object.
 * @param conf configuration to update
 * @param uri default file system URI as a URI object
 */","* Set the default FileSystem URI in a configuration.
   * @param conf the configuration to alter
   * @param uri the new default filesystem uri",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLink,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.net.URI)",55,59,"/**
* Adds a link to the configuration.
* @param conf Configuration object
* @param mountTableName unique table identifier
* @param src source path
* @param target URI of linked resource
*/","* Add a link to the config for the specified mount table
   * @param conf - add the link to this conf
   * @param mountTableName mountTable.
   * @param src - the src path name
   * @param target - the target URI link",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMergeSlash,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",79,83,"/**
* Sets configuration property for link merge slash.
* @param conf Hadoop Configuration object
* @param mountTableName name of the mounted table
* @param target URI of the target resource
*/","* Add a LinkMergeSlash to the config for the specified mount table.
   *
   * @param conf configuration.
   * @param mountTableName mountTable.
   * @param target target.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkFallback,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",102,106,"/**
* Adds link fallback configuration to the given Hadoop config.
* @param conf Hadoop Configuration object
* @param mountTableName name of the table being mounted
* @param target URI of the fallback link
*/","* Add a LinkFallback to the config for the specified mount table.
   *
   * @param conf configuration.
   * @param mountTableName mountTable.
   * @param target targets.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMerge,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])",125,129,"/**
* Adds link merge configuration to the given Configuration object.
* @param conf Hadoop Configuration instance
* @param mountTableName name of the table to configure
* @param targets array of URIs for link merge targets
*/","* Add a LinkMerge to the config for the specified mount table.
   *
   * @param conf configuration.
   * @param mountTableName mountTable.
   * @param targets targets.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkNfly,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",150,156,"/**
* Adds a link to fly configuration setting.
* @param conf Hadoop Configuration object
* @param mountTableName table name for linking
* @param src source data path
* @param settings configuration property key
* @param targets target data paths
*/","* Add nfly link to configuration for the given mount table.
   *
   * @param conf configuration.
   * @param mountTableName mount table.
   * @param src src.
   * @param settings settings.
   * @param targets targets.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkRegex,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkRegex(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",191,202,"/**
* Sets a link regex configuration in the given Hadoop Configuration object.
* @param conf Hadoop Configuration object to update
* @param mountTableName table name for which the link regex is being set
* @param srcRegex source regex pattern
* @param targetStr target string value
* @param interceptorSettings optional settings key-value pairs to append to prefix
*/","* Add a LinkRegex to the config for the specified mount table.
   * @param conf - get mountable config from this conf
   * @param mountTableName - the mountable name of the regex config item
   * @param srcRegex - the src path regex expression that applies to this config
   * @param targetStr - the string of target path
   * @param interceptorSettings - the serialized interceptor string to be
   *                            applied while resolving the mapping",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,setHomeDirConf,"org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",220,228,"/**
* Sets the home directory configuration for a mount table.
* @param conf Configuration object to update
* @param mountTableName Name of the mount table
* @param homedir Home directory path (must start with '/')
*/","* Add config variable for homedir the specified mount table
   * @param conf - add to this conf
   * @param homedir - the home dir path starting with slash
   * @param mountTableName - the mount table.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,setUMask,"org.apache.hadoop.fs.permission.FsPermission:setUMask(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.permission.FsPermission)",400,402,"/**
* Sets the UMask value in the configuration.
* @param conf Hadoop Configuration object
* @param umask file system permissions to set as UMask
*/","* Set the user file creation mask (umask)
   * @param conf configuration.
   * @param umask umask.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,setCodecClasses,"org.apache.hadoop.io.compress.CompressionCodecFactory:setCodecClasses(org.apache.hadoop.conf.Configuration,java.util.List)",156,169,"/**
* Sets codec classes in configuration.
* @param conf Configuration object
* @param classes List of Class objects representing codecs to set
*/","* Sets a list of codec classes in the configuration. In addition to any
   * classes specified using this method, {@link CompressionCodec} classes on
   * the classpath are discovered using a Java ServiceLoader.
   * @param conf the configuration to modify
   * @param classes the list of classes to set",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,setDefaultCompressionType,"org.apache.hadoop.io.SequenceFile:setDefaultCompressionType(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$CompressionType)",262,265,"/**
* Sets default compression type in Configuration.
* @param job Configuration object
* @param val desired CompressionType value
*/","* Set the default compression type for sequence files.
   * @param job the configuration to modify
   * @param val the new compression type (none, block, record)",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,getProxyuserConfiguration,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig),107,119,"/**
* Retrieves configuration for proxy user from init parameters.
* @param filterConfig FilterConfig instance
* @return Configuration object or null if not found
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,getProxyuserConfiguration,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig),158,174,"/**
* Retrieves proxy user configuration from FilterConfig.
* @param filterConfig Filter initialization context
* @return Configuration object with proxy user settings or null if not found
*/","* Returns the proxyuser configuration. All returned properties must start
   * with <code>proxyuser.</code>'
   * <p>
   * Subclasses may override this method if the proxyuser configuration is 
   * read from other place than the filter init parameters.
   *
   * @param filterConfig filter configuration object
   * @return the proxyuser configuration properties.
   * @throws ServletException thrown if the configuration could not be created.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,prepareConf,org.apache.hadoop.security.CompositeGroupsMapping:prepareConf(java.lang.String),175,191,"/**
* Prepares configuration for a specific provider by filtering and restoring properties from the main configuration. 
* @param providerName name of the provider to prepare configuration for
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,init,org.apache.hadoop.security.alias.CredentialShell:init(java.lang.String[]),78,126,"/**
* Parses command line arguments and initializes the tool.
* @param args array of string arguments
* @return int exit status (0 for success, non-zero for failure)
*/","* Parse the command line arguments and initialize the data.
   * <pre>
   * % hadoop credential create alias [-provider providerPath]
   * % hadoop credential list [-provider providerPath]
   * % hadoop credential check alias [-provider providerPath]
   * % hadoop credential delete alias [-provider providerPath] [-f]
   * </pre>
   * @param args args.
   * @return 0 if the argument(s) were recognized, 1 otherwise
   * @throws IOException raised on errors performing I/O.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setAuthenticationMethod,"org.apache.hadoop.security.SecurityUtil:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,org.apache.hadoop.conf.Configuration)",741,748,"/**
* Sets the authentication method for Hadoop security configuration.
* @param authenticationMethod desired authentication method
* @param conf Hadoop Configuration object to modify
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setInt,"org.apache.hadoop.conf.Configuration:setInt(java.lang.String,int)",1582,1584,"/**
 * Sets an integer property with the given name.
 * @param name property name
 * @param value integer value to be set
 */","* Set the value of the <code>name</code> property to an <code>int</code>.
   * 
   * @param name property name.
   * @param value <code>int</code> value of the property.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setLong,"org.apache.hadoop.conf.Configuration:setLong(java.lang.String,long)",1655,1657,"/**
* Sets a named long value.
* @param name unique identifier for this value
* @param value the long value to be stored
*/","* Set the value of the <code>name</code> property to a <code>long</code>.
   * 
   * @param name property name.
   * @param value <code>long</code> value of the property.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setFloat,"org.apache.hadoop.conf.Configuration:setFloat(java.lang.String,float)",1684,1686,"/**
* Sets a named float property with the given value.
* @param name unique property identifier
* @param value the float value to be assigned
*/","* Set the value of the <code>name</code> property to a <code>float</code>.
   * 
   * @param name property name.
   * @param value property value.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setDouble,"org.apache.hadoop.conf.Configuration:setDouble(java.lang.String,double)",1713,1715,"/**
 * Sets a named double value.
 * @param name unique identifier for the value
 * @param value numeric value to be stored
 */","* Set the value of the <code>name</code> property to a <code>double</code>.
   * 
   * @param name property name.
   * @param value property value.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setBoolean,"org.apache.hadoop.conf.Configuration:setBoolean(java.lang.String,boolean)",1750,1752,"/**
* Sets a named boolean attribute to the specified value.
* @param name unique attribute identifier
* @param value true or false value to store
*/","* Set the value of the <code>name</code> property to a <code>boolean</code>.
   * 
   * @param name property name.
   * @param value <code>boolean</code> value of the property.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setTimeDuration,"org.apache.hadoop.conf.Configuration:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)",1868,1870,"/**
* Sets time duration with specified unit.
* @param name identifier
* @param value milliseconds value
* @param unit time unit (e.g. seconds, minutes)
*/","* Set the value of <code>name</code> to the given time duration. This
   * is equivalent to <code>set(&lt;name&gt;, value + &lt;time suffix&gt;)</code>.
   * @param name Property name
   * @param value Time duration
   * @param unit Unit of time",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setStorageSize,"org.apache.hadoop.conf.Configuration:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)",2039,2041,"/**
* Sets storage size with specified unit.
* @param name storage name
* @param value storage size value
* @param unit storage unit (e.g. Byte, KB, MB)
*/","* Sets Storage Size for the specified key.
   *
   * @param name - Key to set.
   * @param value - The numeric value to set.
   * @param unit - Storage Unit to be used.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setPattern,"org.apache.hadoop.conf.Configuration:setPattern(java.lang.String,java.util.regex.Pattern)",2089,2092,"/**
* Sets a regular expression pattern with the specified name.
* @param name unique pattern identifier
* @param pattern non-null regular expression pattern
*/","* Set the given property to <code>Pattern</code>.
   * If the pattern is passed as null, sets the empty pattern which results in
   * further calls to getPattern(...) returning the default value.
   *
   * @param name property name
   * @param pattern new value",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setStrings,"org.apache.hadoop.conf.Configuration:setStrings(java.lang.String,java.lang.String[])",2405,2407,"/**
* Sets multiple strings with a single key.
* @param name unique identifier
* @param values array of string values
*/","* Set the array of string values for the <code>name</code> property as 
   * as comma delimited values.  
   * 
   * @param name property name.
   * @param values The values",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setSocketAddr,"org.apache.hadoop.conf.Configuration:setSocketAddr(java.lang.String,java.net.InetSocketAddress)",2578,2580,"/**
* Sets socket address information using the provided name and address.
* @param name human-readable identifier
* @param addr network address details
*/","* Set the socket address for the <code>name</code> property as
   * a <code>host:port</code>.
   * @param name property name.
   * @param addr inetSocketAddress addr.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setClass,"org.apache.hadoop.conf.Configuration:setClass(java.lang.String,java.lang.Class,java.lang.Class)",2811,2815,"/**
* Sets class metadata with validation.
* @param name unique identifier
* @param theClass class to be registered
* @param xface expected interface type
*/","* Set the value of the <code>name</code> property to the name of a 
   * <code>theClass</code> implementing the given interface <code>xface</code>.
   * 
   * An exception is thrown if <code>theClass</code> does not implement the 
   * interface <code>xface</code>. 
   * 
   * @param name property name.
   * @param theClass property value.
   * @param xface the interface implemented by the named class.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,readFields,org.apache.hadoop.conf.Configuration:readFields(java.io.DataInput),3949,3962,"/**
* Reads fields from the input stream and populates this object.
* @throws IOException if I/O error occurs
*/",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDelete,org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File),169,171,"/**
* Recursively deletes a directory and its contents.
* @param dir directory to delete
* @return true if deletion was successful, false otherwise
*/","* Delete a directory and all its contents.  If
   * we return false, the directory may be partially-deleted.
   * (1) If dir is symlink to a file, the symlink is deleted. The file pointed
   *     to by the symlink is not deleted.
   * (2) If dir is symlink to a directory, symlink is deleted. The directory
   *     pointed to by symlink is not deleted.
   * (3) If dir is a normal file, it is deleted.
   * (4) If dir is a normal directory, then dir and all its contents recursively
   *     are deleted.
   * @param dir dir.
   * @return fully delete status.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDeleteContents,"org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File,boolean)",282,316,"/**
* Recursively deletes all contents within the specified directory.
* @param dir the directory to delete from
* @param tryGrantPermissions whether to attempt granting permissions to access the directory
* @return true if deletion was successful, false otherwise
*/","* Delete the contents of a directory, not the directory itself.  If
   * we return false, the directory may be partially-deleted.
   * If dir is a symlink to a directory, all the contents of the actual
   * directory pointed to by dir will be deleted.
   *
   * @param dir dir.
   * @param tryGrantPermissions if 'true', try grant +rwx permissions to this
   * and all the underlying directories before trying to delete their contents.
   * @return fully delete contents status.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processPath,org.apache.hadoop.fs.shell.FsUsage$Df:processPath(org.apache.hadoop.fs.shell.PathData),129,157,"/**
* Processes a file system path and updates the usages table accordingly.
* @param item PathData object containing file system information
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,open,"org.apache.hadoop.fs.RawLocalFileSystem:open(org.apache.hadoop.fs.Path,int)",392,397,"/**
* Opens a file stream from the local file system with specified buffer size.
* @param f path to the file
* @param bufferSize input/output buffer size in bytes
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,open,"org.apache.hadoop.fs.RawLocalFileSystem:open(org.apache.hadoop.fs.PathHandle,int)",399,409,"/**
* Opens a file stream for the specified path handle.
* @param fd path handle to open
* @param bufferSize input/output buffer size
* @return FSDataInputStream object or throws IOException on error
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,append,"org.apache.hadoop.fs.RawLocalFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",535,545,"/**
* Appends data to a file with specified buffer size and progress tracking.
* @param f file path
* @param bufferSize output buffer size
* @param progress progress tracker
* @throws IOException if directory append attempt or I/O error occurs
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,truncate,"org.apache.hadoop.fs.RawLocalFileSystem:truncate(org.apache.hadoop.fs.Path,long)",675,698,"/**
* Truncates a file to a specified length.
* @param f Path to the file
* @param newLength desired file size in bytes
* @return true on successful truncation, false otherwise
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,listStatus,org.apache.hadoop.fs.RawLocalFileSystem:listStatus(org.apache.hadoop.fs.Path),729,766,"/**
* Lists file statuses for the given path.
* @param f Path to list statuses for
* @return Array of FileStatus objects or null if not found
*/","* {@inheritDoc}
   *
   * (<b>Note</b>: Returned list is not sorted in any given order,
   * due to reliance on Java's {@link File#list()} API.)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,deprecatedGetFileLinkStatusInternal,org.apache.hadoop.fs.RawLocalFileSystem:deprecatedGetFileLinkStatusInternal(org.apache.hadoop.fs.Path),1248,1285,"/**
* Retrieves status for the given file path.
* @param f file path to check
*/","* Deprecated. Remains for legacy support. Should be removed when {@link Stat}
   * gains support for Windows and other operating systems.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,fixFileStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystem:fixFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",532,550,"/**
* Converts and updates FileStatus object with a new viewfs URI path.
* @param orig original FileStatus object
* @param qualified Path object representing the target file system
* @return updated FileStatus object or null if not found
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,delete,"org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,boolean)",370,414,"/**
* Deletes a file or directory on SFTP server.
* @param channel SFTP connection
* @param file Path to delete
* @param recursive Whether to recursively delete contents of directories
* @return true if successful, false otherwise
*/","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,<init>,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:<init>(java.io.InputStream),354,356,"/**
* Initializes BZip2 compression input stream with default settings.
* @param in underlying input stream
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createInputStream,"org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,long,long,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",200,211,"/**
* Creates a compressed input stream with specified seek and read modes.
* @param seekableIn the underlying seekable input stream
* @param decompressor used for decompression
* @param start starting position to seek to
* @param end ending position to use for reading
* @param readMode read mode to apply
*/","* Creates CompressionInputStream to be used to read off uncompressed data
   * in one of the two reading modes. i.e. Continuous or Blocked reading modes
   *
   * @param seekableIn The InputStream
   * @param start The start offset into the compressed stream
   * @param end The end offset into the compressed stream
   * @param readMode Controls whether progress is reported continuously or
   *                 only at block boundaries.
   *
   * @return CompressionInputStream for BZip2 aligned at block boundaries",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,read,"org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read(byte[],int,int)",483,522,"/**
* Reads specified number of bytes from input stream into given byte array.
* @param b byte array to read into
* @param off starting offset in the array
* @param len number of bytes to read
* @return actual number of bytes read, or -1 if end-of-stream is reached",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,add,"org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:add(java.lang.Object,java.lang.reflect.Field)",133,159,"/**
* Adds Metric annotations from a Field to the source Object.
* @param source Object containing fields to annotate
* @param field Field containing Metric annotations
*/","* Change the declared field {@code field} in {@code source} Object to
   * {@link MutableMetric}",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,init,org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:init(org.apache.commons.configuration2.SubsetConfiguration),57,84,"/**
* Initializes the object by parsing SubsetConfiguration and populating tags map.
* @param conf SubsetConfiguration instance
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getCanonicalServiceName,org.apache.hadoop.fs.DelegateToFileSystem:getCanonicalServiceName(),262,265,"/**
* Returns the canonical service name.
* @return Service name as per AbstractFileSystem implementation.",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getCanonicalServiceName,org.apache.hadoop.fs.FilterFs:getCanonicalServiceName(),312,315,"/**
* Returns canonical service name from underlying file system. 
* @return Canonical service name as string or null if not available
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,getConf,"org.apache.hadoop.fs.StorageType:getConf(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.StorageType,java.lang.String)",120,123,"/**
* Retrieves configuration value by type and key.
* @param conf Configuration object
* @param t storage type (e.g. HDFS, S3)
* @param name configuration key
* @return configuration value as String or null if not found
*/","* Get the configured values for different StorageType.
   * @param conf - absolute or fully qualified path
   * @param t - the StorageType
   * @param name - the sub-name of key
   * @return the file system of the path",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getTransferMode,org.apache.hadoop.fs.ftp.FTPFileSystem:getTransferMode(org.apache.hadoop.conf.Configuration),188,209,"/**
* Retrieves transfer mode from Hadoop Configuration.
* @param conf Hadoop configuration object
* @return Transfer mode (FTP.BLOCK_TRANSFER_MODE by default)
*/","* Set FTP's transfer mode based on configuration. Valid values are
   * STREAM_TRANSFER_MODE, BLOCK_TRANSFER_MODE and COMPRESSED_TRANSFER_MODE.
   * <p>
   * Defaults to BLOCK_TRANSFER_MODE.
   *
   * @param conf
   * @return",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,setDataConnectionMode,"org.apache.hadoop.fs.ftp.FTPFileSystem:setDataConnectionMode(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.conf.Configuration)",222,240,"/**
* Sets FTP data connection mode based on configuration.
* @param client FTPClient instance
* @param conf Configuration object with mode settings
*/","* Set the FTPClient's data connection mode based on configuration. Valid
   * values are ACTIVE_LOCAL_DATA_CONNECTION_MODE,
   * PASSIVE_LOCAL_DATA_CONNECTION_MODE and PASSIVE_REMOTE_DATA_CONNECTION_MODE.
   * <p>
   * Defaults to ACTIVE_LOCAL_DATA_CONNECTION_MODE.
   *
   * @param client
   * @param conf
   * @throws IOException",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getHomeDirValue,"org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration,java.lang.String)",245,249,"/**
* Retrieves home directory value from configuration.
* @param conf Configuration object
* @param mountTableName name of the mount table
* @return home directory value or null if not found
*/","* Get the value of the home dir conf value for specified mount table
   * @param conf - from this conf
   * @param mountTableName - the mount table
   * @return home dir value, null if variable is not in conf",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getUMask,org.apache.hadoop.fs.permission.FsPermission:getUMask(org.apache.hadoop.conf.Configuration),325,349,"/**
* Retrieves the default umask from configuration, parsing it if present.
* @param conf Configuration object
* @return FsPermission representing the umask value
*/","* Get the user file creation mask (umask)
   * 
   * {@code UMASK_LABEL} config param has umask value that is either symbolic 
   * or octal.
   * 
   * Symbolic umask is applied relative to file mode creation mask; 
   * the permission op characters '+' clears the corresponding bit in the mask, 
   * '-' sets bits in the mask.
   * 
   * Octal umask, the specified bits are set in the file mode creation mask.
   *
   * @param conf configuration.
   * @return FsPermission UMask.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",791,796,"/**
* Initializes DiskBlockFactory with configuration and directory allocator.
* @param keyToBufferDir key to buffer directory in conf
* @param conf Hadoop configuration object
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodecClasses,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClasses(org.apache.hadoop.conf.Configuration),111,147,"/**
* Retrieves a list of CompressionCodec classes from service providers and configuration.
* @param conf Hadoop Configuration object
* @return List of Class<? extends CompressionCodec> objects representing codec classes
*/","* Get the list of codecs discovered via a Java ServiceLoader, or
   * listed in the configuration. Codecs specified in configuration come
   * later in the returned list, and are considered to override those
   * from the ServiceLoader.
   * @param conf the configuration to look in
   * @return a list of the {@link CompressionCodec} classes",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getDefaultCompressionType,org.apache.hadoop.io.SequenceFile:getDefaultCompressionType(org.apache.hadoop.conf.Configuration),251,255,"/**
* Returns default compression type based on configuration.
* @param job Configuration object with compression type setting
* @return CompressionType enum value or RECORD if not specified
*/","* Get the compression type for the reduce outputs
   * @param job the job config to look in
   * @return the kind of compression to use",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,setConf,org.apache.hadoop.net.SocksSocketFactory:setConf(org.apache.hadoop.conf.Configuration),135,142,"/**
* Sets configuration and applies proxy settings if configured.
* @param conf Configuration object with Hadoop proxy server key
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/AbstractDNSToSwitchMapping.java,isSingleSwitchByScriptPolicy,org.apache.hadoop.net.AbstractDNSToSwitchMapping:isSingleSwitchByScriptPolicy(),135,138,"/**
* Checks if policy is single switch based on script configuration.
* @return true if no script file name set, false otherwise
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,createWebAppContext,"org.apache.hadoop.http.HttpServer2:createWebAppContext(org.apache.hadoop.http.HttpServer2$Builder,org.apache.hadoop.security.authorize.AccessControlList,java.lang.String)",834,860,"/**
* Creates a WebAppContext instance with specified configuration.
* @param b Builder object containing application name and config
* @param adminsAcl Access control list for administrators
* @param appDir Directory path for the web application
* @return A fully configured WebAppContext instance
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,stringifySecurityProperty,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:stringifySecurityProperty(java.lang.String),314,333,"/**
* Converts a security property to a string representation.
* @param property the property name
*/","* Turn a security property into a nicely formatted set of <i>name=value</i>
   * strings, allowing for either the property or the configuration not to be
   * set.
   *
   * @param property the property to stringify
   * @return the stringified property",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateHadoopTokenFiles,org.apache.hadoop.security.KDiag:validateHadoopTokenFiles(org.apache.hadoop.conf.Configuration),521,553,"/**
* Validates Hadoop token files by checking system properties and configuration.
* @param conf the Hadoop Configuration object
*/","* Validate that hadoop.token.files (if specified) exist and are valid.
   * @throws ClassNotFoundException
   * @throws SecurityException
   * @throws NoSuchMethodException
   * @throws KerberosDiagsFailure",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,locateKeystore,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:locateKeystore(),314,339,"/**
* Locates and initializes the Java KeyStore.
* @throws IOException if key store creation or loading fails
*/","* Open up and initialize the keyStore.
   *
   * @throws IOException If there is a problem reading the password file
   * or a problem reading the keystore.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,needsPassword,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:needsPassword(),341,346,"/**
* Determines if password is required based on environment variable or configuration file.
* @return true if password is needed, false otherwise
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getLocalHostName,org.apache.hadoop.security.SecurityUtil:getLocalHostName(org.apache.hadoop.conf.Configuration),255,272,"/**
* Retrieves local host name from configuration or falls back to system query.
* @param conf Configuration object (may be null)
*/","* Retrieve the name of the current host. Multihomed hosts may restrict the
   * hostname lookup to a specific interface and nameserver with {@link
   * org.apache.hadoop.fs.CommonConfigurationKeysPublic#HADOOP_SECURITY_DNS_INTERFACE_KEY}
   * and {@link org.apache.hadoop.fs.CommonConfigurationKeysPublic#HADOOP_SECURITY_DNS_NAMESERVER_KEY}
   *
   * @param conf Configuration object. May be null.
   * @return
   * @throws UnknownHostException",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getClientPrincipal,"org.apache.hadoop.security.SecurityUtil:getClientPrincipal(java.lang.Class,org.apache.hadoop.conf.Configuration)",404,413,"/**
* Retrieves the client principal from configuration using Kerberos info.
* @param protocol Class representing the communication protocol
* @param conf Configuration object containing key-value pairs
* @return Client principal string or null if not found
*/","* Look up the client principal for a given protocol. It searches all known
   * SecurityInfo providers.
   * @param protocol the protocol class to get the information for
   * @param conf configuration object
   * @return client principal or null if it has no client principal defined.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,needsPassword,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:needsPassword(),308,313,"/**
* Checks if password is required based on environment variable and file configuration.
* @return true if password is needed, false otherwise
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getMetricsTimeUnit,org.apache.hadoop.ipc.metrics.RpcMetrics:getMetricsTimeUnit(org.apache.hadoop.conf.Configuration),189,204,"/**
* Retrieves the metrics time unit from configuration.
* @param conf Hadoop Configuration object
* @return TimeUnit value (default or custom)",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,validateSslConfiguration,org.apache.hadoop.util.curator.ZKCuratorManager:validateSslConfiguration(org.apache.hadoop.conf.Configuration),196,221,"/**
* Validates SSL configuration parameters.
* @param config Configuration object containing zookeeper settings
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmed,org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String),1320,1328,"/**
* Retrieves and trims user input by key, returning null if not found.
* @param name unique user identifier
* @return trimmed String value or null if absent
*/","* Get the value of the <code>name</code> property as a trimmed <code>String</code>, 
   * <code>null</code> if no such property exists. 
   * If the key is deprecated, it returns the value of
   * the first key which replaces the deprecated key and is not null
   * 
   * Values are processed for <a href=""#VariableExpansion"">variable expansion</a> 
   * before being returned. 
   * 
   * @param name the property name.
   * @return the value of the <code>name</code> or its replacing property, 
   *         or null if no such property exists.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setIfUnset,"org.apache.hadoop.conf.Configuration:setIfUnset(java.lang.String,java.lang.String)",1499,1503,"/**
* Sets a property to the specified value only if it's currently unset.
* @param name unique property identifier
* @param value new property value
*/","* Sets a property if it is currently unset.
   * @param name the property name
   * @param value the new value",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",1906,1914,"/**
* Converts a duration value from one time unit to another.
* @param name configuration key
* @param defaultValue default value if not set
* @param defaultUnit initial time unit
* @param returnUnit desired output time unit
* @return converted duration in the specified unit
*/","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d). If no unit is
   * provided, the default unit is applied.
   *
   * @param name Property name
   * @param defaultValue Value returned if no mapping exists.
   * @param defaultUnit Default time unit if no valid suffix is provided.
   * @param returnUnit The unit used for the returned value.
   * @throws NumberFormatException If the property stripped of its unit is not
   *         a number
   * @return time duration in given time unit",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",1916,1924,"/**
* Retrieves time duration value for the given name and unit.
* @param name unique identifier
* @param defaultValue default value to use if not found
* @param defaultUnit default unit of time (e.g. SECONDS)
* @param returnUnit desired unit of time to return in
* @return time duration value in specified unit, or -1 if error
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStorageSize,"org.apache.hadoop.conf.Configuration:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)",1988,2005,"/**
* Parses and converts storage size from given key or default value.
* @param name key containing storage size (e.g. ""1GB"")
* @param defaultValue default size in case key is empty
* @param targetUnit desired unit of measurement for result
* @return double representation of storage size in target unit
*/","* Gets the Storage Size from the config, or returns the defaultValue. The
   * unit of return value is specified in target unit.
   *
   * @param name - Key Name
   * @param defaultValue - Default Value -- e.g. 100MB
   * @param targetUnit - The units that we want result to be in.
   * @return double -- formatted in target Units",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStorageSize,"org.apache.hadoop.conf.Configuration:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)",2017,2030,"/**
* Retrieves storage size by name and converts it to the specified unit.
* @param name unique storage identifier
* @param defaultValue default value if not found
* @param targetUnit desired unit for result (e.g. GB, TB)
* @return storage size in target unit or default value if not found
*/","* Gets storage size from a config file.
   *
   * @param name - Key to read.
   * @param defaultValue - The default value to return in case the key is
   * not present.
   * @param targetUnit - The Storage unit that should be used
   * for the return value.
   * @return - double value in the Storage Unit specified.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPattern,"org.apache.hadoop.conf.Configuration:getPattern(java.lang.String,java.util.regex.Pattern)",2067,2079,"/**
* Retrieves a regular expression pattern by name, using a default value if invalid or empty.
* @param name unique identifier for the pattern
* @param defaultValue default pattern to use if invalid
* @return Pattern object or defaultValue if not valid
*/","* Get the value of the <code>name</code> property as a <code>Pattern</code>.
   * If no such property is specified, or if the specified value is not a valid
   * <code>Pattern</code>, then <code>DefaultValue</code> is returned.
   * Note that the returned value is NOT trimmed by this method.
   *
   * @param name property name
   * @param defaultValue default value
   * @return property value as a compiled Pattern, or defaultValue",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStringCollection,org.apache.hadoop.conf.Configuration:getStringCollection(java.lang.String),2310,2313,"/**
* Retrieves a collection of strings from a configuration source.
* @param name unique identifier of the string collection
*/","* Get the comma delimited values of the <code>name</code> property as 
   * a collection of <code>String</code>s.  
   * If no such property is specified then empty collection is returned.
   * <p>
   * This is an optimized version of {@link #getStrings(String)}
   * 
   * @param name property name.
   * @return property value as a collection of <code>String</code>s.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStrings,org.apache.hadoop.conf.Configuration:getStrings(java.lang.String),2324,2327,"/**
* Retrieves an array of strings from a given configuration value.
* @param name unique key in the configuration
*/","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s.  
   * If no such property is specified then <code>null</code> is returned.
   * 
   * @param name property name.
   * @return property value as an array of <code>String</code>s, 
   *         or <code>null</code>.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStrings,"org.apache.hadoop.conf.Configuration:getStrings(java.lang.String,java.lang.String[])",2339,2346,"/**
* Returns an array of strings from a configuration value or default values.
* @param name unique key for the configuration value
* @param defaultValue array of default string values to return if config is missing
*/","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s.  
   * If no such property is specified then default value is returned.
   * 
   * @param name property name.
   * @param defaultValue The default value
   * @return property value as an array of <code>String</code>s, 
   *         or default value.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmedStringCollection,org.apache.hadoop.conf.Configuration:getTrimmedStringCollection(java.lang.String),2356,2363,"/**
* Retrieves a collection of trimmed strings from the specified key.
* @param name unique key identifier
*/","* Get the comma delimited values of the <code>name</code> property as 
   * a collection of <code>String</code>s, trimmed of the leading and trailing whitespace.  
   * If no such property is specified then empty <code>Collection</code> is returned.
   *
   * @param name property name.
   * @return property value as a collection of <code>String</code>s, or empty <code>Collection</code>",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmedStrings,org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String),2374,2377,"/**
* Retrieves trimmed strings from a configuration.
* @param name unique key
*/","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s, trimmed of the leading and trailing whitespace.
   * If no such property is specified then an empty array is returned.
   * 
   * @param name property name.
   * @return property value as an array of trimmed <code>String</code>s, 
   *         or empty array.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmedStrings,"org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String,java.lang.String[])",2389,2396,"/**
* Retrieves trimmed strings from storage by name; returns default values if not found.
* @param name unique key identifier
* @param defaultValue array of fallback string values (optional)
* @return array of trimmed string values or default value array if not found
*/","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s, trimmed of the leading and trailing whitespace.
   * If no such property is specified then default value is returned.
   * 
   * @param name property name.
   * @param defaultValue The default value
   * @return property value as an array of trimmed <code>String</code>s, 
   *         or default value.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPropsWithPrefix,org.apache.hadoop.conf.Configuration:getPropsWithPrefix(java.lang.String),3032,3043,"/**
* Retrieves configuration properties with the specified prefix.
* @param confPrefix prefix to filter configuration properties
* @return Map of filtered configuration properties (key-value pairs)
*/","* Constructs a mapping of configuration and includes all properties that
   * start with the specified configuration prefix.  Property names in the
   * mapping are trimmed to remove the configuration prefix.
   *
   * @param confPrefix configuration prefix
   * @return mapping of configuration properties with prefix stripped",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,appendJSONProperty,"org.apache.hadoop.conf.Configuration:appendJSONProperty(com.fasterxml.jackson.core.JsonGenerator,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",3861,3881,"/**
* Appends JSON property with redacted value to the generator.
* @param name property key
* @param redactor configuration for redacting values
*/","* Write property and its attributes as json format to given
   * {@link JsonGenerator}.
   *
   * @param jsonGen json writer
   * @param config configuration
   * @param name property name
   * @throws IOException raised on errors performing I/O.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationUtil.java,getChangedProperties,"org.apache.hadoop.conf.ReconfigurationUtil:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",39,65,"/**
* Retrieves properties that have changed between two configurations.
* @param newConf the updated configuration
* @param oldConf the previous configuration
* @return a collection of PropertyChanges for modified properties
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,reconfigureProperty,"org.apache.hadoop.conf.ReconfigurableBase:reconfigureProperty(java.lang.String,java.lang.String)",223,241,"/**
* Updates a reconfigurable property value.
* @param property unique property identifier
* @param newVal new value for the property or null to unset
*/","* {@inheritDoc}
   *
   * This method makes the change to this objects {@link Configuration}
   * and calls reconfigurePropertyImpl to update internal data structures.
   * This method cannot be overridden, subclasses should instead override
   * reconfigurePropertyImpl.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,get,org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String),46,51,"/**
* Retrieves and logs a string value from parent storage.
* @param name key to fetch
* @return fetched string value or null if not found
*/",* See {@link Configuration#get(String)}.,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,create,"org.apache.hadoop.ha.NodeFencer:create(org.apache.hadoop.conf.Configuration,java.lang.String)",83,90,"/**
* Creates a NodeFencer instance from configuration.
* @param conf Configuration object
* @param confKey key for the fencing configuration string
* @return NodeFencer object or null if config not found
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getDefaultMountTableName,org.apache.hadoop.fs.viewfs.ConfigUtil:getDefaultMountTableName(org.apache.hadoop.conf.Configuration),260,263,"/**
* Retrieves default mount table name from configuration.
* @param conf configuration object
* @return default mount table name or null if not set
*/","* Get the name of the default mount table to use. If
   * {@link Constants#CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE_NAME_KEY} is specified,
   * it's value is returned. Otherwise,
   * {@link Constants#CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE} is returned.
   *
   * @param conf Configuration to use.
   * @return the name of the default mount table to use.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,getCodecClassName,"org.apache.hadoop.io.erasurecode.CodecUtil:getCodecClassName(org.apache.hadoop.conf.Configuration,java.lang.String)",256,285,"/**
* Retrieves the codec class name based on the provided codec type.
* @param conf Hadoop Configuration object
* @param codec Type of erasure code (e.g. RS, XOR, etc.)
* @return Class name of the corresponding codec or custom codec class name if configured
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,isNativeBzip2Loaded,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration),47,70,"/**
* Determines if the native Bzip2 library is loaded.
* @param conf Hadoop configuration
*/","* Check if native-bzip2 code is loaded &amp; initialized correctly and
   * can be loaded for this job.
   * 
   * @param conf configuration
   * @return <code>true</code> if native-bzip2 is loaded &amp; initialized
   *         and can be loaded for this job, else <code>false</code>",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getDefaultSocketFactory,org.apache.hadoop.net.NetUtils:getDefaultSocketFactory(org.apache.hadoop.conf.Configuration),121,130,"/**
* Retrieves the default socket factory based on configuration settings.
* @param conf Hadoop Configuration object
*/","* Get the default socket factory as specified by the configuration
   * parameter <tt>hadoop.rpc.socket.factory.default</tt>
   * 
   * @param conf the configuration
   * @return the default socket factory as specified in the configuration or
   *         the JVM default socket factory if the configuration does not
   *         contain a default socket factory property.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,load,org.apache.hadoop.net.TableMapping$RawTableMapping:load(),93,125,"/**
* Loads configuration mapping from file.
* @return Map of key-value pairs or null on failure
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/lib/StaticUserWebFilter.java,getUsernameFromConf,org.apache.hadoop.http.lib.StaticUserWebFilter:getUsernameFromConf(org.apache.hadoop.conf.Configuration),133,146,"/**
* Retrieves username from configuration.
* @param conf Configuration object
* @return Username as a string or null if not found
*/",* Retrieve the static username from the configuration.,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,setEnabledProtocols,org.apache.hadoop.http.HttpServer2$Builder:setEnabledProtocols(org.eclipse.jetty.util.ssl.SslContextFactory),665,696,"/**
* Resets and sets enabled SSL/TLS protocols based on configuration.
* @param sslContextFactory SslContextFactory instance
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,parseStaticMapping,org.apache.hadoop.security.Groups:parseStaticMapping(org.apache.hadoop.conf.Configuration),164,191,"/**
* Parses static user-to-group mapping configuration.
* @param conf Hadoop configuration object
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printConfOpt,org.apache.hadoop.security.KDiag:printConfOpt(java.lang.String),907,909,"/**
* Prints configuration option value to console.
* @param option name of the configuration option
*/","* Print a configuration option, or {@link #UNSET} if unset.
   *
   * @param option option to print",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,<init>,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore:<init>(org.apache.hadoop.conf.Configuration),868,873,"/**
* Initializes TruststoreKeystore with configuration settings.
* @param conf Configuration object containing SSL keystore and truststore details
*/","* Configuration for the ZooKeeper connection when SSL/TLS is enabled.
     * When a value is not configured, ensure that empty string is set instead of null.
     *
     * @param conf ZooKeeper Client configuration",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getDirContext,org.apache.hadoop.security.LdapGroupsMapping:getDirContext(),653,706,"/**
* Establishes a connection to the LDAP directory context.
* @throws NamingException if connection fails
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,spawnAutoRenewalThreadForUserCreds,org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForUserCreds(boolean),882,899,"/**
* Spawns a thread to auto-renew Kerberos tickets for user credentials.
* @param force whether to spawn the thread regardless of conditions
*/","* Spawn a thread to do periodic renewals of kerberos credentials. NEVER
   * directly call this method. This method should only be used for ticket cache
   * based kerberos credentials.
   *
   * @param force - used by tests to forcibly spawn thread",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,getHostnameVerifier,org.apache.hadoop.security.ssl.SSLFactory:getHostnameVerifier(org.apache.hadoop.conf.Configuration),206,210,"/**
* Retrieves a hostname verifier based on configuration.
* @param conf application configuration
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getAuthenticationMethod,org.apache.hadoop.security.SecurityUtil:getAuthenticationMethod(org.apache.hadoop.conf.Configuration),730,739,"/**
* Retrieves authentication method from configuration.
* @param conf Configuration object
* @return AuthenticationMethod enum value or throws exception if invalid
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoCodec.java,getCodecClasses,"org.apache.hadoop.crypto.CryptoCodec:getCodecClasses(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)",105,140,"/**
* Retrieves a list of configured crypto codec classes for the specified cipher suite.
* @param conf Hadoop configuration
* @param cipherSuite Cipher suite to fetch classes for
* @return List of CryptoCodec class types or null if not found
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,setConf,org.apache.hadoop.crypto.JceCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration),83,102,"/**
 * Initializes encryption settings from the given configuration.
 * @param conf Hadoop Configuration object
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,setConf,org.apache.hadoop.crypto.random.OsSecureRandom:setConf(org.apache.hadoop.conf.Configuration),83,90,"/**
* Updates configuration and closes current secure random device.
* @param conf new Configuration object
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,<init>,org.apache.hadoop.crypto.key.KeyProvider:<init>(org.apache.hadoop.conf.Configuration),403,417,"/**
* Configures KeyProvider with given configuration.
* @param conf Hadoop Configuration object
*/","* Constructor.
   * 
   * @param conf configuration for the provider",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,"org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",143,146,"/**
* Initializes the builder with context and configuration.
* @param context application context
* @param conf Hadoop configuration object
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getZKAcls,org.apache.hadoop.util.curator.ZKCuratorManager:getZKAcls(org.apache.hadoop.conf.Configuration),97,109,"/**
* Parses and returns a list of ZooKeeper ACLs from configuration.
* @param conf Hadoop Configuration object
* @return List of ACL objects or empty list if not found
*/","* Utility method to fetch the ZK ACLs from the configuration.
   *
   * @param conf configuration.
   * @throws java.io.IOException if the Zookeeper ACLs configuration file
   * cannot be read
   * @return acl list.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/Hash.java,getHashType,org.apache.hadoop.util.hash.Hash:getHashType(org.apache.hadoop.conf.Configuration),64,68,"/**
* Retrieves hash type from configuration.
* @param conf Hadoop Configuration object
*/","* This utility method converts the name of the configured
   * hash type to a symbolic constant.
   * @param conf configuration
   * @return one of the predefined constants",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getEnumSet,"org.apache.hadoop.conf.Configuration:getEnumSet(java.lang.String,java.lang.Class,boolean)",1803,1809,"/**
* Retrieves an EnumSet instance from configuration.
* @param key configuration key
* @param enumClass Enum class to parse into set
* @param ignoreUnknown whether to ignore unknown enum values
*/","* Build an enumset from a comma separated list of values.
   * Case independent.
   * Special handling of ""*"" meaning: all values.
   * @param key key to look for
   * @param enumClass class of enum
   * @param ignoreUnknown should unknown values raise an exception?
   * @return a mutable set of the identified enum values declared in the configuration
   * @param <E> enumeration type
   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,
   *           or there are two entries in the enum which differ only by case.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getRange,"org.apache.hadoop.conf.Configuration:getRange(java.lang.String,java.lang.String)",2296,2298,"/**
* Retrieves integer range from configuration.
* @param name unique identifier
* @param defaultValue default value to return if not found
*/","* Parse the given attribute as a set of integer ranges.
   * @param name the attribute name
   * @param defaultValue the default value if it is not set
   * @return a new set of ranges from the configured value",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigRedactor.java,<init>,org.apache.hadoop.conf.ConfigRedactor:<init>(org.apache.hadoop.conf.Configuration),44,55,"/**
* Initializes the ConfigRedactor with a Configuration object.
* @param conf Hadoop configuration containing sensitive config keys
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,get,"org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String,java.lang.String)",56,62,"/**
* Fetches attribute by name with default value fallback.
* @param name attribute name
* @param defaultValue default value to return if not found
* @return Attribute value or default value if not set
*/","* See {@link Configuration#get(String, String)}.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,getParentZnode,org.apache.hadoop.ha.ZKFailoverController:getParentZnode(),384,391,"/**
* Constructs parent Znode path by appending scope to the configured parent Znode.
* @return parent Znode path with appended scope
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,boolean)",182,185,"/**
* Sets boolean value in configuration under specified key.
* @param key unique configuration key
* @param value boolean value to be set (true or false)
*/","* Set optional boolean parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,optLong,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:optLong(java.lang.String,long)",202,205,"/**
* Converts a long value to string and stores it as a new option.
* @param key unique option identifier
* @param value the long value to convert and store
* @return B object or null if not found
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,optDouble,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:optDouble(java.lang.String,double)",232,235,"/**
* Converts a double to string and stores it in a map using the provided key.
* @param key unique key for the map entry
* @param value double value to be converted and stored
*/","* Set optional double parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,boolean)",268,271,"/**
* Converts boolean to string and passes it as a value in ""must"" operation.
* @param key unique identifier
* @param value boolean value to convert and use
*/","* Set mandatory boolean option.
   *
   * @see #must(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,mustLong,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:mustLong(java.lang.String,long)",273,276,"/**
* Converts a long value to a string and stores it in cache with given key.
* @param key unique cache key
* @param value long value to be cached as a string
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,mustDouble,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:mustDouble(java.lang.String,double)",283,286,"/**
* Converts a numeric value to a string and adds it as an attribute.
* @param key unique attribute identifier
* @param value numeric value to be converted and added
*/","* Set optional double parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setDefaultUri,"org.apache.hadoop.fs.FileSystem:setDefaultUri(org.apache.hadoop.conf.Configuration,java.lang.String)",319,321,"/**
* Sets default URI in configuration. 
* @param conf Configuration object
* @param uri new default URI value
*/","Set the default FileSystem URI in a configuration.
   * @param conf the configuration to alter
   * @param uri the new default filesystem uri",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkNfly,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.net.URI[])",167,175,"/**
* Adds links to a NoFly configuration.
* @param conf the Configuration object
* @param mountTableName the name of the mounted table
* @param src the source URI
* @param settings replication and read settings (default: minReplication=2,repairOnRead=true)
* @param targets zero or more target URIs
*/","* Add nfly link to configuration for the given mount table.
   *
   * @param conf configuration.
   * @param mountTableName mount table.
   * @param src src.
   * @param settings settings.
   * @param targets targets.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,addMappingProvider,"org.apache.hadoop.security.CompositeGroupsMapping:addMappingProvider(java.lang.String,java.lang.Class)",162,168,"/**
* Adds a mapping provider instance to the list.
* @param providerName unique provider name
* @param providerClass class of the provider instance to add
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,setBlockSize,"org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setBlockSize(org.apache.hadoop.conf.Configuration,int)",126,128,"/**
 * Sets BZip2 compression block size in Hadoop Configuration.
 * @param conf Hadoop configuration object
 * @param blockSize new block size value
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,setWorkFactor,"org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setWorkFactor(org.apache.hadoop.conf.Configuration,int)",135,137,"/**
* Sets BZip2 compression work factor in configuration.
* @param conf Hadoop Configuration object
* @param workFactor integer value for compressing work factor
*/",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,setIndexInterval,"org.apache.hadoop.io.MapFile$Writer:setIndexInterval(org.apache.hadoop.conf.Configuration,int)",380,382,"/**
 * Sets the index update interval in the configuration.
 * @param conf configuration object
 * @param interval interval value to be set (in units of time)
 */","* Sets the index interval and stores it in conf.
     * @see #getIndexInterval()
     *
     * @param conf configuration.
     * @param interval interval.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setPingInterval,"org.apache.hadoop.ipc.Client:setPingInterval(org.apache.hadoop.conf.Configuration,int)",175,178,"/**
* Sets IPC ping interval in configuration.
* @param conf Hadoop Configuration object
* @param pingInterval new interval value (in seconds)
*/","* set the ping interval value in configuration
   * 
   * @param conf Configuration
   * @param pingInterval the ping interval",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setConnectTimeout,"org.apache.hadoop.ipc.Client:setConnectTimeout(org.apache.hadoop.conf.Configuration,int)",233,235,"/**
* Sets the connect timeout in milliseconds for the IPC client.
* @param conf configuration object
* @param timeout connect timeout value
*/","* set the connection timeout value in configuration
   * 
   * @param conf Configuration
   * @param timeout the socket connect timeout value",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,setIsNestedMountPointSupported,"org.apache.hadoop.fs.viewfs.ConfigUtil:setIsNestedMountPointSupported(org.apache.hadoop.conf.Configuration,boolean)",279,281,"/**
* Sets whether nested mount point support is enabled.
* @param conf Hadoop configuration object
* @param isNestedMountPointSupported true to enable, false otherwise
*/","* Set the bool value isNestedMountPointSupported in config.
   * @param conf - from this conf
   * @param isNestedMountPointSupported - whether nested mount point is supported",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,java.lang.String[])",242,248,"/**
* Adds an optional configuration option with the given name and values.
* @param key unique option identifier
* @param values variable number of string values for this option
*/","* Set an array of string values as optional parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,java.lang.String[])",318,324,"/**
* Adds a mandatory field with optional values.
* @param key unique field identifier
* @param values one or more allowed values for the field
* @return Builder instance for method chaining
*/","* Set a string array as mandatory option.
   *
   * @see #must(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,updateConnectAddr,"org.apache.hadoop.conf.Configuration:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)",2624,2629,"/**
* Updates and returns the connection address for a given socket.
* @param name socket name
* @param addr current address to update from
* @return updated InetSocketAddress object
*/","* Set the socket address a client can use to connect for the
   * <code>name</code> property as a <code>host:port</code>.  The wildcard
   * address is replaced with the local host's address.
   * @param name property name.
   * @param addr InetSocketAddress of a listener to store in the given property
   * @return InetSocketAddress for clients to connect",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,setProtocolEngine,"org.apache.hadoop.ipc.RPC:setProtocolEngine(org.apache.hadoop.conf.Configuration,java.lang.Class,java.lang.Class)",211,217,"/**
* Sets the RPC engine for a given protocol.
* @param conf configuration object
* @param protocol protocol class to set engine for
* @param engine engine class to use
*/","* Set a protocol to use a non-default RpcEngine if one
   * is not specified in the configuration.
   * @param conf configuration to use
   * @param protocol the protocol interface
   * @param engine the RpcEngine impl",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,delete,"org.apache.hadoop.fs.RawLocalFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",707,721,"/**
* Deletes a file or directory.
* @param p Path to delete
* @param recursive whether to recursively delete directories and their contents
*/","* Delete the given path to a file or directory.
   * @param p the path to delete
   * @param recursive to delete sub-directories
   * @return true if the file or directory and all its contents were deleted
   * @throws IOException if p is non-empty and recursive is false",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDeleteContents,org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File),267,269,"/**
* Recursively deletes all contents within the specified directory.
* @param dir target directory to delete
*/","* Delete the contents of a directory, not the directory itself.  If
   * we return false, the directory may be partially-deleted.
   * If dir is a symlink to a directory, all the contents of the actual
   * directory pointed to by dir will be deleted.
   *
   * @param dir dir.
   * @return fullyDeleteContents Status.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileStatus(org.apache.hadoop.fs.Path),567,574,"/**
* Retrieves file metadata for the specified path.
* @param f Path to fetch information about
* @return FileStatus object or throws an exception if access fails
*/","* {@inheritDoc}
   *
   * If the given path is a symlink(mount link), the path will be resolved to a
   * target path and it will get the resolved path's FileStatus object. It will
   * not be represented as a symlink and isDirectory API returns true if the
   * resolved path is a directory, false otherwise.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:listStatus(org.apache.hadoop.fs.Path),611,628,"/**
* Lists file statuses in the specified directory.
* @param f Path to the directory
* @return array of FileStatus objects or empty array if not found
*/","* {@inheritDoc}
   *
   * Note: listStatus considers listing from fallbackLink if available. If the
   * same directory path is present in configured mount path as well as in
   * fallback fs, then only the fallback path will be listed in the returned
   * result except for link.
   *
   * If any of the the immediate children of the given path f is a symlink(mount
   * link), the returned FileStatus object of that children would be represented
   * as a symlink. It will not be resolved to the target path and will not get
   * the target path FileStatus object. The target path will be available via
   * getSymlink on that children's FileStatus object. Since it represents as
   * symlink, isDirectory on that children's FileStatus will return false.
   * This behavior can be changed by setting an advanced configuration
   * fs.viewfs.mount.links.as.symlinks to false. In this case, mount points will
   * be represented as non-symlinks and all the file/directory attributes like
   * permissions, isDirectory etc will be assigned from it's resolved target
   * directory/file.
   *
   * If you want to get the FileStatus of target path for that children, you may
   * want to use GetFileStatus API with that children's symlink path. Please see
   * {@link ViewFileSystem#getFileStatus(Path f)}
   *
   * Note: In ViewFileSystem, by default the mount links are represented as
   * symlinks.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,read,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read(),524,528,"/**
* Reads a single byte from the underlying stream.
* @return The read byte value or -1 if end of stream reached
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,<init>,"org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:<init>(java.lang.Object,org.apache.hadoop.metrics2.lib.MutableMetricsFactory)",62,74,"/**
* Initializes MetricsSourceBuilder with source object and mutable metrics factory.
* @param source the source object to build metrics from
* @param factory mutable metrics factory instance
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkOneDirWithMode,"org.apache.hadoop.fs.RawLocalFileSystem:mkOneDirWithMode(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.permission.FsPermission)",777,802,"/**
* Creates a directory with specified permissions.
* @param p file path
* @param p2f filesystem representation of the directory
* @param permission desired directory permissions
* @return true if created successfully, false otherwise
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1226,1236,"/**
* Creates a new FSDataOutputStream with specified parameters.
* @param f output file path
* @param overwrite whether to overwrite existing file
* @param bufferSize I/O buffer size
* @param replication replication factor for data storage
* @param blockSize block size for data storage
* @param progress progress indicator for I/O operation
*/","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * @param f the file name to open
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize the size of the buffer to be used.
   * @param progress to report progress.
   * @throws IOException IO failure
   * @return output stream.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getUMask,org.apache.hadoop.fs.FileContext:getUMask(),585,587,"/**
* Retrieves the default Unix mode mask permission.
* @return FsPermission object representing the default UMask or null if not set
*/","* 
   * @return the umask of this FileContext",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,createFactory,"org.apache.hadoop.fs.store.DataBlocks:createFactory(java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String)",129,144,"/**
* Creates a BlockFactory instance based on the specified factory type.
* @param keyToBufferDir directory path for buffered data
* @param configuration application settings
* @param name factory type (e.g. DATA_BLOCKS_BUFFER_ARRAY)
* @return BlockFactory object or throws exception if invalid type","* Create a factory.
   *
   * @param keyToBufferDir Key to buffer directory config for a FS.
   * @param configuration  factory configurations.
   * @param name           factory name -the option from {@link CommonConfigurationKeys}.
   * @return the factory, ready to be initialized.
   * @throws IllegalArgumentException if the name is unknown.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,<init>,org.apache.hadoop.io.compress.CompressionCodecFactory:<init>(org.apache.hadoop.conf.Configuration),177,191,"/**
* Initializes compression codecs based on configuration.
* @param conf Hadoop Configuration object
*/","* Find the codecs specified in the config value io.compression.codecs 
   * and register them. Defaults to gzip and deflate.
   *
   * @param conf configuration.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",81,90,"/**
* Initializes AbstractJavaKeyStoreProvider with configuration and URI.
* @param uri Key store location as a URI
* @param conf Configuration object for provider initialization
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,replacePattern,"org.apache.hadoop.security.SecurityUtil:replacePattern(java.lang.String[],java.lang.String)",235,243,"/**
* Constructs a URI pattern using hostname.
* @param components array of URI components
* @param hostname network host name or null for local resolution
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,registerProtocolAndImpl,"org.apache.hadoop.ipc.RPC$Server:registerProtocolAndImpl(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)",1101,1135,"/**
* Registers a protocol and its implementation with the given RpcKind.
* @param rpcKind type of RPC
* @param protocolClass protocol class
* @param protocolImpl protocol implementation object
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,getKeyProviderUri,"org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration,java.lang.String)",71,79,"/**
* Creates a URI instance from the configured key provider URI string.
* @param conf configuration object containing the provider URI string
* @param configKeyName name of the configuration key holding the provider URI
* @return URI instance or null if not configured
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmed,"org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String,java.lang.String)",1340,1343,"/**
* Returns trimmed version of input name or default value if trim operation fails.
* @param name input string to be trimmed
* @param defaultValue fallback string if trimming fails
*/","* Get the value of the <code>name</code> property as a trimmed <code>String</code>, 
   * <code>defaultValue</code> if no such property exists. 
   * See @{Configuration#getTrimmed} for more details.
   * 
   * @param name          the property name.
   * @param defaultValue  the property default value.
   * @return              the value of the <code>name</code> or defaultValue
   *                      if it is not set.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getInt,"org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)",1546,1555,"/**
* Parses string value to integer using hex or decimal format.
* @param name property name
* @param defaultValue default value to return if parsing fails
*/","* Get the value of the <code>name</code> property as an <code>int</code>.
   *   
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>int</code>,
   * then an error is thrown.
   * 
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as an <code>int</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getLong,"org.apache.hadoop.conf.Configuration:getLong(java.lang.String,long)",1599,1608,"/**
* Parses a long value from the given string or returns default value.
* @param name property name to fetch the value
* @param defaultValue default value if parsing fails
* @return parsed long value or default value if failed
*/","* Get the value of the <code>name</code> property as a <code>long</code>.  
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>long</code>,
   * then an error is thrown.
   * 
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>long</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getLongBytes,"org.apache.hadoop.conf.Configuration:getLongBytes(java.lang.String,long)",1624,1629,"/**
* Converts a string representation of bytes to a long integer.
* @param name string containing byte value
* @param defaultValue default long value to return if input is invalid
* @return long integer representation of bytes or defaultValue if invalid
*/","* Get the value of the <code>name</code> property as a <code>long</code> or
   * human readable format. If no such property exists, the provided default
   * value is returned, or if the specified value is not a valid
   * <code>long</code> or human readable format, then an error is thrown. You
   * can use the following suffix (case insensitive): k(kilo), m(mega), g(giga),
   * t(tera), p(peta), e(exa)
   *
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>long</code>,
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getFloat,"org.apache.hadoop.conf.Configuration:getFloat(java.lang.String,float)",1671,1676,"/**
* Retrieves a floating-point value from configuration by name.
* @param name unique identifier for the value
* @param defaultValue default value to return if not found
* @return float value or defaultValue if not set
*/","* Get the value of the <code>name</code> property as a <code>float</code>.  
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>float</code>,
   * then an error is thrown.
   *
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>float</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getDouble,"org.apache.hadoop.conf.Configuration:getDouble(java.lang.String,double)",1700,1705,"/**
* Retrieves a double value from configuration by key or returns the default value.
* @param name configuration key
* @param defaultValue default double value to return if key not found
*/","* Get the value of the <code>name</code> property as a <code>double</code>.  
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>double</code>,
   * then an error is thrown.
   *
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>double</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getBoolean,"org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)",1727,1742,"/**
* Retrieves a boolean value from configuration, falling back to the provided default if invalid or empty. 
* @param name configuration key
* @param defaultValue default boolean value to use if invalid or missing configuration","* Get the value of the <code>name</code> property as a <code>boolean</code>.  
   * If no such property is specified, or if the specified value is not a valid
   * <code>boolean</code>, then <code>defaultValue</code> is returned.
   * 
   * @param name property name.
   * @param defaultValue default value.
   * @return property value as a <code>boolean</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClass,"org.apache.hadoop.conf.Configuration:getClass(java.lang.String,java.lang.Class)",2730,2739,"/**
* Retrieves a class by name or returns default class.
* @param name class name to search for
* @param defaultValue default class to return if not found
*/","* Get the value of the <code>name</code> property as a <code>Class</code>.  
   * If no such property is specified, then <code>defaultValue</code> is 
   * returned.
   * 
   * @param name the conf key name.
   * @param defaultValue default value.
   * @return property value as a <code>Class</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setBooleanIfUnset,"org.apache.hadoop.conf.Configuration:setBooleanIfUnset(java.lang.String,boolean)",1759,1761,"/**
 * Sets boolean value if field is unset (null).
 */","* Set the given property, if it is currently unset.
   * @param name property name
   * @param value new value",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)",1884,1886,"/**
* Calculates time duration in specified units from input string.
* @param name input string representing time duration (e.g. ""2 days"")
* @param defaultValue default value to return if parsing fails
* @param unit TimeUnit enum value specifying desired output unit
*/","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d).
   *
   * @param name Property name
   * @param defaultValue Value returned if no mapping exists.
   * @param unit Unit to convert the stored property, if it exists.
   * @throws NumberFormatException If the property stripped of its unit is not
   *         a number
   * @return time duration in given time unit",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)",1888,1890,"/**
* Calculates time duration in specified units.
* @param name time interval name
* @param defaultValue default value to use if not found
* @param unit time unit (e.g. DAY, HOUR)
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialProviderFactory.java,getProviders,org.apache.hadoop.security.alias.CredentialProviderFactory:getProviders(org.apache.hadoop.conf.Configuration),73,112,"/**
* Retrieves a list of CredentialProvider instances from the given configuration.
* @param conf Configuration object containing provider paths
* @return List of CredentialProvider objects or an empty list if none found
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderFactory.java,getProviders,org.apache.hadoop.crypto.key.KeyProviderFactory:getProviders(org.apache.hadoop.conf.Configuration),62,81,"/**
* Retrieves a list of KeyProviders from the specified configuration.
* @param conf Configuration object containing key provider paths
* @return List of KeyProvider objects or empty list if none found
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseServiceUserNames,"org.apache.hadoop.ipc.DecayRpcScheduler:parseServiceUserNames(java.lang.String,org.apache.hadoop.conf.Configuration)",409,413,"/**
* Extracts service user names from configuration.
* @param ns namespace for key lookup
* @param conf Configuration object with service users list
* @return Set of unique service user names or empty set if not found
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java,getPackages,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getPackages(),65,73,"/**
* Initializes package list from configuration.
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,getRawCoderNames,"org.apache.hadoop.io.erasurecode.CodecUtil:getRawCoderNames(org.apache.hadoop.conf.Configuration,java.lang.String)",168,174,"/**
* Retrieves raw coder names for a specified codec.
* @param conf Hadoop configuration object
* @param codecName name of the codec to fetch raw coders for
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getSaslProperties,"org.apache.hadoop.security.SaslPropertiesResolver:getSaslProperties(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.SaslRpcServer$QualityOfProtection)",136,150,"/**
* Retrieves SASL properties from the configuration.
* @param conf Hadoop Configuration object
* @param configKey key for QOP values in the configuration
* @param defaultQOP default Quality of Protection value to use if not found
* @return Map of SASL properties (QOP and server authentication)
*/","* A util function to retrieve specific additional sasl property from config.
   * Used by subclasses to read sasl properties used by themselves.
   * @param conf the configuration
   * @param configKey the config key to look for
   * @param defaultQOP the default QOP if the key is missing
   * @return sasl property associated with the given key",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,getKeyFiles,org.apache.hadoop.ha.SshFenceByTcpPort:getKeyFiles(),220,222,"/**
* Retrieves collection of key files from configuration.
* @return Collection of file names or empty collection if not found
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyServers.java,refresh,org.apache.hadoop.security.authorize.ProxyServers:refresh(org.apache.hadoop.conf.Configuration),35,45,"/**
* Refreshes the list of trusted proxy servers from configuration.
* @param conf Hadoop configuration object
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getInts,org.apache.hadoop.conf.Configuration:getInts(java.lang.String),1567,1574,"/**
* Converts string array to integer array.
* @param name input string containing comma-separated integers
* @return int[] array of parsed integers or null if invalid input
*/","* Get the value of the <code>name</code> property as a set of comma-delimited
   * <code>int</code> values.
   * 
   * If no such property exists, an empty array is returned.
   * 
   * @param name property name
   * @return property value interpreted as an array of comma-delimited
   *         <code>int</code> values",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDurations,"org.apache.hadoop.conf.Configuration:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)",1971,1978,"/**
* Retrieves time durations for a given name in the specified unit.
* @param name string to analyze
* @param unit time unit (e.g. TimeUnit.MINUTES)
* @return array of time durations or null if any are invalid
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClasses,"org.apache.hadoop.conf.Configuration:getClasses(java.lang.String,java.lang.Class[])",2703,2718,"/**
* Retrieves an array of Class objects by name, returning default values if not found.
* @param name property name
* @param defaultValue optional default value array
* @return array of Class objects or default values if not found
*/","* Get the value of the <code>name</code> property
   * as an array of <code>Class</code>.
   * The value of the property specifies a list of comma separated class names.  
   * If no such property is specified, then <code>defaultValue</code> is 
   * returned.
   * 
   * @param name the property name.
   * @param defaultValue default value.
   * @return property value as a <code>Class[]</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getFile,"org.apache.hadoop.conf.Configuration:getFile(java.lang.String,java.lang.String)",2861,2874,"/**
* Retrieves a file by searching through a list of local directories.
* @param dirsProp comma-separated string of directory paths
* @param path file to retrieve (relative to each dir)
* @return File object if found, or throws IOException otherwise
*/","* Get a local file name under a directory named in <i>dirsProp</i> with
   * the given <i>path</i>.  If <i>dirsProp</i> contains multiple directories,
   * then one is chosen based on <i>path</i>'s hash code.  If the selected
   * directory does not exist, an attempt is made to create it.
   *
   * @param dirsProp directory in which to locate the file.
   * @param path file-path.
   * @return local file under the directory with the given path.
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,<init>,org.apache.hadoop.io.serializer.SerializationFactory:<init>(org.apache.hadoop.conf.Configuration),58,67,"/**
* Initializes SerializationFactory with configured serialization classes.
* @param conf Configuration object containing serialization settings
*/","* <p>
   * Serializations are found by reading the <code>io.serializations</code>
   * property from <code>conf</code>, which is a comma-delimited list of
   * classnames.
   * </p>
   *
   * @param conf configuration.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,setConf,org.apache.hadoop.security.SaslPropertiesResolver:setConf(org.apache.hadoop.conf.Configuration),60,73,"/**
* Configures RPC protection from the given Hadoop configuration.
* @param conf Hadoop Configuration object
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,getFilterParams,"org.apache.hadoop.security.http.RestCsrfPreventionFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)",229,232,"/**
* Retrieves filter parameters from configuration with specified prefix.
* @param conf Hadoop Configuration object
* @param confPrefix Filter parameter key prefix
* @return Map of filter parameter keys to values or null if not found
*/","* Constructs a mapping of configuration properties to be used for filter
   * initialization.  The mapping includes all properties that start with the
   * specified configuration prefix.  Property names in the mapping are trimmed
   * to remove the configuration prefix.
   *
   * @param conf configuration to read
   * @param confPrefix configuration prefix
   * @return mapping of configuration properties to be used for filter
   *     initialization",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/XFrameOptionsFilter.java,getFilterParams,"org.apache.hadoop.security.http.XFrameOptionsFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)",80,83,"/**
* Retrieves filter parameters from Hadoop configuration.
* @param conf Hadoop configuration object
* @param confPrefix prefix to filter properties by
*/","* Constructs a mapping of configuration properties to be used for filter
   * initialization.  The mapping includes all properties that start with the
   * specified configuration prefix.  Property names in the mapping are trimmed
   * to remove the configuration prefix.
   *
   * @param conf configuration to read
   * @param confPrefix configuration prefix
   * @return mapping of configuration properties to be used for filter
   *     initialization",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,propagateOptions,"org.apache.hadoop.util.functional.FutureIO:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",356,374,"/**
* Applies configuration options with a given prefix to the FSBuilder.
* @param builder FSBuilder instance
* @param conf Hadoop Configuration object
* @param prefix option prefix (including trailing dot if any)
* @param mandatory whether options are required or optional
*/","* Propagate options to any builder, converting everything with the
   * prefix to an option where, if there were 2+ dot-separated elements,
   * it is converted to a schema.
   * <pre>
   *   fs.example.s3a.option becomes ""s3a.option""
   *   fs.example.fs.io.policy becomes ""fs.io.policy""
   *   fs.example.something becomes ""something""
   * </pre>
   * @param builder builder to modify
   * @param conf configuration to read
   * @param prefix prefix to scan/strip
   * @param mandatory are the options to be mandatory or optional?",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationUtil.java,parseChangedProperties,"org.apache.hadoop.conf.ReconfigurationUtil:parseChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",67,70,"/**
* Retrieves changed properties between two configurations.
* @param newConf new configuration
* @param oldConf old configuration
* @return Collection of PropertyChange objects
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,printConf,"org.apache.hadoop.conf.ReconfigurationServlet:printConf(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable)",88,130,"/**
* Prints configuration changes to a PrintWriter.
* @param out PrintWriter for output
* @param reconf Reconfigurable object with current and new configurations
*/",* Print configuration options that can be changed.,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLink,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",67,70,"/**
* Adds a link to the configuration.
* @param conf configuration object
* @param src source URL or identifier
* @param target destination URI
*/","* Add a link to the config for the default mount table
   * @param conf - add the link to this conf
   * @param src - the src path name
   * @param target - the target URI link",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMergeSlash,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.net.URI)",91,93,"/**
* Adds link merge slash to configuration with default mount table name.
* @param conf Hadoop Configuration object
* @param target the target URI for merging
*/","* Add a LinkMergeSlash to the config for the default mount table.
   *
   * @param conf configuration.
   * @param target targets.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkFallback,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.net.URI)",114,116,"/**
* Adds fallback link to default mount table.
* @param conf Hadoop Configuration object
* @param target URI of the link
*/","* Add a LinkFallback to the config for the default mount table.
   *
   * @param conf configuration.
   * @param target targets.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMerge,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.net.URI[])",137,139,"/**
* Adds link merges to the configuration.
* @param conf Configuration object
*/","* Add a LinkMerge to the config for the default mount table.
   *
   * @param conf configuration.
   * @param targets targets array.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,setHomeDirConf,"org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String)",209,212,"/**
* Sets home directory configuration with default mount table name.
* @param conf Hadoop Configuration object
* @param homedir home directory path
*/","* Add config variable for homedir for default mount table
   * @param conf - add to this conf
   * @param homedir - the home dir path starting with slash",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getHomeDirValue,org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration),235,237,"/**
* Retrieves home directory value from configuration.
* @param conf configuration object
*/","* Get the value of the home dir conf value for default mount table
   * @param conf - from this conf
   * @return home dir value, null if variable is not in conf",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createEncoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createEncoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",94,104,"/**
* Creates an instance of the specified Erasure Encoder.
* @param conf configuration object
* @param options ErasureCodecOptions
*/","* Create encoder corresponding to given codec.
   * @param options Erasure codec options
   * @param conf configuration.
   * @return erasure encoder",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createDecoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createDecoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",112,122,"/**
* Creates an ErasureDecoder instance based on the provided configuration and options.
* @param conf      Hadoop Configuration object
* @param options   ErasureCodecOptions containing schema information
* @return ErasureDecoder instance or null if creation fails
*/","* Create decoder corresponding to given codec.
   * @param options Erasure codec options
   * @param conf configuration.
   * @return erasure decoder",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getLibraryName,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getLibraryName(org.apache.hadoop.conf.Configuration),72,78,"/**
* Returns library name based on native Bzip2 loading status.
* @param conf Hadoop Configuration object
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2CompressorType,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2CompressorType(org.apache.hadoop.conf.Configuration),86,90,"/**
* Returns the native Bzip2 compressor class if available, otherwise a dummy implementation.","* Return the appropriate type of the bzip2 compressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the bzip2 compressor.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2DecompressorType,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2DecompressorType(org.apache.hadoop.conf.Configuration),109,113,"/**
* Returns the type of BZIP2 decompressor to use based on native library availability.
* @param conf Hadoop configuration object
*/","* Return the appropriate type of the bzip2 decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the bzip2 decompressor.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2Decompressor,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Decompressor(org.apache.hadoop.conf.Configuration),121,124,"/**
* Returns a Bzip2 decompressor instance based on native library availability.
* @param conf Hadoop configuration object
*/","* Return the appropriate implementation of the bzip2 decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the bzip2 decompressor.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getSocketFactory,"org.apache.hadoop.net.NetUtils:getSocketFactory(org.apache.hadoop.conf.Configuration,java.lang.Class)",96,110,"/**
* Retrieves a SocketFactory instance based on configuration and class type.
* @param conf Hadoop Configuration object
* @param clazz Class type to determine socket factory implementation
* @return SocketFactory instance or null if not found
*/","* Get the socket factory for the given class according to its
   * configuration parameter
   * <tt>hadoop.rpc.socket.factory.class.&lt;ClassName&gt;</tt>. When no
   * such parameter exists then fall back on the default socket factory as
   * configured by <tt>hadoop.rpc.socket.factory.class.default</tt>. If
   * this default socket factory is not configured, then fall back on the JVM
   * default socket factory.
   * 
   * @param conf the configuration
   * @param clazz the class (usually a {@link VersionedProtocol})
   * @return a socket factory",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,resolve,org.apache.hadoop.net.TableMapping$RawTableMapping:resolve(java.util.List),127,147,"/**
* Resolves a list of node names to their corresponding rack IDs.
* @param names list of node names
* @return list of resolved rack IDs or default ID if not found
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,reloadCachedMappings,org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings(),149,160,"/**
* Reloads and updates cached mappings from persistent storage.
* @throws null if loading fails, otherwise clears and updates the cache
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/lib/StaticUserWebFilter.java,initFilter,"org.apache.hadoop.http.lib.StaticUserWebFilter:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",122,128,"/**
* Initializes a filter with user credentials.
* @param container FilterContainer instance
* @param conf Hadoop configuration object
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,<init>,"org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String,java.lang.String,java.lang.String)",520,524,"/**
* Initializes a Hadoop ZooKeeper factory with Kerberos authentication.
* @param zkPrincipal ZooKeeper principal
* @param kerberosPrincipal Kerberos principal
* @param kerberosKeytab Path to Kerberos keytab file
*/","* Constructor for the helper class to configure the ZooKeeper client connection.
     * @param zkPrincipal Optional.
     * @param kerberosPrincipal Optional. Use along with kerberosKeytab.
     * @param kerberosKeytab Optional. Use along with kerberosPrincipal.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,goUpGroupHierarchy,"org.apache.hadoop.security.LdapGroupsMapping:goUpGroupHierarchy(java.util.Set,int,java.util.Set)",592,618,"/**
* Recursively traverses the LDAP group hierarchy.
* @param groupDNs initial group DNs to search from
* @param goUpHierarchy recursive depth level (0-...)
* @param groups set of accumulated groups
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,init,org.apache.hadoop.security.ssl.SSLFactory:init(),194,204,"/**
* Initializes SSL/TLS context and related components.
* @throws GeneralSecurityException on security initialization failure
* @throws IOException on configuration loading failure
*/","* Initializes the factory.
   *
   * @throws  GeneralSecurityException thrown if an SSL initialization error
   * happened.
   * @throws IOException thrown if an IO error happened while reading the SSL
   * configuration.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,isSimpleAuthentication,org.apache.hadoop.security.KDiag:isSimpleAuthentication(org.apache.hadoop.conf.Configuration),427,430,"/**
* Checks if authentication method is simple.
* @param conf Configuration object
* @return true if simple, false otherwise
*/","* Is the authentication method of this configuration ""simple""?
   * @param conf configuration to check
   * @return true if auth is simple (i.e. not kerberos)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HadoopKerberosName.java,setConfiguration,org.apache.hadoop.security.HadoopKerberosName:setConfiguration(org.apache.hadoop.conf.Configuration),63,85,"/**
* Sets configuration for authentication and authorization.
* @param conf Hadoop Configuration object
*/","* Set the static configuration to get and evaluate the rules.
   * <p>
   * IMPORTANT: This method does a NOP if the rules have been set already.
   * If there is a need to reset the rules, the {@link KerberosName#setRules(String)}
   * method should be invoked directly.
   * 
   * @param conf the new configuration
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuthMethods,"org.apache.hadoop.ipc.Server:getAuthMethods(org.apache.hadoop.security.token.SecretManager,org.apache.hadoop.conf.Configuration)",3472,3491,"/**
* Retrieves authentication methods based on configuration and secret manager.
* @param secretManager optional secret manager for token-based auth
* @param conf server configuration
* @return list of supported AuthMethod objects
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoCodec.java,getInstance,"org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)",59,88,"/**
* Retrieves a CryptoCodec instance based on the provided configuration and cipher suite.
* @param conf Configuration object
* @param cipherSuite CipherSuite object
* @return CryptoCodec instance or null if not found
*/","* Get crypto codec for specified algorithm/mode/padding.
   * 
   * @param conf
   *          the configuration
   * @param cipherSuite
   *          algorithm/mode/padding
   * @return CryptoCodec the codec object. Null value will be returned if no
   *         crypto codec classes with cipher suite configured.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,<init>,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(org.apache.hadoop.crypto.key.JavaKeyStoreProvider),115,127,"/**
* Creates a testable instance of JavaKeyStoreProvider, copying properties from another provider.
* @param other the original provider to copy from
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/Hash.java,getInstance,org.apache.hadoop.util.hash.Hash:getInstance(org.apache.hadoop.conf.Configuration),92,95,"/**
* Retrieves an instance of the specified hash function based on configuration.
* @param conf application configuration
*/","* Get a singleton instance of hash function of a type
   * defined in the configuration.
   * @param conf current configuration
   * @return defined hash type, or null if type is invalid",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,buildFlagSet,"org.apache.hadoop.fs.impl.FlagSet:buildFlagSet(java.lang.Class,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",318,325,"/**
* Builds a FlagSet from the given configuration and enum class.
* @param enumClass Class of the enum values
* @param conf Configuration object
* @param key Key to fetch enum set from configuration
* @param ignoreUnknown Whether to ignore unknown flag values
*/","* Build a FlagSet from a comma separated list of values.
   * Case independent.
   * Special handling of ""*"" meaning: all values.
   * @param enumClass class of enum
   * @param conf configuration
   * @param key key to look for
   * @param ignoreUnknown should unknown values raise an exception?
   * @param <E> enumeration type
   * @return a mutable FlagSet
   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,
   * or there are two entries in the enum which differ only by case.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,bind,"org.apache.hadoop.ipc.Server:bind(java.net.ServerSocket,java.net.InetSocketAddress,int,org.apache.hadoop.conf.Configuration,java.lang.String)",690,720,"/**
* Binds a ServerSocket to the specified address and range of ports.
* @param socket server socket to bind
* @param address server address
* @param backlog maximum queue size
* @param conf configuration object
* @param rangeConf port range configuration, if null binds to single port
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,"org.apache.hadoop.conf.Configuration:writeXml(java.lang.String,java.io.Writer,org.apache.hadoop.conf.Configuration)",3622,3640,"/**
* Writes XML representation of property or entire configuration to output writer.
* @param propertyName name of the property to write (null for full config)
* @param out target Writer stream
* @param config optional Configuration object for redaction
*/","* Write out the non-default properties in this configuration to the
   * given {@link Writer}.
   * <ul>
   * <li>
   * When property name is not empty and the property exists in the
   * configuration, this method writes the property and its attributes
   * to the {@link Writer}.
   * </li>
   *
   * <li>
   * When property name is null or empty, this method writes all the
   * configuration properties and their attributes to the {@link Writer}.
   * </li>
   *
   * <li>
   * When property name is not empty but the property doesn't exist in
   * the configuration, this method throws an {@link IllegalArgumentException}.
   * </li>
   * </ul>
   * @param propertyName xml property name.
   * @param out the writer to write to.
   * @param config configuration.
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,dumpConfiguration,"org.apache.hadoop.conf.Configuration:dumpConfiguration(org.apache.hadoop.conf.Configuration,java.io.Writer)",3832,3850,"/**
* Serializes configuration to JSON output stream.
* @param config Configuration object
* @param out Output writer for JSON data
*/","*  Writes out all properties and their attributes (final and resource) to
   *  the given {@link Writer}, the format of the output would be,
   *
   *  <pre>
   *  { ""properties"" :
   *      [ { key : ""key1"",
   *          value : ""value1"",
   *          isFinal : ""key1.isFinal"",
   *          resource : ""key1.resource"" },
   *        { key : ""key2"",
   *          value : ""value2"",
   *          isFinal : ""ke2.isFinal"",
   *          resource : ""key2.resource"" }
   *       ]
   *   }
   *  </pre>
   *
   *  It does not output the properties of the configuration object which
   *  is loaded from an input stream.
   *  <p>
   *
   * @param config the configuration
   * @param out the Writer to write to
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,<init>,org.apache.hadoop.conf.ConfigurationWithLogging:<init>(org.apache.hadoop.conf.Configuration),37,41,"/**
* Initializes configuration with logging.
* @param conf configuration object to initialize from
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,confirmFormat,org.apache.hadoop.ha.ZKFailoverController:confirmFormat(),301,317,"/**
* Confirms format with user, checking for existing parent ZNode.
* @return true if confirmed, false on error or cancellation
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,int)",192,195,"/**
* Sets long value with given key in cache.
* @param key unique cache identifier
* @param value value to be stored
*/","* Set optional int parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,long)",197,200,"/**
* Sets a long value in cache with given key.
* @param key unique cache key
* @param value long value to be stored
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,float)",212,215,"/**
* Converts a floating-point value to a long and fetches an object with that ID.
* @param key unique identifier for the object
* @param value numeric value to be converted and used as ID
* @return B object or null if not found
*/","* Set optional float parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,double)",222,225,"/**
* Converts a floating-point number to its equivalent long value and fetches an object from the cache.
* @param key unique cache identifier
* @param value numeric value to be converted and cached
* @return Object associated with the specified key or null if not found
*/","* Set optional double parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,int)",293,296,"/**
* Converts integer to long value.
* @param key unique identifier
* @param value integer value to convert
*/","* Set mandatory int option.
   *
   * @see #must(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,long)",298,301,"/**
* Ensures a long property exists with the given key and value.
* @param key unique identifier of the property
* @param value long value to be set
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,float)",303,306,"/**
* Converts a float value to long and calls mustLong with it.
* @param key unique identifier
* @param value numeric value to be converted
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,double)",308,311,"/**
* Converts numeric value to long and fetches corresponding value from map.
* @param key unique key identifier
* @param value numeric value to convert
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkNfly,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])",177,180,"/**
* Adds links to a source file with multiple targets.
* @param conf Hadoop configuration object
* @param src source file path
* @param targets array of target URIs
*/",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,handleEmptyDstDirectoryOnWindows,"org.apache.hadoop.fs.RawLocalFileSystem:handleEmptyDstDirectoryOnWindows(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.Path,java.io.File)",646,673,"/**
* Handles empty destination directory on Windows by deleting it and renaming source.
* @param src source path
* @param srcFile source file object
* @param dst destination path
* @param dstFile destination file object
* @return true if successful, false otherwise
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsAnnotations.java,makeSource,org.apache.hadoop.metrics2.lib.MetricsAnnotations:makeSource(java.lang.Object),36,39,"/**
* Creates a MetricsSource instance from an Object.
* @param source Object to build MetricsSource with
*/","* Make an metrics source from an annotated object.
   * @param source  the annotated object.
   * @return a metrics source",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsAnnotations.java,newSourceBuilder,org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder(java.lang.Object),41,44,"/**
* Creates a new MetricsSourceBuilder instance.
* @param source underlying data source object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkOneDir,org.apache.hadoop.fs.RawLocalFileSystem:mkOneDir(java.io.File),773,775,"/**
* Creates a single directory.
* @param p2f file object representing the directory to create
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkdirsWithOptionalPermission,"org.apache.hadoop.fs.RawLocalFileSystem:mkdirsWithOptionalPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",818,839,"/**
* Creates directories with optional permissions if they do not exist.
* @param f path to directory
* @param permission FsPermission to set on created directory
* @return true if creation was successful, false otherwise
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)",1205,1211,"/**
* Creates an output stream to a file with specified parameters.
* @param f the file path
* @param overwrite whether to overwrite existing files
* @param bufferSize buffer size for writing data
* @param replication HDFS replication factor
* @param blockSize block size in bytes
*/","* Create an FSDataOutputStream at the indicated Path.
   * @param f the file name to open
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize the size of the buffer to be used.
   * @throws IOException IO failure
   * @return output stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,create,"org.apache.hadoop.fs.FileContext:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",681,706,"/**
* Creates a new file or directory at the specified path with given permissions and options.
* @param f output file/directory path
* @param createFlag creation flags (e.g. overwrite)
* @param opts additional creation options (e.g. permissions)","* Create or overwrite file on indicated path and returns an output stream for
   * writing into the file.
   * 
   * @param f the file name to open
   * @param createFlag gives the semantics of create; see {@link CreateFlag}
   * @param opts file creation options; see {@link Options.CreateOpts}.
   *          <ul>
   *          <li>Progress - to report progress on the operation - default null
   *          <li>Permission - umask is applied against permission: default is
   *          FsPermissions:getDefault()
   * 
   *          <li>CreateParent - create missing parent path; default is to not
   *          to create parents
   *          <li>The defaults for the following are SS defaults of the file
   *          server implementing the target path. Not all parameters make sense
   *          for all kinds of file system - eg. localFS ignores Blocksize,
   *          replication, checksum
   *          <ul>
   *          <li>BufferSize - buffersize used in FSDataOutputStream
   *          <li>Blocksize - block size for file blocks
   *          <li>ReplicationFactor - replication for blocks
   *          <li>ChecksumParam - Checksum parameters. server default is used
   *          if not specified.
   *          </ul>
   *          </ul>
   * 
   * @return {@link FSDataOutputStream} for created file
   * 
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If file <code>f</code> already exists
   * @throws FileNotFoundException If parent of <code>f</code> does not exist
   *           and <code>createParent</code> is false
   * @throws ParentNotDirectoryException If parent of <code>f</code> is not a
   *           directory.
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is not valid",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,mkdir,"org.apache.hadoop.fs.FileContext:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",799,816,"/**
* Creates a directory with specified permissions and optional parent creation.
* @param dir directory path
* @param permission FsPermission for the new directory (optional)
* @param createParent whether to create parent directories if they don't exist
*/","* Make(create) a directory and all the non-existent parents.
   * 
   * @param dir - the dir to make
   * @param permission - permissions is set permission{@literal &~}umask
   * @param createParent - if true then missing parent dirs are created if false
   *          then parent must exist
   * 
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If directory <code>dir</code> already
   *           exists
   * @throws FileNotFoundException If parent of <code>dir</code> does not exist
   *           and <code>createParent</code> is false
   * @throws ParentNotDirectoryException If parent of <code>dir</code> is not a
   *           directory
   * @throws UnsupportedFileSystemException If file system for <code>dir</code>
   *         is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>dir</code> is not valid",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,main,org.apache.hadoop.io.compress.CompressionCodecFactory:main(java.lang.String[]),301,358,"/**
* Reads and writes compressed files based on command-line input.
* @param args array of file paths; options ""-in"" and ""-out"" toggle read/write mode
*/","* A little test program.
   * @param args arguments.
   * @throws Exception exception.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.KeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",47,50,"/**
* Initializes the KeyStore provider with given URI and configuration.
* @param uri key store location
* @param conf system-wide configuration
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.LocalKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",54,57,"/**
* Initializes KeyStore provider with given URI and configuration.
* @param uri key store location
* @param conf configuration settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getServerPrincipal,"org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.lang.String)",185,196,"/**
* Resolves server principal from configuration and hostname.
* @param principalConfig server principal configuration string
* @param hostname current host name
* @return resolved principal string or original config if invalid
*/","* Convert Kerberos principal name pattern to valid Kerberos principal
   * names. It replaces hostname pattern with hostname, which should be
   * fully-qualified domain name. If hostname is null or ""0.0.0.0"", it uses
   * dynamically looked-up fqdn of the current host instead.
   * 
   * @param principalConfig
   *          the Kerberos principal name conf value to convert
   * @param hostname
   *          the fully-qualified domain name used for substitution
   * @return converted Kerberos principal name
   * @throws IOException if the client address cannot be determined",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getServerPrincipal,"org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.net.InetAddress)",212,227,"/**
* Resolves server principal from configuration and IP address.
* @param principalConfig principal configuration string
* @param addr client's IP address
* @return resolved server principal or original config if invalid
*/","* Convert Kerberos principal name pattern to valid Kerberos principal names.
   * This method is similar to {@link #getServerPrincipal(String, String)},
   * except 1) the reverse DNS lookup from addr to hostname is done only when
   * necessary, 2) param addr can't be null (no default behavior of using local
   * hostname when addr is null).
   * 
   * @param principalConfig
   *          Kerberos principal name pattern to convert
   * @param addr
   *          InetAddress of the host used for substitution
   * @return converted Kerberos principal name
   * @throws IOException if the client address cannot be determined",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,addProtocol,"org.apache.hadoop.ipc.RPC$Server:addProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)",1218,1222,"/**
* Registers and adds a new server protocol implementation.
* @param rpcKind type of RPC to add
* @param protocolClass class implementing the protocol
* @param protocolImpl instance of the protocol implementation
* @return this Server object for method chaining
*/","* Add a protocol to the existing server.
     * @param rpcKind - input rpcKind
     * @param protocolClass - the protocol class
     * @param protocolImpl - the impl of the protocol that will be called
     * @return the server (for convenience)",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,createKeyProvider,"org.apache.hadoop.util.KMSUtil:createKeyProvider(org.apache.hadoop.conf.Configuration,java.lang.String)",59,64,"/**
* Creates a KeyProvider instance from configuration.
* @param conf Hadoop Configuration object
* @param configKeyName name of key provider configuration key
* @return KeyProvider instance or null if not found
*/","* Creates a new KeyProvider from the given Configuration
   * and configuration key name.
   *
   * @param conf Configuration
   * @param configKeyName The configuration key name
   * @return new KeyProvider, or null if no provider was found.
   * @throws IOException if the KeyProvider is improperly specified in
   *                             the Configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,getKeyProviderUri,org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration),66,69,"/**
* Retrieves the key provider URI based on configuration.
* @param conf application configuration
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultUri,org.apache.hadoop.fs.FileSystem:getDefaultUri(org.apache.hadoop.conf.Configuration),297,304,"/**
* Returns the default file system URI based on configuration.
* @param conf Hadoop Configuration object
* @return Default filesystem URI or throws exception if invalid
*/","* Get the default FileSystem URI from a configuration.
   * @param conf the configuration to use
   * @return the uri of the default filesystem",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,setConf,org.apache.hadoop.io.compress.PassthroughCodec:setConf(org.apache.hadoop.conf.Configuration),95,102,"/**
* Sets configuration and updates default extension with dot prefix if required. 
* @param conf Configuration object to update from
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateKinitExecutable,org.apache.hadoop.security.KDiag:validateKinitExecutable(),715,727,"/**
* Validates the Kinit executable command.
* @param none
*/","* A cursory look at the {@code kinit} executable.
   *
   * If it is an absolute path: it must exist with a size > 0.
   * If it is just a command, it has to be on the path. There's no check
   * for that -but the PATH is printed out.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getSocketAddr,"org.apache.hadoop.conf.Configuration:getSocketAddr(java.lang.String,java.lang.String,int)",2566,2570,"/**
* Creates socket address from user-provided or default information.
* @param name user-provided hostname or service name
* @param defaultAddress default IP address to use if not provided by user
* @param defaultPort default port number to use if not specified by user
*/","* Get the socket address for <code>name</code> property as a
   * <code>InetSocketAddress</code>.
   * @param name property name.
   * @param defaultAddress the default value
   * @param defaultPort the default port
   * @return InetSocketAddress",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,updateConnectAddr,"org.apache.hadoop.conf.Configuration:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)",2596,2614,"/**
* Updates connection address with user-specified host and/or port values.
* @param hostProperty property name for custom host
* @param addressProperty property name for custom port or address
* @param defaultAddressValue default port value if not specified
* @param addr original InetSocketAddress to update
* @return updated InetSocketAddress object
*/","* Set the socket address a client can use to connect for the
   * <code>name</code> property as a <code>host:port</code>.  The wildcard
   * address is replaced with the local host's address. If the host and address
   * properties are configured the host component of the address will be combined
   * with the port component of the addr to generate the address.  This is to allow
   * optional control over which host name is used in multi-home bind-host
   * cases where a host can have multiple names
   * @param hostProperty the bind-host configuration name
   * @param addressProperty the service address configuration name
   * @param defaultAddressValue the service default address configuration value
   * @param addr InetSocketAddress of the service listener
   * @return InetSocketAddress for clients to connect",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,initializeMetadataCache,org.apache.hadoop.fs.HarFileSystem:initializeMetadataCache(org.apache.hadoop.conf.Configuration),108,113,"/**
* Initializes metadata cache with specified size.
* @param conf Configuration object to retrieve cache size from
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,build,org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:build(),2971,2990,"/**
* Resolves file system link to fetch FSDataInputStream.
* @return CompletableFuture containing FSDataInputStream or null if not found
*/","* Perform the open operation.
     *
     * @return a future to the input stream.
     * @throws IOException early failure to open
     * @throws UnsupportedOperationException if the specific operation
     * is not supported.
     * @throws IllegalArgumentException if the parameters are not valid.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,build,org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:build(),4941,4958,"/**
* Builds and opens a file with specified options.
* @return FSDataInputStream or null if not found
*/","* Perform the open operation.
     * Returns a future which, when get() or a chained completion
     * operation is invoked, will supply the input stream of the file
     * referenced by the path/path handle.
     * @return a future to the input stream.
     * @throws IOException early failure to open
     * @throws UnsupportedOperationException if the specific operation
     * is not supported.
     * @throws IllegalArgumentException if the parameters are not valid.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,setConfigurationFromURI,"org.apache.hadoop.fs.sftp.SFTPFileSystem:setConfigurationFromURI(java.net.URI,org.apache.hadoop.conf.Configuration)",97,135,"/**
* Configures the application from a URI.
* @param uriInfo URI containing configuration information
* @throws IOException if host or user is null
*/","* Set configuration from UI.
   *
   * @param uri
   * @param conf
   * @throws IOException",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,connect,org.apache.hadoop.fs.sftp.SFTPFileSystem:connect(),143,157,"/**
* Establishes a secure SFTP connection using configuration settings.
* @return ChannelSftp object or throws IOException on failure
*/","* Connecting by using configuration parameters.
   *
   * @return An FTPClient instance
   * @throws IOException",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path)",180,185,"/**
* Initializes ChecksumFSInputChecker with a custom stream buffer size.
* @param fs ChecksumFileSystem instance
* @param file Path to the file being checked
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,initFromFS,org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:initFromFS(),113,116,"/**
* Initializes buffer size from file system configuration.
*/",* Initialize from a filesystem.,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,create,"org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.shell.PathData,boolean)",515,544,"/**
* Creates a file output stream for the given item with optional lazy persist.
* @param item PathData object representing the file to be created
* @param lazyPersist whether to enable lazy persistence (default: false)
* @return FSDataOutputStream instance or null on failure
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,open,org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.Path),996,999,"/**
* Opens file stream with default buffer size.
* @param f Path to the file
*/","* Opens an FSDataInputStream at the indicated Path.
   * @param f the file to open
   * @throws IOException IO failure
   * @return input stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,open,org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.PathHandle),1014,1017,"/**
* Opens an input stream from the given file descriptor.
* @param fd PathHandle to the file
*/","* Open an FSDataInputStream matching the PathHandle instance. The
   * implementation may encode metadata in PathHandle to address the
   * resource directly and verify that the resource referenced
   * satisfies constraints specified at its construciton.
   * @param fd PathHandle object returned by the FS authority.
   * @throws InvalidPathHandleException If {@link PathHandle} constraints are
   *                                    not satisfied
   * @throws IOException IO failure
   * @throws UnsupportedOperationException If {@link #open(PathHandle, int)}
   *                                       not overridden by subclass
   * @return input stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,append,org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path),1516,1519,"/**
* Appends data to a file at specified path.
* @param f Path of the file to append to
*/","* Append to an existing file (optional operation).
   * Same as
   * {@code append(f, getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,
   *     IO_FILE_BUFFER_SIZE_DEFAULT), null)}
   * @param f the existing file to be appended.
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported
   *         (default).
   * @return output stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,append,"org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path,boolean)",1558,1561,"/**
* Initializes a new file stream for appending data.
* @param f the path to the file
* @param appendToNewBlock whether to append to a new block or not
*/","* Append to an existing file (optional operation).
   * @param f the existing file to be appended.
   * @param appendToNewBlock whether to append data to a new block
   * instead of the end of the last partial block
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported
   *         (default).
   * @return output stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,setConf,org.apache.hadoop.fs.ChecksumFileSystem:setConf(org.apache.hadoop.conf.Configuration),83,93,"/**
* Initializes configuration for local file system.
* @param conf Configuration object to set
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getSumBufferSize,"org.apache.hadoop.fs.ChecksumFileSystem:getSumBufferSize(int,int)",156,163,"/**
* Calculates optimal sum buffer size based on input parameters.
* @param bytesPerSum number of bytes per sum unit
* @param bufferSize total buffer size
* @return maximum of calculated and default buffer sizes
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,org.apache.hadoop.fs.FileSystem$Cache:<init>(org.apache.hadoop.conf.Configuration),3657,3663,"/**
* Initializes the cache with a specified number of creation parallel permits.
* @param conf Hadoop configuration object
*/","* Instantiate. The configuration is used to read the
     * count of permits issued for concurrent creation
     * of filesystem instances.
     * @param conf configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Metadata)",2948,2974,"/**
* Initializes a Sorter object with given configuration and parameters.
* @param fs FileSystem to operate on
* @param comparator RawComparator for sorting data
* @param keyClass Class of key objects
* @param valClass Class of value objects
* @param conf Configuration object with sort settings
* @param metadata Metadata associated with the data being sorted","* Sort and merge using an arbitrary {@link RawComparator}.
     * @param fs input FileSystem.
     * @param comparator input RawComparator.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param conf input Configuration.
     * @param metadata input metadata.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBlockSize,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBlockSize(org.apache.hadoop.conf.Configuration),130,133,"/**
* Retrieves block size from configuration or uses default value.
* @param conf Hadoop Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getWorkFactor,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getWorkFactor(org.apache.hadoop.conf.Configuration),139,142,"/**
* Retrieves the work factor from the configuration.
* @param conf Hadoop Configuration object
* @return work factor integer value or default if not set
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createOutputStream,"org.apache.hadoop.io.compress.DefaultCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",60,67,"/**
* Creates a compression output stream.
* @param out underlying output stream
* @param compressor compression algorithm to use
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createInputStream,"org.apache.hadoop.io.compress.DefaultCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",86,93,"/**
* Creates a compression-aware input stream.
* @param in raw input stream
* @param decompressor decompression strategy
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createOutputStream,"org.apache.hadoop.io.compress.Lz4Codec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",82,94,"/**
* Creates a compressed output stream with LZ4 buffering.
* @param out underlying OutputStream
* @param compressor Compressor instance for compression
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createInputStream,"org.apache.hadoop.io.compress.Lz4Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",146,153,"/**
* Creates a compression input stream using the provided decompressor.
* @param in the input stream to compress
* @param decompressor the decompression object
*/","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createDecompressor,org.apache.hadoop.io.compress.Lz4Codec:createDecompressor(),170,176,"/**
* Creates a LZ4 decompressor instance with specified buffer size.
* @return Decompressor object
*/","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createOutputStream,"org.apache.hadoop.io.compress.BZip2Codec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",122,130,"/**
* Creates a compression output stream based on the native Bzip2 availability.
* @param out target output stream
* @param compressor underlying compression algorithm
* @return CompressorStream or BZip2CompressionOutputStream instance
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to, to have it 
   *         compressed
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createInputStream,"org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",177,186,"/**
* Creates a compression-aware input stream from the given InputStream.
* @param in the input stream to decompress
* @param decompressor Decompressor instance for decompression
*/","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}, and return a 
   * stream for uncompressed data.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getCompressionLevel,org.apache.hadoop.io.compress.ZStandardCodec:getCompressionLevel(org.apache.hadoop.conf.Configuration),88,92,"/**
* Retrieves ZStandard compression level from configuration.
* @param conf Hadoop Configuration object
* @return ZStandard compression level (default if not set)
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getBufferSize,org.apache.hadoop.io.compress.ZStandardCodec:getBufferSize(org.apache.hadoop.conf.Configuration),108,111,"/**
* Retrieves ZStandard compression buffer size from configuration.
* @param conf Hadoop Configuration object
* @return Buffer size, defaulting to configured value if not set
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createOutputStream,"org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",51,60,"/**
* Creates a compression-enabled output stream using the specified compressor.
* @param out underlying output stream
* @param compressor compression algorithm to use
* @return CompressionOutputStream or regular OutputStream if no compressor is provided
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createInputStream,"org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",83,93,"/**
* Creates a CompressionInputStream for the given InputStream.
* @param in InputStream to be compressed
* @param decompressor Decompression strategy (or null to use default)
* @return CompressionInputStream or throws IOException if creation fails
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createOutputStream,"org.apache.hadoop.io.compress.SnappyCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",82,94,"/**
* Creates a CompressionOutputStream using the provided Compressor.
* @param out OutputStream to compress
* @param compressor Compressor instance
* @return CompressionOutputStream or null if creation fails
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createCompressor,org.apache.hadoop.io.compress.SnappyCodec:createCompressor(),111,117,"/**
* Creates a Snappy compressor instance with specified buffer size.
* @return Compressor object
*/","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createInputStream,"org.apache.hadoop.io.compress.SnappyCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",143,150,"/**
* Creates a compression input stream with specified decompressor and buffersize.
* @param in input stream to compress
* @param decompressor decompression algorithm instance
*/","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createDecompressor,org.apache.hadoop.io.compress.SnappyCodec:createDecompressor(),167,173,"/**
* Creates a Snappy decompressor instance with specified buffer size.
* @return Decompressor object for decompressing data
*/","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration)",114,118,"/**
* Copies bytes from input stream to output stream based on configuration.
* @param in input stream
* @param out output stream
* @param conf configuration object for buffer size and other settings
*/","* Copies from one stream to another. <strong>closes the input and output streams 
   * at the end</strong>.
   *
   * @param in InputStrem to read from
   * @param out OutputStream to write to
   * @param conf the Configuration object.
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration,boolean)",130,134,"/**
* Copies bytes from input stream to output stream.
* @param in input stream
* @param out output stream
* @param conf configuration object (file buffer size)
* @param close whether to close streams after copying
*/","* Copies from one stream to another.
   *
   * @param in InputStream to read from
   * @param out OutputStream to write to
   * @param conf the Configuration object
   * @param close whether or not close the InputStream and 
   * OutputStream at the end. The streams are closed in the finally clause.
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getChunkBufferSize,org.apache.hadoop.io.file.tfile.TFile:getChunkBufferSize(org.apache.hadoop.conf.Configuration),142,145,"/**
* Returns the chunk buffer size from configuration.
* @param conf Hadoop Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getFSInputBufferSize,org.apache.hadoop.io.file.tfile.TFile:getFSInputBufferSize(org.apache.hadoop.conf.Configuration),147,149,"/**
* Retrieves the FS input buffer size from the Configuration.
* @param conf Hadoop configuration object
* @return configured buffer size in bytes (default: 256KB)
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getFSOutputBufferSize,org.apache.hadoop.io.file.tfile.TFile:getFSOutputBufferSize(org.apache.hadoop.conf.Configuration),151,153,"/**
* Retrieves the output buffer size from Hadoop configuration.
* @param conf Hadoop configuration object
* @return Output buffer size in bytes (default: 256KB)
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getBufferSize,org.apache.hadoop.io.SequenceFile:getBufferSize(org.apache.hadoop.conf.Configuration),1738,1740,"/**
 * Retrieves file buffer size from Hadoop configuration.
 * @param conf Hadoop job configuration
 * @return File buffer size in bytes or default value if not set
 */",Get the configured buffer size,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,setConf,org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:setConf(org.apache.hadoop.conf.Configuration),156,166,"/**
* Initializes configuration for this object.
* @param conf Configuration object to use
*/","* Set the configuration and extract the configuration parameters of interest
     * @param conf the new configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,createHttpChannelConnector,"org.apache.hadoop.http.HttpServer2$Builder:createHttpChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration)",566,581,"/**
* Creates an HTTP channel connector for the server.
* @param server Server instance
* @param httpConfig HTTP configuration settings
* @return ServerConnector object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,doOp,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:doOp(org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$ProviderCallable,int,boolean)",167,234,"/**
* Performs an operation using a sequence of KMS providers with failover and retry policy.
* @param op the operation to perform
* @param currPos the current provider position
* @param isIdempotent whether the operation is idempotent
* @return the result of the operation or null if not found
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,<init>,org.apache.hadoop.crypto.key.KeyProvider$Options:<init>(org.apache.hadoop.conf.Configuration),341,344,"/**
* Initializes encryption options from Hadoop configuration.
* @param conf Hadoop Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,getBufferSize,org.apache.hadoop.crypto.CryptoStreamUtils:getBufferSize(org.apache.hadoop.conf.Configuration),65,68,"/**
* Retrieves the buffer size from the configuration.
* @param conf Hadoop Configuration object
* @return Buffer size value or default if not set
*/","* Read crypto buffer size.
   *
   * @param conf configuration.
   * @return hadoop.security.crypto.buffer.size.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,parseNumLevels,"org.apache.hadoop.ipc.CallQueueManager:parseNumLevels(java.lang.String,org.apache.hadoop.conf.Configuration)",391,411,"/**
* Parses the number of levels from configuration.
* @param ns namespace
* @param conf Hadoop Configuration object
* @return minimum level (at least 1)
*/","* Read the number of levels from the configuration.
   * This will affect the FairCallQueue's overall capacity.
   * @throws IllegalArgumentException on invalid queue count",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getRpcTimeout,org.apache.hadoop.ipc.RPC:getRpcTimeout(org.apache.hadoop.conf.Configuration),829,832,"/**
* Retrieves RPC timeout in milliseconds from configuration.
* @param conf Hadoop Configuration object
*/","* Get the RPC time from configuration;
   * If not set in the configuration, return the default value.
   *
   * @param conf Configuration
   * @return the RPC timeout (ms)",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getPingInterval,org.apache.hadoop.ipc.Client:getPingInterval(org.apache.hadoop.conf.Configuration),187,190,"/**
* Retrieves the Hadoop IPC ping interval from configuration.
* @param conf Hadoop job configuration
* @return Ping interval in milliseconds or default value if not set
*/","* Get the ping interval from configuration;
   * If not set in the configuration, return the default value.
   * 
   * @param conf Configuration
   * @return the ping interval",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getRpcTimeout,org.apache.hadoop.ipc.Client:getRpcTimeout(org.apache.hadoop.conf.Configuration),221,226,"/**
* Retrieves the RPC timeout value from the configuration.
* @param conf Hadoop Configuration object
*/","* The time after which a RPC will timeout.
   *
   * @param conf Configuration
   * @return the timeout period in milliseconds.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedTimeCostProvider.java,init,"org.apache.hadoop.ipc.WeightedTimeCostProvider:init(java.lang.String,org.apache.hadoop.conf.Configuration)",65,90,"/**
* Initializes weights for different timings based on configuration.
* @param namespace unique namespace identifier
* @param conf Configuration object containing weight settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,<init>,"org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration)",94,96,"/**
* Initializes a LineReader instance with an input stream and configuration.
* @param in Input stream to read from
* @param conf Configuration object for file buffer size
*/","* Create a line reader that reads from the given stream using the
   * <code>io.file.buffer.size</code> specified in the given
   * <code>Configuration</code>.
   * @param in input stream
   * @param conf configuration
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,<init>,"org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration,byte[])",138,144,"/**
* Initializes a LineReader with the specified input stream and configuration.
* @param in input stream to read from
* @param conf configuration settings for buffer size
* @param recordDelimiterBytes delimiter bytes for line parsing
*/","* Create a line reader that reads from the given stream using the
   * <code>io.file.buffer.size</code> specified in the given
   * <code>Configuration</code>, and using a custom delimiter of array of
   * bytes.
   * @param in input stream
   * @param conf configuration
   * @param recordDelimiterBytes The delimiter
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getInt,"org.apache.hadoop.conf.ConfigurationWithLogging:getInt(java.lang.String,int)",87,92,"/**
* Retrieves integer value from configuration with logging.
* @param name configuration key
* @param defaultValue default value to return if key not found
*/","* See {@link Configuration#getInt(String, int)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,setConf,org.apache.hadoop.ha.HAAdmin:setConf(org.apache.hadoop.conf.Configuration),337,345,"/**
* Sets configuration for the component, overriding RPC timeout value.
* @param conf Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,getSshConnectTimeout,org.apache.hadoop.ha.SshFenceByTcpPort:getSshConnectTimeout(),215,218,"/**
* Retrieves SSH connect timeout value from configuration. 
* @return Connect timeout in milliseconds
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,getGracefulFenceTimeout,org.apache.hadoop.ha.FailoverController:getGracefulFenceTimeout(org.apache.hadoop.conf.Configuration),82,86,"/**
* Retrieves HA fence timeout in milliseconds from configuration.
* @param conf Hadoop configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,getRpcTimeoutToNewActive,org.apache.hadoop.ha.FailoverController:getRpcTimeoutToNewActive(org.apache.hadoop.conf.Configuration),88,92,"/**
* Retrieves RPC timeout to new active node from configuration.
* @param conf Hadoop Configuration object
* @return Timeout value in milliseconds or default if not set
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,setTimeout,"org.apache.hadoop.fs.ftp.FTPFileSystem:setTimeout(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.conf.Configuration)",173,177,"/**
 * Sets FTP client's control keep-alive timeout based on configuration.
 * @param client FTPClient instance to configure
 * @param conf Configuration object containing timeout value
 */","* Set the FTPClient's timeout based on configuration.
   * FS_FTP_TIMEOUT is set as timeout (defaults to DEFAULT_TIMEOUT).",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FSBuilderSupport.java,getLong,"org.apache.hadoop.fs.impl.FSBuilderSupport:getLong(java.lang.String,long)",77,93,"/**
* Retrieves a long integer option by key or returns the default value if invalid.
* @param key unique option identifier
* @param defVal default long value to return on failure
* @return long option value or defVal if not a valid long integer
*/","* Get a long value with resilience to unparseable values.
   * @param key key to log
   * @param defVal default value
   * @return long value",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,canBeSafelyDeleted,org.apache.hadoop.fs.shell.Delete$Rm:canBeSafelyDeleted(org.apache.hadoop.fs.shell.PathData),129,149,"/**
* Determines whether a PathData item can be safely deleted based on file count and user confirmation.
* @param item PathData object to check
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FileSystem:getDefaultBlockSize(),2753,2757,"/**
* Returns the default block size in bytes, currently set to 32MB.
*/","* Return the number of bytes that large input files should be optimally
   * be split into to minimize I/O time.
   * @deprecated use {@link #getDefaultBlockSize(Path)} instead
   * @return default block size.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,<init>,"org.apache.hadoop.fs.DF:<init>(java.io.File,org.apache.hadoop.conf.Configuration)",49,52,"/**
* Initializes Distributed Filesystem (DFS) with specified configuration.
* @param path DFS directory path
* @param conf Hadoop Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,getInterval,org.apache.hadoop.fs.GetSpaceUsed$Builder:getInterval(),58,67,"/**
* Retrieves the default or configured data upload interval in milliseconds.
* @return interval value or default if not configured
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,getJitter,org.apache.hadoop.fs.GetSpaceUsed$Builder:getJitter(),118,129,"/**
* Retrieves the jitter value from configuration or returns default if not set.
* @return Jitter value as long integer
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,ensureInitialized,org.apache.hadoop.io.nativeio.NativeIO:ensureInitialized(),1039,1048,"/**
* Initializes the cache with user ID to user mapping and sets the cache timeout.
* @see Configuration
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,"org.apache.hadoop.security.ShellBasedIdMapping:<init>(org.apache.hadoop.conf.Configuration,boolean)",106,128,"/**
* Initializes ShellBasedIdMapping with configuration and optional full map construction.
* @param conf Hadoop Configuration object
* @param constructFullMapAtInit whether to build full mapping at initialization
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",71,78,"/**
* Initializes DelegationTokenSecretManager with configuration and token kind.
* @param conf Configuration object
* @param tokenKind Type of delegation token (e.g. ""HADOOP_TOKEN"")
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseDecayPeriodMillis,"org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayPeriodMillis(java.lang.String,org.apache.hadoop.conf.Configuration)",359,377,"/**
* Retrieves the decay period in milliseconds from configuration.
* @param ns namespace
* @param conf Hadoop Configuration object
* @return Decay period in milliseconds (>= 0)
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,serviceInit,org.apache.hadoop.util.JvmPauseMonitor:serviceInit(org.apache.hadoop.conf.Configuration),74,79,"/**
* Initializes service with configuration settings.
* @param conf Configuration object containing threshold values
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getLong,"org.apache.hadoop.conf.ConfigurationWithLogging:getLong(java.lang.String,long)",97,102,"/**
* Retrieves a long value from the underlying data source.
* @param name property name
* @param defaultValue default value to return if not found
*/","* See {@link Configuration#getLong(String, long)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,<init>,"org.apache.hadoop.ha.HealthMonitor:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceTarget)",115,135,"/**
* Initializes the health monitor with configuration and target to monitor.
* @param conf Configuration object
* @param target HAServiceTarget instance
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,initBloomFilter,org.apache.hadoop.io.BloomMapFile$Writer:initBloomFilter(org.apache.hadoop.conf.Configuration),166,180,"/**
* Initializes the Bloom filter with configuration parameters.
* @param conf Hadoop Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getFloat,"org.apache.hadoop.conf.ConfigurationWithLogging:getFloat(java.lang.String,float)",77,82,"/**
* Retrieves a float property by name with optional default value.
* @param name property name
* @param defaultValue default float value if not found
* @return float property value or default value if not set
*/","* See {@link Configuration#getFloat(String, float)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseDecayFactor,"org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayFactor(java.lang.String,org.apache.hadoop.conf.Configuration)",339,357,"/**
* Parses decay factor from configuration.
* @param ns namespace for lookup
* @param conf Configuration object
* @return Decay factor (double) or throws exception on invalid value
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,initialize,"org.apache.hadoop.fs.TrashPolicyDefault:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",87,99,"/**
* Initializes configuration for the file system.
* @param conf Hadoop Configuration object
* @param fs FileSystem instance
* @param home user's home directory path
*/","* @deprecated Use {@link #initialize(Configuration, FileSystem)} instead.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,initialize,"org.apache.hadoop.fs.TrashPolicyDefault:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",101,118,"/**
* Initializes configuration and sets intervals for trash management.
* @param conf application configuration
* @param fs file system instance
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,processRawArguments,org.apache.hadoop.fs.shell.FsCommand:processRawArguments(java.util.LinkedList),102,122,"/**
* Expands raw arguments, checks for missing default FS settings, and processes expanded arguments.
* @param args list of raw command-line arguments
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,closeChildFileSystems,org.apache.hadoop.fs.viewfs.ViewFileSystem:closeChildFileSystems(org.apache.hadoop.fs.FileSystem),1966,1984,"/**
* Closes child file systems of the given file system.
* @param fs File system to close child systems for
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,isNestedMountPointSupported,org.apache.hadoop.fs.viewfs.ConfigUtil:isNestedMountPointSupported(org.apache.hadoop.conf.Configuration),270,272,"/**
* Checks if nested mount point support is enabled.
* @param conf Hadoop configuration object
*/","* Check the bool config whether nested mount point is supported. Default: true
   * @param conf - from this conf
   * @return whether nested mount point is supported",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:<init>(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir,long,org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.viewfs.InodeTree)",1410,1426,"/**
* Initializes InternalDirOfViewFs object with provided directory, creation time, and user context.
* @param dir InodeTree directory
* @param cTime creation timestamp
* @param ugi UserGroupInformation for security context
* @param uri URI of the filesystem
* @param config Configuration settings
* @param fsState InodeTree state of the filesystem
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createCompressor,org.apache.hadoop.io.compress.Lz4Codec:createCompressor(),111,120,"/**
* Creates an instance of LZ4 compressor with specified buffer size and usage.
* @return Compressor object
*/","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,handleChecksumException,org.apache.hadoop.io.SequenceFile$Reader:handleChecksumException(org.apache.hadoop.fs.ChecksumException),2792,2801,"/**
* Handles ChecksumException by skipping or rethrowing it.
* @param e ChecksumException to handle
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,getMultipleLinearRandomRetry,"org.apache.hadoop.io.retry.RetryUtils:getMultipleLinearRandomRetry(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String)",179,201,"/**
* Retrieves Multiple Linear Random Retry Policy configuration.
* @param conf Hadoop Configuration object
* @param retryPolicyEnabledKey key for enabling policy (default false)
* @param defaultRetryPolicyEnabled default enabled value
* @param retryPolicySpecKey key for policy specification
* @param defaultRetryPolicySpec default policy spec
* @return MultipleLinearRandomRetry Policy or null if disabled
*/","* Return the MultipleLinearRandomRetry policy specified in the conf,
   * or null if the feature is disabled.
   * If the policy is specified in the conf but the policy cannot be parsed,
   * the default policy is returned.
   * 
   * Retry policy spec:
   *   N pairs of sleep-time and number-of-retries ""s1,n1,s2,n2,...""
   * 
   * @param conf configuration.
   * @param retryPolicyEnabledKey     conf property key for enabling retry
   * @param defaultRetryPolicyEnabled default retryPolicyEnabledKey conf value 
   * @param retryPolicySpecKey        conf property key for retry policy spec
   * @param defaultRetryPolicySpec    default retryPolicySpecKey conf value
   * @return the MultipleLinearRandomRetry policy specified in the conf,
   *         or null if the feature is disabled.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,isSupported,org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:isSupported(),283,283,"/**
* Checks whether the current platform supports this feature.
* @return true if supported, false otherwise
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addPrometheusServlet,org.apache.hadoop.http.HttpServer2:addPrometheusServlet(org.apache.hadoop.conf.Configuration),818,828,"/**
* Initializes and registers the Prometheus servlet if enabled in configuration.
* @param conf Hadoop configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addDefaultApps,"org.apache.hadoop.http.HttpServer2:addDefaultApps(org.eclipse.jetty.server.handler.ContextHandlerCollection,java.lang.String,org.apache.hadoop.conf.Configuration)",926,973,"/**
* Configures and adds default Jetty contexts for ""/logs/"" and ""/static/*"".
* @param parent ContextHandlerCollection to add contexts to
* @param appDir application directory path
* @param conf Hadoop configuration object
*/","* Add default apps.
   *
   * @param parent contexthandlercollection.
   * @param appDir The application directory
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addDefaultServlets,org.apache.hadoop.http.HttpServer2:addDefaultServlets(org.apache.hadoop.conf.Configuration),985,995,"/**
* Registers default servlets with the specified Configuration.
* @param configuration application settings
*/","* Add default servlets.
   * @param configuration the hadoop configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,create,"org.apache.hadoop.metrics2.source.JvmMetrics:create(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSystem)",112,123,"/**
* Creates a JVM metrics object for the given process and session.
* @param processName name of the process
* @param sessionId unique session identifier
* @param ms MetricsSystem instance
* @return registered JvmMetrics object or null if registration fails
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HttpCrossOriginFilterInitializer.java,initFilter,"org.apache.hadoop.security.HttpCrossOriginFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",39,52,"/**
* Initializes cross-origin filter based on configuration.
* @param container FilterContainer instance
* @param conf Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,validate,org.apache.hadoop.crypto.key.KeyShell$ListCommand:validate(),242,250,"/**
* Validates the object by checking key provider and configuration.
* @return true if valid, false otherwise
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,getServerFailOverEnable,"org.apache.hadoop.ipc.CallQueueManager:getServerFailOverEnable(java.lang.String,org.apache.hadoop.conf.Configuration)",119,141,"/**
* Retrieves server failover enable status from configuration.
* @param namespace unique namespace identifier
* @param conf Configuration object
* @return true if enabled, false otherwise
*/","* Return boolean value configured by property 'ipc.<port>.callqueue.overflow.trigger.failover'
   * if it is present. If the config is not present, default config
   * (without port) is used to derive class i.e 'ipc.callqueue.overflow.trigger.failover',
   * and derived value is returned if configured. Otherwise, default value
   * {@link CommonConfigurationKeys#IPC_CALLQUEUE_SERVER_FAILOVER_ENABLE_DEFAULT} is returned.
   *
   * @param namespace Namespace ""ipc"" + ""."" + Server's listener port.
   * @param conf Configuration properties.
   * @return Value returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseBackOffByResponseTimeEnabled,"org.apache.hadoop.ipc.DecayRpcScheduler:parseBackOffByResponseTimeEnabled(java.lang.String,org.apache.hadoop.conf.Configuration)",467,472,"/**
* Checks if back off by response time is enabled.
* @param ns namespace
* @param conf configuration object
* @return true if enabled, false otherwise
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",1325,1342,"/**
* Initializes a new IPC client with specified configuration and socket factory.
* @param valueClass Writable class type to use for data serialization
* @param conf Configuration object containing client settings
* @param factory SocketFactory instance for network connection management
*/","* Construct an IPC client whose values are of the given {@link Writable}
   * class.
   *
   * @param valueClass input valueClass.
   * @param conf input configuration.
   * @param factory input factory.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getClientBackoffEnable,"org.apache.hadoop.ipc.Server:getClientBackoffEnable(java.lang.String,org.apache.hadoop.conf.Configuration)",920,927,"/**
* Checks if client backoff is enabled.
* @param prefix configuration key prefix
* @param conf Hadoop Configuration object
* @return true if enabled, false otherwise
*/",* Get from config if client backoff is enabled on that port.,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getClientBackoffEnable,"org.apache.hadoop.ipc.Server:getClientBackoffEnable(java.lang.String,int,org.apache.hadoop.conf.Configuration)",941,953,"/**
* Checks if client backoff is enabled in the given namespace and port.
* @param namespace unique namespace identifier
* @param port network port number
* @return true if backoff enabled, false otherwise
*/","* Return boolean value configured by property 'ipc.<port>.backoff.enable'
   * if it is present. If the config is not present, default config
   * (without port) is used to derive class i.e 'ipc.backoff.enable',
   * and derived value is returned if configured. Otherwise, default value
   * {@link CommonConfigurationKeys#IPC_BACKOFF_ENABLE_DEFAULT} is returned.
   *
   * @param namespace Namespace ""ipc"".
   * @param port Server's listener port.
   * @param conf Configuration properties.
   * @return Value returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPasswordFromConfig,org.apache.hadoop.conf.Configuration:getPasswordFromConfig(java.lang.String),2513,2524,"/**
* Retrieves password from configuration for the specified user name.
* @param name unique user identifier
* @return char array containing password or null if not found
*/","* Fallback to clear text passwords in configuration.
   * @param name the property name.
   * @return clear text password or null",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getBoolean,"org.apache.hadoop.conf.ConfigurationWithLogging:getBoolean(java.lang.String,boolean)",67,72,"/**
* Retrieves a boolean property by name with a default value.
* @param name property name
* @param defaultValue default boolean value to return if not found
*/","* See {@link Configuration#getBoolean(String, boolean)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileSystemClass,"org.apache.hadoop.fs.FileSystem:getFileSystemClass(java.lang.String,org.apache.hadoop.conf.Configuration)",3559,3594,"/**
* Retrieves the file system class based on scheme and configuration.
* @param scheme file system scheme
* @param conf Hadoop configuration (optional)
* @return FileSystem class or throws UnsupportedFileSystemException if not found
*/","* Get the FileSystem implementation class of a filesystem.
   * This triggers a scan and load of all FileSystem implementations listed as
   * services and discovered via the {@link ServiceLoader}
   * @param scheme URL scheme of FS
   * @param conf configuration: can be null, in which case the check for
   * a filesystem binding declaration in the configuration is skipped.
   * @return the filesystem
   * @throws UnsupportedFileSystemException if there was no known implementation
   *         for the scheme.
   * @throws IOException if the filesystem could not be loaded",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,createFileSystem,"org.apache.hadoop.fs.AbstractFileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",169,181,"/**
* Creates a file system instance based on the given URI and configuration.
* @param uri unique identifier for the file system
* @param conf Hadoop configuration object
* @return AbstractFileSystem instance or throws UnsupportedFileSystemException if not found
*/","* Create a file system instance for the specified uri using the conf. The
   * conf is used to find the class name that implements the file system. The
   * conf is also passed to the file system for its configuration.
   *
   * @param uri URI of the file system
   * @param conf Configuration for the file system
   * 
   * @return Returns the file system for the given URI
   *
   * @throws UnsupportedFileSystemException file system for <code>uri</code> is
   *           not found",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,loadMappingProviders,org.apache.hadoop.security.CompositeGroupsMapping:loadMappingProviders(),147,160,"/**
* Loads and initializes mapping providers from configuration.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolEngine,"org.apache.hadoop.ipc.RPC:getProtocolEngine(java.lang.Class,org.apache.hadoop.conf.Configuration)",220,230,"/**
* Retrieves an RpcEngine instance for the specified protocol.
* @param protocol Class of the protocol
* @param conf Configuration object
* @return RpcEngine instance or null if not found
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getQueueClass,"org.apache.hadoop.ipc.Server:getQueueClass(java.lang.String,org.apache.hadoop.conf.Configuration)",796,802,"/**
* Retrieves BlockingQueue implementation class for IPC calls.
* @param prefix configuration key prefix
* @param conf Hadoop Configuration object
* @return Class of BlockingQueue or LinkedBlockingQueue if not configured
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getQueueClass,"org.apache.hadoop.ipc.Server:getQueueClass(java.lang.String,int,org.apache.hadoop.conf.Configuration)",816,827,"/**
* Retrieves a BlockingQueue implementation class for IPC calls.
* @param namespace unique namespace identifier
* @param port integer port number
* @param conf Hadoop Configuration object
* @return Class of BlockingQueue<Call> or null if not found
*/","* Return class configured by property 'ipc.<port>.callqueue.impl' if it is
   * present. If the config is not present, default config (without port) is
   * used to derive class i.e 'ipc.callqueue.impl', and derived class is
   * returned if class value is present and valid. If default config is also
   * not present, default class {@link LinkedBlockingQueue} is returned.
   *
   * @param namespace Namespace ""ipc"".
   * @param port Server's listener port.
   * @param conf Configuration properties.
   * @return Class returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getSchedulerClass,"org.apache.hadoop.ipc.Server:getSchedulerClass(java.lang.String,org.apache.hadoop.conf.Configuration)",829,853,"/**
* Resolves and returns the configured RPC scheduler class.
* @param prefix configuration prefix
* @param conf Hadoop Configuration object
* @return Class<? extends RpcScheduler> or null if not found
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getSchedulerClass,"org.apache.hadoop.ipc.Server:getSchedulerClass(java.lang.String,int,org.apache.hadoop.conf.Configuration)",869,898,"/**
* Retrieves the RpcScheduler class based on configuration.
* @param namespace HDFS namespace
* @param port RPC port number
* @param conf configuration object
* @return RpcScheduler class or null if not found
*/","* Return class configured by property 'ipc.<port>.scheduler.impl' if it is
   * present. If the config is not present, and if property
   * 'ipc.<port>.callqueue.impl' represents FairCallQueue class,
   * return DecayRpcScheduler. If config 'ipc.<port>.callqueue.impl'
   * does not have value FairCallQueue, default config (without port) is used
   * to derive class i.e 'ipc.scheduler.impl'. If default config is also not
   * present, default class {@link DefaultRpcScheduler} is returned.
   *
   * @param namespace Namespace ""ipc"".
   * @param port Server's listener port.
   * @param conf Configuration properties.
   * @return Class returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClass,"org.apache.hadoop.conf.Configuration:getClass(java.lang.String,java.lang.Class,java.lang.Class)",2758,2772,"/**
* Retrieves a Class instance by name, applying interface constraint.
* @param name class name to search for
* @param defaultValue default class if not found
* @param xface expected interface type
* @return Class<? extends U> or null if not found
*/","* Get the value of the <code>name</code> property as a <code>Class</code>
   * implementing the interface specified by <code>xface</code>.
   *   
   * If no such property is specified, then <code>defaultValue</code> is 
   * returned.
   * 
   * An exception is thrown if the returned class does not implement the named
   * interface. 
   * 
   * @param name the conf key name.
   * @param defaultValue default value.
   * @param xface the interface implemented by the named class.
   * @param <U> Interface class type.
   * @return property value as a <code>Class</code>, 
   *         or <code>defaultValue</code>.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getInternal,"org.apache.hadoop.fs.FileSystem$Cache:getInternal(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Cache$Key)",3689,3765,"/**
 * Creates or returns an existing instance of FileSystem for the given URI and configuration.
 * @param uri unique identifier for the filesystem
 * @param conf configuration settings
 * @param key cache key
 * @return FileSystem object or null if not found
 */","* Get the FS instance if the key maps to an instance, creating and
     * initializing the FS if it is not found.
     * If this is the first entry in the map and the JVM is not shutting down,
     * this registers a shutdown hook to close filesystems, and adds this
     * FS to the {@code toAutoClose} set if {@code ""fs.automatic.close""}
     * is set in the configuration (default: true).
     * @param uri filesystem URI
     * @param conf configuration
     * @param key key to store/retrieve this FileSystem in the cache
     * @return a cached or newly instantiated FileSystem.
     * @throws IOException If an I/O error occurred.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration),81,102,"/**
* Initializes SQLDelegationTokenSecretManager with configuration settings.
* @param conf Hadoop Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,setConf,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),61,72,"/**
* Sets configuration for shell command execution timeout.
* @param conf Configuration object with timeout settings
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,getShutdownTimeout,org.apache.hadoop.util.ShutdownHookManager:getShutdownTimeout(org.apache.hadoop.conf.Configuration),180,191,"/**
* Calculates the shutdown timeout in milliseconds.
* @param conf HBase configuration
* @return Shutdown duration, capped at minimum threshold
*/","* Get the shutdown timeout in seconds, from the supplied
   * configuration.
   * @param conf configuration to use.
   * @return a timeout, always greater than or equal to {@link #TIMEOUT_MINIMUM}",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,getCredentialProvider,org.apache.hadoop.security.alias.CredentialShell$Command:getCredentialProvider(),144,166,"/**
* Retrieves a non-transient credential provider from the configured providers.
* @return CredentialProvider object or null if not found
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPasswordFromCredentialProviders,org.apache.hadoop.conf.Configuration:getPasswordFromCredentialProviders(java.lang.String),2478,2506,"/**
* Retrieves password from credential providers.
* @param name unique user identifier
* @return char[] password or null if not found
*/","* Try and resolve the provided element name as a credential provider
   * alias.
   * @param name alias of the provisioned credential
   * @return password or null if not found
   * @throws IOException when error in fetching password",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,getKeyProvider,org.apache.hadoop.crypto.key.KeyShell$Command:getKeyProvider(),191,213,"/**
* Retrieves a KeyProvider instance based on configuration and user settings.
*@return non-transient KeyProvider or null if not found
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java,accept,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:accept(java.lang.Class),55,63,"/**
* Checks if a class is acceptable for serialization.
* @param c the Class to check
* @return true if class is acceptable, false otherwise
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawEncoderWithFallback,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",176,202,"/**
* Creates a RawErasureEncoder instance with fallback to the next available coder.
* @param conf configuration object
* @param codecName name of the erasure coder
* @param coderOptions encoder options
* @return created RawErasureEncoder or throws exception if not possible
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawDecoderWithFallback,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",204,230,"/**
* Creates a RawErasureDecoder instance with fallback to alternative decoders.
* @param conf Hadoop configuration
* @param codecName name of the erasure coder
* @param coderOptions ErasureCoderOptions
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,createSession,"org.apache.hadoop.ha.SshFenceByTcpPort:createSession(java.lang.String,org.apache.hadoop.ha.SshFenceByTcpPort$Args)",118,128,"/**
* Creates a SSH session using the provided host and user credentials.
* @param host target server hostname
* @param args SSH connection parameters
* @return established Session object or null on failure
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyServers.java,refresh,org.apache.hadoop.security.authorize.ProxyServers:refresh(),31,33,"/**
 * Refreshes the application using default configuration.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,parseCapacityWeights,"org.apache.hadoop.ipc.CallQueueManager:parseCapacityWeights(int,java.lang.String,org.apache.hadoop.conf.Configuration)",417,440,"/**
* Retrieves capacity weights from configuration, or uses default values if not specified.
* @param priorityLevels number of priority levels
* @param ns namespace for configuration keys
* @param conf application configuration
* @return array of positive integers representing capacity weights
*/","* Read the weights of capacity in callqueue and pass the value to
   * callqueue constructions.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,<init>,"org.apache.hadoop.ipc.metrics.RpcMetrics:<init>(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)",58,111,"/**
* Initializes RPC metrics for a given server with the specified configuration.
* @param server the HBase server instance
* @param conf the configuration object containing metric settings and intervals
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseThresholds,"org.apache.hadoop.ipc.DecayRpcScheduler:parseThresholds(java.lang.String,org.apache.hadoop.conf.Configuration,int)",379,407,"/**
* Parses thresholds from configuration, converting integer values to decimal.
* @param ns namespace
* @param conf Configuration object
* @param numLevels number of levels (thresholds + 1)
* @return array of decimal threshold values or default if not found in config
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.java,<init>,"org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)",56,79,"/**
* Initializes Weighted Round Robin Multiplexer with number of queues and weights.
* @param aNumQueues total number of queues
* @param ns name space for configuration
* @param conf Hadoop Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseBackOffResponseTimeThreshold,"org.apache.hadoop.ipc.DecayRpcScheduler:parseBackOffResponseTimeThreshold(java.lang.String,org.apache.hadoop.conf.Configuration,int)",433,456,"/**
* Parses back off response time thresholds from configuration.
* @param ns namespace
* @param conf configuration object
* @param numLevels number of priority levels
* @return array of response time thresholds in milliseconds or null if not found
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getFilterInitializers,org.apache.hadoop.http.HttpServer2:getFilterInitializers(org.apache.hadoop.conf.Configuration),894,916,"/**
* Retrieves an array of FilterInitializers from the given Configuration.
* @param conf configuration object
* @return array of FilterInitializers or null if not found
*/",Get an array of FilterConfiguration specified in the conf,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getInstances,"org.apache.hadoop.conf.Configuration:getInstances(java.lang.String,java.lang.Class)",2787,2798,"/**
* Retrieves instances of classes implementing specific interface.
* @param name unique identifier for class registry
* @param xface reference to the interface to be implemented
* @return List of objects implementing the specified interface
*/","* Get the value of the <code>name</code> property as a <code>List</code>
   * of objects implementing the interface specified by <code>xface</code>.
   * 
   * An exception is thrown if any of the classes does not exist, or if it does
   * not implement the named interface.
   * 
   * @param name the property name.
   * @param xface the interface implemented by the classes named by
   *        <code>name</code>.
   * @param <U> Interface class type.
   * @return a <code>List</code> of objects implementing <code>xface</code>.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,<init>,"org.apache.hadoop.io.DefaultStringifier:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)",60,73,"/**
* Initializes stringifier with configuration and class type.
* @param conf Configuration object
* @param c Class type to serialize/deserialize
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,init,"org.apache.hadoop.io.SequenceFile$Writer:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,boolean,java.lang.Class,java.lang.Class,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,int)",1292,1354,"/**
* Initializes the configuration and output stream for writing.
* @param config Configuration object
* @param outStream Output stream to write to
* @param ownStream Whether the output stream is owned by this instance
* @param key Class of key data
* @param val Class of value data
* @param compCodec Compression codec to use (null for no compression)
* @param meta Metadata object
* @param syncIntervalVal Sync interval value
*/",Initialize.,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,getFactory,org.apache.hadoop.util.ReflectionUtils:getFactory(org.apache.hadoop.conf.Configuration),330,335,"/**
* Returns a shared SerializationFactory instance.
* @param conf Hadoop Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/IngressPortBasedResolver.java,setConf,org.apache.hadoop.security.IngressPortBasedResolver:setConf(org.apache.hadoop.conf.Configuration),65,79,"/**
* Configures the component with port-to-QOP property mappings from configuration.
* @param conf Configuration object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/WhitelistBasedResolver.java,setConf,org.apache.hadoop.security.WhitelistBasedResolver:setConf(org.apache.hadoop.conf.Configuration),91,109,"/**
* Initializes configuration and sets up whitelisting properties.
* @param conf Hadoop Configuration instance
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,propagateOptions,"org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",159,166,"/**
* Propagates configuration options to the FSBuilder.
* @param builder FSBuilder instance
* @param conf Hadoop Configuration object
* @param prefix Option prefix string
* @param mandatory Whether options are required or not
*/","* Propagate options to any builder.
   * {@link FutureIO#propagateOptions(FSBuilder, Configuration, String, boolean)}
   * @param builder builder to modify
   * @param conf configuration to read
   * @param prefix prefix to scan/strip
   * @param mandatory are the options to be mandatory or optional?",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,propagateOptions,"org.apache.hadoop.util.functional.FutureIO:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",330,340,"/**
* Propagates configuration options to FSBuilder.
* @param builder the FSBuilder instance
* @param conf the Configuration object
* @param prefix optional or mandatory option prefix
*/","* Propagate options to any builder, converting everything with the
   * prefix to an option where, if there were 2+ dot-separated elements,
   * it is converted to a schema.
   * See {@link #propagateOptions(FSBuilder, Configuration, String, boolean)}.
   * @param builder builder to modify
   * @param conf configuration to read
   * @param optionalPrefix prefix for optional settings
   * @param mandatoryPrefix prefix for mandatory settings
   * @param <T> type of result
   * @param <U> type of builder
   * @return the builder passed in.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,getChangedProperties,"org.apache.hadoop.conf.ReconfigurableBase:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",99,103,"/**
* Retrieves changed properties between two configurations.
* @param newConf the new configuration to compare with
* @param oldConf the original configuration to compare against
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,doGet,"org.apache.hadoop.conf.ReconfigurationServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",199,212,"/**
* Handles GET requests by printing a user profile page.
* @param req HTTP request object
* @param resp HTTP response object
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/NativeLibraryChecker.java,main,org.apache.hadoop.util.NativeLibraryChecker:main(java.lang.String[]),45,156,"/**
* Performs native library checks, either for Hadoop only or all libraries.
* @param args command line arguments: -a to check all libraries, -h for help
*/","* A tool to test native library availability.
   * @param args args.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,getCompressorType,org.apache.hadoop.io.compress.BZip2Codec:getCompressorType(),137,140,"/**
* Returns compressor type based on configuration.
*/","* Get the type of {@link Compressor} needed by this {@link CompressionCodec}.
   *
   * @return the type of compressor needed by this codec.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,getDecompressorType,org.apache.hadoop.io.compress.BZip2Codec:getDecompressorType(),218,221,"/**
* Returns the type of decompressor to use based on configuration.
*/","* Get the type of {@link Decompressor} needed by this {@link CompressionCodec}.
   *
   * @return the type of decompressor needed by this codec.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createDecompressor,org.apache.hadoop.io.compress.BZip2Codec:createDecompressor(),228,231,"/**
* Creates a Bzip2 decompressor instance based on configuration.","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,reloadCachedMappings,org.apache.hadoop.net.TableMapping:reloadCachedMappings(),82,86,"/**
* Reloads cached mappings for this and its parent objects.
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,reloadCachedMappings,org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings(java.util.List),162,167,"/**
* Reloads cached table mappings for the specified tables.
* @param names list of table names to refresh mappings for
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,<init>,org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String),510,512,"/**
* Constructs Hadoop Zookeeper factory with principal.
* @param zkPrincipal principal to use in Zookeeper connection
*/","* Constructor for the helper class to configure the ZooKeeper client connection.
     * @param zkPrincipal Optional.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,lookupGroup,"org.apache.hadoop.security.LdapGroupsMapping:lookupGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext,int)",438,475,"/**
* Retrieves the user's group memberships.
* @param result SearchResult object containing user data
* @param c DirContext for LDAP operations
* @param goUpHierarchy whether to traverse up the group hierarchy (true/false)
* @return Set of group names or an empty set if not found
*/","* Perform the second query to get the groups of the user.
   *
   * If posixGroups is enabled, use use posix gid/uid to find.
   * Otherwise, use the general group member attribute to find it.
   *
   * @param result the result object returned from the prior user lookup.
   * @param c the context object of the LDAP connection.
   * @return a list of strings representing group names of the user.
   * @throws NamingException if unable to find group names",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HadoopKerberosName.java,main,org.apache.hadoop.security.HadoopKerberosName:main(java.lang.String[]),87,93,"/**
* Processes command-line arguments as Hadoop Kerberos names and prints them.
* @param args array of user-provided arguments
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoCodec.java,getInstance,org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration),99,103,"/**
* Returns a configured instance of CryptoCodec.
* @param conf Configuration object with cipher suite key
*/","* Get crypto codec for algorithm/mode/padding in config value
   * hadoop.security.crypto.cipher.suite
   * 
   * @param conf
   *          the configuration
   * @return CryptoCodec the codec object Null value will be returned if no
   *         crypto codec classes with cipher suite configured.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,bind,"org.apache.hadoop.ipc.Server:bind(java.net.ServerSocket,java.net.InetSocketAddress,int)",685,688,"/**
* Binds server socket to specified address and port.
* @param socket ServerSocket instance
* @param address InetSocketAddress containing host and port
* @param backlog Maximum number of pending connections
*/","* A convenience method to bind to a given address and report 
   * better exceptions if the address is not a valid host.
   * @param socket the socket to bind
   * @param address the address to bind to
   * @param backlog the number of connections allowed in the queue
   * @throws BindException if the address can't be bound
   * @throws UnknownHostException if the address isn't a valid host name
   * @throws IOException other random errors from bind",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,"org.apache.hadoop.conf.Configuration:writeXml(java.lang.String,java.io.Writer)",3642,3645,"/**
* Writes XML data to the specified writer with the given property name.
* @param propertyName property name (can be null for no header)
* @param out output writer
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,dumpConfiguration,"org.apache.hadoop.conf.Configuration:dumpConfiguration(org.apache.hadoop.conf.Configuration,java.lang.String,java.io.Writer)",3787,3804,"/**
* Dumps specific configuration property or entire config to output stream.
* @param config Configuration object
* @param propertyName Property name (optional) or null for full config
* @param out Output writer
*/","*  Writes properties and their attributes (final and resource)
   *  to the given {@link Writer}.
   *  <ul>
   *  <li>
   *  When propertyName is not empty, and the property exists
   *  in the configuration, the format of the output would be,
   *  <pre>
   *  {
   *    ""property"": {
   *      ""key"" : ""key1"",
   *      ""value"" : ""value1"",
   *      ""isFinal"" : ""key1.isFinal"",
   *      ""resource"" : ""key1.resource""
   *    }
   *  }
   *  </pre>
   *  </li>
   *
   *  <li>
   *  When propertyName is null or empty, it behaves same as
   *  {@link #dumpConfiguration(Configuration, Writer)}, the
   *  output would be,
   *  <pre>
   *  { ""properties"" :
   *      [ { key : ""key1"",
   *          value : ""value1"",
   *          isFinal : ""key1.isFinal"",
   *          resource : ""key1.resource"" },
   *        { key : ""key2"",
   *          value : ""value2"",
   *          isFinal : ""ke2.isFinal"",
   *          resource : ""key2.resource"" }
   *       ]
   *   }
   *  </pre>
   *  </li>
   *
   *  <li>
   *  When propertyName is not empty, and the property is not
   *  found in the configuration, this method will throw an
   *  {@link IllegalArgumentException}.
   *  </li>
   *  </ul>
   *  <p>
   * @param config the configuration
   * @param propertyName property name
   * @param out the Writer to write to
   * @throws IOException raised on errors performing I/O.
   * @throws IllegalArgumentException when property name is not
   *   empty and the property is not found in configuration
   *",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,formatZK,"org.apache.hadoop.ha.ZKFailoverController:formatZK(boolean,boolean)",282,299,"/**
* Formats ZooKeeper ZK node by clearing and recreating its parent.
* @param force true to force format, false otherwise
* @param interactive true for user confirmation, false otherwise
* @return ERR_CODE_FORMAT_DENIED if denied or error code on failure
*/",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,registerSystemSource,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSystemSource(),561,567,"/**
* Initializes and starts system source metrics.
* @param system configuration for system source
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,java.lang.Object)",221,243,"/**
* Registers a new MetricsSource with the given name and description.
* @param name unique source identifier
* @param desc source description
* @return the original source object (e.g. Gauge, Counter, etc.)
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkdirs,org.apache.hadoop.fs.RawLocalFileSystem:mkdirs(org.apache.hadoop.fs.Path),808,811,"/**
 * Creates a directory at the specified path.
 * @param f the Path to create the directory in
 */","* Creates the specified directory hierarchy. Does not
   * treat existence as an error.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkdirs,"org.apache.hadoop.fs.RawLocalFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",813,816,"/**
* Creates directory with specified permissions.
* @param f directory path
* @param permission file system permissions
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,build,org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:build(),728,748,"/**
* Builds and creates a file system entity with specified options.
* @return FSDataOutputStream representing the created entity
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/BouncyCastleFipsKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",41,44,"/**
* Initializes provider with URI and configuration.
* @param uri unique identifier of key store
* @param conf provider configuration settings
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/JavaKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.JavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",40,43,"/**
 * Initializes a new instance of the Java Key Store provider.
 * @param uri URI of the key store
 * @param conf Configuration object for the provider
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalJavaKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",40,43,"/**
 * Initializes the provider with given URI and configuration.
 * @param uri unique identifier of the key store
 * @param conf application configuration settings
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalBouncyCastleFipsKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",41,44,"/**
* Initializes a new instance of LocalBouncyCastleFipsKeyStoreProvider.
* @param uri URI of the key store
* @param conf Configuration object for initialization",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,initSpnego,"org.apache.hadoop.http.HttpServer2:initSpnego(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Properties,java.lang.String,java.lang.String)",1356,1375,"/**
* Initializes Kerberos authentication filter for the web application.
* @param conf Hadoop Configuration object
* @param hostName current hostname
* @param authFilterConfigurationPrefixes configuration prefixes for auth filter
* @param usernameConfKey key for principal in configuration
* @param keytabConfKey key for HTTP keytab file in configuration
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java,getFilterConfigMap,"org.apache.hadoop.security.AuthenticationFilterInitializer:getFilterConfigMap(org.apache.hadoop.conf.Configuration,java.lang.String)",66,91,"/**
* Retrieves a map of filter configuration properties with the specified prefix.
* @param conf Hadoop Configuration object
* @param prefix key prefix to filter properties by
* @return Map of filtered configuration properties or empty map if none found
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,createCuratorClient,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:createCuratorClient(org.apache.hadoop.conf.Configuration,java.lang.String)",185,241,"/**
* Creates a CuratorFramework client instance with specified configuration and namespace.
* @param conf Configuration object containing ZooKeeper connection settings
* @param namespace Namespace for the ZooKeeper client
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,setJaasConfiguration,org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:setJaasConfiguration(org.apache.zookeeper.client.ZKClientConfig),585,597,"/**
* Sets the JAAS configuration for secure ZooKeeper connections.
* @param zkClientConfig ZK client configuration to update
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getServerPrincipal,org.apache.hadoop.security.SaslRpcClient:getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),303,356,"/**
* Retrieves the server's Kerberos principal based on authentication type.
* @param authType SaslAuth object containing protocol and server ID
* @return Server's Kerberos principal as a string or null if not found
*/","* Get the remote server's principal.  The value will be obtained from
   * the config and cross-checked against the server's advertised principal.
   * 
   * @param authType of the SASL client
   * @return String of the server's principal
   * @throws IOException - error determining configured principal",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,initProtocolMetaInfo,org.apache.hadoop.ipc.RPC$Server:initProtocolMetaInfo(org.apache.hadoop.conf.Configuration),1200,1209,"/**
* Initializes protocol meta info by setting up RPC engine and translator.
* @param conf Hadoop Configuration object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKeyProvider,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:createKeyProvider(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",231,242,"/**
* Creates a KeyProvider instance from the provided token or configuration.
* @param token authentication token
* @param conf Hadoop configuration object
* @return KeyProvider instance or null if creation fails
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(java.net.URI,org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)",100,140,"/**
* Initializes a LoadBalancingKMSClientProvider instance.
* @param uri token service URI
* @param providers array of KMS client providers
* @param seed random seed for shuffling providers (0 for deterministic behavior)
* @param conf configuration object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,decodeHarURI,"org.apache.hadoop.fs.HarFileSystem:decodeHarURI(java.net.URI,org.apache.hadoop.conf.Configuration)",218,254,"/**
* Decodes a HAR URI into a standard URI.
* @param rawURI the HAR URI to decode
* @param conf configuration object
* @return decoded URI or null if not found
*/","* decode the raw URI to get the underlying URI
   * @param rawURI raw Har URI
   * @return filtered URI of the underlying fileSystem",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration),288,290,"/**
 * Returns a FileSystem instance based on the default URI and configuration.
 * @param conf Hadoop Configuration object
 */","* Returns the configured FileSystem implementation.
   * @param conf the configuration to use
   * @return FileSystem.
   * @throws IOException If an I/O error occurred.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,initialize,"org.apache.hadoop.fs.FileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",339,350,"/**
* Initializes client with URI and configuration.
* @param name URI to use (scheme defaults to config value)
* @param conf Client configuration
*/","* Initialize a FileSystem.
   *
   * Called after the new FileSystem instance is constructed, and before it
   * is ready for use.
   *
   * FileSystem implementations overriding this method MUST forward it to
   * their superclass, though the order in which it is done, and whether
   * to alter the configuration before the invocation are options of the
   * subclass.
   * @param name a URI whose authority section names the host, port, etc.
   *   for this FileSystem
   * @param conf the configuration
   * @throws IOException on any failure to initialize this instance.
   * @throws IllegalArgumentException if the URI is considered invalid.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstance,org.apache.hadoop.fs.FileSystem:newInstance(org.apache.hadoop.conf.Configuration),621,623,"/**
* Creates a new File System instance.
* @param conf Hadoop Configuration object
*/","* Returns a unique configured FileSystem implementation for the default
   * filesystem of the supplied configuration.
   * This always returns a new FileSystem object.
   * @param conf the configuration to use
   * @return the new FS instance
   * @throws IOException FS creation or initialization failure.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,checkPath,org.apache.hadoop.fs.FileSystem:checkPath(org.apache.hadoop.fs.Path),792,825,"/**
* Verifies the given file system path against the current file system's URI.
* @param path the file system path to check
*/","* Check that a Path belongs to this FileSystem.
   *
   * The base implementation performs case insensitive equality checks
   * of the URIs' schemes and authorities. Subclasses may implement slightly
   * different checks.
   * @param path to check
   * @throws IllegalArgumentException if the path is not considered to be
   * part of this FileSystem.
   *",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getSocketAddr,"org.apache.hadoop.conf.Configuration:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)",2539,2556,"/**
* Constructs socket address using properties or defaults.
* @param hostProperty property for host name
* @param addressProperty property for address (or null to use host)
* @param defaultAddressValue default address if not specified
* @param defaultPort default port value
* @return InetSocketAddress object","* Get the socket address for <code>hostProperty</code> as a
   * <code>InetSocketAddress</code>. If <code>hostProperty</code> is
   * <code>null</code>, <code>addressProperty</code> will be used. This
   * is useful for cases where we want to differentiate between host
   * bind address and address clients should use to establish connection.
   *
   * @param hostProperty bind host property name.
   * @param addressProperty address property name.
   * @param defaultAddressValue the default value
   * @param defaultPort the default port
   * @return InetSocketAddress",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FutureDataInputStreamBuilder.java,build,org.apache.hadoop.fs.FutureDataInputStreamBuilder:build(),47,50,"/**
 * Builds and returns a FSDataInputStream instance.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,open,"org.apache.hadoop.fs.sftp.SFTPFileSystem:open(org.apache.hadoop.fs.Path,int)",507,539,"/**
* Opens a file or directory from an SFTP channel.
* @param f file path
* @param bufferSize buffer size (ignored in this implementation)
* @return FSDataInputStream object for reading the file
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,create,"org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",545,589,"/**
* Creates a file on the remote SFTP server.
* @param f file to create
* @param permission file permissions
* @param overwrite whether to overwrite existing file
* @param bufferSize buffer size for write operations
* @param replication data replication factor
* @param blockSize block size for writes
* @param progress progress monitor
* @return FSDataOutputStream for writing to the file
*/","* A stream obtained via this call must be closed before using other APIs of
   * this class or else the invocation will block.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,rename,"org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",603,612,"/**
* Renames a file on the remote SFTP server.
* @param src source file path
* @param dst destination file path
* @return true if rename operation was successful, false otherwise
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,delete,"org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",614,623,"/**
* Deletes a file or directory on SFTP server.
* @param f Path to the file/directory
* @param recursive whether to delete recursively
* @return true if deletion was successful, false otherwise
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,listStatus,org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(org.apache.hadoop.fs.Path),625,634,"/**
* Lists file statuses for a given path.
* @param f Path to the directory or file
* @return Array of FileStatus objects or null if an error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getHomeDirectory(),657,673,"/**
* Retrieves the user's home directory via SFTP connection.
* @return Path to the user's home directory or null on failure
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",688,697,"/**
* Creates a directory on the SFTP server.
* @param f Path to create
* @param permission File system permissions for the directory
* @return true if created successfully, false otherwise
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getFileStatus,org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(org.apache.hadoop.fs.Path),699,708,"/**
* Retrieves file status for a given Path in an SFTP server.
* @param f the path to retrieve file status for
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,read,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:read(long,byte[],int,int)",230,246,"/**
* Reads up to 'len' bytes from the file at position into byte array.
* @param position offset in file
* @return number of bytes read or 0 if none
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",91,96,"/**
* Initializes a new data input stream builder for the given file system and path.
* @param fileSystem non-null file system instance
* @param path non-null file path to initialize from","* Constructor.
   * @param fileSystem owner FS.
   * @param path path",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",103,108,"/**
* Initializes a new instance of DataInputStreamBuilder with the given file system and path handle.
* @param fileSystem non-null file system to operate on
* @param pathHandle non-null path handle for data access
*/","* Constructor with PathHandle.
   * @param fileSystem owner FS.
   * @param pathHandle path handle",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,openFileOnInstance,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:openFileOnInstance(org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String)",472,498,"/**
* Opens a file on the instance, using high-performance file system API if available.
* @param instance DynamicWrappedIO instance
* @param fs FileSystem object
* @param status FileStatus object
* @return FSDataInputStream or throws IOException if an error occurs
*/","* Open a file.
   * <p>
   * If the WrappedIO class is found, uses
   * {@link #fileSystem_openFile(FileSystem, Path, String, FileStatus, Long, Map)} with
   * {@link #PARQUET_READ_POLICIES} as the list of read policies and passing down
   * the file status.
   * <p>
   * If not, falls back to the classic {@code fs.open(Path)} call.
   * @param instance dynamic wrapped IO instance.
   * @param fs filesystem
   * @param status file status
   * @param readPolicies read policy to use
   * @return the input stream
   * @throws IOException any IO failure.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,getInputStreamForFile,org.apache.hadoop.security.alias.KeyStoreProvider:getInputStreamForFile(),63,66,"/**
 * Opens and returns an InputStream for the file at the current path.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,loadFromPath,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadFromPath(org.apache.hadoop.fs.Path,char[])",291,298,"/**
* Loads file permissions from a path using the provided password.
* @param p Path to load permissions from
* @param password Array of characters used for loading
* @return FsPermission object or throws exception on error
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,checkAppend,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkAppend(org.apache.hadoop.fs.FileSystem),483,495,"/**
* Checks if a file system supports appending to the specified base path.
* @param fs the file system instance
* @return true if append operation is supported, false otherwise
*/","* Test whether the file system supports append and return the answer.
   *
   * @param fs the target file system",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)",2934,2937,"/**
* Constructs a Sorter object.
* @param fs FileSystem to operate on
* @param comparator RawComparator for sorting
* @param keyClass Class of the sort key
* @param valClass Class of the value being sorted
* @param conf Configuration for the sorter
*/","* Sort and merge using an arbitrary {@link RawComparator}.
     * @param fs input FileSystem.
     * @param comparator input RawComparator.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param conf input Configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,<init>,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>(org.apache.hadoop.conf.Configuration),72,76,"/**
* Initializes Bzip2 compressor with configuration settings.
* @param conf Hadoop Configuration object
*/","* Creates a new compressor, taking settings from the configuration.
   * @param conf configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,reinit,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reinit(org.apache.hadoop.conf.Configuration),108,122,"/**
* Reinitializes the compressor with a new configuration.
* @param conf Compression configuration object
*/","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration. It will reset the compressor's block size and
   * and work factor.
   * 
   * @param conf Configuration storing new settings",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createCompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",279,281,"/**
* Creates a compression stream to write data to a downstream output stream.
* @param downStream target output stream
* @param compressor compression algorithm to use
* @param downStreamBufferSize size of the buffer for writing to the downstream stream
* @return OutputStream object or null if an error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,init,org.apache.hadoop.io.SequenceFile$Reader:init(boolean),2022,2161,"/**
* Initializes a SequenceFile reader from a given input stream.
* @param tempReader whether to initialize for a temporary reader
* @throws IOException if an I/O error occurs
*/","* Initialize the {@link Reader}
     * @param tmpReader <code>true</code> if we are constructing a temporary
     *                  reader {@link SequenceFile.Sorter.cloneFileAttributes}, 
     *                  and hence do not initialize every component; 
     *                  <code>false</code> otherwise.
     * @throws IOException",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createDecompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",275,277,"/**
* Creates an input stream for decompressing data from another stream.
* @param downStream original input stream
* @param decompressor decomposition algorithm to use
* @param downStreamBufferSize size of the buffer used for down-stream data
* @return InputStream for decompressed data or null if failed
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,reinit,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:reinit(org.apache.hadoop.conf.Configuration),112,120,"/**
* Reinitializes the compressor with a new compression configuration.
* @param conf Configuration object containing compression settings
*/","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration. It will reset the compressor's compression level
   * and compression strategy.
   *
   * @param conf Configuration storing new settings",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getCompressionBufferSize,org.apache.hadoop.io.compress.ZStandardCodec:getCompressionBufferSize(org.apache.hadoop.conf.Configuration),94,99,"/**
* Calculates recommended compression buffer size based on configuration.
* @param conf application configuration
* @return recommended buffer size
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getDecompressionBufferSize,org.apache.hadoop.io.compress.ZStandardCodec:getDecompressionBufferSize(org.apache.hadoop.conf.Configuration),101,106,"/**
* Calculates decompression buffer size based on configuration.
* @param conf Compression configuration
* @return Recommended buffer size (or default if not specified)
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,writeStreamToFile,"org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:writeStreamToFile(java.io.InputStream,org.apache.hadoop.fs.shell.PathData,boolean,boolean)",499,512,"/**
 * Writes input stream to file at specified target location.
 * @param in input data stream
 * @param target destination file path
 * @param lazyPersist whether to persist immediately or lazily
 * @param direct whether to delete on exit or not
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,printToStdout,org.apache.hadoop.fs.shell.Display$Cat:printToStdout(java.io.InputStream),98,104,"/**
* Copies input stream to standard output.
* @param in InputStream to copy
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareAppendValue,org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendValue(int),554,575,"/**
* Prepares a DataOutputStream for appending a value of specified length.
* @param length length of the value to append
* @return DataOutputStream instance or null for unknown lengths
*/","* Obtain an output stream for writing a value into TFile. This may only be
     * called right after a key appending operation (the key append stream must
     * be closed).
     * 
     * @param length
     *          The expected length of the value. If length of the value is not
     *          known, set length = -1. Otherwise, the application must write
     *          exactly as many bytes as specified here before calling close on
     *          the returned output stream. Advertising the value size up-front
     *          guarantees that the value is encoded in one chunk, and avoids
     *          intermediate chunk buffering.
     * @throws IOException raised on errors performing I/O.
     * @return DataOutputStream.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:<init>(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion,org.apache.hadoop.conf.Configuration)",496,513,"/**
* Initializes RBlockState with compression algorithm and input stream.
* @param compressionAlgo Algorithm used for decompression
* @param fsin Input stream to read from
* @param region BlockRegion containing compressed data
* @param conf Configuration object for buffer size
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:<init>(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.conf.Configuration)",119,139,"/**
* Initializes WBlockState with compression algorithm and output streams.
* @param compressionAlgo Algorithm for data compression
* @param fsOut Output stream to write compressed data
* @param fsOutputBuffer Buffer for storing compressed data
* @param conf Configuration object for file system settings
*/","* @param compressionAlgo
       *          The compression algorithm to be used to for compression.
       * @throws IOException",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,setConf,org.apache.hadoop.net.ScriptBasedMapping:setConf(org.apache.hadoop.conf.Configuration),135,139,"/**
* Sets configuration for this object and its mapping.
* @param conf Configuration to apply
*/","* {@inheritDoc}.
   * <p>
   * This will get called in the superclass constructor, so a check is needed
   * to ensure that the raw mapping is defined before trying to relaying a null
   * configuration.
   * </p>
   * @param conf input Configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,setConf,org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration),130,138,"/**
* Configures the object with a new Configuration instance.
* @param conf new configuration settings
*/","* Set the configuration and extract the configuration parameters of interest
     * @param conf the new configuration",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,createHttpsChannelConnector,"org.apache.hadoop.http.HttpServer2$Builder:createHttpsChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration)",583,632,"/**
* Creates an HTTPS channel connector with custom SSL/TLS configuration.
* @param server the Jetty server instance
* @param httpConfig the HTTP configuration object
* @return a ServerConnector instance for HTTPS connections
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getDelegationToken(java.lang.String),251,264,"/**
* Retrieves a delegation token with the specified renewer.
* @param renewer unique identifier for the token's delegator
* @return DelegationToken object or null on failure
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,renewDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token),266,274,"/**
* Renews a delegationToken for the given token.
* @param token Token to renew
* @return new delegationToken ID or -1 on failure
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,cancelDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token),276,285,"/**
* Cancels a delegated token.
* @param token Token to cancel
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,generateEncryptedKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:generateEncryptedKey(java.lang.String),326,344,"/**
* Generates an encrypted key using the specified key name.
* @param encryptionKeyName name of the encryption key to use
* @return EncryptedKeyVersion object or throws exception on failure
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,decryptEncryptedKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),346,364,"/**
* Decrypts an encrypted key version using the KMS client.
* @param encryptedKeyVersion Encrypted key to decrypt
* @return Decrypted KeyVersion object or throws exception on failure
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,reencryptEncryptedKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),366,384,"/**
* Reencrypts an existing EncryptedKeyVersion using the next available provider.
* @param ekv EncryptedKeyVersion to reencrypt
* @return reencrypted EncryptedKeyVersion or null if not found
* @throws IOException on operation failure
* @throws GeneralSecurityException on encryption/decryption error",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,reencryptEncryptedKeys,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:reencryptEncryptedKeys(java.util.List),386,404,"/**
* Re-encrypts a list of encrypted key versions.
* @param ekvs List of EncryptedKeyVersion objects to re-encrypt
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeyVersion(java.lang.String),406,414,"/**
* Retrieves a key version by name.
* @param versionName the name of the key version to fetch
* @return KeyVersion object or null if not found
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeys,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeys(),416,424,"/**
* Retrieves a list of key identifiers from the KMS client.
* @return List of String keys or empty list if not available
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeysMetadata,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeysMetadata(java.lang.String[]),426,434,"/**
* Retrieves metadata for specified key names.
* @param names variable number of key identifiers
* @return array of Metadata objects or empty array on failure
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeyVersions(java.lang.String),436,445,"/**
* Retrieves key versions by name.
* @param name unique key identifier
* @return list of KeyVersion objects or null if not found
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getCurrentKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCurrentKey(java.lang.String),447,455,"/**
* Retrieves the current key version for a given name.
* @param name key identifier
* @return KeyVersion object or null if not found
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getMetadata,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getMetadata(java.lang.String),457,465,"/**
* Retrieves metadata by name using the configured KMS client.
* @param name metadata identifier
* @return Metadata object or null if not found, or throws IOException on error
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",467,476,"/**
* Creates a new key with the specified name and material.
* @param name unique key identifier
* @param material key material as byte array
* @param options creation options
* @return KeyVersion object or null if failed
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",478,495,"/**
* Creates a key with the specified name and options.
* @param name unique key identifier
* @param options Key creation settings
* @return KeyVersion object or throws exception on failure
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,options,org.apache.hadoop.crypto.key.KeyProvider:options(org.apache.hadoop.conf.Configuration),433,435,"/**
* Creates an Options instance from the provided Configuration.
* @param conf configuration to use when creating the Options
*/","* A helper function to create an options object.
   * @param conf the configuration to use
   * @return a new options object",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,<init>,"org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",142,145,"/**
* Initializes CryptoInputStream with given parameters.
* @param in input stream to encrypt
* @param codec encryption codec to use
* @param key encryption key
* @param iv initialization vector for encryption
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[],long,boolean)",130,135,"/**
* Constructs a CryptoOutputStream with specified parameters.
* @param out underlying OutputStream
* @param codec encryption codec configuration
* @param key cryptographic key
* @param iv initialization vector
* @param streamOffset byte offset from start of encrypted stream
* @param closeOutputStream whether to close the underlying OutputStream
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client$ConnectionId:<init>(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)",1679,1709,"/**
* Initializes a ConnectionId object with the given parameters.
* @param address remote server address
* @param protocol data transfer protocol class
* @param ticket user authentication information
* @param rpcTimeout RPC timeout in milliseconds
* @param connectionRetryPolicy retry policy for connections
* @param conf configuration settings for this connection
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getTimeout,org.apache.hadoop.ipc.Client:getTimeout(org.apache.hadoop.conf.Configuration),202,213,"/**
* Retrieves the RPC timeout value from configuration.
* @param conf Configuration object
* @return RPC timeout value or ping interval if not found, or -1 on failure
*/","* The time after which a RPC will timeout.
   * If ping is not enabled (via ipc.client.ping), then the timeout value is the 
   * same as the pingInterval.
   * If ping is enabled, then there is no timeout value.
   * 
   * @param conf Configuration
   * @return the timeout period in milliseconds. -1 if no timeout value is set
   * @deprecated use {@link #getRpcTimeout(Configuration)} instead",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,parseMetaData,org.apache.hadoop.fs.HarFileSystem$HarMetaData:parseMetaData(),1166,1236,"/**
* Parses metadata from master and archive index files.
* @throws IOException if an I/O error occurs
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,<init>,"org.apache.hadoop.ha.FailoverController:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceProtocol$RequestSource)",61,80,"/**
* Initializes FailoverController with configuration and request source.
* @param conf Configuration object
* @param source RequestSource object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,connect,org.apache.hadoop.fs.ftp.FTPFileSystem:connect(),141,167,"/**
* Establishes an FTP connection using configuration settings.
* @throws IOException if connection or login fails
*/","* Connect to the FTP server using configuration parameters *
   * 
   * @return An FTPClient instance
   * @throws IOException",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FSBuilderSupport.java,getPositiveLong,"org.apache.hadoop.fs.impl.FSBuilderSupport:getPositiveLong(java.lang.String,long)",61,69,"/**
* Returns a positive long value for the specified key, using the default value if it's negative.
* @param key unique identifier
* @param defVal default value to use when input is invalid
* @return long value or the provided default value if invalid
*/","* Get a long value with resilience to unparseable values.
   * Negative values are replaced with the default.
   * @param key key to log
   * @param defVal default value
   * @return long value",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize(),1282,1286,"/**
 * Returns the default block size based on the underlying file system. 
 * @return Default block size in bytes.",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getServerDefaults,org.apache.hadoop.fs.FileSystem:getServerDefaults(),939,954,"/**
* Retrieves server defaults with default checksum type and configuration.
*/","* Return a set of server default configuration values.
   * @return server default configuration values
   * @throws IOException IO failure
   * @deprecated use {@link #getServerDefaults(Path)} instead",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),2766,2768,"/**
* Returns default block size based on file path.
* @param f file path to determine block size for
*/","* Return the number of bytes that large input files should be optimally
   * be split into to minimize I/O time.  The given path will be used to
   * locate the actual filesystem.  The full path does not have to exist.
   * @param f path of file
   * @return the default block size for the path's filesystem",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize(),426,429,"/**
* Returns default block size based on file system settings.
* @return Default block size in bytes.",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,reportChecksumFailure,"org.apache.hadoop.fs.LocalFileSystem:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)",99,149,"/**
* Moves a corrupted file to the highest writable parent directory and renames it.
* @param p Path to the file
*/","* Moves files to a bad file directory on the same device, so that their
   * storage will not be reused.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,<init>,org.apache.hadoop.fs.DU:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),43,48,"/**
* Initializes a new DU instance from a builder.
* @param path file system path
* @param interval measurement interval
* @param jitter measurement jitter
* @param initialUsed initial used space value
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,<init>,org.apache.hadoop.fs.CachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),60,66,"/**
* Constructs a CachingGetSpaceUsed object from a given Builder.
* @throws IOException if an I/O error occurs while constructing.","* This is the constructor used by the builder.
   * All overriding classes should implement this.
   *
   * @param builder builder.
   * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/WindowsGetSpaceUsed.java,<init>,org.apache.hadoop.fs.WindowsGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),34,40,"/**
* Initializes Windows space used metrics with custom parameters.
* @param builder CachingGetSpaceUsed configuration builder
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getOwner,org.apache.hadoop.io.nativeio.NativeIO:getOwner(java.io.FileDescriptor),934,954,"/**
* Retrieves the owner of a file descriptor.
* @param fd FileDescriptor to fetch owner for
* @return owner username as String or null if not found
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,org.apache.hadoop.security.ShellBasedIdMapping:<init>(org.apache.hadoop.conf.Configuration),135,137,"/**
* Initializes ShellBasedIdMapping with configuration.
* @param conf Hadoop Configuration object
* @throws IOException if initialization fails
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,initHM,org.apache.hadoop.ha.ZKFailoverController:initHM(),323,328,"/**
* Initializes and starts the Health Monitor service.
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,<init>,"org.apache.hadoop.fs.TrashPolicyDefault:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)",79,82,"/**
* Initializes default trash policy with given file system and configuration. 
* @param fs FileSystem instance
* @param conf Configuration object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,close,org.apache.hadoop.fs.viewfs.ViewFileSystem:close(),1986,2007,"/**
* Closes the file system and associated resources.
* @throws IOException if an I/O error occurs during closure
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.DataOutputBuffer),2565,2584,"/**
* Reads the next record from a sequence file.
* @param buffer DataOutputBuffer to write record data into
* @return length of key in the next record, or -1 if end-of-file reached
*/","@deprecated Call {@link #nextRaw(DataOutputBuffer,SequenceFile.ValueBytes)}.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,getDefaultRetryPolicy,"org.apache.hadoop.io.retry.RetryUtils:getDefaultRetryPolicy(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String,java.lang.String)",59,85,"/**
* Returns the default retry policy based on configuration and parameters.
* @param conf Hadoop Configuration
* @param retryPolicyEnabledKey key to enable/disable retry policy
* @param defaultRetryPolicyEnabled whether retry is enabled by default
* @param retryPolicySpecKey key for retry policy specification
* @param defaultRetryPolicySpec default retry policy specification
* @param remoteExceptionToRetry exception type to retry remotely
*/","* Return the default retry policy set in conf.
   * 
   * If the value retryPolicyEnabledKey is set to false in conf,
   * use TRY_ONCE_THEN_FAIL.
   * 
   * Otherwise, get the MultipleLinearRandomRetry policy specified in the conf
   * and then
   * (1) use multipleLinearRandomRetry for
   *     - remoteExceptionToRetry, or
   *     - IOException other than RemoteException, or
   *     - ServiceException; and
   * (2) use TRY_ONCE_THEN_FAIL for
   *     - non-remoteExceptionToRetry RemoteException, or
   *     - non-IOException.
   *     
   *
   * @param conf configuration.
   * @param retryPolicyEnabledKey     conf property key for enabling retry
   * @param defaultRetryPolicyEnabled default retryPolicyEnabledKey conf value 
   * @param retryPolicySpecKey        conf property key for retry policy spec
   * @param defaultRetryPolicySpec    default retryPolicySpecKey conf value
   * @param remoteExceptionToRetry    The particular RemoteException to retry
   * @return the default retry policy.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCodec,org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:getCodec(),273,273,"/**
* Retrieves the compression codec used by this instance. 
* @throws IOException if an error occurs while retrieving the codec 
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createDecompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",275,277,"/**
* Creates a decompression stream from an input stream.
* @param downStream original input stream
* @param decompressor decompression algorithm to use
* @param downStreamBufferSize buffer size for the down-stream input stream
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createCompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",279,281,"/**
 * Creates an OutputStream that compresses data before writing it to the specified downStream.
 * @param downStream target stream for compressed data
 * @param compressor compression algorithm to use
 * @param downStreamBufferSize buffer size for downStream
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,init,"org.apache.hadoop.metrics2.source.JvmMetrics$Singleton:init(java.lang.String,java.lang.String)",59,64,"/**
* Initializes JVM metrics with the given process name and session ID.
* @param processName name of the Java process
* @param sessionId unique identifier for this session
* @return initialized JvmMetrics object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)",1349,1351,"/**
* Constructs a new Client instance with default socket factory.
* @param valueClass class of writable value
* @param conf HBase configuration object
*/","* Construct an IPC client with the default SocketFactory.
   * @param valueClass input valueClass.
   * @param conf input Configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,getClient,"org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,java.lang.Class)",50,68,"/**
* Retrieves a cached or constructs a new Client instance.
* @param conf configuration for timeout
* @param factory SocketFactory instance
* @return Client object or null if not found
*/","* Construct &amp; cache an IPC client with the user-provided SocketFactory
   * if no cached client exists.
   * 
   * @param conf Configuration
   * @param factory SocketFactory for client socket
   * @param valueClass Class of the expected response
   * @return an IPC client",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java,<init>,org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>(org.apache.hadoop.conf.Configuration),73,85,"/**
 * Initializes the FsUrlStreamHandlerFactory with a Configuration object.
 * @param conf Hadoop configuration
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java,createURLStreamHandler,org.apache.hadoop.fs.FsUrlStreamHandlerFactory:createURLStreamHandler(java.lang.String),87,111,"/**
* Creates a URLStreamHandler based on the given protocol.
* @param protocol unique protocol identifier
* @return handler object or null for unknown protocols
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,excludeIncompatibleCredentialProviders,"org.apache.hadoop.security.ProviderUtils:excludeIncompatibleCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.Class)",141,199,"/**
* Excludes incompatible credential providers from the configuration.
* @param config the original configuration
* @param fileSystemClass class of the target file system
* @return updated configuration with excluded providers
*/","* There are certain integrations of the credential provider API in
   * which a recursive dependency between the provider and the hadoop
   * filesystem abstraction causes a problem. These integration points
   * need to leverage this utility method to remove problematic provider
   * types from the existing provider path within the configuration.
   *
   * @param config the existing configuration with provider path
   * @param fileSystemClass the class which providers must be compatible
   * @return Configuration clone with new provider path
   * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,get,"org.apache.hadoop.fs.AbstractFileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration)",263,266,"/**
* Creates an instance of AbstractFileSystem based on the provided URI and configuration.
* @param uri unique file system identifier
* @param conf Hadoop configuration object
* @return AbstractFileSystem instance or throws exception if not supported
*/","* The main factory method for creating a file system. Get a file system for
   * the URI's scheme and authority. The scheme of the <code>uri</code>
   * determines a configuration property name,
   * <tt>fs.AbstractFileSystem.<i>scheme</i>.impl</tt> whose value names the
   * AbstractFileSystem class.
   * 
   * The entire URI and conf is passed to the AbstractFileSystem factory method.
   * 
   * @param uri for the file system to be created.
   * @param conf which is passed to the file system impl.
   * 
   * @return file system for the given URI.
   * 
   * @throws UnsupportedFileSystemException if the file system for
   *           <code>uri</code> is not supported.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,setConf,org.apache.hadoop.security.CompositeGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),138,145,"/**
* Sets configuration for the component.
* @param conf Configuration object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,getProtocolMetaInfoProxy,"org.apache.hadoop.ipc.RpcClientUtil:getProtocolMetaInfoProxy(java.lang.Object,org.apache.hadoop.conf.Configuration)",179,187,"/**
* Retrieves a ProtocolMetaInfo proxy using the given proxy and configuration.
* @param proxy Java object serving as a proxy
* @param conf Configuration settings for RPC operations
* @return ProtocolMetaInfoPB proxy instance or throws IOException if failed
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,build,org.apache.hadoop.ipc.RPC$Builder:build(),975,991,"/**
* Builds a Server instance from configuration.
* @throws IOException on I/O error
* @throws HadoopIllegalArgumentException if required fields are missing
*/","* @return Build the RPC Server.
     * @throws IOException on error
     * @throws HadoopIllegalArgumentException when mandatory fields are not set",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicy.java,getInstance,"org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",139,146,"/**
* Retrieves a configured TrashPolicy instance.
* @param conf Hadoop Configuration object
* @param fs FileSystem object
* @param home Home directory Path
* @return Configured TrashPolicy object or null if initialization fails
*/","* Get an instance of the configured TrashPolicy based on the value
   * of the configuration parameter fs.trash.classname.
   *
   * @param conf the configuration to be used
   * @param fs the file system to be used
   * @param home the home directory
   * @return an instance of TrashPolicy
   * @deprecated Use {@link #getInstance(Configuration, FileSystem)} instead.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicy.java,getInstance,"org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",156,162,"/**
* Returns a configured instance of the Trash Policy.
* @param conf Configuration object containing policy settings
* @param fs FileSystem object for initialization
* @return TrashPolicy instance or default implementation if not found
*/","* Get an instance of the configured TrashPolicy based on the value
   * of the configuration parameter fs.trash.classname.
   *
   * @param conf the configuration to be used
   * @param fs the file system to be used
   * @return an instance of TrashPolicy",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getMountTableConfigLoader,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getMountTableConfigLoader(org.apache.hadoop.conf.Configuration),181,203,"/**
* Retrieves a configured MountTableConfigLoader instance.
* @param conf Hadoop Configuration object
* @return loaded MountTableConfigLoader or null if failed
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,getKlass,org.apache.hadoop.fs.GetSpaceUsed$Builder:getKlass(),74,89,"/**
* Resolves the class for getting space used based on OS and configuration.
* @return Class<? extends GetSpaceUsed> or null if not found
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getInstance,"org.apache.hadoop.net.NetworkTopology:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.net.InnerNode$Factory)",77,83,"/**
* Retrieves and initializes the network topology instance.
* @param conf configuration object
* @param factory inner node factory
* @return initialized NetworkTopology object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DomainNameResolverFactory.java,newInstance,"org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.lang.String)",68,75,"/**
* Creates a new instance of the specified DomainNameResolver.
* @param conf Hadoop Configuration object
* @param configKey key to resolve resolver class from configuration
*/","* This function gets the instance based on the config.
   *
   * @param conf Configuration
   * @param configKey config key name.
   * @return Domain name resolver.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getInstance,org.apache.hadoop.security.SaslPropertiesResolver:getInstance(org.apache.hadoop.conf.Configuration),52,58,"/**
* Returns a shared instance of the SASL properties resolver.
* @param conf Hadoop configuration object
*/","* Returns an instance of SaslPropertiesResolver.
   * Looks up the configuration to see if there is custom class specified.
   * Constructs the instance by passing the configuration directly to the
   * constructor to achieve thread safety using final fields.
   * @param conf configuration.
   * @return SaslPropertiesResolver",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,<init>,"org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)",104,153,"/**
* Initializes group mapping and caching.
* @param conf configuration object
* @param timer timer service for cache refreshing
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateSasl,org.apache.hadoop.security.KDiag:validateSasl(java.lang.String),733,748,"/**
* Validates SASL property resolver by loading the specified class.
* @param saslPropsResolverKey key for resolving SASL properties
*/","* Try to load the SASL resolver.
   * @param saslPropsResolverKey key for the SASL resolver",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,getInstance,org.apache.hadoop.security.authorize.ProxyUsers:getInstance(org.apache.hadoop.conf.Configuration),47,53,"/**
* Retrieves a singleton instance of the impersonation provider based on configuration.
* @param conf Hadoop configuration object
*/","* Returns an instance of ImpersonationProvider.
   * Looks up the configuration to see if there is custom class specified.
   * @param conf
   * @return ImpersonationProvider",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,setConf,org.apache.hadoop.crypto.OpensslCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration),84,100,"/**
* Initializes the secure randomizer with a configurable implementation.
* @param conf Configuration object containing the secure randomizer class
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,<init>,"org.apache.hadoop.util.ShutdownHookManager$HookEntry:<init>(java.lang.Runnable,int)",205,209,"/**
* Initializes a new HookEntry with specified Runnable and priority.
* @param hook executable task to be hooked
* @param priority execution order value
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,shutdownExecutor,org.apache.hadoop.util.ShutdownHookManager:shutdownExecutor(org.apache.hadoop.conf.Configuration),142,162,"/**
* Shuts down the executor service with configurable timeout.
* @param conf Configuration object containing shutdown settings
*/","* Shutdown the executor thread itself.
   * @param conf the configuration containing the shutdown timeout setting.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getPasswordFromCredentialProviders,"org.apache.hadoop.security.LdapGroupsMapping:getPasswordFromCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",893,906,"/**
* Retrieves password from credential providers or returns default pass.
* @param config configuration object
* @param alias unique provider identifier
* @param defaultPass default password value
* @return retrieved password or default pass
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPassword,org.apache.hadoop.conf.Configuration:getPassword(java.lang.String),2418,2428,"/**
* Retrieves password for the given user by querying credential providers and configuration.
* @param name username to fetch password for
* @return password as character array or null if not found
*/","* Get the value for a known password configuration element.
   * In order to enable the elimination of clear text passwords in config,
   * this method attempts to resolve the property name as an alias through
   * the CredentialProvider API and conditionally fallsback to config.
   * @param name property name
   * @return password
   * @throws IOException when error in fetching password",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawEncoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoder(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",131,137,"/**
* Creates a raw erasure encoder with the specified configuration and codec.
* @param conf Spark Configuration object
* @param codec Codec name
* @param coderOptions Erasure coder options
*/","* Create RS raw encoder according to configuration.
   * @param conf configuration
   * @param coderOptions coder options that's used to create the coder
   * @param codec the codec to use. If null, will use the default codec
   * @return raw encoder",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawDecoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoder(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",146,152,"/**
* Creates a raw erasure decoder instance.
* @param conf configuration object
* @param codec name of the erasure codec to use
* @param coderOptions options for the erasure coder
*/","* Create RS raw decoder according to configuration.
   * @param conf configuration
   * @param coderOptions coder options that's used to create the coder
   * @param codec the codec to use. If null, will use the default codec
   * @return raw decoder",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,tryFence,"org.apache.hadoop.ha.SshFenceByTcpPort:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",80,115,"/**
* Tries to fence a HAServiceTarget using SSH.
* @param target HA service target
* @param argsStr string representation of fencing arguments
* @return true if fencing was successful, false otherwise
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyServers.java,isProxyServer,org.apache.hadoop.security.authorize.ProxyServers:isProxyServer(java.lang.String),47,52,"/**
* Checks if given IP address is a known proxy server.
* @param remoteAddr the IP address to check
* @return true if it's a proxy server, false otherwise
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,<init>,"org.apache.hadoop.ipc.CallQueueManager:<init>(java.lang.Class,java.lang.Class,boolean,int,java.lang.String,org.apache.hadoop.conf.Configuration)",78,96,"/**
* Initializes CallQueueManager with specified parameters.
* @param backingClass Blocking queue implementation class
* @param schedulerClass Rpc scheduler implementation class
* @param clientBackOffEnabled whether to enable client backoff
* @param maxQueueSize maximum call queue size
* @param namespace configuration namespace
* @param conf overall system configuration
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,swapQueue,"org.apache.hadoop.ipc.CallQueueManager:swapQueue(java.lang.Class,java.lang.Class,int,java.lang.String,org.apache.hadoop.conf.Configuration)",464,495,"/**
* Updates the queue and scheduler with new configuration.
* @param schedulerClass new scheduler class to use
* @param queueClassToUse new queue class to use
* @param maxSize maximum size of the new queue
* @param ns namespace for configuration
* @param conf configuration object
*/","* Replaces active queue with the newly requested one and transfers
   * all calls to the newQ before returning.
   *
   * @param schedulerClass input schedulerClass.
   * @param queueClassToUse input queueClassToUse.
   * @param maxSize input maxSize.
   * @param ns input ns.
   * @param conf input configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,create,"org.apache.hadoop.ipc.metrics.RpcMetrics:create(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)",115,118,"/**
* Creates and registers an RpcMetrics instance with the given server and configuration.
* @param server the server to associate with the metrics
* @param conf the configuration for the metrics
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,"org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,int[],boolean,org.apache.hadoop.conf.Configuration)",119,154,"/**
* Initializes FairCallQueue with specified parameters.
* @param priorityLevels number of priority levels (must be at least 1)
* @param capacity total capacity across all queues
* @param ns namespace for metrics and logging
* @param capacityWeights weights for each queue's capacity
* @param serverFailOverEnabled flag to enable server failover
* @param conf configuration object
*/","* Create a FairCallQueue.
   * @param priorityLevels the total size of all multi-level queue
   *                       priority policies
   * @param capacity the total size of all sub-queues
   * @param ns the prefix to use for configuration
   * @param capacityWeights the weights array for capacity allocation
   *                        among subqueues
   * @param serverFailOverEnabled whether or not to enable callqueue overflow trigger failover
   *                              for stateless servers when RPC call queue is filled
   * @param conf the configuration to read from
   * Notes: Each sub-queue has a capacity of `capacity / numSubqueues`.
   * The first or the highest priority sub-queue has an excess capacity
   * of `capacity % numSubqueues`",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,initializeWebServer,"org.apache.hadoop.http.HttpServer2:initializeWebServer(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String[])",726,795,"/**
* Initializes the web server with the given configuration and path specifications.
* @param name unique server identifier
* @param hostName hostname for binding
* @param conf configuration object
* @param pathSpecs path specifications for the web application
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseCostProvider,"org.apache.hadoop.ipc.DecayRpcScheduler:parseCostProvider(java.lang.String,org.apache.hadoop.conf.Configuration)",281,309,"/**
* Parses and initializes a single CostProvider instance from configuration.
* @param ns namespace to search for cost provider
* @param conf Hadoop Configuration object
* @return initialized CostProvider instance or DefaultCostProvider if none found
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseIdentityProvider,"org.apache.hadoop.ipc.DecayRpcScheduler:parseIdentityProvider(java.lang.String,org.apache.hadoop.conf.Configuration)",312,337,"/**
* Retrieves IdentityProvider instance from configuration.
* @param ns namespace for lookup
* @return IdentityProvider instance or default UserIdentityProvider if not found
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,store,"org.apache.hadoop.io.DefaultStringifier:store(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.String)",110,117,"/**
* Stores an object in the configuration using a stringified representation.
* @param conf Configuration to store in
* @param item Object to serialize and store
* @param keyName Unique identifier for the stored value
*/","* Stores the item in the configuration with the given keyName.
   * 
   * @param <K>  the class of the item
   * @param conf the configuration to store
   * @param item the object to be stored
   * @param keyName the name of the key to use
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,load,"org.apache.hadoop.io.DefaultStringifier:load(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)",130,140,"/**
* Loads a configured object by key, using the provided class and configuration.
* @param conf Configuration instance
* @param keyName unique key for the object to load
* @param itemClass class of the object to instantiate
* @return loaded object or null if not found
*/","* Restores the object from the configuration.
   * 
   * @param <K> the class of the item
   * @param conf the configuration to use
   * @param keyName the name of the key to use
   * @param itemClass the class of the item
   * @return restored object
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,storeArray,"org.apache.hadoop.io.DefaultStringifier:storeArray(org.apache.hadoop.conf.Configuration,java.lang.Object[],java.lang.String)",153,171,"/**
* Stores an array of objects in Hadoop Configuration.
* @param conf Hadoop Configuration object
* @param items array of objects to store
* @param keyName unique identifier for the stored data
*/","* Stores the array of items in the configuration with the given keyName.
   * 
   * @param <K> the class of the item
   * @param conf the configuration to use 
   * @param items the objects to be stored
   * @param keyName the name of the key to use
   * @throws IndexOutOfBoundsException if the items array is empty
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,loadArray,"org.apache.hadoop.io.DefaultStringifier:loadArray(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)",184,203,"/**
* Loads an array of typed items from a configuration key.
* @param conf Configuration object
* @param keyName Key name to load items from
* @param itemClass Type of items in the array
* @return Array of loaded items or null if empty
*/","* Restores the array of objects from the configuration.
   * 
   * @param <K> the class of the item
   * @param conf the configuration to use
   * @param keyName the name of the key to use
   * @param itemClass the class of the item
   * @return restored object
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",1257,1266,"/**
* Initializes a Writer object with given parameters.
* @param fs file system instance
* @param conf configuration object
* @param name path to write data to
* @param keyClass class of key data type
* @param valClass class of value data type
* @param bufferSize buffer size for I/O operations
* @param replication output replication factor
* @param blockSize block size for storage
* @param progress progress callback object
* @param metadata metadata object for file system
*/","* Create the named file with write-progress reporter.
     * @deprecated Use 
     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} 
     *   instead.
     * @param fs input filesystem.
     * @param conf input configuration.
     * @param name input name.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param bufferSize input bufferSize.
     * @param replication input replication.
     * @param blockSize input blockSize.
     * @param progress input progress.
     * @param metadata input metadata.
     * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,copy,"org.apache.hadoop.util.ReflectionUtils:copy(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.Object)",346,361,"/**
* Deep copies a Configuration object or any serializable type T.
* @param conf configuration for serialization
* @param src source object to copy
* @param dst destination object to store the copied data
* @return copied object, null if deserialization fails
*/","* Make a copy of the writable object using serialization to a buffer.
   * @param src the object to copy from
   * @param dst the object to copy into, which is destroyed
   * @param <T> Generics Type.
   * @param conf configuration.
   * @return dst param (the copy)
   * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,propagateOptions,"org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",140,149,"/**
* Propagates configuration options to an FSBuilder.
* @param builder the FSBuilder instance
* @param conf Hadoop Configuration object
* @param optionalPrefix prefix for optional properties
* @param mandatoryPrefix prefix for required properties
*/","* Propagate options to any builder.
   * {@link FutureIO#propagateOptions(FSBuilder, Configuration, String, String)}
   * @param builder builder to modify
   * @param conf configuration to read
   * @param optionalPrefix prefix for optional settings
   * @param mandatoryPrefix prefix for mandatory settings
   * @param <T> type of result
   * @param <U> type of builder
   * @return the builder passed in.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,run,org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread:run(),116,162,"/**
* Performs reconfiguration of properties based on changes.
* @throws ReconfigurationException if property reconfiguration fails
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,doGetGroups,"org.apache.hadoop.security.LdapGroupsMapping:doGetGroups(java.lang.String,int)",509,557,"/**
 * Retrieves the set of groups a user belongs to.
 * @param user unique user identifier
 * @param goUpHierarchy flag to fetch groups above the user in hierarchy (0 for direct membership)
 * @return Set of group names or empty set if not found
 */","* Perform LDAP queries to get group names of a user.
   *
   * Perform the first LDAP query to get the user object using the user's name.
   * If one-query is enabled, retrieve the group names from the user object.
   * If one-query is disabled, or if it failed, perform the second query to
   * get the groups.
   *
   * @param user user name
   * @return a list of group names for the user. If the user can not be found,
   * return an empty string array.
   * @throws NamingException if unable to get group names",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,generateEncryptedKey,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:generateEncryptedKey(java.lang.String),285,305,"/**
* Generates an encrypted key by fetching the specified encryption key,
* generating random bytes for a new key and IV, and encrypting with the fetched key.
* @param encryptionKeyName name of the encryption key to use
* @return EncryptedKeyVersion object or throws exception if failed
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,reencryptEncryptedKeys,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:reencryptEncryptedKeys(java.util.List),356,408,"/**
* Re-encrypts a list of encrypted key versions using the current key.
* @param ekvs list of encrypted key versions to re-encrypt
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,decryptEncryptedKey,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),433,457,"/**
* Decrypts an Encrypted Key Version using the provided encryption key.
* @param encryptedKeyVersion Encrypted key to be decrypted
* @return Decrypted KeyVersion object
*/",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,org.apache.hadoop.conf.Configuration:writeXml(java.io.Writer),3593,3595,"/**
 * Writes XML data to the specified output writer.
 * @param out Writer object to write XML data to
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfServlet.java,writeResponse,"org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String,java.lang.String)",95,105,"/**
* Writes configuration response to output stream in specified format.
* @param conf Hadoop Configuration object
* @param out Writer for output stream
* @param format Response format (JSON/XML)
* @param propertyName Property name for configuration dump
*/",* Guts of the servlet - extracted for easy testing.,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configureSources,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSources(),539,543,"/**
* Configures sources by retrieving filter and instance configs.
* @param config configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,create,"org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.permission.FsPermission)",555,568,"/**
* Creates a file stream with specified parameters and permissions.
* @param f the file path
* @param overwrite whether to overwrite existing files
* @param createParent whether to create parent directories if they don't exist
* @return FSDataOutputStream object or throws exception if creation fails
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createSymlink,"org.apache.hadoop.fs.RawLocalFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1179,1202,"/**
* Creates a symbolic link to the specified target file or directory.
* @param target the target of the symlink
* @param link the path where the symlink will be created
* @param createParent whether to create parent directories if they do not exist
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getFilterProperties,"org.apache.hadoop.http.HttpServer2:getFilterProperties(org.apache.hadoop.conf.Configuration,java.util.List)",872,886,"/**
* Retrieves filter properties from the given configuration and prefixes.
* @param conf Hadoop Configuration object
* @param prefixes list of prefixes to consider
* @return Properties object containing filtered configurations
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilterInitializer.java,createFilterConfig,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:createFilterConfig(org.apache.hadoop.conf.Configuration),42,51,"/**
* Creates a map of filter configurations by combining
* the authentication filter config with proxy user settings.
* @param conf Hadoop configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java,initFilter,"org.apache.hadoop.security.AuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",57,64,"/**
* Initializes authentication filter for the given configuration.
* @param container FilterContainer instance
* @param conf Configuration object
*/","* Initializes hadoop-auth AuthenticationFilter.
   * <p>
   * Propagates to hadoop-auth AuthenticationFilter configuration all Hadoop
   * configuration properties prefixed with ""hadoop.http.authentication.""
   *
   * @param container The filter container
   * @param conf Configuration for run-time parameters",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration),159,183,"/**
* Initializes ZKDelegationTokenSecretManager with ZooKeeper configuration.
* @param conf ZooKeeper configuration
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,newZooKeeper,"org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean,org.apache.zookeeper.client.ZKClientConfig)",555,576,"/**
* Creates a new ZooKeeper instance with specified configuration.
* @param connectString ZK connection string
* @return ZooKeeper object or throws Exception if creation fails
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,createSaslClient,org.apache.hadoop.security.SaslRpcClient:createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),211,270,"/**
* Creates a SASL client for the specified authentication method.
* @param authType SaslAuth object containing authentication details
* @return initialized SaslClient or null if not supported
*/","* Try to create a SaslClient for an authentication type.  May return
   * null if the type isn't supported or the client lacks the required
   * credentials.
   * 
   * @param authType - the requested authentication method
   * @return SaslClient for the authType or null
   * @throws SaslException - error instantiating client
   * @throws IOException - misc errors",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,renew,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:renew(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",189,208,"/**
* Renews a delegation token.
* @param token Token to renew
* @param conf Configuration object
* @return new token value or -1 on failure
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,cancel,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",210,229,"/**
* Cancels a delegation token using the provided KeyProvider.
* @param token Token to cancel
* @param conf Configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(java.net.URI,org.apache.hadoop.crypto.key.kms.KMSClientProvider[],org.apache.hadoop.conf.Configuration)",89,92,"/**
* Constructs a Load Balancing KMS Client Provider with the given parameters.
* @param providerUri URI of the load balancing KMS client provider
* @param providers array of KMS client providers to be loaded balanced
* @param conf configuration for this instance
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)",94,98,"/**
* Constructs a LoadBalancingKMSClientProvider instance for testing purposes.
* @param providers array of KMS client providers
* @param seed random seed value
* @param conf configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getFS,org.apache.hadoop.fs.FsShell:getFS(),79,84,"/**
* Retrieves or initializes the Hadoop file system instance.
* @return the configured file system
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,initialize,"org.apache.hadoop.fs.sftp.SFTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",493,500,"/**
* Initializes HBase table by URI and configuration.
* @param uriInfo URI information
* @param conf HBase configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,<init>,"org.apache.hadoop.fs.DelegateToFileSystem:<init>(java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",49,57,"/**
* Initializes a DelegateToFileSystem instance for accessing files via URI.
* @param theUri file system URI
* @param theFsImpl concrete file system implementation
* @param conf configuration settings
* @param supportedScheme supported file system scheme (e.g., HDFS)
* @param authorityRequired whether authority is required for access
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,initialize,"org.apache.hadoop.fs.http.AbstractHttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",48,52,"/**
* Initializes the object with configuration and URI.
* @param name URI to be initialized
* @param conf Configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,initialize,"org.apache.hadoop.fs.ftp.FTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",102,133,"/**
* Initializes the FTP connection based on the provided URI and configuration.
* @param uri FTP connection URI
* @param conf Configuration object for the FTP connection
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,initialize,"org.apache.hadoop.fs.RawLocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",130,135,"/**
* Initializes the object with given URI and configuration.
* @param uri unique identifier for this object
* @param conf configuration parameters
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createFileSystem,"org.apache.hadoop.fs.FileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",3604,3629,"/**
* Creates a FileSystem instance based on the provided URI and configuration.
* @param uri file system URI (scheme and authority)
* @param conf Hadoop Configuration object
* @return initialized FileSystem object
*/","* Create and initialize a new instance of a FileSystem.
   * @param uri URI containing the FS schema and FS details
   * @param conf configuration to use to look for the FS instance declaration
   * and to pass to the {@link FileSystem#initialize(URI, Configuration)}.
   * @return the initialized filesystem.
   * @throws IOException problems loading or initializing the FileSystem",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,initialize,"org.apache.hadoop.fs.viewfs.ViewFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",310,384,"/**
* Initializes the ViewFs with the given URI and Configuration.
* @param theUri unique identifier of the ViewFs
* @param conf configuration settings for the ViewFs
*/","* Called after a new FileSystem instance is constructed.
   * @param theUri a uri whose authority section names the host, port, etc. for
   *        this FileSystem
   * @param conf the configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,createFileSystem,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",277,291,"/**
* Creates a file system instance based on the provided URI and configuration.
* @param uri unique identifier for the file system
* @param conf Hadoop configuration object
* @return FileSystem instance or throws IOException if creation fails",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,initialize,"org.apache.hadoop.fs.FilterFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",92,104,"/**
* Initializes this Hadoop FS instance with the given configuration.
* @param name URI of the file system
* @param conf Configuration to use for initialization
*/","Called after a new FileSystem instance is constructed.
   * @param name a uri whose authority section names the host, port, etc.
   *   for this FileSystem
   * @param conf the configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,initialize,"org.apache.hadoop.fs.LocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",44,53,"/**
* Initializes the HDFS file system with a new URI and configuration.
* @param name new URI to initialize
* @param conf Hadoop configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,checkPath,org.apache.hadoop.fs.HarFileSystem:checkPath(org.apache.hadoop.fs.Path),338,341,"/**
 * Verifies the existence of a file system path.
 * @param path the file system path to validate
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,makeQualified,org.apache.hadoop.fs.FileSystem:makeQualified(org.apache.hadoop.fs.Path),682,685,"/**
 * Returns a qualified version of the provided file system path.
 * @param path input file system path
 */","* Qualify a path to one which uses this FileSystem and, if relative,
   * made absolute.
   * @param path to qualify.
   * @return this path if it contains a scheme and authority and is absolute, or
   * a new path that includes a path and authority and is fully qualified
   * @see Path#makeQualified(URI, Path)
   * @throws IllegalArgumentException if the path has a schema/URI different
   * from this FileSystem.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,resolvePath,org.apache.hadoop.fs.FileSystem:resolvePath(org.apache.hadoop.fs.Path),975,978,"/**
* Resolves and returns an absolute path.
* @param p input path to be resolved
*/","* Return the fully-qualified path of path, resolving the path
   * through any symlinks or mount point.
   * @param p path to be resolved
   * @return fully qualified path
   * @throws FileNotFoundException if the path is not present
   * @throws IOException for any other error",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,checkPath,org.apache.hadoop.fs.FilterFileSystem:checkPath(org.apache.hadoop.fs.Path),146,149,"/**
* Verifies file system path integrity.
* @param path file system path to validate
*/",Check that a Path belongs to this FileSystem.,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AvroFSInput.java,<init>,"org.apache.hadoop.fs.AvroFSInput:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",55,63,"/**
 * Opens an Avro file and initializes the input stream.
 * @param fc FileContext instance
 * @param p Path to the Avro file
 */","Construct given a {@link FileContext} and a {@link Path}.
   * @param fc filecontext.
   * @param p the path.
   * @throws IOException If an I/O error occurred.
   *",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,copy,"org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)",2208,2248,"/**
* Copies a file or directory from source to destination.
* @param src source path
* @param dst destination path
* @param deleteSource whether to delete the source after copy
* @param overwrite whether to overwrite existing files
* @return true if successful, false otherwise
*/","* Copy from src to dst, optionally deleting src and overwriting dst.
     * @param src src.
     * @param dst dst.
     * @param deleteSource - delete src if true
     * @param overwrite  overwrite dst if true; throw IOException if dst exists
     *         and overwrite is false.
     *
     * @return true if copy is successful
     *
     * @throws AccessControlException If access is denied
     * @throws FileAlreadyExistsException If <code>dst</code> already exists
     * @throws FileNotFoundException If <code>src</code> does not exist
     * @throws ParentNotDirectoryException If parent of <code>dst</code> is not
     *           a directory
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>src</code> or <code>dst</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server
     * 
     * RuntimeExceptions:
     * @throws InvalidPathException If path <code>dst</code> is invalid",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getWorkingDirectory(),641,645,"/**
* Returns the user's home directory as the working directory.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",4913,4917,"/**
* Initializes FSDataInputStreamBuilder with a FileSystem and Path.
* @param fileSystem underlying file system
* @param path input data location","* Path Constructor.
     * @param fileSystem owner
     * @param path path to open.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",4924,4928,"/**
* Initializes an FSDataInputStreamBuilder with a filesystem and path handle.
* @param fileSystem the underlying filesystem
* @param pathHandle identifies the path to stream from
*/","* Construct from a path handle.
     * @param fileSystem owner
     * @param pathHandle path handle of file to open.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,openFile,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String)",449,454,"/**
* Opens a file on the specified filesystem with given status and read policies.
* @param fs file system
* @param status file status
* @param readPolicies read policy configuration
* @return FSDataInputStream for reading or null if not accessible
*/","* Open a file.
   * <p>
   * If the WrappedIO class is found, use it.
   * <p>
   * If not, falls back to the classic {@code fs.open(Path)} call.
   * @param fs filesystem
   * @param status file status
   * @param readPolicies read policy to use
   * @return the input stream
   * @throws IOException any IO failure.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,tryLoadFromPath,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadFromPath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",188,216,"/**
* Tries to load FsPermission from a given file path and its backup,
* handling corruption or incorrect password scenarios.
* @param path primary file path
* @param backupPath backup file path
* @return loaded FsPermission object or null on failure
*/","* Try loading from the user specified path, else load from the backup
   * path in case Exception is not due to bad/wrong password.
   * @param path Actual path to load from
   * @param backupPath Backup path (_OLD)
   * @return The permissions of the loaded file
   * @throws NoSuchAlgorithmException
   * @throws CertificateException
   * @throws IOException",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,loadAndReturnPerm,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadAndReturnPerm(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",252,272,"/**
* Loads FsPermission from one path and deletes another, returning the loaded permission.
* @param pathToLoad path containing FsPermission to load
* @param pathToDelete path to delete after loading
* @return loaded FsPermission or null on failure
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,resetKeyStoreState,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:resetKeyStoreState(org.apache.hadoop.fs.Path),586,598,"/**
* Resets key store state by flushing cache and reloading from previous path.
* @param path Path to the previous key store location
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)",2921,2924,"/**
* Initializes a new Sorter instance with the provided configuration.
* @param fs FileSystem object
* @param keyClass Class of the key to be sorted (WritableComparable)
* @param valClass Class of the value associated with the key
* @param conf Hadoop Configuration object
*/","* Sort and merge files containing the named classes.
     * @param fs input FileSystem.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param conf input Configuration.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2Compressor,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Compressor(org.apache.hadoop.conf.Configuration),98,101,"/**
* Returns a BZIP2 compressor instance based on native library availability.
* @param conf Hadoop configuration
*/","* Return the appropriate implementation of the bzip2 compressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the bzip2 compressor.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,initialize,"org.apache.hadoop.io.SequenceFile$Reader:initialize(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,long,org.apache.hadoop.conf.Configuration,boolean)",1965,1989,"/**
* Initializes the input stream and file metadata.
* @param filename file name
* @param in input stream
* @param start starting position
* @param length data length
* @param conf configuration
* @param tempReader whether to use a temporary reader
*/",Common work of the constructors.,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createOutputStream,"org.apache.hadoop.io.compress.ZStandardCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",137,144,"/**
* Creates a compression output stream using the specified compressor.
* @param out target output stream
* @param compressor compression algorithm to use
* @return CompressionOutputStream instance or throws IOException on error
*/","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to have compressed
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createCompressor,org.apache.hadoop.io.compress.ZStandardCodec:createCompressor(),162,167,"/**
* Creates a compressor instance based on configuration settings.
* @return Compressor object with configured compression level and buffer size
*/","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createInputStream,"org.apache.hadoop.io.compress.ZStandardCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",194,201,"/**
* Creates a compression-aware input stream.
* @param in original input stream
* @param decompressor decompression object
*/","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createDecompressor,org.apache.hadoop.io.compress.ZStandardCodec:createDecompressor(),220,224,"/**
* Creates a decompressor instance with native code.
* @return Decompressor object for Z-standard decompression.","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createDirectDecompressor,org.apache.hadoop.io.compress.ZStandardCodec:createDirectDecompressor(),236,241,"/**
* Creates and returns a ZStandard direct decompressor instance.
* @return ZStandardDirectDecompressor object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,createReader,"org.apache.hadoop.io.file.tfile.BCFile$Reader:createReader(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion)",730,734,"/**
* Creates a BlockReader instance for the specified block region.
* @param compressAlgo compression algorithm to use
* @param region block region to read from
* @return initialized BlockReader object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareMetaBlock,"org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String,org.apache.hadoop.io.file.tfile.Compression$Algorithm)",345,363,"/**
* Prepares a Meta Block Appender instance.
* @param name unique block identifier
* @param compressAlgo compression algorithm to use
* @return BlockAppender instance or throws IOException/MetaBlockAlreadyExists if invalid
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareDataBlock,org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareDataBlock(),417,436,"/**
* Prepares a Data Block for writing.
* @return BlockAppender instance for appending data
*/","* Create a Data Block and obtain an output stream for adding data into the
     * block. There can only be one BlockAppender stream active at any time.
     * Data Blocks may not be created after the first Meta Blocks. The caller
     * must call BlockAppender.close() to conclude the block creation.
     * 
     * @return The BlockAppender stream
     * @throws IOException",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.conf.Configuration),103,106,"/**
 * Initializes ScriptBasedMapping with provided configuration.
 * @param conf Hadoop Configuration object
 */","* Create an instance from the given configuration
   * @param conf configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,setConf,org.apache.hadoop.net.ScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration),85,89,"/**
* Sets configuration for this object and its underlying raw mapping.
* @param conf Configuration object to apply
*/","* {@inheritDoc}.
   * <p>
   * This will get called in the superclass constructor, so a check is needed
   * to ensure that the raw mapping is defined before trying to relaying a null
   * configuration.
   * </p>
   * @param conf input Configuration.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,init,org.apache.hadoop.crypto.key.KeyShell:init(java.lang.String[]),80,168,"/**
* Parses command line arguments for key management operations.
* @param args array of command line parameters
* @return 0 on success, non-zero on error or help requested
*/","* Parse the command line arguments and initialize the data.
   * <pre>
   * % hadoop key create keyName [-size size] [-cipher algorithm]
   *    [-provider providerPath]
   * % hadoop key roll keyName [-provider providerPath]
   * % hadoop key list [-provider providerPath]
   * % hadoop key delete keyName [-provider providerPath] [-i]
   * % hadoop key invalidateCache keyName [-provider providerPath]
   * </pre>
   * @param args Command line arguments.
   * @return 0 on success, 1 on failure.
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataInputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",33,36,"/**
* Initializes CryptoFSDataInputStream with a wrapped FSDataInputStream and encryption parameters.
* @param in input stream to encrypt
* @param codec encryption algorithm
* @param key encryption key
* @param iv initialization vector
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[],long)",125,128,"/**
* Initializes CryptoOutputStream with given parameters.
* @param out underlying OutputStream
* @param codec encryption/decryption algorithm to use
* @param key cryptographic key for encryption/decryption
* @param iv initialization vector for encryption/decryption
* @param streamOffset starting position in the output stream",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getConnectionId,"org.apache.hadoop.ipc.Client$ConnectionId:getConnectionId(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)",1798,1817,"/**
* Creates a ConnectionId object with the specified parameters.
* @param addr InetSocketAddress containing server details
* @param protocol Class of RPC protocol to use
* @param ticket UserGroupInformation for authentication
* @param rpcTimeout timeout in milliseconds for RPC requests
* @param connectionRetryPolicy Retry policy for failed connections
* @param conf Configuration object with retry settings
* @return ConnectionId object or throws IOException if creation fails.","* Returns a ConnectionId object. 
     * @param addr Remote address for the connection.
     * @param protocol Protocol for RPC.
     * @param ticket UGI
     * @param rpcTimeout timeout
     * @param conf Configuration object
     * @return A ConnectionId instance
     * @throws IOException",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,open,"org.apache.hadoop.fs.ftp.FTPFileSystem:open(org.apache.hadoop.fs.Path,int)",276,306,"/**
* Opens a file for reading, handling directory detection and buffer allocation.
*@param file Path to the file to be opened
*@param bufferSize Buffer size to allocate on the FTP server
*@return FSDataInputStream object or throws IOException if operation fails.",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,create,"org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",312,375,"/**
* Creates a file on the HDFS with specified permissions and settings.
* @param file HDFS path to create
* @param permission file permissions
* @param overwrite whether to overwrite existing files
* @param bufferSize buffer size for FTP transfers
* @param replication HDFS replication factor
* @param blockSize block size for FTP transfers
* @param progress progress monitor
* @return FSDataOutputStream for writing to the new file
*/","* A stream obtained via this call must be closed before using other APIs of
   * this class or else the invocation will block.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,delete,"org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",400,409,"/**
* Deletes a remote file or directory via FTP.
* @param file the Path to the remote file/directory
* @param recursive whether to delete recursively
* @return true if deletion was successful, false otherwise
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,listStatus,org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.hadoop.fs.Path),468,477,"/**
* Lists file statuses on remote server via FTP.
* @param file Path to the file
* @return Array of FileStatus objects or null on failure
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFileStatus,org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.hadoop.fs.Path),500,509,"/**
* Retrieves file status via FTP connection.
* @param file Path to the remote file
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",574,583,"/**
* Creates a new directory on the remote server.
* @param file Path to create directory at
* @param permission Desired permissions for the directory
* @return true if successful, false otherwise
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,rename,"org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",631,640,"/**
* Renames a file on an FTP server.
* @param src original file path
* @param dst new file path
* @return true if the renaming was successful, false otherwise
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.ftp.FTPFileSystem:getHomeDirectory(),713,729,"/**
* Retrieves the user's home directory via FTP.
* @return Path object representing the home directory; null if failed
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getServerDefaults,org.apache.hadoop.fs.HarFileSystem:getServerDefaults(),1260,1264,"/**
* Retrieves server defaults from underlying file system.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getServerDefaults,org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults(),158,162,"/**
* Returns server defaults, currently just delegates to FS implementation. 
* @deprecated Use fsImpl.getServerDefaults() directly instead.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getServerDefaults,org.apache.hadoop.fs.FileSystem:getServerDefaults(org.apache.hadoop.fs.Path),963,965,"/**
* Returns server defaults for the given file system location.
* @param p Path to the file system location
* @return FsServerDefaults object or null if not found
*/","* Return a set of server default configuration values.
   * @param p path is used to identify an FS since an FS could have
   *          another FS that it could be delegating the call to
   * @return server default configuration values
   * @throws IOException IO failure",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getServerDefaults,org.apache.hadoop.fs.FilterFileSystem:getServerDefaults(),436,439,"/**
* Retrieves server defaults from underlying file system.
* @return FsServerDefaults object containing server settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),1288,1292,"/**
* Returns default block size for the specified file.
* @param f file path
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean)",1089,1096,"/**
* Creates an output stream to a file.
* @param f path of the file
* @param overwrite whether to overwrite existing file (true) or fail (false)
*/","* Create an FSDataOutputStream at the indicated Path.
   * @param f the file to create
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an exception will be thrown.
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)",1107,1114,"/**
 * Creates a new FSDataOutputStream for writing to the specified file.
 * @param f Path to the file
 * @param progress Progressable object or null for no progress tracking
 */","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * Files are overwritten by default.
   * @param f the file to create
   * @param progress to report progress
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,short)",1124,1131,"/**
* Creates a new output stream to the specified file with the given replication factor.
* @param f file path
* @param replication data replication factor (short)
*/","* Create an FSDataOutputStream at the indicated Path.
   * Files are overwritten by default.
   * @param f the file to create
   * @param replication the replication factor
   * @throws IOException IO failure
   * @return output stream1",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)",1143,1149,"/**
* Creates a new output stream to the specified file with custom replication.
* @param f Path to the output file
* @param replication Replication factor for the output stream
* @param progress Progressable object for tracking write progress
*/","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * Files are overwritten by default.
   * @param f the file to create
   * @param replication the replication factor
   * @param progress to report progress
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int)",1161,1168,"/**
* Creates an FSDataOutputStream for writing to a file.
* @param f the Path to the file
* @param overwrite whether to overwrite existing files
* @param bufferSize buffer size in bytes
*/","* Create an FSDataOutputStream at the indicated Path.
   * @param f the file to create
   * @param overwrite if a path with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)",1183,1191,"/**
* Creates an output stream to the specified file with custom replication and block size.
* @param f file path
* @param overwrite whether to overwrite existing file
* @param bufferSize I/O buffer size
* @param progress callback for reporting progress
*/","* Create an {@link FSDataOutputStream} at the indicated Path
   * with write-progress reporting.
   *
   * The frequency of callbacks is implementation-specific; it may be ""none"".
   * @param f the path of the file to open
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param progress to report progress.
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),976,988,"/**
* Retrieves the default block size from the target file system.
* @param f Path to the file
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),442,445,"/**
* Returns default block size for file.
* @param f Path to file
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,<init>,"org.apache.hadoop.fs.FSDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",130,139,"/**
* Initializes FSDataOutputStreamBuilder with a FileSystem and Path.
* @param fileSystem the underlying filesystem
* @param p the path to write to
*/","* Constructor.
   *
   * @param fileSystem file system.
   * @param p the path.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DFCachingGetSpaceUsed.java,<init>,org.apache.hadoop.fs.DFCachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),39,42,"/**
* Initializes caching get space used with specified path and interval.
* @param builder configuration builder
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable),2462,2506,"/**
* Reads a Writable object from the underlying stream.
* @param key the Writable object to read
* @return true if successful, false otherwise
*/","* @return Read the next key in the file into <code>key</code>, skipping its
     * value.True if another entry exists, and false at end of file.
     *
     * @param key key.
     * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Reader:next(java.lang.Object),2707,2752,"/**
* Fetches a serialized user key by ID, deserializing and validating its contents.
* @param key object containing the key to fetch
* @return deserialized key or null if not found
*/","* Read the next key in the file, skipping its
     * value.
     *
     * @param key input Object key.
     * @throws IOException raised on errors performing I/O.
     * @return Return null at end of file.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,initSingleton,"org.apache.hadoop.metrics2.source.JvmMetrics:initSingleton(java.lang.String,java.lang.String)",129,131,"/**
* Initializes JVM metrics singleton instance.
* @param processName name of the process
* @param sessionId unique session identifier
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>(java.lang.Class,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",162,170,"/**
* Initializes an Invoker instance with the given protocol, connection ID,
* configuration, and factory.
* @param protocol class representing the remote protocol
* @param connId unique identifier for the client connection
* @param conf client configuration settings
* @param factory socket creation factory
* @param alignmentContext context for data alignment
*/","* This constructor takes a connectionId, instead of creating a new one.
     * @param protocol input protocol.
     * @param connId input connId.
     * @param conf input Configuration.
     * @param factory input factory.
     * @param alignmentContext Alignment context",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,getClient,org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration),77,79,"/**
* Retrieves client instance based on provided configuration.
* @param conf application configuration
*/","* Construct &amp; cache an IPC client with the default SocketFactory
   * and default valueClass if no cached client exists. 
   * 
   * @param conf Configuration
   * @return an IPC client",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,getClient,"org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",89,91,"/**
* Retrieves client instance based on configuration and socket factory.
* @param conf application configuration
* @param factory socket creation utility
*/","* Construct &amp; cache an IPC client with the user-provided SocketFactory
   * if no cached client exists. Default response type is ObjectWritable.
   * 
   * @param conf Configuration
   * @param factory SocketFactory for client socket
   * @return an IPC client",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getClient,org.apache.hadoop.ipc.ProtobufRpcEngine2:getClient(org.apache.hadoop.conf.Configuration),370,376,"/**
* Retrieves a Client instance based on the provided Configuration.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:<init>(java.lang.Class,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",170,178,"/**
* Initializes Invoker instance with protocol, connection ID, and configuration.
* @param protocol the protocol class
* @param connId unique connection identifier
* @param conf client configuration
* @param factory socket factory for network operations
* @param alignmentContext context for alignment calculations
*/","* This constructor takes a connectionId, instead of creating a new one.
     *
     * @param protocol input protocol.
     * @param connId input connId.
     * @param conf input Configuration.
     * @param factory input factory.
     * @param alignmentContext Alignment context",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getClient,org.apache.hadoop.ipc.ProtobufRpcEngine:getClient(org.apache.hadoop.conf.Configuration),360,366,"/**
* Retrieves a client instance using the provided configuration.
* @param conf HBase client configuration
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java,<init>,org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>(),69,71,"/**
* Initializes factory with default configuration.",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,isMethodSupported,"org.apache.hadoop.ipc.RpcClientUtil:isMethodSupported(java.lang.Object,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcKind,long,java.lang.String)",108,147,"/**
* Checks if a specific method is supported by the given RPC proxy.
* @param rpcProxy RPC proxy object
* @param protocol class containing the method's signature
* @param rpcKind kind of RPC request (e.g. synchronous)
* @param version protocol version
* @param methodName name of the method to check for support
* @return true if the method is supported, false otherwise
*/","* Returns whether the given method is supported or not.
   * The protocol signatures are fetched and cached. The connection id for the
   * proxy provided is re-used.
   * @param rpcProxy Proxy which provides an existing connection id.
   * @param protocol Protocol for which the method check is required.
   * @param rpcKind The RpcKind for which the method check is required.
   * @param version The version at the client.
   * @param methodName Name of the method.
   * @return true if the method is supported, false otherwise.
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFCRpcServer.java,<init>,"org.apache.hadoop.ha.ZKFCRpcServer:<init>(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,org.apache.hadoop.ha.ZKFailoverController,org.apache.hadoop.security.authorize.PolicyProvider)",47,76,"/**
* Initializes ZKFC RPC server with given configuration and settings.
* @param conf Hadoop configuration object
* @param bindAddr address to bind the server to
* @param zkfc ZooKeeper failover controller instance
* @param policy service-level authorization security policy (optional)
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,<init>,"org.apache.hadoop.fs.Trash:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)",60,63,"/**
* Initializes Trash object with given file system and configuration.
* @param fs FileSystem instance
* @param conf Configuration object for setup
*/","* Construct a trash can accessor for the FileSystem provided.
   * @param fs the FileSystem
   * @param conf a Configuration
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,build,org.apache.hadoop.fs.GetSpaceUsed$Builder:build(),144,176,"/**
* Constructs a GetSpaceUsed object, potentially using fallback implementations based on the platform. 
* @return GetSpaceUsed object or a fallback implementation if creation fails
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getInstance,org.apache.hadoop.net.NetworkTopology:getInstance(org.apache.hadoop.conf.Configuration),73,75,"/**
* Retrieves an instance of NetworkTopology based on configuration and factory type.
* @param conf Configuration object
*/","* Get an instance of NetworkTopology based on the value of the configuration
   * parameter net.topology.impl.
   * 
   * @param conf the configuration to be used
   * @return an instance of NetworkTopology",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DomainNameResolverFactory.java,newInstance,"org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",55,59,"/**
* Creates a new DomainNameResolver instance with custom configuration.
* @param conf Configuration object
* @param host Host name to append to configuration key
* @param configKey Custom configuration key
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setConfigurationInternal,org.apache.hadoop.security.SecurityUtil:setConfigurationInternal(org.apache.hadoop.conf.Configuration),105,125,"/**
* Initializes internal configuration from provided Hadoop Configuration object.
* @param conf Hadoop Configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,<init>,"org.apache.hadoop.security.SaslRpcClient:<init>(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",118,125,"/**
* Initializes a new SASL RPC client instance.
* @param ugi UserGroupInformation for authentication
* @param protocol Class of the RPC protocol to use
* @param serverAddr Address of the RPC server
* @param conf Configuration parameters for the client
*/","* Create a SaslRpcClient that can be used by a RPC client to negotiate
   * SASL authentication with a RPC server
   * @param ugi - connecting user
   * @param protocol - RPC protocol
   * @param serverAddr - InetSocketAddress of remote server
   * @param conf - Configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,<init>,org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration),100,102,"/**
 * Initializes Groups with default timer.
 * @param conf Hadoop configuration
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,refreshSuperUserGroupsConfiguration,"org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration,java.lang.String)",70,80,"/**
* Refreshes super user groups configuration based on proxy user prefix.
* @param conf Hadoop Configuration
* @param proxyUserPrefix Prefix for proxy users
*/","* Refreshes configuration using the specified Proxy user prefix for
   * properties.
   *
   * @param conf configuration
   * @param proxyUserPrefix proxy user configuration prefix",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,setConf,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration),55,59,"/**
* Sets configuration and engine ID from the provided Configuration object.
* @param conf Hadoop Configuration object containing security settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,addShutdownHook,"org.apache.hadoop.util.ShutdownHookManager:addShutdownHook(java.lang.Runnable,int)",294,305,"/**
* Adds a shutdown hook with specified priority.
* @param shutdownHook Runnable to be executed on shutdown
* @param priority hook execution order (lower values run first)
*/","* Adds a shutdownHook with a priority, the higher the priority
   * the earlier will run. ShutdownHooks with same priority run
   * in a non-deterministic order.
   *
   * @param shutdownHook shutdownHook <code>Runnable</code>
   * @param priority priority of the shutdownHook.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getPasswordString,"org.apache.hadoop.http.HttpServer2$Builder:getPasswordString(org.apache.hadoop.conf.Configuration,java.lang.String)",443,450,"/**
* Retrieves password string for given configuration and user name.
* @param conf Hadoop Configuration object
* @param name unique user identifier
* @return password string or null if not set
*/","* A wrapper of {@link Configuration#getPassword(String)}. It returns
     * <code>String</code> instead of <code>char[]</code>.
     *
     * @param conf the configuration
     * @param name the property name
     * @return the password string or null",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getPassword,"org.apache.hadoop.security.LdapGroupsMapping:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",914,927,"/**
* Retrieves user password from configuration.
* @param conf Configuration object
* @param alias unique alias identifier
* @param defaultPass default password value (used if not found in config)
* @return Password string or default pass if retrieval fails
*/","* Passwords should not be stored in configuration. Use
   * {@link #getPasswordFromCredentialProviders(
   *            Configuration, String, String)}
   * to avoid reading passwords from a configuration file.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,getPassword,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",302,315,"/**
* Retrieves the password associated with a given alias from configuration.
* @param conf Hadoop Configuration object
* @param alias unique alias identifier
* @param defaultPass default password to use if not found in config
* @return password string or default pass if not found
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getZKAuthInfos,"org.apache.hadoop.security.SecurityUtil:getZKAuthInfos(org.apache.hadoop.conf.Configuration,java.lang.String)",775,791,"/**
* Retrieves HBase ZooKeeper authentication information from the given configuration.
* @param conf HBase Configuration object
* @param configKey key to fetch auth info for, e.g. ""hbase.zookeeper.quorum""
* @return List of ZKAuthInfo objects or empty list if not found; null on failure
*/","* Utility method to fetch ZK auth info from the configuration.
   *
   * @param conf configuration.
   * @param configKey config key.
   * @throws java.io.IOException if the Zookeeper ACLs configuration file
   * cannot be read
   * @throws ZKUtil.BadAuthFormatException if the auth format is invalid
   * @return ZKAuthInfo List.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,checkCreateRSRawEncoder,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:checkCreateRSRawEncoder(),52,59,"/**
* Retrieves or creates a RawErasureEncoder instance based on configuration. 
* @return RawErasureEncoder object or null if not configured
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,checkCreateRSRawEncoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:checkCreateRSRawEncoder(),61,67,"/**
* Creates and returns a raw encoder for RS erasure code.
* @return RawErasureEncoder instance or null if already created
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,checkCreateXorRawEncoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:checkCreateXorRawEncoder(),69,76,"/**
* Returns the XOR raw encoder instance, creating it if not yet initialized.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,checkCreateXorRawEncoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:checkCreateXorRawEncoder(),75,81,"/**
* Creates and returns an instance of RawErasureEncoder.
* @return RawErasureEncoder object or null if already created
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),40,50,"/**
* Prepares the encoding step for erasure coding.
* @param blockGroup EC block group
* @return ErasureCodingStep object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),40,51,"/**
* Prepares decoding step for erasure coding.
* @param blockGroup EC block group to process
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,checkCreateRSRawDecoder,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:checkCreateRSRawDecoder(),52,58,"/**
* Creates or retrieves RS raw decoder instance.
* @return RawErasureDecoder object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,checkCreateRSRawDecoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:checkCreateRSRawDecoder(),67,73,"/**
* Creates or retrieves RawErasureDecoder instance.
* @return RawErasureDecoder object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,refreshCallQueue,org.apache.hadoop.ipc.Server:refreshCallQueue(org.apache.hadoop.conf.Configuration),903,915,"/**
* Refreshes the call queue with new configuration.
* @param conf updated IPC server configuration
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,"org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,org.apache.hadoop.conf.Configuration)",88,94,"/**
* Initializes a FairCallQueue with specified parameters.
* @param priorityLevels number of priority levels
* @param capacity total queue capacity
* @param ns namespace for the queue
* @param conf configuration object
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,"org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,boolean,org.apache.hadoop.conf.Configuration)",96,102,"/**
* Constructs a FairCallQueue with default weights.
* @param priorityLevels number of priority levels
* @param capacity total queue capacity
* @param ns namespace for this queue
* @param serverFailOverEnabled whether to enable failover
* @param conf configuration settings
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,<init>,"org.apache.hadoop.ipc.DecayRpcScheduler:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)",236,279,"/**
* Initializes a DecayRpcScheduler instance with specified configuration.
* @param numLevels number of priority levels
* @param ns namespace identifier
* @param conf configuration object
*/","* Create a decay scheduler.
   * @param numLevels number of priority levels
   * @param ns config prefix, so that we can configure multiple schedulers
   *           in a single instance.
   * @param conf configuration to use.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,clone,"org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)",218,227,"/**
* Creates a deep copy of the given writable object.
* @param orig original object to clone
* @param conf configuration for cloning
* @return cloned Writable object or null if creation fails
*/","* Make a copy of a writable object using serialization to a buffer.
   *
   * @param <T> Generics Type T.
   * @param orig The object to copy
   * @param conf input Configuration.
   * @return The copied object",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.LdapGroupsMapping:getGroupsSet(java.lang.String),726,760,"/**
* Retrieves a set of groups for the specified user.
* @param user unique user identifier
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,reencryptEncryptedKey,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),327,354,"/**
* Reencrypts an EncryptedKeyVersion using the current encryption key.
* @param ekv EncryptedKeyVersion to reencrypt
* @return updated EncryptedKeyVersion or original if unchanged
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,org.apache.hadoop.conf.Configuration:writeXml(java.io.OutputStream),3589,3591,"/**
* Writes XML data to the specified output stream.
* @param out target output stream
*/","* Write out the non-default properties in this configuration to the given
   * {@link OutputStream} using UTF-8 encoding.
   *
   * @param out the output stream to write to.
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfServlet.java,writeResponse,"org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String)",107,110,"/**
* Writes formatted response to output stream.
* @param conf configuration object
* @param out writer for response output
* @param format desired response format
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configure,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configure(java.lang.String),481,486,"/**
* Configures metrics with specified prefix.
* @param prefix metric prefix string
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,create,"org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",547,553,"/**
* Creates a new file output stream at the specified path.
* @param f Path to the file
* @param overwrite whether to overwrite existing files
* @param other parameters are ignored in this method call
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,create,"org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",592,600,"/**
* Creates a new output stream to the specified file.
* @param f path to the file
* @param overwrite whether to overwrite existing files
* @return FSDataOutputStream instance
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.RawLocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",602,610,"/**
* Creates a non-recursive file output stream with specified permissions.
* @param f path to the file
* @param permission file system permissions
* @param overwrite whether to overwrite existing files
* @return FSDataOutputStream object or throws IOException if failed
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,constructSecretProvider,"org.apache.hadoop.http.HttpServer2:constructSecretProvider(org.apache.hadoop.http.HttpServer2$Builder,javax.servlet.ServletContext)",862,870,"/**
* Constructs a SignerSecretProvider instance based on filter configuration.
* @param b Builder object containing filter settings
* @param ctx Servlet context for accessing environment properties
* @return SignerSecretProvider object or throws Exception if failed
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilterInitializer.java,initFilter,"org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",53,58,"/**
* Initializes the ProxyUserAuthenticationFilter with configuration.
* @param container FilterContainer instance
* @param conf Hadoop Configuration object
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",98,101,"/**
* Initializes the ZKSecretManager with configuration and secret token kind.
* @param conf Hadoop Configuration object
* @param tokenKind Secret token kind as a string
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,newZooKeeper,"org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean)",547,553,"/**
* Creates a new ZooKeeper instance with default configuration.
* @param connectString connection string to the ZooKeeper ensemble
* @param sessionTimeout timeout for client sessions in milliseconds
* @param watcher callback for Watcher events (may be null)
* @param canBeReadOnly whether this ZooKeeper may operate in read-only mode
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,selectSaslClient,org.apache.hadoop.security.SaslRpcClient:selectSaslClient(java.util.List),154,187,"/**
* Selects the first valid SASL authentication type from the list.
* @param authTypes list of available SASL authentication types
* @return selected SASL authentication type or null if none found
*/","* Instantiate a sasl client for the first supported auth type in the
   * given list.  The auth type must be defined, enabled, and the user
   * must possess the required credentials, else the next auth is tried.
   * 
   * @param authTypes to attempt in the given order
   * @return SaslAuth of instantiated client
   * @throws AccessControlException - client doesn't support any of the auths
   * @throws IOException - misc errors",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpFs.java,<init>,"org.apache.hadoop.fs.ftp.FtpFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",50,53,"/**
* Initializes an FTP-based file system.
* @param theUri URI of the FTP server
* @param conf configuration for this file system
*/","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   * 
   * @param theUri which must be that of localFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFs.java,<init>,"org.apache.hadoop.fs.HarFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",28,31,"/**
* Initializes HarFS with the provided configuration and URI.
* @param theUri URI of the HarFS instance
* @param conf Configuration settings for this instance
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,<init>,"org.apache.hadoop.fs.local.RawLocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",55,59,"/**
* Initializes a raw local file system with custom configuration.
* @param theUri URI for the file system
* @param conf configuration settings
*/","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   * 
   * @param theUri which must be that of localFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,initialize,"org.apache.hadoop.fs.http.HttpsFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",48,52,"/**
* Initializes the object with URI and configuration.
* @param name URI to initialize with
* @param conf Configuration for initialization
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,initialize,"org.apache.hadoop.fs.http.HttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",48,52,"/**
* Initializes configuration and sets URI.
* @param name URI to set
* @param conf Configuration object
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,initialize,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",150,179,"/**
* Initializes the ViewFileSystemOverloadScheme with the provided URI and Configuration.
* @param theUri unique identifier for the file system
* @param conf configuration object containing mount table settings
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,initialize,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",135,140,"/**
* Initializes the object with configuration.
* @param name URI to initialize
* @param conf Configuration object for initialization
*/","* Called after a new FileSystem instance is constructed.
   * @param name a uri whose authority section names the host, port, etc.
   *   for this FileSystem
   * @param conf the configuration",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,checkDependencies,"org.apache.hadoop.fs.FileUtil:checkDependencies(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",336,353,"/**
* Checks for invalid dependencies between source and destination file systems.
* @param srcFS source file system
* @param src source path
* @param dstFS destination file system
* @param dst destination path
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",95,104,"/**
* Initializes MultipartUploaderBuilderImpl with a FileSystem and Path.
* @param fileSystem non-null file system instance
* @param p non-null path to initialize the builder with
*/","* Constructor.
   *
   * @param fileSystem fileSystem.
   * @param p path.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.FileStatus)",156,166,"/**
* Initializes PathData object from file system, path string, and status.
* @param fs the file system
* @param pathString the path string
* @param stat the file status
*/","* Creates an object to wrap the given parameters as fields.  The string
   * used to create the path will be recorded since the Path object does not
   * return exactly the same string used to initialize it.
   * @param fs the FileSystem
   * @param pathString a String of the path
   * @param stat the FileStatus (may be null if the path doesn't exist)",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,hasPathCapability,"org.apache.hadoop.fs.FileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",3487,3501,"/**
* Checks if a path has a specified capability.
* @param path qualified file system path
* @param capability specific capability to check (e.g. BULK_DELETE, FS_SYMLINKS)
* @return true if the path supports the capability, false otherwise
*/","* The base FileSystem implementation generally has no knowledge
   * of the capabilities of actual implementations.
   * Unless it has a way to explicitly determine the capabilities,
   * this method returns false.
   * {@inheritDoc}",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.FileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),4973,4978,"/**
* Returns the enclosing root directory of a given file or directory.
* @param path file system path to determine the root for
* @return Path object representing the root directory, or null if invalid
*/","* Return path of the enclosing root for a given path.
   * The enclosing root path is a common ancestor that should be used for temp and staging dirs
   * as well as within encryption zones and other restricted directories.
   *
   * Call makeQualified on the param path to ensure its part of the correct filesystem.
   *
   * @param path file path to find the enclosing root path for
   * @return a path to the enclosing root
   * @throws IOException early checks like failure to resolve path cause IO failures",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,makeQualified,org.apache.hadoop.fs.FilterFileSystem:makeQualified(org.apache.hadoop.fs.Path),124,139,"/**
* Returns a qualified path with scheme swapped if necessary.
* @param path input Path object
* @return qualified Path or original if no swap required
*/",Make sure that a path specifies a FileSystem.,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,resolvePath,org.apache.hadoop.fs.HarFileSystem:resolvePath(org.apache.hadoop.fs.Path),343,346,"/**
 * Resolves a path relative to the file system. 
 * @param p path to be resolved
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,getFileStatus,"org.apache.hadoop.fs.shell.find.BaseExpression:getFileStatus(org.apache.hadoop.fs.shell.PathData,int)",279,290,"/**
* Resolves file status for a given path, handling symlinks if necessary.
* @param item PathData object containing file statistics
* @param depth current recursion depth (0 indicates root)
* @return FileStatus object or null if not found
*/","* Returns the {@link FileStatus} from the {@link PathData} item. If the
   * current options require links to be followed then the returned file status
   * is that of the linked file.
   *
   * @param item
   *          PathData
   * @param depth
   *          current depth in the process directories
   * @return FileStatus
   * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,resolvePath,org.apache.hadoop.fs.viewfs.ViewFileSystem:resolvePath(org.apache.hadoop.fs.Path),412,420,"/**
* Resolves a file path to an absolute path within the target filesystem.
* @param f input file path
* @return resolved Path object or original Path if internal directory
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,resolvePath,org.apache.hadoop.fs.FilterFileSystem:resolvePath(org.apache.hadoop.fs.Path),157,160,"/**
* Resolves a given path to an absolute file system path.
* @param p input path to be resolved
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,fullPath,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:fullPath(org.apache.hadoop.fs.Path),91,97,"/**
* Constructs the full path by combining the root directory and input path.
* @param path input path to combine with root directory
*/","* @param path
   * @return  full path including the chroot",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,copy,"org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2173,2178,"/**
* Copies a file from the source path to the destination path.
* @param src source file path
* @param dst destination file path
*/","* Copy file from src to dest. See
     * {@link #copy(Path, Path, boolean, boolean)}
     *
     * @param src src.
     * @param dst dst.
     * @throws AccessControlException If access is denied.
     * @throws FileAlreadyExistsException If file <code>src</code> already exists.
     * @throws FileNotFoundException if next file does not exist any more.
     * @throws ParentNotDirectoryException If parent of <code>src</code> is not a
     * directory.
     * @throws UnsupportedFileSystemException If file system for
     * <code>src/dst</code> is not supported.
     * @throws IOException If an I/O error occurred.
     * @return if success copy true, not false.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createDataInputStreamBuilder,"org.apache.hadoop.fs.FileSystem:createDataInputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",4877,4883,"/**
* Creates an FSDataInputStreamBuilder for the given filesystem and path.
* @param fileSystem the filesystem to operate on
* @param path the path to create a data input stream builder for
*/","* Create instance of the standard {@link FSDataInputStreamBuilder} for the
   * given filesystem and path.
   * @param fileSystem owner
   * @param path path to read
   * @return a builder.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createDataInputStreamBuilder,"org.apache.hadoop.fs.FileSystem:createDataInputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",4892,4898,"/**
* Creates a builder for FSDataInputStream.
* @param fileSystem file system instance
* @param pathHandle path handle object
*/","* Create instance of the standard {@link FSDataInputStreamBuilder} for the
   * given filesystem and path handle.
   * @param fileSystem owner
   * @param pathHandle path handle of file to open.
   * @return a builder.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,tryLoadIncompleteFlush,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadIncompleteFlush(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",229,250,"/**
* Tries to load incomplete flush permissions from either old or new path.
* @param oldPath original file path
* @param newPath new file path (result of flush operation)
* @return FsPermission object or null if not found, otherwise initialized keystore with default perms
*/","* The KeyStore might have gone down during a flush, In which case either the
   * _NEW or _OLD files might exists. This method tries to load the KeyStore
   * from one of these intermediate files.
   * @param oldPath the _OLD file created during flush
   * @param newPath the _NEW file created during flush
   * @return The permissions of the loaded file
   * @throws IOException
   * @throws NoSuchAlgorithmException
   * @throws CertificateException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createCompressor,org.apache.hadoop.io.compress.BZip2Codec:createCompressor(),147,150,"/**
* Creates BZip2 compressor instance based on configuration.","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getMetaBlock,org.apache.hadoop.io.file.tfile.BCFile$Reader:getMetaBlock(java.lang.String),701,710,"/**
* Retrieves a meta block by name from the index, creating a reader if found.
* @param name unique identifier of the meta block
* @return BlockReader object or throws exception if not found
*/","* Stream access to a Meta Block.
     * 
     * @param name
     *          meta block name
     * @return BlockReader input stream for reading the meta block.
     * @throws IOException
     * @throws MetaBlockDoesNotExist
     *           The Meta Block with the given name does not exist.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getDataBlock,org.apache.hadoop.io.file.tfile.BCFile$Reader:getDataBlock(int),720,728,"/**
* Retrieves a data block by index.
* @param blockIndex the index of the desired block
* @return BlockReader object for the specified block or throws exception if invalid
*/","* Stream access to a Data Block.
     * 
     * @param blockIndex
     *          0-based data block index.
     * @return BlockReader input stream for reading the data block.
     * @throws IOException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,close,org.apache.hadoop.io.file.tfile.BCFile$Writer:close(),303,339,"/**
* Closes the data index, writing meta blocks and version information.
* @throws IOException if an I/O error occurs
*/","* Close the BCFile Writer. Attempting to use the Writer after calling
     * <code>close</code> is not allowed and may lead to undetermined results.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareMetaBlock,"org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)",381,385,"/**
* Prepares meta block with specified name and compression algorithm.
* @param name unique meta block identifier
* @param compressionName name of the compression algorithm to use
*/","* Create a Meta Block and obtain an output stream for adding data into the
     * block. There can only be one BlockAppender stream active at any time.
     * Regular Blocks may not be created after the first Meta Blocks. The caller
     * must call BlockAppender.close() to conclude the block creation.
     * 
     * @param name
     *          The name of the Meta Block. The name must not conflict with
     *          existing Meta Blocks.
     * @param compressionName
     *          The name of the compression algorithm to be used.
     * @return The BlockAppender stream
     * @throws IOException
     * @throws MetaBlockAlreadyExists
     *           If the meta block with the name already exists.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareMetaBlock,org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String),403,406,"/**
* Prepares meta block with default compression algorithm.
* @param name unique meta block identifier
*/","* Create a Meta Block and obtain an output stream for adding data into the
     * block. The Meta Block will be compressed with the same compression
     * algorithm as data blocks. There can only be one BlockAppender stream
     * active at any time. Regular Blocks may not be created after the first
     * Meta Blocks. The caller must call BlockAppender.close() to conclude the
     * block creation.
     * 
     * @param name
     *          The name of the Meta Block. The name must not conflict with
     *          existing Meta Blocks.
     * @return The BlockAppender stream
     * @throws MetaBlockAlreadyExists
     *           If the meta block with the name already exists.
     * @throws IOException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,initDataBlock,org.apache.hadoop.io.file.tfile.TFile$Writer:initDataBlock(),639,644,"/**
* Initializes data block with a new BCF appender.
*/","* Check if we need to start a new data block.
     * 
     * @throws IOException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",42,47,"/**
* Initializes a CryptoFSDataOutputStream with the given FSDataOutputStream and encryption parameters.
* @param out underlying FS data output stream
* @param codec encryption codec to use
* @param key encryption key
* @param iv initialization vector for encryption
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",120,123,"/**
* Initializes a CryptoOutputStream with specified parameters.
* @param out OutputStream to encrypt
* @param codec CryptoCodec instance for encryption
* @param key encryption key
* @param iv initialization vector for encryption
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.ftp.FTPFileSystem:getWorkingDirectory(),707,711,"/**
* Returns the working directory as the user's home directory.
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getServerDefaults,org.apache.hadoop.fs.HarFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),1266,1269,"/**
* Retrieves server defaults from file system.
* @param f Path to server defaults location
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getServerDefaults,org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),164,167,"/**
* Retrieves server defaults from the file system implementation.
* @param f path to fetch server defaults for
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),1004,1013,"/**
* Retrieves server defaults for the given file path.
* @param f Path to the file
* @return FsServerDefaults object or throws exception if not found
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getServerDefaults,org.apache.hadoop.fs.FilterFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),452,455,"/**
* Retrieves server defaults for a given file path.
* @param f file path to retrieve server defaults for
* @return FsServerDefaults object or null if not found
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processArguments,org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processArguments(java.util.LinkedList),392,424,"/**
* Processes input arguments and appends their contents to the target file.
* @param args list of input files to process
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path),1077,1079,"/**
* Creates an output stream to write data to the specified file.
* @param f Path to the file
*/","* Create an FSDataOutputStream at the indicated Path.
   * Files are overwritten by default.
   * @param f the file to create
   * @throws IOException IO failure
   * @return output stream.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,close,org.apache.hadoop.io.BloomMapFile$Writer:close(),192,204,"/**
* Closes the Bloom filter and flushes it to disk.
* @throws IOException if I/O error occurs while writing to file
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,createLogFile,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:createLogFile(org.apache.hadoop.fs.Path),688,716,"/**
* Creates a log file by attempting to create it with incrementing suffixes
* until successful. @param initial the base path for the log file
*/","* Create a new log file and return the {@link FSDataOutputStream}. If a
   * file with the specified path already exists, add a suffix, starting with 1
   * and try again. Keep incrementing the suffix until a nonexistent target
   * path is found.
   *
   * Once the file is open, update {@link #currentFSOutStream},
   * {@link #currentOutStream}, and {@#link #currentFilePath} are set
   * appropriately.
   *
   * @param initial the target path
   * @throws IOException thrown if the call to see if the exists fails",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,createOrAppendLogFile,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:createOrAppendLogFile(org.apache.hadoop.fs.Path),789,820,"/**
* Creates or appends log file at specified path. If the file exists, 
* attempts to append; otherwise, creates a new file and returns output stream.
* @param targetFile Path to create or append log file
*/","* Create a new log file and return the {@link FSDataOutputStream}. If a
   * file with the specified path already exists, open the file for append
   * instead.
   *
   * Once the file is open, update {@link #currentFSOutStream},
   * {@link #currentOutStream}, and {@#link #currentFilePath}.
   *
   * @param initial the target path
   * @throws IOException thrown if the call to see the append operation fails.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,save,"org.apache.hadoop.util.JsonSerialization:save(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Object,boolean)",293,297,"/**
* Saves an instance to a file system location.
* @param fs target file system
* @param path destination file path
* @param instance object to save
* @param overwrite whether to overwrite existing file (true) or not (false)
*/","* Save to a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @param overwrite should any existing file be overwritten
   * @param instance instance
   * @throws IOException IO exception.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",1231,1238,"/**
* Creates a new Writer instance with default compression and specified parameters.
*@param fs FileSystem object
*@param conf Configuration object
*@param name Path to write to
*@param keyClass Class of the key
*@param valClass Class of the value
*@param progress Progressable object for reporting progress
*@param metadata Metadata object
*/","* Create the named file with write-progress reporter.
     * @deprecated Use 
     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} 
     *   instead.
     * @param fs input filesystem.
     * @param conf input configuration.
     * @param name input name.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param progress input progress.
     * @param metadata input metadata.
     * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createNewFile,org.apache.hadoop.fs.FileSystem:createNewFile(org.apache.hadoop.fs.Path),1495,1503,"/**
* Creates a new file at the specified path.
* @param f Path to the file
* @return true if created successfully, false if already exists
*/","* Creates the given Path as a brand-new zero-length file.  If
   * create fails, or if it already existed, return false.
   * <i>Important: the default implementation is not atomic</i>
   * @param f path to use for create
   * @throws IOException IO failure
   * @return if create new file success true,not false.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",4690,4692,"/**
 * Initializes a new FileSystemDataOutputStreamBuilder instance.
 * @param fileSystem the underlying file system
 * @param p the path to build an output stream for","* Constructor.
     * @param fileSystem owner
     * @param p path to create",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,"org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",2518,2530,"/**
* Advances to the next item in the sequence and populates the provided Writable.
* @param key the Writable to store the key
* @param val the Writable to store the value
* @return true if a next item exists, false otherwise
*/","* Read the next key/value pair in the file into <code>key</code> and
     * <code>val</code>.
     * @return Returns true if such a pair exists and false when at
     * end of file.
     *
     * @param key input key.
     * @param val input val.
     * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,read,org.apache.hadoop.fs.shell.Display$TextRecordInputStream:read(),236,257,"/**
* Reads a character from the input buffer and generates key-value pairs as needed.
* @return the read character or -1 if end of file is reached
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",143,152,"/**
* Initializes an Invoker instance with the specified parameters.
* @param protocol remote protocol class
* @param addr server address and port
* @param ticket user authentication information
* @param conf configuration settings
* @param factory socket creation factory
* @param rpcTimeout RPC timeout value
* @param connectionRetryPolicy retry policy for connections
* @param fallbackToSimpleAuth flag to enable simple auth fallback
* @param alignmentContext alignment context object
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",80,88,"/**
* Creates a proxy object for the specified protocol.
* @param protocol target protocol class
* @param clientVersion client version identifier
* @param connId connection ID
* @param conf configuration settings
* @param factory socket factory instance
* @param alignmentContext alignment context information
* @return ProtocolProxy object or throws IOException if an error occurs.",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProtocolMetaInfoProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",121,130,"/**
* Creates a ProtocolProxy instance for the given connection ID.
* @param connId unique connection identifier
* @param conf configuration object
* @param factory socket factory instance
* @return ProtocolProxy<ProtocolMetaInfoPB> instance or throws IOException on failure
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getClient,org.apache.hadoop.ipc.WritableRpcEngine:getClient(org.apache.hadoop.conf.Configuration),279,283,"/**
* Retrieves a ZooKeeper client instance based on configuration.
* @param conf client configuration settings
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",219,230,"/**
* Initializes Invoker with given parameters and settings.
* @param protocol the communication protocol
* @param address the server address
* @param ticket user authentication information
* @param conf configuration details
* @param factory socket creation factory
* @param rpcTimeout RPC request timeout value
* @param fallbackToSimpleAuth whether to fall back to simple auth
* @param alignmentContext alignment context for client connection
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",103,111,"/**
* Creates a proxy for the specified protocol class.
* @param protocol target protocol class
* @param clientVersion client version number
* @param connId connection identifier
* @param conf configuration settings
* @param factory socket creation factory
* @param alignmentContext alignment context data
* @return ProtocolProxy object or throws IOException on error",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProtocolMetaInfoProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",128,137,"/**
* Creates a protocol meta info proxy for the given connection ID.
* @param connId unique connection identifier
* @param conf configuration object
* @param factory socket factory instance
* @return ProtocolProxy instance or throws IOException on failure
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",150,159,"/**
* Initializes Invoker with provided configuration.
* @param protocol communication protocol
* @param addr remote address
* @param ticket user authentication credentials
* @param conf configuration options
* @param factory socket creation factory
* @param rpcTimeout RPC request timeout in milliseconds
* @param connectionRetryPolicy retry policy for connection establishment
* @param fallbackToSimpleAuth flag to enable simple auth fallback
* @param alignmentContext alignment context for client connections
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),60,67,"/**
* Checks if a given method is supported by the RPC client.
* @param methodName name of the method to check
* @return true if method is supported, false otherwise
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),71,78,"/**
* Checks if a method is supported by the rpc proxy.
* @param methodName name of the method to check
* @return true if method is supported, false otherwise
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),106,113,"/**
* Checks if a method is supported by the RPC client.
* @param methodName name of the method to check
* @return true if method is supported, false otherwise
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),60,67,"/**
* Checks if a method is supported by the RPC client.
* @param methodName name of the method to check
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),61,66,"/**
* Checks if a given method is supported by the RPC client.
* @param methodName name of the method to check
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,initRPC,org.apache.hadoop.ha.ZKFailoverController:initRPC(),330,334,"/**
* Initializes the RPC server with specified configuration and address.
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,<init>,org.apache.hadoop.fs.Trash:<init>(org.apache.hadoop.conf.Configuration),50,52,"/**
* Initializes Trash with given configuration.
* @param conf Hadoop Configuration object
*/","* Construct a trash can accessor.
   * @param conf a Configuration
   * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,main,org.apache.hadoop.fs.DU:main(java.lang.String[]),91,102,"/**
* Runs the GetSpaceUsed tool with optional path override.
* @param args command-line arguments (path override if present)
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DomainNameResolverFactory.java,newInstance,"org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.String)",50,53,"/**
* Creates a new instance of DomainNameResolver.
* @param conf configuration object
* @param uri the URI to resolve (host only used)
* @param configKey key for resolver configuration
*/","* Create a domain name resolver to convert the domain name in the config to
   * the actual IP addresses of the Namenode/Router/RM.
   *
   * @param conf Configuration to get the resolver from.
   * @param uri the url that the resolver will be used against
   * @param configKey The config key name suffixed with
   *                  the nameservice/yarnservice.
   * @return Domain name resolver.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setConfiguration,org.apache.hadoop.security.SecurityUtil:setConfiguration(org.apache.hadoop.conf.Configuration),98,103,"/**
* Updates the global configuration with the provided settings.
* @param conf Configuration object containing new settings
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,<init>,org.apache.hadoop.security.UserGroupInformation$TestingGroups:<init>(org.apache.hadoop.security.Groups),1575,1578,"/**
* Constructs a TestingGroups object from an existing Groups instance.
* @param underlyingImplementation Groups implementation to wrap
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getUserToGroupsMappingService,org.apache.hadoop.security.Groups:getUserToGroupsMappingService(org.apache.hadoop.conf.Configuration),471,481,"/**
* Returns singleton instance of Groups service.
* @param conf Hadoop configuration
*/","* Get the groups being used to map user-to-groups.
   * @param conf configuration.
   * @return the groups being used to map user-to-groups.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getUserToGroupsMappingServiceWithLoadedConfiguration,org.apache.hadoop.security.Groups:getUserToGroupsMappingServiceWithLoadedConfiguration(org.apache.hadoop.conf.Configuration),488,495,"/**
* Initializes and returns user-to-groups mapping service with loaded configuration.
* @param conf loaded configuration for the service
* @return Groups service instance or null if initialization fails
*/","* Create new groups used to map user-to-groups with loaded configuration.
   * @param conf configuration.
   * @return the groups being used to map user-to-groups.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,init,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:init(javax.servlet.FilterConfig),53,58,"/**
* Initializes filter with configuration from proxy user settings.
* @param filterConfig Filter configuration object
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,init,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:init(javax.servlet.FilterConfig),177,202,"/**
* Initializes the filter with authentication handler and sets auth method based on handler type.
* @param filterConfig Filter configuration
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,refreshSuperUserGroupsConfiguration,org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration),86,88,"/**
* Refreshes super user groups configuration in Hadoop proxy user settings.
* @param conf Hadoop Configuration object
*/","* Refreshes configuration using the default Proxy user prefix for properties.
   * @param conf configuration",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,deleteOnExit,org.apache.hadoop.fs.FileContext:deleteOnExit(org.apache.hadoop.fs.Path),1706,1724,"/**
* Registers a file for deletion on JVM exit.
* @param f Path to the file to delete
* @return true if registration was successful, false otherwise
*/","* Mark a path to be deleted on JVM shutdown.
   * 
   * @param f the existing path to delete.
   *
   * @return  true if deleteOnExit is successful, otherwise false.
   *
   * @throws AccessControlException If access is denied
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,register,org.apache.hadoop.service.launcher.ServiceShutdownHook:register(int),61,64,"/**
* Registers this instance as a shutdown hook with specified priority.
* @param priority priority of the shutdown hook
*/","* Register the service for shutdown with Hadoop's
   * {@link ShutdownHookManager}.
   * @param priority shutdown hook priority",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,startupShutdownMessage,"org.apache.hadoop.util.StringUtils:startupShutdownMessage(java.lang.Class,java.lang.String[],org.slf4j.Logger)",805,828,"/**
* Logs startup/shutdown messages and registers UNIX signal handlers.
*@param clazz application class
*@param args command line arguments
*@param log logger instance for logging messages
*/","* Print a log message for starting up and shutting down
   * @param clazz the class of the server
   * @param args arguments
   * @param log the target log object",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,loadSSLConfiguration,org.apache.hadoop.http.HttpServer2$Builder:loadSSLConfiguration(),455,483,"/**
* Loads SSL configuration from properties.
* @throws IOException if any required property is missing
*/",* Load SSL properties from the SSL configuration.,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,loadSslConf,org.apache.hadoop.security.LdapGroupsMapping:loadSslConf(org.apache.hadoop.conf.Configuration),874,891,"/**
* Loads SSL/TLS configuration from provided Configuration object.
* @param sslConf Configuration containing SSL/TLS settings
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getPasswordForBindUser,org.apache.hadoop.security.LdapGroupsMapping:getPasswordForBindUser(java.lang.String),977,991,"/**
* Retrieves bind user password from credential providers or file.
* @param keyPrefix unique prefix for configuration keys
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,createTrustManagersFromConfiguration,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createTrustManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,java.lang.String,long)",105,150,"/**
* Configures and schedules a reloading X509 trust manager based on configuration.
* @param mode SSLFactory.Mode
* @param truststoreType type of the trust store
* @param truststoreLocation location of the trust store file
* @param storesReloadInterval interval to reload the trust store (in millis)
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,createKeyManagersFromConfiguration,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createKeyManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,long)",160,205,"/**
* Creates a ReloadingX509KeystoreManager from configuration properties.
* @param mode SSLFactory.Mode
* @param keystoreType type of keystore to use
* @param storesReloadInterval interval in milliseconds to reload the keystore (0 for no reload)
* @throws GeneralSecurityException if configuration is invalid
* @throws IOException on I/O errors
*/","* Implements logic of initializing the KeyManagers with the options
   * to reload keystores.
   * @param mode client or server
   * @param keystoreType The keystore type.
   * @param storesReloadInterval The interval to check if the keystore certificates
   *                             file has changed.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getZKAuths,org.apache.hadoop.util.curator.ZKCuratorManager:getZKAuths(org.apache.hadoop.conf.Configuration),120,123,"/**
* Retrieves ZooKeeper authentication information from Hadoop Configuration.
* @param conf Hadoop configuration object
* @return List of ZKAuthInfo objects or null if not found
*/","* Utility method to fetch ZK auth info from the configuration.
   *
   * @param conf configuration.
   * @throws java.io.IOException if the Zookeeper ACLs configuration file
   * cannot be read
   * @throws ZKUtil.BadAuthFormatException if the auth format is invalid
   * @return ZKAuthInfo List.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,initZK,org.apache.hadoop.ha.ZKFailoverController:initZK(),341,382,"/**
* Initializes the ZooKeeper client with configuration settings.
* @throws HadoopIllegalArgumentException if configuration is invalid
* @throws IOException if ZooKeeper connection fails
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),41,50,"/**
* Prepares encoding step for the given block group.
* @param blockGroup ECBlockGroup object
* @return ErasureCodingStep object or null if not created
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),48,59,"/**
* Prepares encoding step for user data using Erasure Coding.
* @param blockGroup ECBlockGroup object
* @return ErasureCodingStep object
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),41,50,"/**
* Prepares decoding step for EC block group.
* @param blockGroup EC Block Group object
* @return ErasureDecodingStep object or null if failed
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),49,65,"/**
* Prepares decoding step for erasure-coded data.
* @param blockGroup ECBlockGroup object containing input and output blocks
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/RuleBasedLdapGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroupsSet(java.lang.String),92,105,"/**
* Returns a set of user groups, optionally modifying group names to upper or lower case.
* @param user unique user identifier
*/",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getGroups,org.apache.hadoop.security.LdapGroupsMapping:getGroups(java.lang.String),357,360,"/**
 * Retrieves a list of group names associated with the specified user.
 * @param user username or ID to fetch groups for
 * @return list of group names, or an empty list if no groups found
 */","* Returns list of groups for a user.
   * 
   * The LdapCtx which underlies the DirContext object is not thread-safe, so
   * we need to block around this whole method. The caching infrastructure will
   * ensure that performance stays in an acceptable range.
   *
   * @param user get groups for this user
   * @return list of groups for a given user",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,main,org.apache.hadoop.conf.Configuration:main(java.lang.String[]),3945,3947,"/**
* Writes configuration to output stream in XML format.
* @throws Exception if an error occurs during writing
*/","For debugging.  List non-default properties to the terminal and exit.
   * @param args the argument to be parsed.
   * @throws Exception exception.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,start,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:start(),178,194,"/**
* Starts the metrics system with provided prefix.
* @throws NullPointerException if prefix is null
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,<init>,org.apache.hadoop.http.HttpServer2:<init>(org.apache.hadoop.http.HttpServer2$Builder),699,724,"/**
* Initializes a private HTTP server instance with specified configuration.
* @param b builder object containing server configuration details
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",118,125,"/**
* Initializes Token Manager with configuration and token kind.
* @param conf Hadoop Configuration object
* @param tokenKind type of delegation token (e.g. service or user)
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,saslConnect,org.apache.hadoop.security.SaslRpcClient:saslConnect(org.apache.hadoop.ipc.Client$IpcStreams),365,455,"/**
* Initiates SASL negotiation and returns the selected authentication method.
* @param ipcStreams IPC streams to use for communication
* @return AuthMethod enum value representing the selected authentication method
*/","* Do client side SASL authentication with server via the given IpcStreams.
   *
   * @param ipcStreams ipcStreams.
   * @return AuthMethod used to negotiate the connection
   * @throws IOException raised on errors performing I/O.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,<init>,org.apache.hadoop.fs.local.RawLocalFs:<init>(org.apache.hadoop.conf.Configuration),42,44,"/**
* Initializes local file system with given configuration.
* @param conf Hadoop configuration object
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,<init>,"org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",37,41,"/**
* Constructs a new uploader builder with specified file system and path.
* @param fileSystem file system to use
* @param path directory path for upload",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String)",111,113,"/**
* Initializes PathData using provided file system and path string.
* @param fs FileSystem instance
* @param pathString absolute path string
*/","* Looks up the file status for a path.  If the path
   * doesn't exist, then the status will be null
   * @param fs the FileSystem for the path
   * @param pathString a string for a path 
   * @throws IOException if anything goes wrong",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getDirectoryContents,org.apache.hadoop.fs.shell.PathData:getDirectoryContents(),274,285,"/**
* Retrieves contents of a directory at the specified path.
* @return Array of PathData objects representing file/directory entries
*/","* Returns a list of PathData objects of the items contained in the given
   * directory.
   * @return list of PathData objects for its children
   * @throws IOException if anything else goes wrong...",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,maybeIgnoreMissingDirectory,"org.apache.hadoop.fs.FileUtil:maybeIgnoreMissingDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.FileNotFoundException)",2094,2110,"/**
* Ignores FileNotFoundException for a missing directory and logs the issue.
* @param fs FileSystem instance
* @param path Path to the missing directory
* @param e FileNotFoundException to be ignored or rethrown
*/","* Method to call after a FNFE has been raised on a treewalk, so as to
   * decide whether to throw the exception (default), or, if the FS
   * supports inconsistent directory listings, to log and ignore it.
   * If this returns then the caller should ignore the failure and continue.
   * @param fs filesystem
   * @param path path
   * @param e exception caught
   * @throws FileNotFoundException the exception passed in, if rethrown.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.DelegateToFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",287,292,"/**
* Checks if a file system path supports a specific capability.
* @param path file system path to check
* @param capability capability to verify support for
* @return true if path supports the capability, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.http.AbstractHttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",122,131,"/**
* Checks if a path has a specific capability.
* @param path the path to check
* @param capability the capability to look for
* @return true if the path has the capability, false otherwise
*/","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.RawLocalFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1315,1332,"/**
* Checks if a path has specified capability.
* @param path the file system path
* @param capability the capability to check for (e.g. FS_APPEND)
* @return true if path has capability, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.viewfs.ViewFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1350,1371,"/**
* Checks if a path has a specific capability.
* @param path FS path to check
* @param capability desired capability
* @return true if the path has the capability, false otherwise
*/","* Reject the concat operation; forward the rest to the viewed FS.
   * @param path path to query the capability of.
   * @param capability string to query the stream support for.
   * @return the capability
   * @throws IOException if there is no resolved FS, or it raises an IOE.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.viewfs.ViewFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),1373,1389,"/**
* Resolves the enclosing root path for a given file system path.
* @param path file system path to resolve
* @return Path object representing the enclosing root or null if not found
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getEnclosingRoot(org.apache.hadoop.fs.Path),1941,1958,"/**
* Resolves the enclosing root path for a given path.
* @param path input path to resolve
* @return Path object representing the enclosing root or null if not found
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.FilterFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),735,738,"/**
* Retrieves enclosing root directory based on given file system path.
* @param path file system path to retrieve root from
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,fileSystem_getEnclosingRoot,"org.apache.hadoop.io.wrappedio.WrappedIO:fileSystem_getEnclosingRoot(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",202,204,"/**
* Retrieves the enclosing root of a given file system and path.
* @param fs instance of FileSystem to operate on
* @param path location within the file system
* @return Path representing the enclosing root, or null if not found
*/","* Return path of the enclosing root for a given path.
   * The enclosing root path is a common ancestor that should be used for temp and staging dirs
   * as well as within encryption zones and other restricted directories.
   * @param fs filesystem
   * @param path file path to find the enclosing root path for
   * @return a path to the enclosing root
   * @throws IOException early checks like failure to resolve path cause IO failures",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.FilterFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",740,753,"/**
* Checks if a path has a specific capability.
* @param path Filesystem path to check
* @param capability Capability to verify (e.g. FS_MULTIPART_UPLOADER)
* @return true if the path has the capability, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,rename,"org.apache.hadoop.fs.viewfs.ViewFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",694,760,"/**
* Renames a file or directory from one path to another.
* @param src source file/directory path
* @param dst destination file/directory path
* @return true if rename operation was successful, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,create,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",193,199,"/**
* Creates a new output stream with specified permissions and settings.
* @param f the path to create the output stream for
* @param permission file system permissions
* @param overwrite whether to overwrite existing files
* @param bufferSize buffer size for I/O operations
* @param replication data replication factor
* @param blockSize block size for data transfer
* @param progress progress monitor for I/O operations
* @return FSDataOutputStream object for writing data
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",201,208,"/**
* Creates a non-recursive FSDataOutputStream.
* @param f the file path
* @param permission file permissions
* @param flags create flags
* @param bufferSize buffer size
* @param replication replication factor
* @param blockSize block size
* @param progress progress monitor
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,delete,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",210,214,"/**
* Deletes a file or directory.
* @param f Path to delete
* @param recursive whether deletion should be recursive
* @return true if deletion was successful, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",223,228,"/**
* Wraps the call to super method with a custom file status.
* @param fs FileStatus object
* @param start starting position
* @param len length of data
* @return array of BlockLocation objects or null if not found
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),230,234,"/**
* Retrieves file checksum using parent class implementation.
* @param f file path to retrieve checksum for
* @return FileChecksum object or null if an error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileChecksum,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",236,240,"/**
* Retrieves file checksum using superclass implementation.
* @param f file path to retrieve checksum for
* @param length expected file size in bytes
* @return FileChecksum object or null if error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileStatus(org.apache.hadoop.fs.Path),242,246,"/**
* Retrieves file status by full path.
* @param f Path to file
* @return FileStatus object or null if not found
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),248,251,"/**
 * Returns the target of a link in a file path.
 * @param f input file path
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStatus(org.apache.hadoop.fs.Path),259,262,"/**
 * Returns file system status for the given path.
 * @param p file path to retrieve status for
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,listStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listStatus(org.apache.hadoop.fs.Path),264,268,"/**
* Lists status of files in the given directory.
* @param f file path to list status for
* @return array of FileStatus objects or null if an error occurs
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),270,274,"/**
* Lists located file status for the given path.
* @param f Path to list located file status for
* @return Iterator of LocatedFileStatus objects or null if not found
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",276,280,"/**
* Creates directory with specified permissions.
* @param f directory to create
* @param permission file system permissions for the directory
* @return true if created successfully, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,mkdirs,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path),282,285,"/**
* Creates directory and its parents recursively.
* @param f file path to create
* @return true if created, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,open,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:open(org.apache.hadoop.fs.Path,int)",287,291,"/**
* Opens an input stream to a file at the specified path.
* @param f file path
* @param bufferSize buffer size for reading data
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,append,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",293,297,"/**
* Wraps superclass's append method with fullPath conversion.
* @param f file path to append data to
* @param bufferSize buffer size for writing data
* @param progress callback for reporting progress (optional)
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,rename,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",299,304,"/**
* Renames a file by updating its path.
* @param src original file path
* @param dst new file path
* @return true if renaming was successful, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setOwner,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",306,311,"/**
* Sets file owner to specified user and group.
* @param f the file path
* @param username the user name
* @param groupname the group name
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setPermission,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",313,317,"/**
* Sets file system permissions on a file.
* @param f file path
* @param permission desired permissions
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setReplication,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",319,323,"/**
* Sets file replication level.
* @param f file path
* @param replication replication factor (short value)
* @return true if successful, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setTimes,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",325,329,"/**
* Sets file times (last modified and accessed).
* @param f     file path
* @param mtime last modified time in milliseconds
* @param atime last accessed time in milliseconds
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",331,335,"/**
* Calls superclass to modify ACL entries for a file or directory.
* @param path path to the file or directory
* @param aclSpec list of ACL modifications
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",337,341,"/**
* Removes ACL entries from specified file or directory.
* @param path Path to the file or directory
* @param aclSpec List of ACL entries to remove
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path),343,346,"/**
* Removes default ACL from specified file or directory.
* @param path Path to be processed
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeAcl,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAcl(org.apache.hadoop.fs.Path),348,351,"/**
* Removes ACL permissions from the specified file.
* @param path Path to the file that needs ACL removal
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setAcl,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",353,356,"/**
* Sets ACL entries for a file or directory.
* @param path Path to the target resource
* @param aclSpec List of AclEntry objects defining access control rules
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getAclStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getAclStatus(org.apache.hadoop.fs.Path),358,361,"/**
* Retrieves ACL status for the given file or directory.
* @param path filesystem path to retrieve ACL status for
* @return AclStatus object representing the ACL status, or null if not found
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",363,367,"/**
* Sets an extended attribute on a file.
* @param path the file path
* @param name the attribute name
* @param value the attribute value
* @param flag XAttrSetFlag indicating operation mode
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",369,372,"/**
* Retrieves extended attribute by name from the given file.
* @param path file path
* @param name name of the extended attribute to fetch
* @return byte array containing the attribute value or null if not found
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path),374,377,"/**
* Retrieves extended attributes from the specified file or directory.
* @param path file or directory to fetch XAttrs from
* @return map of attribute names to values, or null if not found
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",379,383,"/**
* Fetches extended attributes by name from a file.
* @param path file path
* @param names list of attribute names to fetch
* @return map of attribute names to their values, or null if not found
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,truncate,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:truncate(org.apache.hadoop.fs.Path,long)",385,388,"/**
* Truncates a file to a specified length.
* @param path file path
* @param newLength desired file size in bytes
* @return true if truncation was successful, false otherwise
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,listXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listXAttrs(org.apache.hadoop.fs.Path),390,393,"/**
* Lists extended attributes by path.
* @param path directory or file path to list xattrs from
* @return list of attribute names or empty list if none found
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",395,398,"/**
* Removes an extended attribute from a file by its ID.
* @param name unique extended attribute identifier
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",400,403,"/**
* Creates a snapshot of the given file or directory.
* @param path file system path to snapshot
* @param name unique snapshot identifier
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",405,409,"/**
* Renames a snapshot by ID.
* @param path Path to the snapshot
* @param snapshotOldName current name of the snapshot
* @param snapshotNewName new name for the snapshot
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",411,415,"/**
* Deletes a snapshot by its name in the specified directory.
* @param snapshotDir directory containing the snapshot to delete
* @param snapshotName unique name of the snapshot to delete
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,resolvePath,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:resolvePath(org.apache.hadoop.fs.Path),417,420,"/**
* Resolves a path by delegating to superclass with full path.
* @param p input path to be resolved
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getContentSummary,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getContentSummary(org.apache.hadoop.fs.Path),422,425,"/**
* Retrieves content summary from file system.
* @param f Path to file or directory
* @return ContentSummary object or null if not found
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getQuotaUsage,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path),427,430,"/**
* Retrieves quota usage information for a specified file path.
* @param f Path to the file or directory for which quota usage is needed
* @return QuotaUsage object representing the quota usage details, or null if not available
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),439,442,"/**
* Returns default block size based on full path of file.
* @param f file Path object
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),449,452,"/**
 * Returns default replication value based on file system path.
 * @param f Path object representing file system location
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path),464,467,"/**
* Retrieves storage policy for a given source path.
* @param src source path
* @return BlockStoragePolicySpi object or null if not found
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path),469,472,"/**
* Satisfies storage policy by delegating to superclass with updated path. 
* @param src source Path object 
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",474,477,"/**
* Sets storage policy for file or directory at specified path.
* @param src Path to file or directory
* @param policyName Name of storage policy to apply
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path),479,482,"/**
 * Unsets storage policy on the specified file.
 * @param src Path to the file
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,createFile,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createFile(org.apache.hadoop.fs.Path),484,487,"/**
* Creates an output stream builder for the specified file.
* @param path filesystem path to create the file at
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFile,org.apache.hadoop.fs.FileSystem:openFile(org.apache.hadoop.fs.Path),4761,4765,"/**
* Opens a file at specified Path and returns a DataInputStreamBuilder.
* @param path location of the file to be opened
*/","* Open a file for reading through a builder API.
   * Ultimately calls {@link #open(Path, int)} unless a subclass
   * executes the open command differently.
   *
   * The semantics of this call are therefore the same as that of
   * {@link #open(Path, int)} with one special point: it is in
   * {@code FSDataInputStreamBuilder.build()} in which the open operation
   * takes place -it is there where all preconditions to the operation
   * are checked.
   * @param path file path
   * @return a FSDataInputStreamBuilder object to build the input stream
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFile,org.apache.hadoop.fs.FileSystem:openFile(org.apache.hadoop.fs.PathHandle),4780,4785,"/**
* Opens a file using the provided PathHandle.
* @param pathHandle unique identifier for the file to be opened
* @return FutureDataInputStreamBuilder instance or null if failed
*/","* Open a file for reading through a builder API.
   * Ultimately calls {@link #open(PathHandle, int)} unless a subclass
   * executes the open command differently.
   *
   * If PathHandles are unsupported, this may fail in the
   * {@code FSDataInputStreamBuilder.build()}  command,
   * rather than in this {@code openFile()} operation.
   * @param pathHandle path handle.
   * @return a FSDataInputStreamBuilder object to build the input stream
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,locateKeystore,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:locateKeystore(),145,176,"/**
* Locates and loads the keystore.
* @throws IOException if keystore cannot be loaded or created
*/","* Open up and initialize the keyStore.
   * @throws IOException If there is a problem reading the password file
   * or a problem reading the keystore.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,checkTFileDataIndex,org.apache.hadoop.io.file.tfile.TFile$Reader:checkTFileDataIndex(),882,893,"/**
* Initializes or updates the TFile index if not already loaded.
*/","* Lazily loading the TFile index.
     * 
     * @throws IOException",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getMetaBlock,org.apache.hadoop.io.file.tfile.TFile$Reader:getMetaBlock(java.lang.String),967,970,"/**
* Retrieves a meta block from the BCF reader by name.
* @param name name of the meta block
* @return DataInputStream representing the meta block or null if not found
*/","* Stream access to a meta block.``
     * 
     * @param name
     *          The name of the meta block.
     * @return The input stream.
     * @throws IOException
     *           on I/O error.
     * @throws MetaBlockDoesNotExist
     *           If the meta block with the name does not exist.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.conf.Configuration)",617,645,"/**
* Initializes a Reader object from an FSDataInputStream and configuration.
* @param fin input stream to the file
* @param fileLength total length of the file in bytes
* @param conf configuration object
*/","* Constructor
     * 
     * @param fin
     *          FS input stream.
     * @param fileLength
     *          Length of the corresponding file
     * @throws IOException",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getBlockReader,org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockReader(int),2035,2037,"/**
* Retrieves a block reader instance by index.
* @param blockIndex unique block identifier
* @return BlockReader object or null if invalid index
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareMetaBlock,"org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)",595,606,"/**
* Prepares a meta block for writing.
* @param name unique meta block identifier
* @param compressName compressed meta block name
* @return DataOutputStream object for writing the meta block
*/","* Obtain an output stream for creating a meta block. This function may not
     * be called when there is a key append stream or value append stream
     * active. No more key-value insertion is allowed after a meta data block
     * has been added to TFile.
     * 
     * @param name
     *          Name of the meta block.
     * @param compressName
     *          Name of the compression algorithm to be used. Must be one of the
     *          strings returned by
     *          {@link TFile#getSupportedCompressionAlgorithms()}.
     * @return A DataOutputStream that can be used to write Meta Block data.
     *         Closing the stream would signal the ending of the block.
     * @throws IOException raised on errors performing I/O.
     * @throws MetaBlockAlreadyExists
     *           the Meta Block with the same name already exists.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Writer:close(),300,343,"/**
* Closes the TFile instance and writes out meta and index blocks.
* @throws IOException if an I/O error occurs
*/","* Close the Writer. Resources will be released regardless of the exceptions
     * being thrown. Future close calls will have no effect.
     * 
     * The underlying FSDataOutputStream is not closed.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareMetaBlock,org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String),623,632,"/**
* Prepares a meta block with the given name.
* @param name unique identifier for the meta block
* @return DataOutputStream object to write the meta block, or null if already exists
*/","* Obtain an output stream for creating a meta block. This function may not
     * be called when there is a key append stream or value append stream
     * active. No more key-value insertion is allowed after a meta data block
     * has been added to TFile. Data will be compressed using the default
     * compressor as defined in Writer's constructor.
     * 
     * @param name
     *          Name of the meta block.
     * @return A DataOutputStream that can be used to write Meta Block data.
     *         Closing the stream would signal the ending of the block.
     * @throws IOException raised on errors performing I/O.
     * @throws MetaBlockAlreadyExists
     *           the Meta Block with the same name already exists.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareAppendKey,org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendKey(int),527,537,"/**
* Prepares a new append key with the specified length.
* @param length key length in bytes
* @return DataOutputStream to write key data, or null on failure
*/","* Obtain an output stream for writing a key into TFile. This may only be
     * called when there is no active Key appending stream or value appending
     * stream.
     * 
     * @param length
     *          The expected length of the key. If length of the key is not
     *          known, set length = -1. Otherwise, the application must write
     *          exactly as many bytes as specified here before calling close on
     *          the returned output stream.
     * @return The key appending output stream.
     * @throws IOException raised on errors performing I/O.
     *",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),459,462,"/**
* Retrieves server defaults from the file system.
* @param f Path to the configuration file
* @return FsServerDefaults object or null if not found
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(java.io.File,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)",517,557,"/**
* Recursively copies a file or directory from the source to the destination.
* @param src source file/directory
* @param dstFS target file system
* @param dst destination path
* @param deleteSource whether to delete the source after copying
* @param conf configuration object
* @return true if successful, false otherwise
*/","* Copy local files to a FileSystem.
   *
   * @param src src.
   * @param dstFS dstFs.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return true if the operation succeeded.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,innerComplete,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerComplete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)",195,243,"/**
* Completes an upload by merging parts and creating a final file.
* @param multipartUploadId unique upload identifier
* @return PathHandle object for the completed file
*/","* The upload complete operation.
   * @param multipartUploadId the ID of the upload
   * @param filePath path
   * @param handleMap map of handles
   * @return the path handle
   * @throws IOException failure",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,touch,org.apache.hadoop.fs.shell.TouchCommands$Touch:touch(org.apache.hadoop.fs.shell.PathData),162,175,"/**
* Updates or creates file at specified path with optional timestamp.
* @param item PathData object containing file metadata
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,touchz,org.apache.hadoop.fs.shell.TouchCommands$Touchz:touchz(org.apache.hadoop.fs.shell.PathData),88,90,"/**
* Creates and closes a file at specified PathData location.
* @param item PathData object containing path to create
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",742,749,"/**
* Creates a new output stream for the specified file with custom permissions.
* @param fs FileSystem instance
* @param file Path to the file to be created
* @param permission Custom FsPermission settings
* @return FSDataOutputStream object
*/","* Create a file with the provided permission.
   *
   * The permission of the file is set to be the provided permission as in
   * setPermission, not permission{@literal &~}umask
   *
   * The HDFS implementation is implemented using two RPCs.
   * It is understood that it is inefficient,
   * but the implementation is thread-safe. The other option is to change the
   * value of umask in configuration to be 0, but it is not thread-safe.
   *
   * @param fs FileSystem
   * @param file the name of the file to be created
   * @param permission the permission of the file
   * @return an output stream
   * @throws IOException IO failure",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)",1209,1215,"/**
* Creates a Writer instance for the given file system and configuration.
* @param fs File system instance
* @param conf Configuration object
* @param name Path to the output file
* @param keyClass Class of the key data type
* @param valClass Class of the value data type
*/","* Create the named file.
     * @deprecated Use 
     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} 
     *   instead.
     * @param fs input filesystem.
     * @param conf input configuration.
     * @param name input name.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @throws IOException raised on errors performing I/O.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,rollLogDir,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:rollLogDir(),661,673,"/**
* Rolls the log directory by creating a new log file with current host name.
* @throws IOException if I/O error occurs during file system operation
*/","* Create a new directory based on the current interval and a new log file in
   * that directory.
   *
   * @throws IOException thrown if an error occurs while creating the
   * new directory or new log file",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,readIndex,org.apache.hadoop.io.MapFile$Reader:readIndex(),577,631,"/**
* Reads the index into memory, populating positions array and keys list.
* @throws IOException if an error occurs while reading the index
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,next,"org.apache.hadoop.io.MapFile$Reader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",811,814,"/**
* Advances to the next entry in the underlying data store.
* @param key the key to look up
* @param val the value to retrieve (not used by this method)
*/","* Read the next key/value pair in the map into <code>key</code> and
     * <code>val</code>.  Returns true if such a pair exists and false when at
     * the end of the map.
     *
     * @param key WritableComparable.
     * @param val Writable.
     * @return if such a pair exists true,not false.
     * @throws IOException raised on errors performing I/O.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",106,119,"/**
* Creates a proxy for the specified protocol and client version.
* @param protocol protocol class
* @param clientVersion client version
* @return ProtocolProxy object
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getProxy,"org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",348,367,"/**
* Creates a proxy for the specified protocol.
* @param protocol target interface
* @return ProtocolProxy instance or throws IOException on failure
*/","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   * @param <T> Generics Type.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param ticket input ticket.
   * @param conf input configuration.
   * @param factory input factory.
   * @param rpcTimeout input rpcTimeout.
   * @param connectionRetryPolicy input connectionRetryPolicy.
   * @param fallbackToSimpleAuth input fallbackToSimpleAuth.
   * @param alignmentContext input alignmentContext.
   * @return ProtocolProxy.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",113,126,"/**
* Creates a proxy for the specified protocol.
* @param protocol target protocol class
* @param clientVersion client version number
* @param addr server address
* @param ticket user authentication token
* @param conf configuration settings
* @param factory socket creation factory
* @param rpcTimeout RPC timeout value
* @param connectionRetryPolicy retry policy for connections
* @param fallbackToSimpleAuth flag to fall back to simple auth
* @param alignmentContext alignment context
* @return a ProtocolProxy instance or null if failed
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processArguments,org.apache.hadoop.fs.shell.Delete$Expunge:processArguments(java.util.LinkedList),244,273,"/**
* Processes file system arguments and trashes child file systems.
* @param args list of PathData objects
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getTrash,org.apache.hadoop.fs.FsShell:getTrash(),86,91,"/**
* Retrieves and initializes the Trash object.
* @return initialized Trash object or null if not already created
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getUserToGroupsMappingService,org.apache.hadoop.security.Groups:getUserToGroupsMappingService(),462,464,"/**
* Retrieves Groups service instance with default configuration.
* @return Groups service object
*/","* Get the groups being used to map user-to-groups.
   * @return the groups being used to map user-to-groups.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,initialize,"org.apache.hadoop.security.UserGroupInformation:initialize(org.apache.hadoop.conf.Configuration,boolean)",309,354,"/**
* Initializes configuration and sets up authentication, name rules, and metrics.
* @param conf Hadoop Configuration object
* @param overrideNameRules whether to override default name rules
*/","* Initialize UGI and related classes.
   * @param conf the configuration to use",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,<init>,org.apache.hadoop.security.authorize.AccessControlList:<init>(),73,74,"/**
 * Initializes an empty Access Control List (ACL) instance.
 */",* This constructor exists primarily for AccessControlList to be Writable.,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,<init>,org.apache.hadoop.security.authorize.AccessControlList:<init>(java.lang.String),85,87,"/**
* Initializes an AccessControlList from a string representation.
* @param aclString string containing ACL permissions and identifier
*/","* Construct a new ACL from a String representation of the same.
   * 
   * The String is a a comma separated list of users and groups.
   * The user list comes first and is separated by a space followed 
   * by the group list. For e.g. ""user1,user2 group1,group2""
   * 
   * @param aclString String representation of the ACL",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,<init>,"org.apache.hadoop.security.authorize.AccessControlList:<init>(java.lang.String,java.lang.String)",97,99,"/**
 * Initializes an Access Control List with specified user and group names.
 * @param users comma-separated list of usernames
 * @param groups comma-separated list of group names
 */","* Construct a new ACL from String representation of users and groups
   * 
   * The arguments are comma separated lists
   * 
   * @param users comma separated list of users
   * @param groups comma separated list of groups",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,refreshSuperUserGroupsConfiguration,org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(),58,61,"/**
* Refreshes super user groups configuration from server-side settings.
*/",* refresh Impersonation rules,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,coreServiceLaunch,"org.apache.hadoop.service.launcher.ServiceLauncher:coreServiceLaunch(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean)",571,647,"/**
* Launches a core service instance with the specified configuration and arguments.
* @param conf Service configuration
* @param instance Existing service instance or null to create a new one
* @param processedArgs CLI arguments for the service
* @param addShutdownHook Whether to register a shutdown hook for the service
* @param execute Whether to start and run the service
* @return Exit code of the service execution, 0 on success",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,build,org.apache.hadoop.http.HttpServer2$Builder:build(),485,564,"/**
* Builds an HttpServer2 instance with the specified configuration.
* @throws IOException if an I/O error occurs during server creation
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,initializeBindUsers,org.apache.hadoop.security.LdapGroupsMapping:initializeBindUsers(),950,975,"/**
* Initializes bind users configuration and populates the bind user iterator.
* @throws RuntimeException if bind username or password is not configured
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,init,org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:init(org.apache.hadoop.security.ssl.SSLFactory$Mode),252,300,"/**
* Initializes SSL configuration for the specified mode.
* @param mode SSLFactory.Mode (e.g. CLIENT, SERVER)
* @throws IOException, GeneralSecurityException if initialization fails
*/","* Initializes the keystores of the factory.
   *
   * @param mode if the keystores are to be used in client or server mode.
   * @throws IOException thrown if the keystores could not be initialized due
   * to an IO error.
   * @throws GeneralSecurityException thrown if the keystores could not be
   * initialized due to a security error.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,start,"org.apache.hadoop.util.curator.ZKCuratorManager:start(java.util.List,boolean)",149,193,"/**
* Establishes a ZooKeeper connection with the specified ensemble and configuration.
* @param authInfos list of authentication information
* @param sslEnabled whether to enable SSL encryption
*/","* Start the connection to the ZooKeeper ensemble.
   *
   * @param authInfos  List of authentication keys.
   * @param sslEnabled If the connection should be SSL/TLS encrypted.
   * @throws IOException            If the connection cannot be started.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doRun,org.apache.hadoop.ha.ZKFailoverController:doRun(java.lang.String[]),202,270,"/**
* Runs the failover controller with optional formatting of ZooKeeper.
* @param args command-line arguments
* @return error code or 0 on success
*/",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/RuleBasedLdapGroupsMapping.java,getGroups,org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroups(java.lang.String),76,90,"/**
* Transform user groups by applying specified rule.
* @param user username to fetch groups for
* @return List of group names or empty list if none found
*/","* Returns list of groups for a user.
     * This calls {@link LdapGroupsMapping}'s getGroups and applies the
     * configured rules on group names before returning.
     *
     * @param user get groups for this user
     * @return list of groups for a given user",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,init,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:init(java.lang.String),148,176,"/**
* Initializes the MetricsSystem instance with a specified prefix.
* @param prefix unique identifier for metrics
*/","* Initialized the metrics system with a prefix.
   * @param prefix  the system will look for configs with the prefix
   * @return the metrics system object itself",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,initTokenManager,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:initTokenManager(java.util.Properties),148,163,"/**
* Initializes a TokenManager instance from provided Properties.
* @param config Configuration properties
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setupSaslConnection,org.apache.hadoop.ipc.Client$Connection:setupSaslConnection(org.apache.hadoop.ipc.Client$IpcStreams),571,579,"/**
* Establishes a SASL connection using the provided IpcStreams.
* @param streams IpcStreams object for communication
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/LocalFs.java,<init>,org.apache.hadoop.fs.local.LocalFs:<init>(org.apache.hadoop.conf.Configuration),36,38,"/**
* Initializes local file system with given configuration.
* @param conf application configuration
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,suffix,org.apache.hadoop.fs.shell.PathData:suffix(java.lang.String),241,243,"/**
* Constructs a new PathData instance with the specified file system and path suffix.
* @param extension file name extension to append
*/","* Returns a new PathData with the given extension.
   * @param extension for the suffix
   * @return PathData
   * @throws IOException shouldn't happen",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getPathDataForChild,org.apache.hadoop.fs.shell.PathData:getPathDataForChild(org.apache.hadoop.fs.shell.PathData),307,310,"/**
* Retrieves child PathData with updated path information.
* @param child the child PathData to fetch
* @return updated PathData object or null if not found
*/","* Creates a new object for a child entry in this directory
   * @param child the basename will be appended to this object's path
   * @return PathData for the child
   * @throws IOException if this object does not exist or is not a directory",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,recursePath,org.apache.hadoop.fs.shell.Command:recursePath(org.apache.hadoop.fs.shell.PathData),449,466,"/**
* Recursively processes directory contents.
* @param item PathData object containing directory information
*/","*  Gets the directory listing for a path and invokes
   *  {@link #processPaths(PathData, PathData...)}
   *  @param item {@link PathData} for directory to recurse into
   *  @throws IOException if anything goes wrong...",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.http.HttpsFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",122,131,"/**
* Checks if a path has a specific capability.
* @param path Path object to check
* @param capability Capability name (e.g. FS_READ_ONLY_CONNECTOR)
* @return True if the path has the specified capability, false otherwise
*/","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.http.HttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",122,131,"/**
* Checks if a given path supports a specific capability.
* @param path the file system path to check
* @param capability the capability to verify (e.g. FS_READ_ONLY_CONNECTOR)
* @return true if the path supports the capability, false otherwise
*/","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",495,499,"/**
* Checks if a given file system path has a specific capability.
* @param path file system path to check
* @param capability capability name to verify
* @return true if the path has the specified capability, false otherwise
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.ChecksumFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1128,1140,"/**
* Determines if a path has a specific capability.
* @param path the file system path to check
* @param capability the capability to verify (e.g. FS_APPEND)
* @return true if the path has the capability, false otherwise
*/","* Disable those operations which the checksummed FS blocks.
   * {@inheritDoc}",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,delete,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete(org.apache.hadoop.fs.Path),217,221,"/**
* Deletes a file at the specified location.
* @param f Path to the file to be deleted
* @return true if deletion was successful, false otherwise
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,updateFileStatus,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:updateFileStatus(org.apache.hadoop.fs.Path),131,136,"/**
* Updates file status by fetching from the filesystem.
* @param f Path to the file for which status is updated
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,listStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:listStatus(org.apache.hadoop.fs.Path),804,845,"/**
* Lists file statuses for a given path across all MRNfly nodes.
* @param f the path to list statuses for
* @return array of FileStatus objects or throws IOException if not found
*/","* Returns the closest non-failing destination's result.
   *
   * @param f given path
   * @return array of file statuses according to nfly modes
   * @throws FileNotFoundException
   * @throws IOException",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.NflyFSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",866,873,"/**
* Creates directory with specified permissions on all FlyNodes.
* @param f the path to create
* @param permission desired file system permissions
* @return true if successful on all nodes, false otherwise
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,rename,"org.apache.hadoop.fs.viewfs.NflyFSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",739,765,"/**
* Renames a file by ID on all nodes.
* @param src source file path
* @param dst destination file path
* @return true if successful, false otherwise
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize(),434,437,"/**
* Returns default block size based on root path configuration.
* @return Default block size in bytes.",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication(),444,447,"/**
 * Returns default replication level based on root path.
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",462,504,"/**
* Recursively copies a file system tree from source to destination.
* @param deleteSource whether to delete the source after copy
* @return true on success, false otherwise
*/","* Copy a file/directory tree within/between filesystems.
   * <p>
   * returns true if the operation succeeded. When deleteSource is true,
   * this means ""after the copy, delete(source) returned true""
   * If the destination is a directory, and mkdirs (dest) fails,
   * the operation will return false rather than raise any exception.
   * </p>
   * The overwrite flag is about overwriting files; it has no effect about
   * handing an attempt to copy a file atop a directory (expect an IOException),
   * or a directory over a path which contains a file (mkdir will fail, so
   * ""false"").
   * <p>
   * The operation is recursive, and the deleteSource operation takes place
   * as each subdirectory is copied. Therefore, if an operation fails partway
   * through, the source tree may be partially deleted.
   * </p>
   * @param srcFS source filesystem
   * @param srcStatus status of source
   * @param dstFS destination filesystem
   * @param dst path of source
   * @param deleteSource delete the source?
   * @param overwrite overwrite files at destination?
   * @param conf configuration to use when opening files
   * @return true if the operation succeeded.
   * @throws IOException failure",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.io.File,boolean,org.apache.hadoop.conf.Configuration)",578,605,"/**
* Recursively copies a file system subtree from source to destination.
* @param srcFS source file system
* @param srcStatus status of the source file or directory
* @param dst destination file
* @param deleteSource whether to delete the source after copying
* @return true if successful, false otherwise
*/",Copy FileSystem files to local files.,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,openFile,org.apache.hadoop.fs.shell.PathData:openFile(java.lang.String),632,639,"/**
* Opens a file with specified read policy.
* @param policy file open read policy
*/","* Open a file.
   * @param policy fadvise policy.
   * @return an input stream
   * @throws IOException failure",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFile,org.apache.hadoop.fs.FilterFileSystem:openFile(org.apache.hadoop.fs.Path),709,713,"/**
* Opens a file at the specified path and returns a builder for its data input stream.
* @param path location of the file to be opened
* @return DataInputStreamBuilder instance or null if not supported
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,openFile,"org.apache.hadoop.io.SequenceFile$Reader:openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int,long)",2002,2012,"/**
* Opens a file for sequential read access with specified buffer size.
* @param fs the Hadoop FileSystem instance
* @param file the Path to the file
* @param bufferSize the buffer size in bytes
* @param length the file length (optional)
* @return an FSDataInputStream object or null if not found
*/","* Override this method to specialize the type of
     * {@link FSDataInputStream} returned.
     * @param fs The file system used to open the file.
     * @param file The file being read.
     * @param bufferSize The buffer size used to read the file.
     * @param length The length being read if it is {@literal >=} 0.
     *               Otherwise, the length is not available.
     * @return The opened stream.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,load,"org.apache.hadoop.util.JsonSerialization:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileStatus)",264,283,"/**
* Loads JSON data from a specified path in the file system.
* @param fs FileSystem instance
* @param path Path to load JSON data from
* @param status Optional FileStatus object (null if unknown)
* @return loaded T object or throws IOException/PathIOException
*/","* Load from a Hadoop filesystem.
   * If a file status is supplied, it's passed in to the openFile()
   * call so that FS implementations can optimize their opening.
   * @param fs filesystem
   * @param path path
   * @param status status of the file to open.
   * @return a loaded object
   * @throws PathIOException JSON parse problem
   * @throws EOFException file status references an empty file
   * @throws IOException IO problems",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFile,org.apache.hadoop.fs.FilterFileSystem:openFile(org.apache.hadoop.fs.PathHandle),715,719,"/**
* Opens a file based on the provided handle.
* @param pathHandle unique identifier of the file to be opened
* @return FutureDataInputStreamBuilder instance or null if not found
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getFirstKey,org.apache.hadoop.io.file.tfile.TFile$Reader:getFirstKey(),901,904,"/**
* Retrieves the first key from the TFile index.
* @throws IOException if an I/O error occurs
*/","* Get the first key in the TFile.
     * 
     * @return The first key in the TFile.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLastKey,org.apache.hadoop.io.file.tfile.TFile$Reader:getLastKey(),912,915,"/**
* Retrieves the last key from the TFile index.
* @throws IOException if an I/O error occurs
*/","* Get the last key in the TFile.
     * 
     * @return The last key in the TFile.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getBlockContainsKey,"org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockContainsKey(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",985,995,"/**
* Locates the block containing a given key.
* @param key RawComparable value to search for
* @param greater whether to find the smallest or largest matching block
* @return Location of the containing block or end if not found
*/","* if greater is true then returns the beginning location of the block
     * containing the key strictly greater than input key. if greater is false
     * then returns the beginning location of the block greater than equal to
     * the input key
     * 
     * @param key
     *          the input key
     * @param greater
     *          boolean flag
     * @return
     * @throws IOException",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLocationByRecordNum,org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationByRecordNum(long),997,1000,"/**
* Retrieves location data by record number from indexed T-File.
* @param recNum unique record identifier
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNumByLocation,org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumByLocation(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),1002,1005,"/**
* Retrieves record number by location from TFile index.
* @param location Location object to search for
* @return Record number or -1 if not found, or throws IOException if error occurs
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKeyNear,org.apache.hadoop.io.file.tfile.TFile$Reader:getKeyNear(long),1063,1068,"/**
* Retrieves a key near the specified offset.
* @param offset position to search for a nearby key
* @return RawComparable key object or null if not found
*/","* Get a sample key that is within a block whose starting offset is greater
     * than or equal to the specified offset.
     * 
     * @param offset
     *          The file offset.
     * @return the key that fits the requirement; or null if no such key exists
     *         (which could happen if the offset is close to the end of the
     *         TFile).
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.conf.Configuration)",802,818,"/**
* Initializes Reader with FSDataInputStream, file length, and configuration.
* @param fsdis input stream to read from
* @param fileLength total size of the file
* @param conf Hadoop configuration object
*/","* Constructor
     * 
     * @param fsdis
     *          FS input stream of the TFile.
     * @param fileLength
     *          The length of TFile. This is required because we have no easy
     *          way of knowing the actual size of the input file through the
     *          File input stream.
     * @param conf configuration.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,initBlock,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:initBlock(int),1548,1559,"/**
* Initializes block processing by closing previous block and fetching new one.
* @param blockIndex index of the block to process
*/","* Load a compressed block for reading. Expecting blockIndex is valid.
       * 
       * @throws IOException",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,append,"org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],int,int,byte[],int,int)",380,413,"/**
* Appends key-value pair to storage. 
* @param key byte array containing key data
* @param koff offset into key buffer
* @param klen length of key data
* @param value byte array containing value data
* @param voff offset into value buffer
* @param vlen length of value data
*/","* Adding a new key-value pair to TFile.
     * 
     * @param key
     *          buffer for key.
     * @param koff
     *          offset in key buffer.
     * @param klen
     *          length of key.
     * @param value
     *          buffer for value.
     * @param voff
     *          offset in value buffer.
     * @param vlen
     *          length of value.
     * @throws IOException
     *           Upon IO errors.
     *           <p>
     *           If an exception is thrown, the TFile will be in an inconsistent
     *           state. The only legitimate call after that would be close",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults(),454,457,"/**
* Retrieves server defaults by resolving root path.
* @return FsServerDefaults object
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processPath,org.apache.hadoop.fs.shell.TouchCommands$Touch:processPath(org.apache.hadoop.fs.shell.PathData),148,151,"/**
 * Processes a file path by calling the touch operation.
 * @param item PathData object containing path information
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processNonexistentPath,org.apache.hadoop.fs.shell.TouchCommands$Touch:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),153,160,"/**
* Handles nonexistent paths by checking parent existence and throwing exception or performing touch operation.
* @param item PathData object containing path information
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processPath,org.apache.hadoop.fs.shell.TouchCommands$Touchz:processPath(org.apache.hadoop.fs.shell.PathData),67,77,"/**
* Validates and processes a path item, throwing exceptions for invalid directory or non-zero length files.
*@param item PathData object to process
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processNonexistentPath,org.apache.hadoop.fs.shell.TouchCommands$Touchz:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),79,86,"/**
* Handles nonexistent paths by throwing a PathNotFoundException.
* @param item PathData object containing path information
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,getOutputStreamForKeystore,org.apache.hadoop.security.alias.KeyStoreProvider:getOutputStreamForKeystore(),52,56,"/**
* Returns an output stream for writing to the keystore.
* @return Output stream for writing to the keystore
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,writeToNew,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:writeToNew(org.apache.hadoop.fs.Path),607,620,"/**
* Saves key store to a file at the specified path.
* @param newPath target file system location
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,midKey,org.apache.hadoop.io.MapFile$Reader:midKey(),649,657,"/**
* Returns the middle key from the current set of keys.
* @return WritableComparable object representing the mid-key or null for empty sets
*/","* Get the key at approximately the middle of the file. Or null if the
     *  file is empty.
     *
     * @throws IOException raised on errors performing I/O.
     * @return WritableComparable.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,finalKey,org.apache.hadoop.io.MapFile$Reader:finalKey(org.apache.hadoop.io.WritableComparable),665,681,"/**
* Scans data to final key, seeking to last indexed entry if available.
* @param key WritableComparable key object
*/","* Reads the final key from the file.
     *
     * @param key key to read into
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,seekInternal,"org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable,boolean)",721,780,"/**
* Performs a seek operation to find the given WritableComparable key.
* @param key the key to search for
* @param before whether to search for keys before or at the given key
* @return an integer indicating the position of the key, or 1 if not found
*/","* Positions the reader at the named key, or if none such exists, at the
     * key that falls just before or just after dependent on how the
     * <code>before</code> parameter is set.
     * 
     * @param before - IF true, and <code>key</code> does not exist, position
     * file at entry that falls just before <code>key</code>.  Otherwise,
     * position file at record that sorts just after.
     * @return  0   - exact match found
     *          < 0 - positioned at next record
     *          1   - no more records in file",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,mergePass,org.apache.hadoop.io.MapFile$Merger:mergePass(),1101,1146,"/**
* Merges input streams by key, writing to output stream.
* @throws IOException if merge process fails
*/","* Merge all input files to output map file.<br>
     * 1. Read first key/value from all input files to keys/values array. <br>
     * 2. Select the least key and corresponding value. <br>
     * 3. Write the selected key and value to output file. <br>
     * 4. Replace the already written key/value in keys/values arrays with the
     * next key/value from the selected input <br>
     * 5. Repeat step 2-4 till all keys are read. <br>",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",97,104,"/**
* Retrieves a protocol proxy for the given class and configuration.
* @param protocol target protocol class
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getProxy,"org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",299,307,"/**
* Creates a proxy instance for the specified protocol and client version.
* @param protocol target protocol class
* @param clientVersion client version identifier
* @param addr server address
* @param ticket user authentication information
* @param conf configuration settings
* @param factory socket creation factory
* @param rpcTimeout RPC timeout setting
* @param connectionRetryPolicy retry policy for connections
*/","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   * @param <T> Generics Type T
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param ticket input ticket.
   * @param conf input configuration.
   * @param factory input factory.
   * @param rpcTimeout input rpcTimeout.
   * @param connectionRetryPolicy input connectionRetryPolicy.
   * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getProxy,"org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",322,330,"/**
* Returns a ProtocolProxy instance for the specified protocol and client version.
* @param protocol the protocol class
* @param clientVersion the client version
* @param connId connection identifier
* @param conf configuration object
* @param factory socket factory
* @param alignmentContext alignment context
*/","* Construct a client-side proxy object with a ConnectionId.
   *
   * @param <T> Generics Type T.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param connId input ConnectionId.
   * @param conf input Configuration.
   * @param factory input factory.
   * @param alignmentContext Alignment context
   * @throws IOException raised on errors performing I/O.
   * @return ProtocolProxy.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",93,101,"/**
* Creates a protocol proxy with default retry policy.
* @param protocol the protocol class
* @param clientVersion the client version
* @param addr the server address
* @param ticket user identity
* @param conf configuration
* @param factory socket factory
* @param rpcTimeout RPC timeout
* @param connectionRetryPolicy connection retry policy
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getCurrentTrashDir,org.apache.hadoop.fs.FsShell:getCurrentTrashDir(),125,127,"/**
 * Returns the current trash directory path.
 */","* Returns the Trash object associated with this shell.
   * @return Path to the trash
   * @throws IOException upon error",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getCurrentTrashDir,org.apache.hadoop.fs.FsShell:getCurrentTrashDir(org.apache.hadoop.fs.Path),135,137,"/**
 * Retrieves the current trash directory based on the provided file system path.
 * @param path the file system path to determine the trash directory for
 */","* Returns the current trash location for the path specified
   * @param path to be deleted
   * @return path to the trash
   * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,ensureInitialized,org.apache.hadoop.security.UserGroupInformation:ensureInitialized(),295,303,"/**
* Ensures the user group information is initialized.
* @throws no return value, but initializes on first call
*/","* A method to initialize the fields that depend on a configuration.
   * Must be called before useKerberos or groups is used.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setConfiguration,org.apache.hadoop.security.UserGroupInformation:setConfiguration(org.apache.hadoop.conf.Configuration),362,366,"/**
* Sets configuration using provided Configuration object.
* @param conf Configuration settings to apply.","* Set the static configuration for UGI.
   * In particular, set the security authentication mechanism and the
   * group look up service.
   * @param conf the configuration to use",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ServiceAuthorizationManager.java,refreshWithLoadedConfiguration,"org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refreshWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",152,200,"/**
* Refreshes security configuration with loaded values.
* @param conf Configuration object containing authorization settings
* @param provider PolicyProvider instance for service retrieval
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,init,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:init(java.lang.String),68,101,"/**
* Initializes proxy user settings from configuration.
* @param configurationPrefix prefix to search for ACL and host configurations
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,getSip,org.apache.hadoop.security.authorize.ProxyUsers:getSip(),116,124,"/**
* Retrieves or initializes the SIP ImpersonationProvider instance.
* @return Sip ImpersonationProvider object, may be refreshed in a race situation
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,launchService,"org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean)",485,539,"/**
* Launches a service with the given configuration and parameters.
* @param conf Configuration object
* @param instance Service instance
* @param processedArgs Processed arguments list
* @param addShutdownHook Flag to add shutdown hook
* @param execute Flag to execute the service
* @return ExitException object containing launch result
*/","* Launch a service catching all exceptions and downgrading them to exit codes
   * after logging.
   *
   * Sets {@link #serviceException} to this value.
   * @param conf configuration to use
   * @param instance optional instance of the service.
   * @param processedArgs command line after the launcher-specific arguments
   * have been stripped out.
   * @param addShutdownHook should a shutdown hook be added to terminate
   * this service on shutdown. Tests should set this to false.
   * @param execute execute/wait for the service to stop.
   * @return an exit exception, which will have a status code of 0 if it worked",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,setConf,org.apache.hadoop.security.LdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),767,864,"/**
* Initializes configuration for the LDAP connection.
* @param conf Configuration object containing LDAP settings
*/",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,start,org.apache.hadoop.util.curator.ZKCuratorManager:start(java.util.List),138,140,"/**
* Starts authentication process with given AuthInfo list.
* @param authInfos collection of authentication information
*/","* Start the connection to the ZooKeeper ensemble.
   * @param authInfos List of authentication keys.
   * @throws IOException If the connection cannot be started.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,init,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:init(java.util.Properties),127,132,"/**
* Initializes application components with configuration.
* @param config Properties object containing initialization settings
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/LocalFs.java,<init>,"org.apache.hadoop.fs.local.LocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",49,52,"/**
* Initializes local file system with given configuration and URI.
* @param theUri URI of the local file system
* @param conf Configuration object for the file system
*/","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   * 
   * @param theUri which must be that of localFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,copyStreamToTarget,"org.apache.hadoop.fs.shell.CommandWithDestination:copyStreamToTarget(java.io.InputStream,org.apache.hadoop.fs.shell.PathData)",418,434,"/**
* Copies a stream to the specified target location.
* @param in InputStream to copy from
* @param target PathData representing the target location
*/","* If direct write is disabled ,copies the stream contents to a temporary
   * file ""target._COPYING_"". If the copy is successful, the temporary file
   * will be renamed to the real path, else the temporary file will be deleted.
   * if direct write is enabled , then creation temporary file is skipped.
   *
   * @param in     the input stream for the copy
   * @param target where to store the contents of the stream
   * @throws IOException if copy fails",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,getTargetPath,org.apache.hadoop.fs.shell.CommandWithDestination:getTargetPath(org.apache.hadoop.fs.shell.PathData),330,342,"/**
* Resolves the target path for a given source path.
* @param src PathData object to resolve
* @return PathData object representing the resolved path or null if not found
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,cleanupAllTmpFiles,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:cleanupAllTmpFiles(),399,407,"/**
* Deletes all temporary files.
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,commit,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:commit(),409,444,"/**
* Commits a sequence of file operations.
* @throws IOException if any operation fails
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,delete,"org.apache.hadoop.fs.viewfs.NflyFSystem:delete(org.apache.hadoop.fs.Path,boolean)",768,793,"/**
* Deletes a file or directory by ID with optional recursive removal.
* @param f Path to the file/directory to delete
* @param recursive whether to recursively delete subdirectories
* @return true if deletion was successful for all nodes, false otherwise
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",426,433,"/**
* Copies a file from source to destination HDFS.
* @param srcFS source filesystem
* @param src source path
* @param dstFS destination filesystem
* @param dst destination path
* @param deleteSource whether to delete the source after copy
* @param overwrite whether to overwrite existing destination
* @param conf configuration object
* @return true if successful, false otherwise
*/","* Copy files between FileSystems.
   *
   * @param srcFS srcFs.
   * @param src src.
   * @param dstFS dstFs.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param overwrite overwrite.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return true if the operation succeeded.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,repairAndOpen,"org.apache.hadoop.fs.viewfs.NflyFSystem:repairAndOpen(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode[],org.apache.hadoop.fs.Path,int)",636,713,"/**
* Repairs and opens the MRNfly node array by ID.
* @param mrNodes array of MRNfly nodes
* @param f file path
* @return FSDataInputStream or null if not found
*/","* Iterate all available nodes in the proximity order to attempt repair of all
   * FileNotFound nodes.
   *
   * @param mrNodes work set copy of nodes
   * @param f path to repair and open
   * @param bufferSize buffer size for read RPC
   * @return the closest/most recent replica stream AFTER repair",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.File,boolean,org.apache.hadoop.conf.Configuration)",570,575,"/**
* Copies a file from source to destination with optional deletion of the source.
* @param srcFS source filesystem
* @param src source path
* @param dst destination file
* @param deleteSource whether to delete the source after copy
* @param conf configuration object
* @return true if copy is successful, false otherwise
*/","* Copy FileSystem files to local files.
   *
   * @param srcFS srcFs.
   * @param src src.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return true if the operation succeeded.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,openForSequentialIO,org.apache.hadoop.fs.shell.PathData:openForSequentialIO(),621,624,"/**
* Opens file for sequential I/O access.
* @throws IOException if file cannot be opened
*/","* Open a file for sequential IO.
   * <p>
   * This uses FileSystem.openFile() to request sequential IO;
   * the file status is also passed in.
   * Filesystems may use to optimize their IO.
   * </p>
   * @return an input stream
   * @throws IOException failure",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,dumpToOffset,org.apache.hadoop.fs.shell.Head:dumpToOffset(org.apache.hadoop.fs.shell.PathData),72,77,"/**
* Copies file contents to standard output up to a specified offset.
* @param item PathData object containing the file to dump
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,dumpFromOffset,"org.apache.hadoop.fs.shell.Tail:dumpFromOffset(org.apache.hadoop.fs.shell.PathData,long)",105,121,"/**
* Dumps file contents starting from specified offset.
* @param item PathData containing file to dump
* @param offset starting position (negative offsets are relative to end)
* @return final offset after dumping
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,openFile,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:openFile(org.apache.hadoop.fs.Path),489,493,"/**
* Opens a file at the specified path and returns a builder to manipulate its contents.
* @param path absolute file path
* @return FutureDataInputStreamBuilder instance
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,load,"org.apache.hadoop.util.JsonSerialization:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",248,250,"/**
 * Loads data from the specified file system location.
 * @param fs FileSystem instance
 * @param path File system path to load data from
 */","* Load from a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @return a loaded object
   * @throws PathIOException JSON parse problem
   * @throws IOException IO problems",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNumNear,org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumNear(long),1048,1050,"/**
 * Retrieves record number near specified offset.
 * @param offset offset from which to find nearest location
 */","* Get the RecordNum for the first key-value pair in a compressed block
     * whose byte offset in the TFile is greater than or equal to the specified
     * offset.
     * 
     * @param offset
     *          the user supplied offset.
     * @return the RecordNum to the corresponding entry. If no such entry
     *         exists, it returns the total entry count.
     * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNum,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:getRecordNum(),1629,1631,"/**
* Retrieves record number based on current file location.
* @throws IOException if an I/O error occurs
*/","* Get the RecordNum corresponding to the entry pointed by the cursor.
       * @return The RecordNum corresponding to the entry pointed by the cursor.
       * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,org.apache.hadoop.io.file.tfile.TFile$Reader$Location,org.apache.hadoop.io.file.tfile.TFile$Reader$Location)",1281,1303,"/**
* Initializes a Scanner object with the specified reader and location bounds.
* @param reader input data reader
* @param begin start position of scan range
* @param end end position of scan range
*/","* Constructor
       * 
       * @param reader
       *          The TFile reader object.
       * @param begin
       *          Begin location of the scan.
       * @param end
       *          End location of the scan.
       * @throws IOException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),1396,1429,"/**
* Seeks to a specified location, throwing exceptions for out-of-bounds seeks.
* @param l the target Location
*/","* Move the cursor to the new location. The entry returned by the previous
       * entry() call will be invalid.
       * 
       * @param l
       *          new cursor location. It must fall between the begin and end
       *          location of the scanner.
       * @throws IOException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,advance,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:advance(),1521,1541,"/**
* Advances to the next record or block, returning false if at end.
*/","* Move the cursor to the next key-value pair. The entry returned by the
       * previous entry() call will be invalid.
       * 
       * @return true if the cursor successfully moves. False when cursor is
       *         already at the end location and cannot be advanced.
       * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,append,"org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],byte[])",355,357,"/**
* Appends a key-value pair to storage.
* @param key unique identifier
* @param value associated data
*/","* Adding a new key-value pair to the TFile. This is synonymous to
     * append(key, 0, key.length, value, 0, value.length)
     * 
     * @param key
     *          Buffer for key.
     * @param value
     *          Buffer for value.
     * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,flush,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:flush(),529,584,"/**
* Flushes the cache to disk, persisting keystore updates.
* @throws IOException if an I/O error occurs
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,seekInternal,org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable),704,707,"/**
* Seeks to a specific record in the underlying storage.
* @param key WritableComparable object representing the desired record
*/","* Positions the reader at the named key, or if none such exists, at the
     * first entry after the named key.
     *
     * @return  0   - exact match found
     *          < 0 - positioned at next record
     *          1   - no more records in file",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getClosest,"org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,boolean)",859,875,"/**
* Retrieves closest key to query, with optional directionality.
* @param key target key
* @param val output value
* @param before true for keys before query, false otherwise
* @return nearest WritableComparable or null if not found
*/","* Finds the record that is the closest match to the specified key.
     * 
     * @param key       - key that we're trying to find
     * @param val       - data value if key is found
     * @param before    - IF true, and <code>key</code> does not exist, return
     * the first entry that falls just before the <code>key</code>.  Otherwise,
     * return the record that sorts just after.
     * @return          - the key that was the closest match or null if eof.
     * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",90,95,"/**
* Creates a ProtocolProxy instance for the specified protocol and client version.
* @param protocol Class of the protocol to use
* @param clientVersion Client software version
* @return A new ProtocolProxy object or null on failure
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",86,91,"/**
* Retrieves a protocol proxy instance for the specified client version and address.
* @param protocol Class of the protocol to be proxied
* @param clientVersion Client version number
* @param addr Remote server socket address
* @param ticket User authentication information
* @param conf Configuration settings for the RPC connection
* @param factory Socket factory instance
* @param rpcTimeout Timeout value in milliseconds for RPC requests
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isAuthenticationMethodEnabled,org.apache.hadoop.security.UserGroupInformation:isAuthenticationMethodEnabled(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod),391,396,"/**
* Checks if an authentication method is enabled.
* @param method AuthenticationMethod to check
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isKerberosKeyTabLoginRenewalEnabled,org.apache.hadoop.security.UserGroupInformation:isKerberosKeyTabLoginRenewalEnabled(),398,404,"/**
* Checks if Kerberos keytab login renewal is enabled. 
* @return true if enabled, false otherwise
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getKerberosLoginRenewalExecutor,org.apache.hadoop.security.UserGroupInformation:getKerberosLoginRenewalExecutor(),406,412,"/**
* Retrieves the Kerberos login renewal executor service.
* @return ExecutorService instance or empty if not initialized
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createUserForTesting,"org.apache.hadoop.security.UserGroupInformation:createUserForTesting(java.lang.String,java.lang.String[])",1607,1620,"/**
* Creates a UserGroupInformation object for testing purposes.
* @param user short username
* @param userGroups array of user group names to add
* @return UserGroupInformation object or null if initialization fails
*/","* Create a UGI for testing HDFS and MapReduce
   * @param user the full user principal name
   * @param userGroups the names of the groups that the user belongs to
   * @return a fake user for running unit tests",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createProxyUserForTesting,"org.apache.hadoop.security.UserGroupInformation:createProxyUserForTesting(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.String[])",1634,1645,"/**
* Creates a proxy user for testing purposes.
* @param user short username of the proxy user
* @param realUser actual UserGroupInformation instance
* @param userGroups array of groups to add to the proxy user
* @return created UserGroupInformation object
*/","* Create a proxy user UGI for testing HDFS and MapReduce
   * 
   * @param user
   *          the full user principal name for effective user
   * @param realUser
   *          UGI of the real user
   * @param userGroups
   *          the names of the groups that the user belongs to
   * @return a fake user for running unit tests",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroups,org.apache.hadoop.security.UserGroupInformation:getGroups(),1790,1799,"/**
* Retrieves a list of groups associated with the user.
* @return List of group names or empty list if failed
*/","* Get the group names for this user. {@link #getGroupsSet()} is less
   * expensive alternative when checking for a contained element.
   * @return the list of users with the primary group first. If the command
   *    fails, it returns an empty list.
   * @deprecated Use {@link #getGroupsSet()} instead.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroupsSet,org.apache.hadoop.security.UserGroupInformation:getGroupsSet(),1806,1814,"/**
* Retrieves the set of user groups.
* @return non-empty set of group names or empty set if failed
*/","* Get the groups names for the user as a Set.
   * @return the set of users with the primary group first. If the command
   *     fails, it returns an empty set.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,doSubjectLogin,"org.apache.hadoop.security.UserGroupInformation:doSubjectLogin(javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$LoginParams)",2042,2073,"/**
* Authenticates a user and returns UserGroupInformation.
* @param subject Subject to authenticate, or null for default login
* @param params Login parameters, or null for default login
* @return UserGroupInformation object, or throws KerberosAuthException on failure
*/","* Login a subject with the given parameters.  If the subject is null,
   * the login context used to create the subject will be attached.
   * @param subject to login, null for new subject.
   * @param params for login, null for externally managed ugi.
   * @return UserGroupInformation for subject
   * @throws IOException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,init,org.apache.hadoop.fs.FsShell:init(),100,109,"/**
* Initializes the component by setting quiet mode and configuring user group information.
* Creates a command factory if not already initialized, adds built-in commands, and registers them. 
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,refreshServiceAclWithLoadedConfiguration,"org.apache.hadoop.ipc.Server:refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",778,782,"/**
* Refreshes Service A's ACL with loaded configuration.
* @param conf loaded configuration object
* @param provider policy provider instance
*/","* Refresh the service authorization ACL for the service handled by this server
   * using the specified Configuration.
   *
   * @param conf input Configuration.
   * @param provider input provider.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,getTestProvider,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getTestProvider(),52,59,"/**
* Returns a synchronized instance of the default impersonation provider.
* @return DefaultImpersonationProvider object
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,authorize,"org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",99,102,"/**
* Authorizes user access based on IP address.
* @param user UserGroupInformation object
* @param remoteAddress client's network address
*/","* Authorize the superuser which is doing doAs.
   * {@link #authorize(UserGroupInformation, InetAddress)} should be preferred
   * to avoid possibly re-resolving the ip address.
   *
   * @param user ugi of the effective or proxy user which contains a real user
   * @param remoteAddress the ip address of client
   * @throws AuthorizationException Authorization Exception.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,authorize,"org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)",111,114,"/**
* Authorizes user access by delegating to SIP service.
* @param user UserGroupInformation object
* @param remoteAddress IP address of remote client
*/","* Authorize the superuser which is doing doAs.
   *
   * @param user ugi of the effective or proxy user which contains a real user
   * @param remoteAddress the inet address of client
   * @throws AuthorizationException Authorization Exception.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,getDefaultImpersonationProvider,org.apache.hadoop.security.authorize.ProxyUsers:getDefaultImpersonationProvider(),140,143,"/**
* Returns the default impersonation provider instance.
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,launchService,"org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)",464,469,"/**
* Wraps existing service launch functionality.
* @param conf configuration object
* @return ExitUtil.ExitException object or null if successful
*/","* Launch a service catching all exceptions and downgrading them to exit codes
   * after logging.
   *
   * Sets {@link #serviceException} to this value.
   * @param conf configuration to use
   * @param processedArgs command line after the launcher-specific arguments
   * have been stripped out.
   * @param addShutdownHook should a shutdown hook be added to terminate
   * this service on shutdown. Tests should set this to false.
   * @param execute execute/wait for the service to stop.
   * @return an exit exception, which will have a status code of 0 if it worked",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/RuleBasedLdapGroupsMapping.java,setConf,org.apache.hadoop.security.RuleBasedLdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),56,66,"/**
* Configures the conversion rule from user-provided configuration.
* @param conf Configuration object containing conversion rule settings
*/",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,start,org.apache.hadoop.util.curator.ZKCuratorManager:start(),129,131,"/**
 * Initiates the process by starting with an empty list of tasks.
 */","* Start the connection to the ZooKeeper ensemble.
   * @throws IOException If the connection cannot be started.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/MultiSchemeDelegationTokenAuthenticationHandler.java,init,org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:init(java.util.Properties),99,126,"/**
* Initializes the authentication handler with configured schemes.
* @param config configuration properties
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,copyFileToTarget,"org.apache.hadoop.fs.shell.CommandWithDestination:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",350,367,"/**
* Copies a file from the source to the target location.
* @param src PathData containing source file information
* @param target PathData containing target directory information
*/","* Copies the source file to the target.
   * @param src item to copy
   * @param target where to copy the item
   * @throws IOException if copy fails",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processPathArgument,org.apache.hadoop.fs.shell.CommandWithDestination:processPathArgument(org.apache.hadoop.fs.shell.PathData),247,274,"/**
* Validates and processes path argument, throwing exceptions for invalid directory operations.
* @param src PathData object to validate
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,recursePath,org.apache.hadoop.fs.shell.CommandWithDestination:recursePath(org.apache.hadoop.fs.shell.PathData),299,328,"/**
* Recursively traverses source path and updates target path.
* @param src PathData object to traverse
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,close,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:close(),378,397,"/**
* Closes the output streams and performs necessary cleanup.
* @throws IOException if replication fails or I/O errors occur
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)",366,371,"/**
* Copies a file from source to destination, with optional deletion of the source.
* @param srcFS source file system
* @param src source path
* @param dstFS destination file system
* @param dst destination path
* @param deleteSource whether to delete the source after copy
* @param conf configuration object
*/","* Copy files between FileSystems.
   * @param srcFS src fs.
   * @param src src.
   * @param dstFS dst fs.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param conf configuration.
   * @return if copy success true, not false.
   * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",373,411,"/**
* Copies multiple files from a source file system to a destination directory.
* @param srcFS source file system
* @param srcs array of source file paths
* @param dstFS destination file system
* @param dst destination directory path
* @param deleteSource whether to delete the source files after copying
* @param overwrite whether to overwrite existing files in the destination
* @param conf configuration object
* @return true if all files were copied successfully, false otherwise
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,open,"org.apache.hadoop.fs.viewfs.NflyFSystem:open(org.apache.hadoop.fs.Path,int)",579,621,"/**
* Opens file from a set of MRNflyNodes, handling repair and readMostRecent flags.
* @param f Path to the file
* @return FSDataInputStream or null if not found after repair
*/","* Category: READ.
   *
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.
   * @return input stream according to nfly flags (closest, most recent)
   * @throws IOException
   * @throws FileNotFoundException iff all destinations generate this exception",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processArguments,org.apache.hadoop.fs.shell.CopyCommands$Merge:processArguments(java.util.LinkedList),91,114,"/**
* Writes path data to output stream.
* @param items list of PathData objects
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,getInputStream,org.apache.hadoop.fs.shell.Display$Cat:getInputStream(org.apache.hadoop.fs.shell.PathData),106,109,"/**
* Opens input stream for sequential I/O on given PathData item.
* @param item PathData object representing file or directory
* @return InputStream for sequential I/O operations
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,processPath,org.apache.hadoop.fs.shell.Head:processPath(org.apache.hadoop.fs.shell.PathData),63,70,"/**
* Processes file path data, throwing exception on directory detection.
* @param item PathData object containing file information
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,processPath,org.apache.hadoop.fs.shell.Tail:processPath(org.apache.hadoop.fs.shell.PathData),88,103,"/**
* Continuously dumps data from the specified path until interrupted.
* @param item PathData object containing file information
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScanner,org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(),1077,1079,"/**
* Creates a scanner instance to read from this input stream between specified positions.
* @throws IOException if an I/O error occurs
*/","* Get a scanner than can scan the whole TFile.
     * 
     * @return The scanner object. A valid Scanner is always returned even if
     *         the TFile is empty.
     * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByRecordNum,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByRecordNum(long,long)",1194,1202,"/**
* Creates a scanner for the specified record range.
* @param beginRecNum starting record number (defaulted to 0 if < 0)
* @param endRecNum ending record number (clamped to entry count if out of range)
* @return Scanner object or throws IOException on failure
*/","* Create a scanner that covers a range of records.
     * 
     * @param beginRecNum
     *          The RecordNum for the first record (inclusive).
     * @param endRecNum
     *          The RecordNum for the last record (exclusive). To scan the whole
     *          file, either specify endRecNum==-1 or endRecNum==getEntryCount().
     * @return The TFile scanner that covers the specified range of records.
     * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,long,long)",1264,1268,"/**
* Constructs a new Scanner from a Reader with specified offset range.
* @param reader input Reader to read from
* @param offBegin offset to begin scanning at
* @param offEnd offset to end scanning at
*/","* Constructor
       * 
       * @param reader
       *          The TFile reader object.
       * @param offBegin
       *          Begin byte-offset of the scan.
       * @param offEnd
       *          End byte-offset of the scan.
       * @throws IOException
       * 
       *           The offsets will be rounded to the beginning of a compressed
       *           block whose offset is greater than or equal to the specified
       *           offset.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",1366,1385,"/**
* Advances reader to the specified key or beyond it, returning true if successful. 
* @param key RawComparable object, unique identifier
* @param beyond whether to seek beyond the key
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,rewind,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:rewind(),1437,1439,"/**
* Rewinds the stream to its beginning location.
* @throws IOException if an I/O error occurs during seeking
*/","* Rewind to the first entry in the scanner. The entry returned by the
       * previous entry() call will be invalid.
       * 
       * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,seek,org.apache.hadoop.io.MapFile$Reader:seek(org.apache.hadoop.io.WritableComparable),692,694,"/**
* Seeks to the specified record in the underlying storage.
* @param key WritableComparable object representing the record to seek
* @return true if the record was found, false otherwise
*/","* Positions the reader at the named key, or if none such exists, at the
     * first entry after the named key.  Returns true iff the named key exists
     * in this map.
     *
     * @param key key.
     * @throws IOException raised on errors performing I/O.
     * @return if the named key exists in this map true, not false.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getClosest,"org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",842,846,"/**
* Retrieves closest matching value based on given key and comparison criteria.
* @param key The data to compare against
* @param val The value to find the closest match for
*/","* Finds the record that is the closest match to the specified key.
     * Returns <code>key</code> or if it does not exist, at the first entry
     * after the named key.
     * 
     * @param key key that we're trying to find.
     * @param val data value if key is found.
     * @return the key that was the closest match or null if eof.
     * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isSecurityEnabled,org.apache.hadoop.security.UserGroupInformation:isSecurityEnabled(),387,389,"/**
* Checks if security is enabled by verifying no simple authentication method.
* @return true if security is disabled, false otherwise
*/","* Determine if UserGroupInformation is using Kerberos to determine
   * user identities or is relying on simple authentication
   * 
   * @return true if UGI is working in a secure environment",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logoutUserFromKeytab,org.apache.hadoop.security.UserGroupInformation:logoutUserFromKeytab(),1158,1189,"/**
* Logs out user from Kerberos login context.
* @throws IOException on authentication failure
*/","* Log the current user out who previously logged in using keytab.
   * This method assumes that the user logged in by calling
   * {@link #loginUserFromKeytab(String, String)}.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if a failure occurred in logout,
   * or if the user did not log in by invoking loginUserFromKeyTab() before.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,checkStat,"org.apache.hadoop.io.SecureIOUtils:checkStat(java.io.File,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",282,303,"/**
* Validates file ownership and group membership. 
* @param f File object to check
* @param owner current owner of the file
* @param group current group of the file
* @param expectedOwner expected owner (may be null)
* @param expectedGroup expected group (may be null)",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getPrimaryGroupName,org.apache.hadoop.security.UserGroupInformation:getPrimaryGroupName(),1655,1661,"/**
* Retrieves the primary group name from the set of groups.
* @throws IOException if no primary group exists
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroupNames,org.apache.hadoop.security.UserGroupInformation:getGroupNames(),1778,1781,"/**
* Returns an array of group names.
* @return array of group identifiers
*/","* Get the group names for this user. {@link #getGroupsSet()} is less
   * expensive alternative when checking for a contained element.
   * @return the list of users with the primary group first. If the command
   *    fails, it returns an empty list.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,isUserInList,org.apache.hadoop.security.authorize.AccessControlList:isUserInList(org.apache.hadoop.security.UserGroupInformation),235,249,"/**
* Checks if user is in allowed list based on username or group membership.
* @param ugi UserGroupInformation object to check
* @return true if user is in the list, false otherwise
*/","* Checks if a user represented by the provided {@link UserGroupInformation}
   * is a member of the Access Control List. If user was proxied and
   * USE_REAL_ACLS + the real user name is in the control list, then treat this
   * case as if user were in the ACL list.
   * @param ugi UserGroupInformation to check if contained in the ACL
   * @return true if ugi is member of the list or if USE_REAL_ACLS + real user
   * is in the list",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getUGIFromSubject,org.apache.hadoop.security.UserGroupInformation:getUGIFromSubject(javax.security.auth.Subject),651,664,"/**
* Retrieves UserGroupInformation from a Subject instance, throwing exceptions if the Subject is null or does not contain a KerberosPrincipal.
* @param subject the Subject instance to extract UGI from
* @throws IOException on authentication error
*/","* Create a UserGroupInformation from a Subject with Kerberos principal.
   *
   * @param subject             The KerberosPrincipal to use in UGI.
   *                            The creator of subject is responsible for
   *                            renewing credentials.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if the kerberos login fails
   * @return UserGroupInformation",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createLoginUser,org.apache.hadoop.security.UserGroupInformation:createLoginUser(javax.security.auth.Subject),731,803,"/**
* Creates a UserGroupInformation instance for the logged-in user,
* potentially creating a proxy user if HADOOP_PROXY_USER is set.
* @param subject Subject to authenticate
* @return UserGroupInformation instance or null on failure
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,doFilter,"org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",60,105,"/**
* Authenticates and proxies the current user to a different user ID.
* @param filterChain Filter chain
* @param request HTTP request containing user ID to proxy as
* @throws IOException or ServletException if authentication fails
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,doFilter,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",243,308,"/**
* Authenticates and authorizes incoming HTTP requests based on provided authentication tokens.
* @param filterChain Filter chain to pass through if authentication succeeds
* @param request Incoming HTTP request
* @param response Outgoing HTTP response
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,managementOperation,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",220,355,"/**
* Handles management operations based on the provided token and HTTP request.
* @param token authentication token
* @param request HTTP servlet request
* @param response HTTP servlet response
* @return true if operation continues, false otherwise
*/",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,authorize,"org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.conf.Configuration)",134,138,"/**
* Wraps authorization logic with deprecated alias.
* @param user UserGroupInformation object
* @param remoteAddress client IP address
* @param conf Hadoop configuration
*/","* This function is kept to provide backward compatibility.
   * @param user user.
   * @param remoteAddress remote address.
   * @param conf configuration.
   * @throws AuthorizationException Authorization Exception.
   * @deprecated use {@link #authorize(UserGroupInformation, String)} instead.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,authorizeConnection,org.apache.hadoop.ipc.Server$Connection:authorizeConnection(),3018,3039,"/**
* Authorizes RPC connections based on user authentication.
* @throws RpcServerException if authorization fails
*/","* Authorize proxy users to access this server
     * @throws RpcServerException - user is not allowed to proxy",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,copyFileToTarget,"org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",140,154,"/**
* Copies a file to the target location, potentially executing asynchronously.
* @param src source PathData object
* @param target destination PathData object
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processPath,"org.apache.hadoop.fs.shell.CommandWithDestination:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",287,297,"/**
* Processes source path to target path.
* @param src source file or directory data
* @param dst target file or directory data
* @throws IOException on processing error
*/","* Called with a source and target destination pair
   * @param src for the operation
   * @param dst for the operation
   * @throws IOException if anything goes wrong",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.HarFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",845,849,"/**
* Copies file from source to destination, optionally deleting the original.
* @param delSrc whether to delete the source file
* @param src source file path
* @param dst destination file path
*/",* copies the file in the har filesystem to a local file.,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,rename,"org.apache.hadoop.fs.RawLocalFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",624,644,"/**
* Renames a file from one path to another.
* @param src source file path
* @param dst target file path
* @return true if rename successful, false otherwise
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.ChecksumFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",991,996,"/**
* Copies data from a local file to HDFS.
* @param delSrc whether to delete source file
* @param src local file path
* @param dst HDFS destination path
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",1002,1007,"/**
* Copies a file from the specified source to the local destination.
* @param delSrc whether to delete source after copy
* @param src source file path
* @param dst local destination file path
*/","* The src file is under FS, and the dst is on the local disk.
   * Copy it from FS control to the local dst name.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.LocalFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",83,87,"/**
* Copies data from a local file to another location.
* @param delSrc whether to delete the source file
* @param src source path of the file to copy
* @param dst destination path for the copied data
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.LocalFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",89,93,"/**
* Copies file from source to destination while optionally deleting the source.
* @param delSrc whether to delete the source after copying
* @param src source file path
* @param dst destination file path
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processPath,org.apache.hadoop.fs.shell.Display$Cat:processPath(org.apache.hadoop.fs.shell.PathData),88,96,"/**
* Processes a path by verifying its checksum and printing to stdout.
* @param item PathData object containing file information
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByByteRange,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByByteRange(long,long)",1094,1096,"/**
* Creates a new scanner instance with specified byte range.
* @param offset starting position in bytes
* @param length number of bytes to scan
* @return Scanner object or null on error
*/","* Get a scanner that covers a portion of TFile based on byte offsets.
     * 
     * @param offset
     *          The beginning byte offset in the TFile.
     * @param length
     *          The length of the region.
     * @return The actual coverage of the returned scanner tries to match the
     *         specified byte-region but always round up to the compression
     *         block boundaries. It is possible that the returned scanner
     *         contains zero key-value pairs even if length is positive.
     * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1318,1331,"/**
* Initializes a Scanner from a Reader with specified key boundaries.
* @param reader input source
* @param beginKey starting key for block
* @param endKey ending key for block
*/","* Constructor
       * 
       * @param reader
       *          The TFile reader object.
       * @param beginKey
       *          Begin key of the scan. If null, scan from the first
       *          &lt;K, V&gt; entry of the TFile.
       * @param endKey
       *          End key of the scan. If null, scan up to the last &lt;K, V&gt;
       *          entry of the TFile.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[],int,int)",1361,1364,"/**
* Seeks to a specific byte range within the underlying data store.
* @param key byte array containing the key
* @param keyOffset offset of the key in bytes
* @param keyLen length of the key in bytes
* @return true if successful, false otherwise
*/","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. The entry returned by the previous entry() call will
       * be invalid.
       * 
       * @param key
       *          The input key
       * @param keyOffset
       *          offset in the key buffer.
       * @param keyLen
       *          key buffer length.
       * @return true if we find an equal key; false otherwise.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,lowerBound,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[],int,int)",1477,1480,"/**
* Seeks to the specified byte range in the underlying data source.
* @param key  Byte array representing the target range
* @param keyOffset offset within the byte array
* @param keyLen length of the byte array range
*/","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. The entry returned by the previous entry() call will
       * be invalid.
       * 
       * @param key
       *          The input key
       * @param keyOffset
       *          offset in the key buffer.
       * @param keyLen
       *          key buffer length.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,upperBound,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[],int,int)",1508,1511,"/**
* Seeks to the upper bound of a byte array in the underlying data structure.
* @param key the byte array to search for
* @param keyOffset starting offset within the byte array
* @param keyLen length of the byte array
*/","* Move the cursor to the first entry whose key is strictly greater than
       * the input key. The entry returned by the previous entry() call will be
       * invalid.
       * 
       * @param key
       *          The input key
       * @param keyOffset
       *          offset in the key buffer.
       * @param keyLen
       *          key buffer length.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,seek,org.apache.hadoop.io.SetFile$Reader:seek(org.apache.hadoop.io.WritableComparable),130,134,"/**
* Seeks to the specified key in the underlying storage.
* @param key WritableComparable object representing the target position
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,get,"org.apache.hadoop.io.MapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",823,830,"/**
 * Retrieves a value from the underlying store by seeking to the given key.
 * @param key unique key identifier
 * @param val output parameter for the retrieved value
 * @return the fetched Writable object or null if seek fails
 */","* Return the value for the named key, or null if none exists.
     * @param key key.
     * @param val val.
     * @return Writable if such a pair exists true,not false.
     * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,commit,org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit(),189,235,"/**
* Commits login process by retrieving and authenticating a user.
* @throws LoginException if authentication fails
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",579,587,"/**
* Retrieves a ProtocolProxy instance for the specified protocol.
* @param protocol Class of the target protocol
* @param clientVersion Client version to use
* @param connId Connection ID
* @param conf Configuration object
* @param factory Socket factory
* @param alignmentContext Alignment context
* @return ProtocolProxy instance or throws IOException if failed
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T
   * @param protocol protocol class
   * @param clientVersion client's version
   * @param connId client connection identifier
   * @param conf configuration
   * @param factory socket factory
   * @param alignmentContext StateID alignment context
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean)",661,677,"/**
* Retrieves a protocol proxy instance with specified configuration.
* @param protocol Class of the protocol to use
* @param clientVersion Client version for RPC communication
* @return ProtocolProxy object or throws IOException if failure occurs
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @param connectionRetryPolicy retry policy
   * @param fallbackToSimpleAuth set to true or false during calls to indicate if
   *   a secure client falls back to simple auth
   * @return the proxy
   * @throws IOException if any error occurs",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",698,715,"/**
* Retrieves a protocol proxy for the specified protocol and parameters.
* @param protocol target protocol class
* @param clientVersion client version information
* @param addr server address details
* @param ticket user authentication token
* @param conf configuration settings
* @param factory socket creation factory
* @param rpcTimeout RPC request timeout value
* @param connectionRetryPolicy retry policy for failed connections
* @param fallbackToSimpleAuth flag to enable simple auth fallback
* @return ProtocolProxy object or null on failure","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @param connectionRetryPolicy retry policy
   * @param fallbackToSimpleAuth set to true or false during calls to indicate
   *   if a secure client falls back to simple auth
   * @param alignmentContext state alignment context
   * @param <T> Generics Type T.
   * @return the proxy
   * @throws IOException if any error occurs",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setFallBackToSimpleAuth,org.apache.hadoop.ipc.Client$Connection:setFallBackToSimpleAuth(java.util.concurrent.atomic.AtomicBoolean),858,890,"/**
* Enables or disables falling back to SIMPLE authentication based on the auth method and protocol.
* @param fallbackToSimpleAuth flag indicating whether to enable fallback
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,forceSecureOpenForRandomRead,"org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRandomRead(java.io.File,java.lang.String,java.lang.String,java.lang.String)",126,143,"/**
* Opens a secure RandomAccessFile for random read access.
* @param f the file to open
* @param mode the mode to use (e.g. ""r"")
* @param expectedOwner the expected owner of the file
* @param expectedGroup the expected group of the file
* @return an open RandomAccessFile or null if not secure
*/","* @return Same as openForRandomRead except that it will run even if security is off.
   * This is used by unit tests.
   *
   * @param f input f.
   * @param mode input mode.
   * @param expectedOwner input expectedOwner.
   * @param expectedGroup input expectedGroup.
   * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,forceSecureOpenFSDataInputStream,"org.apache.hadoop.io.SecureIOUtils:forceSecureOpenFSDataInputStream(java.io.File,java.lang.String,java.lang.String)",174,192,"/**
* Opens FSDataInputStream for a file with secure access checks.
* @param file File to open
* @param expectedOwner Expected owner of the file
* @param expectedGroup Expected group of the file
* @return FSDataInputStream object or null on failure
*/","* Same as openFSDataInputStream except that it will run even if security is
   * off. This is used by unit tests.
   *
   * @param file input file.
   * @param expectedOwner input expectedOwner.
   * @param expectedGroup input expectedGroup.
   * @throws IOException raised on errors performing I/O.
   * @return FSDataInputStream.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,forceSecureOpenForRead,"org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRead(java.io.File,java.lang.String,java.lang.String)",224,241,"/**
* Opens a FileInputStream for reading with security checks.
* @param f the file to open
* @param expectedOwner expected owner of the file
* @param expectedGroup expected group of the file
* @return FileInputStream object or null if access denied
*/","* @return Same as openForRead() except that it will run even if security is off.
   * This is used by unit tests.
   * @param f input f.
   * @param expectedOwner input expectedOwner.
   * @param expectedGroup input expectedGroup.
   * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileStatus(org.apache.hadoop.fs.Path),1080,1087,"/**
* Retrieves FileStatus for the given Path.
* @param f file path
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileLinkStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path),1089,1128,"/**
* Retrieves the status of a file link.
* @param f the path to the file
* @return FileStatus object or throws FileNotFoundException if not found
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path),1406,1413,"/**
* Creates ACL status for a given path.
* @param path file system path
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileStatus(org.apache.hadoop.fs.Path),1544,1551,"/**
* Retrieves FileStatus for the given file path.
* @param f file path to retrieve status for
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path),1554,1620,"/**
* Lists file statuses for a given directory path, considering fallback links and internal directory contents.
* @param f the directory path to list statuses for
* @return array of FileStatus objects representing files and directories in the path
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path),1832,1839,"/**
* Creates ACL status for a given path.
* @param path file system path
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dumpUGI,"org.apache.hadoop.security.KDiag:dumpUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",666,690,"/**
* Dumps user group information, including credentials and tokens.
* @param ugi UserGroupInformation instance to dump
*/","* Dump a UGI.
   *
   * @param title title of this section
   * @param ugi UGI to dump
   * @throws IOException",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,print,org.apache.hadoop.security.UserGroupInformation:print(),2022,2032,"/**
* Prints user details and group memberships.
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,isUserAllowed,org.apache.hadoop.security.authorize.AccessControlList:isUserAllowed(org.apache.hadoop.security.UserGroupInformation),251,253,"/**
* Checks user permission by checking their group membership.
* @param ugi UserGroupInformation object containing user credentials
*/",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getLoginUser,org.apache.hadoop.security.UserGroupInformation:getLoginUser(),673,698,"/**
* Retrieves the current logged-in user's information.
* @return UserGroupInformation object representing the login user, or null if not found
*/","* Get the currently logged in user.  If no explicit login has occurred,
   * the user will automatically be logged in with either kerberos credentials
   * if available, or as the local OS user, based on security settings.
   * @return the logged in user
   * @throws IOException if login fails",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,loginUserFromSubject,org.apache.hadoop.security.UserGroupInformation:loginUserFromSubject(javax.security.auth.Subject),725,729,"/**
* Logs in user from provided Subject instance.
* @param subject authentication data for the user to log in. 
*/","* Log in a user using the given subject
   * @param subject the subject to use when logging in a user, or null to
   * create a new subject.
   *
   * If subject is not null, the creator of subject is responsible for renewing
   * credentials.
   *
   * @throws IOException if login fails",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processConnectionContext,org.apache.hadoop.ipc.Server$Connection:processConnectionContext(org.apache.hadoop.ipc.RpcWritable$Buffer),2678,2724,"/**
* Processes the RPC connection context, authenticates and authorizes the user.
* @param buffer RpcWritable.Buffer object containing the connection context
*/","Reads the connection context following the connection header
     * @throws RpcServerException - if the header cannot be
     *         deserialized, or the user is not authorized",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processPath,org.apache.hadoop.fs.shell.CommandWithDestination:processPath(org.apache.hadoop.fs.shell.PathData),276,279,"/**
 * Processes the given path data and delegates to processPath with the resolved target path.
 */",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,moveFromLocalFile,"org.apache.hadoop.fs.RawLocalFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",877,880,"/**
 * Moves a file from a local source to a destination path.
 * @param src source file path
 * @param dst destination file path
 */",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByKey,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1174,1181,"/**
* Creates a scanner instance for the given key range.
* @param beginKey start key of the range
* @param endKey end key of the range
* @return Scanner object or null if invalid range
*/","* Get a scanner that covers a specific key range.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[]),1343,1345,"/**
 * Seeks to specified byte range in underlying storage.
 * @param key byte array containing key to seek to
 */","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. Synonymous to seekTo(key, 0, key.length). The entry
       * returned by the previous entry() call will be invalid.
       * 
       * @param key
       *          The input key
       * @return true if we find an equal key.
       * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,lowerBound,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[]),1460,1462,"/**
* Finds the lower bound of a binary search tree using the given byte array as a key.
* @param key byte array representing the key to find
*/","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. Synonymous to lowerBound(key, 0, key.length). The
       * entry returned by the previous entry() call will be invalid.
       * 
       * @param key
       *          The input key
       * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,upperBound,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[]),1491,1493,"/**
* Finds the upper bound of a byte array in an index.
* @param key the byte array to search
*/","* Move the cursor to the first entry whose key is strictly greater than
       * the input key. Synonymous to upperBound(key, 0, key.length). The entry
       * returned by the previous entry() call will be invalid.
       * 
       * @param key
       *          The input key
       * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,get,org.apache.hadoop.io.SetFile$Reader:get(org.apache.hadoop.io.WritableComparable),156,163,"/**
* Retrieves the WritableComparable object associated with the given key.
* @param key unique key to search for
* @return matching WritableComparable object or null if not found
*/","* Read the matching key from a set into <code>key</code>.
     *
     * @param key input key.
     * @return Returns <code>key</code>, or null if no match exists.
     * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,get,"org.apache.hadoop.io.BloomMapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",281,288,"/**
* Optimized fetch of Writable value associated with the given key.
* @param key unique identifier
* @param val ignored (only used for consistency with superclass)
* @return Writable value or null if not found
*/","* Fast version of the
     * {@link MapFile.Reader#get(WritableComparable, Writable)} method. First
     * it checks the Bloom filter for the existence of the key, and only if
     * present it performs the real get operation. This yields significant
     * performance improvements for get operations on sparsely populated files.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",558,563,"/**
* Retrieves a ProtocolProxy instance for the specified protocol and client version.
* @param protocol Class of the protocol to proxy
* @param clientVersion Client version number
* @param connId Unique connection identifier
* @param conf Configuration object
* @param factory Socket factory used for creation
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T
   * @param protocol protocol class
   * @param clientVersion client's version
   * @param connId client connection identifier
   * @param conf configuration
   * @param factory socket factory
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",631,641,"/**
* Creates a protocol proxy instance with default parameters.
* @param protocol Class of the protocol to create a proxy for
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @param connectionRetryPolicy retry policy
   * @return the proxy
   * @throws IOException if any error occurs",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setupIOstreams,org.apache.hadoop.ipc.Client$Connection:setupIOstreams(java.util.concurrent.atomic.AtomicBoolean),764,856,"/**
* Establishes and configures IPC connection with server.
* @param fallbackToSimpleAuth indicates whether to fall back to simple auth
*/","Connect to the server and set up the I/O streams. It then sends
     * a header to the server and starts
     * the connection thread that waits for responses.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,openForRandomRead,"org.apache.hadoop.io.SecureIOUtils:openForRandomRead(java.io.File,java.lang.String,java.lang.String,java.lang.String)",107,114,"/**
* Opens a file for random read access with security checks.
* @param f the file to open
* @param mode the file access mode
* @param expectedOwner the expected owner of the file
* @param expectedGroup the expected group of the file
* @return RandomAccessFile object or null if not secure
*/","* @return Open the given File for random read access, verifying the expected user/
   * group constraints if security is enabled.
   * 
   * Note that this function provides no additional security checks if hadoop
   * security is disabled, since doing the checks would be too expensive when
   * native libraries are not available.
   * 
   * @param f file that we are trying to open
   * @param mode mode in which we want to open the random access file
   * @param expectedOwner the expected user owner for the file
   * @param expectedGroup the expected group owner for the file
   * @throws IOException if an IO error occurred or if the user/group does
   * not match when security is enabled.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,openFSDataInputStream,"org.apache.hadoop.io.SecureIOUtils:openFSDataInputStream(java.io.File,java.lang.String,java.lang.String)",156,162,"/**
* Opens an FSDataInputStream for the specified file with security checks.
* @param file File to open
* @param expectedOwner Expected owner of the file (for security)
* @param expectedGroup Expected group of the file (for security)
* @return FSDataInputStream or null if not secure
*/","* Opens the {@link FSDataInputStream} on the requested file on local file
   * system, verifying the expected user/group constraints if security is
   * enabled.
   * @param file absolute path of the file
   * @param expectedOwner the expected user owner for the file
   * @param expectedGroup the expected group owner for the file
   * @throws IOException if an IO Error occurred or the user/group does not
   * match if security is enabled
   * @return FSDataInputStream.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,openForRead,"org.apache.hadoop.io.SecureIOUtils:openForRead(java.io.File,java.lang.String,java.lang.String)",208,214,"/**
* Opens a file for reading with security checks enabled.
* @param f the file to open
* @param expectedOwner expected owner of the file (HDFS)
* @param expectedGroup expected group of the file (HDFS)
* @return FileInputStream object or null if not found
*/","* Open the given File for read access, verifying the expected user/group
   * constraints if security is enabled.
   *
   * @return Note that this function provides no additional checks if Hadoop
   * security is disabled, since doing the checks would be too expensive
   * when native libraries are not available.
   *
   * @param f the file that we are trying to open
   * @param expectedOwner the expected user owner for the file
   * @param expectedGroup the expected group owner for the file
   * @throws IOException if an IO Error occurred, or security is enabled and
   * the user/group does not match",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getLinkTarget(org.apache.hadoop.fs.Path),1334,1338,"/**
* Retrieves the target path of a symbolic link.
* @param f the symbolic link file to resolve
* @return the linked file's path or null if not found
*/",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getContentSummary,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getContentSummary(org.apache.hadoop.fs.Path),1659,1678,"/**
* Retrieves content summary for a given file system path.
* @param f the file system path to summarize
* @return ContentSummary object containing total length, file count, and directory count
*/",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStatus(org.apache.hadoop.fs.Path),1680,1694,"/**
* Calculates aggregated file system status for a given path.
* @param p the path to aggregate
* @return aggregated FsStatus object or throws IOException if error occurs
*/",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,userHasAdministratorAccess,"org.apache.hadoop.http.HttpServer2:userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)",1727,1734,"/**
* Checks if a user has administrator access.
* @param servletContext web application context
* @param remoteUser username of the user to check
* @return true if the user is an admin, false otherwise
*/","* Get the admin ACLs from the given ServletContext and check if the given
   * user is in the ACL.
   *
   * @param servletContext the context containing the admin ACL.
   * @param remoteUser the remote user to check for.
   * @return true if the user is present in the ACL, false if no ACL is set or
   *         the user is not present",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ServiceAuthorizationManager.java,authorize,"org.apache.hadoop.security.authorize.ServiceAuthorizationManager:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,org.apache.hadoop.conf.Configuration,java.net.InetAddress)",88,138,"/**
* Authorizes a user to access a service using the specified protocol.
* @param user UserGroupInformation object
* @param protocol Class of the service protocol
* @param conf Configuration object (Kerberos principal key)
* @param addr InetAddress of the host (optional)
*/","* Authorize the user to access the protocol being used.
   * 
   * @param user user accessing the service 
   * @param protocol service being accessed
   * @param conf configuration to use
   * @param addr InetAddress of the client
   * @throws AuthorizationException on authorization failure",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,authorize,"org.apache.hadoop.security.authorize.DefaultImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)",108,135,"/**
* Verifies user authorization to impersonate and connect as a super-user.
* @param user UserGroupInformation object
* @param remoteAddress InetAddress of connecting machine
*/",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getCurrentUser,org.apache.hadoop.security.UserGroupInformation:getCurrentUser(),583,594,"/**
* Retrieves the current user's identity.
* @return UserGroupInformation object representing the current user or a default user if none found
*/","* Return the current user, including any doAs in the current stack.
   * @return the current user
   * @throws IOException if login fails",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isLoginKeytabBased,org.apache.hadoop.security.UserGroupInformation:isLoginKeytabBased(),1417,1421,"/**
* Checks if login is based on a keytab.
* @return true if user logged in from a keytab, false otherwise
*/","* Did the login happen via keytab.
   * @return true or false
   * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isLoginTicketBased,org.apache.hadoop.security.UserGroupInformation:isLoginTicketBased(),1428,1430,"/**
* Checks if login is ticket-based.
* @return true if login is from a ticket, false otherwise
*/","* Did the login happen via ticket cache.
   * @return true or false
   * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsLoginUserOrFatal,org.apache.hadoop.security.SecurityUtil:doAsLoginUserOrFatal(java.security.PrivilegedAction),508,522,"/**
* Executes the specified action as the current login user or exits fatally if security is enabled.
* @param action PrivilegedAction to be executed
* @return Result of the action, or null/exception if fatal exit occurred
*/","* Perform the given action as the daemon's login user. If the login
   * user cannot be determined, this will log a FATAL error and exit
   * the whole JVM.
   *
   * @param action action.
   * @param <T> generic type T.
   * @return generic type T.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsLoginUser,org.apache.hadoop.security.SecurityUtil:doAsLoginUser(java.security.PrivilegedExceptionAction),533,536,"/**
* Executes an action as the current login user.
* @param action Privileged exception action to perform
* @return Result of the action, or null if not applicable
*/","* Perform the given action as the daemon's login user. If an
   * InterruptedException is thrown, it is converted to an IOException.
   *
   * @param action the action to perform
   * @param <T> Generics Type T.
   * @return the result of the action
   * @throws IOException in the event of error",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,cedeActive,org.apache.hadoop.ha.ZKFailoverController:cedeActive(int),575,588,"/**
* Cedes active resources after a specified time period.
* @param millisToCede time in milliseconds to wait before ceding
*/","* Request from graceful failover to cede active role. Causes
   * this ZKFC to transition its local node to standby, then quit
   * the election for the specified period of time, after which it
   * will rejoin iff it is healthy.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,gracefulFailoverToYou,org.apache.hadoop.ha.ZKFailoverController:gracefulFailoverToYou(),630,643,"/**
* Initiates a graceful failover to the current user.
* Throws an exception if interrupted or service fails.
*/","* Coordinate a graceful failover to this node.
   * @throws ServiceFailedException if the node fails to become active
   * @throws IOException some other error occurs",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByKey,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey(byte[],byte[])",1132,1137,"/**
* Creates a scanner for the specified key range.
* @param beginKey starting byte array key
* @param endKey ending byte array key
* @return Scanner object or throws IOException on error
*/","* Get a scanner that covers a portion of TFile based on keys.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScanner,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1155,1159,"/**
* Creates a new Scanner instance by key range.
* @param beginKey starting key for scanning
* @param endKey ending key for scanning
*/","* Get a scanner that covers a specific key range.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.
     * 
     * @deprecated Use {@link #createScannerByKey(RawComparable, RawComparable)}
     *             instead.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",535,543,"/**
* Retrieves a protocol proxy object based on the provided parameters.
* @param protocol class of the protocol to be proxied
* @param clientVersion client version for configuration
* @param addr address of the service endpoint
* @param ticket user identity for authentication
* @param conf configuration settings for the connection
* @param factory socket creation factory
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param ticket user group information
   * @param conf configuration to use
   * @param factory socket factory
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",604,613,"/**
* Retrieves a proxy instance for the specified protocol.
* @param protocol Class of the protocol
* @param clientVersion Client version
* @param addr Server address
* @param ticket User authentication information
* @param conf Configuration settings
* @param factory Socket creation factory
* @param rpcTimeout RPC timeout value
* @return Proxy instance or null if failed
*/","* Construct a client-side proxy that implements the named protocol,
   * talking to a server at the named address.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @return the proxy
   * @throws IOException if any error occurs",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,hasAdministratorAccess,"org.apache.hadoop.http.HttpServer2:hasAdministratorAccess(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",1686,1716,"/**
* Checks if a user has administrator access, considering security authorization and ACLs.
* @param servletContext application context
* @param request HTTP request
* @param response HTTP response
* @return true if user has admin access, false otherwise
*/","* Does the user sending the HttpServletRequest has the administrator ACLs? If
   * it isn't the case, response will be modified to send an error to the user.
   *
   * @param servletContext servletContext.
   * @param request request.
   * @param response used to send the error response if user does not have admin access.
   * @return true if admin-authorized, false otherwise
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,authorize,"org.apache.hadoop.ipc.Server:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.net.InetAddress)",3806,3821,"/**
* Authorizes user access based on protocol and address.
* @param user UserGroupInformation object
* @param protocolName name of the protocol to authorize against
* @param addr InetAddress of the host being accessed
*/","* Authorize the incoming client connection.
   * 
   * @param user client user
   * @param protocolName - the protocol
   * @param addr InetAddress of incoming connection
   * @throws AuthorizationException when the client isn't authorized to talk the protocol",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getHomeDirectory,org.apache.hadoop.fs.FileSystem:getHomeDirectory(),2445,2456,"/**
* Retrieves the user's home directory as a Path object.
* @return Home directory path or null on failure
*/","Return the current user's home directory in this FileSystem.
   * The default implementation returns {@code ""/user/$USER/""}.
   * @return the path.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,checkAccessPermissions,"org.apache.hadoop.fs.FileSystem:checkAccessPermissions(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction)",2855,2877,"/**
* Verifies access permissions for a file based on user/group ownership and FsAction mode.
* @param stat FileStatus object containing file metadata
* @param mode FsAction enum value representing desired access rights
* @throws AccessControlException if permission is denied
*/","* This method provides the default implementation of
   * {@link #access(Path, FsAction)}.
   *
   * @param stat FileStatus to check
   * @param mode type of access to check
   * @throws AccessControlException if access is denied
   * @throws IOException for any error",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(),280,283,"/**
* Initializes the ViewFileSystem instance with current user and creation time.
*/","* This is the  constructor with the signature needed by
   * {@link FileSystem#createFileSystem(URI, Configuration)}
   *
   * After this constructor is called initialize() is called.
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",227,290,"/**
* Initializes a ViewFs object from the given URI and configuration.
* @param theUri unique identifier of the view file system
* @param conf configuration parameters for the view file system
*/","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   *
   * @param theUri which must be that of ViewFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree:<init>(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI,boolean)",617,770,"/**
* Initializes ViewFS with the given configuration and URI.
* @param config Configuration object
* @param viewName Name of the mount table (or null to use default)
* @param uri The URI for which this ViewFS is being initialized
* @param initingUriAsFallbackOnNoMounts Whether to treat an empty mount table as a fallback link if no mounts are found
*/","* Create Inode Tree from the specified mount-table specified in Config.
   *
   * @param config the mount table keys are prefixed with
   *               FsConstants.CONFIG_VIEWFS_PREFIX.
   * @param viewName the name of the mount table
   *                 if null use defaultMT name.
   * @param theUri heUri.
   * @param initingUriAsFallbackOnNoMounts initingUriAsFallbackOnNoMounts.
   * @throws UnsupportedFileSystemException file system for <code>uri</code> is
   *                                        not found.
   * @throws URISyntaxException if the URI does not have an authority
   *                            it is badly formed.
   * @throws FileAlreadyExistsException there is a file at the path specified
   *                                    or is discovered on one of its ancestors.
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration,long)",3881,3889,"/**
* Initializes a Key object from a URI and configuration.
* @param uri Hadoop URI to process
* @param conf Configuration for the key
* @param unique Unique identifier for the key
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.AbstractFileSystem:getHomeDirectory(),461,472,"/**
* Returns the home directory path for the current user.
* @return Path object representing the user's home directory
*/","* Return the current user's home directory in this file system.
   * The default implementation returns ""/user/$USER/"".
   * 
   * @return current user's home directory.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,openConnection,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",283,335,"/**
* Opens a URL connection with optional authentication and delegation token.
* @param url The URL to connect to
* @param token Optional authentication token
* @param doAs Proxy user ID (null for no proxy)
* @return HttpURLConnection object or null on failure
*/","* Returns an authenticated {@link HttpURLConnection}. If the Delegation
   * Token is present, it will be used taking precedence over the configured
   * <code>Authenticator</code>. If the <code>doAs</code> parameter is not NULL,
   * the request will be done on behalf of the specified <code>doAs</code> user.
   *
   * @param url the URL to connect to. Only HTTP/S URLs are supported.
   * @param token the authentication token being used for the user.
   * @param doAs user to do the the request on behalf of, if NULL the request is
   * as self.
   * @return an authenticated {@link HttpURLConnection}.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,authenticate,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",134,153,"/**
* Authenticates user using provided URL and token.
* @param url URL to authenticate against
* @param token AuthenticatedURL Token
* @throws IOException if authentication fails
* @throws AuthenticationException if authentication exception occurs
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getBestUGI,"org.apache.hadoop.security.UserGroupInformation:getBestUGI(java.lang.String,java.lang.String)",606,615,"/**
* Determines the best UGI based on ticket cache path and user.
* @param ticketCachePath path to ticket cache or null
* @param user username or null
*/","* Find the most appropriate UserGroupInformation to use
   *
   * @param ticketCachePath    The Kerberos ticket cache path, or NULL
   *                           if none is specfied
   * @param user               The user name, or NULL if none is specified.
   *
   * @return                   The most appropriate UserGroupInformation
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,loginUserFromKeytabAndReturnUGI,"org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytabAndReturnUGI(java.lang.String,java.lang.String)",1388,1399,"/**
* Logs in a user from keytab and returns the UGI.
* @param user principal name
* @param path keytab file path
*/","* Log a user in from a keytab file. Loads a user identity from a keytab
   * file and login them in. This new user does not affect the currently
   * logged-in user.
   * @param user the principal name to load from the keytab
   * @param path the path to the keytab file
   * @throws IOException if the keytab file can't be read
   * @return UserGroupInformation.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logAllUserInfo,"org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.slf4j.Logger,org.apache.hadoop.security.UserGroupInformation)",1999,2010,"/**
* Logs all user information to the specified logger.
* @param log target logger
* @param ugi UserGroupInformation object containing user details
*/","* Log all (current, real, login) UGI and token info into specified log.
   * @param ugi - UGI
   * @param log - log.
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,<init>,org.apache.hadoop.security.alias.UserProvider:<init>(),44,47,"/**
 * Initializes the UserProvider instance with current user and credentials.
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsCurrentUser,org.apache.hadoop.security.SecurityUtil:doAsCurrentUser(java.security.PrivilegedExceptionAction),547,550,"/**
* Executes privileged operation as current user.
* @param action Privileged action to execute
* @return result of the action or null if failed
*/","* Perform the given action as the daemon's current user. If an
   * InterruptedException is thrown, it is converted to an IOException.
   *
   * @param action the action to perform
   * @param <T> generic type T.
   * @return the result of the action
   * @throws IOException in the event of error",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,<init>,org.apache.hadoop.security.SaslRpcServer:<init>(org.apache.hadoop.security.SaslRpcServer$AuthMethod),89,120,"/**
* Initializes SaslRpcServer with authentication method.
* @param authMethod authentication protocol (SIMPLE, TOKEN, KERBEROS)",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,create,"org.apache.hadoop.security.SaslRpcServer:create(org.apache.hadoop.ipc.Server$Connection,java.util.Map,org.apache.hadoop.security.token.SecretManager)",122,173,"/**
* Creates a SASL server instance based on authentication method and properties.
* @param connection HBase connection
* @param saslProperties SASL properties map
* @param secretManager Secret manager for token-based authentication
* @return SaslServer object, or null if creation fails
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,<init>,org.apache.hadoop.crypto.key.UserProvider:<init>(org.apache.hadoop.conf.Configuration),47,51,"/**
* Initializes UserProvider with given configuration.
* @param conf Hadoop Configuration instance
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getDoAsUser,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDoAsUser(),1129,1134,"/**
* Retrieves the short username of the proxy user.
* @return short username or null if not a proxy user
*/","* Get the doAs user name.
   *
   * 'actualUGI' is the UGI of the user creating the client
   * It is possible that the creator of the KMSClientProvier
   * calls this method on behalf of a proxyUser (the doAsUser).
   * In which case this call has to be made as the proxy user.
   *
   * @return the doAs user name.
   * @throws IOException",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProtocolProxy,"org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.io.retry.RetryPolicy,long)",411,453,"/**
* Waits for a ProtocolProxy to become available, retrying on connection issues.
* @param protocol Class of the protocol
* @return ProtocolProxy instance or throws IOException if timed out
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param rpcTimeout timeout for each RPC
   * @param connectionRetryPolicy input connectionRetryPolicy.
   * @param timeout time in milliseconds before giving up
   * @return the proxy
   * @throws IOException if the far end through a RemoteException.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,shouldAuthenticateOverKrb,org.apache.hadoop.ipc.Client$Connection:shouldAuthenticateOverKrb(),556,569,"/**
* Determines whether to authenticate over Kerberos based on login user and credentials.
* @throws IOException if an I/O error occurs
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getRestrictParserDefault,org.apache.hadoop.conf.Configuration$Resource:getRestrictParserDefault(java.lang.Object),288,299,"/**
* Determines if the default parser should be restricted based on the resource and current user.
* @param resource object to check for restriction
* @return true if restriction applies, false otherwise
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,run,org.apache.hadoop.ipc.Client$Connection$1:run(),1082,1109,"/**
* Runs the thread, starting and managing IPC parameter sending and response handling.
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doKerberosRelogin,org.apache.hadoop.ipc.Server:doKerberosRelogin(),3407,3425,"/**
* Performs a Kerberos re-login, forcing a ticket refresh.
* @throws IOException on login failure
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,run,org.apache.hadoop.ha.ZKFailoverController:run(java.lang.String[]),173,199,"/**
* Runs the ZK failover controller with given arguments.
* @param args command-line arguments
* @return int result code or ERR_CODE_AUTO_FAILOVER_NOT_ENABLED if auto-failover not enabled
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFCRpcServer.java,cedeActive,org.apache.hadoop.ha.ZKFCRpcServer:cedeActive(int),91,96,"/**
* Cedes active leader role after specified time.
* @param millisToCede milliseconds to wait before ceding
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFCRpcServer.java,gracefulFailover,org.apache.hadoop.ha.ZKFCRpcServer:gracefulFailover(),98,102,"/**
* Initiates a graceful failover to the current leader.
*/",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScanner,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(byte[],byte[])",1113,1117,"/**
* Creates a new scanner instance using custom key range.
* @param beginKey beginning key bytes
* @param endKey ending key bytes
* @return Scanner object or null if creation fails
*/","* Get a scanner that covers a portion of TFile based on keys.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.
     * 
     * @deprecated Use {@link #createScannerByKey(byte[], byte[])} instead.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",488,494,"/**
* Creates a protocol proxy instance for the given protocol class.
* @param protocol Class of the protocol to create a proxy for
* @param clientVersion Client version number
* @param addr Server address
* @param conf Configuration object
* @param factory Socket factory
* @return ProtocolProxy instance or throws IOException if creation fails
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param factory socket factory
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",511,519,"/**
* Retrieves a proxy object for the specified protocol.
* @param protocol class of the protocol to fetch a proxy for
* @param clientVersion version of the client
* @param addr server address
* @param ticket user authentication information
* @param conf configuration settings
* @param factory socket creation factory
* @return T proxy object or null if not found
*/","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   *
   * @param <T> Generics Type T.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param ticket input tocket.
   * @param conf input conf.
   * @param factory input factory.
   * @return the protocol proxy.
   * @throws IOException raised on errors performing I/O.
   *",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,<init>,"org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",46,54,"/**
* Creates a ZKFC protocol client-side translator.
* @param addr server address
* @param conf configuration settings
* @param socketFactory socket factory instance
* @param timeout connection timeout in milliseconds
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,<init>,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",74,82,"/**
* Creates HA service protocol client-side translator.
* @param addr address to connect to
* @param conf configuration object
* @param socketFactory factory for creating sockets
* @param timeout connection timeout in milliseconds
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,doGet,"org.apache.hadoop.log.LogLevel$Servlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",325,360,"/**
* Handles GET requests to set log levels.
* @param request HttpServletRequest object
* @param response HttpServletResponse object
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/AdminAuthorizedServlet.java,doGet,"org.apache.hadoop.http.AdminAuthorizedServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",36,45,"/**
* Authorizes administrator access and delegates to superclass doGet method.
* @throws ServletException if authorization fails
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,isInstrumentationAccessAllowed,"org.apache.hadoop.http.HttpServer2:isInstrumentationAccessAllowed(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",1660,1674,"/**
* Checks if instrumentation access is allowed based on admin privileges.
* @param servletContext Servlet context
* @param request HTTP request
* @param response HTTP response
* @return true if access is allowed, false otherwise
*/","* Checks the user has privileges to access to instrumentation servlets.
   * <p>
   * If <code>hadoop.security.instrumentation.requires.admin</code> is set to FALSE
   * (default value) it always returns TRUE.
   * <p>
   * If <code>hadoop.security.instrumentation.requires.admin</code> is set to TRUE
   * it will check that if the current user is in the admin ACLS. If the user is
   * in the admin ACLs it returns TRUE, otherwise it returns FALSE.
   *
   * @param servletContext the servlet context.
   * @param request the servlet request.
   * @param response the servlet response.
   * @return TRUE/FALSE based on the logic decribed above.
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.DelegateToFileSystem:getHomeDirectory(),169,172,"/**
* Returns the home directory of the file system implementation.
* @return Filesystem's home directory as a Path object.",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getTrashRoot,org.apache.hadoop.fs.FileSystem:getTrashRoot(org.apache.hadoop.fs.Path),3439,3442,"/**
* Returns root directory of user's trash.
* @param path user-specific directory path
*/","* Get the root directory of Trash for current user when the path specified
   * is deleted.
   *
   * @param path the trash root of the path to be determined.
   * @return the default implementation returns {@code /user/$USER/.Trash}",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getTrashRoots,org.apache.hadoop.fs.FileSystem:getTrashRoots(boolean),3452,3478,"/**
* Retrieves trash root directories for a user or all users.
* @param allUsers true to include system-wide trash, false for user-specific
* @return Collection of FileStatus objects representing trash roots
*/","* Get all the trash roots for current user or all users.
   *
   * @param allUsers return trash roots for all users if true.
   * @return all the trash root directories.
   *         Default FileSystem returns .Trash under users' home directories if
   *         {@code /user/$USER/.Trash} exists.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.FilterFileSystem:getHomeDirectory(),297,300,"/**
* Returns the home directory as per file system.
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,access,"org.apache.hadoop.fs.FileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",2840,2844,"/**
* Checks file access permissions.
* @param path HDFS file path
* @param mode requested filesystem action (read/write/delete)
*/","* Checks if the user can access a path.  The mode specifies which access
   * checks to perform.  If the requested permissions are granted, then the
   * method returns normally.  If access is denied, then the method throws an
   * {@link AccessControlException}.
   * <p>
   * The default implementation calls {@link #getFileStatus(Path)}
   * and checks the returned permissions against the requested permissions.
   *
   * Note that the {@link #getFileStatus(Path)} call will be subject to
   * authorization checks.
   * Typically, this requires search (execute) permissions on each directory in
   * the path's prefix, but this is implementation-defined.  Any file system
   * that provides a richer authorization model (such as ACLs) may override the
   * default implementation so that it checks against that model instead.
   * <p>
   * In general, applications should avoid using this method, due to the risk of
   * time-of-check/time-of-use race conditions.  The permissions on a file may
   * change immediately after the access call returns.  Most applications should
   * prefer running specific file system actions as the desired user represented
   * by a {@link UserGroupInformation}.
   *
   * @param path Path to check
   * @param mode type of access to check
   * @throws AccessControlException if access is denied
   * @throws FileNotFoundException if the path does not exist
   * @throws IOException see specific implementation",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,access,"org.apache.hadoop.fs.AbstractFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",1046,1050,"/**
* Checks file system permissions for the given path and action.
* @param path file system path
* @param mode access mode (e.g. read, write)
*/","* The specification of this method matches that of
   * {@link FileContext#access(Path, FsAction)}
   * except that an UnresolvedLinkException may be thrown if a symlink is
   * encountered in the path.
   *
   * @param path the path.
   * @param mode fsaction mode.
   * @throws AccessControlException access control exception.
   * @throws FileNotFoundException file not found exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",392,396,"/**
 * Initializes ViewFileSystem with given URI and configuration.
 * @param theUri file system URI
 * @param conf configuration settings for file system view
 */","* Convenience Constructor for apps to call directly.
   * @param theUri which must be that of ViewFileSystem
   * @param conf conf configuration.
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,<init>,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:<init>(),123,125,"/**
* Initializes the ViewFileSystemOverloadScheme instance.
* Throws IOException on initialization failure.",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,<init>,org.apache.hadoop.fs.viewfs.ViewFs:<init>(org.apache.hadoop.conf.Configuration),213,216,"/**
* Initializes ViewFs with given configuration.
* @param conf Hadoop Configuration object
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",3877,3879,"/**
* Initializes Key object with given URI and configuration.
* @param uri unique identifier
* @param conf Hadoop configuration object
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getUnique,"org.apache.hadoop.fs.FileSystem$Cache:getUnique(java.net.URI,org.apache.hadoop.conf.Configuration)",3671,3674,"/**
* Retrieves a unique file system based on the provided URI and configuration.
* @param uri unique identifier of the file system
* @param conf Hadoop configuration object
* @return FileSystem object or null if not found
*/",The objects inserted into the cache using this method are all unique.,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getHomeDirectory,org.apache.hadoop.fs.viewfs.ChRootedFs:getHomeDirectory(),151,154,"/**
 * Returns the home directory of the file system.
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,<init>,"org.apache.hadoop.fs.FileContext:<init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.conf.Configuration)",243,271,"/**
* Initializes a FileContext with default file system and configuration.
* @param defFs default file system
* @param aConf configuration object
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getHomeDirectory,org.apache.hadoop.fs.FileContext:getHomeDirectory(),577,579,"/**
* Returns the home directory of the default file system.
*/","* Return the current user's home directory in this file system.
   * The default implementation returns ""/user/$USER/"".
   * @return the home directory",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getHomeDirectory,org.apache.hadoop.fs.FilterFs:getHomeDirectory(),83,86,"/**
* Returns the home directory of this file system.
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,openConnection,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",230,235,"/**
* Opens a connection to the specified URL with authentication.
* @param url URL to connect to
* @param token authenticated token for authorization
*/","* Returns an authenticated {@link HttpURLConnection}, it uses a Delegation
   * Token only if the given auth token is an instance of {@link Token} and
   * it contains a Delegation Token, otherwise use the configured
   * {@link DelegationTokenAuthenticator} to authenticate the connection.
   *
   * @param url the URL to connect to. Only HTTP/S URLs are supported.
   * @param token the authentication token being used for the user.
   * @return an authenticated {@link HttpURLConnection}.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,"org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)",268,280,"/**
* Retrieves a Hadoop FileSystem instance using the provided URI and configuration.
* @param uri file system URI
* @param conf Hadoop configuration object
* @param user current user identity
* @return Hadoop FileSystem instance or null on failure
*/","* Get a FileSystem instance based on the uri, the passed in
   * configuration and the user.
   * @param uri of the filesystem
   * @param conf the configuration to use
   * @param user to perform the get as
   * @return the filesystem instance
   * @throws IOException failure to load
   * @throws InterruptedException If the {@code UGI.doAs()} call was
   * somehow interrupted.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstance,"org.apache.hadoop.fs.FileSystem:newInstance(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)",571,583,"/**
* Creates a new Hadoop File System instance using the provided configuration.
* @param uri file system URI
* @param conf Hadoop configuration object
* @param user current user name
* @return FileSystem object or throws an exception if creation fails
*/","* Returns the FileSystem for this URI's scheme and authority and the
   * given user. Internally invokes {@link #newInstance(URI, Configuration)}
   * @param uri uri of the filesystem.
   * @param conf the configuration to use
   * @param user to perform the get as
   * @return filesystem instance
   * @throws IOException if the FileSystem cannot be instantiated.
   * @throws InterruptedException If the {@code UGI.doAs()} call was
   *         somehow interrupted.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getUGIFromTicketCache,"org.apache.hadoop.security.UserGroupInformation:getUGIFromTicketCache(java.lang.String,java.lang.String)",627,638,"/**
* Retrieves UGI from ticket cache if Kerberos is enabled; otherwise,
* falls back to getBestUGI with null credentials.
* @param ticketCache ticket cache string
* @param user username
* @return UserGroupInformation object or null on failure
*/","* Create a UserGroupInformation from a Kerberos ticket cache.
   * 
   * @param user                The principal name to load from the ticket
   *                            cache
   * @param ticketCache     the path to the ticket cache file
   *
   * @throws IOException        if the kerberos login fails
   * @return UserGroupInformation.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,loginFromKeytab,org.apache.hadoop.security.KDiag:loginFromKeytab(),628,657,"/**
* Attempts login using a keytab file and principal.
* @throws IOException if an I/O error occurs
*/","* Log in from a keytab, dump the UGI, validate it, then try and log in again.
   *
   * That second-time login catches JVM/Hadoop compatibility problems.
   * @throws IOException Keytab loading problems",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,loginUserFromKeytab,"org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytab(java.lang.String,java.lang.String)",1127,1147,"/**
* Logs in user from a keytab file.
* @param user username
* @param path keytab file path
*/","* Log a user in from a keytab file. Loads a user identity from a keytab
   * file and logs them in. They become the currently logged-in user.
   * @param user the principal name to load from the keytab
   * @param path the path to the keytab file
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if it's a kerberos login exception.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logAllUserInfo,org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.apache.hadoop.security.UserGroupInformation),2017,2020,"/**
* Logs all user information to the specified log.
* @param ugi UserGroupInformation object containing user credentials and group membership
*/","* Log all (current, real, login) UGI and token info into UGI debug log.
   * @param ugi - UGI
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getActualUgi,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getActualUgi(),1167,1190,"/**
* Returns the actual UGI (User Group Information) to use based on current security context.
* @return UserGroupInformation object representing the actual user
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,buildNegotiateResponse,org.apache.hadoop.ipc.Server:buildNegotiateResponse(java.util.List),3445,3467,"/**
* Builds an RPC SASL negotiate response based on provided authentication methods.
* @param authMethods list of supported authentication methods
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,createSaslServer,org.apache.hadoop.ipc.Server$Connection:createSaslServer(org.apache.hadoop.security.SaslRpcServer$AuthMethod),2621,2626,"/**
* Creates a SASL server instance based on the specified authentication method.
* @param authMethod selected authentication method
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProtocolProxy,"org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)",366,372,"/**
* Waits for a ProtocolProxy instance with specified protocol and client version.
* @param protocol target protocol class
* @param clientVersion expected client version
* @param addr server address to connect to
* @param conf configuration options
* @param connTimeout connection timeout in milliseconds
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param connTimeout time in milliseconds before giving up
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProxy,"org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,long)",387,394,"/**
* Retrieves a proxy instance for the specified protocol.
* @param protocol Class of the protocol to fetch
* @param clientVersion Client version associated with this proxy
* @param addr Address to connect to
* @param conf Configuration settings for this proxy
* @param rpcTimeout RPC timeout value
* @param timeout overall timeout value
* @return Proxy instance or null on failure
*/","* Get a proxy connection to a remote server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param rpcTimeout timeout for each RPC
   * @param timeout time in milliseconds before giving up
   * @return the proxy
   * @throws IOException if the far end through a RemoteException",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,java.lang.String)",261,263,"/**
* Constructs a new Resource instance with default restrictions.
* @param resource underlying object
* @param name identifier string
*/",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",467,473,"/**
* Retrieves a proxy object for the specified protocol.
* @param protocol Class of the protocol to use
* @param clientVersion Client version identifier
* @param addr Socket address to connect to
* @param conf Configuration settings
* @param factory Socket factory to create connections
* @return Proxy object or throws IOException if creation fails.","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   * @param <T> Generics Type T.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param conf input Configuration.
   * @param factory input factory.
   * @throws IOException raised on errors performing I/O.
   * @return proxy.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",773,780,"/**
* Retrieves a proxy for the specified network protocol.
* @param protocol Class of the protocol to fetch a proxy for
* @param clientVersion Client version number
* @param addr Network address to connect to
* @param conf Configuration object for the connection
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server
   * 
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param conf input configuration.
   * @param <T> Generics Type T.
   * @return a protocol proxy
   * @throws IOException if the thread is interrupted.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,getUgmProtocol,org.apache.hadoop.tools.GetGroupsBase:getUgmProtocol(),97,105,"/**
* Retrieves the protocol instance for user group mappings.
*/","* Get a client of the {@link GetUserMappingsProtocol}.
   * @return A {@link GetUserMappingsProtocol} client proxy.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getZKFCProxy,"org.apache.hadoop.ha.HAServiceTarget:getZKFCProxy(org.apache.hadoop.conf.Configuration,int)",164,173,"/**
* Retrieves a ZooKeeper FC proxy instance.
* @param timeoutMs connection timeout in milliseconds
* @return ZKFCProtocol object or throws IOException on failure
*/","* @return a proxy to the ZKFC which is associated with this HA service.
   * @param conf configuration.
   * @param timeoutMs timeout in milliseconds.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getProxyForAddress,"org.apache.hadoop.ha.HAServiceTarget:getProxyForAddress(org.apache.hadoop.conf.Configuration,int,int,java.net.InetSocketAddress)",146,156,"/**
* Retrieves a proxy for the specified address with custom configuration and timeouts.
* @param conf Configuration object
* @param timeoutMs connection timeout in milliseconds
* @param retries max number of connection retries
* @param addr InetSocketAddress to fetch proxy for
* @return HAServiceProtocol instance or throws IOException if failed
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,isInstrumentationAccessAllowed,"org.apache.hadoop.jmx.JMXJsonServlet:isInstrumentationAccessAllowed(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",152,156,"/**
* Checks if instrumentation access is allowed based on HTTP request and response.
* @param request incoming HTTP request
* @param response outgoing HTTP response
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,doGet,"org.apache.hadoop.http.HttpServer2$StackServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",1745,1758,"/**
* Handles GET requests by printing thread information to the response.
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileOutputServlet.java,doGet,"org.apache.hadoop.http.ProfileOutputServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",48,77,"/**
* Handles GET requests and enforces instrumentation access restrictions.
* @throws ServletException if authorization fails
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,doGet,"org.apache.hadoop.http.ProfileServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",187,327,"/**
* Handles HTTP GET requests for profiling. Checks instrumentation access, 
* environment variables, and process ID before launching the async profiler.
* @param req HttpServletRequest
* @param resp HttpServletResponse
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfServlet.java,doGet,"org.apache.hadoop.conf.ConfServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",57,83,"/**
* Handles GET requests by fetching and writing user configuration data.
* @param request HTTP request object
* @param response HTTP response object
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,moveToTrash,org.apache.hadoop.fs.TrashPolicyDefault:moveToTrash(org.apache.hadoop.fs.Path),129,204,"/**
* Moves a file or directory to the trash.
* @param path Path of the item to move
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,getCurrentTrashDir,org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir(),241,244,"/**
* Returns current trash directory path.
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,getCurrentTrashDir,org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir(org.apache.hadoop.fs.Path),246,249,"/**
* Returns the current trash directory of the given file system.
* @param path file system path
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getTrashRoot,org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoot(org.apache.hadoop.fs.Path),1180,1220,"/**
* Resolves the trash root path for a given file system and user.
* @param path input file system path
*/","* Get the trash root directory for current user when the path
   * specified is deleted.
   *
   * If FORCE_INSIDE_MOUNT_POINT flag is not set, return the default trash root
   * from targetFS.
   *
   * When FORCE_INSIDE_MOUNT_POINT is set to true,
   * <ol>
   *   <li>
   *     If the trash root for path p is in the same mount point as path p,
   *       and one of:
   *       <ol>
   *         <li>The mount point isn't at the top of the target fs.</li>
   *         <li>The resolved path of path is root (in fallback FS).</li>
   *         <li>The trash isn't in user's target fs home directory
   *            get the corresponding viewFS path for the trash root and return
   *            it.
   *         </li>
   *       </ol>
   *   </li>
   *   <li>
   *     else, return the trash root under the root of the mount point
   *     (/{mntpoint}/.Trash/{user}).
   *   </li>
   * </ol>
   *
   * These conditions handle several different important cases:
   * <ul>
   *   <li>File systems may need to have more local trash roots, such as
   *         encryption zones or snapshot roots.</li>
   *   <li>The fallback mount should use the user's home directory.</li>
   *   <li>Cloud storage systems should not use trash in an implicity defined
   *        home directory, per a container, unless it is the fallback fs.</li>
   * </ul>
   *
   * @param path the trash root of the path to be determined.
   * @return the trash root path.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getTrashRoot,org.apache.hadoop.fs.FilterFileSystem:getTrashRoot(org.apache.hadoop.fs.Path),689,692,"/**
* Returns the root of the trash directory.
* @param path current file system path context
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,run,org.apache.hadoop.fs.TrashPolicyDefault$Emptier:run(),278,320,"/**
* Periodically empties trash directories based on the configured interval.
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,createCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(java.util.Date),212,220,"/**
* Creates a checkpoint at the specified date for all trash roots.
* @param date the date to create the checkpoint for
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(boolean),232,239,"/**
* Recursively deletes checkpoint files based on Trash Policy.
* @param deleteImmediately flag to force immediate deletion
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getTrashRoots,org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoots(boolean),1231,1297,"/**
* Retrieves all trash roots from target file systems and view file system.
* @param allUsers whether to include user-specific trash or not
* @return a collection of FileStatus objects representing trash roots
*/","* Get all the trash roots for current user or all users.
   *
   * When FORCE_INSIDE_MOUNT_POINT is set to true, we also return trash roots
   * under the root of each mount point, with their viewFS paths.
   *
   * @param allUsers return trash roots for all users if true.
   * @return all Trash root directories.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getTrashRoots,org.apache.hadoop.fs.FilterFileSystem:getTrashRoots(boolean),694,697,"/**
* Retrieves trash roots from the file system.
* @param allUsers whether to include all users' trash roots
* @return Collection of FileStatus objects or empty if no trash found
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,testAccess,"org.apache.hadoop.fs.shell.Test:testAccess(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.permission.FsAction)",110,118,"/**
* Checks file system access for a given PathData item.
* @param item PathData object containing fs and path
* @param action FsAction to perform on the path
* @return true if access is granted, false otherwise
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,access,"org.apache.hadoop.fs.viewfs.ViewFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",576,582,"/**
* Grants or denies access to a file system path.
* @param path the file system path
* @param mode the desired action (read, write, etc.)
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,access,"org.apache.hadoop.fs.FilterFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",470,474,"/**
* Verifies file system access permission for the specified path.
* @param path file system path to check
* @param mode desired access mode (read, write, execute)
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,access,"org.apache.hadoop.fs.viewfs.ChRootedFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",208,211,"/**
* Performs file system access operation on specified path.
* @param path filesystem path
* @param mode access mode (read, write, execute)
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,access,"org.apache.hadoop.fs.viewfs.ViewFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",428,434,"/**
* Performs access control on the specified file system path.
* @param path the target file system path
* @param mode the desired access mode (read, write, etc.)
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,access,"org.apache.hadoop.fs.FilterFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",132,137,"/**
* Performs file system access control on a given Path.
* @param path the file system path to operate on
* @param mode the desired FsAction (e.g. READ, WRITE)
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(org.apache.hadoop.conf.Configuration),403,405,"/**
* Constructs a ViewFileSystem instance.
* @param conf Hadoop configuration object
*/","* Convenience Constructor for apps to call directly.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,addFileSystemForTesting,"org.apache.hadoop.fs.FileSystem:addFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",240,244,"/**
* Adds a file system to test cache.
* @param uri URI of the file system
* @param conf configuration for the file system
* @param fs file system object to be cached
*/","* This method adds a FileSystem instance to the cache so that it can
   * be retrieved later. It is only for testing.
   * @param uri the uri to store it under
   * @param conf the configuration to store it under
   * @param fs the FileSystem to store
   * @throws IOException if the current user cannot be determined.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,removeFileSystemForTesting,"org.apache.hadoop.fs.FileSystem:removeFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",246,250,"/**
* Removes file system from cache by URI and configuration.
* @param uri unique identifier of the file system
* @param conf configuration object associated with the file system
* @param fs file system instance to be removed
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,"org.apache.hadoop.fs.FileSystem$Cache:get(java.net.URI,org.apache.hadoop.conf.Configuration)",3665,3668,"/**
* Retrieves a FileSystem object based on the provided URI and configuration.
* @param uri file system location (e.g., HDFS path)
* @param conf configuration parameters for the file system
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstance,"org.apache.hadoop.fs.FileSystem:newInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",594,611,"/**
* Creates a new FileSystem instance based on the provided URI and configuration.
* @param uri file system URI (e.g., ""hdfs://..."")
* @param config configuration object
* @return new FileSystem instance or null if not found
*/","* Returns the FileSystem for this URI's scheme and authority.
   * The entire URI is passed to the FileSystem instance's initialize method.
   * This always returns a new FileSystem object.
   * @param uri FS URI
   * @param config configuration to use
   * @return the new FS instance
   * @throws IOException FS creation or initialization failure.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,"org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.conf.Configuration)",373,376,"/**
* Creates and returns a FileContext instance from the given file system and configuration.
* @param defFS default file system to use
* @param aConf configuration settings for the context
*/","* Create a FileContext with specified FS as default using the specified
   * config.
   * 
   * @param defFS default fs.
   * @param aConf configutration.
   * @return new FileContext with specified FS as default.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,openConnection,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",230,235,"/**
* Opens an HTTP connection to the specified URL with the provided authentication token.
* @param url target URL
* @param token authenticated token for authorization
*/","* Returns an authenticated {@link HttpURLConnection}, it uses a Delegation
   * Token only if the given auth token is an instance of {@link Token} and
   * it contains a Delegation Token, otherwise use the configured
   * {@link DelegationTokenAuthenticator} to authenticate the connection.
   *
   * @param url the URL to connect to. Only HTTP/S URLs are supported.
   * @param token the authentication token being used for the user.
   * @return an authenticated {@link HttpURLConnection}.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,execute,org.apache.hadoop.security.KDiag:execute(),282,420,"/**
* Performs Hadoop Kerberos diagnostics, including checking configuration options and authentication setup.
* @throws Exception if any diagnostic fails
*/","* Execute diagnostics.
   * <p>
   * Things it would be nice if UGI made accessible
   * <ol>
   *   <li>A way to enable JAAS debug programatically</li>
   *   <li>Access to the TGT</li>
   * </ol>
   * @return true if security was enabled and all probes were successful
   * @throws KerberosDiagsFailure explicitly raised failure
   * @throws Exception other security problems",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,maybeDoLoginFromKeytabAndPrincipal,org.apache.hadoop.security.token.DtUtilShell:maybeDoLoginFromKeytabAndPrincipal(java.lang.String[]),82,106,"/**
* Parses command line arguments for login from keytab and principal,
* performing Kerberos login if both are provided.
*@param args command line arguments
*@return parsed argument list or original list if no login performed
*/","* Parse arguments looking for Kerberos keytab/principal.
   * If both are found: remove both from the argument list and attempt login.
   * If only one of the two is found: remove it from argument list, log warning
   * and do not attempt login.
   * If neither is found: return original args array, doing nothing.
   * Return the pruned args array if either flag is present.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,main,org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[]),2300,2318,"/**
* Demonstrates user authentication and grouping using UserGroupInformation.
* Prints current UGI details, then authenticates from keytab if provided command-line args.
* @param [0] Keytab file path
* @param [1] Username
*/","* A test method to print out the current user's UGI.
   * @param args if there are two arguments, read the user from the keytab
   * and print it out.
   * @throws Exception Exception.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,login,"org.apache.hadoop.security.SecurityUtil:login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)",309,329,"/**
* Logs in a user using the provided keytab file and username.
* @param conf Hadoop configuration
* @param keytabFileKey Key for keytab file location
* @param userNameKey Key for principal name (or system property 'user.name')
* @throws IOException if login fails","* Login as a principal specified in config. Substitute $host in user's Kerberos principal 
   * name with hostname. If non-secure mode - return. If no keytab available -
   * bail out with an exception
   * 
   * @param conf
   *          conf to use
   * @param keytabFileKey
   *          the key to look for keytab file in conf
   * @param userNameKey
   *          the key to look for user's Kerberos principal name in conf
   * @param hostname
   *          hostname to use for substitution
   * @throws IOException if the config doesn't specify a keytab",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createConnection,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createConnection(java.net.URL,java.lang.String)",502,537,"/**
* Creates an HTTP connection to the specified URL using the given request method.
* @param url target URL
* @param method HTTP request method (e.g. GET, POST, PUT)
* @return HttpURLConnection object or throws IOException if failed
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDelegationToken(java.lang.String),1025,1059,"/**
* Retrieves a delegation token for the specified renewer.
* @param renewer user ID of the token's owner
* @return Token object or null if not obtained
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,renewDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token),1061,1087,"/**
* Revives a delegation token by renewing it on the configured system.
* @param dToken Token to be renewed
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,cancelDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token),1089,1116,"/**
* Cancels a delegation token.
* @param dToken Token to cancel
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)",3307,3405,"/**
* Initializes a Server instance with specified configuration.
* @param bindAddress server address
* @param port server port
* @param rpcRequestClass RPC request class
* @param handlerCount number of handlers
* @param numReaders number of readers or -1 for default
* @param queueSizePerHandler queue size per handler or -1 for default
* @param conf configuration object
* @param serverName server name
* @param secretManager secret manager
* @param portRangeConfig port range configuration
*/","* Constructs a server listening on the named port and address.  Parameters passed must
   * be of the named class.  The <code>handlerCount</code> determines
   * the number of handler threads that will be used to process calls.
   * If queueSizePerHandler or numReaders are not -1 they will be used instead of parameters
   * from configuration. Otherwise the configuration will be picked up.
   * 
   * If rpcRequestClass is null then the rpcRequestClass must have been 
   * registered via {@link #registerProtocolEngine(RPC.RpcKind,
   *  Class, RPC.RpcInvoker)}
   * This parameter has been retained for compatibility with existing tests
   * and usage.
   *
   * @param bindAddress input bindAddress.
   * @param port input port.
   * @param rpcRequestClass input rpcRequestClass.
   * @param handlerCount input handlerCount.
   * @param numReaders input numReaders.
   * @param queueSizePerHandler input queueSizePerHandler.
   * @param conf input Configuration.
   * @param serverName input serverName.
   * @param secretManager input secretManager.
   * @param portRangeConfig input portRangeConfig.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,buildSaslNegotiateResponse,org.apache.hadoop.ipc.Server$Connection:buildSaslNegotiateResponse(),2603,2619,"/**
* Builds SASL negotiate response with optional token challenge.
* @return RpcSaslProto object
*/","* Process the Sasl's Negotiate request, including the optimization of 
     * accelerating token negotiation.
     * @return the response to Negotiate request - the list of enabled 
     *         authMethods and challenge if the TOKENS are supported. 
     * @throws SaslException - if attempt to generate challenge fails.
     * @throws IOException - if it fails to create the SASL server for Tokens",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProtocolProxy,"org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",326,332,"/**
* Waits for a protocol proxy instance with specified configuration.
* @param protocol Class of the protocol to wait for
* @param clientVersion Expected client version
* @param addr Address to connect to
* @param conf Configuration options
*/","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProxy,"org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)",346,351,"/**
* Waits for a proxy to become available and returns it.
* @param protocol the protocol class
* @param clientVersion the client version
* @param addr the server address
* @param conf the configuration
* @param connTimeout connection timeout in milliseconds
* @return T an instance of the desired type, or throws IOException if not found
*/","* Get a proxy connection to a remote server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param connTimeout time in milliseconds before giving up
   * @return the proxy
   * @throws IOException if the far end through a RemoteException",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,java.lang.String)",998,1000,"/**
* Adds a new resource to the system.
* @param in input stream containing resource data
* @param name unique identifier for the resource
*/","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param in InputStream to deserialize the object from.
   * @param name the name of the resource because InputStream.toString is not
   * very descriptive some times.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object),253,255,"/**
* Constructs a new Resource instance from an arbitrary Object.
* The Object's toString representation is used to initialize the Resource. 
* @param resource any object to be wrapped as a Resource
*/",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",728,734,"/**
* Retrieves a proxy object for the specified protocol and address.
* @param protocol Class of the protocol to fetch proxy for
* @param clientVersion Client version number
* @param addr Address to connect to
* @param conf Configuration settings
* @return Proxy object or null if failed
*/","* Construct a client-side proxy object with the default SocketFactory.
    *
    * @param <T> Generics Type T.
    * @param protocol input protocol.
    * @param clientVersion input clientVersion.
    * @param addr input addr.
    * @param conf input Configuration.
    * @return a proxy instance
    * @throws IOException  if the thread is interrupted.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,run,org.apache.hadoop.tools.GetGroupsBase:run(java.lang.String[]),62,79,"/**
* Prints user/group membership information.
* @param args array of usernames to display
*/","* Get the groups for the users given and print formatted output to the
   * {@link PrintStream} configured earlier.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,gracefulFailoverThroughZKFCs,org.apache.hadoop.ha.HAAdmin:gracefulFailoverThroughZKFCs(org.apache.hadoop.ha.HAServiceTarget),276,290,"/**
* Performs graceful failover through ZooKeeper Federation Controllers.
* @param toNode HAServiceTarget node to fail over to
* @return 0 on success, -1 on failure","* Initiate a graceful failover by talking to the target node's ZKFC.
   * This sends an RPC to the ZKFC, which coordinates the failover.
   *
   * @param toNode the node to fail to
   * @return status code (0 for success)
   * @throws IOException if failover does not succeed",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,cedeRemoteActive,"org.apache.hadoop.ha.ZKFailoverController:cedeRemoteActive(org.apache.hadoop.ha.HAServiceTarget,int)",749,756,"/**
* Requests remote HA service target to cede its active state.
* @param remote HAServiceTarget instance
* @param timeout time in milliseconds for the ceding operation
* @return ZKFCProtocol proxy or null if failed
*/","* Ask the remote zkfc to cede its active status and wait for the specified
   * timeout before attempting to claim leader status.
   * @param remote node to ask
   * @param timeout amount of time to cede
   * @return the {@link ZKFCProtocol} used to talk to the ndoe
   * @throws IOException",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getHealthMonitorProxy,"org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int,int)",131,138,"/**
* Retrieves a health monitor proxy instance based on configuration and timeouts.
* @param conf HA service configuration
* @param timeoutMs connection timeout in milliseconds
* @param retries maximum number of retries
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getProxyForAddress,"org.apache.hadoop.ha.HAServiceTarget:getProxyForAddress(org.apache.hadoop.conf.Configuration,int,java.net.InetSocketAddress)",140,144,"/**
* Reduces connection timeout for address-based proxy retrieval.
* @param conf configuration object
* @param timeoutMs initial timeout in milliseconds
* @param addr InetSocketAddress to fetch proxy for
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,doGet,"org.apache.hadoop.jmx.JMXJsonServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",175,232,"/**
* Handles GET requests for MBean attributes.
* @param request HTTP request object
* @param response HTTP response object
*/","* Process a GET request for the specified resource.
   * 
   * @param request
   *          The servlet request we are processing
   * @param response
   *          The servlet response we are creating",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,createCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(),206,210,"/**
 * Creates a checkpoint with the current date and time.
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(),222,225,"/**
 * Deletes the current checkpoint.
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpointsImmediately,org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpointsImmediately(),227,230,"/**
 * Immediately deletes all checkpoints.
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,processPath,org.apache.hadoop.fs.shell.Test:processPath(org.apache.hadoop.fs.shell.PathData),77,108,"/**
* Evaluates the file system path based on the specified flag.
* @param item PathData object containing file attributes
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,access,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",253,257,"/**
* Performs file system access operation on the specified path.
* @param path target file system path
* @param mode file system access action (read, write, execute)
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,"org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration)",536,558,"/**
* Resolves a URI to a FileSystem instance, using caching and fallbacks.
* @param uri input URI
* @param conf Configuration object
* @return FileSystem instance or null if not found
*/","* Get a FileSystem for this URI's scheme and authority.
   * <ol>
   * <li>
   *   If the configuration has the property
   *   {@code ""fs.$SCHEME.impl.disable.cache""} set to true,
   *   a new instance will be created, initialized with the supplied URI and
   *   configuration, then returned without being cached.
   * </li>
   * <li>
   *   If the there is a cached FS instance matching the same URI, it will
   *   be returned.
   * </li>
   * <li>
   *   Otherwise: a new FS instance will be created, initialized with the
   *   configuration and URI, cached and returned to the caller.
   * </li>
   * </ol>
   * @param uri uri of the filesystem.
   * @param conf configrution.
   * @return filesystem instance.
   * @throws IOException if the FileSystem cannot be instantiated.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstanceLocal,org.apache.hadoop.fs.FileSystem:newInstanceLocal(org.apache.hadoop.conf.Configuration),631,634,"/**
* Creates a new instance of LocalFileSystem.
* @param conf Configuration object
*/","* Get a unique local FileSystem object.
   * @param conf the configuration to configure the FileSystem with
   * @return a new LocalFileSystem object.
   * @throws IOException FS creation or initialization failure.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/FsGetter.java,getNewInstance,"org.apache.hadoop.fs.viewfs.FsGetter:getNewInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",42,45,"/**
* Creates a new Hadoop file system instance.
* @param uri URI of the file system
* @param conf Hadoop configuration
*/","* Gets new file system instance of given uri.
   * @param uri uri.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return file system.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getNewInstance,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:getNewInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",234,250,"/**
* Initializes a new file system instance based on the given URI and configuration.
* @param uri URI object representing the file system location
* @param conf Configuration object for the file system
* @return Newly initialized file system or null if not created
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatusForFallbackLink,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatusForFallbackLink(),1244,1265,"/**
* Lists file statuses for a fallback link.
* @return array of FileStatus objects or empty if not applicable
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem),385,388,"/**
* Returns a FileContext instance based on the provided default file system.
* @param defaultFS the default file system to use
*/","* Create a FileContext for specified file system using the default config.
   * 
   * @param defaultFS default fs.
   * @return a FileContext with the specified AbstractFileSystem
   *                 as the default FS.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,"org.apache.hadoop.fs.FileContext:getFileContext(java.net.URI,org.apache.hadoop.conf.Configuration)",456,473,"/**
* Retrieves FileContext based on default file system URI and configuration.
* @param defaultFsUri default file system URI
* @param aConf Hadoop Configuration object
* @return FileContext instance or throws UnsupportedFileSystemException if not found
*/","* Create a FileContext for specified default URI using the specified config.
   * 
   * @param defaultFsUri defaultFsUri.
   * @param aConf configrution.
   * @return new FileContext for specified uri
   * @throws UnsupportedFileSystemException If the file system with specified is
   *           not supported
   * @throws RuntimeException If the file system specified is supported but
   *         could not be instantiated, or if login fails.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,run,org.apache.hadoop.security.KDiag:run(java.lang.String[]),197,244,"/**
* Runs the command with given arguments, configuring and executing it.
* @param argv array of command-line arguments
* @return exit code (0 for success, non-zero for failure)
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,init,org.apache.hadoop.security.token.DtUtilShell:init(java.lang.String[]),116,177,"/**
* Initializes application based on command-line arguments.
* @param args array of string arguments
*/","* Parse the command line arguments and initialize subcommand.
   * Also will attempt to perform Kerberos login if both -principal and -keytab
   * flags are passed in args array.
   * @param args args.
   * @return 0 if the argument(s) were recognized, 1 otherwise
   * @throws Exception Exception.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,login,"org.apache.hadoop.security.SecurityUtil:login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",287,292,"/**
* Logs in to Hadoop cluster using Kerberos credentials.
* @param conf Hadoop configuration
* @param keytabFileKey Keytab file key
* @param userNameKey User name key
*/","* Login as a principal specified in config. Substitute $host in
   * user's Kerberos principal name with a dynamically looked-up fully-qualified
   * domain name of the current host.
   * 
   * @param conf
   *          conf to use
   * @param keytabFileKey
   *          the key to look for keytab file in conf
   * @param userNameKey
   *          the key to look for user's Kerberos principal name in conf
   * @throws IOException if login fails",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,call,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class,int)",544,608,"/**
* Makes an HTTP request to the specified URL with optional JSON payload.
* @param jsonOutput JSON data to be sent in the request body
* @param expectedResponse expected HTTP response code
* @param klass Class of the object to be deserialized from the response
* @param authRetryCount number of times to retry authentication (0 for no retries)
* @return deserialized object or null if not found
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,<init>,"org.apache.hadoop.ipc.RPC$Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)",1189,1198,"/**
* Initializes a Server instance with specified configuration.
* @param bindAddress network address to bind to
* @param port TCP port number
* @param paramClass Writable class for parameter serialization
* @param handlerCount number of worker handlers
* @param numReaders number of readers
* @param queueSizePerHandler output queue size per handler
* @param conf configuration object
* @param serverName unique server identifier
* @param secretManager secret manager instance
* @param portRangeConfig port range configuration string
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,org.apache.hadoop.conf.Configuration)",3264,3271,"/**
* Initializes a new Server instance with default configuration.
* @param bindAddress server address to bind to
* @param port server listening port number
* @param paramClass class of input parameters
* @param handlerCount number of worker handlers
* @param conf server configuration
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager)",3273,3280,"/**
* Constructs a new Server instance.
* @param bindAddress address to bind the server on
* @param port server listen port
* @param rpcRequestClass class of RPC requests handled by this server
*/",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processSaslMessage,org.apache.hadoop.ipc.Server$Connection:processSaslMessage(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto),2327,2385,"/**
* Processes SASL message and returns response.
* @param saslMessage SASL protocol message
* @return SASL protocol response or null if no response needed
*/","* Process a saslMessge.
     * @param saslMessage received SASL message
     * @return the sasl response to send back to client
     * @throws SaslException if authentication or generating response fails, 
     *                       or SASL protocol mixup
     * @throws IOException if a SaslServer cannot be created
     * @throws AccessControlException if the requested authentication type 
     *         is not supported or trying to re-attempt negotiation.
     * @throws InterruptedException",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProxy,"org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",305,312,"/**
* Returns a proxy object for the specified protocol and address.
* @param protocol class of the desired protocol
* @param clientVersion version of the client to use
* @param addr server address to connect to
* @param conf configuration settings for the connection
* @return Proxy object or throws IOException on failure
*/","* Get a proxy connection to a remote server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @return the proxy
   * @throws IOException if the far end through a RemoteException",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(java.lang.String),923,925,"/**
* Adds a new resource with the specified name.
* @param name unique resource identifier
*/","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param name resource to be added, the classpath is examined for a file 
   *             with that name.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(java.net.URL),941,943,"/**
* Adds a resource by URL.
* @param url unique resource identifier
*/","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param url url of the resource to be added, the local filesystem is 
   *            examined directly to find the resource, without referring to 
   *            the classpath.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.fs.Path),959,961,"/**
* Adds a resource to the collection by creating a new Resource object from the provided file path.
* @param file Path to the file representing the resource
*/","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param file file-path of resource to be added, the local filesystem is
   *             examined directly to find the resource, without referring to 
   *             the classpath.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream),980,982,"/**
* Adds resource to system from input stream.
* @param in InputStream containing resource data
*/","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * WARNING: The contents of the InputStream will be cached, by this method. 
   * So use this sparingly because it does increase the memory consumption.
   * 
   * @param in InputStream to deserialize the object from. In will be read from
   * when a get or set is called next.  After it is read the stream will be
   * closed.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,<init>,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",66,72,"/**
* Initializes an HA service protocol client-side translator.
* @param addr server address
* @param conf configuration settings
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doGracefulFailover,org.apache.hadoop.ha.ZKFailoverController:doGracefulFailover(),660,739,"/**
* Performs a graceful failover by yielding the current active node and 
* waiting for the local node to become active.
*/","* Coordinate a graceful failover. This proceeds in several phases:
   * 1) Pre-flight checks: ensure that the local node is healthy, and
   * thus a candidate for failover.
   * 2a) Determine the current active node. If it is the local node, no
   * need to failover - return success.
   * 2b) Get the other nodes
   * 3a) Ask the other nodes to yield from election for a number of seconds
   * 3b) Ask the active node to yield from the election for a number of seconds.
   * 4) Allow the normal election path to run in other threads. Wait until
   * we either become unhealthy or we see an election attempt recorded by
   * the normal code path.
   * 5) Allow the old active to rejoin the election, so a future
   * failback is possible.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,createProxy,org.apache.hadoop.ha.HealthMonitor:createProxy(),191,193,"/**
* Creates an HA service proxy instance.
* @return HAServiceProtocol object
*/","* Connect to the service to be monitored. Stubbed out for easier testing.
   *
   * @throws IOException raised on errors performing I/O.
   * @return HAServiceProtocol.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getHealthMonitorProxy,"org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int)",126,129,"/**
* Returns a health monitor proxy instance with default priority.
* @param conf Configuration object
* @param timeoutMs Timeout value in milliseconds
*/","* Returns a proxy to connect to the target HA service for health monitoring.
   * If {@link #getHealthMonitorAddress()} is implemented to return a non-null
   * address, then this proxy will connect to that address.  Otherwise, the
   * returned proxy defaults to using {@link #getAddress()}, which means this
   * method's behavior is identical to {@link #getProxy(Configuration, int)}.
   *
   * @param conf configuration.
   * @param timeoutMs timeout in milliseconds
   * @return a proxy to connect to the target HA service for health monitoring
   * @throws IOException if there is an error",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getProxy,"org.apache.hadoop.ha.HAServiceTarget:getProxy(org.apache.hadoop.conf.Configuration,int)",100,103,"/**
* Retrieves HAServiceProtocol instance using configuration and timeout.
* @param conf Configuration object
* @param timeoutMs connection timeout in milliseconds
* @return HAServiceProtocol instance or null if failed
*/","* @return a proxy to connect to the target HA Service.
   * @param timeoutMs timeout in milliseconds.
   * @param conf Configuration.
   * @throws IOException raised on errors performing I/O.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,initialize,"org.apache.hadoop.fs.HarFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",128,175,"/**
* Initializes the Har Filesystem with the given URI and configuration.
* @param name URI of the Har Filesystem
* @param conf Hadoop Configuration object
* @throws IOException if an error occurs during initialization
*/","* Initialize a Har filesystem per har archive. The 
   * archive home directory is the top level directory
   * in the filesystem that contains the HAR archive.
   * Be careful with this method, you do not want to go 
   * on creating new Filesystem instances per call to 
   * path.getFileSystem().
   * the uri of Har is 
   * har://underlyingfsscheme-host:port/archivepath.
   * or 
   * har:///archivepath. This assumes the underlying filesystem
   * to be used in case not specified.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,moveToAppropriateTrash,"org.apache.hadoop.fs.Trash:moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",77,122,"/**
* Moves a file to its appropriate trash location.
* @param fs the FileSystem instance
* @param p the Path of the file to move
* @param conf the Configuration object
* @return true if the file was moved successfully, false otherwise
*/","* In case of the symlinks or mount points, one has to move the appropriate
   * trashbin in the actual volume of the path p being deleted.
   *
   * Hence we get the file system of the fully-qualified resolved-path and
   * then move the path p to the trashbin in that volume,
   * @param fs - the filesystem of path p
   * @param p - the path being deleted - to be moved to trash
   * @param conf - configuration
   * @return false if the item is already in the trash or trash is disabled
   * @throws IOException on error",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlConnection.java,connect,org.apache.hadoop.fs.FsUrlConnection:connect(),55,75,"/**
* Establishes a connection to the specified file system.
* @throws IOException if an I/O error occurs during connection
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",88,90,"/**
* Constructs a PathData object from a string representation of a file system path.
* @param pathString string representation of the file system path
* @param conf Hadoop Configuration instance used to resolve URI and other settings
*/","* Creates an object to wrap the given parameters as fields.  The string
   * used to create the path will be recorded since the Path object does not
   * return exactly the same string used to initialize it
   * @param pathString a string for a path
   * @param conf the configuration file
   * @throws IOException if anything goes wrong...",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFSofPath,"org.apache.hadoop.fs.FileSystem:getFSofPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",427,435,"/**
* Retrieves a FileSystem instance for the specified absolute or fully-qualified path.
* @param absOrFqPath absolute or fully-qualified file system path
* @param conf Hadoop Configuration object
* @return FileSystem instance, throws exceptions if not found
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getNamed,"org.apache.hadoop.fs.FileSystem:getNamed(java.lang.String,org.apache.hadoop.conf.Configuration)",477,481,"/**
* Retrieves a named file system based on the provided configuration.
* @param name unique identifier for the file system
* @param conf Configuration object for file system settings
*/","* @deprecated call {@link #get(URI, Configuration)} instead.
   *
   * @param name name.
   * @param conf configuration.
   * @return file system.
   * @throws IOException If an I/O error occurred.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getLocal,org.apache.hadoop.fs.FileSystem:getLocal(org.apache.hadoop.conf.Configuration),508,511,"/**
 * Retrieves a local file system instance from configuration.
 * @param conf Hadoop Configuration object
 */","* Get the local FileSystem.
   * @param conf the configuration to configure the FileSystem with
   * if it is newly instantiated.
   * @return a LocalFileSystem
   * @throws IOException if somehow the local FS cannot be instantiated.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",124,127,"/**
* Creates a new instance of ChRootedFileSystem from the given URI and configuration.
* @param uri file system URI
* @param conf Hadoop configuration
*/","* Constructor.
   * @param uri base file system
   * @param conf configuration
   * @throws IOException",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/FsGetter.java,get,"org.apache.hadoop.fs.viewfs.FsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)",55,57,"/**
* Creates and returns a file system object based on provided URI and configuration.
* @param uri unique identifier of the file system
* @param conf configuration parameters for the file system
*/","* Gets file system instance of given uri.
   *
   * @param uri uri.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return FileSystem.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,get,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)",258,275,"/**
* Resolves file system instance for a given URI.
* @param uri URI to resolve
* @param conf Configuration object
* @return Filesystem instance or null if not found
*/","* When ViewFileSystemOverloadScheme scheme and target uri scheme are
     * matching, it will not take advantage of FileSystem cache as it will
     * create instance directly. For caching needs please set
     * ""fs.viewfs.enable.inner.cache"" to true.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getFileSystem,org.apache.hadoop.fs.Path:getFileSystem(org.apache.hadoop.conf.Configuration),365,367,"/**
 * Retrieves a file system instance based on the provided configuration.
 * @param conf Configuration object used to initialize the file system
 */","* Return the FileSystem that owns this Path.
   *
   * @param conf the configuration to use when resolving the FileSystem
   * @return the FileSystem that owns this Path
   * @throws java.io.IOException thrown if there's an issue resolving the
   * FileSystem",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getFileSystem,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getFileSystem(),458,476,"/**
* Retrieves a FileSystem instance, either from the suppliedFilesystem or
* by creating one from the base path URI and configuration.
*@throws MetricsException if file system creation fails. 
*/","* Return the supplied file system for testing or otherwise get a new file
   * system.
   *
   * @return the file system to use
   * @throws MetricsException thrown if the file system could not be retrieved",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,get,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:get(java.net.URI,org.apache.hadoop.conf.Configuration)",129,153,"/**
* Retrieves a FileSystem instance for the given URI and configuration.
* @param uri file system URI
* @param config file system configuration
* @return FileSystem object or null if not found
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path),1159,1226,"/**
* Lists file statuses for the given path, considering internal directories and fallback links.
* @param f the input path
* @return an array of FileStatus objects
*/","* {@inheritDoc}
     *
     * Note: listStatus on root(""/"") considers listing from fallbackLink if
     * available. If the same directory name is present in configured mount
     * path as well as in fallback link, then only the configured mount path
     * will be listed in the returned result.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(java.net.URI),440,443,"/**
* Retrieves FileContext instance based on provided default file system URI.
* @param defaultFsUri URI of the default file system
*/","* Create a FileContext for specified URI using the default config.
   * 
   * @param defaultFsUri defaultFsUri.
   * @return a FileContext with the specified URI as the default FS.
   * 
   * @throws UnsupportedFileSystemException If the file system for
   *           <code>defaultFsUri</code> is not supported",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.conf.Configuration),485,496,"/**
* Retrieves FileContext based on configuration.
* @param aConf Configuration object
* @throws UnsupportedFileSystemException if default file system URI is invalid
*/","* Create a FileContext using the passed config. Generally it is better to use
   * {@link #getFileContext(URI, Configuration)} instead of this one.
   * 
   * 
   * @param aConf configration.
   * @return new FileContext
   * @throws UnsupportedFileSystemException If file system in the config
   *           is not supported",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getLocalFSFileContext,org.apache.hadoop.fs.FileContext:getLocalFSFileContext(org.apache.hadoop.conf.Configuration),506,509,"/**
* Retrieves local file system context from configuration.
* @param aConf application configuration
*/","* @param aConf - from which the FileContext is configured
   * @return a FileContext for the local file system using the specified config.
   * 
   * @throws UnsupportedFileSystemException If default file system in the config
   *           is not supported
   *",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,init,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:init(org.apache.commons.configuration2.SubsetConfiguration),233,265,"/**
* Initializes the metrics system with configuration from SubsetConfiguration.
* @param metrics2Properties configuration settings for metric collection
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,call,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class)",539,542,"/**
* Makes an HTTP request and returns a typed response.
*@param conn HttpURLConnection object for the request
*@param jsonOutput JSON output to be sent with the request
*@param expectedResponse expected HTTP status code of the response
*@param klass Class type of the expected response
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",479,493,"/**
* Initializes a Server instance with specified configuration and protocol.
* @param protocolClass class of the protocol to use
* @param protocolImpl implementation of the protocol
*/","* Construct an RPC server.
     *
     * @param protocolClass the class of protocol
     * @param protocolImpl the protocolImpl whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param numReaders number of read threads
     * @param queueSizePerHandler the size of the queue contained
     *                            in each Handler
     * @param verbose whether each call should be logged
     * @param secretManager the server-side secret manager for each token type
     * @param portRangeConfig A config parameter that can be used to restrict
     * the range of ports used when port is 0 (an ephemeral port)
     * @param alignmentContext provides server state info on client responses
     * @throws IOException raised on errors performing I/O.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",495,535,"/**
* Initializes a server with the specified configuration and protocol.
* @param protocolClass target protocol class
* @param protocolImpl protocol implementation object
* @throws IOException if initialization fails
*/","* Construct an RPC server.
     * @param protocolClass - the protocol being registered
     *     can be null for compatibility with old usage (see below for details)
     * @param protocolImpl the protocol impl that will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param alignmentContext provides server state info on client responses
     * @param numReaders input numReaders.
     * @param portRangeConfig input portRangeConfig.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param secretManager input secretManager.
     * @throws IOException raised on errors performing I/O.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,saslProcess,org.apache.hadoop.ipc.Server$Connection:saslProcess(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto),2242,2314,"/**
* Processes a SASL message and establishes the authentication context.
* @param saslMessage the incoming SASL message
*/","* Process saslMessage and send saslResponse back
     * @param saslMessage received SASL message
     * @throws RpcServerException setup failed due to SASL negotiation
     *         failure, premature or invalid connection context, or other state 
     *         errors. This exception needs to be sent to the client. This 
     *         exception will wrap {@link RetriableException}, 
     *         {@link InvalidToken}, {@link StandbyException} or 
     *         {@link SaslException}.
     * @throws IOException if sending reply fails
     * @throws InterruptedException",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,readSSLConfiguration,"org.apache.hadoop.security.ssl.SSLFactory:readSSLConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.ssl.SSLFactory$Mode)",162,184,"/**
* Fetches and merges SSL configuration for a given mode.
* @param conf base Configuration object
* @param mode execution mode (CLIENT or SERVER)
* @return merged Configuration object with SSL settings
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ServiceAuthorizationManager.java,refresh,"org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refresh(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",140,150,"/**
* Loads and applies policy configuration from hadoop.policy.file.
* @param conf Hadoop configuration object
* @param provider PolicyProvider instance
*/",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/HCFSMountTableConfigLoader.java,load,"org.apache.hadoop.fs.viewfs.HCFSMountTableConfigLoader:load(java.lang.String,org.apache.hadoop.conf.Configuration)",57,113,"/**
* Loads the latest mount table configuration from a specified path and adds it to the provided Configuration object.
* @param mountTableConfigPath path to the mount table configuration
* @param conf target Configuration object","* Loads the mount-table configuration from hadoop compatible file system and
   * add the configuration items to given configuration. Mount-table
   * configuration format should be suffixed with version number.
   * Format: {@literal mount-table.<versionNumber>.xml}
   * Example: mount-table.1.xml
   * When user wants to update mount-table, the expectation is to upload new
   * mount-table configuration file with monotonically increasing integer as
   * version number. This API loads the highest version number file. We can
   * also configure single file path directly.
   *
   * @param mountTableConfigPath : A directory path where mount-table files
   *          stored or a mount-table file path. We recommend to configure
   *          directory with the mount-table version files.
   * @param conf : to add the mount table as resource.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,tryConnect,org.apache.hadoop.ha.HealthMonitor:tryConnect(),170,183,"/**
* Attempts to establish a connection to the local service.
* @throws IOException if connection fails
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,isOtherTargetNodeActive,"org.apache.hadoop.ha.HAAdmin:isOtherTargetNodeActive(java.lang.String,boolean)",187,215,"/**
* Checks if target node is already active.
* @param targetNodeToActivate identifier of target node to check
* @param forceActive whether to force activation even on error
* @return true if node is active, false otherwise
*/","* Checks whether other target node is active or not
   * @param targetNodeToActivate
   * @return true if other target node is active or some other exception 
   * occurred and forceActive was set otherwise false
   * @throws IOException",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,transitionToStandby,org.apache.hadoop.ha.HAAdmin:transitionToStandby(org.apache.commons.cli.CommandLine),217,234,"/**
* Transitions to standby mode using the provided command line arguments.
* @param cmd Command line object containing user input
* @return 0 on success, -1 on failure or error conditions
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkHealth,org.apache.hadoop.ha.HAAdmin:checkHealth(org.apache.commons.cli.CommandLine),292,309,"/**
* Performs health checks on a service using the provided command line arguments.
* @param cmd command line object containing service identifier
* @return 0 if health check is successful, -1 otherwise
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,getServiceState,org.apache.hadoop.ha.HAAdmin:getServiceState(org.apache.commons.cli.CommandLine),311,324,"/**
* Retrieves service state based on the provided command line arguments.
* @param cmd CommandLine object containing user input
* @throws IOException if I/O operation fails
* @throws ServiceFailedException if service is unavailable
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,getAllServiceState,org.apache.hadoop.ha.HAAdmin:getAllServiceState(),443,464,"/**
* Retrieves the overall service state by resolving and querying targets.
*@return 0 if successful, -1 if failed
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,becomeActive,org.apache.hadoop.ha.ZKFailoverController:becomeActive(),408,443,"/**
* Transitions service to active state.
* @throws ServiceFailedException if transition fails
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,becomeStandby,org.apache.hadoop.ha.ZKFailoverController:becomeStandby(),514,529,"/**
* Transitions current service instance into standby mode.
* @throws Exception if transition fails
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doCedeActive,org.apache.hadoop.ha.ZKFailoverController:doCedeActive(int),590,623,"/**
* Cedes active role, transitioning local node to standby and quitting election.
* @param millisToCede time in milliseconds before rejoining
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,preFailoverChecks,"org.apache.hadoop.ha.FailoverController:preFailoverChecks(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean)",109,156,"/**
* Performs pre-failover checks on a HAServiceTarget.
* @param from current service target
* @param target target to fail over to
* @param forceActive whether to force the failover even if not ready
*/","* Perform pre-failover checks on the given service we plan to
   * failover to, eg to prevent failing over to a service (eg due
   * to it being inaccessible, already active, not healthy, etc).
   *
   * An option to ignore toSvc if it claims it is not ready to
   * become active is provided in case performing a failover will
   * allow it to become active, eg because it triggers a log roll
   * so the standby can learn about new blocks and leave safemode.
   *
   * @param from currently active service
   * @param target service to make active
   * @param forceActive ignore toSvc if it reports that it is not ready
   * @throws FailoverFailedException if we should avoid failover",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,tryGracefulFence,org.apache.hadoop.ha.FailoverController:tryGracefulFence(org.apache.hadoop.ha.HAServiceTarget),168,186,"/**
* Attempts a graceful service transition by making the target standby.
* @param svc HAServiceTarget instance
* @return true on success, false otherwise
*/","* Try to get the HA state of the node at the given address. This
   * function is guaranteed to be ""quick"" -- ie it has a short timeout
   * and no retries. Its only purpose is to avoid fencing a node that
   * has already restarted.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,moveToTrash,org.apache.hadoop.fs.shell.Delete$Rm:moveToTrash(org.apache.hadoop.fs.shell.PathData),151,167,"/**
* Moves a file to trash, throwing exceptions if failed.
* @param item PathData object containing file information
* @return true if moved to trash successfully, false otherwise
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlConnection.java,getInputStream,org.apache.hadoop.fs.FsUrlConnection:getInputStream(),77,83,"/**
* Returns an input stream to the connected resource. 
* If not already connected, establishes a connection first.
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,recursePath,org.apache.hadoop.fs.shell.find.Find:recursePath(org.apache.hadoop.fs.shell.PathData),345,371,"/**
* Recursively traverses the file system, following symbolic links if enabled.
* @param item current PathData object
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,isPathRecursable,org.apache.hadoop.fs.shell.find.Find:isPathRecursable(org.apache.hadoop.fs.shell.PathData),373,392,"/**
* Checks if a file path is recursable based on its type and user options.
* @param item PathData object containing file statistics
* @return true if the path can be recursively traversed, false otherwise
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,expandArgument,org.apache.hadoop.fs.shell.Head:expandArgument(java.lang.String),56,61,"/**
* Expands a single argument into a list of PathData objects.
* @param arg the input string to expand
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,expandArgument,org.apache.hadoop.fs.shell.Tail:expandArgument(java.lang.String),81,86,"/**
* Expands a command-line argument into a list of PathData objects.
* @param arg the command-line argument to expand
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemLinkResolver.java,resolve,"org.apache.hadoop.fs.FileSystemLinkResolver:resolve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",71,111,"/**
* Resolves a symbolic link to its target.
* @param filesys the file system containing the path
* @param path the path to resolve
* @return the resolved object or null if not found
*/","* Attempt calling overridden {@link #doCall(Path)} method with
   * specified {@link FileSystem} and {@link Path}. If the call fails with an
   * UnresolvedLinkException, it will try to resolve the path and retry the call
   * by calling {@link #next(FileSystem, Path)}.
   * @param filesys FileSystem with which to try call
   * @param path Path with which to try call
   * @return Generic type determined by implementation
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",100,102,"/**
 * Constructs a new PathData object from a local file path and Hadoop configuration.
 * @param localPath local URI of the file
 * @param conf Hadoop configuration used for file system operations
 */","* Creates an object to wrap the given parameters as fields.  The string
   * used to create the path will be recorded since the Path object does not
   * return exactly the same string used to initialize it
   * @param localPath a local URI
   * @param conf the configuration file
   * @throws IOException if anything goes wrong...",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",2571,2576,"/**
* Copies files from local sources to a destination.
* @param delSrc true to delete source file after copying
* @param overwrite true to overwrite existing destination
* @param srcs array of source file paths
* @param dst destination file path
*/","* The src files are on the local disk.  Add it to the filesystem at
   * the given dst name.
   * delSrc indicates if the source should be removed
   * @param delSrc whether to delete the src
   * @param overwrite whether to overwrite an existing file
   * @param srcs array of paths which are source
   * @param dst path
   * @throws IOException IO failure",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2588,2593,"/**
* Copies data from a local file to another location.
* @param delSrc whether to delete the source file
* @param overwrite whether to overwrite existing destination file
* @param src source file path
* @param dst destination file path
*/","* The src file is on the local disk.  Add it to the filesystem at
   * the given dst name.
   * delSrc indicates if the source should be removed
   * @param delSrc whether to delete the src
   * @param overwrite whether to overwrite an existing file
   * @param src path
   * @param dst path
   * @throws IOException IO failure",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",2648,2658,"/**
* Copies a file from a Hadoop filesystem to a local file system.
* @param delSrc whether to delete the source file
* @param src HDFS file path
* @param dst local file system destination
* @param useRawLocalFileSystem true for raw local FS, false for cached FS
*/","* The src file is under this filesystem, and the dst is on the local disk.
   * Copy it from the remote filesystem to the local dst name.
   * delSrc indicates if the src will be removed
   * or not. useRawLocalFileSystem indicates whether to use RawLocalFileSystem
   * as the local file system or not. RawLocalFileSystem is non checksumming,
   * So, It will not create any crc files at local.
   *
   * @param delSrc
   *          whether to delete the src
   * @param src
   *          path
   * @param dst
   *          path
   * @param useRawLocalFileSystem
   *          whether to use RawLocalFileSystem as local file system or not.
   *
   * @throws IOException for any IO error",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,confChanged,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:confChanged(org.apache.hadoop.conf.Configuration),309,360,"/**
* Updates context with new local directories configuration.
* @param conf Configuration object containing new directory settings
* @return updated Context object or throws IOException if config is invalid
*/","This method gets called everytime before any read/write to make sure
     * that any change to localDirs is reflected immediately.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getLocalPath,"org.apache.hadoop.conf.Configuration:getLocalPath(java.lang.String,java.lang.String)",2828,2848,"/**
* Tries each local directory to find a valid path.
* @param dirsProp comma-separated list of local directories
* @param path file or directory path to resolve
* @return Path object if found, null otherwise (not thrown)
* @throws IOException if no valid dir is found
*/","* Get a local file under a directory named by <i>dirsProp</i> with
   * the given <i>path</i>.  If <i>dirsProp</i> contains multiple directories,
   * then one is chosen based on <i>path</i>'s hash code.  If the selected
   * directory does not exist, an attempt is made to create it.
   *
   * @param dirsProp directory in which to locate the file.
   * @param path file-path.
   * @return local file under the directory with the given path.
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:<init>(java.lang.String,java.lang.String,java.net.URI,org.apache.hadoop.conf.Configuration)",94,97,"/**
* Creates an NflyNode instance with a configurable file system.
* @param hostName name of the host
* @param rackName name of the rack
* @param uri URI for the file system
* @param conf configuration object for the file system
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getRawFileSystem,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getRawFileSystem(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",328,340,"/**
* Resolves file system for a given path and configuration.
* @param path Path to resolve
* @param conf Configuration object
* @return Raw FileSystem object, or throws exception if not found
*/","* This is an admin only API to give access to its child raw file system, if
   * the path is link. If the given path is an internal directory(path is from
   * mount paths tree), it will initialize the file system of given path uri
   * directly. If path cannot be resolved to any internal directory or link, it
   * will throw NotInMountpointException. Please note, this API will not return
   * chrooted file system. Instead, this API will get actual raw file system
   * instances.
   *
   * @param path - fs uri path
   * @param conf - configuration
   * @throws IOException raised on errors performing I/O.
   * @return file system.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getMountPathInfo,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getMountPathInfo(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",351,372,"/**
* Resolves mount path info by URI or file system for a given path.
* @param path path to resolve
* @param conf configuration
* @return MountPathInfo object containing resolved path and file system, or null if not found
*/","* Gets the mount path info, which contains the target file system and
   * remaining path to pass to the target file system.
   *
   * @param path the path.
   * @param conf configuration.
   * @return mount path info.
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,expandAsGlob,"org.apache.hadoop.fs.shell.PathData:expandAsGlob(java.lang.String,org.apache.hadoop.conf.Configuration)",344,396,"/**
* Expands a glob pattern into an array of PathData objects.
* @param pattern the glob pattern to expand
* @param conf Hadoop configuration object
* @return array of PathData objects or null if not found
*/","* Expand the given path as a glob pattern.  Non-existent paths do not
   * throw an exception because creation commands like touch and mkdir need
   * to create them.  The ""stat"" field will be null if the path does not
   * exist.
   * @param pattern the pattern to expand as a glob
   * @param conf the hadoop configuration
   * @return list of {@link PathData} objects.  if the pattern is not a glob,
   * and does not exist, the list will contain a single PathData with a null
   * stat 
   * @throws IOException anything else goes wrong...",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",1894,1932,"/**
* Configures a Reader instance based on provided options and configuration.
* @param conf Hadoop Configuration object
* @param opts variable number of Options to customize reader behavior
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,initBloomFilter,"org.apache.hadoop.io.BloomMapFile$Reader:initBloomFilter(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",237,254,"/**
* Initializes the Bloom filter from a file.
* @param dirName directory containing the Bloom filter file
* @param conf Hadoop configuration object
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFileDumper.java,dumpInfo,"org.apache.hadoop.io.file.tfile.TFileDumper:dumpInfo(java.lang.String,java.io.PrintStream,org.apache.hadoop.conf.Configuration)",96,295,"/**
 * Dumps information about a TFile.
 * @param file the path to the TFile
 * @param out where to print the output
 * @param conf Hadoop configuration
 */","* Dump information about TFile.
   * 
   * @param file
   *          Path string of the TFile
   * @param out
   *          PrintStream to output the information.
   * @param conf
   *          The configuration object.
   * @throws IOException",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readTokenStorageFile,"org.apache.hadoop.security.Credentials:readTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",225,241,"/**
* Reads Credentials from a file specified by the given filename and configuration.
* @param filename Path to the token storage file
* @param conf Configuration object
* @return Credentials object or throws IOException if an error occurs
*/","* Convenience method for reading a token storage file and loading its Tokens.
   * @param filename filename.
   * @param conf configuration.
   * @throws IOException  raised on errors performing I/O.
   * @return Credentials.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageFile,"org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials$SerializedFormat)",341,347,"/**
* Writes token storage to a file.
* @param filename the path to the output file
* @param conf configuration object
* @param format serialization format
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,initFileSystem,org.apache.hadoop.security.alias.KeyStoreProvider:initFileSystem(java.net.URI),81,85,"/**
* Initializes file system with given URI.
* @param uri unique identifier of file system
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,<init>,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",129,138,"/**
* Initializes a JavaKeyStoreProvider instance with the given URI and Configuration.
* @param uri URI of the key store
* @param conf configuration settings for file system access
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,getLibJars,org.apache.hadoop.util.GenericOptionsParser:getLibJars(org.apache.hadoop.conf.Configuration),372,389,"/**
* Retrieves a list of URLs for library JARs from the configuration.
* @param conf Hadoop Configuration object
* @return Array of URLs or null if no JARs are specified
*/","* If libjars are set in the conf, parse the libjars.
   * @param conf input Configuration.
   * @return libjar urls
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,initFs,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:initFs(),271,304,"/**
* Initializes the file system and creates the base directory.
* @return true if initialization is successful, false otherwise
*/","* Initialize the connection to HDFS and create the base directory. Also
   * launch the flush thread.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getLocalFSFileContext,org.apache.hadoop.fs.FileContext:getLocalFSFileContext(),426,429,"/**
* Retrieves local file system context.
* @return FileContext object representing the local file system
*/","* @return a FileContext for the local file system using the default config.
   * @throws UnsupportedFileSystemException If the file system for
   *           {@link FsConstants#LOCAL_FS_URI} is not supported.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,<init>,org.apache.hadoop.fs.shell.Display$AvroFileInputStream:<init>(org.apache.hadoop.fs.FileStatus),278,289,"/**
* Initializes an AvroFileInputStream from a FileStatus.
* @param status FileStatus object containing input data
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(),416,419,"/**
* Retrieves a FileContext instance based on default configuration.
* @throws UnsupportedFileSystemException if file system is not supported
*/","* Create a FileContext using the default config read from the
   * $HADOOP_CONFIG/core.xml, Unspecified key-values for config are defaulted
   * from core-defaults.xml in the release jar.
   * 
   * @throws UnsupportedFileSystemException If the file system from the default
   *           configuration is not supported
   * @return file context.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeyVersion(java.lang.String),616,624,"/**
* Retrieves a KeyVersion object by its unique name.
* @param versionName the name of the key version to fetch
* @return KeyVersion object or throws IOException if an error occurs
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getCurrentKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getCurrentKey(java.lang.String),626,634,"/**
* Fetches the current key version by name.
* @param name unique key identifier
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeys,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeys(),636,644,"/**
* Retrieves a list of key names from the KMS REST API.
* @return List of String keys or empty list if unsuccessful
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeysMetadata,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeysMetadata(java.lang.String[]),675,694,"/**
* Retrieves metadata for specified key names.
* @param keyNames variable number of key identifiers
* @return array of Metadata objects or empty array if none found
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKeyInternal,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKeyInternal(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",696,722,"/**
* Creates a new key version with specified material and options.
* @param name Key name
* @param material Optional key material (null if not provided)
* @param options Key creation options
* @return Newly created KeyVersion object
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,invalidateCache,org.apache.hadoop.crypto.key.kms.KMSClientProvider:invalidateCache(java.lang.String),741,750,"/**
* Invalidates cache for given resource by name.
* @param name unique resource identifier
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,decryptEncryptedKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),801,835,"/**
* Decrypts an encrypted key version using the KMS REST API.
* @param encryptedKeyVersion Encrypted key version to decrypt
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,reencryptEncryptedKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),837,864,"/**
* Reencrypts an existing Encrypted Key Version.
* @param ekv the Encrypted Key Version to reencrypt
* @return a new Encrypted Key Version with updated encryption
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,reencryptEncryptedKeys,org.apache.hadoop.crypto.key.kms.KMSClientProvider:reencryptEncryptedKeys(java.util.List),866,906,"/**
* Reencrypts a list of encrypted key versions in batches.
* @param ekvs List of EncryptedKeyVersion objects to reencrypt
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeyVersions(java.lang.String),908,923,"/**
* Retrieves key versions by name.
* @param name unique key identifier
* @return List of KeyVersion objects or empty list if not found
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getMetadata,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getMetadata(java.lang.String),925,933,"/**
* Retrieves metadata for a given key by name.
* @param name unique key identifier
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,deleteKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:deleteKey(java.lang.String),935,941,"/**
* Deletes a key by name.
* @param name unique key identifier
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getServer,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",380,390,"/**
* Creates and returns an RPC server instance with specified parameters.
* @param protocol the protocol class to use
* @param protocolImpl implementation of the protocol
* @param conf configuration for the server
* @return RPC.Server object or null on failure
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",446,455,"/**
* Initializes a Server instance with specified configuration and parameters.
* @param protocolClass protocol implementation class
* @throws IOException if initialization fails
*/","* Construct an RPC server.
     * 
     * @param protocolClass the class of protocol
     * @param protocolImpl the protocolImpl whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param portRangeConfig A config parameter that can be used to restrict
     * the range of ports used when port is 0 (an ephemeral port)
     * @param alignmentContext provides server state info on client responses
     * @param secretManager input secretManager.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param numReaders input numReaders.
     * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getServer,"org.apache.hadoop.ipc.WritableRpcEngine:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",371,382,"/**
* Creates an RPC server instance.
* @param protocolClass the protocol class to use
* @return an initialized RPC.Server object
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String)",466,476,"/**
* Legacy constructor for Server instance.
* @param protocolClass protocol implementation class
* @throws IOException on initialization error
*/","* Construct an RPC server.
     * @param protocolClass - the protocol being registered
     *     can be null for compatibility with old usage (see below for details)
     * @param protocolImpl the protocol impl that will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param secretManager input secretManager.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param portRangeConfig input portRangeConfig.
     * @param numReaders input numReaders.
     *
     * @deprecated use Server#Server(Class, Object,
     *      Configuration, String, int, int, int, int, boolean, SecretManager)
     * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,saslReadAndProcess,org.apache.hadoop.ipc.Server$Connection:saslReadAndProcess(org.apache.hadoop.ipc.RpcWritable$Buffer),2178,2196,"/**
* Processes incoming SASL message from client.
* @param buffer SASL message payload
*/",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,<init>,"org.apache.hadoop.security.ssl.SSLFactory:<init>(org.apache.hadoop.security.ssl.SSLFactory$Mode,org.apache.hadoop.conf.Configuration)",136,160,"/**
* Initializes SSLFactory with mode and configuration.
* @param mode encryption mode
* @param conf TLS configuration settings
*/","* Creates an SSLFactory.
   *
   * @param mode SSLFactory mode, client or server.
   * @param conf Hadoop configuration from where the SSLFactory configuration
   * will be read.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,refreshServiceAcl,"org.apache.hadoop.ipc.Server:refreshServiceAcl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",767,769,"/**
* Refreshes Service ACL (Access Control List) using provided Configuration and PolicyProvider.
* @param conf configuration object
* @param provider policy provider instance
*/","* Refresh the service authorization ACL for the service handled by this server.
   *
   * @param conf input Configuration.
   * @param provider input PolicyProvider.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,loopUntilConnected,org.apache.hadoop.ha.HealthMonitor:loopUntilConnected(),161,168,"/**
* Continuously attempts to establish a connection until successful.
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,transitionToActive,org.apache.hadoop.ha.HAAdmin:transitionToActive(org.apache.commons.cli.CommandLine),155,178,"/**
* Transitions a target to active state.
* @param cmd CommandLine object with transition details
* @return 0 on success, -1 on failure or exception
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doFence,org.apache.hadoop.ha.ZKFailoverController:doFence(org.apache.hadoop.ha.HAServiceTarget),543,566,"/**
* Attempts to fence a HAServiceTarget instance.
* @param target HAServiceTarget instance to fence
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,failover,"org.apache.hadoop.ha.FailoverController:failover(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean,boolean)",198,260,"/**
* Initiates a failover from one HA service target to another.
* @param fromSvc the original active service target
* @param toSvc the target service to become active
* @param forceFence whether to fence the original service immediately
* @param forceActive whether to make the new service active even if fencing fails
*/","* Failover from service 1 to service 2. If the failover fails
   * then try to failback.
   *
   * @param fromSvc currently active service
   * @param toSvc service to make active
   * @param forceFence to fence fromSvc even if not strictly necessary
   * @param forceActive try to make toSvc active even if it is not ready
   * @throws FailoverFailedException if the failover fails",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processPath,org.apache.hadoop.fs.shell.Delete$Rm:processPath(org.apache.hadoop.fs.shell.PathData),110,127,"/**
* Deletes a file system path.
* @param item PathData object containing the path to be deleted
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,expandArgument,org.apache.hadoop.fs.shell.CopyCommands$Put:expandArgument(java.lang.String),295,304,"/**
* Expands a command argument into a list of PathData objects.
* @param arg the argument to expand
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Merge:processOptions(java.util.LinkedList),71,89,"/**
* Parses command-line options and initializes data structures.
* @param args list of command-line arguments
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,expandArgument,org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:expandArgument(java.lang.String),357,375,"/**
* Resolves a single command-line argument into a list of PathData objects.
* @param arg command-line argument to resolve (e.g. file path or URI)
* @return List of resolved PathData objects, or empty list if invalid
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,getLocalDestination,org.apache.hadoop.fs.shell.CommandWithDestination:getLocalDestination(java.util.LinkedList),182,195,"/**
* Initializes local destination from command-line arguments.
* @param args list of path components (e.g., ""dir1/dir2/file.txt"")
*/","*  The last arg is expected to be a local path, if only one argument is
   *  given then the destination will be the current directory 
   *  @param args is the list of arguments
   * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,moveFromLocalFile,"org.apache.hadoop.fs.FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",2530,2533,"/**
* Copies files from local file system to specified destination.
* @param srcs array of source file paths
* @param dst destination directory path
*/","* The src files is on the local disk.  Add it to filesystem at
   * the given dst name, removing the source afterwards.
   * @param srcs source paths
   * @param dst path
   * @throws IOException IO failure",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",360,365,"/**
* Copies files from local file system to Hadoop FS.
* @param delSrc whether to delete source files
* @param overwrite whether to overwrite existing target files
* @param srcs array of local file paths to copy
* @param dst destination path on Hadoop FS
*/","* The src files are on the local disk.  Add it to FS at
   * the given dst name.
   * delSrc indicates if the source should be removed",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2556,2559,"/**
* Copies file from local source to destination.
* @param delSrc whether to delete the source file
* @param overwrite whether to overwrite existing destination file
* @param src source file path
* @param dst destination file path
*/","* The src file is on the local disk.  Add it to the filesystem at
   * the given dst name.
   * delSrc indicates if the source should be removed
   * @param delSrc whether to delete the src
   * @param src path
   * @param dst path
   * @throws IOException IO failure.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",372,377,"/**
* Copies a local file to a Hadoop FS location.
* @param delSrc whether to delete the source file
* @param overwrite whether to overwrite the destination file if it exists
* @param src path of the local file to copy from
* @param dst path where the file will be copied to
*/","* The src file is on the local disk.  Add it to FS at
   * the given dst name.
   * delSrc indicates if the source should be removed",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2624,2627,"/**
* Copies file from source to destination with optional deletion of source.
* @param delSrc whether to delete the source after copying
* @param src source file path
* @param dst destination file path
*/","* Copy it a file from a remote filesystem to the local one.
   * delSrc indicates if the src will be removed or not.
   * @param delSrc whether to delete the src
   * @param src path src file in the remote filesystem
   * @param dst path local destination
   * @throws IOException IO failure",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)",394,492,"/**
* Fetches a local write path for the given string path and size.
* @param pathStr user-provided file path
* @param size requested file size
* @param conf Hadoop configuration object
* @param checkWrite whether to perform write checks on the destination
* @return Path object representing the valid local directory or null if not found
*/","Get a path from the local FS. If size is known, we go
     *  round-robin over the set of disks (via the configured dirs) and return
     *  the first complete path which has enough space.
     *  
     *  If size is not known, use roulette selection -- pick directories
     *  with probability proportional to their available space.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathToRead,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",518,539,"/**
* Resolves local file system path by searching through configured directories.
* @param pathStr relative file path to resolve
* @param conf Hadoop configuration with local directory paths
* @return Path object if found, or throws DiskErrorException otherwise
*/","Get a path from the local FS for reading. We search through all the
     *  configured dirs for the file's existence and return the complete
     *  path to the file when we find one",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getAllLocalPathsToRead,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",603,610,"/**
* Retrieves all local paths to read from the specified directory.
* @param pathStr directory path string
* @param conf configuration object
* @return iterable of local file system paths or empty if not found
*/","* Get all of the paths that currently exist in the working directories.
     * @param pathStr the path underneath the roots
     * @param conf the configuration to look up the roots in
     * @return all of the paths that exist under any of the roots
     * @throws IOException",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet,org.apache.hadoop.fs.viewfs.FsGetter)",228,275,"/**
* Initializes NflyFSystem with the given URIs, configuration, and replication settings.
* @param uris array of destination URIs
* @param conf Hadoop Configuration object
* @param minReplication minimum number of replicas required
* @param nflyFlags set of Nfly keys
* @param fsGetter FsGetter instance (optional)
*/","* Creates a new Nfly instance.
   *
   * @param uris the list of uris in the mount point
   * @param conf configuration object
   * @param minReplication minimum copies to commit a write op
   * @param nflyFlags modes such readMostRecent
   * @param fsGetter to get the file system instance with the given uri
   * @throws IOException",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,runAll,org.apache.hadoop.fs.shell.Command:runAll(),128,142,"/**
* Runs all specified source files as glob patterns.
* @return non-zero exit code if any errors occur
*/","* For each source path, execute the command
   * 
   * @return 0 if it runs successfully; -1 if it fails",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,expandArgument,org.apache.hadoop.fs.shell.Command:expandArgument(java.lang.String),264,271,"/**
* Expands an argument into a list of PathData objects.
* @param arg input string to expand as glob or literal path
*/","* Expand the given argument into a list of {@link PathData} objects.
   * The default behavior is to expand globs.  Commands may override to
   * perform other expansions on an argument.
   * @param arg string pattern to expand
   * @return list of {@link PathData} objects
   * @throws IOException if anything goes wrong...",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,getRemoteDestination,org.apache.hadoop.fs.shell.CommandWithDestination:getRemoteDestination(java.util.LinkedList),203,221,"/**
* Resolves remote destination path from provided arguments.
* @param args list of strings containing path or glob pattern
*/","*  The last arg is expected to be a remote path, if only one argument is
   *  given then the destination will be the remote user's directory 
   *  @param args is the list of arguments
   *  @throws PathIOException if path doesn't exist or matches too many times",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",1942,1946,"/**
* Constructs a Reader instance from a FileSystem and Path.
* @param fs underlying filesystem
* @param file input path to read from
* @param conf configuration for the reader
*/","* Construct a reader by opening a file from the given file system.
     * @param fs The file system used to open the file.
     * @param file The file being read.
     * @param conf Configuration
     * @throws IOException raised on errors performing I/O.
     * @deprecated Use Reader(Configuration, Option...) instead.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,int,long,long,org.apache.hadoop.conf.Configuration)",1958,1962,"/**
* Constructs a Reader object from FSDataInputStream.
* @param in input stream to read from
* @param buffersize buffer size for reading
* @param start starting position for reading
* @param length desired length of data to read
* @param conf configuration settings
*/","* Construct a reader by the given input stream.
     * @param in An input stream.
     * @param buffersize unused
     * @param start The starting position.
     * @param length The length being read.
     * @param conf Configuration
     * @throws IOException raised on errors performing I/O.
     * @deprecated Use Reader(Configuration, Reader.Option...) instead.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,createDataFileReader,"org.apache.hadoop.io.MapFile$Reader:createDataFileReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",568,575,"/**
* Creates a SequenceFile reader for the specified data file.
* @param dataFile Path to the data file
* @param conf Hadoop configuration
* @return SequenceFile reader or null on error
*/","* Override this method to specialize the type of
     * {@link SequenceFile.Reader} returned.
     *
     * @param dataFile data file.
     * @param conf configuration.
     * @param options options.
     * @throws IOException raised on errors performing I/O.
     * @return SequenceFile.Reader.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawKey,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawKey(),3832,3857,"/**
* Retrieves the next raw key from input.
* @return true if a valid key was found, false otherwise
*/","* Fills up the rawKey object with the key returned by the Reader.
       * @return true if there is a key returned; false, otherwise
       * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",1071,1195,"/**
* Constructs a SequenceFile Writer with specified options.
* @param conf Hadoop Configuration object
* @param opts Hadoop Options array
*/","* Construct a uncompressed writer from a set of options.
     * @param conf the configuration to use
     * @param opts the options used when creating the writer
     * @throws IOException if it fails",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,main,org.apache.hadoop.io.file.tfile.TFile:main(java.lang.String[]),2348,2366,"/**
* Dumps information from one or more TFiles and prints to console.
* @param args array of TFile paths to dump
*/","* Dumping the TFile information.
   * 
   * @param args
   *          A list of TFile paths.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageFile,"org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",335,339,"/**
* Writes token storage file to specified location.
* @param filename Path to output file
* @param conf Configuration object (not used)
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,doFormattedWrite,"org.apache.hadoop.security.token.DtFileOperations:doFormattedWrite(java.io.File,java.lang.String,org.apache.hadoop.security.Credentials,org.apache.hadoop.conf.Configuration)",104,114,"/**
* Writes user credentials to a file in the specified format.
* @param f File to write to
* @param format Format string (e.g. ""protobuf"")
* @param creds Credentials object
* @param conf Configuration object
*/","Write out a Credentials object as a local file.
   *  @param f a local File object.
   *  @param format a string equal to FORMAT_PB or FORMAT_JAVA.
   *  @param creds the Credentials object to be written out.
   *  @param conf a Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,rollLogDirIfNeeded,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:rollLogDirIfNeeded(),504,542,"/**
* Rolls log directory if necessary, based on system clock and file flushing criteria.
* @throws MetricsException if rolling log directory fails
*/","* Check the current directory against the time stamp.  If they're not
   * the same, create a new directory and a new log file in that directory.
   *
   * @throws MetricsException thrown if an error occurs while creating the
   * new directory or new log file",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,getJarsInDirectory,"org.apache.hadoop.fs.FileUtil:getJarsInDirectory(java.lang.String,boolean)",1777,1796,"/**
* Retrieves a list of JAR files in the specified directory.
* @param path directory path to search, may include wildcard (*)
* @param useLocal whether to use local file system context
* @return List of Path objects representing found JAR files or an empty list if none were found","* Returns all jars that are in the directory. It is useful in expanding a
   * wildcard path to return all jars from the directory to use in a classpath.
   *
   * @param path the path to the directory. The path may include the wildcard.
   * @param useLocal use local.
   * @return the list of jars as URLs, or an empty list if there are no jars, or
   * the directory does not exist",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,getInputStream,org.apache.hadoop.fs.shell.Display$Text:getInputStream(org.apache.hadoop.fs.shell.PathData),123,173,"/**
* Fetches InputStream for PathData item with compression detection.
* @param item PathData object
* @return InputStream object or null if not found
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",724,728,"/**
* Creates a new key with the given name and options.
* @param name unique key identifier
* @param options configuration for the new key
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",730,739,"/**
* Creates a new key with the given name and material.
* @param name unique key identifier
* @param material key material data
* @param options configuration options for key creation
* @return KeyVersion object representing the created key
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,rollNewVersionInternal,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersionInternal(java.lang.String,byte[])",752,768,"/**
* Rolls a new version of a Key with the given material.
* @param name unique key identifier
* @param material optional cryptographic material for the new version
* @return newly created KeyVersion object or null on failure
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,invalidateCache,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:invalidateCache(java.lang.String),319,324,"/**
* Invalidates cache for all KMS clients with specified key name.
* @param keyName unique cache key identifier
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getServer,"org.apache.hadoop.ipc.ProtobufRpcEngine:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",368,378,"/**
* Creates an RPC server instance with specified configuration.
* @param protocol the protocol class
* @param protocolImpl the protocol implementation object
* @return RPC.Server instance or null on failure
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)",413,418,"/**
* Initializes a Server instance with the given protocol class and implementation.
* @param protocolClass protocol class to use
* @param protocolImpl protocol implementation object
* @param conf server configuration
* @param bindAddress address to bind to
* @param port port number
*/","Construct an RPC server.
     * @param protocolClass class
     * @param protocolImpl the instance whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager)",436,445,"/**
* Legacy constructor for Server instance.
* @param secretManager SecretManager instance
*/","* Construct an RPC server.
     * @param protocolImpl the instance whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param numReaders input numberReaders.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param secretManager input secretManager.
     * 
     * @deprecated use Server#Server(Class, Object, 
     *      Configuration, String, int, int, int, int, boolean, SecretManager)
     * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processRpcOutOfBandRequest,"org.apache.hadoop.ipc.Server$Connection:processRpcOutOfBandRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)",2984,3012,"/**
* Processes an RPC request header and executes the corresponding action.
* @param header RpcRequestHeaderProto object
* @param buffer RpcWritable.Buffer containing request data
*/","* Establish RPC connection setup by negotiating SASL if required, then
     * reading and authorizing the connection header
     * @param header - RPC header
     * @param buffer - stream to request payload
     * @throws RpcServerException - setup failed due to SASL
     *         negotiation failure, premature or invalid connection context,
     *         or other state errors. This exception needs to be sent to the 
     *         client.
     * @throws IOException - failed to send a response back to the client
     * @throws InterruptedException",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,connect,org.apache.hadoop.log.LogLevel$CLI:connect(java.net.URL),260,284,"/**
* Establishes an authenticated HTTP/HTTPS connection to the specified URL.
* @param url target URL
* @return configured URLConnection object or throws Exception if failed
*/","* Connect to the URL. Supports HTTP/HTTPS and supports SPNEGO
     * authentication. It falls back to simple authentication if it fails to
     * initiate SPNEGO.
     *
     * @param url the URL address of the daemon servlet
     * @return a connected connection
     * @throws Exception if it can not establish a connection.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",378,428,"/**
* Initializes the KMS client provider with a given URI and configuration.
* @param uri KMS service URL
* @param conf Configuration object for authentication and timeouts
*/",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,runCmd,org.apache.hadoop.ha.HAAdmin:runCmd(java.lang.String[]),391,441,"/**
* Executes a command with the given arguments.
* @param argv array of command-line arguments
* @return non-negative integer if successful, -1 otherwise
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,fenceOldActive,org.apache.hadoop.ha.ZKFailoverController:fenceOldActive(byte[]),532,541,"/**
* Fences an old active service by ID.
* @param data byte array containing service target information
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2518,2521,"/**
* Copies data from local file at 'src' to 'dst'.
*/","* The src file is on the local disk.  Add it to filesystem at
   * the given dst name and the source is kept intact afterwards
   * @param src path
   * @param dst path
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,moveFromLocalFile,"org.apache.hadoop.fs.FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2542,2545,"/**
* Copies file from local source to destination.
* @param src source file path
* @param dst destination file path
*/","* The src file is on the local disk.  Add it to the filesystem at
   * the given dst name, removing the source afterwards.
   * @param src local path
   * @param dst path
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",349,353,"/**
* Copies file from local source to destination on distributed file system.
* @param delSrc whether to delete original file
* @param src path of source file
* @param dst path of destination file
*/","* The src file is on the local disk.  Add it to FS at
   * the given dst name.
   * delSrc indicates if the source should be removed",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2601,2603,"/**
 * Copies file from source to destination on local file system.
 * @param src source file path
 * @param dst destination file path
 */","* Copy it a file from the remote filesystem to the local one.
   * @param src path src file in the remote filesystem
   * @param dst path local destination
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,moveToLocalFile,"org.apache.hadoop.fs.FileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2612,2614,"/**
* Copies file from source to destination on local file system.
* @param src source file path
* @param dst destination file path
*/","* Copy a file to the local filesystem, then delete it from the
   * remote filesystem (if successfully copied).
   * @param src path src file in the remote filesystem
   * @param dst path local destination
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",384,388,"/**
* Copies file from source to destination location.
* @param delSrc true to delete original file after copying
* @param src source file path
* @param dst destination file path
*/","* The src file is under FS, and the dst is on the local disk.
   * Copy it from FS control to the local dst name.
   * delSrc indicates if the src will be removed or not.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)",162,167,"/**
* Retrieves a local write path for the given file.
* @param pathStr file path string
* @param size file size in bytes
* @param conf configuration object
* @param checkWrite flag to check write permissions (true) or not (false)
*/","Get a path from the local FS. Pass size as 
   *  SIZE_UNKNOWN if not known apriori. We
   *  round-robin over the set of disks (via the configured dirs) and return
   *  the first complete path which has enough space 
   *  @param pathStr the requested path (this will be created on the first 
   *  available disk)
   *  @param size the size of the file that is going to be written
   *  @param conf the Configuration object
   *  @param checkWrite ensure that the path is writable
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,createTmpFileForWrite,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",500,512,"/**
* Creates a temporary write-enabled file with specified size.
* @param pathStr base path for the file
* @param size desired file size
* @param conf Hadoop configuration object
* @return File object representing the temporary file
*/","Creates a file on the local FS. Pass size as 
     * {@link LocalDirAllocator.SIZE_UNKNOWN} if not known apriori. We
     *  round-robin over the set of disks (via the configured dirs) and return
     *  a file on the first path which has enough space. The file is guaranteed
     *  to go away when the JVM exits.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathToRead,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",177,181,"/**
* Returns a local file path to read from based on the provided input string and configuration.
* @param pathStr input string representing the path
* @param conf Hadoop Configuration object
*/","Get a path from the local FS for reading. We search through all the
   *  configured dirs for the file's existence and return the complete
   *  path to the file when we find one 
   *  @param pathStr the requested file (this will be searched)
   *  @param conf the Configuration object
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getAllLocalPathsToRead,"org.apache.hadoop.fs.LocalDirAllocator:getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",190,198,"/**
* Retrieves all local paths to read based on the provided path string and configuration.
* @param pathStr string representing the path
* @param conf Hadoop Configuration object
* @return Iterable of Path objects or null if not found
*/","* Get all of the paths that currently exist in the working directories.
   * @param pathStr the path underneath the roots
   * @param conf the configuration to look up the roots in
   * @return all of the paths that exist under any of the roots
   * @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet)",213,216,"/**
* Constructs an instance of NflyFSystem from given URIs and configuration.
* @param uris array of file system URIs
* @param conf Hadoop Configuration object
* @param minReplication minimum replication factor for the file system
*/","* Creates a new Nfly instance.
   *
   * @param uris the list of uris in the mount point
   * @param conf configuration object
   * @param minReplication minimum copies to commit a write op
   * @param nflyFlags modes such readMostRecent
   * @throws IOException",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,createFileSystem,"org.apache.hadoop.fs.viewfs.NflyFSystem:createFileSystem(java.net.URI[],org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.viewfs.FsGetter)",944,971,"/**
* Creates a file system instance with specified settings.
* @param uris array of URIs
* @param conf configuration object
* @param settings string containing key-value pairs for Nfly settings
* @param fsGetter FsGetter object
* @return newly created FileSystem object or null on failure","* Initializes an nfly mountpoint in viewfs.
   *
   * @param uris destinations to replicate writes to
   * @param conf file system configuration
   * @param settings comma-separated list of k=v pairs.
   * @return an Nfly filesystem
   * @throws IOException",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,expandArguments,org.apache.hadoop.fs.shell.Command:expandArguments(java.util.LinkedList),243,254,"/**
* Expands a list of command-line arguments into PathData objects.
* @param args list of string arguments to expand
* @return linked list of expanded PathData objects or null if expansion fails
*/","*  Expands a list of arguments into {@link PathData} objects.  The default
   *  behavior is to call {@link #expandArgument(String)} on each element
   *  which by default globs the argument.  The loop catches IOExceptions,
   *  increments the error count, and displays the exception.
   * @param args strings to expand into {@link PathData} objects
   * @return list of all {@link PathData} objects the arguments
   * @throws IOException if anything goes wrong...",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,open,"org.apache.hadoop.io.MapFile$Reader:open(org.apache.hadoop.fs.Path,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",532,556,"/**
* Opens a SequenceFile for data and index access.
* @param dir directory containing files
* @param comparator optional key comparator
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,adjustPriorityQueue,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:adjustPriorityQueue(org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor),3598,3609,"/**
* Adjusts priority queue by reading and processing next segment.
* @param ms SegmentDescriptor object containing stream and position information
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$BlockCompressWriter:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",1609,1620,"/**
* Initializes the BlockCompressWriter with configuration and options.
* @param conf Configuration object
* @param options additional configuration options
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$RecordCompressWriter:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",1539,1542,"/**
 * Initializes a Record Compress Writer with configurable settings.
 * @param conf configuration object
 */",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,getTokenFile,"org.apache.hadoop.security.token.DtFileOperations:getTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)",175,220,"/**
* Fetches and stores tokens for a given service using DtFetcher implementations.
* @param tokenFile target file to store tokens
* @param fileFormat format of the stored tokens (e.g. JSON)
* @param alias optional alias for the service
* @param service service designation
* @param url URL associated with the service
* @param renewer token renewal information
* @param conf configuration object
*/","Fetch a token from a service and save to file in the local filesystem.
   *  @param tokenFile a local File object to hold the output.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias overwrite service field of fetched token with this text.
   *  @param service use a DtFetcher implementation matching this service text.
   *  @param url pass this URL to fetcher after stripping any http/s prefix.
   *  @param renewer pass this renewer to the fetcher.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,aliasTokenFile,"org.apache.hadoop.security.token.DtFileOperations:aliasTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",230,243,"/**
* Creates and writes an alias for a service in the given token file.
* @param tokenFile file to store token credentials
* @param fileFormat format of the output file (e.g. JSON)
* @param alias new service name for aliasing
* @param service original service name being aliased
* @param conf configuration settings for writing the file
*/","Alias a token from a file and save back to file in the local filesystem.
   *  @param tokenFile a local File object to hold the input and output.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias overwrite service field of fetched token with this text.
   *  @param service only apply alias to tokens matching this service text.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,appendTokenFiles,"org.apache.hadoop.security.token.DtFileOperations:appendTokenFiles(java.util.ArrayList,java.lang.String,org.apache.hadoop.conf.Configuration)",251,264,"/**
* Merges multiple token files into a single Credentials object and writes it to disk.
* @param tokenFiles list of token storage files to merge
* @param fileFormat output file format (e.g. HDFS, local file system)
* @param conf Hadoop Configuration object for storage settings
*/","Append tokens from list of files in local filesystem, saving to last file.
   *  @param tokenFiles list of local File objects.  Last file holds the output.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,removeTokenFromFile,"org.apache.hadoop.security.token.DtFileOperations:removeTokenFromFile(boolean,java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",275,291,"/**
* Cancels and updates managed tokens from a file.
* @param cancel whether to cancel managed tokens
* @param tokenFile file containing stored credentials
* @param fileFormat output format of the updated file
* @param alias Text alias for matching tokens
* @param conf Hadoop configuration
*/","Remove a token from a file in the local filesystem, matching alias.
   *  @param cancel cancel token as well as remove from file.
   *  @param tokenFile a local File object.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias remove only tokens matching alias; null matches all.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.
   *  @throws InterruptedException if the thread is interrupted.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,renewTokenFile,"org.apache.hadoop.security.token.DtFileOperations:renewTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",301,313,"/**
* Renews tokens in the specified file and updates it with the new credentials.
* @param tokenFile File containing token storage
* @param fileFormat Format to write the updated credentials in
* @param alias Text alias for matching managed tokens
* @param conf Application configuration
*/","Renew a token from a file in the local filesystem, matching alias.
   *  @param tokenFile a local File object.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias renew only tokens matching alias; null matches all.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.
   *  @throws InterruptedException if the thread is interrupted.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,importTokenFile,"org.apache.hadoop.security.token.DtFileOperations:importTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,java.lang.String,org.apache.hadoop.conf.Configuration)",323,339,"/**
* Imports tokens from a file and adds them to the credentials.
* @param tokenFile File containing token data
* @param fileFormat Format of the token file (e.g. JSON)
* @param alias Text alias for the token service
* @param base64 Base64 encoded token string
* @param conf Configuration object for logging and storage
*/","Import a token from a base64 encoding into the local filesystem.
   * @param tokenFile A local File object.
   * @param fileFormat A string equal to FORMAT_PB or FORMAT_JAVA, for output.
   * @param alias overwrite Service field of fetched token with this text.
   * @param base64 urlString Encoding of the token to import.
   * @param conf Configuration object passed along.
   * @throws IOException Error to import the token into the file.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,putMetrics,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),822,861,"/**
* Writes metrics record to output stream and flushes data to disk.
* @param record MetricsRecord object containing timestamped data
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,getJarsInDirectory,org.apache.hadoop.fs.FileUtil:getJarsInDirectory(java.lang.String),1764,1766,"/**
* Retrieves a list of JAR files in the specified directory.
* @param path absolute path to the directory
*/","* Returns all jars that are in the directory. It is useful in expanding a
   * wildcard path to return all jars from the directory to use in a classpath.
   * It operates only on local paths.
   *
   * @param path the path to the directory. The path may include the wildcard.
   * @return the list of jars as URLs, or an empty list if there are no jars, or
   * the directory does not exist locally",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,expandWildcard,"org.apache.hadoop.util.GenericOptionsParser:expandWildcard(java.util.List,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)",495,512,"/**
* Expands a wildcard directory into individual JAR paths.
* @param finalPaths list to append expanded JAR paths
* @param path directory containing JARs
* @throws IOException if an I/O error occurs
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersion(java.lang.String),771,775,"/**
* Creates a new version of the specified key.
* @param name unique key identifier
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersion(java.lang.String,byte[])",777,786,"/**
* Creates a new key version with the given name and material.
* @param name unique identifier for the new version
* @param material cryptographic material to use
* @return KeyVersion object representing the new version
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,deleteKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:deleteKey(java.lang.String),497,507,"/**
* Deletes a KMS key by name.
* @param name unique identifier of the key to delete
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String,byte[])",509,520,"/**
* Rolls a new version of the key with given name and material.
* @param name unique identifier for the key
* @param material byte array representing the key's material
* @return KeyVersion object representing the new key version
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String),522,541,"/**
* Rolls a new key version with the given name.
* @param name unique identifier for the new key
* @return KeyVersion object representing the new key or null on failure
*/",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)",398,402,"/**
* Legacy constructor to initialize server with deprecated parameters.
* @param instance 
* @param conf configuration object
* @param bindAddress server binding address
* @param port server listen port
*/","* Construct an RPC server.
     * @param instance the instance whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * 
     * @deprecated Use #Server(Class, Object, Configuration, String, int)
     * @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processOneRpc,org.apache.hadoop.ipc.Server$Connection:processOneRpc(java.nio.ByteBuffer),2785,2823,"/**
* Processes a single RPC request from the client.
* @param bb ByteBuffer containing the RPC request data
*/","* Process one RPC Request from buffer read from socket stream 
     *  - decode rpc in a rpc-Call
     *  - handle out-of-band RPC requests such as the initial connectionContext
     *  - A successfully decoded RpcCall will be deposited in RPC-Q and
     *    its response will be sent later when the request is processed.
     * 
     * Prior to this call the connectionHeader (""hrpc..."") has been handled and
     * if SASL then SASL has been established and the buf we are passed
     * has been unwrapped from SASL.
     * 
     * @param bb - contains the RPC request header and the rpc request
     * @throws IOException - internal error that should not be returned to
     *         client, typically failure to respond to client
     * @throws InterruptedException",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,process,org.apache.hadoop.log.LogLevel$CLI:process(java.lang.String),292,311,"/**
* Retrieves and processes data from a specified URL.
* @param urlString unique URL string to fetch data from
*/","* Configures the client to send HTTP/HTTPS request to the URL.
     * Supports SPENGO for authentication.
     * @param urlString URL and query string to the daemon's web UI
     * @throws Exception if unable to connect",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createProviders,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory:createProviders(org.apache.hadoop.conf.Configuration,java.net.URL,int,java.lang.String)",311,326,"/**
* Creates an array of KMSClientProviders from a list of hosts.
* @param conf configuration object
* @param origUrl original URL to generate provider URLs from
* @param port port number for provider URLs
* @param hostsPart comma-separated list of host names
* @return array of KMSClientProvider objects",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,run,org.apache.hadoop.ha.HAAdmin:run(java.lang.String[]),347,361,"/**
* Executes a command with given arguments and handles exceptions.
* @param argv array of command-line arguments
* @return int result code or -1 on failure
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,completeLocalOutput,"org.apache.hadoop.fs.FileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2686,2689,"/**
* Copies temporary local file to final output location.
* @param fsOutputFile destination path on file system
* @param tmpLocalFile temporary file to copy from
*/","* Called when we're all done writing to the target.
   * A local FS will do nothing, because we've written to exactly the
   * right place.
   * A remote FS will copy the contents of tmpLocalFile to the correct target at
   * fsOutputFile.
   * @param fsOutputFile path of output file
   * @param tmpLocalFile path to local tmp file
   * @throws IOException IO failure",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1019,1043,"/**
* Copies a file or directory from HDFS to local storage.
* @param src source path in HDFS
* @param dst destination path on local FS
* @param copyCrc whether to copy checksum file as well
*/","* The src file is under FS, and the dst is on the local disk.
   * Copy it from FS control to the local dst name.
   * If src and dst are directories, the copyCrc parameter
   * determines whether to copy CRC files.
   * @param src src path.
   * @param dst dst path.
   * @param copyCrc copy csc flag.
   * @throws IOException if an I/O error occurs.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",145,148,"/**
* Retrieves local write path based on input parameters.
* @param pathStr string representation of the path
* @param size file size in bytes
* @param conf Hadoop configuration object
*/","Get a path from the local FS. Pass size as 
   *  SIZE_UNKNOWN if not known apriori. We
   *  round-robin over the set of disks (via the configured dirs) and return
   *  the first complete path which has enough space 
   *  @param pathStr the requested path (this will be created on the first 
   *  available disk)
   *  @param size the size of the file that is going to be written
   *  @param conf the Configuration object
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,createTmpFileForWrite,"org.apache.hadoop.fs.LocalDirAllocator:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",211,215,"/**
* Creates a temporary file for write operations.
* @param pathStr path string for the temporary file
* @param size expected size of the file in bytes
* @param conf Hadoop configuration object
* @return File object representing the temporary file or null on failure
*/","Creates a temporary file in the local FS. Pass size as -1 if not known 
   *  apriori. We round-robin over the set of disks (via the configured dirs) 
   *  and select the first complete path which has enough space. A file is
   *  created on this directory. The file is guaranteed to go away when the
   *  JVM exits.
   *  @param pathStr prefix for the temporary file
   *  @param size the size of the file that is going to be written
   *  @param conf the Configuration object
   *  @return a unique temporary file
   *  @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processRawArguments,org.apache.hadoop.fs.shell.Command:processRawArguments(java.util.LinkedList),229,232,"/**
 * Processes raw command-line arguments.
 * @param args list of raw argument strings
 */","* Allows commands that don't use paths to handle the raw arguments.
   * Default behavior is to expand the arguments via
   * {@link #expandArguments(LinkedList)} and pass the resulting list to
   * {@link #processArguments(LinkedList)} 
   * @param args the list of argument strings
   * @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",490,499,"/**
* Opens an HDFS sequence file reader.
* @param dir path to the directory containing the file
* @param conf Hadoop configuration object
* @param opts options for opening the reader (may include comparator)
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:next(),3565,3591,"/**
* Advances to the next segment in priority queue order.
* @throws IOException if an I/O error occurs
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",274,294,"/**
* Creates a writer with optional compression based on provided options.
* @param conf configuration object
* @param opts writer options (compression type can be specified here)
* @return Writer instance or throws IOException if an error occurs
*/","* Create a new Writer with the given options.
   * @param conf the configuration to use
   * @param opts the options to create the file with
   * @return a new Writer
   * @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Get:execute(),241,245,"/**
* Fetches token file using DtFileOperations.
* @throws Exception if operation fails
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Edit:execute(),271,277,"/**
* Alias tokens in multiple files using provided configuration and aliases.
* @throws Exception if an error occurs during file operations
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Append:execute(),289,292,"/**
* Executes token file operations.
* @throws Exception if an error occurs during execution
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Remove:execute(),320,326,"/**
* Removes tokens from specified files.
* @throws Exception if any file operation fails
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Renew:execute(),350,355,"/**
* Executes file operations to renew tokens for each specified file.
* @throws Exception if an error occurs during execution
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Import:execute(),381,385,"/**
* Imports token file using DtFileOperations.
* @throws Exception if import operation fails
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,createJarWithClassPath,"org.apache.hadoop.fs.FileUtil:createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Map)",1668,1753,"/**
* Creates a JAR file with the specified classpath.
* @param inputClassPath comma-separated list of classpath entries
* @param pwd working directory for the JAR file
* @param targetDir base directory for relative classpath entries
* @param callerEnv environment variables to replace in classpath entries
* @return array containing the path to the created JAR file and an unexpanded wildcard classpath entry (if any)
*/","* Create a jar file at the given path, containing a manifest with a classpath
   * that references all specified entries.
   *
   * Some platforms may have an upper limit on command line length.  For example,
   * the maximum command line length on Windows is 8191 characters, but the
   * length of the classpath may exceed this.  To work around this limitation,
   * use this method to create a small intermediate jar with a manifest that
   * contains the full classpath.  It returns the absolute path to the new jar,
   * which the caller may set as the classpath for a new process.
   *
   * Environment variable evaluation is not supported within a jar manifest, so
   * this method expands environment variables before inserting classpath entries
   * to the manifest.  The method parses environment variables according to
   * platform-specific syntax (%VAR% on Windows, or $VAR otherwise).  On Windows,
   * environment variables are case-insensitive.  For example, %VAR% and %var%
   * evaluate to the same value.
   *
   * Specifying the classpath in a jar manifest does not support wildcards, so
   * this method expands wildcards internally.  Any classpath entry that ends
   * with * is translated to all files at that path with extension .jar or .JAR.
   *
   * @param inputClassPath String input classpath to bundle into the jar manifest
   * @param pwd Path to working directory to save jar
   * @param targetDir path to where the jar execution will have its working dir
   * @param callerEnv Map {@literal <}String, String{@literal >} caller's
   * environment variables to use for expansion
   * @return String[] with absolute path to new jar in position 0 and
   *   unexpanded wild card entry path in position 1
   * @throws IOException if there is an I/O error while writing the jar file",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,constructUrlsFromClasspath,org.apache.hadoop.util.ApplicationClassLoader:constructUrlsFromClasspath(java.lang.String),107,126,"/**
* Constructs an array of URLs from the given classpath, resolving directories and files to absolute URLs.
* @param classpath classpath string containing directory or file paths
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,validateFiles,"org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String,boolean)",425,488,"/**
* Validates and expands a comma-separated list of file names or wildcards.
* @param files comma-separated list of file names or wildcards
* @param expandWildcard whether to expand wildcard patterns
* @return string representation of the validated file paths, or null if invalid
*/","* takes input as a comma separated list of files
   * and verifies if they exist. It defaults for file:///
   * if the files specified do not have a scheme.
   * it returns the paths uri converted defaulting to file:///.
   * So an input of  /home/user/file1,/home/user/file2 would return
   * file:///home/user/file1,file:///home/user/file2.
   *
   * @param files the input files argument
   * @param expandWildcard whether a wildcard entry is allowed and expanded. If
   * true, any directory followed by a wildcard is a valid entry and is replaced
   * with the list of jars in that directory. It is used to support the wildcard
   * notation in a classpath.
   * @return a comma-separated list of validated and qualified paths, or null
   * if the input files argument is null",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,readAndProcess,org.apache.hadoop.ipc.Server$Connection:readAndProcess(),2478,2565,"/**
* Continuously reads and processes incoming data from a channel until a fatal response is sent or an error occurs.
*/","* This method reads in a non-blocking fashion from the channel: 
     * this method is called repeatedly when data is present in the channel; 
     * when it has enough data to process one rpc it processes that rpc.
     * 
     * On the first pass, it processes the connectionHeader, 
     * connectionContext (an outOfBand RPC) and at most one RPC request that 
     * follows that. On future passes it will process at most one RPC request.
     *  
     * Quirky things: dataLengthBuffer (4 bytes) is used to read ""hrpc"" OR 
     * rpc request length.
     *    
     * @return -1 in case of error, else num bytes read so far
     * @throws IOException - internal error that should not be returned to
     *         client, typically failure to respond to client
     * @throws InterruptedException - if the thread is interrupted.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,unwrapPacketAndProcessRpcs,org.apache.hadoop.ipc.Server$Connection:unwrapPacketAndProcessRpcs(byte[]),2733,2767,"/**
* Unwraps and processes incoming RPCs from a SaslServer response.
* @param inBuf input byte array to unwrap
*/","* Process a wrapped RPC Request - unwrap the SASL packet and process
     * each embedded RPC request 
     * @param inBuf - SASL wrapped request of one or more RPCs
     * @throws IOException - SASL packet cannot be unwrapped
     * @throws InterruptedException",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,doGetLevel,org.apache.hadoop.log.LogLevel$CLI:doGetLevel(),236,238,"/**
* Sends GET request to fetch log level from server.
*/","* Send HTTP/HTTPS request to get log level.
     *
     * @throws HadoopIllegalArgumentException if arguments are invalid.
     * @throws Exception if unable to connect",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,doSetLevel,org.apache.hadoop.log.LogLevel$CLI:doSetLevel(),246,249,"/**
* Sends HTTP request to set log level.
* @throws Exception on protocol or network errors
*/","* Send HTTP/HTTPS request to set log level.
     *
     * @throws HadoopIllegalArgumentException if arguments are invalid.
     * @throws Exception if unable to connect",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createProvider,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)",279,309,"/**
* Creates a KeyProvider instance for the given URI and configuration.
* @param providerUri URI of the KMS client provider
* @param conf Configuration object
* @return LoadBalancingKMSClientProvider instance or null if not found
*/","* This provider expects URIs in the following form :
     * {@literal kms://<PROTO>@<AUTHORITY>/<PATH>}
     *
     * where :
     * - PROTO = http or https
     * - AUTHORITY = {@literal <HOSTS>[:<PORT>]}
     * - HOSTS = {@literal <HOSTNAME>[;<HOSTS>]}
     * - HOSTNAME = string
     * - PORT = integer
     *
     * This will always create a {@link LoadBalancingKMSClientProvider}
     * if the uri is correct.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,completeLocalOutput,"org.apache.hadoop.fs.FilterFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",408,412,"/**
* Completes local output by delegating to FileSystems.
*/","* Called when we're all done writing to the target.  A local FS will
   * do nothing, because we've written to exactly the right place.  A remote
   * FS will copy the contents of tmpLocalFile to the correct target at
   * fsOutputFile.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,createTmpFileForWrite,"org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",830,838,"/**
* Creates a temporary writable file with specified name and size.
* @param pathStr file system path for the file
* @param size expected size of the file in bytes
* @param conf configuration object (not used)
* @return File object representing the created temp file
*/","* Demand create the directory allocator, then create a temporary file.
     * This does not mark the file for deletion when a process exits.
     * {@link LocalDirAllocator#createTmpFileForWrite(String, long, Configuration)}.
     *
     * @param pathStr prefix for the temporary file.
     * @param size    the size of the file that is going to be written.
     * @param conf    the Configuration object.
     * @return a unique temporary file.
     * @throws IOException IO problems",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,org.apache.hadoop.conf.Configuration)",129,132,"/**
* Returns local write path for given file string and configuration.
* @param pathStr file string
* @param conf Hadoop configuration object
*/","Get a path from the local FS. This method should be used if the size of 
   *  the file is not known apriori. We go round-robin over the set of disks
   *  (via the configured dirs) and return the first complete path where
   *  we could create the parent directory of the passed path. 
   *  @param pathStr the requested path (this will be created on the first 
   *  available disk)
   *  @param conf the Configuration object
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,run,org.apache.hadoop.fs.shell.Command:run(java.lang.String[]),184,201,"/**
* Executes command with optional arguments and handles deprecated warnings.
* @param argv variable number of command line arguments
* @return process exit code or error status
*/","* Invokes the command handler.  The default behavior is to process options,
   * expand arguments, and then process each argument.
   * <pre>
   * run
   * |{@literal ->} {@link #processOptions(LinkedList)}
   * \{@literal ->} {@link #processRawArguments(LinkedList)}
   *      |{@literal ->} {@link #expandArguments(LinkedList)}
   *      |   \{@literal ->} {@link #expandArgument(String)}*
   *      \{@literal ->} {@link #processArguments(LinkedList)}
   *          |{@literal ->} {@link #processArgument(PathData)}*
   *          |   |{@literal ->} {@link #processPathArgument(PathData)}
   *          |   \{@literal ->} {@link #processPaths(PathData, PathData...)}
   *          |        \{@literal ->} {@link #processPath(PathData)}*
   *          \{@literal ->} {@link #processNonexistentPath(PathData)}
   * </pre>
   * Most commands will chose to implement just
   * {@link #processOptions(LinkedList)} and {@link #processPath(PathData)}
   * 
   * @param argv the list of command line arguments
   * @return the exit code for the command
   * @throws IllegalArgumentException if called with invalid arguments",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,"org.apache.hadoop.io.ArrayFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",101,104,"/**
 * Initializes a new Reader instance with the specified configuration and file.
 * @param fs FileSystem instance
 * @param file File path to read from
 * @param conf Configuration object for reader settings
 */","* Construct an array reader for the named file.
     * @param fs FileSystem.
     * @param file file.
     * @param conf configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",124,127,"/**
* Initializes a new Reader instance with the specified file system,
* directory name, and writable comparator.
* @param fs file system to access
* @param dirName directory path
* @param comparator comparator for sorting keys
* @param conf configuration parameters
*/","* Construct a set reader for the named set using the named comparator.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param comparator input comparator.
     * @param conf input Configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",510,514,"/**
* Creates a Reader for the specified directory.
* @param fs file system instance
* @param dirName name of the directory to read from
* @param conf configuration object
*/","* Construct a map reader for the named map.
     * @deprecated
     *
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param conf configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",526,530,"/**
* Creates a new reader instance with specified configuration and comparator.
* @param fs FileSystem instance
* @param dirName directory name
* @param comparator WritableComparator instance
* @param conf Configuration object
* @throws IOException if an I/O error occurs
*/","* Construct a map reader for the named map using the named comparator.
     * @deprecated
     *
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param comparator WritableComparator.
     * @param conf Configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",213,217,"/**
* Initializes a SequenceFile reader from a directory and configuration.
* @param dir path to the directory containing the file
* @param conf Hadoop configuration object
* @param options sequence file reader options (optional)
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,cloneFileAttributes,"org.apache.hadoop.io.SequenceFile$Sorter:cloneFileAttributes(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)",3406,3422,"/**
* Creates a new writer with the same compression attributes as the input file.
* @param inputFile path to the original file
* @param outputFile path to the cloned file
* @param prog progress monitor
* @return Writer object for the cloned file or null on failure
*/","* Clones the attributes (like compression of the input file and creates a 
     * corresponding Writer
     * @param inputFile the path of the input file whose attributes should be 
     * cloned
     * @param outputFile the path of the output file 
     * @param prog the Progressable to report status during the file write
     * @return Writer
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,fix,"org.apache.hadoop.io.MapFile:fix(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.conf.Configuration)",934,1014,"/**
* Creates an index for a given directory, writing key positions to the index
* @param fs Hadoop FileSystem instance
* @param dir directory path where the data and index files are stored
* @param keyClass Class of the keys in the data file
* @param valueClass Class of the values in the data file
* @param dryrun whether this is a dry run, writing nothing to disk
* @param conf Hadoop Configuration instance
* @return number of key-value pairs written to the index or -1 if no fixing was needed
*/","* This method attempts to fix a corrupt MapFile by re-creating its index.
   * @param fs filesystem
   * @param dir directory containing the MapFile data and index
   * @param keyClass key class (has to be a subclass of Writable)
   * @param valueClass value class (has to be a subclass of Writable)
   * @param dryrun do not perform any changes, just report what needs to be done
   * @param conf configuration.
   * @return number of valid entries in this MapFile, or -1 if no fixing was needed
   * @throws Exception Exception.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,flush,"org.apache.hadoop.io.SequenceFile$Sorter$SortPass:flush(int,int,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,boolean)",3217,3251,"/**
* Flushes a sorted segment to disk.
* @param count number of records in the segment
* @param bytesProcessed total bytes processed so far
* @param compressionType type of compression used
* @param codec compression codec instance
* @param done whether this is the final flush (true) or intermediate flush (false)
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$Writer$Option[])",312,357,"/**
* Creates a new SequenceFile writer with options for data and index files.
* @param conf Hadoop configuration
* @param dirName directory path to create
* @param opts additional options (key class or comparator)
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)",308,315,"/**
* Creates a writer instance from configuration and filesystem settings.
* @param fs FileSystem to operate on
* @param conf Configuration for the writer
* @param name Path to write data to
* @param keyClass Class of keys in the output data
* @param valClass Class of values in the output data
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",330,339,"/**
* Creates a new writer instance with specified configuration and file system. 
* @param fs underlying file system
* @param conf Hadoop configuration object
* @param name path to the output file
* @param keyClass class of the key data type
* @param valClass class of the value data type
* @param compressionType compression type for writer
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",355,366,"/**
* Creates a new writer instance.
* @param fs File system to operate on
* @param conf Configuration object
* @param name Path to write to
* @param keyClass Class of key values
* @param valClass Class of value data
* @param compressionType Compression type for output
* @param progress Progressable callback for updates
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param progress The Progressable object to track progress.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",382,392,"/**
* Creates a new writer instance with specified configuration and compression settings.
* @param fs       FileSystem instance
* @param conf     Configuration object
* @param name     Path to the file or directory
* @param keyClass Class of key data type
* @param valClass Class of value data type
* @param compressionType Compression type (e.g. NONE, DEFLATE)
* @param codec     Compression codec instance
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",410,423,"/**
* Creates a new writer with specified configuration and metadata.
* @param fs file system instance
* @param conf configuration settings
* @param name output path
* @param keyClass data key class
* @param valClass data value class
* @param compressionType compression type (optional)
* @param codec compression codec (optional)
* @param progress progress update callback (optional)
* @param metadata additional metadata (optional)
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param progress The Progressable object to track progress.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",444,461,"/**
* Creates a writer instance with specified configuration and settings.
* @param fs the file system to use
* @param conf Hadoop configuration object
* @return a configured writer instance or null on failure
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem.
   * @param conf The configuration.
   * @param name The name of the file.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param bufferSize buffer size for the underlaying outputstream.
   * @param replication replication factor for the file.
   * @param blockSize block size for the file.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param progress The Progressable object to track progress.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",539,551,"/**
* Creates a writer for Hadoop data storage.
* @param fs FileSystem object
* @param conf Configuration object
* @param name Path to the file
* @param keyClass Class of the keys
* @param valClass Class of the values
* @param compressionType Type of compression
* @param codec Compression Codec object
* @param progress Progressable object for monitoring
* @return Writer object or null on failure
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param progress The Progressable object to track progress.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)",567,577,"/**
* Creates a new writer with specified configuration and compression settings.
* @param conf Hadoop Configuration object
* @throws IOException if an I/O error occurs
*/","* Construct the preferred type of 'raw' SequenceFile Writer.
   * @param conf The configuration.
   * @param out The stream on top which the writer is to be constructed.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",592,600,"/**
* Creates a writer for Hadoop data output stream.
* @param conf Hadoop configuration
* @param out FSDataOutputStream to write to
* @return Writer object or null on failure
*/","* Construct the preferred type of 'raw' SequenceFile Writer.
   * @param conf The configuration.
   * @param out The stream on top which the writer is to be constructed.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,createJarWithClassPath,"org.apache.hadoop.fs.FileUtil:createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,java.util.Map)",1632,1635,"/**
* Creates a JAR file with the specified class path and environment variables.
* @param inputClassPath class path to include in the JAR
* @param pwd current working directory
* @param callerEnv environment variables for the caller process
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,<init>,"org.apache.hadoop.util.ApplicationClassLoader:<init>(java.lang.String,java.lang.ClassLoader,java.util.List)",102,105,"/**
* Constructs an ApplicationClassLoader instance.
* @param classpath path to the application's classes
* @param parent parent ClassLoader (optional)
* @param systemClasses list of system classes to include
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,validateFiles,org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String),405,407,"/**
* Validates file list with default error handling.
* @param files comma-separated list of file paths
*/","* Takes input as a comma separated list of files
   * and verifies if they exist. It defaults for file:///
   * if the files specified do not have a scheme.
   * it returns the paths uri converted defaulting to file:///.
   * So an input of  /home/user/file1,/home/user/file2 would return
   * file:///home/user/file1,file:///home/user/file2.
   *
   * This method does not recognize wildcards.
   *
   * @param files the input files argument
   * @return a comma-separated list of validated and qualified paths, or null
   * if the input files argument is null",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRead,org.apache.hadoop.ipc.Server$Listener:doRead(java.nio.channels.SelectionKey),1641,1671,"/**
* Processes incoming data from client connection and updates server state.
* @param key SelectionKey representing the client connection
*/",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,sendLogLevelRequest,org.apache.hadoop.log.LogLevel$CLI:sendLogLevelRequest(),126,139,"/**
* Handles log level requests based on operation type.
* @throws HadoopIllegalArgumentException if invalid operation is specified
*/","* Send HTTP/HTTPS request to the daemon.
     * @throws HadoopIllegalArgumentException if arguments are invalid.
     * @throws Exception if unable to connect",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,create,"org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",807,817,"/**
* Creates a DataBlock instance by writing to temporary file.
* @param index unique block identifier
* @param limit maximum block size
* @return DataBlock object or throws IOException on failure
*/","* Create a temp file and a {@link DiskBlock} instance to manage it.
     *
     * @param index      block index.
     * @param limit      limit of the block.
     * @param statistics statistics to update.
     * @return the new block.
     * @throws IOException IO problems",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,getTempFilePath,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getTempFilePath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",649,658,"/**
* Generates a temporary file path with specific permissions.
* @param conf Hadoop configuration
* @param localDirAllocator local directory allocator
* @return Path to the generated temp file
*/","* Create temporary file based on the file path retrieved from local dir allocator
   * instance. The file is created with .bin suffix. The created file has been granted
   * posix file permissions available in TEMP_FILE_ATTRS.
   *
   * @param conf the configuration.
   * @param localDirAllocator the local dir allocator instance.
   * @return path of the file created.
   * @throws IOException if IO error occurs while local dir allocator tries to retrieve path
   * from local FS or file creation fails or permission set fails.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,run,org.apache.hadoop.fs.FsShell:run(java.lang.String[]),300,351,"/**
* Runs the FsShell command with optional arguments.
* @param argv array of command-line arguments
* @return int exit code or -1 on failure
*/",* run,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",112,114,"/**
* Initializes a Reader instance with file system, directory name and configuration.
* @param fs the file system to operate on
* @param dirName the directory name
* @param conf the Hadoop configuration","* Construct a set reader for the named set.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param conf input Configuration.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",219,223,"/**
* Constructs a Reader instance from a directory name.
* @param dirName path to the directory
* @param conf configuration object
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,boolean)",225,229,"/**
* Creates a reader from a file system and directory name.
* @param fs the file system instance
* @param dirName the directory name
* @param comparator the writable comparator
* @param conf configuration object
* @param open whether to open the reader
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",231,235,"/**
* Constructs a Reader instance from the given configuration and comparator.
* @param fs FileSystem object
* @param dirName directory name
* @param comparator WritableComparator instance
* @param conf Configuration object
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:merge(),3623,3723,"/**
* Performs a multi-level merge of sorted segments.
*/","This is the single level merge that is called multiple times 
       * depending on the factor size and the number of segments
       * @return RawKeyValueIterator
       * @throws IOException",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,run,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:run(boolean),3100,3179,"/**
* Processes input files in batches, handling compression and sorting.
* @param deleteInput whether to delete input files after processing
* @return number of processed segments
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.io.SequenceFile$CompressionType)",82,89,"/**
* Initializes a new SequenceFile writer with the specified configuration.
* @param conf Hadoop Configuration object
* @param fs HDFS file system instance
* @param dirName directory name for the sequence file
* @param comparator key comparator to use
* @param compress compression type (e.g. NONE, DEFLATE)
*/","* Create a set naming the element comparator and compression type.
     *
     * @param conf input Configuration.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param comparator input comparator.
     * @param compress input compress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$Writer$Option[])",158,164,"/**
* Initializes a new Writer instance for sequence file operations.
* @param conf Hadoop configuration
* @param dir directory path where the sequence file will be written
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,"org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)",50,55,"/**
* Initializes a Hadoop writer with configuration and file system.
* @param conf Hadoop configuration
* @param fs Hadoop file system
* @param file path to the data file
* @param valClass class of writable values
*/","* Create the named file for values of the named class.
     *
     * @param conf configuration.
     * @param fs file system.
     * @param file file.
     * @param valClass valClass.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,"org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",68,77,"/**
 * Initializes a Hadoop writer with configuration and file settings.
 * @param conf Hadoop configuration
 * @param fs Hadoop file system
 * @param file output file path
 * @param valClass Writable class for values
 * @param compress compression type (e.g. NONE, DEFLATE)
 * @param progress progressable object for monitoring
 */","* Create the named file for values of the named class.
     *
     * @param conf configuration.
     * @param fs file system.
     * @param file file.
     * @param valClass valClass.
     * @param compress compress.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)",112,117,"/**
* Legacy constructor for creating a Hadoop Writer instance.
* @param conf Hadoop configuration
* @param fs file system reference
* @param dirName output directory name
* @param keyClass WritableComparable class type
* @param valClass value class type
*/","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs filesystem.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",132,139,"/**
* Creates a Hadoop Writer instance from configuration and directory name.
* @param conf Hadoop Configuration object
* @param dirName directory path for the writer
* @param keyClass class of writable comparable keys
* @param valClass class of writable values
* @param compress compression type for output data
* @param progress Progressable callback for writing progress
*/","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs fs.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @param compress compress.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",155,162,"/**
* Legacy constructor for creating a Writer instance.
* @param conf configuration object
* @param fs file system to operate on
* @param dirName directory name of the output location
* @param keyClass class type of the key WritableComparable
* @param valClass class type of the value
* @param compress compression type for data storage
* @param codec specific compression algorithm to use
* @param progress progress callback object","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @param compress compress.
     * @param codec codec.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",175,181,"/**
* Convenience constructor for creating a Writer instance.
* @param conf Hadoop configuration
* @param fs File system
* @param dirName directory name
* @param keyClass WritableComparable class
* @param valClass value class
* @param compress compression type
*/","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     * @param conf configuration.
     * @param fs fs.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @param compress compress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)",192,198,"/**
* Initializes a Writer instance with the given configuration, file system, and directory name.
* @param conf configuration object
* @param fs file system object
* @param dirName name of the directory
* @param comparator writable comparator to use
* @param valClass class type for values
* @throws IOException if an I/O error occurs","Create the named map using the named key comparator. 
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     * @param conf configuration.
     * @param fs fs.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",210,216,"/**
* Constructs a Writer instance with the given configuration and file system.
* @param conf Configuration object
* @param fs File system instance
* @param dirName Directory name for the sequence file
* @param comparator Comparator class for key comparison
* @param valClass Class of values stored in the sequence file
* @param compress Compression type for the sequence file
*/","Create the named map using the named key comparator.
     * @param conf configuration.
     * @param fs filesystem.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @param compress compress.
     * @throws IOException raised on errors performing I/O.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",231,239,"/**
* Constructs a SequenceFileWriter instance with default parameters.
* @param conf Hadoop configuration
* @param fs File system handle
* @param dirName directory name
* @param comparator key comparator
* @param valClass value class type
* @param compress compression type
* @param progress progress handler
*/","* Create the named map using the named key comparator.
     * @deprecated Use Writer(Configuration, Path, Option...)} instead.
     *
     * @param conf configuration.
     * @param fs filesystem.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @param compress CompressionType.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",255,263,"/**
* Creates a SequenceFile Writer with specified configuration.
* @param conf Configuration object
* @param fs FileSystem instance
* @param dirName directory name for the sequence file
* @param comparator WritableComparator instance
* @param valClass Class of value being written
* @param compress compression type
* @param codec CompressionCodec instance
* @param progress Progressable instance
*/","* Create the named map using the named key comparator.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @param compress CompressionType.
     * @param codec codec.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,open,"org.apache.hadoop.io.MapFile$Merger:open(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",1059,1090,"/**
* Initializes map file merge by opening input files and creating a writer for the output file.
* @param inMapFiles array of input file paths
* @param outMapFile path to the output file
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",513,522,"/**
* Creates a writer for the given file context and configuration.
* @param fc FileContext object
* @return Writer object or null on failure
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fc The context for the specified file.
   * @param conf The configuration.
   * @param name The name of the file.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param metadata The metadata of the file.
   * @param createFlag gives the semantics of create: overwrite, append etc.
   * @param opts file creation options; see {@link CreateOpts}.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Classpath.java,main,org.apache.hadoop.util.Classpath:main(java.lang.String[]),64,113,"/**
* Parses command-line arguments and writes classpath into a JAR file if requested.
* @param args command-line arguments
*/","* Main entry point.
   *
   * @param args command-line arguments",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,createClassLoader,"org.apache.hadoop.util.RunJar:createClassLoader(java.io.File,java.io.File)",344,383,"/**
* Creates a ClassLoader instance for the given file and work directory.
* @param file jar or zip file containing user classes
* @param workDir working directory where classes are stored
* @return ClassLoader instance or null on failure
*/","* Creates a classloader based on the environment that was specified by the
   * user. If HADOOP_USE_CLIENT_CLASSLOADER is specified, it creates an
   * application classloader that provides the isolation of the user class space
   * from the hadoop classes and their dependencies. It forms a class space for
   * the user jar as well as the HADOOP_CLASSPATH. Otherwise, it creates a
   * classloader that simply adds the user jar to the classpath.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,processGeneralOptions,org.apache.hadoop.util.GenericOptionsParser:processGeneralOptions(org.apache.commons.cli.CommandLine),291,364,"/**
* Processes general options from the command line.
* @param line CommandLine object containing parsed options
*/","* Modify configuration according user-specified generic options.
   *
   * @param line User-specified generic options",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRunLoop,org.apache.hadoop.ipc.Server$Listener$Reader:doRunLoop(),1486,1527,"/**
* Runs the event loop, consuming pending connections and processing reads.
* @throws InterruptedException if interrupted while waiting for I/O
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,run,org.apache.hadoop.log.LogLevel$CLI:run(java.lang.String[]),109,119,"/**
* Runs the application, parsing arguments and sending log level request.
* @param args command line arguments
* @return 0 on success, -1 on failure
*/",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,getCacheFilePath,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getCacheFilePath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",496,500,"/**
* Returns cache file path based on temporary file path.
* @param conf Hadoop configuration
* @param localDirAllocator allocator for local directories
*/","* Return temporary file created based on the file path retrieved from local dir allocator.
   *
   * @param conf The configuration object.
   * @param localDirAllocator Local dir allocator instance.
   * @return Path of the temporary file created.
   * @throws IOException if IO error occurs while local dir allocator tries to retrieve path
   * from local FS or file creation fails or permission set fails.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,isCacheSpaceAvailable,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:isCacheSpaceAvailable(long,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",621,633,"/**
* Checks if cache space is available for a given file size.
* @param fileSize size of the file to be cached
* @param conf configuration object
* @param localDirAllocator allocator for local directories
* @return true if sufficient cache space is available, false otherwise
*/","* Determine if the cache space is available on the local FS.
   *
   * @param fileSize The size of the file.
   * @param conf The configuration.
   * @param localDirAllocator Local dir allocator instance.
   * @return True if the given file size is less than the available free space on local FS,
   * False otherwise.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(java.util.List,org.apache.hadoop.fs.Path)",3313,3319,"/**
* Merges a list of segment descriptors into a single iterator.
* @param segments list of SegmentDescriptor objects
* @param tmpDir temporary directory for merge process
* @return RawKeyValueIterator instance or null on failure
*/","* Merges the list of segments of type <code>SegmentDescriptor</code>
     * @param segments the list of SegmentDescriptors
     * @param tmpDir the directory to write temporary files into
     * @return RawKeyValueIterator
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],boolean,int,org.apache.hadoop.fs.Path)",3349,3364,"/**
* Merges multiple input key-value stores into one.
* @param inNames array of input file paths
* @param deleteInputs whether to delete original inputs after merge
* @param factor scaling factor for merged output
* @param tmpDir temporary directory for merge process
* @return iterator over merged key-value pairs or null on failure
*/","* Merges the contents of files passed in Path[]
     * @param inNames the array of path names
     * @param deleteInputs true if the input files should be deleted when 
     * unnecessary
     * @param factor the factor that will be used as the maximum merge fan-in
     * @param tmpDir the directory to write temporary files into
     * @return RawKeyValueIteratorMergeQueue
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",3375,3394,"/**
* Merges multiple input files into a single output file.
* @param inNames array of input file paths
* @param tempDir temporary directory for merge outputs
* @param deleteInputs whether to delete original inputs
*/","* Merges the contents of files passed in Path[]
     * @param inNames the array of path names
     * @param tempDir the directory for creating temp files during merge
     * @param deleteInputs true if the input files should be deleted when 
     * unnecessary
     * @return RawKeyValueIteratorMergeQueue
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",3479,3489,"/**
* Merges two segments into a single output.
* @param inName input file path
* @param indexIn index file path
* @param tmpDir temporary directory for merge process
* @return RawKeyValueIterator instance or null on failure
*/","Used by mergePass to merge the output of the sort
     * @param inName the name of the input file containing sorted segments
     * @param indexIn the offsets of the sorted segments
     * @param tmpDir the relative directory to store intermediate results in
     * @return RawKeyValueIterator
     * @throws IOException",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sortPass,org.apache.hadoop.io.SequenceFile$Sorter:sortPass(boolean),3064,3076,"/**
* Performs a sorting pass on data using the SortPass and MergeSort algorithms.
* @param deleteInput true to delete input after processing, false otherwise
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",65,70,"/**
* Initializes a Writer for writing to a SequenceFile.
* @param conf Hadoop configuration
* @param fs FileSystem instance
* @param dirName directory name
* @param keyClass Class of the key WritableComparable
* @param compress compression type
*/","* Create a set naming the element class and compression type.
     *
     * @param conf input Configuration.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param keyClass input keyClass.
     * @param compress input compress.
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",90,97,"/**
* Wrappers constructor for deprecated Hadoop API.
* @param conf Hadoop configuration
* @param fs filesystem to use
* @param dirName directory name in the filesystem
* @param keyClass class of key WritableComparable
* @param valClass class of value Writable
* @param compress compression type
* @param codec compression codec
* @param progress progressable object for reporting progress
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",99,106,"/**
* Constructs a Writer instance from deprecated configuration.
* @param conf Configuration object
* @param fs FileSystem object
* @param dirName Directory name
* @param keyClass Key class type
* @param valClass Value class type
* @param compress Compression type
* @param progress Progressable object
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",108,115,"/**
* Legacy constructor for creating a Writer instance.
* @param conf Hadoop Configuration object
* @param fs HDFS FileSystem instance
* @param dirName directory path
* @param keyClass class of WritableComparable to use
* @param valClass class of values to store
* @param compress compression type",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",117,125,"/**
* Constructs a Writer instance from the given parameters.
* @param conf configuration object
* @param fs file system object
* @param dirName directory name
* @param comparator comparator object
* @param valClass value class
* @param compress compression type
* @param codec compression codec
* @param progress progressable object
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",127,134,"/**
* Constructor for a deprecated MapFileInputFormat. 
* @param conf Hadoop configuration
* @throws IOException if an I/O error occurs
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",136,142,"/**
* Constructor wrapper for creating a Writer object.",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)",144,149,"/**
* Convenience constructor for creating a Writer instance.
* @param conf Hadoop configuration object
* @param fs File system handle
* @param dirName directory path to write to
* @param comparator key comparator function
* @param valClass class of values being written
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)",151,156,"/**
* Legacy constructor to initialize a Writer instance.
* @param conf Hadoop configuration
* @param fs file system
* @param dirName output directory name
* @param keyClass WritableComparable class type
* @param valClass data value class type
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,main,org.apache.hadoop.io.MapFile:main(java.lang.String[]),1160,1191,"/**
* Copies a MapFile from one location to another.
* @param args input and output file names
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)",50,53,"/**
* Initializes a new Hadoop writer with specified file system and directory name.
* @param fs the underlying file system
* @param dirName the output directory path
* @param keyClass the type of writable comparable for key data
*/","* Create the named set for keys of the named class.
     * @deprecated pass a Configuration too
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param keyClass input keyClass.
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,merge,"org.apache.hadoop.io.MapFile$Merger:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)",1039,1053,"/**
* Merges input map files and writes output to a single file.
* @param inMapFiles array of input map files
* @param deleteInputs whether to delete the input files after merging
* @param outMapFile Path to write merged output to
*/","* Merge multiple MapFiles to one Mapfile.
     *
     * @param inMapFiles input inMapFiles.
     * @param deleteInputs deleteInputs.
     * @param outMapFile input outMapFile.
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,boolean,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)",480,496,"/**
* Creates a writer with specified configuration and settings.
* @param fs file system instance
* @param conf Hadoop configuration object
* @return a new Writer instance or null if creation fails
*/","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem.
   * @param conf The configuration.
   * @param name The name of the file.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param bufferSize buffer size for the underlaying outputstream.
   * @param replication replication factor for the file.
   * @param blockSize block size for the file.
   * @param createParent create parent directory if non-existent
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,run,org.apache.hadoop.util.RunJar:run(java.lang.String[]),248,334,"/**
* Runs a JAR file with the specified main class and arguments.
* @param args array of command-line arguments
*/",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,parseGeneralOptions,"org.apache.hadoop.util.GenericOptionsParser:parseGeneralOptions(org.apache.commons.cli.Options,java.lang.String[])",572,588,"/**
* Parses general command-line options.
* @param opts Options object to populate
* @param args Command-line arguments array
* @return true if parsing successful, false otherwise
*/","* Parse the user-specified options, get the generic options, and modify
   * configuration accordingly.
   *
   * @param opts Options to use for parsing args.
   * @param args User-specified arguments
   * @return true if the parse was successful",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Listener$Reader:run(),1472,1484,"/**
* Runs the event loop, attempting to close the read selector on exit.",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,put,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:put(int,java.nio.ByteBuffer,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",370,413,"/**
* Puts a block into the cache, updating internal data structures and statistics.
* @param blockNumber unique block identifier
* @param buffer ByteBuffer containing block data
* @throws IOException if an I/O error occurs during file operations
*/","* Puts the given block in this cache.
   *
   * @param blockNumber the block number, used as a key for blocks map.
   * @param buffer buffer contents of the given block to be added to this cache.
   * @param conf the configuration.
   * @param localDirAllocator the local dir allocator instance.
   * @throws IOException if either local dir allocator fails to allocate file or if IO error
   * occurs while writing the buffer content to the file.
   * @throws IllegalArgumentException if buffer is null, or if buffer.limit() is zero or negative.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)",3331,3337,"/**
* Merges multiple input key-value stores into a single iterator.
* @param inNames array of input store paths
* @param deleteInputs whether to delete original inputs on merge completion
* @param tmpDir temporary directory for merge operation
*/","* Merges the contents of files passed in Path[] using a max factor value
     * that is already set
     * @param inNames the array of path names
     * @param deleteInputs true if the input files should be deleted when 
     * unnecessary
     * @param tmpDir the directory to write temporary files into
     * @return RawKeyValueIteratorMergeQueue
     * @throws IOException raised on errors performing I/O.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,mergePass,org.apache.hadoop.io.SequenceFile$Sorter:mergePass(org.apache.hadoop.fs.Path),3458,3470,"/**
* Executes a merge pass on the output file.
* @param tmpDir temporary directory for merge operations
*/",sort calls this to generate the final merged output,,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,main,org.apache.hadoop.util.RunJar:main(java.lang.String[]),244,246,"/**
* Runs the application with command-line arguments.
* @param args array of command-line arguments
*/","Run a Hadoop job jar.  If the main class is not in the jar's manifest,
   * then it must be provided on the command line.
   *
   * @param args args.
   * @throws Throwable error.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,"org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])",178,182,"/**
* Initializes parser with configuration and command-line arguments.
* @param conf Configuration object
* @param options Options object
* @param args Array of command-line arguments
*/","* Create a <code>GenericOptionsParser</code> to parse given options as well 
   * as generic Hadoop options. 
   * 
   * The resulting <code>CommandLine</code> object can be obtained by 
   * {@link #getCommandLine()}.
   * 
   * @param conf the configuration to modify  
   * @param options options built by the caller 
   * @param args User-specified arguments
   * @throws IOException raised on errors performing I/O.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sortAndIterate,"org.apache.hadoop.io.SequenceFile$Sorter:sortAndIterate(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",3032,3052,"/**
* Sorts and iterates over input files, merging into a single output file.
* @param inFiles array of input file paths
* @param tempDir temporary directory for sorting and merging
* @return RawKeyValueIterator instance or null if no segments found
*/","* Perform a file sort from a set of input files and return an iterator.
     * @param inFiles the files to be sorted
     * @param tempDir the directory where temp files are created during sort
     * @param deleteInput should the input files be deleted as they are read?
     * @return iterator the RawKeyValueIterator
     * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",3445,3455,"/**
* Merges multiple input files into a single output file.
* @param inFiles array of input paths
* @param outFile path to the merged file
*/","Merge the provided files.
     * @param inFiles the array of input path names
     * @param outFile the final output file
     * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sort,"org.apache.hadoop.io.SequenceFile$Sorter:sort(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",3009,3022,"/**
* Sorts input files and merges them into a single output file.
* @param inFiles array of paths to input files
* @param outFile path to the merged output file
* @param deleteInput whether to delete original input files
*/","* Perform a file sort from a set of input files into an output file.
     * @param inFiles the files to be sorted
     * @param outFile the sorted output file
     * @param deleteInput should the input files be deleted as they are read?
     * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,<init>,"org.apache.hadoop.service.launcher.ServiceLauncher$MinimalGenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])",1081,1084,"/**
* Initializes parser with configuration and command-line arguments.
* @param conf Hadoop Configuration object
* @param options Command-line option set
* @param args Array of command-line arguments
*/",,,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,"org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.commons.cli.Options,java.lang.String[])",135,138,"/**
* Constructs a GenericOptionsParser instance with default configuration.
* @param opts Options object
* @param args Command-line arguments array
*/","* Create an options parser with the given options to parse the args.
   * @param opts the options
   * @param args the command line arguments
   * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,org.apache.hadoop.util.GenericOptionsParser:<init>(java.lang.String[]),145,148,"/**
* Constructs parser with default configuration and options.
* @param args input arguments
*/","* Create an options parser to parse the args.
   * @param args the command line arguments
   * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,"org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,java.lang.String[])",161,164,"/**
* Constructs GenericOptionsParser instance from configuration and command-line arguments.
* @param conf Configuration object
* @param args Command-line argument array","* Create a <code>GenericOptionsParser</code> to parse only the generic
   * Hadoop arguments.
   * 
   * The array of string arguments other than the generic arguments can be 
   * obtained by {@link #getRemainingArgs()}.
   * 
   * @param conf the <code>Configuration</code> to modify.
   * @param args command-line arguments.
   * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sort,"org.apache.hadoop.io.SequenceFile$Sorter:sort(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",3060,3062,"/**
 * Sorts input file and writes result to output file.
 * @param inFile path to unsorted input file
 * @param outFile path to sorted output file
 */","* The backwards compatible interface to sort.
     * @param inFile the input file to sort.
     * @param outFile the sorted output file.
     * @throws IOException raised on errors performing I/O.",,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,createGenericOptionsParser,"org.apache.hadoop.service.launcher.ServiceLauncher:createGenericOptionsParser(org.apache.hadoop.conf.Configuration,java.lang.String[])",979,982,"/**
* Creates GenericOptionsParser instance with minimal options.
* @param conf configuration object
* @param argArray array of command-line arguments
* @return GenericOptionsParser instance or null if failed
*/","* Override point: create a generic options parser or subclass thereof.
   * @param conf Hadoop configuration
   * @param argArray array of arguments
   * @return a generic options parser to parse the arguments
   * @throws IOException on any failure",,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfTest.java,main,org.apache.hadoop.util.ConfTest:main(java.lang.String[]),227,300,"/**
* Validates Hadoop configuration files.
* @param args command-line arguments
*/",,,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ToolRunner.java,run,"org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])",62,83,"/**
* Runs a Hadoop Tool with given configuration and arguments.
* @param conf configuration for the tool
* @param tool Tool instance to execute
* @param args command-line arguments
* @return exit status of the tool execution
*/","* Runs the given <code>Tool</code> by {@link Tool#run(String[])}, after 
   * parsing with the given generic arguments. Uses the given 
   * <code>Configuration</code>, or builds one if null.
   * 
   * Sets the <code>Tool</code>'s configuration with the possibly modified 
   * version of the <code>conf</code>.  
   * 
   * @param conf <code>Configuration</code> for the <code>Tool</code>.
   * @param tool <code>Tool</code> to run.
   * @param args command-line arguments to the tool.
   * @return exit code of the {@link Tool#run(String[])} method.
   * @throws Exception Exception.",,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,parseCommandArgs,"org.apache.hadoop.service.launcher.ServiceLauncher:parseCommandArgs(org.apache.hadoop.conf.Configuration,java.util.List)",917,970,"/**
* Parses command line arguments and updates configuration.
* @param conf Configuration object
* @param args Command line arguments
* @return Remaining command line arguments as a list of strings
*/","* Parse the command arguments, extracting the service class as the last
   * element of the list (after extracting all the rest).
   *
   * The field {@link #commandOptions} field must already have been set.
   * @param conf configuration to use
   * @param args command line argument list
   * @return the remaining arguments
   * @throws ServiceLaunchException if processing of arguments failed",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,exec,"org.apache.hadoop.security.KDiag:exec(org.apache.hadoop.conf.Configuration,java.lang.String[])",1052,1056,"/**
* Runs tool with given configuration and arguments.
* @param conf Hadoop configuration
* @param argv array of command-line arguments
* @return exit status code
*/","* Inner entry point, with no logging or system exits.
   *
   * @param conf configuration
   * @param argv argument list
   * @return an exception
   * @throws Exception Exception.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,main,org.apache.hadoop.security.token.DtUtilShell:main(java.lang.String[]),393,395,"/**
* Runs Hadoop tool with given configuration and arguments.
* @param args command-line arguments
*/",,,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,main,org.apache.hadoop.security.alias.CredentialShell:main(java.lang.String[]),534,537,"/**
* Runs the Hadoop tool with the given configuration and credential shell.
* @throws Exception if an error occurs during execution
*/","* Main program.
   *
   * @param args
   *          Command line arguments
   * @throws Exception exception.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,main,org.apache.hadoop.crypto.key.KeyShell:main(java.lang.String[]),552,555,"/**
* Runs the Hadoop application with KeyShell driver.
* @throws Exception if an error occurs during execution
*/","* main() entry point for the KeyShell.  While strictly speaking the
   * return is void, it will System.exit() with a return code: 0 is for
   * success and 1 for failure.
   *
   * @param args Command line arguments.
   * @throws Exception raised on errors performing I/O.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ToolRunner.java,run,"org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.util.Tool,java.lang.String[])",95,98,"/**
* Runs the Tool instance with given arguments.
* @param tool Tool instance
* @param args Command-line arguments
* @return Exit status code
*/","* Runs the <code>Tool</code> with its <code>Configuration</code>.
   * 
   * Equivalent to <code>run(tool.getConf(), tool, args)</code>.
   * 
   * @param tool <code>Tool</code> to run.
   * @param args command-line arguments to the tool.
   * @return exit code of the {@link Tool#run(String[])} method.
   * @throws Exception exception.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,extractCommandOptions,"org.apache.hadoop.service.launcher.ServiceLauncher:extractCommandOptions(org.apache.hadoop.conf.Configuration,java.util.List)",896,905,"/**
* Extracts command options from the given arguments.
* @param conf configuration object
* @param args list of input arguments
* @return list of extracted command options or empty list if invalid
*/","* Extract the command options and apply them to the configuration,
   * building an array of processed arguments to hand down to the service.
   *
   * @param conf configuration to update.
   * @param args main arguments. {@code args[0]}is assumed to be
   * the service classname and is skipped.
   * @return the remaining arguments
   * @throws ExitUtil.ExitException if JVM exiting is disabled.",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,main,org.apache.hadoop.security.KDiag:main(java.lang.String[]),1062,1072,"/**
* Terminates the application with exit status.
* @throws ExitUtil.ExitException if termination is requested
*/","* Main entry point.
   * @param argv args list",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,main,org.apache.hadoop.fs.FsShell:main(java.lang.String[]),383,395,"/**
* Runs the FsShell instance with command-line arguments.
* @param argv array of command-line arguments
*/","* main() has some simple utility methods
   * @param argv the command and its arguments
   * @throws Exception upon error",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,main,org.apache.hadoop.log.LogLevel:main(java.lang.String[]),73,76,"/**
* Runs the command-line interface (CLI) using the provided configuration.
*/","* A command line implementation
   * @param args input args.
   * @throws Exception exception.",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,main,org.apache.hadoop.util.FindClass:main(java.lang.String[]),378,386,"/**
* Runs the FindClass tool with command-line arguments and exits with a status code.
*@param args command-line arguments for the tool
*/","* Main entry point. 
   * Runs the class via the {@link ToolRunner}, then
   * exits with an appropriate exit code. 
   * @param args argument list",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,launchServiceAndExit,org.apache.hadoop.service.launcher.ServiceLauncher:launchServiceAndExit(java.util.List),281,315,"/**
* Launches service with given arguments, exits on completion or error.
* @param args command-line arguments
*/","* Launch the service and exit.
   *
   * <ol>
   * <li>Parse the command line.</li> 
   * <li>Build the service configuration from it.</li>
   * <li>Start the service.</li>
   * <li>If it is a {@link LaunchableService}: execute it</li>
   * <li>Otherwise: wait for it to finish.</li>
   * <li>Exit passing the status code to the {@link #exit(int, String)}
   * method.</li>
   * </ol>
   * @param args arguments to the service. {@code arg[0]} is 
   * assumed to be the service classname.",,,True,42
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,serviceMain,org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.util.List),1064,1073,"/**
* Launches a service based on the first command-line argument.
* @param argsList list of command-line arguments
*/",,,,True,43
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,main,org.apache.hadoop.service.launcher.ServiceLauncher:main(java.lang.String[]),1043,1045,"/**
 * Starts the application with command-line arguments.
 * @param args list of command-line arguments
 */","* This is the JVM entry point for the service launcher.
   *
   * Converts the arguments to a list, then invokes {@link #serviceMain(List)}
   * @param args command line arguments.",,,True,44
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,serviceMain,org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.lang.String[]),1052,1054,"/**
* Wraps the main service invocation with an argument list.
* @param args variable number of command-line arguments
*/","* Varargs version of the entry point for testing and other in-JVM use.
   * Hands off to {@link #serviceMain(List)}
   * @param args command line arguments.",,,True,44
