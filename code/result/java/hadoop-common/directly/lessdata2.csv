file_path,Name,full_name,Start Line,End Line,Comment,Pre_Comment,child Name,domain,inner_method,node_level
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileRange.java,createFileRange,"org.apache.hadoop.fs.FileRange:createFileRange(long,int)",73,75,"/**
 * Creates a FileRange object with the given offset and length.
 */","* Factory method to create a FileRange object.
   * @param offset starting offset of the range.
   * @param length length of the range.
   * @return a new instance of FileRangeImpl.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileRange.java,createFileRange,"org.apache.hadoop.fs.FileRange:createFileRange(long,int,java.lang.Object)",84,86,"/**
 * Creates a FileRange object with the given offset, length, and reference.
 */","* Factory method to create a FileRange object.
   * @param offset starting offset of the range.
   * @param length length of the range.
   * @param reference nullable reference to store in the range.
   * @return a new instance of FileRangeImpl.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,validateRangeRequest,org.apache.hadoop.fs.VectoredReadUtils:validateRangeRequest(org.apache.hadoop.fs.FileRange),65,75,"/**
 * Validates a FileRange request.
 * @param range The FileRange object to validate.
 * @return The validated FileRange object.
 * @throws EOFException if offset is negative.
 */
","* Validate a single range.
   * @param range range to validate.
   * @return the range.
   * @param <T> range type
   * @throws IllegalArgumentException the range length is negative or other invalid condition
   * is met other than the those which raise EOFException or NullPointerException.
   * @throws EOFException the range offset is negative
   * @throws NullPointerException if the range is null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNull,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNull(java.lang.Object,java.lang.String)",45,47,"/**
 * Checks if the given object is not null.
 * @param obj Object to check.
 * @param argName Name of the argument.
 */
","* Validates that the given reference argument is not null.
   * @param obj the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPositiveInteger,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPositiveInteger(long,java.lang.String)",54,56,"/**
 * Checks if the given value is a positive integer.
 * @param value The value to check.
 * @param argName Name of the argument being checked.
 */
","* Validates that the given integer argument is not zero or negative.
   * @param value the argument value to validate
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNegative,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNegative(long,java.lang.String)",63,65,"/**
 * Checks if the given value is non-negative.
 * @param value The value to check.
 * @param argName Name of the argument being checked.
 */
","* Validates that the given integer argument is not negative.
   * @param value the argument value to validate
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkRequired,"org.apache.hadoop.fs.impl.prefetch.Validate:checkRequired(boolean,java.lang.String)",72,74,"/**
 * Checks if a boolean argument is present; throws exception if not.
 * @param isPresent boolean value to check
 * @param argName name of the argument being checked
 */
","* Validates that the expression (that checks a required field is present) is true.
   * @param isPresent indicates whether the given argument is present.
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkValid,"org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String)",81,83,"/**
 * Checks if a boolean argument is valid, throwing an exception if not.
 * @param isValid The boolean value to check.
 * @param argName Name of the argument being validated.
 */
","* Validates that the expression (that checks a field is valid) is true.
   * @param isValid indicates whether the given argument is valid.
   * @param argName the name of the argument being validated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkValid,"org.apache.hadoop.fs.impl.prefetch.Validate:checkValid(boolean,java.lang.String,java.lang.String)",91,96,"/**
 * Checks if a boolean value is valid, throws exception if not.
 * @param isValid The boolean value to check.
 * @param argName Argument name.
 * @param validValues Valid values string.
 */
","* Validates that the expression (that checks a field is valid) is true.
   * @param isValid indicates whether the given argument is valid.
   * @param argName the name of the argument being validated.
   * @param validValues the list of values that are allowed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkValuesEqual,"org.apache.hadoop.fs.impl.prefetch.Validate:checkValuesEqual(long,java.lang.String,long,java.lang.String)",201,213,"/**
 * Checks if two long values are equal, throwing an exception if not.
 * @param value1Name Name of the first value.
 * @param value2Name Name of the second value.
 */
","* Validates that the given two values are equal.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkIntegerMultiple,"org.apache.hadoop.fs.impl.prefetch.Validate:checkIntegerMultiple(long,java.lang.String,long,java.lang.String)",222,234,"/**
 * Checks if value1 is an integer multiple of value2.
 * @param value1 First value.
 * @param value1Name Name of first value.
 * @param value2 Second value.
 * @param value2Name Name of second value.
 */
","* Validates that the first value is an integer multiple of the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkGreater,"org.apache.hadoop.fs.impl.prefetch.Validate:checkGreater(long,java.lang.String,long,java.lang.String)",243,255,"/**
 * Checks if value1 is greater than value2, throws exception if not.
 * @param value1 First value.
 * @param value1Name Name of first value.
 * @param value2 Second value.
 * @param value2Name Name of second value.
 */
","* Validates that the first value is greater than the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkGreaterOrEqual,"org.apache.hadoop.fs.impl.prefetch.Validate:checkGreaterOrEqual(long,java.lang.String,long,java.lang.String)",264,276,"/**
 * Checks if value1 is greater than or equal to value2.
 * @param value1 First value.
 * @param value2 Second value.
 */
","* Validates that the first value is greater than or equal to the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkLessOrEqual,"org.apache.hadoop.fs.impl.prefetch.Validate:checkLessOrEqual(long,java.lang.String,long,java.lang.String)",285,297,"/**
 * Checks if value1 is less than or equal to value2.
 * @param value1 First value.
 * @param value1Name Name of the first value.
 * @param value2 Second value.
 * @param value2Name Name of the second value.
 */
","* Validates that the first value is less than or equal to the second value.
   * @param value1 the first value to check.
   * @param value1Name the name of the first argument.
   * @param value2 the second value to check.
   * @param value2Name the name of the second argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkWithinRange,"org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(long,java.lang.String,long,long)",306,318,"/**
 * Checks if a value is within a specified inclusive range.
 * @param value The value to check.
 * @param valueName Name of the value being checked.
 */","* Validates that the given value is within the given range of values.
   * @param value the value to check.
   * @param valueName the name of the argument.
   * @param minValueInclusive inclusive lower limit for the value.
   * @param maxValueInclusive inclusive upper limit for the value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkWithinRange,"org.apache.hadoop.fs.impl.prefetch.Validate:checkWithinRange(double,java.lang.String,double,double)",327,339,"/**
 * Checks if a value is within a specified inclusive range.
 * @param value The value to check.
 * @param valueName Name of the value being checked.
 */
","* Validates that the given value is within the given range of values.
   * @param value the value to check.
   * @param valueName the name of the argument.
   * @param minValueInclusive inclusive lower limit for the value.
   * @param maxValueInclusive inclusive upper limit for the value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotEmpty(int,java.lang.String)",393,398,"/**
* Validates array size; throws exception if size is not positive.
* @param arraySize Size of the array to validate.
* @param argName Name of the array argument.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BulkDeleteUtils.java,validateBulkDeletePaths,"org.apache.hadoop.fs.BulkDeleteUtils:validateBulkDeletePaths(java.util.Collection,int,org.apache.hadoop.fs.Path)",39,48,"/**
 * Validates a collection of paths, ensuring they are absolute
 * and under the specified base path, with a page size limit.
 */","* Preconditions for bulk delete paths.
   * @param paths paths to delete.
   * @param pageSize maximum number of paths to delete in a single operation.
   * @param basePath base path for the delete operation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:<init>(java.io.File),177,182,"/**
 * Initializes BlockUploadData with a file.
 * @param file The file to be uploaded. Throws if file doesn't exist.
 */
","* File constructor; input stream and byteArray will be null.
     *
     * @param file file to upload",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,requireIOStatisticsSnapshot,org.apache.hadoop.io.wrappedio.WrappedStatistics:requireIOStatisticsSnapshot(java.io.Serializable),352,356,"/**
 * Casts the snapshot to IOStatisticsSnapshot and validates type.
 * @param snapshot The snapshot to cast; must be an IOStatisticsSnapshot.
 * @return The casted snapshot.
 */
","* Require the parameter to be an instance of {@link IOStatisticsSnapshot}.
   * @param snapshot object to validate
   * @return cast value
   * @throws IllegalArgumentException if the supplied class is not a snapshot",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputWrapper.java,<init>,"org.apache.hadoop.net.SocketInputWrapper:<init>(java.net.Socket,java.io.InputStream)",43,52,"/**
 * Constructs a SocketInputWrapper with a socket and input stream.
 * @param s The socket associated with the input stream.
 * @param is The input stream to wrap.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfigurationHelper.java,mapEnumNamesToValues,"org.apache.hadoop.util.ConfigurationHelper:mapEnumNamesToValues(java.lang.String,java.lang.Class)",109,124,"/**
 * Maps enum names to values with a prefix.
 * @param prefix Prefix to add to enum names.
 * @param enumClass Enum class to map.
 * @return Map of enum name (with prefix) to enum value.
 */
","* Given an enum class, build a map of lower case names to values.
   * @param prefix prefix (with trailing ""."") for path capabilities probe
   * @param enumClass class of enum
   * @param <E> enum type
   * @return a mutable map of lower case names to enum values
   * @throws IllegalArgumentException if there are two entries which differ only by case.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,sortRanges,org.apache.hadoop.fs.VectoredReadUtils:sortRanges(java.util.List),358,361,"/**
 * Sorts a list of FileRange objects and returns them as an array.
 */","* Sort the input ranges by offset; no validation is done.
   * <p>
   * This method is used externally and must be retained with
   * the signature unchanged.
   * @param input input ranges.
   * @return a new list of the ranges, sorted by offset.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,perms,org.apache.hadoop.fs.Options$CreateOpts:perms(org.apache.hadoop.fs.permission.FsPermission),65,67,"/**
 * Creates a Perms object from a FsPermission.
 * @param perm The FsPermission to wrap.
 * @return A new Perms object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,seekInternal,org.apache.hadoop.fs.sftp.SFTPInputStream:seekInternal(),79,96,"/**
 * Advances the stream position to match the next position.
 * Skips or resets the stream based on relative positions.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,checkNotClosed,org.apache.hadoop.fs.sftp.SFTPInputStream:checkNotClosed(),135,141,"/**
 * Throws IOException if the stream is closed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,isParentOf,"org.apache.hadoop.fs.ftp.FTPFileSystem:isParentOf(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",648,657,"/**
 * Checks if a path is a parent of another path.
 * @param parent Parent path.
 * @param child Child path.
 * @return True if parent is a parent of child.
 */
","* Probe for a path being a parent of another
   * @param parent parent path
   * @param child possible child path
   * @return true if the parent's path matches the start of the child's",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,isSameFS,"org.apache.hadoop.fs.FileContext:isSameFS(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2306,2312,"/**
 * Checks if two paths are on the same filesystem.
 * @param qualPath1 First path.
 * @param qualPath2 Second path.
 */
","* Are qualSrc and qualDst of the same file system?
   * @param qualPath1 - fully qualified path
   * @param qualPath2 - fully qualified path
   * @return is same fs true,not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,deleteOnExit,org.apache.hadoop.fs.FileSystem:deleteOnExit(org.apache.hadoop.fs.Path),1804,1812,"/**
 * Registers a file to be deleted when the JVM exits.
 * @param f the file to delete on exit
 * @return true if registration was successful, false otherwise
 */
","* Mark a path to be deleted when its FileSystem is closed.
   * When the JVM shuts down cleanly, all cached FileSystem objects will be
   * closed automatically. These the marked paths will be deleted as a result.
   *
   * If a FileSystem instance is not cached, i.e. has been created with
   * {@link #createFileSystem(URI, Configuration)}, then the paths will
   * be deleted in when {@link #close()} is called on that instance.
   *
   * The path must exist in the filesystem at the time of the method call;
   * it does not have to exist at the time of JVM shutdown.
   *
   * Notes
   * <ol>
   *   <li>Clean shutdown of the JVM cannot be guaranteed.</li>
   *   <li>The time to shut down a FileSystem will depends on the number of
   *   files to delete. For filesystems where the cost of checking
   *   for the existence of a file/directory and the actual delete operation
   *   (for example: object stores) is high, the time to shutdown the JVM can be
   *   significantly extended by over-use of this feature.</li>
   *   <li>Connectivity problems with a remote filesystem may delay shutdown
   *   further, and may cause the files to not be deleted.</li>
   * </ol>
   * @param f the path to delete.
   * @return  true if deleteOnExit is successful, otherwise false.
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,processDeleteOnExit,org.apache.hadoop.fs.FileSystem:processDeleteOnExit(),1833,1848,"/**
 * Processes paths to delete on exit, ignoring deletion failures.
 */","* Delete all paths that were marked as delete-on-exit. This recursively
   * deletes all files and directories in the specified paths.
   *
   * The time to process this operation is {@code O(paths)}, with the actual
   * time dependent on the time for existence and deletion operations to
   * complete, successfully or not.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,keystoreExists,org.apache.hadoop.security.alias.KeyStoreProvider:keystoreExists(),58,61,"/**
 * Checks if the keystore file exists on the file system.
 * @return True if the keystore exists, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,compareTo,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:compareTo(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode),140,151,"/**
 * Compares nodes based on modification time, newer nodes first.
 * @param other The node to compare against.
 * @return An integer representing the comparison result.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getModificationTime,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getModificationTime(),504,507,"/**
 * Returns the modification time of the underlying status.
 * @return Modification timestamp.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getModificationTime,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getModificationTime(),80,83,"/**
 * Gets the modification time.
 * @return Modification time as a long value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,decodeFileName,org.apache.hadoop.fs.HarFileSystem:decodeFileName(java.lang.String),261,268,"/**
 * Decodes filename based on metadata version.
 * @param fname Filename to decode.
 * @return Decoded filename or original if no decoding needed.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,compareTo,org.apache.hadoop.fs.shell.PathData:compareTo(org.apache.hadoop.fs.shell.PathData),593,596,"/**
 * Compares this PathData with another based on their paths.
 * @param o the other PathData to compare to
 * @return negative, zero, or positive int based on path comparison
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,isChecksumFile,org.apache.hadoop.fs.ChecksumFs:isChecksumFile(org.apache.hadoop.fs.Path),98,101,"/**
 * Checks if a file is a checksum file (.crc).
 * @param file The Path object representing the file.
 * @return True if the file is a checksum file, false otherwise.
 */
","* Return true iff file is a checksum file name.
   *
   * @param file the file path.
   * @return if is checksum file true,not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,isChecksumFile,org.apache.hadoop.fs.ChecksumFileSystem:isChecksumFile(org.apache.hadoop.fs.Path),130,133,"/**
 * Checks if a file is a checksum file (.crc).
 * @param file The Path object representing the file.
 * @return True if the file is a checksum file, false otherwise.
 */
","* Return true if file is a checksum file name.
   *
   * @param file the file path.
   * @return if file is a checksum file true, not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,fixBlockLocations,"org.apache.hadoop.fs.HarFileSystem:fixBlockLocations(org.apache.hadoop.fs.BlockLocation[],long,long,long)",423,457,"/**
 * Adjusts BlockLocation offsets/lengths based on start, len, and file offset.
 */","* Fix offset and length of block locations.
   * Note that this method modifies the original array.
   * @param locations block locations of har part file
   * @param start the start of the desired range in the contained file
   * @param len the length of the desired range
   * @param fileOffsetInHar the offset of the desired file in the har part file
   * @return block locations with fixed offset and length",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,compareTo,org.apache.hadoop.fs.FileStatus:compareTo(org.apache.hadoop.fs.FileStatus),411,413,"/**
 * Compares this file status with another based on their paths.
 */","* Compare this FileStatus to another FileStatus based on lexicographical
   * order of path.
   * @param   o the FileStatus to be compared.
   * @return  a negative integer, zero, or a positive integer as this object
   *   is less than, equal to, or greater than the specified object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,stat2Paths,org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[]),114,122,"/**
 * Converts FileStatus[] to Path[].
 * @param stats array of FileStatus objects
 * @return array of Path objects
 */
","* convert an array of FileStatus to an array of Path
   *
   * @param stats
   *          an array of FileStatus objects
   * @return an array of paths corresponding to the input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,addPartFileStatuses,org.apache.hadoop.fs.HarFileSystem$HarMetaData:addPartFileStatuses(org.apache.hadoop.fs.Path),1148,1152,"/**
 * Adds part file statuses from a path to the map.
 * @param path Path to list file statuses from.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,merge,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:merge(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileStatus[])",1228,1242,"/**
 * Merges two arrays of FileStatus, avoiding duplicates based on name.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getPath,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getPath(),529,532,"/**
 * Returns the path associated with the real status.
 * @return Path object representing the file path.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,merge,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:merge(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.FileStatus[])",1622,1636,"/**
 * Merges two arrays of FileStatus, avoiding duplicate paths.
 * @param toStatuses First array of FileStatus.
 * @param fromStatuses Second array of FileStatus.
 * @return Merged FileStatus array.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,resolveIntermediate,org.apache.hadoop.fs.FileContext:resolveIntermediate(org.apache.hadoop.fs.Path),2353,2361,"/**
 * Resolves an intermediate path by following symbolic links.
 * @param f The path to resolve.
 * @return The resolved Path.
 */
","* Resolves all symbolic links in the specified path leading up 
   * to, but not including the final path component.
   * @param f path to resolve
   * @return the new path object.
   * @throws IOException If an I/O error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getReplication,org.apache.hadoop.fs.FileSystem:getReplication(org.apache.hadoop.fs.Path),1603,1606,"/**
 * Gets the replication factor of a file.
 * @param src The path to the file.
 * @return The replication factor.
 */
","* Get the replication factor.
   *
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @param src file name
   * @return file replication
   * @throws FileNotFoundException if the path does not resolve.
   * @throws IOException an IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getReplication,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getReplication(),499,502,"/**
 * Returns the replication status from the underlying realStatus.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getReplication,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getReplication(),75,78,"/**
 * Gets the replication factor from the file system.
 * @return Replication factor as a short.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBlockSize,org.apache.hadoop.fs.FileSystem:getBlockSize(org.apache.hadoop.fs.Path),2742,2745,"/**
 * Gets the block size of a file.
 * @param f Path to the file. Returns block size as long.
 */","* Get the block size for a particular file.
   * @param f the filename
   * @return the number of bytes in a block
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @throws FileNotFoundException if the path is not present
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getBlockSize,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getBlockSize(),494,497,"/**
 * Returns the block size from the underlying real status.
 * @return The block size as a long value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getBlockSize,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getBlockSize(),70,73,"/**
 * Returns the block size of the file system.
 * @return Block size as a long value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getAccessTime,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getAccessTime(),509,512,"/**
 * Returns the access time from the underlying real status.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getAccessTime,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getAccessTime(),85,88,"/**
 * Returns the access time of the file system.
 * @return Access time in milliseconds.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getPermission,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getPermission(),514,517,"/**
 * Gets the permission associated with this file system status.
 * @return The FsPermission object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getPermission,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getPermission(),90,93,"/**
 * Gets the permission associated with this file system.
 * @return The permission object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,stashOriginalFilePermissions,org.apache.hadoop.security.alias.KeyStoreProvider:stashOriginalFilePermissions(),73,79,"/**
 * Saves the original file permissions for potential keystore rewrite.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,isPermissionLoaded,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:isPermissionLoaded(),927,929,"/**
 * Checks if permission is loaded based on owner's presence.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getOwner,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getOwner(),519,522,"/**
 * Returns the owner of the real status.
 * @return String representing the owner's name.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getOwner,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getOwner(),95,98,"/**
 * Returns the owner of the file system.
 * @return String representing the owner.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getGroup,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getGroup(),524,527,"/**
 * Returns the group associated with the real status.
 * @return String representing the group name.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getGroup,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getGroup(),100,103,"/**
 * Returns the group associated with this file system.
 * @return The group name as a String.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,msync,org.apache.hadoop.fs.HarFileSystem:msync(),661,664,"/**
* Synchronizes file system data to persistent storage.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,msync,org.apache.hadoop.fs.FilterFileSystem:msync(),465,468,"/**
* Synchronizes file system data to disk.
* Delegates to the underlying file system's msync method.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.HarFileSystem:getDefaultReplication(),1294,1298,"/**
 * Returns the default replication factor from the file system.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultReplication,org.apache.hadoop.fs.FileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),2785,2787,"/**
 * Returns the default replication factor.
 * No path is used, always returns the default.
 */","* Get the default replication for a path.
   * The given path will be used to locate the actual FileSystem to query.
   * The full path does not have to exist.
   * @param path of the file
   * @return default replication for the path's filesystem",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.FilterFileSystem:getDefaultReplication(),431,434,"/**
 * Returns the default replication factor from the file system.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,cleanUp,org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReference:cleanUp(),4148,4159,"/**
 * Cleans up thread-local StatisticsData, merging into rootData.
 */","* Performs clean-up action when the associated thread is garbage
       * collected.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlConnection.java,<init>,"org.apache.hadoop.fs.FsUrlConnection:<init>(org.apache.hadoop.conf.Configuration,java.net.URL)",48,53,"/**
 * Constructs a FsUrlConnection with the given Hadoop configuration and URL.
 * @param conf Hadoop configuration.
 * @param url URL to connect to.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,validatePositionedReadArgs,"org.apache.hadoop.fs.FSInputStream:validatePositionedReadArgs(long,byte[],int,int)",102,116,"/**
 * Validates arguments for positioned read operations.
 * @param position Read position.
 * @param buffer Buffer.
 * @param offset Offset within the buffer.
 * @param length Number of bytes to read.
 */
","* Validation code, available for use in subclasses.
   * @param position position: if negative an EOF exception is raised
   * @param buffer destination buffer
   * @param offset offset within the buffer
   * @param length length of bytes to read
   * @throws EOFException if the position is negative
   * @throws IndexOutOfBoundsException if there isn't space for the amount of
   * data requested.
   * @throws IllegalArgumentException other arguments are invalid.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkUploadId,org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkUploadId(byte[]),80,85,"/**
 * Validates the uploadId: must be non-null and non-empty.
 * @param uploadId The upload ID to validate.
 * @throws IllegalArgumentException if uploadId is invalid.
 */
","* Utility method to validate uploadIDs.
   * @param uploadId Upload ID
   * @throws IllegalArgumentException invalid ID",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkPartHandles,org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPartHandles(java.util.Map),92,100,"/**
 * Validates part handles; ensures map is non-empty and keys are positive.
 */
","* Utility method to validate partHandles.
   * @param partHandles handles
   * @throws IllegalArgumentException if the parts are invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/PathCapabilitiesSupport.java,validatePathCapabilityArgs,"org.apache.hadoop.fs.impl.PathCapabilitiesSupport:validatePathCapabilityArgs(org.apache.hadoop.fs.Path,java.lang.String)",42,49,"/**
 * Validates and converts capability to lowercase.
 * @param path The path being validated.
 * @param capability Capability string to validate.
 * @return Lowercase capability string.
 */
","* Validate the arguments to
   * {@link PathCapabilities#hasPathCapability(Path, String)}.
   * @param path path to query the capability of.
   * @param capability non-null, non-empty string to query the path for support.
   * @return the string to use in a switch statement.
   * @throws IllegalArgumentException if a an argument is invalid.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,<init>,"org.apache.hadoop.service.launcher.InterruptEscalator:<init>(org.apache.hadoop.service.launcher.ServiceLauncher,int)",74,78,"/**
 * Constructs an InterruptEscalator with an owner and shutdown time.
 * @param owner The ServiceLauncher owning this escalator.
 * @param shutdownTimeMillis Shutdown time in milliseconds.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/IrqHandler.java,<init>,"org.apache.hadoop.service.launcher.IrqHandler:<init>(java.lang.String,org.apache.hadoop.service.launcher.IrqHandler$Interrupted)",78,83,"/**
 * Creates an IrqHandler with the given name and interrupt handler.
 * @param name handler name
 * @param handler interrupt handler to execute
 */
","* Create an IRQ handler bound to the specific interrupt.
   * @param name signal name
   * @param handler handler",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,partition,"org.apache.hadoop.util.Lists:partition(java.util.List,int)",269,284,"/**
 * Partitions a list into sublists of the specified page size.
 * @param originalList List to partition.
 * @param pageSize Size of each sublist.
 * @return List of sublists.
 */
","* Returns consecutive sub-lists of a list, each of the same size
   * (the final list may be smaller).
   * @param originalList original big list.
   * @param pageSize desired size of each sublist ( last one
   *                 may be smaller)
   * @param <T> Generics Type.
   * @return a list of sub lists.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,<init>,"org.apache.hadoop.util.JsonSerialization:<init>(java.lang.Class,boolean,boolean)",105,113,"/**
 * Creates a JsonSerialization instance.
 * @param classType Type to serialize.
 * @param failOnUnknownProperties Whether to fail on unknown props.
 * @param pretty Whether to format the JSON output.
 */
","* Create an instance bound to a specific type.
   * @param classType class to marshall
   * @param failOnUnknownProperties fail if an unknown property is encountered.
   * @param pretty generate pretty (indented) output?",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,fetch,"org.apache.hadoop.fs.FileSystemStorageStatistics:fetch(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsData,java.lang.String)",86,116,"/**
 * Fetches a statistic value by key from the provided data.
 * @param data StatisticsData object.
 * @param key Statistic key to retrieve. Returns null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,<init>,"org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.statistics.IOStatistics)",47,54,"/**
 * Creates a StorageStatisticsFromIOStatistics object.
 * @param name Name of the storage statistics.
 * @param scheme Storage scheme.
 * @param ioStatistics IO statistics used for calculations.
 */
","* Instantiate.
   * @param name storage statistics name.
   * @param scheme FS scheme; may be null.
   * @param ioStatistics IOStatistics source.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/EmptyStorageStatistics.java,<init>,org.apache.hadoop.fs.EmptyStorageStatistics:<init>(java.lang.String),28,30,"/**
 * Constructs an EmptyStorageStatistics object with the given name.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/UnionStorageStatistics.java,<init>,"org.apache.hadoop.fs.UnionStorageStatistics:<init>(java.lang.String,org.apache.hadoop.fs.StorageStatistics[])",79,92,"/**
 * Constructs a UnionStorageStatistics with a name and array of stats.
 * @param name Name of the union storage statistics.
 * @param stats Array of StorageStatistics objects.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,getScheme,org.apache.hadoop.fs.FileSystemStorageStatistics:getScheme(),127,130,"/**
 * Returns the scheme from the underlying stats object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatistics,org.apache.hadoop.fs.FileSystem:getStatistics(),4560,4567,"/**
 * Returns a map of all statistics, scheme as key, Statistics object as value.
 */
","* Get the Map of Statistics object indexed by URI Scheme.
   * @return a Map having a key as URI scheme and value as Statistics object
   * @deprecated use {@link #getGlobalStorageStatistics()}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFs:listXAttrs(org.apache.hadoop.fs.Path),846,851,"/**
 * Lists extended attributes for a given path.
 * @param path The path to list extended attributes for.
 * @return List of extended attribute names.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listXAttrs,org.apache.hadoop.fs.FilterFs:listXAttrs(org.apache.hadoop.fs.Path),387,390,"/**
 * Lists extended attributes for a given path.
 * @param path The path to list attributes for.
 * @return List of extended attribute names.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",1275,1284,"/**
 * Creates a data output stream for writing to a file.
 * @param f Path to the file, permissions, flags, buffer size, etc.
 * @return FSDataOutputStream for writing to the file.
 */
","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * @param f the file name to open
   * @param permission file permission
   * @param flags {@link CreateFlag}s to use for this stream.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize block size
   * @param progress the progress reporter
   * @throws IOException IO failure
   * @see #setPermission(Path, FsPermission)
   * @return output stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,create,"org.apache.hadoop.fs.FilterFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",201,212,"/**
 * Delegates file creation to the underlying file system.
 * @param f Path to create.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createNonRecursive,"org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1456,1463,"/**
 * Creates a non-recursive data output stream.
 * @param f Path to create stream on.
 * @param permission FsPermission for the file.
 */","* Opens an FSDataOutputStream at the indicated Path with write-progress
   * reporting. Same as create(), except fails if parent directory doesn't
   * already exist.
   * @param f the file name to open
   * @param permission file permission
   * @param overwrite if a file with this name already exists, then if true,
   * the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize block size
   * @param progress the progress reporter
   * @throws IOException IO failure
   * @see #setPermission(Path, FsPermission)
   * @return output stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.FilterFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",222,229,"/**
 * Creates a non-recursive data output stream.
 * @param f Path to create stream on, permission, flags, etc.
 * @return FSDataOutputStream
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathAccessDeniedException.java,<init>,org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String),24,26,"/**
 * Constructs a PathAccessDeniedException with the given path.
 * @param path The path that access was denied for.
 */
",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String),26,28,"/**
 * Constructs a PathPermissionException with the given path.
 * @param path The path for which permission is denied.
 */
",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,"org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.String)",34,36,"/**
 * Constructs a PathPermissionException with the given path and error.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String),26,28,"/**
* Constructs a PathNotFoundException with the given path.
* @param path The path that was not found.
*/
",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,"org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.String)",34,36,"/**
 * Constructs a PathNotFoundException with a path and error message.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathExistsException.java,<init>,org.apache.hadoop.fs.PathExistsException:<init>(java.lang.String),26,28,"/**
 * Constructs a PathExistsException with the given path.
 * @param path The path of the existing file.
 */
",@param path for the exception,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathExistsException.java,<init>,"org.apache.hadoop.fs.PathExistsException:<init>(java.lang.String,java.lang.String)",30,32,"/**
 * Constructs a PathExistsException with a path and error message.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,<init>,org.apache.hadoop.fs.PathIOException:<init>(java.lang.String),43,45,"/**
 * Constructs a PathIOException with the given path.
 * @param path The path associated with the I/O error.
 */
","* Constructor a generic I/O error exception
   *  @param path for the exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ClosedIOException.java,<init>,"org.apache.hadoop.fs.ClosedIOException:<init>(java.lang.String,java.lang.String)",36,38,"/**
 * Constructs a ClosedIOException with a path and message.
 * @param path The file path.
 * @param message The error message.
 */
","* Appends the custom error-message to the default error message.
   * @param path path that encountered the closed resource.
   * @param message custom error message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,getThisBuilder,org.apache.hadoop.fs.FSDataOutputStreamBuilder:getThisBuilder(),102,102,"/**
 * Returns a builder object representing the current object.
 */",* Return the concrete implementation of the builder instance.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.permission.FsPermission),43,47,"/**
 * Converts a FsPermission to its protocol buffer representation.
 * @param p FsPermission to convert
 * @return FsPermissionProto object
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,checkReturnValue,"org.apache.hadoop.fs.FileUtil:checkReturnValue(boolean,java.io.File,org.apache.hadoop.fs.permission.FsPermission)",1510,1518,"/**
 * Throws IOException if permission setting fails for a path.
 * @param rv boolean indicating success/failure
 * @param p file path
 * @param permission permission to set
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,write,org.apache.hadoop.fs.permission.FsPermission:write(java.io.DataOutput),179,183,"/**
 * Writes the short representation of the object to the output.
 * @param out DataOutput to write to.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,toExtendedShort,org.apache.hadoop.fs.permission.FsPermission:toExtendedShort(),240,243,"/**
 * Converts to an extended short, returning the short value.
 */","* Encodes the object to a short.  Unlike {@link #toShort()}, this method may
   * return values outside the fixed range 00000 - 01777 if extended features
   * are encoded into this permission, such as the ACL bit.
   *
   * @return short extended short representation of this permission",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,toOctal,org.apache.hadoop.fs.permission.FsPermission:toOctal(),251,255,"/**
 * Converts the short value to its octal representation.
 * Returns the octal value as a short.
 */
","* Returns the FsPermission in an octal format.
   *
   * @return short Unlike {@link #toShort()} which provides a binary
   * representation, this method returns the standard octal style permission.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,hashCode,org.apache.hadoop.fs.permission.FsPermission:hashCode(),269,270,"/**
 * Returns the hash code, same as the short representation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringInterner.java,internStringsInArray,org.apache.hadoop.util.StringInterner:internStringsInArray(java.lang.String[]),81,86,"/**
 * Interns strings in the input array using weakIntern.
 * @param strings Array of strings to be interned.
 * @return The array with interned strings.
 */
","* Interns all the strings in the given array in place,
   * returning the same array.
   *
   * @param strings strings.
   * @return internStringsInArray.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleStartProperty,org.apache.hadoop.conf.Configuration$Parser:handleStartProperty(),3267,3294,"/**
 * Resets and populates configuration properties from XML attributes.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,isDir,org.apache.hadoop.fs.FileStatus:isDir(),240,243,"/**
 * Checks if the file is a directory. Deprecated, use isDirectory().
 */","* Old interface, instead use the explicit {@link FileStatus#isFile()},
   * {@link FileStatus#isDirectory()}, and {@link FileStatus#isSymlink()}
   * @return true if this is a directory.
   * @deprecated Use {@link FileStatus#isFile()},
   * {@link FileStatus#isDirectory()}, and {@link FileStatus#isSymlink()}
   * instead.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,isDirectory,org.apache.hadoop.fs.ChecksumFs:isDirectory(org.apache.hadoop.fs.Path),446,453,"/**
 * Checks if a Path represents a directory.
 * @param f the Path to check
 * @return true if it's a directory, false otherwise.
 */
","True iff the named path is a directory.
   * Note: Avoid using this method. Instead reuse the FileStatus 
   * returned by getFileStatus() or listStatus() methods.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processPath,org.apache.hadoop.fs.shell.CopyCommands$Merge:processPath(org.apache.hadoop.fs.shell.PathData),133,143,"/**
 * Processes a PathData object: recurses directories or adds files.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,isPathRecursable,org.apache.hadoop.fs.shell.Command:isPathRecursable(org.apache.hadoop.fs.shell.PathData),418,420,"/**
* Checks if a path is recursable (is a directory).
* @param item PathData object to check.
* @throws IOException if an I/O error occurs.
*/
","* Determines whether a {@link PathData} item is recursable. Default
   * implementation is to recurse directories but can be overridden to recurse
   * through symbolic links.
   *
   * @param item
   *          a {@link PathData} object
   * @return true if the item is recursable, false otherwise
   * @throws IOException
   *           if anything goes wrong in the user-implementation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,getAclEntries,org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:getAclEntries(org.apache.hadoop.fs.shell.PathData),283,289,"/**
 * Returns ACL entries based on recursive flag and item type.
 */","* Returns the ACL entries to use in the API call for the given path.  For a
     * recursive operation, returns all specified ACL entries if the item is a
     * directory or just the access ACL entries if the item is a file.  For a
     * non-recursive operation, returns all specified ACL entries.
     *
     * @param item PathData path to check
     * @return List<AclEntry> ACL entries to use in the API call",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processPathArgument,org.apache.hadoop.fs.shell.FsUsage$Du:processPathArgument(org.apache.hadoop.fs.shell.PathData),209,217,"/**
 * Processes a path argument, recursing into directories if not in summary mode.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,isDirectory,org.apache.hadoop.fs.FileSystem:isDirectory(org.apache.hadoop.fs.Path),1877,1884,"/**
 * Checks if a path represents a directory.
 * @param f the path to check
 * @throws IOException if an I/O error occurs
 */
","True iff the named path is a directory.
   * Note: Avoid using this method. Instead reuse the FileStatus
   * returned by getFileStatus() or listStatus() methods.
   *
   * @param f path to check
   * @throws IOException IO failure
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @return if f is directory true, not false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,isDirectory,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isDirectory(),484,487,"/**
 * Checks if the file is a directory.
 * @return True if it's a directory, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,isDirectory,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isDirectory(),60,63,"/**
* Checks if the file system object is a directory.
* Delegates to the underlying FileSystem object.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ChmodParser.java,applyNewPermission,org.apache.hadoop.fs.permission.ChmodParser:applyNewPermission(org.apache.hadoop.fs.FileStatus),48,54,"/**
 * Applies a new permission to a file status.
 * @param file the FileStatus to modify
 * @return the new permission as a short
 */
","* Apply permission against specified file and determine what the
   * new mode would be
   * @param file File against which to apply mode
   * @return File's new mode if applied.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,isFile,org.apache.hadoop.fs.FileStatus:isFile(),220,222,"/**
 * Checks if the file is a regular file, not a directory or symlink.
 */
","* Is this a file?
   * @return true if this is a file",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,getSymlink,org.apache.hadoop.fs.FileStatus:getSymlink(),393,398,"/**
 * Returns the target path of the symbolic link.
 * @throws IOException if the path is not a symbolic link.
 */
","* @return The contents of the symbolic link.
   *
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,isSymlink,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isSymlink(),489,492,"/**
 * Checks if the file is a symbolic link.
 * @return True if it's a symlink, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,isSymlink,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isSymlink(),65,68,"/**
 * Checks if the file is a symbolic link.
 * Delegates to the underlying FileSystem's isSymlink() method.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getFileLength,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:getFileLength(),270,275,"/**
 * Returns the length of the file.
 * Caches the length for subsequent calls.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getFileLength,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getFileLength(),330,335,"/**
 * Gets the length of the file. Returns -1 if not cached.
 */","* Calculate length of file if not already cached.
     * @return file length.
     * @throws IOException any IOE.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,totalPartsLen,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:totalPartsLen(java.util.List),168,174,"/**
 * Calculates the total length of files represented by part handles.
 * @param partHandles List of file paths (handles)
 * @return Total file length in bytes.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getLength,org.apache.hadoop.fs.FileSystem:getLength(org.apache.hadoop.fs.Path),1912,1915,"/**
 * Gets the length of a file.
 * @param f Path to the file.
 * @return File length in bytes.
 */
","* The number of bytes in a file.
   * @param f the path.
   * @return the number of bytes; 0 for a directory
   * @deprecated Use {@link #getFileStatus(Path)} instead.
   * @throws FileNotFoundException if the path does not resolve
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getLen,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getLen(),474,477,"/**
 * Returns the length of the underlying status.
 * @return Length value as a long.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getLen,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getLen(),50,53,"/**
 * Returns the length of the file system object.
 * @return File system object length as a long.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,<init>,"org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String,byte)",82,91,"/**
 * Constructs a FsServerDefaults with default encryption status.
 * @param blockSize Block size.
 * @param checksumType Checksum type.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFs:getStoragePolicy(org.apache.hadoop.fs.Path),418,422,"/**
 * Gets the storage policy for a given path.
 * @param src Path to check for storage policy.
 * @return Storage policy object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs:getStoragePolicy(org.apache.hadoop.fs.Path),914,919,"/**
 * Retrieves the storage policy for a given source path.
 * @param src Path for which to retrieve the storage policy.
 * @return The BlockStoragePolicySpi associated with the path.
 */
","* Retrieve the storage policy for a given file or directory.
   *
   * @param src file or directory path.
   * @return storage policy for give file.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getStoragePolicy,org.apache.hadoop.fs.FilterFs:getStoragePolicy(org.apache.hadoop.fs.Path),432,436,"/**
 * Gets the storage policy for the given path.
 * @param src Path to retrieve the storage policy for.
 * @return Storage policy object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",816,822,"/**
 * Sets an extended attribute on a file.
 * @param path File path.
 * @param name Attribute name.
 * @param value Attribute value.
 * @param flag Attribute set flags.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,setXAttr,"org.apache.hadoop.fs.AbstractFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",1358,1362,"/**
* Sets an extended attribute on a path.
* @param path Path to set the attribute on.
* @param name Attribute name.
* @param value Attribute value.
*/
","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setXAttr,"org.apache.hadoop.fs.FilterFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",365,369,"/**
 * Sets an extended attribute on a path.
 * @param path Path to set the attribute on.
 * @param name Attribute name.
 * @param value Attribute value.
 * @param flag Set of flags for the operation.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,blockSize,org.apache.hadoop.fs.Options$CreateOpts:blockSize(long),46,48,"/**
 * Creates a BlockSize object with the given block size.
 * @param bs The size of the block.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,bufferSize,org.apache.hadoop.fs.Options$CreateOpts:bufferSize(int),49,51,"/**
 * Creates a new BufferSize object with the given size.
 * @param bs the buffer size
 * @return a new BufferSize object
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,repFac,org.apache.hadoop.fs.Options$CreateOpts:repFac(short),52,54,"/**
 * Creates a ReplicationFactor object with the given replication factor.
 * @param rf The replication factor value.
 * @return A new ReplicationFactor instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,bytesPerChecksum,org.apache.hadoop.fs.Options$CreateOpts:bytesPerChecksum(short),55,57,"/**
 * Creates a BytesPerChecksum object for the given CRC value.
 * @param crc The CRC value.
 * @return A BytesPerChecksum object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,checksumParam,org.apache.hadoop.fs.Options$CreateOpts:checksumParam(org.apache.hadoop.fs.Options$ChecksumOpt),58,61,"/**
 * Creates a ChecksumParam object from a ChecksumOpt.
 * @param csumOpt The ChecksumOpt to create the param from.
 * @return A new ChecksumParam object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,progress,org.apache.hadoop.fs.Options$CreateOpts:progress(org.apache.hadoop.util.Progressable),62,64,"/**
 * Creates a new Progress object wrapping the given Progressable.
 * @param prog The Progressable to wrap.
 * @return A new Progress object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,createParent,org.apache.hadoop.fs.Options$CreateOpts:createParent(),68,70,"/**
 * Creates a new CreateParent object with the 'enabled' flag set to true.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,donotCreateParent,org.apache.hadoop.fs.Options$CreateOpts:donotCreateParent(),71,73,"/**
 * Creates a CreateParent object with parent creation disabled.
 * @return CreateParent object with creation set to false.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathAccessDeniedException.java,<init>,"org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",32,36,"/**
 * Constructs a PathAccessDeniedException with path, error, and cause.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,"org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",38,42,"/**
 * Constructs a PathPermissionException with path, error, and cause.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,"org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.String,java.lang.Throwable)",38,42,"/**
 * Constructs a PathNotFoundException with path, error, and cause.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,<init>,"org.apache.hadoop.fs.PathIOException:<init>(java.lang.String,java.lang.Throwable)",52,54,"/**
* Constructs a PathIOException with the given path and cause.
*/
","* Appends the text of a Throwable to the default error message
   * @param path for the exception
   * @param cause a throwable to extract the error message",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFs:removeAcl(org.apache.hadoop.fs.Path),794,800,"/**
 * Removes ACL entries for the given path.
 * @param path The path for which to remove ACLs.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeAcl,org.apache.hadoop.fs.FilterFs:removeAcl(org.apache.hadoop.fs.Path),344,347,"/**
 * Removes the ACL (Access Control List) for the given path.
 * @param path The path for which to remove the ACL.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AvroFSInput.java,seek,org.apache.hadoop.fs.AvroFSInput:seek(long),75,78,"/**
* Seeks to the specified position in the stream.
* @param p the position to seek to
* @throws IOException if an I/O error occurs
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long,long,int)",924,937,"/**
 * Initializes a HarFsInputStream with given parameters.
 * @param fs FileSystem, path, start, length, bufferSize
 * @throws IOException if an I/O error occurs
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,skip,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:skip(long),1006,1022,"/**
* Skips specified byte count. Returns actual bytes skipped.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,seek,org.apache.hadoop.io.SequenceFile$Reader:seek(long),2818,2824,"/**
 * Seeks to the specified position in the input stream.
 * @param position The position to seek to.
 */
","* Set the current byte position in the input file.
     *
     * <p>The position passed must be a position returned by {@link
     * SequenceFile.Writer#getLength()} when writing this file.  To seek to an arbitrary
     * position, use {@link SequenceFile.Reader#sync(long)}. </p>
     *
     * @param position input position.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.java,read,"org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[],int,int)",87,106,"/**
 * Reads up to 'len' bytes from the input stream into the buffer.
 * @param b buffer to read into
 * @param off offset in buffer to start reading
 * @param len number of bytes to read
 * @return number of bytes read, or -1 if end of stream
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AvroFSInput.java,tell,org.apache.hadoop.fs.AvroFSInput:tell(),80,83,"/**
 * Returns the current position of the input stream as a long.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,available,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:available(),939,946,"/**
 * Returns the number of bytes available for reading.
 * Returns Integer.MAX_VALUE if exceeding int limits.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readRecordLength,org.apache.hadoop.io.SequenceFile$Reader:readRecordLength(),2538,2558,"/**
 * Reads the record length from the input stream.
 * Returns -1 if end of stream is reached.
 */","* Read and return the next record length, potentially skipping over 
     * a sync block.
     * @return the length of the next record or -1 if there is no next record
     * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getPosition,org.apache.hadoop.io.SequenceFile$Reader:getPosition(),2873,2875,"/**
 * Returns the current position of the input stream.
 * @return The current position as a long.
 */","* @return Return the current byte position in the input file.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setOwner,"org.apache.hadoop.fs.DelegateToFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",214,219,"/**
* Sets the owner of a file system path.
* @param f path to the file, username, groupname
* @throws IOException if an I/O error occurs
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processPath,org.apache.hadoop.fs.FsShellPermissions$Chown:processPath(org.apache.hadoop.fs.shell.PathData),173,190,"/**
* Sets the owner and group of a PathData item if they have changed.
* @param item The PathData item to process.
* @throws IOException if an error occurs while changing ownership.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setOwner,"org.apache.hadoop.fs.FilterFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",533,537,"/**
* Sets the owner of a path.
* @param p the path
* @param username the username
* @param groupname the groupname
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,registerExpression,org.apache.hadoop.fs.shell.find.ExpressionFactory:registerExpression(java.lang.Class),59,69,"/**
 * Registers an expression class with the factory.
 * @param expressionClass Class to register, must have a register method.
 */
","* Invokes ""static void registerExpression(FindExpressionFactory)"" on the
   * given class. This method abstracts the contract between the factory and the
   * expression class. Do not assume that directly invoking registerExpression
   * on the given class will have the same effect.
   *
   * @param expressionClass
   *          class to allow an opportunity to register",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,registerCommands,org.apache.hadoop.fs.shell.CommandFactory:registerCommands(java.lang.Class),64,72,"/**
 * Registers commands using reflection.
 * @param registrarClass Class with registerCommands method.
 */
","* Invokes ""static void registerCommands(CommandFactory)"" on the given class.
   * This method abstracts the contract between the factory and the command
   * class.  Do not assume that directly invoking registerCommands on the
   * given class will have the same effect.
   * @param registrarClass class to allow an opportunity to register",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doResponse,"org.apache.hadoop.ipc.Server$RpcCall:doResponse(java.lang.Throwable,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto)",1308,1328,"/**
 * Sends a response to the client, handling errors or successful results.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileLinkStatus,org.apache.hadoop.fs.viewfs.ViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path),436,443,"/**
 * Gets the status of a file link.
 * @param f Path to the file; returns its status.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getUri,org.apache.hadoop.fs.FilterFs:getUri(),179,182,"/**
 * Returns the URI of the file system.
 * @return The URI object representing the file system's location.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,setSymlink,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:setSymlink(org.apache.hadoop.fs.Path),544,547,"/**
 * Sets the symlink path for the real status.
 * @param p The path to the symlink.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,read,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(),968,972,"/**
 * Reads a single byte from the input stream.
 * Returns -1 if no byte is available.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,read,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(byte[]),978,982,"/**
 * Reads up to {@code len} bytes from this input stream.
 * @param b the buffer to read into
 * @return the number of bytes read
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,seek,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:seek(long),1029,1034,"/**
* Seeks to the specified position in the stream.
* @param pos The offset from the stream's start.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,read,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:read(long,byte[],int,int)",1058,1071,"/**
 * Reads data from the stream, adjusting length if needed.
 * @param pos byte offset
 * @param b buffer
 * @param offset offset within buffer
 * @param length number of bytes to read
 * @return bytes read, or -1 if end of stream
 */
",* implementing position readable.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,readFully,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:readFully(long,byte[],int,int)",1076,1087,"/**
 * Reads up to {@code length} bytes from the stream at position {@code pos}.
 */",* position readable again.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,setReadahead,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:setReadahead(java.lang.Long),1089,1092,"/**
* Sets the readahead value of the underlying stream.
* @param readahead The readahead value to set.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,setDropBehind,org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream$HarFsInputStream:setDropBehind(java.lang.Boolean),1094,1097,"/**
* Sets the drop-behind flag on the underlying stream.
* @param dropBehind Boolean value indicating drop-behind behavior
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,getCurrentTrashDir,org.apache.hadoop.fs.Trash:getCurrentTrashDir(org.apache.hadoop.fs.Path),198,200,"/**
 * Gets the current trash directory for the given path.
 * @param path The path to determine the trash directory for.
 * @return The current trash directory.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,completed,"org.apache.hadoop.fs.RawLocalFileSystem$AsyncHandler:completed(java.lang.Integer,java.lang.Integer)",365,383,"/**
 * Processes the result of a read operation.
 * @param result Number of bytes read, -1 indicates EOF.
 * @param r Index of the range/buffer being processed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobExpander.java,expandLeftmost,org.apache.hadoop.fs.GlobExpander:expandLeftmost(org.apache.hadoop.fs.GlobExpander$StringWithOffset),86,146,"/**
 * Expands the leftmost alternative in a file pattern.
 * @param filePatternWithOffset Pattern with offset information.
 * @return List of expanded patterns or null if no match.
 */
","* Expand the leftmost outer curly bracket pair containing a
   * slash character (""/"") in <code>filePattern</code>.
   * @param filePatternWithOffset
   * @return expanded file patterns
   * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatusBatch,"org.apache.hadoop.fs.FileSystem:listStatusBatch(org.apache.hadoop.fs.Path,byte[])",2060,2068,"/**
 * Lists directory entries as a batch.
 * @param f Path to directory.
 * @param token Token for batching (ignored).
 * @return DirectoryEntries object.
 */
","* Given an opaque iteration token, return the next batch of entries in a
   * directory. This is a private API not meant for use by end users.
   * <p>
   * This method should be overridden by FileSystem subclasses that want to
   * use the generic {@link FileSystem#listStatusIterator(Path)} implementation.
   * @param f Path to list
   * @param token opaque iteration token returned by previous call, or null
   *              if this is the first call.
   * @return directory entries.
   * @throws FileNotFoundException when the path does not exist.
   * @throws IOException If an I/O error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/XAttrCodec.java,encodeValue,"org.apache.hadoop.fs.XAttrCodec:encodeValue(byte[],org.apache.hadoop.fs.XAttrCodec)",109,119,"/**
 * Encodes byte array to string based on specified encoding.
 * @param value byte array to encode
 * @param encoding encoding type (HEX, BASE64, or default)
 * @return Encoded string representation of the byte array.
 */
","* Encode byte[] value to string representation with encoding. 
   * Values encoded as text strings are enclosed in double quotes (\""), 
   * while strings encoded as hexadecimal and base64 are prefixed with 
   * 0x and 0s, respectively.
   * @param value byte[] value
   * @param encoding encoding.
   * @return String string representation of value
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,"org.apache.hadoop.fs.FileSystem:listStatus(java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2076,2085,"/**
 * Filters file statuses based on a path filter and adds to results.
 * @param results List to add matching FileStatus objects.
 * @param f Path to list.
 * @param filter PathFilter to apply.
 */
","* Filter files/directories in the given path using the user-supplied path
   * filter. Results are added to the given array <code>results</code>.
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsTag.java,<init>,"org.apache.hadoop.metrics2.MetricsTag:<init>(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",43,46,"/**
 * Creates a MetricsTag with given info and value.
 * @param info MetricsInfo object
 * @param value Tag value as a string
 */
","* Construct the tag with name, description and value
   * @param info  of the tag
   * @param value of the tag",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounter.java,<init>,org.apache.hadoop.metrics2.lib.MutableCounter:<init>(org.apache.hadoop.metrics2.MetricsInfo),35,37,"/**
 * Constructs a MutableCounter with the provided MetricsInfo.
 * @param info MetricsInfo object containing counter details.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGauge.java,<init>,org.apache.hadoop.metrics2.lib.MutableGauge:<init>(org.apache.hadoop.metrics2.MetricsInfo),35,37,"/**
 * Constructs a MutableGauge with the provided MetricsInfo.
 * @param info MetricsInfo object containing gauge details.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,<init>,org.apache.hadoop.metrics2.lib.MutableRates:<init>(org.apache.hadoop.metrics2.lib.MetricsRegistry),48,50,"/**
 * Constructs a MutableRates with the given metrics registry.
 * @param registry The MetricsRegistry to use.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsInfoImpl.java,<init>,"org.apache.hadoop.metrics2.lib.MetricsInfoImpl:<init>(java.lang.String,java.lang.String)",34,37,"/**
 * Constructs a MetricsInfo object with the given name and description.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/AbstractMetric.java,<init>,org.apache.hadoop.metrics2.AbstractMetric:<init>(org.apache.hadoop.metrics2.MetricsInfo),41,43,"/**
 * Constructs a new AbstractMetric with the given metrics info.
 * @param info The MetricsInfo object containing metric details.
 */
","* Construct the metric
   * @param info  about the metric",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String,java.lang.String)",389,403,"/**
 * Obtains a delegation token from the KDC.
 * @param url KDC URL, token, renewer, doAsUser - for token creation.
 * @return Delegation token or null on failure.
 */
","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @param doAsUser the user to do as, which will be the token owner.
   * @return a delegation token.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",433,446,"/**
 * Renews a delegation token.
 * @param url URL for renewal, @param token Token to renew,
 * @param doAsUser User to act as. Returns new expiry.
 */
","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",472,484,"/**
 * Cancels a delegation token.
 * @param url URL to cancel on, @param token Token to cancel,
 * @param doAsUser User to act as.
 */
","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,<init>,"org.apache.hadoop.crypto.key.kms.ValueQueue:<init>(int,float,long,int,org.apache.hadoop.crypto.key.kms.ValueQueue$SyncGenerationPolicy,org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller)",222,260,"/**
 * Constructs a ValueQueue with specified parameters for value queuing.
 */","* Constructor takes the following tunable configuration parameters
   * @param numValues The number of values cached in the Queue for a
   *    particular key.
   * @param lowWatermark The ratio of (number of current entries/numValues)
   *    below which the <code>fillQueueForKey()</code> funciton will be
   *    invoked to fill the Queue.
   * @param expiry Expiry time after which the Key and associated Queue are
   *    evicted from the cache.
   * @param numFillerThreads Number of threads to use for the filler thread
   * @param policy The SyncGenerationPolicy to use when client
   *    calls ""getAtMost""
   * @param refiller implementation of the QueueRefiller",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Preconditions.java,checkNotNull,org.apache.hadoop.util.Preconditions:checkNotNull(java.lang.Object),68,70,"/**
 * Checks if the object is null; throws NPE if so.
 * @param obj The object to check.
 */","* <p>Preconditions that the specified argument is not {@code null},
   * throwing a NPE exception otherwise.
   *
   * <p>The message of the exception is
   * &quot;The validated object is null&quot;.</p>
   *
   * @param <T> the object type
   * @param obj  the object to check
   * @return the validated object
   * @throws NullPointerException if the object is {@code null}
   * @see #checkNotNull(Object, Object)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFs:getAclStatus(org.apache.hadoop.fs.Path),809,814,"/**
 * Gets the ACL status for a given path.
 * @param path The path to check.
 * @return ACLStatus object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getAclStatus,org.apache.hadoop.fs.FilterFs:getAclStatus(org.apache.hadoop.fs.Path),354,357,"/**
 * Gets the ACL status for the given path.
 * @param path The path to retrieve the ACL status for.
 * @return The ACL status object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobalStorageStatistics.java,put,"org.apache.hadoop.fs.GlobalStorageStatistics:put(java.lang.String,org.apache.hadoop.fs.GlobalStorageStatistics$StorageStatisticsProvider)",73,93,"/**
* Adds or retrieves storage statistics for a given name.
* @param name Statistics name.
* @param provider Provider for creating statistics.
* @return StorageStatistics object.
*/
","* Create or return the StorageStatistics object with the given name.
   *
   * @param name        The storage statistics object name.
   * @param provider    An object which can create a new StorageStatistics
   *                      object if needed.
   * @return            The StorageStatistics object with the given name.
   * @throws RuntimeException  If the StorageStatisticsProvider provides a null
   *                           object or a new StorageStatistics object with the
   *                           wrong name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobalStorageStatistics.java,next,org.apache.hadoop.fs.GlobalStorageStatistics$StorageIterator:next(),127,139,"/**
 * Returns the next StorageStatistics in sorted order.
 * Throws NoSuchElementException if no next element exists.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,clearStatistics,org.apache.hadoop.fs.FileSystem:clearStatistics(),4610,4612,"/**
 * Resets global storage statistics to their initial values.
 */",* Reset all statistics for all file systems.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,<init>,org.apache.hadoop.fs.FsShell$UnknownCommandException:<init>(),410,410,"/**
 * Default constructor for UnknownCommandException.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,close,org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:close(),258,266,"/**
 * Closes the block upload data, cleaning up resources.
 * Releases stream, deletes file, and resets internal state.
 */
","* Close: closes any upload stream and byteArray provided in the
     * constructor.
     *
     * @throws IOException inherited exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,copyFileUnbuffered,"org.apache.hadoop.io.nativeio.NativeIO:copyFileUnbuffered(java.io.File,java.io.File)",1138,1161,"/**
 * Copies a file from src to dst without buffering.
 * @param src Source file.
 * @param dst Destination file.
 */
","* Unbuffered file copy from src to dst without tainting OS buffer cache
   *
   * In POSIX platform:
   * It uses FileChannel#transferTo() which internally attempts
   * unbuffered IO on OS with native sendfile64() support and falls back to
   * buffered IO otherwise.
   *
   * It minimizes the number of FileChannel#transferTo call by passing the the
   * src file size directly instead of a smaller size as the 3rd parameter.
   * This saves the number of sendfile64() system call when native sendfile64()
   * is supported. In the two fall back cases where sendfile is not supported,
   * FileChannle#transferTo already has its own batching of size 8 MB and 8 KB,
   * respectively.
   *
   * In Windows Platform:
   * It uses its own native wrapper of CopyFileEx with COPY_FILE_NO_BUFFERING
   * flag, which is supported on Windows Server 2008 and above.
   *
   * Ideally, we should use FileChannel#transferTo() across both POSIX and Windows
   * platform. Unfortunately, the wrapper(Java_sun_nio_ch_FileChannelImpl_transferTo0)
   * used by FileChannel#transferTo for unbuffered IO is not implemented on Windows.
   * Based on OpenJDK 6/7/8 source code, Java_sun_nio_ch_FileChannelImpl_transferTo0
   * on Windows simply returns IOS_UNSUPPORTED.
   *
   * Note: This simple native wrapper does minimal parameter checking before copy and
   * consistency check (e.g., size) after copy.
   * It is recommended to use wrapper function like
   * the Storage#nativeCopyFileUnbuffered() function in hadoop-hdfs with pre/post copy
   * checks.
   *
   * @param src                  The source path
   * @param dst                  The destination path
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,closeStream,org.apache.hadoop.io.IOUtils:closeStream(java.io.Closeable),276,280,"/**
 * Closes the provided Closeable stream, handling null input.
 */","* Closes the stream ignoring {@link Throwable}.
   * Must only be called in cleaning up from exception handlers.
   *
   * @param stream the Stream to close",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,closeStreams,org.apache.hadoop.io.IOUtils:closeStreams(java.io.Closeable[]),288,292,"/**
 * Closes all provided Closeable streams. Handles null input.
 */","* Closes the streams ignoring {@link Throwable}.
   * Must only be called in cleaning up from exception handlers.
   *
   * @param streams the Streams to close",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,stop,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:stop(),207,218,"/**
 * Stops the sink operation, interrupts the thread, and cleans up resources.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,close,org.apache.hadoop.crypto.OpensslCtrCryptoCodec:close(),111,117,"/**
 * Closes the underlying Random object if it implements Closeable.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,close,org.apache.hadoop.crypto.random.OsSecureRandom:close(),120,126,"/**
 * Closes the stream, releasing resources.
 * Sets stream to null after cleanup.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,diskIoCheckWithoutNativeIo,org.apache.hadoop.util.DiskChecker:diskIoCheckWithoutNativeIo(java.io.File),283,302,"/**
 * Checks disk I/O without native I/O, deletes the test file.
 */","* Try to perform some disk IO by writing to the given file
   * without using Native IO.
   *
   * @param file
   * @throws IOException if there was a non-retriable error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hflush,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hflush(),501,504,"/**
 * Flushes the underlying stream. Delegates to the flush() method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hsync,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hsync(),510,514,"/**
* Flushes the buffer and calls FileDescriptor.sync() for synchronization.
*/","* HSync calls sync on fhe file descriptor after a local flush() call.
     * @throws IOException failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,close,org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:close(),95,105,"/**
 * Closes the resource, recording stats and duration.
 */","* Set the finished time and then update the statistics.
   * If the operation failed then the key + .failures counter will be
   * incremented by one.
   * The operation min/mean/max values will be updated with the duration;
   * on a failure these will all be the .failures metrics.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,skip,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:skip(long),274,283,"/**
 * Skips specified number of bytes from the input stream.
 * @param n number of bytes to skip
 * @return actual number of skipped bytes
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,write,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(byte[],int,int)",479,488,"/**
 * Writes data to the output stream, updating statistics on error.
 * @param b buffer containing data
 * @param off offset within the buffer
 * @param len number of bytes to write
 * @throws IOException if an I/O error occurs
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,write,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:write(int),490,499,"/**
 * Writes a byte to the file output stream.
 * @param b the byte to be written
 * @throws IOException if an I/O error occurs
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hasCapability,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream:hasCapability(java.lang.String),516,527,"/**
 * Checks if the capability is supported.
 * @param capability Capability string to check.
 * @return True if capability is supported, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,<init>,"org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,java.util.List,org.apache.hadoop.ipc.RemoteException)",52,58,"/**
 * Constructs a PartialListing with either data or an exception.
 * @param listedPath Path of the listing.
 * @param partialListing Partial listing data.
 * @param exception Remote exception, if any.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,setCount,org.apache.hadoop.io.DataOutputBuffer$Buffer:setCount(int),78,83,"/**
 * Sets the count to a new value and returns the old count.
 * @param newCount The new count value (0 <= newCount <= buf.length)
 * @return The previous count value.
 */
","* Set the count for the current buf.
     * @param newCount the new count to set
     * @return the original count",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,"org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Object,java.lang.Throwable,org.apache.hadoop.io.retry.CallReturn$State)",60,65,"/**
 * Creates a CallReturn object with a result, throwable, and state.
 * Result and throwable must be mutually exclusive.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getConnectorAddress,org.apache.hadoop.http.HttpServer2:getConnectorAddress(int),1330,1342,"/**
 * Gets the InetSocketAddress of the connector at the given index.
 * @param index connector index; returns null if out of bounds.
 */
","* Get the address that corresponds to a particular connector.
   *
   * @param index index.
   * @return the corresponding address for the connector, or null if there's no
   *         such connector or the connector is not bounded or was closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec:calculateIV(byte[],long,byte[],int)",63,80,"/**
* Calculates the IV by XORing initIV with counter and storing in iv.
* @param initIV Initial IV byte array.
* @param counter Counter value to XOR with initIV.
* @param iv Output IV byte array.
* @param blockSize Block size for IV and initIV.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.JceCtrCryptoCodec:calculateIV(byte[],long,byte[],int)",55,72,"/**
 * Calculates the IV by XORing with initIV and counter.
 * @param initIV Initial IV byte array.
 * @param counter Counter long value to incorporate.
 * @param iv Output IV byte array.
 * @param blockSize Block size in bytes.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,<init>,"org.apache.hadoop.util.GcTimeMonitor:<init>(long,long,int,org.apache.hadoop.util.GcTimeMonitor$GcTimeAlertHandler)",125,151,"/**
 * Constructs a GcTimeMonitor with given parameters for GC time monitoring.
 */","* Create an instance of GCTimeMonitor. Once it's started, it will stay alive
   * and monitor GC time percentage until shutdown() is called. If you don't
   * put a limit on the number of GCTimeMonitor instances that you create, and
   * alertHandler != null, you should necessarily call shutdown() once the given
   * instance is not needed. Otherwise, you may create a memory leak, because
   * each running GCTimeMonitor will keep its alertHandler object in memory,
   * which in turn may reference and keep in memory many more other objects.
   *
   * @param observationWindowMs the interval over which the percentage
   *   of GC time should be calculated. A practical value would be somewhere
   *   between 30 sec and several minutes.
   * @param sleepIntervalMs how frequently this thread should wake up to check
   *   GC timings. This is also a frequency with which alertHandler will be
   *   invoked if GC time percentage exceeds the specified limit. A practical
   *   value would likely be 500..1000 ms.
   * @param maxGcTimePercentage A GC time percentage limit (0..100) within
   *   observationWindowMs. Once this is exceeded, alertHandler will be
   *   invoked every sleepIntervalMs milliseconds until GC time percentage
   *   falls below this limit.
   * @param alertHandler a single method in this interface is invoked when GC
   *   time percentage exceeds the specified limit.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ServletUtil.java,getRawPath,"org.apache.hadoop.util.ServletUtil:getRawPath(javax.servlet.http.HttpServletRequest,java.lang.String)",107,110,"/**
 * Extracts the raw path from the request URI.
 * @param request HTTP request object.
 * @param servletName Servlet name prefix.
 * @return Raw path after the servlet name.
 */
","* Parse the path component from the given request and return w/o decoding.
   * @param request Http request to parse
   * @param servletName the name of servlet that precedes the path
   * @return path component, null if the default charset is not supported",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,getMovableTypes,org.apache.hadoop.fs.StorageType:getMovableTypes(),78,80,"/**
 * Returns a list of storage types that are movable.
 * Delegates to getNonTransientTypes().
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,getTypesSupportingQuota,org.apache.hadoop.fs.StorageType:getTypesSupportingQuota(),82,84,"/**
 * Returns a list of storage types that support quota.
 * Returns the list of non-transient storage types.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,parseStorageType,org.apache.hadoop.fs.StorageType:parseStorageType(java.lang.String),90,92,"/**
 * Parses a storage type from a string.
 * @param s The string to parse.
 * @return The corresponding StorageType.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,initMode,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initMode(),631,638,"/**
 * Determines the initialization mode from system properties/env vars.
 * Returns InitMode, defaults to NORMAL if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",838,844,"/**
 * Retrieves extended attributes for a path.
 * @param path The path to retrieve attributes from.
 * @param names Attribute names to fetch.
 * @return Map of attribute names to byte array values.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getXAttrs,"org.apache.hadoop.fs.FilterFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",381,385,"/**
 * Retrieves extended attributes for a path.
 * @param path The path to retrieve attributes from.
 * @param names Attribute names to fetch.
 * @return Map of attribute names to byte array values.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,unbuffer,org.apache.hadoop.fs.FSDataInputStream:unbuffer(),237,240,"/**
 * Delegates unbuffering to StreamCapabilitiesPolicy.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,equals,org.apache.hadoop.fs.FileStatus:equals(java.lang.Object),435,445,"/**
 * Checks if this file status is equal to another.
 * @param o The object to compare to.
 * @return True if equal, false otherwise.
 */
","Compare if this object is equal to another object
   * @param   o the object to be compared.
   * @return  true if two file status has the same path name; false if not.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,equals,org.apache.hadoop.fs.shell.PathData:equals(java.lang.Object),598,603,"/**
 * Checks if two PathData objects are equal by comparing their paths.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,hashCode,org.apache.hadoop.fs.FileStatus:hashCode(),453,456,"/**
 * Returns the hash code, based on the path.
 */","* Returns a hash code value for the object, which is defined as
   * the hash code of the path name.
   *
   * @return  a hash code value for the path name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,hashCode,org.apache.hadoop.fs.shell.PathData:hashCode(),605,608,"/**
 * Returns the hash code, based on the path's hash code.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,setPath,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:setPath(org.apache.hadoop.fs.Path),534,537,"/**
* Sets the path for the real status.
* @param p The path to set.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,calculateFolderSize,org.apache.hadoop.fs.DUHelper:calculateFolderSize(java.lang.String),38,43,"/**
 * Calculates the size of a folder in bytes.
 * @param folder Path to the folder.
 * @return Size of the folder in bytes.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,check,org.apache.hadoop.fs.DUHelper:check(java.lang.String),45,53,"/**
 * Calculates folder size and disk usage.
 * @param folder The path to the folder.
 * @return String with folder size, file count, disk usage.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",853,858,"/**
 * Removes an extended attribute from a file.
 * @param path Path to the file.
 * @param name Name of the attribute to remove.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeXAttr,"org.apache.hadoop.fs.FilterFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",392,395,"/**
 * Removes an extended attribute from a path.
 * @param path The path to remove the attribute from.
 * @param name The name of the attribute to remove.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,setSamplesAndSum,"org.apache.hadoop.fs.statistics.MeanStatistic:setSamplesAndSum(long,long)",157,161,"/**
* Sets the sample count and sum.
* @param sampleCount Number of samples.
* @param newSum The new sum value.
*/
","* Set the sum and samples.
   * Synchronized.
   * @param sampleCount new sample count.
   * @param newSum new sum",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,add,org.apache.hadoop.fs.statistics.MeanStatistic:add(org.apache.hadoop.fs.statistics.MeanStatistic),212,230,"/**
 * Adds another MeanStatistic to this one, updating sum and count.
 * @param other The MeanStatistic to add. Returns this.
 */
","* Add another MeanStatistic.
   * @param other other value
   * @return mean statistic.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,equals,org.apache.hadoop.fs.statistics.MeanStatistic:equals(java.lang.Object),254,269,"/**
 * Checks if two MeanStatistic objects are equal.
 * Compares sum and samples; handles empty instances.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,toString,org.apache.hadoop.fs.statistics.MeanStatistic:toString(),285,289,"/**
 * Returns a string representation of the object's data.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,mapToString,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:mapToString(java.lang.StringBuilder,java.lang.String,java.util.Map,java.lang.String)",133,149,"/**
 * Appends map content to StringBuilder, formatted by type and separator.
 * @param sb StringBuilder to append to.
 * @param type Type string.
 * @param map Map to format.
 * @param separator Separator string.
 */
","* Given a map, add its entryset to the string.
   * The entries are only sorted if the source entryset
   * iterator is sorted, such as from a TreeMap.
   * @param sb string buffer to append to
   * @param type type (for output)
   * @param map map to evaluate
   * @param separator separator
   * @param <E> type of values of the map",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,entryToString,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:entryToString(java.util.Map$Entry),136,139,"/**
 * Converts a map entry to a string.
 * @param entry Map entry with key (String) and value (E)
 * @return String representation of the entry
 */
","* Convert an entry to the string format used in logging.
   *
   * @param entry entry to evaluate
   * @param <E> entry type
   * @return formatted string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationTrackerFactory.java,trackDuration,"org.apache.hadoop.fs.statistics.DurationTrackerFactory:trackDuration(java.lang.String,long)",48,50,"/**
 * Creates a DurationTracker for the given key and count.
 * @param key Identifier for the duration being tracked.
 * @param count Initial count value.
 * @return A DurationTracker instance.
 */
","* Initiate a duration tracking operation by creating/returning
   * an object whose {@code close()} call will
   * update the statistics.
   *
   * The statistics counter with the key name will be incremented
   * by the given count.
   *
   * The expected use is within a try-with-resources clause.
   *
   * The default implementation returns a stub duration tracker.
   * @param key statistic key prefix
   * @param count  #of times to increment the matching counter in this
   * operation.
   * @return an object to close after an operation completes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/EmptyPrefetchingStatistics.java,prefetchOperationStarted,org.apache.hadoop.fs.impl.prefetch.EmptyPrefetchingStatistics:prefetchOperationStarted(),45,48,"/**
 * Returns a stub DurationTracker for prefetch operation start.
 * @return A stub DurationTracker instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,getLong,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:getLong(java.lang.String),97,104,"/**
 * Retrieves a Long value associated with the given key.
 * Returns null if not found in counters or gauges.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,isTracked,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:isTracked(java.lang.String),106,110,"/**
 * Checks if a metric is tracked by either counters or gauges.
 * @param key The key of the metric to check.
 * @return True if tracked, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,toLongStatistic,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:toLongStatistic(java.util.Map$Entry),85,87,"/**
 * Creates a LongStatistic from a map entry (key, value).
 */","* Convert a counter/gauge entry to a long statistics.
   * @param e entry
   * @return statistic",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,next,org.apache.hadoop.fs.FileSystemStorageStatistics$LongStatisticIterator:next(),70,78,"/**
 * Returns the next LongStatistic in the sequence.
 * Throws NoSuchElementException if no more elements.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EvaluatingStatisticsMap.java,<init>,org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:<init>(),51,53,"/**
 * Creates an EvaluatingStatisticsMap with a passthrough function.
 */",* Construct with the copy function being simple passthrough.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addCounterFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addCounterFunction(java.lang.String,java.util.function.Function)",91,93,"/**
* Adds a function to the counters with the given key.
* @param key Function key.
* @param eval Function to evaluate.
*/
","* add a mapping of a key to a counter function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addGaugeFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addGaugeFunction(java.lang.String,java.util.function.Function)",100,102,"/**
* Adds a gauge function to the collection.
* @param key Function key.
* @param eval Function that evaluates to a Long.
*/
","* add a mapping of a key to a gauge function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addMinimumFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMinimumFunction(java.lang.String,java.util.function.Function)",109,111,"/**
* Adds a function to the minimums collection.
* @param key Function key.
* @param eval Function to evaluate.
*/
","* add a mapping of a key to a minimum function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addMaximumFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMaximumFunction(java.lang.String,java.util.function.Function)",118,120,"/**
* Adds a function to the maximums tracker.
* @param key Function key.
* @param eval Function to evaluate.
*/
","* add a mapping of a key to a maximum function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,addMeanStatisticFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:addMeanStatisticFunction(java.lang.String,java.util.function.Function)",127,130,"/**
 * Adds a function to calculate mean statistic for a key.
 * @param key Function key
 * @param eval Function to calculate the mean statistic
 */
","* add a mapping of a key to a meanStatistic function.
   * @param key the key
   * @param eval the evaluator",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,wrap,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:wrap(org.apache.hadoop.fs.statistics.IOStatistics),116,118,"/**
 * Wraps an IOStatistics object in a SourceWrappedStatistics.
 * @param statistics The IOStatistics object to wrap.
 * @return A SourceWrappedStatistics instance.
 */
","* Take an IOStatistics instance and wrap it in a source.
   * @param statistics statistics.
   * @return a source which will return the values",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EmptyIOStatisticsContextImpl.java,getAggregator,org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getAggregator(),49,52,"/**
 * Returns the IO statistics aggregator (EmptyIOStatisticsStore).
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,emptyStatisticsStore,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:emptyStatisticsStore(),107,109,"/**
 * Returns an empty IO statistics store instance.
 */","* Get the shared instance of the immutable empty statistics
   * store.
   * @return an empty statistics object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EmptyIOStatisticsContextImpl.java,getIOStatistics,org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:getIOStatistics(),54,57,"/**
 * Returns an instance of EmptyIOStatistics.
 * Provides default IO statistics when no data is available.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,emptyStatistics,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:emptyStatistics(),98,100,"/**
 * Returns a shared instance of empty IO statistics.
 */","* Get the shared instance of the immutable empty statistics
   * object.
   * @return an empty statistics object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setCounter,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setCounter(java.lang.String,long)",172,176,"/**
 * Sets the counter value for the given key.
 * @param key Counter key.
 * @param value New counter value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setMaximum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMaximum(java.lang.String,long)",199,202,"/**
 * Sets the maximum value for a given key in the maximum map.
 * @param key The key for which to set the maximum value.
 * @param value The maximum value to set.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setMinimum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMinimum(java.lang.String,long)",209,212,"/**
* Sets the minimum value associated with the given key.
* @param key key for which to set the minimum value
* @param value the new minimum value to set
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setGauge,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setGauge(java.lang.String,long)",235,238,"/**
* Sets the value of a gauge.
* @param key Gauge key.
* @param value The new gauge value.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementCounter,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementCounter(java.lang.String,long)",178,197,"/**
 * Increments counter by value; returns new value.
 * @param key counter key
 * @param value increment value
 * @return new counter value
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementMaximum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementMaximum(java.lang.String,long)",204,207,"/**
 * Increments the maximum value associated with the given key.
 * @param key The key for which to increment the maximum.
 * @param value The value to add to the current maximum.
 * @return The new maximum value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementMinimum,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementMinimum(java.lang.String,long)",214,217,"/**
 * Increments the minimum value associated with a key.
 * @param key Key for the minimum value.
 * @param value Value to increment by.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,incrementGauge,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:incrementGauge(java.lang.String,long)",240,243,"/**
 * Increments the gauge value for the given key.
 * @param key gauge key
 * @param value value to increment by
 * @return updated gauge value
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addMinimumSample,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMinimumSample(java.lang.String,long)",219,225,"/**
 * Adds a value to the minimum map for a given key.
 * @param key key for the minimum value
 * @param value value to be considered as a new minimum
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addMaximumSample,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMaximumSample(java.lang.String,long)",227,233,"/**
 * Adds a value to the maximum value associated with a key.
 * @param key Key for the maximum value.
 * @param value Value to potentially update the maximum.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addMeanStatisticSample,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addMeanStatisticSample(java.lang.String,long)",253,259,"/**
 * Adds a sample value to the mean statistic for the given key.
 * @param key statistic key
 * @param value sample value to add
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getCounterReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getCounterReference(java.lang.String),372,375,"/**
 * Retrieves the AtomicLong counter reference for the given key.
 * @param key The key to look up in the counter map.
 * @return The AtomicLong counter or null if not found.
 */
","* Get a reference to the atomic instance providing the
   * value for a specific counter. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getMaximumReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMaximumReference(java.lang.String),385,388,"/**
 * Retrieves the maximum reference for the given key.
 * @param key The key to look up in the maximumMap.
 * @return AtomicLong representing the maximum reference.
 */
","* Get a reference to the atomic instance providing the
   * value for a specific maximum. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getMinimumReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMinimumReference(java.lang.String),398,401,"/**
 * Retrieves the minimum reference for the given key.
 * @param key The key to look up.
 * @return AtomicLong or null if not found.
 */
","* Get a reference to the atomic instance providing the
   * value for a specific minimum. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getGaugeReference,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getGaugeReference(java.lang.String),411,414,"/**
 * Retrieves a gauge reference by key.
 * @param key The key of the gauge to retrieve.
 * @return AtomicLong representing the gauge value.
 */
","* Get a reference to the atomic instance providing the
   * value for a specific gauge. This is useful if
   * the value is passed around.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,getMeanStatistic,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:getMeanStatistic(java.lang.String),422,425,"/**
* Retrieves the mean statistic by key.
* @param key The key to look up the statistic by.
* @return The MeanStatistic object, or null if not found.
*/
","* Get a mean statistic.
   * @param key statistic name
   * @return the reference
   * @throws NullPointerException if there is no entry of that name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/PairedDurationTrackerFactory.java,asDuration,org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory$PairedDurationTracker:asDuration(),87,90,"/**
 * Returns the duration of the first duration as a Duration.
 */",* @return the global duration,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,counters,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:counters(),55,58,"/**
 * Returns the counters map from the wrapped object.
 * Delegates to the wrapped object's counters() method.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,gauges,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:gauges(),79,82,"/**
* Returns the gauges from the wrapped object.
* Delegates to the wrapped object's gauges method.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,minimums,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:minimums(),84,87,"/**
 * Delegates to the wrapped object to get minimums.
 * Returns a map of String to Long.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,maximums,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:maximums(),89,92,"/**
 * Returns the maximum values from the wrapped data source.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,meanStatistics,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:meanStatistics(),94,97,"/**
 * Returns the mean statistics from the wrapped object.
 * Delegates to the wrapped object's meanStatistics() method.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,setWrapped,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:setWrapped(org.apache.hadoop.fs.statistics.IOStatistics),73,77,"/**
 * Sets the wrapped statistics, overwriting if none exist.
 * @param wrapped The IOStatistics instance to wrap.
 */
","* Set the wrapped statistics.
   * Will fail if the field is already set.
   * @param wrapped new value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,activeInstance,org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:activeInstance(),63,66,"/**
 * Returns the active DynamicIOStatistics instance.
 * Checks if the instance is initialized; throws exception if not.
 */","* Get the statistics instance.
   * @return the instance to build/return
   * @throws IllegalStateException if the builder has already been built.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,checkMutable,org.apache.hadoop.fs.impl.FlagSet:checkMutable(),125,128,"/**
 * Verifies the FlagSet is mutable. Throws exception if immutable.
 */","* Check for mutability before any mutating operation.
   * @throws IllegalStateException if the set is still mutable",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,toByteArray,org.apache.hadoop.fs.store.DataBlocks$BlockUploadData:toByteArray(),235,250,"/**
 * Converts the block data to a byte array.
 * Reads from file or stream, caches result.
 * @throws IOException if an I/O error occurs.
 */
","* Convert to a byte array.
     * If the data is stored in a file, it will be read and returned.
     * If the data was passed in via an input stream (which happens if the
     * data is stored in a bytebuffer) then it will be converted to a byte
     * array -which will then be cached for any subsequent use.
     *
     * @return byte[] after converting the uploadBlock.
     * @throws IOException throw if an exception is caught while reading
     *                     File/InputStream or closing InputStream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/IrqHandler.java,bind,org.apache.hadoop.service.launcher.IrqHandler:bind(),89,100,"/**
 * Binds the handler to the signal.
 * Throws IllegalArgumentException if handler is already bound.
 */
","* Bind to the interrupt handler.
   * @throws IllegalArgumentException if the exception could not be set",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CloseableReferenceCount.java,unreference,org.apache.hadoop.util.CloseableReferenceCount:unreference(),65,70,"/**
 * Decrements the reference count and checks if it's closed.
 * @return True if the reference count reached STATUS_CLOSED_MASK.
 */
","* Decrement the reference count.
   *
   * @return          True if the object is closed and has no outstanding
   *                  references.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,run,org.apache.hadoop.ha.HealthMonitor$MonitorDaemon:run(),285,296,"/**
 * Main loop: attempts connection, performs health checks.
 * Exits when shouldRun is false. Handles interruption.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,setZooKeeperRef,org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:setZooKeeperRef(org.apache.zookeeper.ZooKeeper),1226,1231,"/**
 * Sets the ZooKeeper reference, ensuring it's set only once.
 * @param zk The ZooKeeper instance to set.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,snapshotMap,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:snapshotMap(java.util.Map,java.util.function.Function)",214,221,"/**
 * Creates a concurrent snapshot of a map, copying values.
 * @param source Source map to snapshot.
 * @param copyFn Function to copy values.
 * @return ConcurrentHashMap containing the snapshot.
 */
","* Take a snapshot of a supplied map, using the copy function
   * to replicate the source values.
   * @param source source map
   * @param copyFn function to copy the value
   * @param <E> type of values.
   * @return a concurrent hash map referencing the same values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,trackDuration,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDuration(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.CallableRaisingIOE)",445,450,"/**
 * Tracks the duration of an operation and returns its result.
 * @param factory DurationTrackerFactory instance
 * @param statistic Statistic name
 * @param input Callable raising IOE, returns result
 * @return Result of the operation
 */
","* Given an IOException raising callable/lambda expression,
   * execute it and update the relevant statistic.
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @param <B> return type.
   * @return the result of the operation.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,pairedTrackerFactory,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:pairedTrackerFactory(org.apache.hadoop.fs.statistics.DurationTrackerFactory,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",687,691,"/**
 * Creates a PairedDurationTrackerFactory from two factories.
 * @param first The first DurationTrackerFactory.
 * @param second The second DurationTrackerFactory.
 * @return A PairedDurationTrackerFactory.
 */
","* Create a DurationTrackerFactory which aggregates the tracking
   * of two other factories.
   * @param first first tracker factory
   * @param second second tracker factory
   * @return a factory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterLong.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableCounterLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",60,66,"/**
 * Records metric value to builder, conditionally based on 'all'.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,getCacheHit,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheHit(),82,84,"/**
 * Returns the current value of the cache hit counter.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,getCacheCleared,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheCleared(),86,88,"/**
 * Returns the number of cache clears.
 * @return Long value representing cache clear count.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,getCacheUpdated,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:getCacheUpdated(),90,92,"/**
 * Returns the value of the cache updated timestamp.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getClientBackoffDisconnected,org.apache.hadoop.ipc.metrics.RpcMetrics:getClientBackoffDisconnected(),358,360,"/**
 * Returns the backoff duration for disconnected clients.
 * @return Long value representing the backoff duration.
 */
","* Returns the number of disconnected backoffs.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getRpcSlowCalls,org.apache.hadoop.ipc.metrics.RpcMetrics:getRpcSlowCalls(),420,422,"/**
 * Returns the number of slow RPC calls.
 * @return The count of slow RPC calls.
 */
","* Returns the number of slow calls.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getRpcRequeueCalls,org.apache.hadoop.ipc.metrics.RpcMetrics:getRpcRequeueCalls(),428,431,"/**
 * Returns the number of RPC requeue calls.
 */","* Returns the number of requeue calls.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,counters,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:counters(),48,51,"/**
 * Returns the counters from the inner statistics.
 * @return Map of string keys to long values representing counters.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,gauges,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:gauges(),53,56,"/**
 * Returns gauges from the inner statistics.
 * Delegates to the inner statistics' gauges method.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,minimums,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:minimums(),58,61,"/**
* Returns the minimum statistics from the inner statistics.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,maximums,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:maximums(),63,66,"/**
 * Returns the maximum statistics from the inner statistics.
 * @return A map of string to long representing maximum values.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,meanStatistics,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:meanStatistics(),68,71,"/**
 * Returns the mean statistics from the inner statistics.
 * Delegates to the inner statistics object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,aggregate,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:aggregate(org.apache.hadoop.fs.statistics.IOStatistics),73,76,"/**
 * Delegates aggregation to the inner statistics object.
 * @param statistics Statistics object to aggregate, may be null.
 * @return True if aggregation was successful, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementCounter,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementCounter(java.lang.String,long)",78,81,"/**
 * Increments the counter for the given key by the specified value.
 * @param key counter key
 * @param value amount to increment by
 * @return updated counter value
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setCounter,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setCounter(java.lang.String,long)",83,86,"/**
* Sets the counter value for a given key in inner statistics.
* @param key The key for the counter.
* @param value The counter value to set.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setGauge,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setGauge(java.lang.String,long)",88,91,"/**
* Sets the value of a gauge statistic.
* @param key gauge key
* @param value gauge value
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementGauge,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementGauge(java.lang.String,long)",93,96,"/**
 * Increments the gauge value for the given key.
 * @param key Gauge key.
 * @param value Value to increment.
 * @return The new gauge value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setMaximum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMaximum(java.lang.String,long)",98,101,"/**
* Sets the maximum value for a given key in inner stats.
* @param key Key for which to set the maximum value.
* @param value The maximum value to set.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementMaximum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementMaximum(java.lang.String,long)",103,106,"/**
 * Increments the maximum value for a given key.
 * @param key Key to update.
 * @param value Value to increment by.
 * @return The new maximum value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setMinimum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMinimum(java.lang.String,long)",108,112,"/**
 * Sets the minimum value for the given key in inner statistics.
 * @param key key for the minimum value
 * @param value the minimum value to set
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,incrementMinimum,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:incrementMinimum(java.lang.String,long)",114,118,"/**
 * Increments the minimum value associated with a key.
 * @param key Key for the minimum value.
 * @param value Value to increment the minimum by.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addMinimumSample,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMinimumSample(java.lang.String,long)",120,124,"/**
* Adds a minimum sample to the inner statistics.
* @param key key for the sample
* @param value the minimum value observed for the key
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addMaximumSample,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMaximumSample(java.lang.String,long)",126,129,"/**
* Adds a maximum sample to the inner statistics.
* @param key sample key
* @param value sample value
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,setMeanStatistic,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",131,135,"/**
 * Sets the mean statistic for the given key.
 * @param key Statistic key.
 * @param value MeanStatistic value to set.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addMeanStatisticSample,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addMeanStatisticSample(java.lang.String,long)",137,141,"/**
 * Adds a sample value for the given key to the inner statistics.
 * @param key statistic key
 * @param value sample value to add
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,reset,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:reset(),143,146,"/**
 * Resets the statistics by calling the inner statistics' reset method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getCounterReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getCounterReference(java.lang.String),148,151,"/**
* Retrieves a counter reference for the given key.
* @param key The key for the counter reference.
* @return AtomicLong counter reference.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getMaximumReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMaximumReference(java.lang.String),153,156,"/**
 * Gets the maximum reference for the given key.
 * @param key the key to look up
 * @return AtomicLong representing the maximum reference
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getMinimumReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMinimumReference(java.lang.String),158,161,"/**
 * Gets the minimum reference count for the given key.
 * @param key The key to retrieve the minimum reference for.
 * @return The minimum reference count.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getGaugeReference,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getGaugeReference(java.lang.String),163,166,"/**
 * Returns a gauge reference for the given key.
 * @param key The key for the gauge.
 * @return The gauge reference.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,getMeanStatistic,org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:getMeanStatistic(java.lang.String),168,171,"/**
 * Retrieves the mean statistic for the given key.
 * @param key The key to identify the statistic.
 * @return The mean statistic object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addTimedOperation(java.lang.String,long)",173,178,"/**
 * Adds a timed operation to the inner statistics.
 * @param prefix Operation prefix.
 * @param durationMillis Duration in milliseconds.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/ForwardingIOStatisticsStore.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.ForwardingIOStatisticsStore:addTimedOperation(java.lang.String,java.time.Duration)",180,184,"/**
* Adds a timed operation to the inner statistics.
* @param prefix Operation prefix.
* @param duration Duration of the operation.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreBuilderImpl.java,withDurationTracking,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withDurationTracking(java.lang.String[]),77,93,"/**
 * Enables duration tracking with specified prefixes for counters.
 * @param prefixes Prefixes for duration-related statistics.
 * @return The builder instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreBuilderImpl.java,withSampleTracking,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:withSampleTracking(java.lang.String[]),95,105,"/**
 * Enables sample tracking with specified prefixes.
 * @param prefixes Prefixes for tracking statistics.
 * @return Returns the builder instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextImpl.java,reset,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:reset(),101,105,"/**
 * Resets the IO statistics context, clearing the internal statistics.
 */",* Reset the thread +.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,<init>,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:<init>(),113,115,"/**
 * Initializes the IOStatisticsSnapshot by creating necessary maps.
 */",* Construct.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setCounter,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setCounter(java.lang.String,long)",226,229,"/**
* Sets the counter value for the given key.
* @param key counter key
* @param value counter value
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setGauge,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setGauge(java.lang.String,long)",231,235,"/**
* Sets the value of a gauge.
* @param key Gauge key.
* @param value Gauge value.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setMaximum,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMaximum(java.lang.String,long)",237,241,"/**
* Sets the maximum value associated with the given key.
* @param key The key for the maximum value.
* @param value The maximum value to set.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setMinimum,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMinimum(java.lang.String,long)",243,246,"/**
 * Sets the minimum value for a given key.
 * @param key The key for which to set the minimum.
 * @param value The minimum value to set.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,setMeanStatistic,"org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",248,251,"/**
* Sets the mean statistic for the given key.
* @param key statistic key
* @param value mean statistic value
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsContext.java,enabled,org.apache.hadoop.fs.statistics.IOStatisticsContext:enabled(),95,97,"/**
 * Checks if IO statistics at thread level are enabled.
 * Returns true if enabled, false otherwise.
 */
","* Static probe to check if the thread-level IO statistics enabled.
   *
   * @return if the thread-level IO statistics enabled.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSupport.java,retrieveIOStatistics,org.apache.hadoop.fs.statistics.IOStatisticsSupport:retrieveIOStatistics(java.lang.Object),78,88,"/**
 * Retrieves IOStatistics from a source object.
 * @param source Object that may contain IOStatistics.
 * @return IOStatistics object or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,<init>,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:<init>(),123,133,"/**
 * Initializes a new BuiltInGzipDecompressor instance.
 * Resets the state and CRC.
 */",* Creates a new (pure Java) gzip decompressor.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,available,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:available(),189,192,"/**
* Returns the number of bytes available for reading.
* Sums the available bytes of the data and super classes.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,available,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:available(),225,228,"/**
 * Returns the number of bytes available for reading.
 * Sums the available bytes from this stream and its superclass.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,seekToNewSource,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seekToNewSource(long),221,227,"/**
 * Seeks to a new data source based on the target position.
 * @param targetPos The position to seek to.
 * @return True if a new data source was sought.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,readChunk,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:readChunk(long,byte[],int,int,byte[])",229,267,"/**
 * Reads a chunk of data from the file, handling checksums if needed.
 * @param pos file position
 * @param buf buffer to read into
 * @param offset offset in buffer
 * @param len number of bytes to read
 * @param checksum checksum buffer
 * @return number of bytes read
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,verifySums,"org.apache.hadoop.fs.FSInputChecker:verifySums(byte[],int,int)",336,359,"/**
 * Verifies checksums for a byte array against precalculated values.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,throwChecksumException,"org.apache.hadoop.util.DataChecksum:throwChecksumException(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,java.lang.String,long,int,int)",514,521,"/**
 * Throws a ChecksumException when checksums don't match.
 * @param type Checksum type, filename, position, expected/computed values.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getCounter,org.apache.hadoop.crypto.CryptoInputStream:getCounter(long),288,290,"/**
 * Calculates the counter based on the position and codec.
 * @param position The position in the data stream.
 * @return The calculated counter value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getPadding,org.apache.hadoop.crypto.CryptoInputStream:getPadding(long),292,294,"/**
* Calculates padding size based on position and cipher block size.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,updateEncryptor,org.apache.hadoop.crypto.CryptoOutputStream:updateEncryptor(),220,228,"/**
 * Updates the encryptor with the current stream offset.
 * Calculates padding and IV for encryption.
 */
",Update the {@link #encryptor}: calculate counter and {@link #padding}.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,checkBufferSize,"org.apache.hadoop.crypto.CryptoStreamUtils:checkBufferSize(org.apache.hadoop.crypto.CryptoCodec,int)",90,95,"/**
 * Adjusts buffer size to be a multiple of the cipher block size.
 * @param codec CryptoCodec instance. @param bufferSize Initial size.
 */
","* Check and floor buffer size.
   *
   * @param codec crypto codec.
   * @param bufferSize the size of the buffer to be used.
   * @return calc buffer size.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,link,"org.apache.hadoop.io.nativeio.NativeIO:link(java.io.File,java.io.File)",1080,1087,"/**
 * Creates a hard link from src to dst. Uses native method if available.
 */","* Creates a hardlink ""dst"" that points to ""src"".
   *
   * This is deprecated since JDK7 NIO can create hardlinks via the
   * {@link java.nio.file.Files} API.
   *
   * @param src source file
   * @param dst hardlink location
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,getInstance,org.apache.hadoop.fs.DelegationTokenRenewer:getInstance(),200,205,"/**
 * Retrieves the singleton DelegationTokenRenewer instance.
 * Creates a new instance if it doesn't already exist.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BatchedRemoteIterator.java,makeRequestIfNeeded,org.apache.hadoop.fs.BatchedRemoteIterator:makeRequestIfNeeded(),84,96,"/**
 * Makes a request if needed based on index and entries.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32GzipFileChecksum.java,<init>,"org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)",38,40,"/**
 * Constructs a MD5MD5CRC32GzipFileChecksum object.
 * @param bytesPerCRC Bytes per CRC value.
 * @param crcPerBlock CRC value per block.
 * @param md5 The MD5 hash object.
 */
","* Create a MD5FileChecksum.
   *
   * @param bytesPerCRC bytesPerCRC.
   * @param crcPerBlock crcPerBlock.
   * @param md5 md5.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,<init>,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:<init>(),43,45,"/**
* Default constructor for MD5MD5CRC32FileChecksum.
* Initializes with default values.
*/
","Same as this(0, 0, null)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32CastagnoliFileChecksum.java,<init>,"org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:<init>(int,long,org.apache.hadoop.io.MD5Hash)",38,40,"/**
 * Constructs a CastagnoliFileChecksum with given parameters.
 * @param bytesPerCRC Bytes per CRC value.
 * @param crcPerBlock CRC value per block.
 * @param md5 MD5 hash object.
 */
","* Create a MD5FileChecksum.
   *
   * @param bytesPerCRC bytesPerCRC.
   * @param crcPerBlock crcPerBlock.
   * @param md5 md5.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,accept,org.apache.hadoop.fs.GlobFilter:accept(org.apache.hadoop.fs.Path),79,82,"/**
 * Checks if a path matches the pattern and passes user filter.
 * @param path The Path to check.
 * @return True if both conditions are met, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobPattern.java,set,org.apache.hadoop.fs.GlobPattern:set(java.lang.String),74,157,"/**
* Converts a glob string to a regex pattern, handling special chars.
* @param glob the glob string to convert
*/
","* Set and compile a glob pattern
   * @param glob  the glob pattern string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFsStatus,org.apache.hadoop.fs.FilterFs:getFsStatus(org.apache.hadoop.fs.Path),146,150,"/**
 * Gets the filesystem status for a given path.
 * @param f The path to get the status for.
 * @return FsStatus object representing the status.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listStatusIterator,org.apache.hadoop.fs.FilterFileSystem:listStatusIterator(org.apache.hadoop.fs.Path),291,295,"/**
 * Returns an iterator for file statuses at the given path.
 * @param f the path to list
 * @return RemoteIterator of FileStatus objects
 */
",Return a remote iterator for listing in a directory,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,isRegularFile,org.apache.hadoop.fs.FileUtil:isRegularFile(java.io.File),632,634,"/**
 * Checks if a file is a regular file.
 * @param file The file to check.
 * @return True if it's a regular file, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,makeShellPath,"org.apache.hadoop.fs.FileUtil:makeShellPath(java.io.File,boolean)",696,703,"/**
 * Converts a File object to a shell-compatible path string.
 * @param file The file to convert.
 * @param makeCanonicalPath Whether to use canonical path.
 */
","* Convert a os-native filename to a path that works for the shell.
   * @param file The filename to convert
   * @param makeCanonicalPath
   *          Whether to make canonical path for the file passed
   * @return The unix pathname
   * @throws IOException on windows, there can be problems with the subprocess",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,permissionsFromMode,org.apache.hadoop.fs.FileUtil:permissionsFromMode(int),783,793,"/**
 * Extracts Posix file permissions from a mode integer.
 * @param mode integer representing file mode bits
 * @return Set of PosixFilePermission objects
 */
","* The permission operation of this method only involves users, user groups, and others.
   * If SUID is set, only executable permissions are reserved.
   * @param mode Permissions are represented by numerical values
   * @return The original permissions for files are stored in collections",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unpackEntries,"org.apache.hadoop.fs.FileUtil:unpackEntries(org.apache.commons.compress.archivers.tar.TarArchiveInputStream,org.apache.commons.compress.archivers.tar.TarArchiveEntry,java.io.File)",1130,1186,"/**
 * Extracts a tar archive entry to the specified output directory.
 * @param tis Input stream for the tar archive.
 * @param entry Tar archive entry to extract.
 * @param outputDir Directory to extract the entry to.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,join,"org.apache.hadoop.util.StringUtils:join(char,java.lang.String[])",1084,1086,"/**
 * Joins array of strings using a separator.
 * @param separator Separator character between strings
 * @param strings Array of strings to join
 * @return Joined string
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execute,org.apache.hadoop.util.Shell$ShellCommandExecutor:execute(),1275,1283,"/**
 * Executes the command sequence, throwing IOException on null entries.
 */","* Execute the shell command.
     * @throws IOException if the command fails, or if the command is
     * not well constructed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,checkWindowsCommandLineLength,org.apache.hadoop.util.Shell:checkWindowsCommandLineLength(java.lang.String[]),127,140,"/**
 * Checks if the combined length of commands exceeds the Windows limit.
 * @param commands Array of commands to check.
 * @throws IOException if the combined length is too long.
 */
","* Checks if a given command (String[]) fits in the Windows maximum command
   * line length Note that the input is expected to already include space
   * delimiters, no extra count will be added for delimiters.
   *
   * @param commands command parts, including any space delimiters
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/PowerShellFencer.java,buildPSScript,"org.apache.hadoop.ha.PowerShellFencer:buildPSScript(java.lang.String,java.lang.String)",115,158,"/**
 * Builds a PowerShell script to terminate a process on a host.
 * @param processName Process name to terminate.
 * @param host Hostname where the process runs.
 * @return Absolute path to the generated PowerShell script.
 */","* Build a PowerShell script to kill a java.exe process in a remote machine.
   *
   * @param processName Name of the process to kill. This is an attribute in
   *                    CommandLine.
   * @param host Host where the process is.
   * @return Path of the PowerShell script.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,toString,org.apache.hadoop.fs.permission.FsPermission:toString(),272,283,"/**
 * Returns a string representation of the combined actions.
 * Modifies last char based on stickyBit and otheraction.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,join,"org.apache.hadoop.util.StringUtils:join(char,java.lang.Iterable)",1058,1060,"/**
 * Joins strings in an iterable using a separator.
 * @param separator char used to separate strings
 * @param strings iterable of strings to join
 * @return joined string
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,close,org.apache.hadoop.fs.sftp.SFTPFileSystem$2:close(),710,722,"/**
 * Closes the connection pool and releases resources.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,close,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:close(),644,653,"/**
 * Closes the resource, flushing the buffer and closing related resources.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,close,org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:close(),391,400,"/**
 * Closes the resource, flushing the buffer and closing associated resources.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,<init>,"org.apache.hadoop.fs.DU:<init>(java.io.File,long,long,long)",36,41,"/**
 * Constructs a DiskUsage object.
 * @param path File path, interval, jitter, initialUsed - config.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,refresh,org.apache.hadoop.fs.DU:refresh(),50,58,"/**
 * Refreshes disk usage information. Starts the refresh process.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java,connect,"org.apache.hadoop.fs.sftp.SFTPConnectionPool:connect(java.lang.String,int,java.lang.String,java.lang.String,java.lang.String)",123,183,"/**
 * Connects to an SFTP server.
 * @param host SFTP host address
 * @param port Server port
 * @param user Username
 * @param password Password
 * @param keyFile Key file path
 * @return ChannelSftp object
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java,disconnect,org.apache.hadoop.fs.sftp.SFTPConnectionPool:disconnect(com.jcraft.jsch.ChannelSftp),185,211,"/**
 * Disconnects an SFTP channel, either closing or returning to pool.
 * @param channel The SFTP channel to disconnect.
 * @throws IOException if an error occurs during disconnection.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,<init>,"org.apache.hadoop.fs.FSDataOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics,long)",86,90,"/**
 * Creates a FSDataOutputStream wrapping an OutputStream.
 * @param out The wrapped OutputStream.
 * @param stats Statistics for the stream.
 * @param startPosition Initial file position.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,getChecksumSize,org.apache.hadoop.fs.FSOutputSummer:getChecksumSize(),197,199,"/**
 * Returns the checksum size from the underlying sum object.
 */",@return the size for a checksum.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,getChecksumSize,org.apache.hadoop.util.DataChecksum:getChecksumSize(int),345,347,"/**
 * Calculates the total size of checksums for given data size.
 * @param dataSize The size of the data in bytes.
 * @return The total checksum size in bytes.
 */
","* the required checksum size given the data length.
   * @param dataSize data size.
   * @return the required checksum size given the data length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,convertToByteStream,"org.apache.hadoop.fs.FSOutputSummer:convertToByteStream(java.util.zip.Checksum,int)",237,239,"/**
 * Converts a checksum value to a byte stream of specified size.
 * @param sum Checksum object.
 * @param checksumSize Size of the byte array.
 */
","* Converts a checksum integer value to a byte stream
   *
   * @param sum check sum.
   * @param checksumSize check sum size.
   * @return byte stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesRead,org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesRead(long),4206,4208,"/**
* Adds new bytes read to the thread statistics.
* @param newBytes The number of bytes to add.
*/
","* Increment the bytes read in the statistics.
     * @param newBytes the additional bytes read",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesWritten,org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesWritten(long),4214,4216,"/**
 * Increments the total bytes written by the specified amount.
 * @param newBytes The number of bytes to add to the total.
 */
","* Increment the bytes written in the statistics.
     * @param newBytes the additional bytes written",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementReadOps,org.apache.hadoop.fs.FileSystem$Statistics:incrementReadOps(int),4222,4224,"/**
 * Increments the read operations count by the given amount.
 * @param count The amount to increment the read operations by.
 */
","* Increment the number of read operations.
     * @param count number of read operations",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementLargeReadOps,org.apache.hadoop.fs.FileSystem$Statistics:incrementLargeReadOps(int),4230,4232,"/**
 * Increments the number of large read operations by the given count.
 */","* Increment the number of large read operations.
     * @param count number of large read operations",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementWriteOps,org.apache.hadoop.fs.FileSystem$Statistics:incrementWriteOps(int),4238,4240,"/**
 * Increments the number of write operations by the given count.
 * @param count The amount to increment write operations by.
 */
","* Increment the number of write operations.
     * @param count number of write operations",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesReadErasureCoded,org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesReadErasureCoded(long),4246,4248,"/**
 * Increments the number of erasure-coded bytes read.
 * @param newBytes The number of bytes to add.
 */
","* Increment the bytes read on erasure-coded files in the statistics.
     * @param newBytes the additional bytes read",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,incrementBytesReadByDistance,"org.apache.hadoop.fs.FileSystem$Statistics:incrementBytesReadByDistance(int,long)",4258,4275,"/**
 * Increments bytes read based on distance.
 * @param distance distance value; determines which counter to increment.
 * @param newBytes number of bytes read.
 */
","* Increment the bytes read by the network distance in the statistics
     * In the common network topology setup, distance value should be an even
     * number such as 0, 2, 4, 6. To make it more general, we group distance
     * by {1, 2}, {3, 4} and {5 and beyond} for accounting.
     * @param distance the network distance
     * @param newBytes the additional bytes read",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,increaseRemoteReadTime,org.apache.hadoop.fs.FileSystem$Statistics:increaseRemoteReadTime(long),4281,4283,"/**
* Increases the remote read time by the given duration in milliseconds.
*/
","* Increment the time taken to read bytes from remote in the statistics.
     * @param durationMS time taken in ms to read bytes from remote",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,visitAll,org.apache.hadoop.fs.FileSystem$Statistics:visitAll(org.apache.hadoop.fs.FileSystem$Statistics$StatisticsAggregator),4295,4302,"/**
 * Visits all data and aggregates results using the visitor.
 * @param visitor Aggregator to process and combine data.
 * @return Aggregated result from visiting all data.
 */
","* Apply the given aggregator to all StatisticsData objects associated with
     * this Statistics object.
     *
     * For each StatisticsData object, we will call accept on the visitor.
     * Finally, at the end, we will call aggregate to get the final total.
     *
     * @param         visitor to use.
     * @return        The total.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.FilterFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path),603,606,"/**
 * Removes the default ACL for the given path.
 * @param path The path for which to remove the default ACL.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,org.apache.hadoop.fs.ContentSummary$Builder:<init>(),47,48,"/**
 * Default constructor for the Builder class.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeConsumed,org.apache.hadoop.fs.ContentSummary$Builder:typeConsumed(long[]),108,112,"/**
 * Sets the type consumed array and returns the builder.
 * @param typeConsumed Array of consumed types.
 * @return The builder instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeQuota,"org.apache.hadoop.fs.ContentSummary$Builder:typeQuota(org.apache.hadoop.fs.StorageType,long)",114,118,"/**
* Sets the storage type quota and returns the builder.
* @param type Storage type to set quota for.
* @param quota Quota value for the specified type.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeConsumed,"org.apache.hadoop.fs.ContentSummary$Builder:typeConsumed(org.apache.hadoop.fs.StorageType,long)",120,124,"/**
* Records storage type consumption; calls superclass method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,typeQuota,org.apache.hadoop.fs.ContentSummary$Builder:typeQuota(long[]),126,130,"/**
 * Sets the type quota array and returns the builder.
 * @param typeQuota Array of type quotas.
 * @return The builder instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,build,org.apache.hadoop.fs.QuotaUsage$Builder:build(),93,95,"/**
 * Creates a QuotaUsage object from the builder's configuration.
 * @return A QuotaUsage object representing the built configuration.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,org.apache.hadoop.fs.ContentSummary:<init>(org.apache.hadoop.fs.ContentSummary$Builder),194,204,"/**
 * Constructs a ContentSummary with values from the Builder.
 * @param builder The Builder object containing initialization values.
 */
","* Constructor for ContentSummary.Builder.
   *
   * @param builder builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,getAlgorithmName,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getAlgorithmName(),60,64,"/**
 * Returns the name of the algorithm used.
 * Combines CRC type and block/bytes info.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,getChecksumOpt,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getChecksumOpt(),94,97,"/**
 * Returns a ChecksumOpt object with CRC type and bytes per CRC.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,<init>,org.apache.hadoop.fs.Options$ChecksumOpt:<init>(),255,257,"/**
 * Default constructor. Initializes with default checksum type.
 */",* Create a uninitialized one,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,createDisabled,org.apache.hadoop.fs.Options$ChecksumOpt:createDisabled(),287,289,"/**
 * Creates a disabled ChecksumOpt with a null checksum type.
 */","* Create a ChecksumOpts that disables checksum.
     *
     * @return ChecksumOpt.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CompositeCrcFileChecksum.java,getChecksumOpt,org.apache.hadoop.fs.CompositeCrcFileChecksum:getChecksumOpt(),69,72,"/**
 * Returns a ChecksumOpt object with the current CRC type and bytes per CRC.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,write,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:write(java.io.DataOutput),106,111,"/**
 * Writes the data to the output stream.
 * Writes bytesPerCRC, crcPerBlock, and MD5 hash.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,hasPattern,org.apache.hadoop.fs.GlobFilter:hasPattern(),75,77,"/**
 * Checks if the pattern contains any wildcard characters.
 * @return True if the pattern has wildcards, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CreateFlag.java,validate,org.apache.hadoop.fs.CreateFlag:validate(java.util.EnumSet),149,162,"/**
 * Validates CreateFlag enum set, throwing exception on invalid state.
 */","* Validate the CreateFlag and throw exception if it is invalid
   * @param flag set of CreateFlag
   * @throws HadoopIllegalArgumentException if the CreateFlag is invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/XAttrSetFlag.java,validate,"org.apache.hadoop.fs.XAttrSetFlag:validate(java.lang.String,boolean,java.util.EnumSet)",53,70,"/**
 * Validates flags based on existence of XAttr.
 * @param xAttrName Attribute name.
 * @param xAttrExists Whether the attribute exists.
 * @param flag Set of XAttrSetFlag values.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,checkScheme,"org.apache.hadoop.fs.AbstractFileSystem:checkScheme(java.net.URI,java.lang.String)",291,300,"/**
 * Validates URI scheme matches the supported scheme.
 * @param uri The URI to validate.
 * @param supportedScheme The expected scheme.
 */
","* Check that the Uri's scheme matches.
   *
   * @param uri name URI of the FS.
   * @param supportedScheme supported scheme.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/InvalidPathException.java,<init>,org.apache.hadoop.fs.InvalidPathException:<init>(java.lang.String),38,40,"/**
 * Constructs an InvalidPathException with the given path.
 * @param path The invalid path name.
 */
","* Constructs exception with the specified detail message.
   * 
   * @param path invalid path.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/InvalidPathException.java,<init>,"org.apache.hadoop.fs.InvalidPathException:<init>(java.lang.String,java.lang.String)",48,51,"/**
 * Constructs an InvalidPathException with a path and reason.
 * @param path The invalid path.
 * @param reason Optional reason for the invalid path.
 */
","* Constructs exception with the specified detail message.
   * 
   * @param path invalid path.
   * @param reason Reason <code>path</code> is invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java,findFirstValidInput,org.apache.hadoop.io.erasurecode.coder.util.HHUtil:findFirstValidInput(java.lang.Object[]),210,219,"/**
 * Finds the first non-null element in the input array.
 * @param inputs Array of elements to check.
 * @return The first non-null element, or throws exception.
 */
","* Find the valid input from all the inputs.
   *
   * @param <T> Generics Type T.
   * @param inputs input buffers to look for valid input
   * @return the first valid input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,findFirstValidInput,org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:findFirstValidInput(java.lang.Object[]),163,172,"/**
 * Finds the first non-null element in an array.
 * @param inputs Array of elements to check.
 * @return First non-null element or throws exception.
 */
","* Find the valid input from all the inputs.
   * @param inputs input buffers to look for valid input
   * @return the first valid input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayEncodingState.java,checkBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:checkBuffers(byte[][]),91,103,"/**
 * Validates buffers, ensuring they are non-null and of correct length.
 */
","* Check and ensure the buffers are of the desired length.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,checkOutputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:checkOutputBuffers(java.nio.ByteBuffer[]),129,145,"/**
 * Validates the provided output buffers against expected properties.
 * @param buffers Array of ByteBuffers to validate.
 */
","* Check and ensure the buffers are of the desired length and type, direct
   * buffers or not.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,checkOutputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:checkOutputBuffers(byte[][]),121,133,"/**
 * Validates buffers, ensuring they are non-null and of correct length.
 */
","* Check and ensure the buffers are of the desired length.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferEncodingState.java,checkBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:checkBuffers(java.nio.ByteBuffer[]),91,107,"/**
 * Validates buffers, ensuring they are non-null, correct length, and directness.
 */","* Check and ensure the buffers are of the desired length and type, direct
   * buffers or not.
   * @param buffers the buffers to check",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,checkPrimitive,org.apache.hadoop.io.ArrayPrimitiveWritable:checkPrimitive(java.lang.Class),71,79,"/**
 * Checks if the given type is a supported primitive type.
 * @param componentType Class of the array component type.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,checkDeclaredComponentType,org.apache.hadoop.io.ArrayPrimitiveWritable:checkDeclaredComponentType(java.lang.Class),81,88,"/**
 * Checks if the component type matches the declared type.
 * Throws exception if types don't match.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,checkArray,org.apache.hadoop.io.ArrayPrimitiveWritable:checkArray(java.lang.Object),90,98,"/**
 * Checks if the given object is a non-null array.
 * @param value Object to validate; must be a non-null array.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseGetLevelArgs,"org.apache.hadoop.log.LogLevel$CLI:parseGetLevelArgs(java.lang.String[],int)",172,188,"/**
 * Parses -getlevel arguments, sets operation, and advances index.
 * @param args Command-line arguments.
 * @param index Current argument index.
 * @return Updated argument index.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseSetLevelArgs,"org.apache.hadoop.log.LogLevel$CLI:parseSetLevelArgs(java.lang.String[],int)",190,207,"/**
 * Parses -setlevel arguments, sets operation, and extracts values.
 * @param args Command-line arguments.
 * @param index Starting index for parsing.
 * @return Next index after parsing.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,stopProxy,org.apache.hadoop.ipc.RPC:stopProxy(java.lang.Object),792,821,"/**
 * Closes the given proxy or its invocation handler, if possible.
 * Throws exception if neither is Closeable.
 */
","* Stop the proxy. Proxy must either implement {@link Closeable} or must have
   * associated {@link RpcInvocationHandler}.
   * 
   * @param proxy
   *          the RPC proxy object to be stopped
   * @throws HadoopIllegalArgumentException
   *           if the proxy does not implement {@link Closeable} interface or
   *           does not have closeable {@link InvocationHandler}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,<init>,org.apache.hadoop.util.ZKUtil$BadAclFormatException:<init>(java.lang.String),206,208,"/**
 * Constructs a BadAclFormatException with the given error message.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,<init>,org.apache.hadoop.util.ZKUtil$BadAuthFormatException:<init>(java.lang.String),216,218,"/**
 * Constructs a BadAuthFormatException with the given error message.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,processChecksumOpt,"org.apache.hadoop.fs.Options$ChecksumOpt:processChecksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt,org.apache.hadoop.fs.Options$ChecksumOpt,int)",302,328,"/**
 * Determines the ChecksumOpt based on user and default options.
 * @param defaultOpt Default ChecksumOpt.
 * @param userOpt User-specified ChecksumOpt.
 * @param userBytesPerChecksum User-specified bytes per checksum.
 * @return Combined ChecksumOpt.
 */
","* A helper method for processing user input and default value to 
     * create a combined checksum option. This is a bit complicated because
     * bytesPerChecksum is kept for backward compatibility.
     *
     * @param defaultOpt Default checksum option
     * @param userOpt User-specified checksum option. Ignored if null.
     * @param userBytesPerChecksum User-specified bytesPerChecksum
     *                Ignored if {@literal <} 0.
     * @return ChecksumOpt.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setPermission,"org.apache.hadoop.fs.DelegateToFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",221,226,"/**
 * Sets the permission for a given path.
 * @param f The path to set the permission on.
 * @param permission The permission to apply.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setPermission,"org.apache.hadoop.fs.FilterFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",545,549,"/**
 * Sets the file system permission for a given path.
 * @param p the path to set the permission on
 * @param permission the desired FsPermission
 * @throws IOException if an I/O error occurs
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",877,882,"/**
 * Deletes a snapshot from the file system at the given path.
 * @param path The path to the snapshot.
 * @param snapshotName Name of the snapshot to delete.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,deleteSnapshot,"org.apache.hadoop.fs.FilterFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",409,413,"/**
 * Deletes a snapshot from the specified path.
 * @param path Path of the snapshot to delete.
 * @param snapshotName Name of the snapshot to delete.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getDefaultPortIfDefined,org.apache.hadoop.fs.DelegateToFileSystem:getDefaultPortIfDefined(org.apache.hadoop.fs.FileSystem),69,72,"/**
 * Gets default port, using FS default if FS default is 0.
 * @param theFsImpl FileSystem implementation.
 * @return Default port number.
 */
","* Returns the default port if the file system defines one.
   * {@link FileSystem#getDefaultPort()} returns 0 to indicate the default port
   * is undefined.  However, the logic that consumes this value expects to
   * receive -1 to indicate the port is undefined, which agrees with the
   * contract of {@link URI#getPort()}.
   *
   * @param theFsImpl file system to check for default port
   * @return default port, or -1 if default port is undefined",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,canonicalizeUri,org.apache.hadoop.fs.FileSystem:canonicalizeUri(java.net.URI),402,417,"/**
 * Canonicalizes a URI, setting the default port if missing.
 * @param uri The URI to canonicalize.
 * @return The canonicalized URI.
 */
","* Canonicalize the given URI.
   *
   * This is implementation-dependent, and may for example consist of
   * canonicalizing the hostname using DNS and adding the default
   * port if not specified.
   *
   * The default implementation simply fills in the default port if
   * not specified and if {@link #getDefaultPort()} returns a
   * default port.
   *
   * @param uri url.
   * @return URI
   * @see NetUtils#getCanonicalUri(URI, int)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.DelegateToFileSystem:getInitialWorkingDirectory(),74,77,"/**
 * Gets the initial working directory from the file system implementation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.FilterFileSystem:getInitialWorkingDirectory(),324,327,"/**
 * Gets the initial working directory from the file system.
 * @return Path representing the initial working directory.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.FilterFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),484,488,"/**
 * Gets the status of a file link.
 * @param f Path to the file link.
 * @return FileStatus object representing the link status.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.LocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),162,165,"/**
 * Gets the status of a file link.
 * @param f Path to the file link.
 * @return FileStatus object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.DelegateToFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),136,146,"/**
 * Gets the file link status.
 * @param f the path to check
 * @return FileStatus object, or null if not found
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getLinkTarget,org.apache.hadoop.fs.DelegateToFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),257,260,"/**
 * Gets the target of a symbolic link.
 * @param f Path to the symbolic link
 * @return Path to the link target
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getLinkTarget,org.apache.hadoop.fs.FilterFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),494,496,"/**
 * Gets the target of a symbolic link.
 * @param f the Path object representing the link
 * @return the Path object representing the link target
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,getLinkTarget,org.apache.hadoop.fs.LocalFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),167,170,"/**
 * Gets the target of a symbolic link.
 * @param f Path to the symbolic link
 * @return Path to the link target
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,truncate,"org.apache.hadoop.fs.DelegateToFileSystem:truncate(org.apache.hadoop.fs.Path,long)",200,204,"/**
 * Truncates a file to the specified length.
 * @param f The file to truncate.
 * @param newLength The new length of the file.
 * @return True if successful, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,truncate,"org.apache.hadoop.fs.FilterFileSystem:truncate(org.apache.hadoop.fs.Path,long)",260,263,"/**
* Truncates a file to the specified length.
* @param f The file to truncate.
* @param newLength The new length of the file.
* @return True if successful, false otherwise.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setReplication,"org.apache.hadoop.fs.DelegateToFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",228,233,"/**
 * Sets the replication factor for a given file.
 * @param f Path to the file. @param replication New replication count.
 * @return True if successful, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setReplication,"org.apache.hadoop.fs.FilterFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",240,243,"/**
 * Sets the replication factor for a given path.
 * @param src The path to set replication for.
 * @param replication The new replication factor.
 * @return True if successful, false otherwise.
 */
","* Set replication for an existing file.
   * 
   * @param src file name
   * @param replication new replication
   * @throws IOException raised on errors performing I/O.
   * @return true if successful;
   *         false if file does not exist or is a directory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setTimes,"org.apache.hadoop.fs.DelegateToFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",235,239,"/**
 * Sets modification and access times for a file.
 * @param f Path to the file. mtime, atime are timestamps.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,updateTime,org.apache.hadoop.fs.shell.TouchCommands$Touch:updateTime(org.apache.hadoop.fs.shell.PathData),177,195,"/**
* Updates the timestamp of a PathData item, using provided or current time.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setTimes,"org.apache.hadoop.fs.FilterFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",539,543,"/**
 * Sets modification and access times for a given path.
 * @param p The path to update.
 * @param mtime Modification time.
 * @param atime Access time.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,setVerifyChecksum,org.apache.hadoop.fs.DelegateToFileSystem:setVerifyChecksum(boolean),241,244,"/**
* Sets whether to verify checksums during file system operations.
* @param verifyChecksum flag to enable/disable checksum verification
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setVerifyChecksum,org.apache.hadoop.fs.FilterFileSystem:setVerifyChecksum(boolean),512,515,"/**
 * Sets whether to verify checksums.
 * @param verifyChecksum Flag to enable/disable checksum verification.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,supportsSymlinks,org.apache.hadoop.fs.DelegateToFileSystem:supportsSymlinks(),246,249,"/**
 * Checks if the file system supports symbolic links.
 * Delegates to the underlying file system implementation.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,supportsSymlinks,org.apache.hadoop.fs.FilterFileSystem:supportsSymlinks(),490,492,"/**
 * Checks if the filesystem supports symbolic links.
 * @return True if symlinks are supported, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,createSymlink,"org.apache.hadoop.fs.DelegateToFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",251,255,"/**
 * Creates a symbolic link at the specified link path.
 * @param target The target path the link points to.
 * @param link The link path to create.
 * @param createParent Whether to create parent directories.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createSymlink,"org.apache.hadoop.fs.FilterFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",476,482,"/**
 * Creates a symbolic link at the specified link path.
 * @param target The file or directory to link to.
 * @param link The link path.
 * @param createParent Whether to create parent directories.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,createSymlink,"org.apache.hadoop.fs.LocalFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",156,160,"/**
 * Creates a symbolic link.
 * @param target The path to link to.
 * @param link The link path.
 * @param createParent Whether to create parent directories.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFs:truncate(org.apache.hadoop.fs.Path,long)",559,566,"/**
 * Truncates a file to the specified length.
 * @param f Path to file. @param newLength New file length.
 * @return True if successful.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,create,"org.apache.hadoop.fs.http.HttpsFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",69,75,"/**
 * Creates a data output stream at the given path.
 * @throws UnsupportedOperationException Always thrown.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,create,"org.apache.hadoop.fs.http.HttpFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",69,75,"/**
* Creates a data output stream at the given path.
* Throws UnsupportedOperationException.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,append,"org.apache.hadoop.fs.http.HttpsFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",77,81,"/**
 * Appends data to a file. Throws UnsupportedOperationException.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,append,"org.apache.hadoop.fs.http.HttpFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",77,81,"/**
* Appends data to a file. Throws UnsupportedOperationException.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,rename,"org.apache.hadoop.fs.http.HttpsFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",83,86,"/**
* Renames a file or directory. Throws UnsupportedOperationException.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,rename,"org.apache.hadoop.fs.http.HttpFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",83,86,"/**
* Renames a file or directory. Throws UnsupportedOperationException.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,delete,"org.apache.hadoop.fs.http.HttpsFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",88,91,"/**
* Throws an UnsupportedOperationException; deletes a file/directory.
* @param path Path to delete
* @param b Unused boolean value
* @throws IOException if an I/O error occurs
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,delete,"org.apache.hadoop.fs.http.HttpFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",88,91,"/**
 * Deletes a file or directory at the given path. Throws UnsupportedOperationException.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,listStatus,org.apache.hadoop.fs.http.HttpsFileSystem:listStatus(org.apache.hadoop.fs.Path),93,96,"/**
 * Lists status of files and directories under the given path.
 * @param path Path to list status for; throws UnsupportedOperationException.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,listStatus,org.apache.hadoop.fs.http.HttpFileSystem:listStatus(org.apache.hadoop.fs.Path),93,96,"/**
 * Lists status of files and directories under the given path.
 * @param path Path to list status for.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,mkdirs,"org.apache.hadoop.fs.http.HttpsFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",107,111,"/**
 * Creates directory recursively with specified permissions.
 * @param path Path to create
 * @param fsPermission Permissions for the directory
 * @return false (currently)
 * @throws IOException if an I/O error occurs
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,mkdirs,"org.apache.hadoop.fs.http.HttpFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",107,111,"/**
 * Attempts to create directories recursively. Returns false.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.http.HttpsFileSystem:getWorkingDirectory(),102,105,"/**
 * Returns the working directory as a Path object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.http.HttpFileSystem:getWorkingDirectory(),102,105,"/**
 * Returns the working directory path.
 * @return Path object representing the working directory.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.http.HttpsFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),98,100,"/**
 * Sets the working directory to the provided Path.
 * @param path The new working directory.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.http.HttpFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),98,100,"/**
 * Sets the working directory to the provided Path.
 * @param path The new working directory.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getUri,org.apache.hadoop.fs.http.HttpsFileSystem:getUri(),56,59,"/**
 * Returns the URI of the resource.
 * @return The URI object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getUri,org.apache.hadoop.fs.http.HttpFileSystem:getUri(),56,59,"/**
 * Returns the URI associated with this object.
 * @return The URI, or null if not set.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,skip,org.apache.hadoop.fs.BufferedFSInputStream:skip(long),70,78,"/**
 * Skips n bytes from the current position.
 * @param n number of bytes to skip; must be positive
 * @return number of skipped bytes
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,minSeekForVectorReads,org.apache.hadoop.fs.BufferedFSInputStream:minSeekForVectorReads(),169,172,"/**
* Delegates to the wrapped PositionedReadable for minSeek.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,minSeekForVectorReads,org.apache.hadoop.fs.FSDataInputStream:minSeekForVectorReads(),294,297,"/**
* Delegates to the underlying PositionedReadable for minSeek.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,maxReadSizeForVectorReads,org.apache.hadoop.fs.BufferedFSInputStream:maxReadSizeForVectorReads(),174,177,"/**
 * Delegates to the underlying PositionedReadable for max read size.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,maxReadSizeForVectorReads,org.apache.hadoop.fs.FSDataInputStream:maxReadSizeForVectorReads(),299,302,"/**
 * Delegates to the underlying PositionedReadable for max read size.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBPartHandle.java,from,org.apache.hadoop.fs.BBPartHandle:from(java.nio.ByteBuffer),40,42,"/**
 * Creates a PartHandle from a ByteBuffer.
 * @param byteBuffer The buffer containing part data.
 * @return A PartHandle object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBPartHandle.java,equals,org.apache.hadoop.fs.BBPartHandle:equals(java.lang.Object),54,62,"/**
 * Checks if two PartHandle objects are equal based on their bytes.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",891,897,"/**
 * Sets the storage policy for a file system path.
 * @param path Path to the file system.
 * @param policyName Name of the storage policy.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setStoragePolicy,"org.apache.hadoop.fs.FilterFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",420,424,"/**
 * Sets the storage policy for a given path.
 * @param path The path to apply the policy to.
 * @param policyName The name of the storage policy.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,disconnect,org.apache.hadoop.fs.ftp.FTPFileSystem:disconnect(org.apache.commons.net.ftp.FTPClient),248,260,"/**
 * Disconnects the FTP client, logging out and handling errors.
 * @param client The FTPClient to disconnect.
 * @throws IOException if an I/O error occurs.
 */
","* Logout and disconnect the given FTPClient. *
   * 
   * @param client
   * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,close,org.apache.hadoop.fs.ftp.FTPFileSystem$1:close(),104,107,"/**
 * Closes the underlying output stream.
 */",* Close the underlying output stream.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPInputStream.java,close,org.apache.hadoop.fs.ftp.FTPInputStream:close(),103,121,"/**
 * Closes the FTP connection, logging out and disconnecting.
 * Throws IOException if connection fails or transfer incomplete.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFsAction,"org.apache.hadoop.fs.ftp.FTPFileSystem:getFsAction(int,org.apache.commons.net.ftp.FTPFile)",440,453,"/**
 * Determines the FsAction based on file permissions.
 * @param accessGroup Access group to check.
 * @param ftpFile The FTP file to check permissions for.
 * @return The resulting FsAction.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,hasNext,org.apache.hadoop.fs.FileSystem$DirListingIterator:hasNext(),2320,2324,"/**
 * Checks if there are more entries to process.
 * Returns true if entries exist or more are available.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,org.apache.hadoop.fs.ContentSummary:<init>(),149,150,"/**
 * Default constructor for ContentSummary. Deprecated.
 */",Constructor deprecated by ContentSummary.Builder,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,"org.apache.hadoop.fs.ContentSummary:<init>(long,long,long,long,long,long)",177,187,"/**
 * Creates a ContentSummary object with provided data.
 * @param length File length, fileCount, directoryCount, quota, etc.
 */
","* Constructor, deprecated by ContentSummary.Builder.
   *
   * @param length length.
   * @param fileCount file count.
   * @param directoryCount directory count.
   * @param quota quota.
   * @param spaceConsumed space consumed.
   * @param spaceQuota space quota.
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,equals,org.apache.hadoop.fs.ContentSummary:equals(java.lang.Object),257,275,"/**
* Checks if two ContentSummary objects are equal.
* Compares all fields for equality.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,hashCode,org.apache.hadoop.fs.ContentSummary:hashCode(),277,284,"/**
 * Calculates hash code based on internal state, including erasure policy.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",824,829,"/**
 * Retrieves extended attribute for a path.
 * @param path Path to retrieve the attribute from.
 * @param name Attribute name. Returns byte[] or null.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getXAttr,"org.apache.hadoop.fs.FilterFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",371,374,"/**
 * Retrieves extended attribute for a given path.
 * @param path Path to fetch the attribute from.
 * @param name Attribute name.
 * @return Byte array representing the attribute value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,getDelay,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:getDelay(java.util.concurrent.TimeUnit),82,86,"/**
 * Calculates the remaining delay based on the given time unit.
 * @param unit Time unit for delay calculation (e.g., SECONDS)
 * @return Delay in the specified time unit.
 */
",Get the delay until this event should happen.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,updateRenewalTime,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:updateRenewalTime(long),116,118,"/**
* Updates the renewal time by adding a delay, adjusted slightly.
*/
","* Set a new time for the renewal.
     * It can only be called when the action is not in the queue or any
     * collection because the hashCode may change
     * @param delay the renewal time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,touch,org.apache.hadoop.ipc.Client$Connection:touch(),465,467,"/**
 * Updates the last activity timestamp to the current time.
 */",Update lastActivity with the current time.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ThreadUtil.java,sleepAtLeastIgnoreInterrupts,org.apache.hadoop.util.ThreadUtil:sleepAtLeastIgnoreInterrupts(long),39,50,"/**
 * Sleeps for at least 'millis' milliseconds, ignoring interrupts.
 */
","* Cause the current thread to sleep as close as possible to the provided
   * number of milliseconds. This method will log and ignore any
   * {@link InterruptedException} encountered.
   * 
   * @param millis the number of milliseconds for the current thread to sleep",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Timer.java,now,org.apache.hadoop.util.Timer:now(),39,41,"/**
 * Returns the current time in milliseconds.
 */","* Current system time.  Do not use this to calculate a duration or interval
   * to sleep, because it will be broken by settimeofday.  Instead, use
   * monotonicNow.
   * @return current time in msec.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/AsyncDiskService.java,awaitTermination,org.apache.hadoop.util.AsyncDiskService:awaitTermination(long),131,147,"/**
 * Waits for all executor threads to terminate, timeout in ms.
 * @param milliseconds Timeout duration in milliseconds.
 * @return True if all threads terminated, false otherwise.
 */
","* Wait for the termination of the thread pools.
   * 
   * @param milliseconds  The number of milliseconds to wait
   * @return   true if all thread pools are terminated without time limit
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,ceiling,"org.apache.hadoop.fs.TrashPolicyDefault$Emptier:ceiling(long,long)",322,324,"/**
 * Calculates the ceiling of a time value based on an interval.
 * @param time The time value.
 * @param interval The interval to use for calculation.
 * @return The ceiling value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,readChunk,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readChunk(long,byte[],int,int,byte[])",266,307,"/**
 * Reads a chunk of data from the file, handling checksums if needed.
 * @param pos  file position to read from
 * @param buf  buffer to read data into
 * @param offset offset into the buffer
 * @param len  number of bytes to read
 * @param checksum checksum buffer, if checksums are needed
 * @return number of bytes read
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,seekToNewSource,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:seekToNewSource(long),258,264,"/**
 * Seeks to a new data source, attempting to resume from targetPos.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,checkBytes,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:checkBytes(java.nio.ByteBuffer,long,java.nio.ByteBuffer,long,int,org.apache.hadoop.fs.Path)",376,426,"/**
 * Verifies checksums of data chunks against sums, throws exception on mismatch.
 * @param sumsBytes Byte buffer containing checksum sums.
 * @param data Byte buffer containing data to check.
 */
","* Check the data against the checksums.
     * @param sumsBytes the checksum data
     * @param sumsOffset where from the checksum file this buffer started
     * @param data the file data
     * @param dataOffset where the file data started (must be a multiple of
     *                  bytesPerSum)
     * @param bytesPerSum how many bytes per a checksum
     * @param file the path of the filename
     * @return the data buffer
     * @throws CompletionException if the checksums don't match",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getSumBufferSize,"org.apache.hadoop.fs.ChecksumFs:getSumBufferSize(int,int,org.apache.hadoop.fs.Path)",124,131,"/**
 * Calculates the sum buffer size based on provided parameters.
 * @param bytesPerSum Bytes per sum; bufferSize, file.
 * @return The calculated sum buffer size.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,open,org.apache.hadoop.fs.AbstractFileSystem:open(org.apache.hadoop.fs.Path),721,724,"/**
 * Opens a file for input.
 * @param f Path to the file to open.
 * @return FSDataInputStream for reading the file.
 */
","* The specification of this method matches that of
   * {@link FileContext#open(Path)} except that Path f must be for this
   * file system.
   *
   * @param f the path.
   * @throws AccessControlException access control exception.
   * @throws FileNotFoundException file not found exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.
   * @return input stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getServerDefaults,org.apache.hadoop.fs.FilterFs:getServerDefaults(org.apache.hadoop.fs.Path),163,166,"/**
 * Gets server defaults for a given path.
 * @param f Path to retrieve defaults for.
 * @return FsServerDefaults object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getChecksumFileLength,"org.apache.hadoop.fs.ChecksumFs:getChecksumFileLength(org.apache.hadoop.fs.Path,long)",111,113,"/**
 * Calculates checksum length based on file size.
 * @param file Path to the file. @param fileSize File size in bytes.
 */
","* Return the length of the checksum file given the size of the
   * actual file.
   *
   * @param file the file path.
   * @param fileSize file size.
   * @return check sum file length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,listLocatedStatus,org.apache.hadoop.fs.ChecksumFs:listLocatedStatus(org.apache.hadoop.fs.Path),582,614,"/**
 * Lists located file statuses, filtering out checksum files.
 * @param f the path to list
 * @return RemoteIterator of LocatedFileStatus objects
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,useStatIfAvailable,org.apache.hadoop.fs.RawLocalFileSystem:useStatIfAvailable(),96,99,"/**
 * Sets useDeprecatedFileStatus based on Stat availability.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createOutputStream,"org.apache.hadoop.fs.RawLocalFileSystem:createOutputStream(org.apache.hadoop.fs.Path,boolean)",570,573,"/**
 * Creates an output stream for the given file path.
 * @param f file path
 * @param append append mode
 * @return OutputStream object
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatus,org.apache.hadoop.fs.FileSystem:getStatus(org.apache.hadoop.fs.Path),3041,3043,"/**
 * Returns a fixed FsStatus object for a given path.
 * @param p The path to check (not used in the implementation).
 * @return A pre-defined FsStatus object.
 */
","* Returns a status object describing the use and capacity of the
   * filesystem. If the filesystem has multiple partitions, the
   * use and capacity of the partition pointed to by the specified
   * path is reflected.
   * @param p Path for which status should be obtained. null means
   * the default partition.
   * @return a FsStatus object
   * @throws IOException
   *           see specific implementation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFsStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFsStatus(),1130,1133,"/**
 * Returns a default FsStatus object with all values set to 0.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFsStatus,org.apache.hadoop.fs.viewfs.ViewFs:getFsStatus(),445,449,"/**
 * Returns a default FsStatus object.
 * @return FsStatus object with default values.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,registerCommands,org.apache.hadoop.fs.FsShellPermissions:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),50,54,"/**
 * Registers command classes with the given command factory.
 * @param factory CommandFactory instance to register commands.
 */
","* Register the permission related commands with the factory
   * @param factory the command factory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,registerCommands,org.apache.hadoop.fs.shell.Test:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),37,39,"/**
 * Registers a command class with the given factory.
 * @param factory CommandFactory instance to register with.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,registerCommands,org.apache.hadoop.fs.shell.SnapshotCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),41,45,"/**
 * Registers snapshot commands with the given command factory.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,registerCommands,org.apache.hadoop.fs.shell.find.Find:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),49,51,"/**
 * Registers the Find command with the given command factory.
 */","* Register the names for the count command
   * 
   * @param factory the command factory that will instantiate this class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,registerCommands,org.apache.hadoop.fs.shell.Head:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),40,42,"/**
 * Registers the ""head"" command with the given command factory.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,registerCommands,org.apache.hadoop.fs.shell.Ls:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),45,48,"/**
 * Registers Ls and Lsr commands with the given command factory.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,registerCommands,org.apache.hadoop.fs.shell.Tail:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),42,44,"/**
 * Registers the Tail command with the given command factory.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,registerCommands,org.apache.hadoop.fs.shell.FsUsage:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),46,50,"/**
 * Registers command classes with the given command factory.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,registerCommands,org.apache.hadoop.fs.shell.XAttrCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),42,45,"/**
* Registers command classes with the given factory.
* @param factory CommandFactory instance to register commands.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,registerCommands,org.apache.hadoop.fs.shell.Delete:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),50,55,"/**
 * Registers command classes with the given command factory.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,registerCommands,org.apache.hadoop.fs.shell.Count:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),46,48,"/**
 * Registers the ""count"" command with the given command factory.
 */","* Register the names for the count command
   * @param factory the command factory that will instantiate this class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,registerCommands,org.apache.hadoop.fs.shell.TouchCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),43,46,"/**
 * Registers command classes with the given factory.
 * @param factory CommandFactory instance to register commands.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,registerCommands,org.apache.hadoop.fs.shell.Mkdir:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),39,41,"/**
 * Registers the Mkdir command with the given command factory.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Concat.java,registerCommands,org.apache.hadoop.fs.shell.Concat:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),38,40,"/**
 * Registers the Concat command with the given factory.
 * @param factory CommandFactory instance to register with
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,registerCommands,org.apache.hadoop.fs.shell.CopyCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),44,52,"/**
 * Registers command classes with the provided command factory.
 * @param factory CommandFactory instance to register commands with.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,registerCommands,org.apache.hadoop.fs.shell.MoveCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),35,39,"/**
 * Registers command classes with the given command factory.
 * @param factory CommandFactory instance to register commands.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Stat.java,registerCommands,org.apache.hadoop.fs.shell.Stat:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),53,55,"/**
 * Registers the Stat command with the given CommandFactory.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,registerCommands,org.apache.hadoop.fs.shell.Display:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),62,66,"/**
 * Registers command classes with the given factory.
 * @param factory CommandFactory to register commands with.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,registerCommands,org.apache.hadoop.fs.shell.AclCommands:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),47,50,"/**
 * Registers commands with the given command factory.
 * @param factory CommandFactory to register commands with.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,registerCommands,org.apache.hadoop.fs.shell.Truncate:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),35,37,"/**
 * Registers the Truncate command with the given factory.
 * @param factory CommandFactory instance to register with.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,registerCommands,org.apache.hadoop.fs.shell.SetReplication:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),37,39,"/**
 * Registers the SetReplication command with the given factory.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,<init>,"org.apache.hadoop.fs.shell.CommandFormat:<init>(java.lang.String,int,int,java.lang.String[])",45,48,"/**
 * Constructs a CommandFormat with a name.
 * @param name Command name.
 * @param min Minimum number of arguments.
 * @param max Maximum number of arguments.
 */","* @deprecated use replacement since name is an unused parameter
   * @param name of command, but never used
   * @param min see replacement
   * @param max see replacement
   * @param possibleOpt see replacement
   * @see #CommandFormat(int, int, String...)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpoint,"org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(org.apache.hadoop.fs.Path,boolean)",361,403,"/**
 * Deletes trash checkpoints older than deletionInterval.
 * @param trashRoot Root directory of the trash.
 * @param deleteImmediately Flag to force immediate deletion.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystemPathHandle.java,verify,org.apache.hadoop.fs.LocalFileSystemPathHandle:verify(org.apache.hadoop.fs.FileStatus),54,61,"/**
 * Verifies a FileStatus, throwing an exception on failure.
 * @param stat The FileStatus to verify.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",802,807,"/**
 * Sets the ACL for a given path using the target file system.
 * @param path The path to set the ACL for.
 * @param aclSpec The ACL entries to apply.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setAcl,"org.apache.hadoop.fs.FilterFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",349,352,"/**
* Sets the ACL for a given path using the provided ACL specification.
* @param path the path to set the ACL on
* @param aclSpec the ACL entries to apply
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,startUpload,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:startUpload(org.apache.hadoop.fs.Path),98,110,"/**
 * Starts an upload by creating a collector path and handle.
 * @param filePath Path to the file being uploaded.
 * @return CompletableFuture wrapping the UploadHandle.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,putPart,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:putPart(org.apache.hadoop.fs.UploadHandle,int,org.apache.hadoop.fs.Path,java.io.InputStream,long)",112,122,"/**
 * Uploads a part of a file.
 * @param uploadId Upload handle.
 * @param partNumber Part number.
 * @param filePath Path to the file.
 * @param inputStream Input stream.
 * @param lengthInBytes Length of the part.
 * @return CompletableFuture<PartHandle>
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,complete,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:complete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)",176,185,"/**
 * Completes an upload, writing parts to a file.
 * @param uploadId Upload identifier.
 * @param filePath Path to write to.
 * @param handleMap Part handles.
 * @return CompletableFuture<PathHandle>
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,eval,org.apache.hadoop.fs.impl.FutureIOSupport:eval(org.apache.hadoop.util.functional.CallableRaisingIOE),179,182,"/**
 * Executes a CallableRaisingIOE in a thread and returns a CompletableFuture.
 */","* Evaluate a CallableRaisingIOE in the current thread,
   * converting IOEs to RTEs and propagating.
   * See {@link FutureIO#eval(CallableRaisingIOE)}.
   *
   * @param callable callable to invoke
   * @param <T> Return type.
   * @return the evaluated result.
   * @throws UnsupportedOperationException fail fast if unsupported
   * @throws IllegalArgumentException invalid argument",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,concat,"org.apache.hadoop.fs.FilterFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])",188,191,"/**
* Concatenates paths using the file system.
* @param f target path
* @param psrcs source paths to concatenate
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,rejectUnknownMandatoryKeys,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:rejectUnknownMandatoryKeys(java.util.Collection,java.lang.String)",358,362,"/**
* Rejects unknown mandatory keys, delegating to a private method.
* @param knownKeys Keys known to the system
* @param extraErrorText Additional error text to append
* @throws IllegalArgumentException if unknown keys are found
*/
","* Reject a configuration if one or more mandatory keys are
   * not in the set of mandatory keys.
   * The first invalid key raises the exception; the order of the
   * scan and hence the specific key raising the exception is undefined.
   * @param knownKeys a possibly empty collection of known keys
   * @param extraErrorText extra error text to include.
   * @throws IllegalArgumentException if any key is unknown.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileRangeImpl.java,toString,org.apache.hadoop.fs.impl.FileRangeImpl:toString(),54,58,"/**
 * Returns a string representation of the range.
 * Includes offset, length, and reference.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,pathCapabilities,org.apache.hadoop.fs.impl.FlagSet:pathCapabilities(),209,213,"/**
 * Returns a list of capability names. Filters based on hasCapability.
 */
","* Generate the list of capabilities.
   * @return a possibly empty list.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,buildHttpReferrer,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:buildHttpReferrer(),190,223,"/**
 * Builds an HTTP Referer header string from attributes.
 * Returns an empty string if URI construction fails.
 */","* Build the referrer string.
   * This includes dynamically evaluating all of the evaluated
   * attributes.
   * If there is an error creating the string it will be logged once
   * per entry, and """" returned.
   * @return a referrer string or """"",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,<init>,"org.apache.hadoop.util.WeakReferenceMap:<init>(java.util.function.Function,java.util.function.Consumer)",100,106,"/**
 * Constructs a WeakReferenceMap with a factory and optional listener.
 * @param factory Creates values for keys.
 * @param referenceLost Called when a reference is lost.
 */
","* instantiate.
   * @param factory supplier of new instances
   * @param referenceLost optional callback on lost references.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/StoreImplementationUtils.java,hasCapability,"org.apache.hadoop.fs.impl.StoreImplementationUtils:hasCapability(java.io.OutputStream,java.lang.String)",80,82,"/**
 * Checks if the OutputStream has the specified capability.
 * @param out The OutputStream to check.
 * @param capability Capability string to check for.
 */
","* Probe for an output stream having a capability; returns true
   * if the stream implements {@link StreamCapabilities} and its
   * {@code hasCapabilities()} method returns true for the capability.
   * @param out output stream
   * @param capability capability to probe for
   * @return true if the stream declares that it supports the capability.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/StoreImplementationUtils.java,hasCapability,"org.apache.hadoop.fs.impl.StoreImplementationUtils:hasCapability(java.io.InputStream,java.lang.String)",92,94,"/**
 * Checks if an input stream has a specific capability.
 * @param in InputStream to check
 * @param capability Capability string to check for
 */
","* Probe for an input stream having a capability; returns true
   * if the stream implements {@link StreamCapabilities} and its
   * {@code hasCapabilities()} method returns true for the capability.
   * @param in input stream
   * @param capability capability to probe for
   * @return true if the stream declares that it supports the capability.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/ExecutorServiceFuturePool.java,shutdown,"org.apache.hadoop.fs.impl.prefetch.ExecutorServiceFuturePool:shutdown(org.slf4j.Logger,long,java.util.concurrent.TimeUnit)",81,83,"/**
 * Shuts down the executor gracefully within a timeout.
 * @param logger Logger for logging shutdown events.
 * @param timeout Timeout duration.
 * @param unit Time unit of the timeout.
 */
","* Utility to shutdown the {@link ExecutorService} used by this class. Will wait up to a
   * certain timeout for the ExecutorService to gracefully shutdown.
   *
   * @param logger Logger
   * @param timeout the maximum time to wait
   * @param unit the time unit of the timeout argument",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,<init>,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:<init>(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation),128,131,"/**
 * Constructs an End object with the given operation.
 * @param op The operation associated with this end.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getSummary,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:getSummary(java.lang.StringBuilder),133,137,"/**
 * Appends ""E"" to the summary StringBuilder, then calls super.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getDebugInfo,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:getDebugInfo(),139,142,"/**
 * Retrieves debug info, removing the first three characters.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,add,org.apache.hadoop.fs.impl.prefetch.BlockOperations:add(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation),160,166,"/**
 * Adds an operation to the queue and returns it.
 * @param op The operation to add.
 * @return The added operation.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,canRelease,org.apache.hadoop.fs.impl.prefetch.BufferPool:canRelease(org.apache.hadoop.fs.impl.prefetch.BufferData),318,322,"/**
 * Checks if buffer data can be released based on its state.
 * @param data BufferData object to check.
 * @return True if state is DONE or READY, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,get,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager$PrefetchTask:get(),411,420,"/**
 * Prefetches a block and handles potential exceptions.
 * Returns null.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,distance,"org.apache.hadoop.fs.impl.prefetch.BufferPool:distance(org.apache.hadoop.fs.impl.prefetch.BufferData,int)",226,228,"/**
 * Calculates the absolute distance between block numbers.
 * @param data BufferData object containing block number.
 * @param blockNumber The block number to calculate distance from.
 * @return The absolute distance between the two block numbers.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,find,org.apache.hadoop.fs.impl.prefetch.BufferPool:find(int),305,316,"/**
 * Finds a BufferData with the given block number and non-DONE state.
 * @param blockNumber The block number to search for.
 * @return BufferData object or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,close,org.apache.hadoop.fs.impl.prefetch.BufferPool:close(),257,275,"/**
 * Closes the resource, canceling pending actions and releasing resources.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,acquire,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:acquire(),70,73,"/**
 * Acquires an element. Delegates to acquireHelper with 'true'.
 */",* Acquires a resource blocking if necessary until one becomes available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,tryAcquire,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:tryAcquire(),78,81,"/**
 * Attempts to acquire the resource.
 * Delegates to acquireHelper with 'tryMode' as false.
 */",* Acquires a resource blocking if one is immediately available. Otherwise returns null.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,close,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:close(),113,124,"/**
 * Closes all resources and clears internal data structures.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,numAvailable,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:numAvailable(),148,150,"/**
 * Returns the number of available items.
 * Calculates based on size, created items, and items in the list.
 */
","* Number of items available to be acquired. Mostly for testing purposes.
   * @return the number available.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,duration,org.apache.hadoop.fs.impl.prefetch.BlockOperations$End:duration(),144,146,"/**
 * Calculates the duration in seconds.
 * @return Duration between the timestamp and operation timestamp.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,analyze,org.apache.hadoop.fs.impl.prefetch.BlockOperations:analyze(java.lang.StringBuilder),297,367,"/**
 * Analyzes operation blocks and appends results to the provided StringBuilder.
 * @param sb StringBuilder to append analysis results.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,<init>,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:<init>(org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics,int,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",223,234,"/**
 * Constructs a SingleFilePerBlockCache with given statistics,
 * max blocks, and duration tracker factory.
 */","* Constructs an instance of a {@code SingleFilePerBlockCache}.
   *
   * @param prefetchingStatistics statistics for this stream.
   * @param maxBlocksCount max blocks count to be kept in cache at any time.
   * @param trackerFactory tracker with statistics to update",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,<init>,"org.apache.hadoop.util.SemaphoredDelegatingExecutor:<init>(java.util.concurrent.ExecutorService,int,boolean,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",71,82,"/**
 * Constructs a SemaphoredDelegatingExecutor.
 * @param executorDelegatee Executor to delegate tasks to.
 * @param permitCount Semaphore permit count.
 */
","* Instantiate.
   * @param executorDelegatee Executor to delegate to
   * @param permitCount number of permits into the queue permitted
   * @param fair should the semaphore be ""fair""
   * @param trackerFactory duration tracker factory.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,addToLinkedListHead,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToLinkedListHead(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry),314,321,"/**
 * Adds an entry to the head of the linked list under a write lock.
 * @param entry The entry to add.
 */
","* Helper method to add the given entry to the head of the linked list.
   *
   * @param entry Block entry to add.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,validateEntry,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:validateEntry(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry,java.nio.ByteBuffer)",551,566,"/**
 * Validates an entry's size and checksum against the buffer.
 * @param entry The entry to validate.
 * @param buffer The buffer containing the entry data.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setDone,org.apache.hadoop.fs.impl.prefetch.BufferData:setDone(),223,231,"/**
 * Marks the operation as done, verifying checksum if present.
 */
",* Indicates that this block is no longer of use and can be reclaimed.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,close,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:close(),502,508,"/**
 * Closes the resource, logs stats, and deletes cache files.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,toString,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:toString(),540,549,"/**
 * Returns a string representation of the object, including stats and blocks.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,toString,org.apache.hadoop.fs.impl.prefetch.BufferData:toString(),289,299,"/**
 * Returns a string representation of this object's state.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,throwIfInvalidBuffer,org.apache.hadoop.fs.impl.prefetch.FilePosition:throwIfInvalidBuffer(),298,300,"/**
 * Throws an exception if the buffer is null, ensuring validity.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,bufferSize,org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:bufferSize(int),133,136,"/**
 * Sets the buffer size for the data input stream.
 * @param bufSize The desired buffer size in bytes.
 * @return This builder instance for chaining.
 */
","* Set the size of the buffer to be used.
   *
   * @param bufSize buffer size.
   * @return FutureDataInputStreamBuilder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,builder,org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:builder(),146,148,"/**
 * Returns a builder for constructing FutureDataInputStream.
 */","* Get the builder.
   * This must be used after the constructor has been invoked to create
   * the actual builder: it allows for subclasses to do things after
   * construction.
   *
   * @return FutureDataInputStreamBuilder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,getForCurrentThread,org.apache.hadoop.fs.impl.WeakReferenceThreadMap:getForCurrentThread(),45,47,"/**
 * Retrieves value associated with the current thread's ID.
 * @return Value for the current thread, or null if not found.
 */
","* Get the value for the current thread, creating if needed.
   * @return an instance.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,removeForCurrentThread,org.apache.hadoop.fs.impl.WeakReferenceThreadMap:removeForCurrentThread(),53,55,"/**
 * Removes the entry associated with the current thread.
 * @return The removed value, or null if not found.
 */
","* Remove the reference for the current thread.
   * @return any reference value which existed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,setForCurrentThread,org.apache.hadoop.fs.impl.WeakReferenceThreadMap:setForCurrentThread(java.lang.Object),70,90,"/**
 * Sets a value for the current thread, returning the old value.
 * @param newVal The new value to set.
 * @return The previous value associated with the thread.
 */","* Set the new value for the current thread.
   * @param newVal new reference to set for the active thread.
   * @return the previously set value, possibly null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/CombinedFileRange.java,<init>,"org.apache.hadoop.fs.impl.CombinedFileRange:<init>(long,long,org.apache.hadoop.fs.FileRange)",44,47,"/**
 * Creates a CombinedFileRange with given offset, end, and original range.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/CombinedFileRange.java,merge,"org.apache.hadoop.fs.impl.CombinedFileRange:merge(long,long,org.apache.hadoop.fs.FileRange,int,int)",79,89,"/**
 * Merges another range into this range if conditions are met.
 * @param other The range to merge.
 * @return True if merged, false otherwise.
 */
","* Merge this input range into the current one, if it is compatible.
   * It is assumed that otherOffset is greater or equal the current offset,
   * which typically happens by sorting the input ranges on offset.
   * @param otherOffset the offset to consider merging
   * @param otherEnd the end to consider merging
   * @param other the underlying FileRange to add if we merge
   * @param minSeek the minimum distance that we'll seek without merging the
   *                ranges together
   * @param maxSize the maximum size that we'll merge into a single range
   * @return true if we have merged the range into this one",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createBulkDelete,org.apache.hadoop.fs.FileSystem:createBulkDelete(org.apache.hadoop.fs.Path),5002,5006,"/**
 * Creates a BulkDelete operation for the given path.
 * @param path Path to delete; throws IllegalArgumentException if invalid.
 * @return A new BulkDelete operation.
 */
","* Create a bulk delete operation.
   * The default implementation returns an instance of {@link DefaultBulkDeleteOperation}.
   * @param path base path for the operation.
   * @return an instance of the bulk delete.
   * @throws IllegalArgumentException any argument is invalid.
   * @throws IOException if there is an IO problem.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getBufferSize,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getBufferSize(),64,67,"/**
 * Returns the buffer size, delegating to the superclass.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getReplication,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getReplication(),69,72,"/**
 * Returns the replication factor inherited from the superclass.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getFlags,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getFlags(),74,77,"/**
* Returns the flags associated with this object.
* Delegates to the superclass implementation.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getChecksumOpt,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getChecksumOpt(),79,82,"/**
 * Returns the ChecksumOpt from the superclass.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getBlockSize,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getBlockSize(),84,87,"/**
 * Returns the block size, inheriting from the superclass.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processOptions,org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processOptions(java.util.LinkedList),153,177,"/**
 * Processes command-line options, sets name, value, and xname.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeXAttr,"org.apache.hadoop.fs.FilterFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",656,659,"/**
* Removes an extended attribute from a given path.
* @param path Path to remove attribute from.
* @param name Name of the attribute to remove.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processOptions,org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processOptions(java.util.LinkedList),53,59,"/**
 * Processes command-line options; throws if ""-t"" is present.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/And.java,registerExpression,org.apache.hadoop.fs.shell.find.And:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory),31,35,"/**
 * Registers the And class with the ExpressionFactory.
 * Adds aliases ""-a"" and ""-and"" for the And expression.
 */
",Registers this expression with the specified factory.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,registerExpression,org.apache.hadoop.fs.shell.find.Print:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory),30,34,"/**
 * Registers ExpressionFactory classes for printing.
 * @param factory The ExpressionFactory to register with.
 */
",Registers this expression with the specified factory.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,registerExpression,org.apache.hadoop.fs.shell.find.Name:registerExpression(org.apache.hadoop.fs.shell.find.ExpressionFactory),33,37,"/**
 * Registers ExpressionFactory classes.
 * Adds Name and Iname classes to the factory.
 */
",Registers this expression with the specified factory.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,<init>,org.apache.hadoop.fs.shell.find.Print:<init>(),45,47,"/**
* Constructs a Print object with a newline character as the initial value.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,<init>,org.apache.hadoop.fs.shell.find.Name:<init>(boolean),57,62,"/**
 * Constructs a Name object.
 * @param caseSensitive Flag to determine case sensitivity.
 */
","* Construct a Name {@link Expression} with a specified case sensitivity.
   *
   * @param caseSensitive if true the comparisons are case sensitive.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,apply,"org.apache.hadoop.fs.shell.find.Name:apply(org.apache.hadoop.fs.shell.PathData,int)",82,93,"/**
 * Checks if the path name matches the glob pattern.
 * @param item PathData object
 * @param depth Depth of the path
 * @return PASS if matches, FAIL otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,addCodec,org.apache.hadoop.io.compress.CompressionCodecFactory:addCodec(org.apache.hadoop.io.compress.CompressionCodec),64,75,"/**
 * Adds a codec to the codec registry, indexed by extension, class name, and name.
 * @param codec The codec to add.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodec,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodec(org.apache.hadoop.fs.Path),199,217,"/**
 * Finds a CompressionCodec based on the file's reversed name.
 * @param file The path to the file.
 * @return The matching CompressionCodec or null.
 */
","* Find the relevant compression codec for the given file based on its
   * filename suffix.
   * @param file the filename to check
   * @return the codec object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsConfig:<init>(org.apache.commons.configuration2.Configuration,java.lang.String)",94,96,"/**
 * Constructs a MetricsConfig object.
 * @param c Configuration object.
 * @param prefix Metric prefix.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,isLocalhost,org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:isLocalhost(java.lang.String),491,501,"/**
 * Checks if the given host is a localhost address.
 * @param host The host string to check.
 * @return True if the host is a localhost, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,resolvePropertyName,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:resolvePropertyName(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String)",216,221,"/**
 * Formats a template string using the mode's lowercase string representation.
 */","* Resolves a property name to its client/server version if applicable.
   * <p>
   * NOTE: This method is public for testing purposes.
   *
   * @param mode client/server mode.
   * @param template property name template.
   * @return the resolved property name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,hasCapability,org.apache.hadoop.crypto.CryptoInputStream:hasCapability(java.lang.String),860,880,"/**
 * Checks if the stream has the specified capability.
 * @param capability Capability to check for.
 * @return True if the stream has the capability, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CipherSuite.java,getConfigSuffix,org.apache.hadoop.crypto.CipherSuite:getConfigSuffix(),98,106,"/**
 * Generates a config suffix from the name, converting parts to lowercase.
 */
","* Returns suffix of cipher suite configuration.
   * @return String configuration suffix",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Result.java,combine,org.apache.hadoop.fs.shell.find.Result:combine(org.apache.hadoop.fs.shell.find.Result),59,62,"/**
 * Combines this result with another. Returns a new Result.
 * @param other The other Result to combine with.
 * @return A new Result representing the combined pass/descend status.
 */
","* Returns the combination of this and another result.
   * @param other other.
   * @return result.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Result.java,negate,org.apache.hadoop.fs.shell.find.Result:negate(),68,70,"/**
 * Negates the result: inverts pass status, keeps descend flag.
 */","* Negate this result.
   * @return Result.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Result.java,toString,org.apache.hadoop.fs.shell.find.Result:toString(),72,75,"/**
 * Returns a string representation of the object's state.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,<init>,org.apache.hadoop.fs.shell.find.Name$Iname:<init>(),97,99,"/**
 * Constructs an Iname object, initializing it with a false Name.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,<init>,org.apache.hadoop.fs.shell.find.Print$Print0:<init>(),72,74,"/**
 * Constructs a Print0 object, initializing with a null character.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,createOptions,org.apache.hadoop.fs.shell.find.Find:createOptions(),244,252,"/**
 * Creates and configures FindOptions object for command execution.
 * @return FindOptions object with configured parameters.
 */
",Create a new set of find options.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,isExpression,org.apache.hadoop.fs.shell.find.Find:isExpression(java.lang.String),445,448,"/**
 * Checks if the given expression name is a valid expression.
 * @param expressionName Name of the expression to check.
 * @return True if the expression is valid, false otherwise.
 */
",Asks the factory whether an expression is recognized.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,setOptions,org.apache.hadoop.fs.shell.find.BaseExpression:setOptions(org.apache.hadoop.fs.shell.find.FindOptions),67,73,"/**
 * Sets the options for this object and its children.
 * @param options FindOptions object to set
 * @throws IOException if an I/O error occurs
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,prepare,org.apache.hadoop.fs.shell.find.BaseExpression:prepare(),75,80,"/**
 * Recursively prepares each child expression.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,finish,org.apache.hadoop.fs.shell.find.BaseExpression:finish(),82,87,"/**
 * Recursively finishes processing all child expressions.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,isAction,org.apache.hadoop.fs.shell.find.BaseExpression:isAction(),147,155,"/**
 * Checks if this expression or any child is an action.
 * Returns true if any child is an action, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,toString,org.apache.hadoop.fs.shell.find.BaseExpression:toString(),119,145,"/**
 * Generates a string representation of the object, including args & children.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,addChildren,"org.apache.hadoop.fs.shell.find.BaseExpression:addChildren(java.util.Deque,int)",219,223,"/**
 * Adds multiple children expressions.
 * @param exprs Deque of expressions to add.
 * @param count Number of expressions to add.
 */
","* Add a specific number of children to this expression. The children are
   * popped off the head of the expressions.
   *
   * @param exprs
   *          deque of expressions from which to take the children
   * @param count
   *          number of children to be added",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,addArguments,"org.apache.hadoop.fs.shell.find.BaseExpression:addArguments(java.util.Deque,int)",250,254,"/**
 * Adds arguments to the argument list by popping from the deque.
 * @param args Deque of arguments to add.
 * @param count Number of arguments to add.
 */
","* Add a specific number of arguments to this expression. The children are
   * popped off the head of the expressions.
   *
   * @param args
   *          deque of arguments from which to take the argument
   * @param count
   *          number of children to be added",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,validate,org.apache.hadoop.security.alias.CredentialShell$DeleteCommand:validate(),244,274,"/**
 * Validates the credential deletion process. Returns true if valid.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,validate,org.apache.hadoop.crypto.key.KeyShell$DeleteCommand:validate(),354,381,"/**
 * Validates the configuration, prompting for confirmation if interactive.
 * @return True if validation succeeds, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,confirmForceManual,org.apache.hadoop.ha.HAAdmin:confirmForceManual(),466,479,"/**
* Prompts user to confirm the dangerous --forcemanual flag.
* Returns true if user confirms, false otherwise.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,relativize,"org.apache.hadoop.fs.shell.PathData:relativize(java.net.URI,java.net.URI,boolean)",410,435,"/**
 * Calculates a relative URI path from cwdUri to srcUri.
 * @param cwdUri current working directory URI
 * @param srcUri source URI
 * @param isDir whether srcUri is a directory
 * @return relative URI path string
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,stringToUri,org.apache.hadoop.fs.shell.PathData:stringToUri(java.lang.String),550,591,"/**
 * Converts a string to a URI, handling scheme and authority.
 * @param pathString String representation of the URI
 * @return URI object or throws IllegalArgumentException on error
*/
","Construct a URI from a String with unescaped special characters
   *  that have non-standard semantics. e.g. /, ?, #. A custom parsing
   *  is needed to prevent misbehavior.
   *  @param pathString The input path in string form
   *  @return URI",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setHumanReadable,org.apache.hadoop.fs.shell.FsUsage$Df:setHumanReadable(boolean),69,71,"/**
 * Sets whether the output should be human-readable.
 * @param humanReadable flag to enable/disable human-readable output
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setHumanReadable,org.apache.hadoop.fs.shell.FsUsage$Du:setHumanReadable(boolean),69,71,"/**
 * Sets whether the output should be human-readable.
 * @param humanReadable flag to enable/disable human-readable output
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Df:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder),65,67,"/**
 * Sets the usages table using the provided TableBuilder.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,setUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Du:setUsagesTable(org.apache.hadoop.fs.shell.FsUsage$TableBuilder),65,67,"/**
 * Sets the usages table using a TableBuilder.
 * @param usagesTable The TableBuilder for the usages table.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,getUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Df:getUsagesTable(),61,63,"/**
 * Returns the usages table builder.
 * @return TableBuilder instance for usage data.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,getUsagesTable,org.apache.hadoop.fs.shell.FsUsage$Du:getUsagesTable(),61,63,"/**
 * Returns the usages table builder.
 * @return TableBuilder instance for usage data.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,isSorted,org.apache.hadoop.fs.shell.Ls:isSorted(),248,254,"/**
 * Checks if the list is sorted based on order criteria.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,initialiseOrderComparator,org.apache.hadoop.fs.shell.Ls:initialiseOrderComparator(),374,402,"/**
 * Initialises the order comparator based on time, size, or default.
 */","* Initialise the comparator to be used for sorting files. If multiple options
   * are selected then the order is chosen in the following precedence: -
   * Modification time (or access time if requested) - File size - File name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getAdditionalTokenIssuers,org.apache.hadoop.fs.FileSystem:getAdditionalTokenIssuers(),718,723,"/**
 * Returns additional token issuers, which are child file systems.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,<init>,"org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException:<init>(int,int)",218,220,"/**
* Constructs a NotEnoughArgumentsException with expected and actual counts.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,<init>,"org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException:<init>(int,int)",202,204,"/**
 * Constructs a TooManyArgumentsException with expected and actual counts.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,getMessage,org.apache.hadoop.fs.shell.CommandFormat$NotEnoughArgumentsException:getMessage(),222,225,"/**
* Returns a message indicating insufficient arguments.
* Includes the message from the parent class.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,getMessage,org.apache.hadoop.fs.shell.CommandFormat$TooManyArgumentsException:getMessage(),206,209,"/**
 * Returns a message indicating too many arguments were provided.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processOptions,org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processOptions(java.util.LinkedList),70,100,"/**
 * Processes command-line options from the provided argument list.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getXAttrs,org.apache.hadoop.fs.FilterFileSystem:getXAttrs(org.apache.hadoop.fs.Path),640,643,"/**
 * Retrieves extended attributes for a given path.
 * @param path The path to retrieve attributes for.
 * @return Map of attribute names and values.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getXAttr,"org.apache.hadoop.fs.FilterFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",635,638,"/**
 * Retrieves extended attribute for a given path.
 * @param path Path to retrieve the attribute from.
 * @param name Attribute name.
 * @return Byte array representing the attribute value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,popPreserveOption,org.apache.hadoop.fs.shell.CopyCommands$Cp:popPreserveOption(java.util.List),191,210,"/**
 * Parses arguments to set preserve options.
 * Removes ""-p"" arguments and sets preserve flags accordingly.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processArguments,org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processArguments(java.util.LinkedList),159,171,"/**
 * Renames a snapshot.
 * @param items List of PathData objects (must contain one item).
 * @throws IOException if rename fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.FilterFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",579,583,"/**
 * Renames a snapshot within a specified path.
 * @param path Path containing the snapshot.
 * @param snapshotOldName Old snapshot name.
 * @param snapshotNewName New snapshot name.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,isDeprecated,org.apache.hadoop.fs.shell.Command:isDeprecated(),557,559,"/**
 * Checks if the command is deprecated.
 * Returns true if a replacement command is defined.
 */
","* Is the command deprecated?
   * @return boolean",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,getName,org.apache.hadoop.fs.shell.Command:getName(),519,523,"/**
 * Returns the name, stripping ""-"" if present.
 * Returns command name if name is null.
 */
","* The name of the command.  Will first try to use the assigned name
   * else fallback to the command's preferred name
   * @return name of the command",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getAclStatus,org.apache.hadoop.fs.FilterFileSystem:getAclStatus(org.apache.hadoop.fs.Path),618,621,"/**
 * Gets the ACL status for the given path.
 * @param path The path to check.
 * @return The ACL status.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,setPreserve,org.apache.hadoop.fs.shell.CommandWithDestination:setPreserve(boolean),125,133,"/**
 * Sets whether to preserve file attributes.
 * @param preserve True to preserve, false to clear preservation status.
 */
","* If true, the last modified time, last access time,
   * owner, group and permission information of the source
   * file will be preserved as far as target {@link FileSystem}
   * implementation allows.
   *
   * @param preserve preserve.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setAcl,"org.apache.hadoop.fs.FilterFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",613,616,"/**
 * Sets the ACL for the given path using the provided ACL specification.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,<init>,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:<init>(java.lang.String,java.lang.String)",41,45,"/**
 * Constructs an MBeanInfoBuilder with a name and description.
 * @param name MBean name.
 * @param desc MBean description.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:<init>(org.apache.hadoop.metrics2.MetricsCollector,org.apache.hadoop.metrics2.MetricsInfo,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,boolean)",58,68,"/**
 * Constructs a MetricsRecordBuilderImpl with given parameters.
 * @param parent MetricsCollector parent, info, filters, acceptable
 */
","* @param parent {@link MetricsCollector} using this record builder
   * @param info metrics information
   * @param rf
   * @param mf
   * @param acceptable",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,<init>,"org.apache.hadoop.util.ChunkedArrayList:<init>(int,int)",103,107,"/**
 * Constructs a ChunkedArrayList with initial capacity and max size.
 * @param initialChunkCapacity Initial chunk capacity.
 * @param maxChunkSize Maximum chunk size.
 */
","* @param initialChunkCapacity the capacity of the first chunk to be
   * allocated
   * @param maxChunkSize the maximum size of any chunk allocated",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ScopedAclEntries.java,calculatePivotOnDefaultEntries,org.apache.hadoop.fs.permission.ScopedAclEntries:calculatePivotOnDefaultEntries(java.util.List),87,94,"/**
 * Finds the index of the first default AclEntry.
 * @param aclBuilder List of AclEntry objects.
 * @return Index of the default entry, or PIVOT_NOT_FOUND.
 */
","* Returns the pivot point in the list between the access entries and the
   * default entries.  This is the index of the first element in the list that
   * is a default entry.
   *
   * @param aclBuilder ArrayList<AclEntry> containing entries to build
   * @return int pivot point, or -1 if list contains no default entries",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeAcl,org.apache.hadoop.fs.FilterFileSystem:removeAcl(org.apache.hadoop.fs.Path),608,611,"/**
 * Removes ACL entries for the given path.
 * @param path The path for which to remove ACLs.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.FilterFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",591,595,"/**
* Modifies ACL entries for a given path.
* @param path Path to modify.
* @param aclSpec List of ACL entries to apply.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.FilterFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",597,601,"/**
 * Removes ACL entries from a path.
 * @param path The path to remove ACL entries from.
 * @param aclSpec List of ACL entries to remove.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processArguments,org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processArguments(java.util.LinkedList),77,88,"/**
 * Processes path data, creates a snapshot if no errors occurred.
 * @param items LinkedList of PathData objects to process.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createSnapshot,org.apache.hadoop.fs.FileSystem:createSnapshot(org.apache.hadoop.fs.Path),3089,3091,"/**
 * Creates a snapshot of the given path.
 * @param path Path to snapshot.
 * @return Path to the created snapshot.
 */
","* Create a snapshot with a default name.
   * @param path The directory where snapshots will be taken.
   * @return the snapshot path.
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createSnapshot,"org.apache.hadoop.fs.FilterFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",573,577,"/**
 * Creates a snapshot of the given path with the specified name.
 * @param path Path to snapshot.
 * @param snapshotName Snapshot name.
 * @return The created snapshot path.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,addOptionWithValue,org.apache.hadoop.fs.shell.CommandFormat:addOptionWithValue(java.lang.String),73,78,"/**
 * Adds an option to the optionsWithValue map.
 * Throws exception if option already exists.
 */
","* add option with value
   *
   * @param option option name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processArguments,org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processArguments(java.util.LinkedList),117,128,"/**
 * Deletes a snapshot using the first PathData item.
 * Returns if errors occurred during path collection.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.FilterFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",585,589,"/**
 * Deletes a snapshot from the specified path.
 * @param path Path to the directory containing the snapshot.
 * @param snapshotName Name of the snapshot to delete.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,<init>,org.apache.hadoop.fs.shell.FsUsage$TableBuilder:<init>(java.lang.Object[]),274,278,"/**
 * Creates a table with the given headers as the first row.
 * @param headers Array of objects to use as table headers.
 */
","* Create a table with headers
     * @param headers list of headers",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,isEmpty,org.apache.hadoop.fs.shell.FsUsage$TableBuilder:isEmpty(),348,350,"/**
 * Checks if the collection is empty.
 * @return True if the collection is empty, false otherwise.
 */
","* Does table have any rows 
     * @return boolean",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFs:getXAttrs(org.apache.hadoop.fs.Path),831,836,"/**
 * Retrieves extended attributes for a given path.
 * @param path The path to retrieve attributes for.
 * @return Map of attribute names and values.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getXAttrs,org.apache.hadoop.fs.FilterFs:getXAttrs(org.apache.hadoop.fs.Path),376,379,"/**
 * Retrieves extended attributes for a given path.
 * @param path The path to retrieve attributes for.
 * @return Map of attribute names and values.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,<init>,org.apache.hadoop.fs.Options$HandleOpt$Location:<init>(boolean),496,498,"/**
 * Constructs a Location object.
 * @param allowChanged Whether to allow location changes.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,<init>,org.apache.hadoop.fs.Options$HandleOpt$Data:<init>(boolean),471,473,"/**
 * Constructs a Data object.
 * @param allowChanged Flag to indicate if changes are allowed.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CompositeCrcFileChecksum.java,toString,org.apache.hadoop.fs.CompositeCrcFileChecksum:toString(),84,87,"/**
 * Returns a string representation of the CRC object.
 * Includes algorithm name and CRC value in hexadecimal.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell:<init>(long),910,912,"/**
 * Constructs a Shell with a given interval.
 * @param interval The interval for the shell.
 */
","* Create an instance with a minimum interval between executions; stderr is
   * not merged with stdout.
   * @param interval interval in milliseconds between command executions.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawPathHandle.java,equals,org.apache.hadoop.fs.RawPathHandle:equals(java.lang.Object),72,79,"/**
 * Checks if two PathHandle objects are equal by comparing their bytes.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawPathHandle.java,hashCode,org.apache.hadoop.fs.RawPathHandle:hashCode(),81,84,"/**
 * Calculates hash code based on the underlying byte array.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawPathHandle.java,toString,org.apache.hadoop.fs.RawPathHandle:toString(),86,89,"/**
 * Returns a string representation of the byte array.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,run,org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:run(),209,236,"/**
 * Continuously refreshes disk space usage with jittered intervals.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,setOwner,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setOwner(org.apache.hadoop.io.Text),93,99,"/**
 * Sets the owner of the object. Creates a new Text if null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,setRealUser,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setRealUser(org.apache.hadoop.io.Text),122,128,"/**
 * Sets the realUser. If null, initializes to a new Text object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,"org.apache.hadoop.security.token.Token:<init>(byte[],byte[],org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",86,91,"/**
 * Constructs a Token object.
 * @param identifier Identifier bytes.
 * @param password Password bytes.
 * @param kind Text kind.
 * @param service Text service.
 */
","* Construct a token from the components.
   * @param identifier the token identifier
   * @param password the token's password
   * @param kind the kind of token
   * @param service the service for this token",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,org.apache.hadoop.security.token.Token:<init>(),96,101,"/**
 * Default constructor. Initializes token fields to empty byte arrays.
 */",* Default constructor.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",65,72,"/**
 * Constructs a Globber with provided context, pattern, and filter.
 * @param fc FileContext, Path pattern, filter to apply.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter,boolean)",100,110,"/**
 * Constructs a Globber with a file context, path pattern, filter,
 * and symlink resolution flag.
 */
","* File Context constructor for use by {@link GlobBuilder}.
   * @param fc file context
   * @param pathPattern path pattern
   * @param filter optional filter
   * @param resolveSymlinks should symlinks be resolved.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,getMessage,org.apache.hadoop.fs.PathIOException:getMessage(),86,104,"/**
 * Constructs a detailed error message including operation, path, and cause.
 * @return A string representation of the error message.
 */
","Format:
   * cmd: {operation} `path' {to `target'}: error string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/UnionStorageStatistics.java,hasNext,org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:hasNext(),49,52,"/**
 * Checks if there's a next element in the iteration.
 * @return True if an element exists, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/UnionStorageStatistics.java,next,org.apache.hadoop.fs.UnionStorageStatistics$LongStatisticIterator:next(),64,71,"/**
 * Returns the next element in the iteration.
 * Throws NoSuchElementException if no next element.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getAndIncrDirNumLastAccessed,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$Context:getAndIncrDirNumLastAccessed(),282,284,"/**
 * Gets and increments the number of last accessed directories.
 * @return The current count.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",860,866,"/**
 * Creates a snapshot of a file system at a given path.
 * @param path The path to snapshot.
 * @param snapshotName Snapshot name.
 * @return Path to the created snapshot.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createSnapshot,"org.apache.hadoop.fs.FilterFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",397,401,"/**
 * Creates a snapshot of the given path with the given name.
 * @param path Path to snapshot.
 * @param snapshotName Name of the snapshot.
 * @return Path to the created snapshot.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAbstractFileSystem,"org.apache.hadoop.fs.FileContext:getAbstractFileSystem(org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration)",339,363,"/**
 * Gets an AbstractFileSystem for a URI, executing as the given user.
 * @param user UserGroupInformation to execute as.
 * @param uri URI of the filesystem.
 * @param conf Hadoop configuration.
 * @return AbstractFileSystem object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsUser,"org.apache.hadoop.security.SecurityUtil:doAsUser(org.apache.hadoop.security.UserGroupInformation,java.security.PrivilegedExceptionAction)",552,559,"/**
 * Executes an action as the specified user, handling InterruptedExceptions.
 * @param ugi UserGroupInformation to impersonate.
 * @param action Action to execute.
 * @return Result of the action.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,handleSaslConnectionFailure,"org.apache.hadoop.ipc.Client$Connection:handleSaslConnectionFailure(int,int,java.io.IOException,java.util.Random,org.apache.hadoop.security.UserGroupInformation)",705,757,"/**
 * Handles SASL connection failure, retries if possible, or throws.
 * @param currRetries Current retry count.
 * @param maxRetries Max retry count.
 */
","* If multiple clients with the same principal try to connect to the same
     * server at the same time, the server assumes a replay attack is in
     * progress. This is a feature of kerberos. In order to work around this,
     * what is done is that the client backs off randomly and tries to initiate
     * the connection again. The other problem is to do with ticket expiry. To
     * handle that, a relogin is attempted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.java,isViewFileSystem,org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:isViewFileSystem(org.apache.hadoop.fs.FileSystem),50,52,"/**
 * Checks if the given file system is a ViewFS.
 * @param fileSystem The file system to check.
 * @return True if ViewFS, false otherwise.
 */
","* Check if the FileSystem is a ViewFileSystem.
   *
   * @param fileSystem file system.
   * @return true if the fileSystem is ViewFileSystem",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,open,"org.apache.hadoop.fs.FilterFileSystem:open(org.apache.hadoop.fs.PathHandle,int)",171,175,"/**
 * Opens a file using the underlying file system.
 * @param fd PathHandle representing the file.
 * @param bufferSize Buffer size for I/O operations.
 * @return FSDataInputStream for reading the file.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,primitiveMkdir,"org.apache.hadoop.fs.FilterFileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",561,566,"/**
 * Creates a directory using the underlying filesystem.
 * @param f The path of the directory to create.
 * @param abdolutePermission Permissions for the new directory.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setQuota,"org.apache.hadoop.fs.FileSystem:setQuota(org.apache.hadoop.fs.Path,long,long)",1965,1968,"/**
 * Sets quota for a given path. Throws UnsupportedOperationException.
 */","* Set quota for the given {@link Path}.
   *
   * @param src the target path to set quota for
   * @param namespaceQuota the namespace quota (i.e., # of files/directories)
   *                       to set
   * @param storagespaceQuota the storage space quota to set
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setQuotaByStorageType,"org.apache.hadoop.fs.FileSystem:setQuotaByStorageType(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.StorageType,long)",1978,1981,"/**
* Sets quota for a storage type. Currently a no-op.
* @param src Path to the storage.
* @param type Storage type.
* @param quota Quota value.
* @throws IOException if an I/O error occurs (unlikely).
*/
","* Set per storage type quota for the given {@link Path}.
   *
   * @param src the target path to set storage type quota for
   * @param type the storage type to set
   * @param quota the quota to set for the given storage type
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createMultipartUploader,org.apache.hadoop.fs.FileSystem:createMultipartUploader(org.apache.hadoop.fs.Path),4987,4992,"/**
 * Creates a MultipartUploaderBuilder. Currently throws an exception.
 */","* Create a multipart uploader.
   * @param basePath file path under which all files are uploaded
   * @return a MultipartUploaderBuilder object to build the uploader
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listCorruptFileBlocks,org.apache.hadoop.fs.FilterFileSystem:listCorruptFileBlocks(org.apache.hadoop.fs.Path),277,281,"/**
 * Lists corrupt file blocks for a given path.
 * @param path Path to check for corrupt blocks.
 * @return Iterator of Path objects.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listLocatedStatus,org.apache.hadoop.fs.FileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),2261,2264,"/**
 * Lists located file statuses for a path.
 * @param f the path to list
 * @return RemoteIterator of LocatedFileStatus objects
 */
","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   * Return the file's status and block locations If the path is a file.
   *
   * If a returned status is a file, it contains the file's block locations.
   *
   * @param f is the path
   *
   * @return an iterator that traverses statuses of the files/directories
   *         in the given path
   *
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws IOException If an I/O error occurred",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,listLocatedStatus,org.apache.hadoop.fs.ChecksumFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),980,984,"/**
 * Lists located file statuses for a path, using the default filter.
 * @param f The path to list statuses for.
 * @return A RemoteIterator of LocatedFileStatus objects.
 */
","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   *
   * @param f
   *          given path
   * @return the statuses of the files/directories in the given patch
   * @throws IOException if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listLocatedStatus,"org.apache.hadoop.fs.FilterFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",214,219,"/**
 * Lists located status for a path, filtered by the given filter.
 * @param f path to list
 * @param filter filter to apply
 * @return RemoteIterator of LocatedFileStatus objects
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFileSystem:readOnlyMountTable(java.lang.String,java.lang.String)",97,102,"/**
 * Creates an AccessControlException for readonly mount table access.
 * @param operation The operation attempted.
 * @param p The path involved.
 * @return An AccessControlException.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFs:readOnlyMountTable(java.lang.String,java.lang.String)",175,180,"/**
 * Creates an AccessControlException for readonly ViewFileSystem operations.
 * @param operation The operation attempted.
 * @param p The path where the operation failed.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,<init>,org.apache.hadoop.security.authorize.AuthorizationException:<init>(java.lang.String),41,43,"/**
 * Constructs an AuthorizationException with the given message.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,resolveLink,org.apache.hadoop.fs.FilterFileSystem:resolveLink(org.apache.hadoop.fs.Path),498,500,"/**
 * Resolves a symbolic link to its target path.
 * @param f Path to the symbolic link.
 * @return The resolved path.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileChecksum,org.apache.hadoop.fs.FileSystem:getFileChecksum(org.apache.hadoop.fs.Path),2980,2982,"/**
* Gets the checksum of a file.
* @param f Path to the file.
* @return FileChecksum object.
*/
","* Get the checksum of a file, if the FS supports checksums.
   *
   * @param f The file path
   * @return The file checksum.  The default return value is null,
   *  which indicates that no checksum algorithm is implemented
   *  in the corresponding FileSystem.
   * @throws IOException IO failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileChecksum,"org.apache.hadoop.fs.FilterFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",507,510,"/**
 * Gets the checksum for the file at the given path and length.
 * @param f Path to the file
 * @param length File length
 * @return FileChecksum object
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setXAttr,"org.apache.hadoop.fs.FileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",3244,3248,"/**
* Sets an extended attribute on a path.
* @param path Path to set the attribute on.
* @param name Attribute name.
* @param value Attribute value.
*/
","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported
   *         (default outcome).",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setXAttr,"org.apache.hadoop.fs.FilterFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",629,633,"/**
 * Sets an extended attribute on a path.
 * @param path Path to set the attribute on.
 * @param name Attribute name.
 * @param value Attribute value.
 * @param flag Set of flags for the operation.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getXAttrs,"org.apache.hadoop.fs.FilterFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",645,649,"/**
 * Retrieves extended attributes for a given path.
 * @param path Path to retrieve attributes from.
 * @param names Attribute names to retrieve.
 * @return Map of attribute names to byte array values.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listXAttrs,org.apache.hadoop.fs.FilterFileSystem:listXAttrs(org.apache.hadoop.fs.Path),651,654,"/**
 * Lists extended attributes for a given path.
 * @param path Path to list attributes for.
 * @return List of extended attribute names.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.FilterFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path),661,664,"/**
* Satisfies the storage policy for the given path.
* @param src The path to satisfy the storage policy on.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.FilterFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",666,670,"/**
 * Sets the storage policy for a given path.
 * @param src Path to set the policy on.
 * @param policyName Name of the storage policy.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.FilterFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path),672,675,"/**
 * Removes the storage policy from a given path.
 * @param src The path from which to remove the policy.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.FilterFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path),677,681,"/**
 * Gets the storage policy for a given path.
 * @param src The path to check.
 * @return The storage policy.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAllStoragePolicies,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAllStoragePolicies(),1925,1939,"/**
 * Retrieves all BlockStoragePolicySpi instances from child filesystems.
 * Returns a Collection of BlockStoragePolicySpi objects.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getAllStoragePolicies,org.apache.hadoop.fs.FilterFileSystem:getAllStoragePolicies(),683,687,"/**
 * Retrieves all storage policies.
 * @return Collection of BlockStoragePolicySpi implementations.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",4806,4816,"/**
 * Opens a file with specified options.
 * @param path Path to the file.
 * @param parameters OpenFileParameters object.
 * @return CompletableFuture with FSDataInputStream.
 */
","* Execute the actual open file operation.
   *
   * This is invoked from {@code FSDataInputStreamBuilder.build()}
   * and from {@link DelegateToFileSystem} and is where
   * the action of opening the file should begin.
   *
   * The base implementation performs a blocking
   * call to {@link #open(Path, int)} in this call;
   * the actual outcome is in the returned {@code CompletableFuture}.
   * This avoids having to create some thread pool, while still
   * setting up the expectation that the {@code get()} call
   * is needed to evaluate the result.
   * @param path path to the file
   * @param parameters open file parameters from the builder.
   * @return a future which will evaluate to the opened file.
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.ChecksumFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",1090,1101,"/**
 * Opens a file with specified options.
 * @param path Path to the file.
 * @param parameters OpenFile parameters.
 * @return CompletableFuture with FSDataInputStream.
 */
","* Open the file as a blocking call to {@link #open(Path, int)}.
   *
   * {@inheritDoc}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.AbstractFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",1603,1612,"/**
 * Opens a file with specified options.
 * @param path Path to the file.
 * @param parameters OpenFileParameters object.
 * @return CompletableFuture with FSDataInputStream.
 */
","* Open a file with the given set of options.
   * The base implementation performs a blocking
   * call to {@link #open(Path, int)}in this call;
   * the actual outcome is in the returned {@code CompletableFuture}.
   * This avoids having to create some thread pool, while still
   * setting up the expectation that the {@code get()} call
   * is needed to evaluate the result.
   * @param path path to the file
   * @param parameters open file parameters from the builder.
   * @return a future which will evaluate to the opened file.
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FileSystem:openFileWithOptions(org.apache.hadoop.fs.PathHandle,org.apache.hadoop.fs.impl.OpenFileParameters)",4834,4852,"/**
* Opens a file with specified options and buffer size.
* @param pathHandle Path to the file.
* @param parameters OpenFileParameters object.
* @return CompletableFuture wrapping FSDataInputStream.
*/
","* Execute the actual open file operation.
   * The base implementation performs a blocking
   * call to {@link #open(Path, int)} in this call;
   * the actual outcome is in the returned {@code CompletableFuture}.
   * This avoids having to create some thread pool, while still
   * setting up the expectation that the {@code get()} call
   * is needed to evaluate the result.
   * @param pathHandle path to the file
   * @param parameters open file parameters from the builder.
   * @return a future which will evaluate to the opened file.
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key
   * @throws UnsupportedOperationException PathHandles are not supported.
   * This may be deferred until the future is evaluated.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,isValidName,org.apache.hadoop.fs.FilterFs:isValidName(java.lang.String),322,325,"/**
 * Checks if the given name is valid according to FileSystem.
 * @param src The name to validate.
 * @return True if valid, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",770,776,"/**
 * Modifies ACL entries for a given path.
 * @param path The path to modify.
 * @param aclSpec List of ACL entries to apply.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,modifyAclEntries,"org.apache.hadoop.fs.FilterFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",327,331,"/**
* Modifies ACL entries for a given path using the file system.
* @param path The path to modify ACL entries for.
* @param aclSpec List of ACL entries to apply.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",778,784,"/**
 * Removes ACL entries from a path.
 * @param path Path to remove entries from.
 * @param aclSpec List of ACL entries to remove.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeAclEntries,"org.apache.hadoop.fs.FilterFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",333,337,"/**
 * Removes ACL entries from a given path.
 * @param path The path to remove ACL entries from.
 * @param aclSpec List of ACL entries to remove.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path),786,792,"/**
 * Removes the default ACL for a given path.
 * @param path The path for which to remove the default ACL.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,removeDefaultAcl,org.apache.hadoop.fs.FilterFs:removeDefaultAcl(org.apache.hadoop.fs.Path),339,342,"/**
 * Removes the default ACL for the given path.
 * @param path The path for which to remove the default ACL.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",868,875,"/**
 * Renames a snapshot.
 * @param path Path to snapshot.
 * @param snapshotOldName Old snapshot name.
 * @param snapshotNewName New snapshot name.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,renameSnapshot,"org.apache.hadoop.fs.FilterFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",403,407,"/**
* Renames a snapshot.
* @param path Snapshot path.
* @param snapshotOldName Old snapshot name.
* @param snapshotNewName New snapshot name.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),401,404,"/**
* Satisfies the storage policy for the given path.
* @param path The path to satisfy the storage policy on.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),884,889,"/**
 * Applies storage policy to a path within the file system.
 * @param path The path to apply the policy to.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.FilterFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),415,418,"/**
 * Satisfies the storage policy for the given path.
 * @param path The path to satisfy the storage policy on.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),899,905,"/**
 * Removes storage policy for a path in the filesystem.
 * @param src Path for which to remove the storage policy.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,unsetStoragePolicy,org.apache.hadoop.fs.FilterFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),426,430,"/**
* Removes the storage policy from a given path.
* @param src The path from which to remove the policy.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getAllStoragePolicies,org.apache.hadoop.fs.viewfs.ChRootedFs:getAllStoragePolicies(),424,428,"/**
 * Retrieves all BlockStoragePolicySpi instances.
 * Delegates to the underlying FileSystem implementation.
 * @return Collection of BlockStoragePolicySpi instances
 * @throws IOException if an I/O error occurs
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAllStoragePolicies,org.apache.hadoop.fs.FileContext:getAllStoragePolicies(),2916,2919,"/**
 * Retrieves all available BlockStoragePolicySpis.
 * @return Collection of BlockStoragePolicySpi instances.
 */
","* Retrieve all the storage policies supported by this file system.
   *
   * @return all storage policies supported by this filesystem.
   * @throws IOException If an I/O error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getAllStoragePolicies,org.apache.hadoop.fs.FilterFs:getAllStoragePolicies(),438,442,"/**
 * Retrieves all available BlockStoragePolicySpi instances.
 * Delegates to the underlying FileSystem implementation.
 * @return A collection of BlockStoragePolicySpi objects.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,supportsSymlinks,org.apache.hadoop.fs.viewfs.ChRootedFs:supportsSymlinks(),436,439,"/**
 * Checks if the filesystem supports symbolic links.
 * Delegates to the underlying Filesystem object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,supportsSymlinks,org.apache.hadoop.fs.FilterFs:supportsSymlinks(),296,299,"/**
 * Checks if the file system supports symbolic links.
 * Delegates to the underlying FileSystem object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createSymlink,"org.apache.hadoop.fs.FilterFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",301,305,"/**
 * Creates a symbolic link.
 * @param target The target Path.
 * @param link The link Path.
 * @param createParent Whether to create parent directories.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ViewFs:getLinkTarget(org.apache.hadoop.fs.Path),669,674,"/**
 * Resolves the target path of a symbolic link.
 * @param f Path object representing the link
 * @return Path object of the link target
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getLinkTarget,org.apache.hadoop.fs.FilterFs:getLinkTarget(org.apache.hadoop.fs.Path),307,310,"/**
 * Gets the target of a symbolic link.
 * @param f Path to the symbolic link
 * @return Path to the link target
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getDelegationTokens,org.apache.hadoop.fs.viewfs.ChRootedFs:getDelegationTokens(java.lang.String),459,462,"/**
 * Gets delegation tokens for the given renewer.
 * @param renewer renewer's name.
 * @return List of delegation tokens.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getDelegationTokens,org.apache.hadoop.fs.FilterFs:getDelegationTokens(java.lang.String),317,320,"/**
 * Gets delegation tokens for a given renewer.
 * @param renewer The renewer identifier.
 * @return List of delegation tokens.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileChecksum(org.apache.hadoop.fs.Path),1073,1078,"/**
 * Throws FileNotFoundException if path is a directory.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,open,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path,int)",1301,1306,"/**
 * Opens a file for input. Throws exception if path is a directory.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,initializeMountedFileSystems,org.apache.hadoop.fs.viewfs.ViewFileSystem:initializeMountedFileSystems(java.util.List),943,959,"/**
 * Initializes a map of FileSystem objects from mount points.
 * @param mountPoints list of mount points to initialize
 * @return Map of mount paths to FileSystem objects
 */
","* Initialize the target filesystem for all mount points.
   * @param mountPoints The mount points
   * @return Mapping of mount point and the initialized target filesystems
   * @throws RuntimeException when the target file system cannot be initialized",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getDelegationTokens,org.apache.hadoop.fs.viewfs.ViewFs:getDelegationTokens(java.lang.String),732,761,"/**
 * Retrieves delegation tokens for a renewer from all file systems.
 * @param renewer The renewer identifier.
 * @return List of delegation tokens.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1422,1425,"/**
 * Gets extended attribute by name. Throws exception if not supported.
 * @param path Path to file. @param name Attribute name.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path),1427,1430,"/**
 * Throws NotInMountpointException for getXAttrs calls.
 * @param path The path for which xattrs are requested.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",1432,1436,"/**
 * Throws NotInMountpointException for getXAttrs calls.
 * @param path The path being accessed.
 * @param names List of attribute names (unused).
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listXAttrs(org.apache.hadoop.fs.Path),1438,1441,"/**
 * Lists extended attributes for a path.
 * @param path The path to list attributes for.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getServerDefaults(org.apache.hadoop.fs.Path),1785,1788,"/**
 * Throws NotInMountpointException when called.
 * No actual default server settings are returned.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultBlockSize(org.apache.hadoop.fs.Path),1790,1793,"/**
 * Returns default block size; throws exception if not a mountpoint.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getDefaultReplication(org.apache.hadoop.fs.Path),1795,1798,"/**
 * Returns default replication factor, throws exception if not mounted.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1848,1851,"/**
 * Gets extended attribute by name. Throws exception if not supported.
 * @param path Path to file. @param name Attribute name.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path),1853,1856,"/**
 * Returns xattrs for a path. Throws NotInMountpointException.
 * @param path the path to get xattrs for
 * @throws IOException if an I/O error occurs
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",1858,1862,"/**
 * Throws NotInMountpointException for getXAttrs calls.
 * @param path The path being accessed.
 * @param names List of attribute names (unused).
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listXAttrs(org.apache.hadoop.fs.Path),1864,1867,"/**
 * Lists extended attributes for a path.
 * @param path The path to list attributes for.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getQuotaUsage,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getQuotaUsage(org.apache.hadoop.fs.Path),1896,1899,"/**
* Returns quota usage for a path; throws NotInMountpointException.
* @param f the path to check
* @throws IOException if an I/O error occurs
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStoragePolicy(org.apache.hadoop.fs.Path),1920,1923,"/**
* Returns storage policy for a path.
* Throws NotInMountpointException if path is not in a mount point.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java,serializeToString,org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:serializeToString(),111,116,"/**
 * Serializes configuration data to a string.
 * Concatenates config name, src regex, and replace string.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize(),961,964,"/**
 * Returns the default block size. Throws exception if not supported.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultReplication(),966,969,"/**
 * Returns default replication factor. Throws exception if not configured.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults(),971,974,"/**
 * Throws NotInMountpointException, getServerDefaults not supported.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,delete,"org.apache.hadoop.fs.viewfs.ViewFs:delete(org.apache.hadoop.fs.Path,boolean)",366,378,"/**
 * Deletes a file or directory.
 * @param f The Path to delete.
 * @param recursive If true, deletes recursively.
 * @return True if successful.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatusIterator,org.apache.hadoop.fs.viewfs.ViewFs:listStatusIterator(org.apache.hadoop.fs.Path),451,469,"/**
 * Lists status iterator for a given path.
 * @param f the path to list
 * @return RemoteIterator of FileStatus objects
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.ViewFs:listLocatedStatus(org.apache.hadoop.fs.Path),471,490,"/**
 * Lists located status for a path.
 * @param f path to list
 * @return RemoteIterator of LocatedFileStatus objects
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,buildResolveResultForRegexMountPoint,"org.apache.hadoop.fs.viewfs.InodeTree:buildResolveResultForRegexMountPoint(org.apache.hadoop.fs.viewfs.InodeTree$ResultKind,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)",1053,1081,"/**
 * Builds a ResolveResult for a regex mount point.
 * @param resultKind Result kind.
 * @param resolvedPathStr Resolved path string.
 * @param targetOfResolvedPathStr Target file system URI.
 * @param remainingPath Remaining path.
 * @return ResolveResult object or null on error.
 */
","* Build resolve result.
   * Here's an example
   * Mountpoint: fs.viewfs.mounttable.mt
   *     .linkRegex.replaceresolveddstpath:_:-#.^/user/(??&lt;username&gt;\w+)
   * Value: /targetTestRoot/$username
   * Dir path to test:
   * viewfs://mt/user/hadoop_user1/hadoop_dir1
   * Expect path: /targetTestRoot/hadoop-user1/hadoop_dir1
   * resolvedPathStr: /user/hadoop_user1
   * targetOfResolvedPathStr: /targetTestRoot/hadoop-user1
   * remainingPath: /hadoop_dir1
   *
   * @param resultKind resultKind.
   * @param resolvedPathStr resolvedPathStr.
   * @param targetOfResolvedPathStr targetOfResolvedPathStr.
   * @param remainingPath remainingPath.
   * @return targetFileSystem or null on exceptions.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,fsGetter,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:fsGetter(),216,219,"/**
 * Creates and returns a ChildFsGetter instance.
 * Uses the scheme of the parent FsObject.
 */
","* This method is overridden because in ViewFileSystemOverloadScheme if
   * overloaded scheme matches with mounted target fs scheme, file system
   * should be created without going into {@literal fs.<scheme>.impl} based
   * resolution. Otherwise it will end up in an infinite loop as the target
   * will be resolved again to ViewFileSystemOverloadScheme as
   * {@literal fs.<scheme>.impl} points to ViewFileSystemOverloadScheme.
   * So, below method will initialize the
   * {@literal fs.viewfs.overload.scheme.target.<scheme>.impl}.
   * Other schemes can follow fs.newInstance",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,getBlockLocations,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:getBlockLocations(),112,115,"/**
 * Returns the block locations from the underlying file system.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",167,169,"/**
 * Constructs a directory node with the given path and user group info.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.Object,java.lang.String[])",344,349,"/**
 * Constructs a NodeLink with provided path, UGI, target FS, and links.
 */",* Construct a mergeLink or nfly.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.util.function.Function,java.lang.String)",354,362,"/**
 * Initializes a NodeLink with a path, UGI, and file system creation method.
 * @param pathToNode Node path
 * @param aUgi UserGroupInformation
 * @param createFileSystemMethod Method to create file system
 * @param aTargetDirLink Target directory link
 */
",* Construct a simple link (i.e. not a mergeLink).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,addLink,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addLink(java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)",222,228,"/**
 * Adds a link to the children map.
 * @param pathComponent Link path component.
 * @param link The link to add.
 * @throws FileAlreadyExistsException if path already exists.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,buildLinkRegexEntry,"org.apache.hadoop.fs.viewfs.InodeTree:buildLinkRegexEntry(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)",818,845,"/**
 * Builds a LinkEntry from mount point data, extracting settings & key.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,processThrowable,"org.apache.hadoop.fs.viewfs.NflyFSystem:processThrowable(org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode,java.lang.String,java.lang.Throwable,java.util.List,org.apache.hadoop.fs.Path[])",917,933,"/**
 * Adds an IOException to the list, wrapping the given Throwable.
 * @param nflyNode Node context, op operation, t Throwable,
 * @param ioExceptions List to add IOException to, f paths
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getWorkingDirectory,org.apache.hadoop.fs.viewfs.NflyFSystem:getWorkingDirectory(),861,864,"/**
 * Gets the working directory from the first node.
 * Returns the working directory Path object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,replaceRegexCaptureGroupInPath,"org.apache.hadoop.fs.viewfs.RegexMountPoint:replaceRegexCaptureGroupInPath(java.lang.String,java.util.regex.Matcher,java.lang.String,java.util.Set)",246,261,"/**
 * Replaces variable names in path with regex group value.
 * @param parsedDestPath Path to modify.
 * @param srcMatcher Regex matcher.
 * @param regexGroupNameOrIndexStr Group name/index.
 * @param groupRepresentationStrSetInDest Variable names to replace.
 */","* Use capture group named regexGroupNameOrIndexStr in mather to replace
   * parsedDestPath.
   * E.g. link: ^/user/(?<username>\\w+) => s3://$user.apache.com/_${user}
   * srcMatcher is from /user/hadoop.
   * Then the params will be like following.
   * parsedDestPath: s3://$user.apache.com/_${user},
   * regexGroupNameOrIndexStr: user
   * groupRepresentationStrSetInDest: {user:$user; user:${user}}
   * return value will be s3://hadoop.apache.com/_hadoop
   * @param parsedDestPath
   * @param srcMatcher
   * @param regexGroupNameOrIndexStr
   * @param groupRepresentationStrSetInDest
   * @return return parsedDestPath while ${var},$var replaced or
   * parsedDestPath nothing found.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRootDir,org.apache.hadoop.fs.viewfs.InodeTree:getRootDir(),520,523,"/**
 * Returns the root INodeDir. Asserts that root is internal.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRootLink,org.apache.hadoop.fs.viewfs.InodeTree:getRootLink(),525,528,"/**
 * Returns the root INodeLink. Asserts root is not an internal directory.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRootFallbackLink,org.apache.hadoop.fs.viewfs.InodeTree:getRootFallbackLink(),543,546,"/**
 * Returns the fallback link for the root directory.
 * Asserts root is internal. Returns the link.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,tryStart,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStart(),156,184,"/**
 * Starts the AsyncCallQueue processor thread if not already running.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,kill,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:kill(org.apache.hadoop.util.Daemon),192,198,"/**
 * Stops a daemon.
 * @param d The daemon to stop.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,offer,org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:offer(java.lang.Object),101,104,"/**
* Adds an element to the queue.
* @param c the element to add
* @throws IllegalStateException if the queue is full
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(org.apache.hadoop.crypto.Decryptor,java.nio.ByteBuffer,java.nio.ByteBuffer,byte)",246,265,"/**
 * Decrypts data from inBuffer to outBuffer using the given decryptor.
 * @param decryptor Decryptor implementation
 * @param inBuffer Input buffer containing encrypted data
 * @param outBuffer Output buffer for decrypted data
 * @param padding Padding value
 */","* Do the decryption using inBuffer as input and outBuffer as output.
   * Upon return, inBuffer is cleared; the decrypted data starts at 
   * outBuffer.position() and ends at outBuffer.limit();",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,checkState,org.apache.hadoop.crypto.OpensslCipher:checkState(),289,291,"/**
 * Verifies that the context is not zero.
 */",Check whether context is initialized.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,parentZNodeExists,org.apache.hadoop.ha.ActiveStandbyElector:parentZNodeExists(),341,350,"/**
 * Checks if the parent znode exists.
 * @return True if the znode exists, false otherwise.
 */
","* @return true if the configured parent znode exists
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException interrupted exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getConfigViewFsPrefix,org.apache.hadoop.fs.viewfs.ConfigUtil:getConfigViewFsPrefix(),43,46,"/**
 * Gets the config view FS prefix.
 * Delegates to the default mount table.
 */","* Get the config variable prefix for the default mount table
   * @return the config variable prefix for the default mount table",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileChecksum(org.apache.hadoop.fs.Path),1537,1542,"/**
 * Returns checksum for a file.
 * @param f Path to the file.
 * @throws FileNotFoundException if path is a directory
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,open,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:open(org.apache.hadoop.fs.Path,int)",1732,1737,"/**
 * Opens a file for input. Throws exception if path is a directory.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPointResolvedDstPathReplaceInterceptor.java,deserializeFromString,org.apache.hadoop.fs.viewfs.RegexMountPointResolvedDstPathReplaceInterceptor:deserializeFromString(java.lang.String),125,136,"/**
 * Deserializes RegexMountPointResolvedDstPathReplaceInterceptor from string.
 * @param serializedString String representation of the interceptor.
 * @return RegexMountPointResolvedDstPathReplaceInterceptor object.
 */
","* Create interceptor from config string. The string should be in
   * replaceresolvedpath:wordToReplace:replaceString
   * Note that we'll assume there's no ':' in the regex for the moment.
   *
   * @return Interceptor instance or null on bad config.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,getReturnValue,org.apache.hadoop.io.retry.CallReturn:getReturnValue(),71,77,"/**
 * Returns the return value.
 * Throws an exception if state is EXCEPTION.
 * Requires state to be RETURNED.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputWrapper.java,getReadableByteChannel,org.apache.hadoop.net.SocketInputWrapper:getReadableByteChannel(),81,86,"/**
 * Returns the readable byte channel associated with this socket.
 * Returns (SocketInputStream)in.
 */","* @return an underlying ReadableByteChannel implementation.
   * @throws IllegalStateException if this socket does not have a channel",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getChecksumFileLength,"org.apache.hadoop.fs.ChecksumFileSystem:getChecksumFileLength(org.apache.hadoop.fs.Path,long)",143,145,"/**
 * Calculates checksum length based on file size and bytes per sum.
 * @param file Path to the file. @param fileSize File size in bytes.
 */
","* Return the length of the checksum file given the size of the
   * actual file.
   *
   * @param file the file path.
   * @param fileSize file size.
   * @return checksum length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,getFilesystem,org.apache.hadoop.fs.DF:getFilesystem(),72,82,"/**
 * Returns the filesystem type (e.g., ""ntfs"" or ""ext4"").
 * Uses Windows path or executes a command to determine.
 */
","* @return a string indicating which filesystem volume we're checking.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,getMount,org.apache.hadoop.fs.DF:getMount(),110,127,"/**
 * Retrieves the mount point of the directory.
 * Returns mount string, throws IOException if path doesn't exist.
 */
","* @return the filesystem mount point for the indicated volume.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DFCachingGetSpaceUsed.java,refresh,org.apache.hadoop.fs.DFCachingGetSpaceUsed:refresh(),44,47,"/**
 * Updates the 'used' field with the value from df.getUsed().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,getPercentUsed,org.apache.hadoop.fs.DF:getPercentUsed(),100,104,"/**
 * Calculates the percentage of storage used.
 * Returns the percentage as an integer.
 */
","@return the amount of the volume full, as a percent.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,startPositionWithoutWindowsDrive,org.apache.hadoop.fs.Path:startPositionWithoutWindowsDrive(java.lang.String),324,330,"/**
 * Returns the start position of a path, skipping Windows drive if present.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,toString,org.apache.hadoop.fs.Path:toString(),476,503,"/**
 * Returns a string representation of the URI, unescaped for glob processing.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,<init>,"org.apache.hadoop.fs.FSInputChecker:<init>(org.apache.hadoop.fs.Path,int,boolean,java.util.zip.Checksum,int,int)",87,91,"/**
 * Constructs an FSInputChecker with file, retries, and checksum options.
 * @param file The path to the file.
 */
","Constructor
   * 
   * @param file The name of the file to be read
   * @param numOfRetries Number of read retries when ChecksumError occurs
   * @param sum the type of Checksum engine
   * @param chunkSize maximun chunk size
   * @param checksumSize the number byte of each checksum
   * @param verifyChecksum verify check sum.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,seek,org.apache.hadoop.fs.FSInputChecker:seek(long),428,451,"/**
 * Seeks to the specified position. Throws EOFException if negative.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ByteBufferUtil.java,streamHasByteBufferRead,org.apache.hadoop.fs.ByteBufferUtil:streamHasByteBufferRead(java.io.InputStream),37,46,"/**
 * Checks if an InputStream can be read using a ByteBuffer.
 */
",* Determine if a stream can do a byte buffer read via read(ByteBuffer buf),,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,init,org.apache.hadoop.fs.audit.CommonAuditContext:init(),193,197,"/**
 * Initializes the audit context by setting the thread ID.
 */",* Initialize.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,noteEntryPoint,org.apache.hadoop.fs.audit.CommonAuditContext:noteEntryPoint(java.lang.Object),278,288,"/**
 * Sets a global context entry for command based on the tool's class.
 */","* Add the entry point as a context entry with the key
   * {@link AuditConstants#PARAM_COMMAND}
   * if it has not  already been recorded.
   * This is called via ToolRunner but may be used at any
   * other entry point.
   * @param tool object loaded/being launched.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,<init>,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder:<init>(),404,405,"/**
 * Private constructor to prevent direct instantiation of Builder.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,int)",93,95,"/**
 * Retrieves a value associated with the key as an optional long.
 * @param key The key to search for.
 * @param value The value to use if not found.
 * @return An OptionalLong containing the value or default.
 */
","* Set optional int parameter for the Builder.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #opt(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,float)",108,111,"/**
 * Deprecated: Returns B from long value for key.
 * @param key key to look up
 * @param value float value to convert to long
 * @return B object or null if not found
 */
","* This parameter is converted to a long and passed
   * to {@link #optLong(String, long)} -all
   * decimal precision is lost.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #opt(String, String)
   * @deprecated use {@link #optDouble(String, double)}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,long)",121,123,"/**
 * Sets the value associated with the given key to the provided long.
 * @param key key to associate with the value
 * @param value the long value to set
 */
","* Set optional long parameter for the Builder.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @deprecated use  {@link #optLong(String, long)} where possible.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,opt,"org.apache.hadoop.fs.FSBuilder:opt(java.lang.String,double)",136,139,"/**
 * Deprecated: Returns B by key, casting value to long.
 */","* Pass an optional double parameter for the Builder.
   * This parameter is converted to a long and passed
   * to {@link #optLong(String, long)} -all
   * decimal precision is lost.
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #opt(String, String)
   * @deprecated use {@link #optDouble(String, double)}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,int)",207,209,"/**
 * Delegates to mustLong, setting a key-value pair.
 * @param key The key to set.
 * @param value The integer value.
 */
","* Set mandatory int option.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #must(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,float)",221,224,"/**
* Deprecated: Converts a float value to a long and persists it.
*/","* This parameter is converted to a long and passed
   * to {@link #mustLong(String, long)} -all
   * decimal precision is lost.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @deprecated use {@link #mustDouble(String, double)} to set floating point.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,long)",234,237,"/**
 * Deprecated: Creates a B object with the given key and value.
 */","* Set mandatory long option.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #must(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSBuilder.java,must,"org.apache.hadoop.fs.FSBuilder:must(java.lang.String,double)",248,251,"/**
 * Deprecated: Converts a key-value pair to a long and returns it.
 */","* Set mandatory long option, despite passing in a floating
   * point value.
   *
   * @param key key.
   * @param value value.
   * @return generic type B.
   * @see #must(String, String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,getRow,org.apache.hadoop.tools.TableListing$Column:getRow(int),100,116,"/**
 * Retrieves a row from the data, wraps if needed, and justifies.
 * @param idx row index
 * @return String array representing the formatted row
 */
","* Return the ith row of the column as a set of wrapped strings, each at
     * most wrapWidth in length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBUploadHandle.java,from,org.apache.hadoop.fs.BBUploadHandle:from(java.nio.ByteBuffer),40,42,"/**
 * Creates an UploadHandle from a ByteBuffer.
 * @param byteBuffer The byte buffer to wrap.
 * @return A BBUploadHandle instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BBUploadHandle.java,equals,org.apache.hadoop.fs.BBUploadHandle:equals(java.lang.Object),54,61,"/**
 * Checks if two UploadHandle objects have the same byte content.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,startLocalOutput,"org.apache.hadoop.fs.FilterFileSystem:startLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",396,400,"/**
 * Starts local output using FileSystem.
 * @param fsOutputFile Output file path.
 * @param tmpLocalFile Temporary file path.
 * @return Path representing the started output.
 */
","* Returns a local File that the user can write output to.  The caller
   * provides both the eventual FS target name and the local working
   * file.  If the FS is local, we write directly into the target.  If
   * the FS is remote, we write into the tmp local area.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setWriteChecksum,org.apache.hadoop.fs.FilterFileSystem:setWriteChecksum(boolean),517,520,"/**
* Sets whether to write checksums during file writes.
* @param writeChecksum flag indicating checksum writing
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,"org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,boolean)",86,88,"/**
 * Constructs a FsPermission with specified user, group, other actions.
 * @param u User action
 * @param g Group action
 * @param o Other action
 * @param sb Boolean for secure boolean
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,fromShort,org.apache.hadoop.fs.permission.FsPermission:fromShort(short),174,177,"/**
* Parses an action from a short value.
* @param n short value containing action data.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclStatus.java,getEffectivePermission,"org.apache.hadoop.fs.permission.AclStatus:getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry,org.apache.hadoop.fs.permission.FsPermission)",247,277,"/**
 * Calculates the effective permission based on ACL entry and permission.
 * @param entry The ACL entry to evaluate.
 * @param permArg The permission argument.
 * @return The effective FsAction permission.
 */
","* Get the effective permission for the AclEntry. <br>
   * Recommended to use this API ONLY if client communicates with the old
   * NameNode, needs to pass the Permission for the path to get effective
   * permission, else use {@link AclStatus#getEffectivePermission(AclEntry)}.
   * @param entry AclEntry to get the effective action
   * @param permArg Permission for the path. However if the client is NOT
   *          communicating with old namenode, then this argument will not have
   *          any preference.
   * @return Returns the effective permission for the entry.
   * @throws IllegalArgumentException If the client communicating with old
   *           namenode and permission is not passed as an argument.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,<init>,"org.apache.hadoop.fs.permission.PermissionStatus$2:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",72,76,"/**
* Constructs a PermissionStatus with username, group, and permission.
*/
","* Constructor.
   *
   * @param user user.
   * @param group group.
   * @param permission permission.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,parseAclEntry,"org.apache.hadoop.fs.permission.AclEntry:parseAclEntry(java.lang.String,boolean)",263,323,"/**
 * Parses an ACL entry string and creates an AclEntry object.
 * @param aclStr ACL entry string to parse.
 * @param includePermission Whether to include permission in parsing.
 * @return AclEntry object representing the parsed ACL entry.
 */
","* Parses a string representation of an ACL into a AclEntry object.<br>
   * The expected format of ACL entries in the string parameter is the same
   * format produced by the {@link #toStringStable()} method.
   * 
   * @param aclStr
   *          String representation of an ACL.<br>
   *          Example: ""user:foo:rw-""
   * @param includePermission
   *          for setAcl operations this will be true. i.e. Acl should include
   *          permissions.<br>
   *          But for removeAcl operation it will be false. i.e. Acl should not
   *          contain permissions.<br>
   *          Example: ""user:foo,group:bar,mask::""
   * @return Returns an {@link AclEntry} object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,<init>,"org.apache.hadoop.fs.permission.FsCreateModes:<init>(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",65,70,"/**
 * Constructs a FsCreateModes with masked and unmasked permissions.
 * @param masked The masked permission.
 * @param unmasked The unmasked permission.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,equals,org.apache.hadoop.fs.permission.FsCreateModes:equals(java.lang.Object),88,101,"/**
 * Checks if this FsCreateModes object is equal to another.
 * @param o the object to compare to
 * @return true if objects are equal, false otherwise
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,toStringStable,org.apache.hadoop.fs.permission.AclEntry:toStringStable(),119,136,"/**
 * Generates a stable string representation of the AclEntry.
 * Builds a string based on scope, type, name, and permission.
 */
","* Returns a string representation guaranteed to be stable across versions to
   * satisfy backward compatibility requirements, such as for shell command
   * output or serialization.  The format of this string representation matches
   * what is expected by the {@link #parseAclSpec(String, boolean)} and
   * {@link #parseAclEntry(String, boolean)} methods.
   *
   * @return stable, backward compatible string representation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntryType.java,toString,org.apache.hadoop.fs.permission.AclEntryType:toString(),59,65,"/**
 * Returns a string representation of the object, delegating to stable version.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getStrings,"org.apache.hadoop.util.StringUtils:getStrings(java.lang.String,java.lang.String)",418,424,"/**
 * Converts a string to a string array using a delimiter.
 * @param str input string
 * @param delim delimiter string
 * @return String array or null if empty
 */
","* Returns an arraylist of strings.
   * @param str the string values
   * @param delim delimiter to separate the values
   * @return the arraylist of the separated string values",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getStringCollection,org.apache.hadoop.util.StringUtils:getStringCollection(java.lang.String),431,434,"/**
* Splits a string into a collection of strings using ',' as delimiter.
* @param str The string to split.
* @return A Collection of strings.
*/
","* Returns a collection of strings.
   * @param str comma separated string values
   * @return an <code>ArrayList</code> of string values",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionParser.java,<init>,"org.apache.hadoop.fs.permission.PermissionParser:<init>(java.lang.String,java.util.regex.Pattern,java.util.regex.Pattern)",51,62,"/**
 * Parses permission string using symbolic or octal patterns.
 * @param modeStr Input permission string.
 * @throws IllegalArgumentException if parsing fails.
 */
","* Begin parsing permission stored in modeStr
   * 
   * @param modeStr Permission mode, either octal or symbolic
   * @param symbolic Use-case specific symbolic pattern to match against
   * @throws IllegalArgumentException if unable to parse modeStr",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionParser.java,combineModes,"org.apache.hadoop.fs.permission.PermissionParser:combineModes(int,boolean)",175,183,"/**
 * Combines mode segments based on existing modes and execution OK.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",649,651,"/**
 * Constructs a ByteBufferBlockFactory.
 * @param keyToBufferDir Path to buffer directory.
 * @param conf Configuration object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",526,528,"/**
 * Constructs an ArrayBlockFactory with a key and configuration.
 * @param keyToBufferDir Key for buffer directory.
 * @param conf Configuration object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,requestBuffer,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:requestBuffer(int),659,663,"/**
 * Retrieves a buffer from the buffer pool with the specified limit.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,releaseBuffer,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory:releaseBuffer(java.nio.ByteBuffer),665,669,"/**
 * Releases a buffer back to the buffer pool and decrements count.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$DiskBlock:<init>(java.io.File,int,long,org.apache.hadoop.fs.store.BlockUploadStatistics)",853,863,"/**
 * Initializes a DiskBlock with file, limit, index, and statistics.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:<init>(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",574,581,"/**
 * Initializes a ByteArrayBlock with given index, limit, and stats.
 * @param index block index
 * @param limit buffer limit
 * @param statistics upload statistics
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,hasCapacity,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:hasCapacity(long),869,872,"/**
 * Checks if there's enough capacity for the given bytes.
 * @param bytes Number of bytes to check for capacity.
 * @return True if there's enough capacity, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,toString,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:toString(),945,955,"/**
 * Returns a string representation of this FileBlock object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,innerClose,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:innerClose(),905,933,"/**
* Closes the block, handling different states and buffer file deletion.
*/","* The close operation will delete the destination file if it still
     * exists.
     *
     * @throws IOException IO problems",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,checkOpenState,org.apache.hadoop.fs.store.ByteBufferInputStream:checkOpenState(),89,92,"/**
 * Verifies the stream is open; throws exception if closed.
 */","* Check the open state.
   * @throws IllegalStateException if the stream is closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,enterState,"org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterState(org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState,org.apache.hadoop.fs.store.DataBlocks$DataBlock$DestState)",349,355,"/**
 * Transitions the state from current to next, verifying the current state.
 */","* Atomically enter a state, verifying current state.
     *
     * @param current current state. null means ""no check""
     * @param next    next state
     * @throws IllegalStateException if the current state is not as expected",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$DataBlock:write(byte[],int,int)",424,433,"/**
 * Writes data from buffer to the output stream.
 * @param buffer The buffer containing the data to write.
 * @param offset The offset within the buffer.
 * @param length The number of bytes to write.
 * @return The number of bytes written.
 */
","* Write a series of bytes from the buffer, from the offset.
     * Returns the number of bytes written.
     * Only valid in the state {@code Writing}.
     * Base class verifies the state but does no writing.
     *
     * @param buffer buffer.
     * @param offset offset.
     * @param length length of write.
     * @return number of bytes written.
     * @throws IOException trouble",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,flush,org.apache.hadoop.fs.store.DataBlocks$DataBlock:flush(),442,444,"/**
 * Flushes the buffer to ensure data is written.
 * Throws IOException if in an invalid state.
 */
","* Flush the output.
     * Only valid in the state {@code Writing}.
     * In the base class, this is a no-op
     *
     * @throws IOException any IO problem.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,set,"org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:set(java.lang.String,java.lang.String)",246,248,"/**
* Sets an attribute with the given key and value.
* @param key Attribute key, cannot be null.
* @param value Attribute value.
*/
","* Set an attribute. If the value is non-null/empty,
   * it will be used as a query parameter.
   *
   * @param key key to set
   * @param value value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,extractQueryParameters,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:extractQueryParameters(java.lang.String),331,347,"/**
 * Extracts query parameters from a URI header string.
 * @param header The URI header string to parse.
 * @return A map of parameter names and values.
 */
","* Split up the string. Uses httpClient: make sure it is on the classpath.
   * Any query param with a name but no value, e.g ?something is
   * returned in the map with an empty string as the value.
   * @param header URI to parse
   * @return a map of parameters.
   * @throws URISyntaxException failure to build URI from header.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,hasCapacity,org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:hasCapacity(long),602,605,"/**
 * Checks if the buffer has enough capacity for the given bytes.
 * @param bytes The number of bytes to check for.
 * @return True if there's enough capacity, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,remainingCapacity,org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:remainingCapacity(),607,610,"/**
 * Returns the remaining capacity, limit minus current data size.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,dataSize,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:dataSize(),718,720,"/**
* Returns the data size, using dataSize if available, else buffer capacity.
*/
","* Get the amount of data; if there is no buffer then the size is 0.
       *
       * @return the amount of data available to upload.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,hasCapacity,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:hasCapacity(long),733,736,"/**
 * Checks if the buffer has enough capacity for the given bytes.
 * @param bytes The number of bytes to check against remaining capacity.
 * @return True if there's enough capacity, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,hashCode,org.apache.hadoop.fs.FileSystem$Cache$Key:hashCode(),3891,3894,"/**
 * Calculates the hash code based on scheme, authority, ugi, and unique.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,hashCode,org.apache.hadoop.security.UserGroupInformation$RealUser:hashCode(),492,495,"/**
 * Returns the hash code, based on the wrapped real user's hash code.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,hashCode,org.apache.hadoop.ipc.Client$ConnectionId:hashCode(),1843,1859,"/**
 * Calculates the hash code based on connection policy attributes.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,equals,org.apache.hadoop.fs.FileSystem$Cache$Key:equals(java.lang.Object),3900,3913,"/**
 * Checks if two Key objects are equal based on their attributes.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,equals,org.apache.hadoop.security.UserGroupInformation$RealUser:equals(java.lang.Object),481,490,"/**
 * Checks if two RealUser objects are equal based on their realUser field.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getInitialWorkingDirectory,org.apache.hadoop.fs.FilterFs:getInitialWorkingDirectory(),78,81,"/**
 * Gets the initial working directory from the file system.
 * Returns a Path object representing the directory.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,resolvePath,org.apache.hadoop.fs.FileContext:resolvePath(org.apache.hadoop.fs.Path),616,619,"/**
 * Resolves a Path, throwing exceptions on failure.
 * @param f The Path to resolve.
 * @return The resolved Path.
 */
","* Resolve the path following any symlinks or mount points
   * @param f to be resolved
   * @return fully qualified resolved path
   * 
   * @throws FileNotFoundException  If <code>f</code> does not exist
   * @throws AccessControlException if access denied
   * @throws IOException If an IO Error occurred
   * @throws UnresolvedLinkException If unresolved link occurred.
   *
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server
   *
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is not valid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,msync,org.apache.hadoop.fs.FileContext:msync(),1267,1269,"/**
 * Calls msync on the default file system.
 * Throws IOException or UnsupportedOperationException.
 */
","* Synchronize client metadata state.
   *
   * @throws IOException If an I/O error occurred.
   * @throws UnsupportedOperationException If file system for <code>f</code> is
   *                                       not supported.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,msync,org.apache.hadoop.fs.FilterFs:msync(),127,130,"/**
* Calls the underlying file system's msync method.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,printStatistics,org.apache.hadoop.fs.FileContext:printStatistics(),2412,2414,"/**
 * Prints filesystem statistics to the console.
 */","* Prints the statistics to standard output. File System is identified by the
   * scheme and authority.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getStatistics,org.apache.hadoop.fs.AbstractFileSystem:getStatistics(java.net.URI),191,204,"/**
 * Retrieves statistics for a URI, creating if not present.
 * @param uri The URI to retrieve statistics for.
 * @return Statistics object for the URI.
 */
","* Get the statistics for a particular file system.
   * 
   * @param uri
   *          used as key to lookup STATISTICS_TABLE. Only scheme and authority
   *          part of the uri are used.
   * @return a statistics object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listCorruptFileBlocks,org.apache.hadoop.fs.FilterFs:listCorruptFileBlocks(org.apache.hadoop.fs.Path),209,213,"/**
 * Lists corrupt file blocks for a given path.
 * @param path The path to check for corrupt blocks.
 * @return An iterator over corrupt file blocks.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,createMultipartUploader,org.apache.hadoop.fs.AbstractFileSystem:createMultipartUploader(org.apache.hadoop.fs.Path),1634,1639,"/**
* Creates a MultipartUploaderBuilder. Returns null; method is unstable.
*/","* Create a multipart uploader.
   * @param basePath file path under which all files are uploaded
   * @return a MultipartUploaderBuilder object to build the uploader
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,obtainContext,org.apache.hadoop.fs.LocalDirAllocator:obtainContext(java.lang.String),107,117,"/**
 * Retrieves an AllocatorPerContext for the given context name.
 * @param contextCfgItemName Context name to retrieve.
 * @return AllocatorPerContext object.
 */
","This method must be used to obtain the dir allocation context for a 
   * particular value of the context name. The context name must be an item
   * defined in the Configuration object for which we want to control the 
   * dir allocations (e.g., <code>mapred.local.dir</code>). The method will
   * create a context for that name if it doesn't already exist.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getStatistics,org.apache.hadoop.fs.FilterFs:getStatistics(),68,71,"/**
 * Returns the file system statistics.
 * Delegates to the underlying FileSystem object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,getPos,org.apache.hadoop.fs.FSDataOutputStream:getPos(),97,99,"/**
 * Retrieves the position from the cached output.
 * @return The position value as a long.
 */
","* Get the current position in the output stream.
   *
   * @return the current position in the output stream",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,syncFs,org.apache.hadoop.io.SequenceFile$Writer:syncFs(),1382,1387,"/**
 * Flushes output stream to the file system.
 * Deprecated method.
 */","* flush all currently written data to the file system.
     * @deprecated Use {@link #hsync()} or {@link #hflush()} instead
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,hflush,org.apache.hadoop.io.SequenceFile$Writer:hflush(),1396,1401,"/**
 * Flushes the output stream. Calls the underlying stream's hflush().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,hsync,org.apache.hadoop.io.SequenceFile$Writer:hsync(),1389,1394,"/**
 * Performs a horizontal synchronization on the output stream.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,quota,org.apache.hadoop.fs.ContentSummary$Builder:quota(long),90,94,"/**
* Sets the quota and returns this builder for chaining.
* @param quota The quota value to set.
* @return This builder instance.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,spaceConsumed,org.apache.hadoop.fs.ContentSummary$Builder:spaceConsumed(long),96,100,"/**
* Sets the space consumed and returns the builder.
* @param spaceConsumed The amount of space consumed.
* @return This builder instance.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,spaceQuota,org.apache.hadoop.fs.ContentSummary$Builder:spaceQuota(long),102,106,"/**
 * Sets the space quota and returns the builder instance.
 * @param spaceQuota The desired space quota value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,build,org.apache.hadoop.fs.ContentSummary$Builder:build(),132,136,"/**
 * Builds and returns a ContentSummary object.
 * Uses file and directory counts from the builder.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,<init>,"org.apache.hadoop.util.ApplicationClassLoader:<init>(java.net.URL[],java.lang.ClassLoader,java.util.List)",87,100,"/**
 * Constructs an ApplicationClassLoader with URLs, parent, and system classes.
 * @param urls URLs for class loading
 * @param parent Parent classloader
 * @param systemClasses System classes to load
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getTrimmedStringCollection,org.apache.hadoop.util.StringUtils:getTrimmedStringCollection(java.lang.String),490,495,"/**
 * Returns a collection of trimmed strings, removing empty strings.
 */","* Splits a comma separated value <code>String</code>, trimming leading and
   * trailing whitespace on each value. Duplicate and empty values are removed.
   *
   * @param str a comma separated <code>String</code> with values, may be null
   * @return a <code>Collection</code> of <code>String</code> values, empty
   *         Collection if null String input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/LoggingStateChangeListener.java,<init>,org.apache.hadoop.service.LoggingStateChangeListener:<init>(),51,53,"/**
 * Constructs a LoggingStateChangeListener with the default logger.
 */",* Log events to the static log for this class,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,<init>,org.apache.hadoop.service.ServiceStateException:<init>(java.lang.String),48,50,"/**
 * Constructs a ServiceStateException with a message.
 * @param message The error message.
 */
","* Instantiate
   * @param message error message",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,<init>,"org.apache.hadoop.service.ServiceStateException:<init>(int,java.lang.String,java.lang.Throwable)",78,83,"/**
 * Constructs ServiceStateException with exit code, message, and cause.
 * @param exitCode The exit code associated with the exception.
 * @param message Exception message.
 * @param cause The underlying cause of the exception.
 */
","* Instantiate, using the specified exit code as the exit code
   * of the exception, irrespetive of any exit code supplied by any inner
   * cause.
   *
   * @param exitCode exit code to declare
   * @param message exception message
   * @param cause inner cause",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,convert,"org.apache.hadoop.service.ServiceStateException:convert(java.lang.String,java.lang.Throwable)",120,126,"/**
 * Converts a Throwable to a RuntimeException or ServiceStateException.
 * @param text Error message; @param fault The Throwable to convert.
 */
","* Convert any exception into a {@link RuntimeException}.
   * If the caught exception is already of that type, it is typecast to a
   * {@link RuntimeException} and returned.
   *
   * All other exception types are wrapped in a new instance of
   * {@code ServiceStateException}.
   * @param text text to use if a new exception is created
   * @param fault exception or throwable
   * @return a {@link RuntimeException} to rethrow",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateException.java,convert,org.apache.hadoop.service.ServiceStateException:convert(java.lang.Throwable),101,107,"/**
 * Converts a Throwable to a RuntimeException or ServiceStateException.
 */
","* Convert any exception into a {@link RuntimeException}.
   * All other exception types are wrapped in a new instance of
   * {@code ServiceStateException}.
   * @param fault exception or throwable
   * @return a {@link RuntimeException} to rethrow",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,<init>,org.apache.hadoop.service.ServiceStateModel:<init>(java.lang.String),60,62,"/**
 * Constructs a ServiceStateModel with a name and initial state.
 * @param name The name of the service state model.
 */
","* Create the service state model in the {@link Service.STATE#NOTINITED}
   * state.
   *
   * @param name input name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,isInState,org.apache.hadoop.service.AbstractService:isInState(org.apache.hadoop.service.Service$STATE),451,454,"/**
 * Checks if the service is in the expected state.
 * @param expected The expected state to check against.
 * @return True if in the expected state, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,isValidStateTransition,"org.apache.hadoop.service.ServiceStateModel:isValidStateTransition(org.apache.hadoop.service.Service$STATE,org.apache.hadoop.service.Service$STATE)",149,153,"/**
 * Checks if a state transition is valid based on the statemap.
 * @param current Current state of the service.
 * @param proposed Proposed new state.
 * @return True if transition is valid, false otherwise.
 */
","* Is a state transition valid?
   * There are no checks for current==proposed
   * as that is considered a non-transition.
   *
   * using an array kills off all branch misprediction costs, at the expense
   * of cache line misses.
   *
   * @param current current state
   * @param proposed proposed new state
   * @return true if the transition to a new state is valid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,toString,org.apache.hadoop.service.ServiceStateModel:toString(),159,163,"/**
 * Returns a string representation of the object's state.
 */
","* return the state text as the toString() value
   * @return the current state's description",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/HadoopUncaughtExceptionHandler.java,<init>,org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:<init>(),71,73,"/**
 * Default constructor. Initializes with a null handler.
 */","* Basic exception handler -logs simple exceptions, then continues.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/IrqHandler.java,handle,org.apache.hadoop.service.launcher.IrqHandler:handle(sun.misc.Signal),125,131,"/**
 * Handles a signal by incrementing count and notifying the handler.
 * @param s The signal to handle.
 */
","* Handler for the JVM API for signal handling.
   * @param s signal raised",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,getService,org.apache.hadoop.service.launcher.InterruptEscalator:getService(),84,87,"/**
 * Retrieves the service from the owner, or null if owner is null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,<init>,"org.apache.hadoop.service.launcher.InterruptEscalator$ServiceForcedShutdown:<init>(org.apache.hadoop.service.Service,int)",188,191,"/**
 * Constructs a ServiceForcedShutdown with the given service and timeout.
 * @param service The service to be forcibly shut down.
 * @param shutdownTimeMillis Shutdown timeout in milliseconds.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,lookup,org.apache.hadoop.service.launcher.InterruptEscalator:lookup(java.lang.String),153,160,"/**
 * Finds an IrqHandler by name.
 * @param signalName Name of the handler to find.
 * @return IrqHandler object or null if not found.
 */
","* Look up the handler for a signal.
   * @param signalName signal name
   * @return a handler if found",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.Throwable)",49,51,"/**
* Constructs a ServiceLaunchException with exit code and cause.
*/
","* Create an exception with the specific exit code.
   * @param exitCode exit code
   * @param cause cause of the exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.String)",58,60,"/**
 * Constructs a ServiceLaunchException with exit code and message.
 */
","* Create an exception with the specific exit code and text.
   * @param exitCode exit code
   * @param message message to use in exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.String,java.lang.Object[])",74,79,"/**
 * Constructs a ServiceLaunchException with exit code, format, and args.
 * Optionally sets cause if last arg is a Throwable.
 */
","* Create a formatted exception.
   * <p>
   * This uses {@link String#format(String, Object...)}
   * to build the formatted exception in the ENGLISH locale.
   * <p>
   * If the last argument is a throwable, it becomes the cause of the exception.
   * It will also be used as a parameter for the format.
   * @param exitCode exit code
   * @param format format for message to use in exception
   * @param args list of arguments",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.String)",1083,1086,"/**
 * Constructs a KerberosDiagsFailure with category and message.
 * @param category Failure category.
 * @param message Failure message.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLaunchException.java,<init>,"org.apache.hadoop.service.launcher.ServiceLaunchException:<init>(int,java.lang.Throwable,java.lang.String,java.lang.Object[])",91,94,"/**
 * Constructs a ServiceLaunchException with exit code, cause, and format string.
 */
","* Create a formatted exception.
   * <p>
   * This uses {@link String#format(String, Object...)}
   * to build the formatted exception in the ENGLISH locale.
   * @param exitCode exit code
   * @param cause inner cause
   * @param format format for message to use in exception
   * @param args list of arguments",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,run,org.apache.hadoop.service.launcher.ServiceShutdownHook:run(),82,85,"/**
 * Executes the task by shutting down the system.
 */
","* Shutdown handler.
   * Query the service hook reference -if it is still valid the 
   * {@link Service#stop()} operation is invoked.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,<init>,org.apache.hadoop.service.launcher.ServiceLauncher:<init>(java.lang.String),184,186,"/**
 * Constructs a ServiceLauncher with the same name for class and service.
 * @param serviceClassName Name of the service class.
 */
","* Create an instance of the launcher.
   * @param serviceClassName classname of the service",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,toString,org.apache.hadoop.service.launcher.ServiceLauncher:toString(),253,264,"/**
 * Returns a string representation of the ServiceLauncher.
 * Includes serviceName, serviceClassName (if defined), and service.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,noteException,org.apache.hadoop.service.launcher.ServiceLauncher:noteException(org.apache.hadoop.util.ExitUtil$ExitException),331,345,"/**
 * Logs an ExitException and sets service exit code/exception.
 * @param exitException The exception to log and process.
 */
","* Record that an Exit Exception has been raised.
   * Save it to {@link #serviceException}, with its exit code in
   * {@link #serviceExitCode}
   * @param exitException exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,bindCommandOptions,org.apache.hadoop.service.launcher.ServiceLauncher:bindCommandOptions(),321,323,"/**
 * Initializes command options by creating a new options object.
 */","* Set the {@link #commandOptions} field to the result of
   * {@link #createOptions()}; protected for subclasses and test access.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,loadConfigurationClasses,org.apache.hadoop.service.launcher.ServiceLauncher:loadConfigurationClasses(),422,449,"/**
 * Loads configuration classes and returns the number loaded.
 */","* @return This creates all the configurations defined by
   * {@link #getConfigurationsToCreate()} , ensuring that
   * the resources have been pushed in.
   * If one cannot be loaded it is logged and the operation continues
   * except in the case that the class does load but it isn't actually
   * a subclass of {@link Configuration}.
   * @throws ExitUtil.ExitException if a loaded class is of the wrong type",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,registerServiceListener,org.apache.hadoop.service.AbstractService:registerServiceListener(org.apache.hadoop.service.ServiceStateChangeListener),354,357,"/**
 * Registers a ServiceStateChangeListener to receive service state updates.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,registerGlobalListener,org.apache.hadoop.service.AbstractService:registerGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener),369,371,"/**
 * Registers a ServiceStateChangeListener to receive service state updates.
 * @param l The listener to register.
 */
","* Register a global listener, which receives notifications
   * from the state change events of all services in the JVM
   * @param l listener",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,unregisterServiceListener,org.apache.hadoop.service.AbstractService:unregisterServiceListener(org.apache.hadoop.service.ServiceStateChangeListener),359,362,"/**
 * Removes a ServiceStateChangeListener from the listener list.
 * @param l The listener to remove.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,unregisterGlobalListener,org.apache.hadoop.service.AbstractService:unregisterGlobalListener(org.apache.hadoop.service.ServiceStateChangeListener),378,380,"/**
 * Removes a global service state change listener.
 * @param l The listener to remove. Returns true if removed.
 */
","* unregister a global listener.
   * @param l listener to unregister
   * @return true if the listener was found (and then deleted)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,resetGlobalListeners,org.apache.hadoop.service.AbstractService:resetGlobalListeners(),385,388,"/**
 * Resets the global listeners to their initial state.
 */",* Package-scoped method for testing -resets the global listener list,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,notifyListeners,org.apache.hadoop.service.AbstractService:notifyListeners(),409,416,"/**
 * Notifies listeners and global listeners about changes.
 * Handles exceptions during notification.
 */
","* Notify local and global listeners of state changes.
   * Exceptions raised by listeners are NOT passed up.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,getServiceState,org.apache.hadoop.service.AbstractService:getServiceState(),118,121,"/**
 * Returns the current service state from the state model.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,serviceInit,org.apache.hadoop.service.AbstractService:serviceInit(org.apache.hadoop.conf.Configuration),312,317,"/**
 * Initializes the service, overriding config if necessary.
 * @param conf The configuration to use for initialization.
 */
","* All initialization code needed by a service.
   *
   * This method will only ever be called once during the lifecycle of
   * a specific service instance.
   *
   * Implementations do not need to be synchronized as the logic
   * in {@link #init(Configuration)} prevents re-entrancy.
   *
   * The base implementation checks to see if the subclass has created
   * a new configuration instance, and if so, updates the base class value
   * @param conf configuration
   * @throws Exception on a failure -these will be caught,
   * possibly wrapped, and will trigger a service stop",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,serviceStop,org.apache.hadoop.util.JvmPauseMonitor:serviceStop(),88,100,"/**
 * Stops the service, interrupts the monitor thread, and joins it.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,serviceStart,org.apache.hadoop.service.CompositeService:serviceStart(),115,126,"/**
 * Starts all registered services.
 * Starts each service; throws exception if any fail.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,addIfService,org.apache.hadoop.service.CompositeService:addIfService(java.lang.Object),88,95,"/**
 * Adds a Service object if it's an instance of Service.
 * @param object The object to check and potentially add.
 * @return True if added, false otherwise.
 */
","* If the passed object is an instance of {@link Service},
   * add it to the list of services managed by this {@link CompositeService}
   * @param object object.
   * @return true if a service is added, false otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceOperations.java,stopQuietly,"org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.commons.logging.Log,org.apache.hadoop.service.Service)",79,88,"/**
 * Stops a service, logging warnings on failure.
 * @param log  Logger instance for warnings.
 * @param service Service to stop. Returns exception if failed.
 */
","* Stop a service; if it is null do nothing. Exceptions are caught and
   * logged at warn level. (but not Throwables). This operation is intended to
   * be used in cleanup operations
   *
   * @param log the log to warn at
   * @param service a service; may be null
   * @return any exception that was caught; null if none was.
   * @deprecated to be removed with 3.4.0. Use {@link #stopQuietly(Logger, Service)} instead.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceOperations.java,stopQuietly,"org.apache.hadoop.service.ServiceOperations:stopQuietly(org.slf4j.Logger,org.apache.hadoop.service.Service)",100,108,"/**
 * Stops a service, logs warnings on failure, returns exception.
 */
","* Stop a service; if it is null do nothing. Exceptions are caught and
   * logged at warn level. (but not Throwables). This operation is intended to
   * be used in cleanup operations
   *
   * @param log the log to warn at
   * @param service a service; may be null
   * @return any exception that was caught; null if none was.
   * @see ServiceOperations#stopQuietly(Service)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$ProgressableOption:<init>(org.apache.hadoop.util.Progressable),969,971,"/**
 * Constructs a ProgressableOption with the given Progressable value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ShortWritable.java,<init>,org.apache.hadoop.io.ShortWritable:<init>(short),37,39,"/**
 * Constructs a ShortWritable with the given short value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$LengthOption:<init>(long),1874,1876,"/**
 * Initializes a LengthOption with the given value.
 * @param value The length value for this option.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$StartOption:<init>(long),1867,1869,"/**
 * Constructs a StartOption with the given long value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$BlockSizeOption:<init>(long),925,927,"/**
 * Initializes a BlockSizeOption with the given value.
 * @param value The block size value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,read,org.apache.hadoop.io.DataInputByteBuffer$Buffer:read(),31,37,"/**
 * Reads a single byte from the input stream.
 * Returns the byte as an integer or -1 if EOF.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,reset,org.apache.hadoop.io.DataInputByteBuffer:reset(java.nio.ByteBuffer[]),83,85,"/**
 * Resets the internal buffers using the provided input buffers.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,byteBufferPositionedReadable_readFullyAvailable,org.apache.hadoop.io.wrappedio.WrappedIO:byteBufferPositionedReadable_readFullyAvailable(java.io.InputStream),233,246,"/**
 * Checks if an InputStream has pread capability.
 * @param in The InputStream to check.
 * @return True if pread capability exists, false otherwise.
 */
","* Probe to see if the input stream is an instance of ByteBufferPositionedReadable.
   * If the stream is an FSDataInputStream, the wrapped stream is checked.
   * @param in input stream
   * @return true if the stream implements the interface (including a wrapped stream)
   * and that it declares the stream capability.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,isAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:isAvailable(),433,435,"/**
 * Checks if the singleton instance is loaded.
 * @return True if loaded, false otherwise.
 */
","* Is the wrapped IO class loaded?
   * @return true if the instance is loaded.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,toString,org.apache.hadoop.util.JsonSerialization:toString(java.lang.Object),352,359,"/**
 * Converts an instance to a JSON string.
 * @param instance The object to convert.
 * @return JSON string representation or error message.
 */
","* Convert an instance to a string form for output. This is a robust
   * operation which will convert any JSON-generating exceptions into
   * error text.
   * @param instance non-null instance
   * @return a JSON string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FunctionalIO.java,toUncheckedFunction,org.apache.hadoop.util.functional.FunctionalIO:toUncheckedFunction(org.apache.hadoop.util.functional.FunctionRaisingIOE),84,86,"/**
 * Converts a FunctionRaisingIOE to an UncheckedFunction.
 * @param fun The function to convert.
 * @return An UncheckedFunction wrapping the input.
 */
","* Convert a {@link FunctionRaisingIOE} as a {@link Supplier}.
   * @param fun function to wrap
   * @param <T> type of input
   * @param <R> type of return value.
   * @return a new function which invokes the inner function and wraps
   * exceptions.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,fromInstance,org.apache.hadoop.util.JsonSerialization:fromInstance(java.lang.Object),236,238,"/**
 * Converts an instance to JSON and back to the original type.
 * @param instance The instance to convert.
 * @return The original instance after JSON serialization/deserialization.
 */
","* clone by converting to JSON and back again.
   * This is much less efficient than any Java clone process.
   * @param instance instance to duplicate
   * @return a new instance
   * @throws IOException IO problems.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,fromBytes,org.apache.hadoop.util.JsonSerialization:fromBytes(byte[]),331,333,"/**
 * Parses a byte array as JSON and returns an object of type T.
 * @param bytes Byte array containing JSON data.
 * @return Object of type T parsed from JSON.
 * @throws IOException If an I/O error occurs.
 */
","* Deserialize from a byte array.
   * @param bytes byte array
   * @throws IOException IO problems
   * @throws EOFException not enough data
   * @return byte array.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VIntWritable.java,<init>,org.apache.hadoop.io.VIntWritable:<init>(int),38,38,"/**
* Constructs a VIntWritable with the given integer value.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,equals,org.apache.hadoop.io.ElasticByteBufferPool$Key:equals(java.lang.Object),57,68,"/**
 * Checks if this Key is equal to another Key using compareTo.
 * @param rhs the Key object to compare to
 * @return true if equal, false otherwise
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$SyncIntervalOption:<init>(int),1009,1013,"/**
 * Initializes the SyncIntervalOption with a value,
 * defaulting to SYNC_INTERVAL if negative.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$ReplicationOption:<init>(int),932,934,"/**
 * Constructs a ReplicationOption with the given integer value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$BufferSizeOption:<init>(int),1881,1883,"/**
 * Constructs a BufferSizeOption with the given value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$BufferSizeOption:<init>(int),919,921,"/**
 * Constructs a BufferSizeOption with the given value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,<init>,org.apache.hadoop.io.UTF8:<init>(org.apache.hadoop.io.UTF8),78,80,"/**
 * Constructs a new UTF8 object by copying from an existing one.
 */","* Construct from a given string.
   * @param utf8 input utf8.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,writeString,"org.apache.hadoop.io.UTF8:writeString(java.io.DataOutput,java.lang.String)",351,365,"/**
* Writes a string to a DataOutput, truncating if too long.
* @param out DataOutput to write to
* @param s String to write
* @return Length of the written string
*/
","* @return Write a UTF-8 encoded string.
   *
   * @see DataOutput#writeUTF(String)
   * @param out input out.
   * @param s input s.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,skip,org.apache.hadoop.io.UTF8:skip(java.io.DataInput),144,147,"/**
 * Skips a specified number of bytes from the input stream.
 * @param in DataInput stream to skip bytes from.
 */
","* Skips over one UTF8 in the input.
   * @param in datainput.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,skipCompressedByteArray,org.apache.hadoop.io.WritableUtils:skipCompressedByteArray(java.io.DataInput),54,59,"/**
 * Skips a compressed byte array from the input.
 * Reads length, then skips 'length' bytes from input.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compare,"org.apache.hadoop.io.SequenceFile$Sorter$SortPass$SeqFileComparator:compare(org.apache.hadoop.io.IntWritable,org.apache.hadoop.io.IntWritable)",3258,3263,"/**
 * Compares two IntWritable objects using a provided comparator.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,org.apache.hadoop.io.SetFile:<init>(),35,35,"/**
* Private constructor to prevent external instantiation of SetFile.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,org.apache.hadoop.io.ArrayFile:<init>(),35,35,"/**
 * Private constructor prevents external instantiation of ArrayFile.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/LongWritable.java,<init>,org.apache.hadoop.io.LongWritable:<init>(long),37,37,"/**
* Constructs a LongWritable with the given long value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,seek,org.apache.hadoop.io.ArrayFile$Reader:seek(long),112,115,"/**
 * Seeks to the specified position in the input stream.
 * @param n the offset in the stream to seek to
 */
","* Positions the reader before its <code>n</code>th value.
     *
     * @param n n key.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,get,"org.apache.hadoop.io.ArrayFile$Reader:get(long,org.apache.hadoop.io.Writable)",147,151,"/**
 * Retrieves a value associated with the given key.
 * @param n the key
 * @param value the value to be used if not found
 * @return The retrieved value, or the provided value if not found.
 */
","* Return the <code>n</code>th value in the file.
     * @param n n key.
     * @param value value.
     * @throws IOException raised on errors performing I/O.
     * @return writable.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$ValueClassOption:<init>(java.lang.Class),952,954,"/**
 * Constructs a ValueClassOption with the given class.
 * @param value The class to be wrapped.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,org.apache.hadoop.io.MapFile$Writer$KeyClassOption:<init>(java.lang.Class),270,272,"/**
 * Constructs a KeyClassOption with the given class.
 * @param value The class associated with this option.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$KeyClassOption:<init>(java.lang.Class),945,947,"/**
 * Constructs a KeyClassOption with the given class.
 * @param value The class associated with this option.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ByteWritable.java,<init>,org.apache.hadoop.io.ByteWritable:<init>(byte),34,34,"/**
 * Constructs a ByteWritable with the given byte value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,<init>,org.apache.hadoop.io.OutputBuffer:<init>(),71,73,"/**
 * Constructs a new OutputBuffer using a default Buffer.
 */",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,getData,org.apache.hadoop.io.OutputBuffer:getData(),86,86,"/**
 * Returns the underlying data buffer.
 * @return byte array containing the data.
 */
","* Returns the current contents of the buffer.
   *  Data is only valid to {@link #getLength()}.
   *
   * @return the current contents of the buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,getLength,org.apache.hadoop.io.OutputBuffer:getLength(),93,93,"/**
 * Returns the length of the underlying buffer.
 */","* Returns the length of the valid data currently in the buffer.
   * @return the length of the valid data
   *          currently in the buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,reset,org.apache.hadoop.io.OutputBuffer:reset(),96,99,"/**
 * Resets the internal buffer to its initial state.
 * Returns a reference to this OutputBuffer object.
 */
",@return Resets the buffer to empty.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,compare,"org.apache.hadoop.io.WritableComparator:compare(java.lang.Object,java.lang.Object)",215,218,"/**
 * Compares two objects, casting them to WritableComparable.
 */","* Compare two Object.
   *
   * @param a the first object to be compared.
   * @param b the second object to be compared.
   * @return compare result.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,binarySearch,org.apache.hadoop.io.MapFile$Reader:binarySearch(org.apache.hadoop.io.WritableComparable),782,799,"/**
 * Searches for the key using binary search.
 * @param key The key to search for.
 * @return Index of the key if found, - (insertion point) otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,compareBytes,"org.apache.hadoop.io.WritableComparator:compareBytes(byte[],int,int,byte[],int,int)",230,233,"/**
 * Compares two byte arrays from specified start offsets and lengths.
 */","* Lexicographic order of binary data.
   * @param b1 b1.
   * @param s1 s1.
   * @param l1 l1.
   * @param b2 b2.
   * @param s2 s2.
   * @param l2 l2.
   * @return compare bytes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,hashBytes,"org.apache.hadoop.io.WritableComparator:hashBytes(byte[],int)",255,257,"/**
 * Calculates a hash value for the first 'length' bytes of 'bytes'.
 */","* Compute hash for binary data.
   * @param bytes bytes.
   * @param length length.
   * @return hash for binary data.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readFloat,"org.apache.hadoop.io.WritableComparator:readFloat(byte[],int)",290,292,"/**
 * Reads a float from the byte array, starting at the given index.
 */","* Parse a float from a byte array.
   * @param bytes bytes.
   * @param start start.
   * @return float from a byte array",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readLong,"org.apache.hadoop.io.WritableComparator:readLong(byte[],int)",300,303,"/**
 * Reads a long value from a byte array, starting at the given index.
 */","* Parse a long from a byte array.
   * @param bytes bytes.
   * @param start start.
   * @return long from a byte array",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readVInt,"org.apache.hadoop.io.WritableComparator:readVInt(byte[],int)",347,349,"/**
 * Reads a variable-length integer from the byte array.
 * @param bytes byte array to read from
 * @param start starting index in the byte array
 * @return The integer value read from the byte array.
 */
","* Reads a zero-compressed encoded integer from a byte array and returns it.
   * @param bytes byte array with the encoded integer
   * @param start start index
   * @throws IOException raised on errors performing I/O.
   * @return deserialized integer",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,<init>,org.apache.hadoop.io.BytesWritable:<init>(byte[]),61,63,"/**
 * Constructs a BytesWritable with the given byte array.
 * @param bytes The byte array to be wrapped.
 */
","* Create a BytesWritable using the byte array as the initial value.
   * @param bytes This array becomes the backing storage for the object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,get,org.apache.hadoop.io.BytesWritable:get(),102,105,"/**
 * Deprecated method; use getBytes() instead.
 */","* Get the data from the BytesWritable.
   * @deprecated Use {@link #getBytes()} instead.
   * @return data from the BytesWritable.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,getSize,org.apache.hadoop.io.BytesWritable:getSize(),120,123,"/**
 * Returns the size of the data structure. Deprecated, use getLength().
 */","* Get the current size of the buffer.
   * @deprecated Use {@link #getLength()} instead.
   * @return current size of the buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,setCapacity,org.apache.hadoop.io.BytesWritable:setCapacity(int),155,160,"/**
 * Sets the capacity of the buffer, resizing if necessary.
 * @param capacity The new capacity for the buffer.
 */
","* Change the capacity of the backing storage. The data is preserved.
   *
   * @param capacity The new capacity in bytes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IntWritable.java,<init>,org.apache.hadoop.io.IntWritable:<init>(int),37,37,"/**
 * Constructs an IntWritable with the given integer value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,getBuffer,"org.apache.hadoop.io.ElasticByteBufferPool:getBuffer(boolean,int)",89,101,"/**
 * Retrieves a buffer of specified length, direct or indirect.
 * @param direct if true, allocates a direct buffer; otherwise indirect.
 * @param length desired buffer length.
 * @return A reusable ByteBuffer.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,putBuffer,org.apache.hadoop.io.ElasticByteBufferPool:putBuffer(java.nio.ByteBuffer),103,119,"/**
 * Adds a buffer to the tree map, indexed by capacity and timestamp.
 * @param buffer The ByteBuffer to add.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ElasticByteBufferPool.java,size,org.apache.hadoop.io.ElasticByteBufferPool:size(boolean),127,131,"/**
 * Returns the size of the buffer tree, based on the 'direct' flag.
 */","* Get the size of the buffer pool, for the specified buffer type.
   *
   * @param direct Whether the size is returned for direct buffers
   * @return The size",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable$Internal:<init>(),159,161,"/**
 * Default constructor for read operations.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,updateProgress,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:updateProgress(long),3611,3616,"/**
 * Updates the progress based on the number of bytes processed.
 * @param bytesProcessed Number of bytes processed in this step.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ReadaheadPool.java,readaheadStream,"org.apache.hadoop.io.ReadaheadPool:readaheadStream(java.lang.String,java.io.FileDescriptor,long,long,long,org.apache.hadoop.io.ReadaheadPool$ReadaheadRequest)",102,149,"/**
 * Schedules a readahead request or returns the last one.
 * @param identifier stream identifier, fd file descriptor,
 * curPos current position, readaheadLength length to read,
 * maxOffsetToRead max offset, lastReadahead previous request.
 */
","* Issue a request to readahead on the given file descriptor.
   * 
   * @param identifier a textual identifier that will be used in error
   * messages (e.g. the file name)
   * @param fd the file descriptor to read ahead
   * @param curPos the current offset at which reads are being issued
   * @param readaheadLength the configured length to read ahead
   * @param maxOffsetToRead the maximum offset that will be readahead
   *        (useful if, for example, only some segment of the file is
   *        requested by the user). Pass {@link Long#MAX_VALUE} to allow
   *        readahead to the end of the file.
   * @param lastReadahead the result returned by the previous invocation
   *        of this function on this file descriptor, or null if this is
   *        the first call
   * @return an object representing this outstanding request, or null
   *        if no readahead was performed",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,append,org.apache.hadoop.io.SetFile$Writer:append(org.apache.hadoop.io.WritableComparable),97,99,"/**
 * Appends a key-value pair to the file, value defaults to NullWritable.
 * @param key The key to append.
 * @throws IOException if an I/O error occurs.
 */
","* Append a key to a set.  The key must be strictly greater than the
     * previous key added to the set.
     * @param key input key.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,next,org.apache.hadoop.io.SetFile$Reader:next(org.apache.hadoop.io.WritableComparable),144,147,"/**
* Advances to the next key/value pair.
* @param key The key to return the next pair for.
* @return True if a next pair exists, false otherwise.
*/
","* Read the next key in a set into <code>key</code>.
     *
     * @param key input key.
     * @return Returns true if such a key exists
     *    and false when at the end of the set.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,compare,"org.apache.hadoop.io.Text$Comparator:compare(byte[],int,int,byte[],int,int)",433,439,"/**
 * Compares two byte arrays based on variable-length integers.
 * @param b1,s1,l1 first array, start, length
 * @param b2,s2,l2 second array, start, length
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,key,org.apache.hadoop.io.ArrayFile$Reader:key(),136,138,"/**
 * Retrieves the key value.
 * @return The key as a long.
 */
","* Returns the key associated with the most recent call to {@link
     * #seek(long)}, {@link #next(Writable)}, or {@link
     * #get(long,Writable)}.
     *
     * @return key key.
     * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$InputStreamOption:<init>(org.apache.hadoop.fs.FSDataInputStream),1860,1862,"/**
 * Constructs an InputStreamOption with the given FSDataInputStream.
 * @param value The FSDataInputStream to wrap.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,comparator,org.apache.hadoop.io.MapFile$Writer:comparator(org.apache.hadoop.io.WritableComparator),289,291,"/**
 * Creates a ComparatorOption using the provided WritableComparator.
 * @param value The WritableComparator to use.
 * @return A ComparatorOption instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DoubleWritable.java,<init>,org.apache.hadoop.io.DoubleWritable:<init>(double),41,43,"/**
 * Constructs a DoubleWritable with the given double value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VersionedWritable.java,readFields,org.apache.hadoop.io.VersionedWritable:readFields(java.io.DataInput),49,54,"/**
 * Reads fields from DataInput, validating version.
 * @param in DataInput to read from.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,comparator,org.apache.hadoop.io.MapFile$Reader:comparator(org.apache.hadoop.io.WritableComparator),476,478,"/**
 * Creates a ComparatorOption using the provided WritableComparator.
 * @param value The WritableComparator to use.
 * @return A ComparatorOption instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,access,"org.apache.hadoop.io.nativeio.NativeIO$Windows:access(java.lang.String,org.apache.hadoop.io.nativeio.NativeIO$Windows$AccessRight)",815,818,"/**
 * Checks file access permission.
 * @param path File path to check.
 * @param desiredAccess Desired access right.
 * @return True if access is granted, false otherwise.
 */
","* Checks whether the current process has desired access rights on
     * the given path.
     * 
     * Longer term this native function can be substituted with JDK7
     * function Files#isReadable, isWritable, isExecutable.
     *
     * @param path input path
     * @param desiredAccess ACCESS_READ, ACCESS_WRITE or ACCESS_EXECUTE
     * @return true if access is allowed
     * @throws IOException I/O exception on error",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,isAvailable,org.apache.hadoop.io.nativeio.NativeIO$POSIX:isAvailable(),360,362,"/**
 * Checks if native code is loaded.
 * @return True if native code is loaded, false otherwise.
 */
",* @return Return true if the JNI-based native IO extensions are available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,isAvailable,org.apache.hadoop.io.nativeio.NativeIO:isAvailable(),869,871,"/**
* Checks if native code is loaded and nativeLoaded is true.
*/",* @return Return true if the JNI-based native IO extensions are available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.java,<init>,org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:<init>(),38,48,"/**
 * Initializes the Unix groups mapping implementation.
 * Uses JNI if available, otherwise falls back to shell.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.java,<init>,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:<init>(),37,47,"/**
 * Initializes the group mapping implementation, using JNI if available.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/NativeCrc32.java,isAvailable,org.apache.hadoop.util.NativeCrc32:isAvailable(),35,41,"/**
 * Checks if the native code is loaded, false if on SPARC.
 */",* Return true if the JNI-based native CRC extensions are available.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,setPmdkSupportState,org.apache.hadoop.io.nativeio.NativeIO$POSIX:setPmdkSupportState(int),173,181,"/**
 * Sets the PMDK support state based on the provided state code.
 * @param stateCode The integer code representing the state.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getPmdkSupportStateMessage,org.apache.hadoop.io.nativeio.NativeIO$POSIX:getPmdkSupportStateMessage(),183,189,"/**
 * Returns a PMDK support state message, including lib path if available.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,isPmdkAvailable,org.apache.hadoop.io.nativeio.NativeIO$POSIX:isPmdkAvailable(),191,194,"/**
 * Checks if PMDK is available based on the current support state.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,chmod,"org.apache.hadoop.io.nativeio.NativeIO$POSIX:chmod(java.lang.String,int)",387,404,"/**
* Changes file mode. Calls chmodImpl or handles NativeIO errors.
* @param path file path
* @param mode new file mode
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,posixFadviseIfPossible,"org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:posixFadviseIfPossible(java.lang.String,java.io.FileDescriptor,long,long,int)",293,298,"/**
 * Calls posix_fadvise_if_possible, throws NativeIOException on failure.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,munmap,org.apache.hadoop.io.nativeio.NativeIO$POSIX:munmap(java.nio.MappedByteBuffer),491,501,"/**
 * Unmaps the given MappedByteBuffer. Uses CleanerUtil if supported.
 */","* Unmaps the block from memory. See munmap(2).
     *
     * There isn't any portable way to unmap a memory region in Java.
     * So we use the sun.nio method here.
     * Note that unmapping a memory region could cause crashes if code
     * continues to reference the unmapped code.  However, if we don't
     * manually unmap the memory, we are dependent on the finalizer to
     * do it, and we have no idea when the finalizer will run.
     *
     * @param buffer    The buffer to unmap.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,freeDB,org.apache.hadoop.crypto.CryptoStreamUtils:freeDB(java.nio.ByteBuffer),47,57,"/**
 * Frees a ByteBuffer, using cleaner if supported, logs otherwise.
 */","* Forcibly free the direct buffer.
   *
   * @param buffer buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getName,"org.apache.hadoop.io.nativeio.NativeIO$POSIX:getName(org.apache.hadoop.io.nativeio.NativeIO$POSIX$IdCache,int)",629,648,"/**
 * Retrieves a name for a given ID, caching the result.
 * @param domain IdCache enum (USER or GROUP)
 * @param id ID of the entity
 * @return Name associated with the ID
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getOperatingSystemPageSize,org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:getOperatingSystemPageSize(),289,291,"/**
 * Returns the operating system page size.
 * Uses NativeIO to retrieve the value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,memSync,org.apache.hadoop.io.nativeio.NativeIO$POSIX$Pmem:memSync(org.apache.hadoop.io.nativeio.NativeIO$POSIX$PmemMappedRegion),252,258,"/**
 * Synchronizes memory region. Drains if PMEM, syncs otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayWritable.java,<init>,"org.apache.hadoop.io.ArrayWritable:<init>(java.lang.Class,org.apache.hadoop.io.Writable[])",57,60,"/**
* Constructs an ArrayWritable with a given value class and values.
* @param valueClass Class of the elements in the array.
* @param values The array of Writable objects.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BoundedByteArrayOutputStream.java,<init>,"org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(byte[],int,int)",59,61,"/**
 * Initializes with a buffer, offset, and limit.
 * @param buf The buffer.
 * @param offset The offset within the buffer.
 * @param limit The limit of the buffer to use.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,write,"org.apache.hadoop.io.DataOutputBuffer:write(java.io.DataInput,int)",132,134,"/**
* Writes data from the input stream to the buffer.
* @param in DataInput stream to read from
* @param length Number of bytes to write
*/
","* Writes bytes from a DataInput directly into the buffer.
   * @param in data input.
   * @param length length.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/TwoDArrayWritable.java,<init>,"org.apache.hadoop.io.TwoDArrayWritable:<init>(java.lang.Class,org.apache.hadoop.io.Writable[][])",38,41,"/**
* Constructs a TwoDArrayWritable with specified value class and array.
* @param valueClass Class of the array elements.
* @param values The 2D array to be wrapped.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,<init>,"org.apache.hadoop.io.EnumSetWritable:<init>(java.util.EnumSet,java.lang.Class)",70,72,"/**
* Constructs an EnumSetWritable with the given EnumSet and element type.
*/","* Construct a new EnumSetWritable. If the <tt>value</tt> argument is null or
   * its size is zero, the <tt>elementType</tt> argument must not be null. If
   * the argument <tt>value</tt>'s size is bigger than zero, the argument
   * <tt>elementType</tt> is not be used.
   * 
   * @param value enumSet value.
   * @param elementType elementType.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readStringArray,org.apache.hadoop.io.WritableUtils:readStringArray(java.io.DataInput),165,173,"/**
 * Reads a String array of specified length from the DataInput.
 * @param in DataInput to read from. Returns null if len is -1.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeStringArray,"org.apache.hadoop.io.WritableUtils:writeStringArray(java.io.DataOutput,java.lang.String[])",136,141,"/**
 * Writes a string array to a DataOutput.
 * @param out Output stream.
 * @param s String array to write.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,equals,org.apache.hadoop.io.SortedMapWritable:equals(java.lang.Object),200,216,"/**
 * Checks if two SortedMapWritable objects are equal.
 * Compares size and entry sets for equality.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,write,"org.apache.hadoop.io.OutputBuffer$Buffer:write(java.io.InputStream,int)",56,65,"/**
 * Writes data from an input stream into the internal buffer.
 * @param in Input stream to read from.
 * @param len Number of bytes to read.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,fillReservoir,org.apache.hadoop.crypto.random.OsSecureRandom:fillReservoir(int),61,73,"/**
 * Fills the reservoir with data from the stream if needed.
 * Refills stream if close to reservoir end.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,<init>,org.apache.hadoop.io.DataInputBuffer:<init>(),134,136,"/**
 * Constructs a DataInputBuffer with a new internal buffer.
 */",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,reset,"org.apache.hadoop.io.DataInputBuffer:reset(byte[],int)",149,151,"/**
* Resets the internal buffer with data from the input array.
* @param input The byte array to reset the buffer with.
* @param length The number of bytes to use from the array.
*/
","* Resets the data that the buffer reads.
   *
   * @param input input.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,reset,"org.apache.hadoop.io.DataInputBuffer:reset(byte[],int,int)",160,162,"/**
 * Resets the internal buffer with data from the input array.
 * @param input Input byte array.
 * @param start Start index in the input array.
 * @param length Length of data to copy.
 */
","* Resets the data that the buffer reads.
   *
   * @param input input.
   * @param start start.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,getData,org.apache.hadoop.io.DataInputBuffer:getData(),164,166,"/**
 * Returns the underlying data buffer.
 * @return byte array containing the data.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,getPosition,org.apache.hadoop.io.DataInputBuffer:getPosition(),173,173,"/**
 * Returns the current position of the buffer.
 */","* Returns the current position in the input.
   *
   * @return position.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputBuffer.java,getLength,org.apache.hadoop.io.DataInputBuffer:getLength(),181,181,"/**
 * Returns the length of the underlying buffer.
 */","* Returns the index one greater than the last valid character in the input
   * stream buffer.
   *
   * @return length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECSchema.java,<init>,org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.util.Map),69,93,"/**
 * Constructs an ECSchema from provided options.
 * @param allOptions Map of schema options; must contain codec name.
 */
","* Constructor with schema name and provided all options. Note the options may
   * contain additional information for the erasure codec to interpret further.
   * @param allOptions all schema options",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECSchema.java,<init>,"org.apache.hadoop.io.erasurecode.ECSchema:<init>(java.lang.String,int,int)",101,103,"/**
 * Constructs an ECSchema with codec name, data units, and parity units.
 */","* Constructor with key parameters provided.
   * @param codecName codec name
   * @param numDataUnits number of data units used in the schema
   * @param numParityUnits number os parity units used in the schema",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,setCodecOptions,org.apache.hadoop.io.erasurecode.codec.ErasureCodec:setCodecOptions(org.apache.hadoop.io.erasurecode.ErasureCodecOptions),65,68,"/**
 * Sets codec options and schema.
 * @param options Codec options to set.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,getRequiredNumDataBlocks,org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:getRequiredNumDataBlocks(),54,56,"/**
 * Returns the number of data blocks required by the schema.
 */","* Get required data blocks count in a BlockGroup.
   * @return count of required data blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,getRequiredNumParityBlocks,org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:getRequiredNumParityBlocks(),62,64,"/**
 * Returns the number of parity blocks required by the schema.
 */","* Get required parity blocks count in a BlockGroup.
   * @return count of required parity blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.ErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",40,47,"/**
 * Initializes an ErasureCodec with configuration and options.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ErasureCoderOptions.java,<init>,"org.apache.hadoop.io.erasurecode.ErasureCoderOptions:<init>(int,int)",34,36,"/**
 * Creates ErasureCoderOptions with default settings.
 * @param numDataUnits Number of data units.
 * @param numParityUnits Number of parity units.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,getName,org.apache.hadoop.io.erasurecode.codec.ErasureCodec:getName(),49,51,"/**
 * Returns the codec name associated with the schema.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/ErasureCodec.java,createBlockGrouper,org.apache.hadoop.io.erasurecode.codec.ErasureCodec:createBlockGrouper(),86,91,"/**
 * Creates and configures a BlockGrouper instance.
 * Sets the schema from the current schema.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecRegistry.java,<init>,org.apache.hadoop.io.erasurecode.CodecRegistry:<init>(),62,69,"/**
 * Initializes the CodecRegistry with coders from ServiceLoader.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecRegistry.java,getCoderByName,"org.apache.hadoop.io.erasurecode.CodecRegistry:getCoderByName(java.lang.String,java.lang.String)",161,172,"/**
 * Finds a RawErasureCoderFactory by coder name.
 * @param codecName codec name
 * @param coderName coder name to search for
 * @return RawErasureCoderFactory or null if not found
 */
","* Get a specific coder factory defined by codec name and coder name.
   * @param codecName name of the codec
   * @param coderName name of the coder
   * @return the specific coder, null if not exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,makeBlockGroup,"org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:makeBlockGroup(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[])",72,77,"/**
 * Creates an ECBlockGroup from data and parity blocks.
 * @param dataBlocks Data blocks for the group.
 * @param parityBlocks Parity blocks for the group.
 * @return A new ECBlockGroup object.
 */
","* Calculating and organizing BlockGroup, to be called by ECManager
   * @param dataBlocks Data blocks to compute parity blocks against
   * @param parityBlocks To be computed parity blocks
   * @return ECBlockGroup.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECBlockGroup.java,getErasedCount,org.apache.hadoop.io.erasurecode.ECBlockGroup:getErasedCount(),61,73,"/**
 * Returns the total count of erased data and parity blocks.
 */","* Get erased blocks count
   * @return erased count of blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getNumErasedBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumErasedBlocks(org.apache.hadoop.io.erasurecode.ECBlock[]),143,152,"/**
 * Counts the number of erased blocks in the input array.
 * @param inputBlocks array of ECBlock objects
 * @return the number of erased blocks
 */
","* Find out how many blocks are erased.
   * @param inputBlocks all the input blocks
   * @return number of erased blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,hasCodec,org.apache.hadoop.io.erasurecode.CodecUtil:hasCodec(java.lang.String),163,165,"/**
 * Checks if a codec with the given name exists in the registry.
 * @param codecName Name of the codec to check.
 * @return True if codec exists, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECChunk.java,toBuffers,org.apache.hadoop.io.erasurecode.ECChunk:toBuffers(org.apache.hadoop.io.erasurecode.ECChunk[]),83,97,"/**
 * Converts an array of ECChunks to an array of ByteBuffers.
 * @param chunks Array of ECChunk objects to convert.
 * @return Array of ByteBuffer objects.
 */
","* Convert an array of this chunks to an array of ByteBuffers
   * @param chunks chunks to convert into buffers
   * @return an array of ByteBuffers",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,release,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:release(),61,66,"/**
 * Releases resources held by the raw encoder, if it exists.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,release,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:release(),78,86,"/**
 * Releases resources held by the encoder instances.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncoder.java,getOutputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),70,72,"/**
 * Retrieves parity blocks from the given block group.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureDecoder.java,getOutputBlocks,org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),59,84,"/**
 * Retrieves erased blocks from a block group for recovery.
 * @param blockGroup Group of EC blocks; contains erased blocks.
 * @return Array of ECBlock objects representing erased blocks.
 */
","* Which blocks were erased ? For XOR it's simple we only allow and return one
   * erased block, either data or parity.
   * @param blockGroup blockGroup.
   * @return output blocks to recover",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncoder.java,getInputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:getInputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),66,68,"/**
 * Retrieves input blocks from the given block group.
 * @param blockGroup The ECBlockGroup to get blocks from.
 * @return An array of ECBlock objects.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,getNumDataUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumDataUnits(),175,177,"/**
 * Returns the number of data units configured in coder options.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,getNumDataUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumDataUnits(),152,154,"/**
 * Returns the number of data units configured in coder options.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,getNumParityUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumParityUnits(),179,181,"/**
 * Returns the number of parity units configured.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,getNumParityUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumParityUnits(),156,158,"/**
 * Returns the number of parity units configured.
 * @return The number of parity units.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getInputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getInputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),71,82,"/**
 * Extracts data and parity blocks from a block group.
 * @param blockGroup The block group to extract from.
 * @return An array containing the data and parity blocks.
 */
","* We have all the data blocks and parity blocks as input blocks for
   * recovering by default. It's codec specific
   * @param blockGroup blockGroup.
   * @return input blocks",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,release,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:release(),60,65,"/**
 * Releases resources held by the rsRawDecoder, if it exists.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,release,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:release(),88,96,"/**
 * Releases resources held by the decoder and encoder.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),35,37,"/**
 * Constructs an XORRawDecoder with the given options.
 * @param coderOptions ErasureCoderOptions object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Constructs a DummyRawDecoder with the given options.
 * @param coderOptions ErasureCoderOptions object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),43,45,"/**
 * Constructs a NativeRawDecoder with provided ErasureCoderOptions.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawDecoder.java,preferDirectBuffer,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:preferDirectBuffer(),101,104,"/**
 * Indicates if direct buffers are preferred.
 * Returns true to signify preference.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawEncoder.java,preferDirectBuffer,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:preferDirectBuffer(),98,101,"/**
 * Indicates if direct buffers are preferred.
 * Returns true to signify preference.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,add,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:add(int,int)",150,153,"/**
 * Calculates the XOR of two integers.
 * @param x The first integer.
 * @param y The second integer.
 * @return The XOR result of x and y.
 */
","* Compute the sum of two fields
   *
   * @param x input field
   * @param y input field
   * @return result of addition",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,multiply,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:multiply(int,int)",162,165,"/**
 * Multiplies two integers using a precomputed multiplication table.
 */","* Compute the multiplication of two fields
   *
   * @param x input field
   * @param y input field
   * @return result of multiplication",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,divide,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:divide(int,int)",174,177,"/**
 * Divides x by y using a precomputed division table.
 * @param x dividend
 * @param y divisor
 * @return The result of x divided by y.
 */
","* Compute the division of two fields
   *
   * @param x input field
   * @param y input field
   * @return x/y",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,power,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:power(int,int)",186,200,"/**
 * Calculates x to the power of n using precomputed tables.
 * @param x base value
 * @param n exponent
 * @return result of x^n
 */
","* Compute power n of a field
   *
   * @param x input field
   * @param n power
   * @return x^n",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/DumpUtil.java,dumpChunk,org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpChunk(org.apache.hadoop.io.erasurecode.ECChunk),93,102,"/**
 * Prints a chunk's data as a hexadecimal string to the console.
 * @param chunk The ECChunk object to dump.
 */
","* Print data in hex format in a chunk.
   * @param chunk chunk.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),43,45,"/**
 * Constructs a NativeRawEncoder with the given options.
 * @param coderOptions Encoder options to use.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),35,37,"/**
 * Constructs an XORRawEncoder with provided options.
 * @param coderOptions ErasureCoderOptions for encoder configuration.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Constructs a DummyRawEncoder with the provided options.
 * @param coderOptions ErasureCoderOptions object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,getNumAllUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:getNumAllUnits(),183,185,"/**
 * Returns the number of all units from coder options.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,getNumAllUnits,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:getNumAllUnits(),160,162,"/**
 * Returns the number of all units from coder options.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,allowChangeInputs,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:allowChangeInputs(),202,204,"/**
 * Checks if changing inputs is allowed based on coder options.
 */","* Allow change into input buffers or not while perform encoding/decoding.
   * @return true if it's allowed to change inputs, false otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,allowChangeInputs,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:allowChangeInputs(),179,181,"/**
 * Checks if changing inputs is allowed based on coder options.
 * @return True if allowed, false otherwise.
 */
","* Allow change into input buffers or not while perform encoding/decoding.
   * @return true if it's allowed to change inputs, false otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,allowVerboseDump,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:allowVerboseDump(),210,212,"/**
 * Checks if verbose dump is allowed based on coder options.
 */","* Allow to dump verbose info during encoding/decoding.
   * @return true if it's allowed to do verbose dump, false otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,allowVerboseDump,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:allowVerboseDump(),187,189,"/**
 * Checks if verbose dump is allowed based on coder options.
 * @return True if verbose dump is allowed, false otherwise.
 */
","* Allow to dump verbose info during encoding/decoding.
   * @return true if it's allowed to do verbose dump, false otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecodeImpl,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecodeImpl(byte[][],int[],int,int[],byte[][],int[])",98,109,"/**
 * Decodes data by correcting errors using erasure coding.
 * @param inputs,inputOffsets,dataLen,erasedIndexes,outputs,outputOffsets decode parameters
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetBuffer,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetBuffer(java.nio.ByteBuffer,int)",63,69,"/**
 * Resets the buffer to a specified length.
 * @param buffer The buffer to reset.
 * @param len The length to reset to.
 * @return The reset buffer.
 */
","* Ensure a buffer filled with ZERO bytes from current readable/writable
   * position.
   * @param buffer a buffer ready to read / write certain size bytes
   * @return the buffer itself, with ZERO bytes written, the position and limit
   *         are not changed after the call",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetBuffer,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetBuffer(byte[],int,int)",77,82,"/**
* Copies an empty chunk to a buffer at a specified offset.
* @param buffer target buffer
* @param offset starting offset
* @param len number of bytes to copy
* @return the modified buffer
*/
","* Ensure the buffer (either input or output) ready to read or write with ZERO
   * bytes fully in specified length of len.
   * @param buffer bytes array buffer
   * @return the buffer itself",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferEncodingState.java,convertToByteArrayState,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:convertToByteArrayState(),62,84,"/**
 * Creates a ByteArrayEncodingState with encoded data and offsets.
 * @return A ByteArrayEncodingState object.
 */
",* Convert to a ByteArrayEncodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayEncodingState.java,convertToByteBufferState,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:convertToByteBufferState(),69,85,"/**
 * Creates a ByteBufferEncodingState with cloned inputs and new outputs.
 * @return ByteBufferEncodingState object.
 */
",* Convert to a ByteBufferEncodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,convertToByteBufferState,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:convertToByteBufferState(),73,89,"/**
 * Creates a ByteBufferDecodingState with cloned inputs and new outputs.
 * @return ByteBufferDecodingState object for decoding.
 */
",* Convert to a ByteBufferDecodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,convertToByteArrayState,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:convertToByteArrayState(),66,91,"/**
 * Creates a ByteArrayDecodingState object with decoded data and offsets.
 * @return ByteArrayDecodingState object containing decoding state.
 */
",* Convert to a ByteArrayDecodingState when it's backed by on-heap arrays.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,initTables,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:initTables(int,int,byte[],int,byte[])",47,58,"/**
 * Initializes Galois field vector multiplication tables.
 * @param k dimension, @param rows number of rows, @param matrix coding matrix
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,genCauchyMatrix,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:genCauchyMatrix(byte[],int,int)",67,80,"/**
 * Generates a Cauchy matrix in byte array 'a' of size m x k.
 */","* Ported from Intel ISA-L library.
   *
   * @param k k.
   * @param a a.
   * @param m m.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GF256.java,gfInvertMatrix,"org.apache.hadoop.io.erasurecode.rawcoder.util.GF256:gfInvertMatrix(byte[],byte[],int)",203,262,"/**
 * Calculates the inverse of a GF matrix.
 * @param inMatrix input matrix
 * @param outMatrix output matrix (inverse)
 * @param n matrix dimension
 */","* Invert a matrix assuming it's invertible.
   *
   * Ported from Intel ISA-L library.
   *
   * @param inMatrix inMatrix.
   * @param outMatrix outMatrix.
   * @param n n",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,encodeData,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:encodeData(byte[],int,byte[][],int[],byte[][],int[])",97,143,"/**
 * Encodes data using GF tables and input/output offsets.
 */","* Encode a group of inputs data and generate the outputs. It's also used for
   * decoding because, in this implementation, encoding and decoding are
   * unified.
   *
   * The algorithm is ported from Intel ISA-L library for compatible. It
   * leverages Java auto-vectorization support for performance.
   *
   * @param gfTables gfTables.
   * @param dataLen dataLen.
   * @param inputs inputs.
   * @param inputOffsets inputOffsets.
   * @param outputs outputs.
   * @param outputOffsets outputOffsets.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,encodeData,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:encodeData(byte[],java.nio.ByteBuffer[],java.nio.ByteBuffer[])",152,200,"/**
 * Encodes data using GF tables, inputs, and outputs.
 * @param gfTables GF tables for encoding.
 * @param inputs Input byte buffers.
 * @param outputs Output byte buffers.
 */
","* See above. Try to use the byte[] version when possible.
   *
   * @param gfTables gfTables.
   * @param inputs inputs.
   * @param outputs outputs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,getInstance,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getInstance(int,int)",102,115,"/**
 * Retrieves a GaloisField instance, creating it if it doesn't exist.
 * @param fieldSize Size of the field.
 * @param primitivePolynomial Primitive polynomial.
 * @return GaloisField instance.
 */
","* Get the object performs Galois field arithmetics.
   *
   * @param fieldSize           size of the field
   * @param primitivePolynomial a primitive polynomial corresponds to the size
   * @return GaloisField.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,solveVandermondeSystem,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:solveVandermondeSystem(int[],int[])",210,212,"/**
 * Solves Vandermonde system using given x and y values.
 * @param x x-coordinates of the points.
 * @param y y-coordinates of the points.
 */
","* Given a Vandermonde matrix V[i][j]=x[j]^i and vector y, solve for z such
   * that Vz=y. The output z will be placed in y.
   *
   * @param x the vector which describe the Vandermonde matrix
   * @param y right-hand side of the Vandermonde system equation. will be
   *          replaced the output in this vector",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/ECBlock.java,<init>,org.apache.hadoop.io.erasurecode.ECBlock:<init>(),37,39,"/**
 * Default constructor for ECBlock, initializes with default flags.
 */
",* A default constructor. isParity and isErased are false by default.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/LongWritable.java,compare,"org.apache.hadoop.io.LongWritable$DecreasingComparator:compare(byte[],int,int,byte[],int,int)",110,113,"/**
 * Compares two byte arrays using the superclass's compare method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,close,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:close(),444,453,"/**
 * Closes the input stream and calls the superclass's close method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,updatePos,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:updatePos(boolean),560,564,"/**
 * Updates the compressed stream position based on addOn flag.
 * @param shouldAddOn True to add 1 to the position, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,updateReportedByteCount,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:updateReportedByteCount(int),184,187,"/**
 * Updates reported byte count and processed byte count.
 * @param count The number of bytes to add to the counters.
 */
","* This method is called by the client of this
   * class in case there are any corrections in
   * the stream position.  One common example is
   * when client of this code removes starting BZ
   * characters from the compressed stream.
   *
   * @param count count bytes are added to the reported bytes
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,readAByte,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:readAByte(java.io.InputStream),196,202,"/**
 * Reads a single byte from the input stream.
 * @param inStream Input stream to read from.
 * @return Byte value or -1 if end of stream.
 */
","* This method reads a Byte from the compressed stream. Whenever we need to
  * read from the underlying compressed stream, this method should be called
  * instead of directly calling the read method of the underlying compressed
  * stream. This method does important record keeping to have the statistic
  * that how many bytes have been read off the compressed stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CRC.java,<init>,org.apache.hadoop.io.compress.bzip2.CRC:<init>(),86,88,"/**
 * Initializes the CRC calculation state.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,endBlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:endBlock(),572,589,"/**
* Calculates block CRC, checks for errors, and updates combined CRC.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,createHuffmanDecodingTables,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:createHuffmanDecodingTables(int,int)",793,819,"/**
 * Creates Huffman decoding tables for specified groups.
 * Populates minLens array with minimum code lengths.
 */",* Called by recvDecodingTables() exclusively.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,initBlock,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:initBlock(),774,786,"/**
 * Initializes the block, CRC, last index, and allowable block size.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,bsPutUByte,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsPutUByte(int),940,942,"/**
 * Writes an unsigned byte (c) to the bit stream, using 8 bits.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,bsPutInt,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:bsPutInt(int),944,949,"/**
 * Writes an integer to the bit stream, 4 bytes each.
 * @param u the integer to write
 * @throws IOException if an I/O error occurs
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues4,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues4(),1199,1241,"/**
 * Sends MTF values for the 4th block, updating usage flags.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues5,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues5(int,int)",1243,1278,"/**
 * Sends MTF values for a specified number of groups and selectors.
 * @param nGroups Number of groups.
 * @param nSelectors Number of selectors.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues1,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues1(int,int)",1029,1145,"/**
* Calculates MTF values, selects coding tables, and updates frequencies.
* @param nGroups Number of coding tables.
* @param alphaSize Size of the alphabet.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues3,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues3(int,int)",1174,1197,"/**
 * Assigns Huffman codes for each group.
 * @param nGroups Number of groups.
 * @param alphaSize Size of the alphabet.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,mainQSort3,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainQSort3(org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream$Data,int,int,int)",1634,1736,"/**
 * Recursive Quicksort implementation using a stack.
 * @param dataShadow Data object containing sort state.
 */","* Method ""mainQSort3"", file ""blocksort.c"", BZip2 1.0.2",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,<init>,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:<init>(),66,68,"/**
 * Creates a Bzip2Decompressor with default settings.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,setInput,"org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:setInput(byte[],int,int)",70,88,"/**
* Sets the input data buffer, offset, and length.
* Throws exceptions for invalid input.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,needsInput,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:needsInput(),112,130,"/**
 * Checks if more input is needed. Returns true if no input available.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,getBytesWritten,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getBytesWritten(),182,185,"/**
 * Returns the number of bytes written to the stream.
 * Uses the internal stream to retrieve the byte count.
 */
","* Returns the total number of uncompressed bytes output so far.
   *
   * @return the total (non-negative) number of uncompressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,getBytesRead,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getBytesRead(),192,195,"/**
 * Returns the number of bytes read from the stream.
 * Calls getBytesRead(stream) to retrieve the value.
 */
","* Returns the total number of compressed bytes input so far.
   *
   * @return the total (non-negative) number of compressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,getRemaining,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:getRemaining(),204,208,"/**
 * Gets the remaining bytes in the stream buffer.
 * Returns the number of bytes remaining.
 */
","* Returns the number of bytes remaining in the input buffers; normally
   * called when finished() is true to determine amount of post-gzip-stream
   * data.
   *
   * @return the total (non-negative) number of unprocessed bytes in input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Decompressor.java,reset,org.apache.hadoop.io.compress.bzip2.Bzip2Decompressor:reset(),213,223,"/**
 * Resets the stream to its initial state for reuse.
 */
",* Resets everything including the input buffers (user and direct).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,<init>,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>(),64,66,"/**
 * Creates a Bzip2Compressor with default block size and factors.
 */
","* Creates a new compressor with a default values for the
   * compression block size and work factor.  Compressed data will be
   * generated in bzip2 format.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,setInput,"org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:setInput(byte[],int,int)",124,142,"/**
* Sets the input data buffer, offset, and length.
* @param b The input byte array.
* @param off The offset within the array.
* @param len The number of bytes to read.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,needsInput,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:needsInput(),158,181,"/**
 * Checks if more input data is needed.
 * Returns true if no compressed/uncompressed data is available.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,getBytesWritten,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:getBytesWritten(),244,248,"/**
* Gets the number of bytes written to the stream.
* Delegates to the underlying stream's bytes written count.
*/
","* Returns the total number of compressed bytes output so far.
   *
   * @return the total (non-negative) number of compressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,getBytesRead,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:getBytesRead(),255,259,"/**
 * Returns the number of bytes read from the stream.
 * Delegates to the internal stream's bytesRead property.
 */
","* Returns the total number of uncompressed bytes input so far.
   *
   * @return the total (non-negative) number of uncompressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,reset,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reset(),261,274,"/**
 * Resets the compressor to its initial state.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,<init>,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:<init>(java.io.OutputStream),251,255,"/**
 * Creates a BZip2CompressionOutputStream writing to the specified OutputStream.
 * @param out The OutputStream to write compressed data to.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,<init>,"org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",36,47,"/**
 * Initializes a CompressorStream with an OutputStream, compressor, and buffer size.
 * @param out Output stream for compressed data.
 * @param compressor Compressor implementation.
 * @param bufferSize Size of the internal buffer.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,<init>,org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream),58,60,"/**
* Initializes a new CompressorStream with the given output stream.
*/","* Allow derived classes to directly set the underlying stream.
   * 
   * @param out Underlying output stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,writeStreamHeader,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:writeStreamHeader(),257,261,"/**
 * Writes the stream header to the output stream.
 * Uses the parent class's output stream if available.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,write,"org.apache.hadoop.io.compress.CompressorStream:write(byte[],int,int)",62,78,"/**
 * Writes data to the stream, compressing as needed.
 * @param b buffer containing data to write
 * @param off offset within the buffer
 * @param len number of bytes to write
 * @throws IOException if stream is finished
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,finish,org.apache.hadoop.io.compress.CompressorStream:finish(),87,95,"/**
 * Completes compression and flushes remaining data.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,<init>,org.apache.hadoop.io.compress.snappy.SnappyDecompressor:<init>(),65,67,"/**
 * Default constructor, uses the default direct buffer size.
 */
",* Creates a new decompressor with the default buffer size.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,setInput,"org.apache.hadoop.io.compress.snappy.SnappyDecompressor:setInput(byte[],int,int)",83,101,"/**
 * Sets the input data buffer, offset, and length.
 * @param b The byte array.
 * @param off Start offset in the array.
 * @param len Number of bytes to use.
 */
","* Sets input data for decompression.
   * This should be called if and only if {@link #needsInput()} returns
   * <code>true</code> indicating that more input data is required.
   * (Both native and non-native versions of various Decompressors require
   * that the data passed in via <code>b[]</code> remain unmodified until
   * the caller is explicitly notified--via {@link #needsInput()}--that the
   * buffer may be safely modified.  With this requirement, an extra
   * buffer-copy can be avoided.)
   *
   * @param b   Input data
   * @param off Start offset
   * @param len Length",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,needsInput,org.apache.hadoop.io.compress.snappy.SnappyDecompressor:needsInput(),138,156,"/**
 * Checks if more input is needed. Returns true if no data remains.
 */","* Returns true if the input data buffer is empty and
   * {@link #setInput(byte[], int, int)} should be called to
   * provide more input.
   *
   * @return <code>true</code> if the input data buffer is empty and
   *         {@link #setInput(byte[], int, int)} should be called in
   *         order to provide more input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,finished,org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:finished(),310,313,"/**
 * Checks if the input is finished, considering both flags.
 * @return True if endOfInput and super.finished() are true.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,decompress,"org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompress(byte[],int,int)",192,230,"/**
 * Decompresses data from the direct buffer into the provided byte array.
 * @param b destination byte array
 * @param off offset within the array
 * @param len maximum number of bytes to read
 * @return number of bytes decompressed
 */
","* Fills specified buffer with uncompressed data. Returns actual number
   * of bytes of uncompressed data. A return value of 0 indicates that
   * {@link #needsInput()} should be called in order to determine if more
   * input data is required.
   *
   * @param b   Buffer for the uncompressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of compressed data.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,decompressDirect,"org.apache.hadoop.io.compress.snappy.SnappyDecompressor:decompressDirect(java.nio.ByteBuffer,java.nio.ByteBuffer)",275,305,"/**
 * Decompresses data from src to dst, returning the number of bytes written.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyDecompressor.java,reset,org.apache.hadoop.io.compress.snappy.SnappyDecompressor$SnappyDirectDecompressor:reset(),315,319,"/**
 * Resets the object state, also marking end of input.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java,<init>,org.apache.hadoop.io.compress.snappy.SnappyCompressor:<init>(),67,69,"/**
 * Default constructor, uses the default direct buffer size.
 */
",* Creates a new compressor with the default buffer size.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java,compress,"org.apache.hadoop.io.compress.snappy.SnappyCompressor:compress(byte[],int,int)",177,225,"/**
 * Compresses data from the direct buffer into the provided byte array.
 * @param b destination byte array
 * @param off offset in the byte array
 * @param len maximum number of bytes to compress
 * @return number of bytes compressed
 */
","* Fills specified buffer with compressed data. Returns actual number
   * of bytes of compressed data. A return value of 0 indicates that
   * needsInput() should be called in order to determine if more input
   * data is required.
   *
   * @param b   Buffer for the compressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of compressed data.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/snappy/SnappyCompressor.java,reinit,org.apache.hadoop.io.compress.snappy.SnappyCompressor:reinit(org.apache.hadoop.conf.Configuration),248,251,"/**
 * Reinitializes the object using the provided configuration.
 * @param conf Configuration object for reinitialization.
 */
","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration
   *
   * @param conf Configuration from which new setting are fetched",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,"org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int,int)",51,66,"/**
 * Initializes a DecompressorStream with an input stream, decompressor, buffer sizes.
 * @param in Input stream.
 * @param decompressor Decompression algorithm.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream),85,87,"/**
 * Creates a DecompressorStream with the given input stream.
 * @param in the input stream to decompress
 * @throws IOException if I/O error occurs
 */
","* Allow derived classes to directly set the underlying stream.
   * 
   * @param in Underlying input stream.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SplitCompressionInputStream.java,<init>,"org.apache.hadoop.io.compress.SplitCompressionInputStream:<init>(java.io.InputStream,long,long)",39,44,"/**
 * Constructs a SplitCompressionInputStream with start and end offsets.
 * @param in Input stream to compress.
 * @param start Start offset for compression.
 * @param end End offset for compression.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,getCompressedData,org.apache.hadoop.io.compress.DecompressorStream:getCompressedData(),175,180,"/**
 * Reads compressed data from the input stream.
 * @return Number of bytes read or -1 if EOF.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,available,org.apache.hadoop.io.compress.DecompressorStream:available(),215,219,"/**
 * Checks if the stream is ready for reading.
 * @return 1 if ready, 0 if end-of-stream, throws IOException.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,resetState,org.apache.hadoop.io.compress.BlockDecompressorStream:resetState(),137,142,"/**
 * Resets the state of the compressor, including block size and byte count.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,compress,org.apache.hadoop.io.compress.BlockCompressorStream:compress(),147,155,"/**
 * Compresses the buffer and writes the compressed chunk to the output.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,setInput,"org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:setInput(byte[],int,int)",87,104,"/**
 * Sets the input byte array and related parameters for processing.
 * @param b byte array, off offset, len length to consume.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,needsInput,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:needsInput(),134,151,"/**
 * Checks if more input is needed.
 * Returns true if all input is consumed, otherwise false.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,finished,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:finished(),302,305,"/**
* Checks if the input is finished, considering endOfInput.
* @return True if both endOfInput and super.finished() are true.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,getRemaining,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:getRemaining(),216,221,"/**
 * Returns the number of bytes remaining to be read from stream.
 */","* <p>Returns the number of bytes remaining in the input buffers;
   * normally called when finished() is true to determine amount of post-stream
   * data.</p>
   *
   * @return the total (non-negative) number of unprocessed bytes in input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,reset,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:reset(),226,238,"/**
 * Resets the decompressor to its initial state.
 */
",* Resets everything including the input buffers (user and direct).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,decompress,"org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:decompress(byte[],int,int)",166,207,"/**
 * Decompresses data from the input buffer into the output buffer.
 * @param b output buffer, @param off offset, @param len length
 * @return number of bytes decompressed
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,setInput,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:setInput(byte[],int,int)",122,139,"/**
 * Sets the input data buffer, offset, and length.
 * @param b The byte array.
 * @param off The starting offset.
 * @param len The number of bytes to read.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,needsInput,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:needsInput(),156,181,"/**
* Checks if more input is needed. Considers compressed/uncompressed buffers.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,compress,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:compress(byte[],int,int)",195,243,"/**
 * Compresses data from input buffer to output buffer.
 * @param b output buffer
 * @param off offset in output buffer
 * @param len number of bytes to compress
 * @return number of compressed bytes written
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,getBytesWritten,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getBytesWritten(),250,254,"/**
 * Returns the number of bytes written to the stream.
 * @return The number of bytes written.
 */
","* Returns the total number of compressed bytes output so far.
   *
   * @return the total (non-negative) number of compressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,getBytesRead,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:getBytesRead(),261,265,"/**
 * Returns the number of bytes read from the stream.
 * @return The number of bytes read, as a long.
 */
","* <p>Returns the total number of uncompressed bytes input so far.</p>
   *
   * @return the total (non-negative) number of uncompressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,reset,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:reset(),267,283,"/**
 * Resets the state of the compressor to its initial configuration.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,getCompressedData,org.apache.hadoop.io.compress.BlockDecompressorStream:getCompressedData(),114,135,"/**
 * Reads compressed data from the stream.
 * @return Length of the compressed data read.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,<init>,org.apache.hadoop.io.compress.lz4.Lz4Decompressor:<init>(),77,79,"/**
 * Creates a Lz4Decompressor with the default direct buffer size.
 */
",* Creates a new decompressor with the default buffer size.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,setInput,"org.apache.hadoop.io.compress.lz4.Lz4Decompressor:setInput(byte[],int,int)",95,113,"/**
 * Sets the input data buffer and length.
 * @param b the input byte array
 * @param off the offset within the array
 * @param len the number of bytes to use
 */
","* Sets input data for decompression.
   * This should be called if and only if {@link #needsInput()} returns
   * <code>true</code> indicating that more input data is required.
   * (Both native and non-native versions of various Decompressors require
   * that the data passed in via <code>b[]</code> remain unmodified until
   * the caller is explicitly notified--via {@link #needsInput()}--that the
   * buffer may be safely modified.  With this requirement, an extra
   * buffer-copy can be avoided.)
   *
   * @param b   Input data
   * @param off Start offset
   * @param len Length",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,needsInput,org.apache.hadoop.io.compress.lz4.Lz4Decompressor:needsInput(),150,168,"/**
 * Checks if more input is needed. Returns true if no input remains.
 */","* Returns true if the input data buffer is empty and
   * {@link #setInput(byte[], int, int)} should be called to
   * provide more input.
   *
   * @return <code>true</code> if the input data buffer is empty and
   *         {@link #setInput(byte[], int, int)} should be called in
   *         order to provide more input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Decompressor.java,decompress,"org.apache.hadoop.io.compress.lz4.Lz4Decompressor:decompress(byte[],int,int)",204,242,"/**
 * Decompresses data from the direct buffer into the provided byte array.
 * @param b destination byte array
 * @param off offset in the array
 * @param len maximum number of bytes to read
 * @return number of bytes decompressed
 */
","* Fills specified buffer with uncompressed data. Returns actual number
   * of bytes of uncompressed data. A return value of 0 indicates that
   * {@link #needsInput()} should be called in order to determine if more
   * input data is required.
   *
   * @param b   Buffer for the compressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of uncompressed data.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,<init>,org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>(int),95,97,"/**
 * Creates a Lz4Compressor with a specified direct buffer size.
 * @param directBufferSize Size of the direct buffer.
 */
","* Creates a new compressor.
   *
   * @param directBufferSize size of the direct buffer to be used.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,compress,"org.apache.hadoop.io.compress.lz4.Lz4Compressor:compress(byte[],int,int)",212,260,"/**
 * Compresses data from the input buffer into the output buffer.
 * @param b output buffer
 * @param off offset in output buffer
 * @param len max number of bytes to compress
 * @return number of bytes compressed
 */
","* Fills specified buffer with compressed data. Returns actual number
   * of bytes of compressed data. A return value of 0 indicates that
   * needsInput() should be called in order to determine if more input
   * data is required.
   *
   * @param b   Buffer for the compressed data
   * @param off Start offset of the data
   * @param len Size of the buffer
   * @return The actual number of bytes of compressed data.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,reinit,org.apache.hadoop.io.compress.lz4.Lz4Compressor:reinit(org.apache.hadoop.conf.Configuration),283,286,"/**
 * Reinitializes the object with the provided configuration.
 * @param conf Configuration object for reinitialization.
 */
","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration
   *
   * @param conf Configuration from which new setting are fetched",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodecByName,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecByName(java.lang.String),246,257,"/**
 * Retrieves a CompressionCodec by class name or name.
 * @param codecName The codec class name or name to find.
 * @return The CompressionCodec or null if not found.
 */
","* Find the relevant compression codec for the codec's canonical class name
   * or by codec alias.
   * <p>
   * Codec aliases are case insensitive.
   * <p>
   * The code alias is the short class name (without the package name).
   * If the short class name ends with 'Codec', then there are two aliases for
   * the codec, the complete short class name and the short class name without
   * the 'Codec' ending. For example for the 'GzipCodec' codec class name the
   * alias are 'gzip' and 'gzipcodec'.
   *
   * @param codecName the canonical class name of the codec
   * @return the codec object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,payback,"org.apache.hadoop.io.compress.CodecPool:payback(java.util.Map,java.lang.Object)",105,122,"/**
 * Adds a codec to the pool's set for its class.
 * @param pool Pool of codecs, grouped by class.
 * @param codec Codec to add; returns true if added.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,updateLeaseCount,"org.apache.hadoop.io.compress.CodecPool:updateLeaseCount(org.apache.hadoop.thirdparty.com.google.common.cache.LoadingCache,java.lang.Object,int)",131,137,"/**
 * Updates the usage count for a codec class in the cache.
 * @param usageCounts Cache of class usage counts.
 * @param codec The codec object.
 * @param delta The amount to add to the count.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getLeasedCompressorsCount,org.apache.hadoop.io.compress.CodecPool:getLeasedCompressorsCount(org.apache.hadoop.io.compress.CompressionCodec),245,248,"/**
 * Gets the number of leased compressors for a given codec.
 * @param codec CompressionCodec to check; returns 0 if null.
 */
","* Return the number of leased {@link Compressor}s for this
   * {@link CompressionCodec}.
   *
   * @param codec codec.
   * @return the number of leased.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getLeasedDecompressorsCount,org.apache.hadoop.io.compress.CodecPool:getLeasedDecompressorsCount(org.apache.hadoop.io.compress.CompressionCodec),257,260,"/**
 * Gets the number of leased decompressors for a codec.
 * @param codec CompressionCodec to check; returns 0 if null.
 */
","* Return the number of leased {@link Decompressor}s for this
   * {@link CompressionCodec}.
   *
   * @param codec codec.
   * @return the number of leased",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,checkNativeCodeLoaded,org.apache.hadoop.io.compress.ZStandardCodec:checkNativeCodeLoaded(),62,77,"/**
 * Checks if native zStandard libraries are loaded.
 * Throws RuntimeException if not available.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,isNativeCodeLoaded,org.apache.hadoop.io.compress.ZStandardCodec:isNativeCodeLoaded(),79,82,"/**
 * Checks if native code is loaded for both compressors/decompressors.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,flush,org.apache.hadoop.io.file.tfile.Compression$FinishOnFlushCompressionStream:flush(),66,72,"/**
 * Flushes the underlying compression stream and resets its state.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,getCompressorType,org.apache.hadoop.io.compress.GzipCodec:getCompressorType(),69,74,"/**
 * Returns the compressor class based on native Zlib availability.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,getDecompressorType,org.apache.hadoop.io.compress.GzipCodec:getDecompressorType(),102,107,"/**
* Returns the class of the Decompressor to use based on config.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibCompressorType,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressorType(org.apache.hadoop.conf.Configuration),97,101,"/**
 * Returns the Zlib compressor class based on native library loading.
 * @param conf Hadoop configuration object.
 * @return Compressor class (ZlibCompressor or BuiltInZlibDeflater).
 */
","* Return the appropriate type of the zlib compressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the zlib compressor.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibDecompressorType,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressorType(org.apache.hadoop.conf.Configuration),121,125,"/**
 * Returns the Class for the ZlibDecompressor, based on config.
 */","* Return the appropriate type of the zlib decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the zlib decompressor.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,loadNativeZLib,org.apache.hadoop.io.compress.zlib.ZlibFactory:loadNativeZLib(),52,64,"/**
 * Loads native Zlib library if native code is loaded.
 * Sets nativeZlibLoaded based on ZlibCompressor/Decompressor.
 */","* Load native library and set the flag whether to use native library. The
   * method is also used for reset the flag modified by setNativeZlibLoaded",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,init,org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:init(org.apache.hadoop.conf.Configuration),156,165,"/**
 * Initializes the Deflater with compression level and strategy.
 * @param conf Configuration object containing compression settings.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInZlibDeflater.java,reinit,org.apache.hadoop.io.compress.zlib.BuiltInZlibDeflater:reinit(org.apache.hadoop.conf.Configuration),65,83,"/**
 * Reinitializes the compressor with configuration from {@link Configuration}.
 * @param conf Configuration object containing compression settings.
 */
","* reinit the compressor with the given configuration. It will reset the
   * compressor's compression level and compression strategy. Different from
   * <tt>ZlibCompressor</tt>, <tt>BuiltInZlibDeflater</tt> only support three
   * kind of compression strategy: FILTERED, HUFFMAN_ONLY and DEFAULT_STRATEGY.
   * It will use DEFAULT_STRATEGY as default if the configured compression
   * strategy is not supported.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,<init>,"org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionLevel,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionStrategy,org.apache.hadoop.io.compress.zlib.ZlibCompressor$CompressionHeader,int)",261,274,"/**
 * Initializes a ZlibCompressor with specified compression settings.
 */","* Creates a new compressor using the specified compression level.
   * Compressed data will be generated in ZLIB format.
   * 
   * @param level Compression level #CompressionLevel
   * @param strategy Compression strategy #CompressionStrategy
   * @param header Compression header #CompressionHeader
   * @param directBufferSize Size of the direct buffer to be used.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,setInput,"org.apache.hadoop.io.compress.zlib.ZlibCompressor:setInput(byte[],int,int)",300,318,"/**
* Sets the input data buffer, offset, and length.
* @param b The byte array.
* @param off The offset within the array.
* @param len The number of bytes to read.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,needsInput,org.apache.hadoop.io.compress.zlib.ZlibCompressor:needsInput(),340,367,"/**
 * Checks if more input data is needed. Considers compressed & uncompressed buffers.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,getBytesWritten,org.apache.hadoop.io.compress.zlib.ZlibCompressor:getBytesWritten(),432,436,"/**
 * Returns the number of bytes written to the stream.
 * Delegates to the underlying stream's byte count.
 */
","* Returns the total number of compressed bytes output so far.
   *
   * @return the total (non-negative) number of compressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,getBytesRead,org.apache.hadoop.io.compress.zlib.ZlibCompressor:getBytesRead(),443,447,"/**
 * Returns the number of bytes read from the stream.
 */","* Returns the total number of uncompressed bytes input so far.
   *
   * @return the total (non-negative) number of uncompressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,reset,org.apache.hadoop.io.compress.zlib.ZlibCompressor:reset(),449,461,"/**
 * Resets the compressor to its initial state.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,compress,"org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:compress(byte[],int,int)",81,132,"/**
 * Compresses data.
 * @param b input byte array
 * @param off offset in the array
 * @param len number of bytes to compress
 * @return number of compressed bytes written
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,finished,org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:finished(),360,363,"/**
 * Checks if the input is finished, considering both flags.
 * @return True if both endOfInput and super.finished() are true.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,"org.apache.hadoop.io.compress.zlib.ZlibDecompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader,int)",107,115,"/**
 * Initializes the ZlibDecompressor with header and direct buffer size.
 */","* Creates a new decompressor.
   * @param header header.
   * @param directBufferSize directBufferSize.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,setInput,"org.apache.hadoop.io.compress.zlib.ZlibDecompressor:setInput(byte[],int,int)",121,139,"/**
 * Sets the input data buffer, offset, and length.
 * @param b the input byte array
 * @param off the offset within the array
 * @param len the number of bytes to use
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,needsInput,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:needsInput(),170,188,"/**
 * Determines if more input data is needed.
 * Returns true if no more input is available.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,getBytesWritten,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getBytesWritten(),242,245,"/**
 * Returns the number of bytes written to the stream.
 */
","* Returns the total number of uncompressed bytes output so far.
   *
   * @return the total (non-negative) number of uncompressed bytes output so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,getBytesRead,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getBytesRead(),252,255,"/**
 * Returns the number of bytes read from the stream.
 */
","* Returns the total number of compressed bytes input so far.
   *
   * @return the total (non-negative) number of compressed bytes input so far",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,getRemaining,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:getRemaining(),264,268,"/**
 * Returns the remaining bytes to be read from the stream.
 */","* Returns the number of bytes remaining in the input buffers; normally
   * called when finished() is true to determine amount of post-gzip-stream
   * data.
   *
   * @return the total (non-negative) number of unprocessed bytes in input",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,reset,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:reset(),273,283,"/**
 * Resets the compressor to its initial state.
 */
",* Resets everything including the input buffers (user and direct).,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,finalize,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:finalize(),293,296,"/**
 * Calls the 'end' method during object finalization.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,executeTrailerState,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:executeTrailerState(),367,416,"/**
 * Processes the gzip trailer to verify CRC and size.
 * Throws IOException if verification fails.
 */","* Parse the gzip trailer (assuming we're in the appropriate state).
   * In order to deal with degenerate cases (e.g., user buffer is one byte
   * long), we copy trailer bytes (all 8 of 'em) to a local buffer.</p>
   *
   * See http://www.ietf.org/rfc/rfc1952.txt for the gzip spec.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,processBasicHeader,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:processBasicHeader(),509,524,"/**
 * Validates gzip header data and extracts flags.
 * Throws IOException if validation fails.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readCompressedString,org.apache.hadoop.io.WritableUtils:readCompressedString(java.io.DataInput),87,91,"/**
 * Reads a compressed string from the input stream.
 * @param in DataInput to read from; returns null if empty.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeVInt,"org.apache.hadoop.io.WritableUtils:writeVInt(java.io.DataOutput,int)",257,259,"/**
 * Writes a variable-length integer to the output stream.
 * @param stream Output stream to write to.
 * @param i Integer value to write.
 */
","* Serializes an integer to a binary stream with zero-compressed encoding.
   * For -112 {@literal <=} i {@literal <=} 127, only one byte is used with the
   * actual value.
   * For other values of i, the first byte value indicates whether the
   * integer is positive or negative, and the number of bytes that follow.
   * If the first byte value v is between -113 and -116, the following integer
   * is positive, with number of bytes that follow are -(v+112).
   * If the first byte value v is between -121 and -124, the following integer
   * is negative, with number of bytes that follow are -(v+120). Bytes are
   * stored in the high-non-zero-byte-first order.
   *
   * @param stream Binary output stream
   * @param i Integer to be serialized
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VLongWritable.java,write,org.apache.hadoop.io.VLongWritable:write(java.io.DataOutput),54,57,"/**
 * Writes the long value to the DataOutput stream.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readVLong,org.apache.hadoop.io.WritableUtils:readVLong(java.io.DataInput),313,326,"/**
 * Reads a variable-length long from the input stream.
 * Returns the long value or throws IOException on error.
 */
","* Reads a zero-compressed encoded long from input stream and returns it.
   * @param stream Binary input stream
   * @throws IOException raised on errors performing I/O.
   * @return deserialized long from stream.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WeakReferencedElasticByteBufferPool.java,getBuffer,"org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:getBuffer(boolean,int)",82,107,"/**
 * Retrieves a buffer from the pool, or allocates a new one.
 * @param direct if true, allocates a direct buffer; otherwise, a regular buffer.
 * @param length desired buffer length.
 * @return A ByteBuffer.
 */
","* {@inheritDoc}
   *
   * @param direct whether we want a direct byte buffer or a heap one.
   * @param length length of requested buffer.
   * @return returns equal or next greater than capacity buffer from
   * pool if already available and not garbage collected else creates
   * a new buffer and return it.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WeakReferencedElasticByteBufferPool.java,putBuffer,org.apache.hadoop.io.WeakReferencedElasticByteBufferPool:putBuffer(java.nio.ByteBuffer),113,130,"/**
 * Adds a buffer to the tree map, using capacity and timestamp as key.
 * @param buffer The ByteBuffer to add.
 */
","* Return buffer to the pool.
   * @param buffer buffer to be returned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,charAt,org.apache.hadoop.io.Text:charAt(int),163,169,"/**
 * Returns the character code point at the given position.
 * @param position index of the character to retrieve
 * @return code point at position, or -1 if out of bounds.
 */
","* Returns the Unicode Scalar Value (32-bit integer value)
   * for the character at <code>position</code>. Note that this
   * method avoids using the converter or doing String instantiation.
   *
   * @param position input position.
   * @return the Unicode scalar value at position or -1
   *          if the position is invalid or points to a
   *          trailing byte",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,org.apache.hadoop.io.Text:set(java.lang.String),228,237,"/**
 * Sets the internal byte array from a string, encoding it.
 * @param string The string to encode into a byte array.
 */
","* Set to contain the contents of a string.
   *
   * @param string input string.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,encode,org.apache.hadoop.io.Text:encode(java.lang.String),514,517,"/**
 * Encodes a string into a ByteBuffer using default charset.
 * @param string The string to encode.
 * @return ByteBuffer containing the encoded string.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,buildCacheKey,org.apache.hadoop.security.token.Token:buildCacheKey(),449,452,"/**
 * Generates a cache key based on kind, identifier, and password.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,"org.apache.hadoop.io.Text:set(byte[],int,int)",272,277,"/**
* Copies bytes from utf8 array to internal bytes array.
* @param utf8 Source byte array.
* @param start Start index in utf8.
* @param len Number of bytes to copy.
*/
","* Set the Text to range of bytes.
   *
   * @param utf8 the data to copy from
   * @param start the first position of the new string
   * @param len the number of bytes of the new string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,append,"org.apache.hadoop.io.Text:append(byte[],int,int)",286,294,"/**
 * Appends data to the buffer.
 * @param utf8 byte array to append
 * @param start start index in utf8
 * @param len number of bytes to append
 */
","* Append a range of bytes to the end of the given text.
   *
   * @param utf8 the data to copy from
   * @param start the first position to append from utf8
   * @param len the number of bytes to append",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readWithKnownLength,"org.apache.hadoop.io.Text:readWithKnownLength(java.io.DataInput,int)",383,388,"/**
 * Reads a fixed-length byte sequence from the input stream.
 * @param in Input stream to read from.
 * @param len Number of bytes to read.
 */
","* Read a Text object whose length is already known.
   * This allows creating Text from a stream which uses a different serialization
   * format.
   *
   * @param in input in.
   * @param len input len.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,decode,org.apache.hadoop.io.Text:decode(byte[]),457,459,"/**
 * Decodes a byte array in UTF-8 to a String.
 * @param utf8 byte array containing UTF-8 encoded data
 * @return Decoded String, or null if decoding fails
 */
","* @return Converts the provided byte array to a String using the
   * UTF-8 encoding. If the input is malformed,
   * replace by a default value.
   *
   * @param utf8 input utf8.
   * @throws CharacterCodingException when a character
   *                                  encoding or decoding error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,decode,"org.apache.hadoop.io.Text:decode(byte[],int,int)",461,464,"/**
 * Decodes a byte array into a String using UTF-8.
 * @param utf8 byte array to decode
 * @param start start index in the byte array
 * @param length number of bytes to decode
 * @return Decoded String
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,decode,"org.apache.hadoop.io.Text:decode(byte[],int,int,boolean)",480,483,"/**
 * Decodes a byte array to a String, optionally replacing invalid chars.
 * @param utf8 byte array to decode
 * @param start start index in the byte array
 * @param length length of the byte array to decode
 * @param replace whether to replace invalid chars
 * @return decoded String
 */
","* @return Converts the provided byte array to a String using the
   * UTF-8 encoding. If <code>replace</code> is true, then
   * malformed input is replaced with the
   * substitution character, which is U+FFFD. Otherwise the
   * method throws a MalformedInputException.
   *
   * @param utf8 input utf8.
   * @param start input start.
   * @param length input length.
   * @param replace input replace.
   * @throws CharacterCodingException when a character
   *                                  encoding or decoding error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,validateUTF8,org.apache.hadoop.io.Text:validateUTF8(byte[]),626,628,"/**
 * Validates the input byte array as a UTF-8 encoded string.
 */","* Check if a byte array contains valid UTF-8.
   *
   * @param utf8 byte array
   * @throws MalformedInputException if the byte array contains invalid UTF-8",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,addToMap,org.apache.hadoop.io.AbstractMapWritable:addToMap(java.lang.Class),91,101,"/**
 * Adds a class to the map, assigning a unique ID.
 * Throws exception if adding exceeds allowed limit.
 */","* Add a Class to the maps if it is not already present.
   * @param clazz clazz.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,<init>,org.apache.hadoop.io.AbstractMapWritable:<init>(),145,166,"/**
 * Initializes the AbstractMapWritable with default Writable types.
 */
",constructor.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,readFields,org.apache.hadoop.io.AbstractMapWritable:readFields(java.io.DataInput),194,216,"/**
 * Reads fields from the input stream, loading new classes.
 * @param in DataInput stream to read from.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,write,org.apache.hadoop.io.AbstractMapWritable:write(java.io.DataOutput),180,192,"/**
 * Writes class table information to the output stream.
 * Writes the number of new classes and their names.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Metadata:<init>(),735,737,"/**
 * Default constructor, initializes with a new, empty TreeMap.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VLongWritable.java,<init>,org.apache.hadoop.io.VLongWritable:<init>(long),38,38,"/**
* Constructs a VLongWritable with the given long value.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,getSerializer,org.apache.hadoop.io.serializer.SerializationFactory:getSerializer(java.lang.Class),81,87,"/**
 * Gets a Serializer for the given class.
 * @param c The class to get a serializer for.
 * @return Serializer for the class or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,getDeserializer,org.apache.hadoop.io.serializer.SerializationFactory:getDeserializer(java.lang.Class),89,95,"/**
 * Gets the deserializer for the given class.
 * @param c The class to get the deserializer for.
 * @return Deserializer for the class, or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/JavaSerializationComparator.java,compare,"org.apache.hadoop.io.serializer.JavaSerializationComparator:compare(java.lang.Object,java.lang.Object)",47,51,"/**
 * Compares two objects using their natural ordering.
 * @param o1 The first object to compare.
 * @param o2 The second object to compare.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/JavaSerialization.java,deserialize,org.apache.hadoop.io.serializer.JavaSerialization$JavaSerializationDeserializer:deserialize(java.lang.Object),54,63,"/**
 * Deserializes an object from the input stream.
 * @param object Unused object, used for type safety.
 * @return Deserialized object of type T.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,filesystem,org.apache.hadoop.io.SequenceFile$Writer:filesystem(org.apache.hadoop.fs.FileSystem),1002,1005,"/**
 * Creates a FileSystemOption for sequence files.
 * @param fs The FileSystem to use.
 * @return A FileSystemOption object.
 */
","* @deprecated only used for backwards-compatibility in the createWriter methods
     * that take FileSystem.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,<init>,org.apache.hadoop.io.DataInputByteBuffer:<init>(),74,76,"/**
 * Constructs a DataInputByteBuffer with a new internal buffer.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,getData,org.apache.hadoop.io.DataInputByteBuffer:getData(),87,89,"/**
 * Returns the array of buffers containing the data.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,getPosition,org.apache.hadoop.io.DataInputByteBuffer:getPosition(),91,93,"/**
 * Returns the current position of the underlying buffer.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataInputByteBuffer.java,getLength,org.apache.hadoop.io.DataInputByteBuffer:getLength(),95,97,"/**
 * Returns the length of the underlying buffer.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,<init>,"org.apache.hadoop.util.bloom.Key:<init>(byte[],double)",98,100,"/**
 * Constructs a Key object with the given value and weight.
 * @param value The key's byte array value.
 * @param weight The key's associated weight.
 */
","* Constructor.
   * <p>
   * Builds a key with a specified weight.
   * @param value The value of <i>this</i> key.
   * @param weight The weight associated to <i>this</i> key.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,cleanup,org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:cleanup(),3916,3921,"/**
 * Cleans up resources. Calls super.close() and parent cleanup.
 */","The default cleanup. Subclasses can override this with a custom 
       * cleanup",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,grow,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:grow(),3193,3200,"/**
 * Increases the capacity of internal arrays to accommodate more data.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$CompressionOption:<init>(org.apache.hadoop.io.SequenceFile$CompressionType),977,979,"/**
 * Constructs a CompressionOption with the given compression type.
 * @param value The compression type for the option.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compression,"org.apache.hadoop.io.SequenceFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",1056,1059,"/**
 * Creates a CompressionOption with the given type and codec.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericsUtil.java,toArray,org.apache.hadoop.util.GenericsUtil:toArray(java.util.List),86,88,"/**
 * Converts a list to an array of the list's component type.
 * @param list The list to convert.
 * @return An array containing the list's elements.
 */
","* Converts the given <code>List&lt;T&gt;</code> to a an array of 
   * <code>T[]</code>. 
   * @param list the list to convert
   * @param <T> Generics Type T.
   * @throws ArrayIndexOutOfBoundsException if the list is empty. 
   * Use {@link #toArray(Class, List)} if the list may be empty.
   * @return T Array.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,<init>,org.apache.hadoop.io.InputBuffer:<init>(),69,71,"/**
 * Constructs a new InputBuffer using a default Buffer.
 */
",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,reset,"org.apache.hadoop.io.InputBuffer:reset(byte[],int)",83,85,"/**
 * Resets the buffer with data from the input array.
 * @param input Input byte array.
 * @param length Length of data to reset with.
 */
","* Resets the data that the buffer reads.
   * @param input input.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,reset,"org.apache.hadoop.io.InputBuffer:reset(byte[],int,int)",93,95,"/**
* Resets the internal buffer with data from the input array.
* @param input Data source.
* @param start Start index in the input array.
* @param length Number of bytes to read.
*/
","* Resets the data that the buffer reads.
   * @param input input.
   * @param start start.
   * @param length length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,getPosition,org.apache.hadoop.io.InputBuffer:getPosition(),101,101,"/**
 * Returns the current position of the buffer.
 */","* Returns the current position in the input.
   * @return the current position in the input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/InputBuffer.java,getLength,org.apache.hadoop.io.InputBuffer:getLength(),107,107,"/**
 * Returns the length of the underlying buffer.
 */","* Returns the length of the input.
   * @return length of the input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,read,org.apache.hadoop.io.MD5Hash:read(java.io.DataInput),87,91,"/**
 * Reads an MD5Hash object from the DataInput stream.
 * @param in DataInput stream to read from
 * @return Populated MD5Hash object
 */
","* Constructs, reads and returns an instance.
   * @param in in.
   * @throws IOException raised on errors performing I/O.
   * @return MD5Hash.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(java.io.InputStream),138,147,"/**
 * Calculates the MD5 hash of an input stream.
 * @param in Input stream to be hashed.
 * @return MD5Hash object representing the digest.
 * @throws IOException if an I/O error occurs.
 */
","* Construct a hash value for the content from the InputStream.
   * @param in input stream.
   * @return MD5Hash.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,"org.apache.hadoop.io.MD5Hash:digest(byte[],int,int)",156,162,"/**
 * Calculates MD5 hash of data segment.
 * @param data byte array
 * @param start start index
 * @param len length of data segment
 * @return MD5Hash object
 */
","* Construct a hash value for a byte array.
   * @param data data.
   * @param start start.
   * @param len len.
   * @return MD5Hash.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,"org.apache.hadoop.io.MD5Hash:digest(byte[][],int,int)",171,179,"/**
 * Calculates MD5 hash of data.
 * @param dataArr Data array to hash.
 * @param start Start index in data array.
 * @param len Length of data to hash.
 * @return MD5Hash object containing the digest.
 */
","* Construct a hash value for an array of byte array.
   * @param dataArr dataArr.
   * @param start start.
   * @param len len.
   * @return MD5Hash.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,hashCode,org.apache.hadoop.io.MD5Hash:hashCode(),234,237,"/**
 * Returns the hash code, based on the quarter digest.
 */
","Returns a hash code value for this object.
   * Only uses the first 4 bytes, since md5s are evenly distributed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,setDigest,org.apache.hadoop.io.MD5Hash:setDigest(java.lang.String),283,293,"/**
 * Sets the MD5 digest from a hexadecimal string.
 * @param hex Hexadecimal representation of the digest.
 */
","* Sets the digest value from a hex string.
   * @param hex hex.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,metadata,org.apache.hadoop.io.SequenceFile$Writer:metadata(org.apache.hadoop.io.SequenceFile$Metadata),1048,1050,"/**
 * Creates a MetadataOption from the given Metadata value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$StreamOption:<init>(org.apache.hadoop.fs.FSDataOutputStream),912,914,"/**
 * Constructs a StreamOption with the provided output stream.
 * @param stream The FSDataOutputStream to wrap.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,<init>,org.apache.hadoop.io.ObjectWritable:<init>(java.lang.Object),48,50,"/**
 * Constructs an ObjectWritable with the given object.
 * @param instance The object to be wrapped in the writable.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,tryInstantiateProtobuf,"org.apache.hadoop.io.ObjectWritable:tryInstantiateProtobuf(java.lang.Class,java.io.DataInput)",351,389,"/**
 * Instantiates a Protobuf message from a DataInput.
 * @param protoClass Protobuf class to instantiate.
 * @param dataIn Input stream containing the Protobuf data.
 * @return Protobuf Message object.
 * @throws IOException if an I/O error occurs.
 */
","* Try to instantiate a protocol buffer of the given message class
   * from the given input stream.
   * 
   * @param protoClass the class of the generated protocol buffer
   * @param dataIn the input stream to read from
   * @return the instantiated Message instance
   * @throws IOException if an IO problem occurs",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,fsync,org.apache.hadoop.io.IOUtils:fsync(java.io.File),392,413,"/**
 * Synchronizes a file or directory to persistent storage.
 * Handles file existence and directory access restrictions.
 */","* Ensure that any writes to the given file is written to the storage device
   * that contains it. This method opens channel on given File and closes it
   * once the sync is done.<br>
   * Borrowed from Uwe Schindler in LUCENE-5588
   * @param fileToSync the file to fsync
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,equals,org.apache.hadoop.io.MapWritable:equals(java.lang.Object),78,94,"/**
 * Checks if this map is equal to the given object.
 * @param obj object to compare to, must be a MapWritable
 * @return true if equal, false otherwise
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,putAll,org.apache.hadoop.io.MapWritable:putAll(java.util.Map),123,128,"/**
* Adds all entries from a map to this map.
* @param t the map whose entries are added
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$AppendIfExistsOption:<init>(boolean),939,941,"/**
* Calls superclass constructor.
* @param value The value to pass to the superclass.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$OnlyHeaderOption:<init>(),1889,1891,"/**
 * Initializes a OnlyHeaderOption. Calls super constructor with true.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Reader$FileOption:<init>(org.apache.hadoop.fs.Path),1852,1854,"/**
 * Constructs a FileOption with the given Path.
 * @param value The Path representing the file option.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,org.apache.hadoop.io.SequenceFile$Writer$FileOption:<init>(org.apache.hadoop.fs.Path),890,892,"/**
 * Constructs a FileOption with the given file path.
 * @param path The path to the file.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BooleanWritable.java,<init>,org.apache.hadoop.io.BooleanWritable:<init>(boolean),41,43,"/**
 * Constructs a BooleanWritable with the given boolean value.
 */",* @param value value.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BooleanWritable.java,toString,org.apache.hadoop.io.BooleanWritable:toString(),102,105,"/**
 * Returns the boolean value as a String representation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,waitAsyncValue,"org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:waitAsyncValue(long,java.util.concurrent.TimeUnit)",205,217,"/**
 * Waits for a value, returning it or throwing TimeoutException.
 * @param timeout Timeout duration.
 * @param unit Time unit for the timeout.
 * @return The value or null if timeout occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$TryOnceThenFail:shouldRetry(java.lang.Exception,int,int,boolean)",225,230,"/**
 * Determines if a retry should occur. Returns FAIL with a message.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicy.java,<init>,org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision),49,51,"/**
 * Constructs a RetryAction with the given decision and default values.
 * @param action The retry decision to execute.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicy.java,<init>,"org.apache.hadoop.io.retry.RetryPolicy$RetryAction:<init>(org.apache.hadoop.io.retry.RetryPolicy$RetryAction$RetryDecision,long)",53,55,"/**
 * Creates a RetryAction with a delay time.
 * @param action The retry decision.
 * @param delayTime Delay in milliseconds before retry.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithFixedSleep:<init>(int,long,java.util.concurrent.TimeUnit)",331,333,"/**
 * Initializes a RetryUpToMaximumCountWithFixedSleep.
 * @param maxRetries Max retries, sleepTime, timeUnit for retry.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$ExponentialBackoffRetry:<init>(int,long,java.util.concurrent.TimeUnit)",627,638,"/**
 * Constructs an ExponentialBackoffRetry with retry limits and sleep time.
 * @param maxRetries Max retry attempts.
 * @param sleepTime Initial sleep time.
 * @param timeUnit Time unit for sleepTime.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumCountWithProportionalSleep:<init>(int,long,java.util.concurrent.TimeUnit)",367,369,"/**
* Initializes a RetryUpToMaximumCountWithProportionalSleep.
* @param maxRetries Max retries, sleepTime, timeUnit for retry.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int)",669,672,"/**
 * Constructs a FailoverOnNetworkExceptionRetry with default settings.
 * @param fallbackPolicy The fallback policy to use.
 * @param maxFailovers The maximum number of failover attempts.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:<init>(org.apache.hadoop.io.retry.RetryPolicy,int,long,long)",674,677,"/**
 * Constructs a FailoverOnNetworkExceptionRetry with default initial delay.
 * @param fallbackPolicy The fallback policy to use.
 * @param maxFailovers Max number of failover attempts.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,"org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int,int,long,long)",217,222,"/**
 * Creates a FailoverOnNetworkExceptionRetry policy.
 * @param fallbackPolicy Fallback policy when network fails.
 * @return RetryPolicy instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryByRemoteException,"org.apache.hadoop.io.retry.RetryPolicies:retryByRemoteException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",177,181,"/**
 * Creates a RemoteExceptionDependentRetry policy.
 * Uses default policy and exception-specific overrides.
 */
","* <p>
   * A retry policy for RemoteException
   * Set a default policy with some explicit handlers for specific exceptions.
   * </p>
   *
   * @param defaultPolicy defaultPolicy.
   * @param exceptionToPolicyMap exceptionToPolicyMap.
   * @return RetryPolicy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$RemoteExceptionDependentRetry:shouldRetry(java.lang.Exception,int,int,boolean)",582,594,"/**
 * Determines if a retry action should be taken based on exception.
 * @param e Exception to check. Returns RetryAction based on policy.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,<init>,"org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:<init>(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler,org.apache.hadoop.io.retry.AsyncCallHandler)",237,243,"/**
 * Constructs an AsyncCall with provided parameters and handler.
 * @param asyncCallHandler handles the asynchronous call.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,processWaitTimeAndRetryInfo,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:processWaitTimeAndRetryInfo(),268,277,"/**
 * Determines wait/retry behavior. Returns WAIT_RETRY or RETRY.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:<init>(long,org.apache.hadoop.io.retry.RetryPolicy$RetryAction,long,java.lang.Exception)",250,257,"/**
 * Creates a RetryInfo object with delay, action, count, and exception.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,isEmpty,org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:isEmpty(long),96,99,"/**
* Checks if the queue is empty after a given time.
* @param time Time in nanoseconds. Returns true if empty.
*/
",Is the queue empty for more than the given time in millisecond?,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,checkEmpty,org.apache.hadoop.io.retry.AsyncCallHandler$ConcurrentQueue:checkEmpty(),106,110,"/**
 * Records the start time if the queue is empty.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,clearNameMaps,org.apache.hadoop.security.ShellBasedIdMapping:clearNameMaps(),154,159,"/**
 * Clears the uid/gid name maps and updates the last update time.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,isExpired,org.apache.hadoop.security.ShellBasedIdMapping:isExpired(),161,163,"/**
 * Checks if the object has expired based on the timeout.
 * @return True if expired, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$MetricsUpdateRunner:run(),4207,4223,"/**
 * Updates metrics: calculates total requests per second.
 * Uses monotonic time and internal state for accurate updates.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,now,org.apache.hadoop.util.SysInfoWindows:now(),61,64,"/**
 * Returns the current monotonic time in milliseconds.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Timer.java,monotonicNow,org.apache.hadoop.util.Timer:monotonicNow(),50,50,"/**
 * Returns the current monotonic time in nanoseconds.
 */","* Current time from some arbitrary time base in the past, counting in
   * milliseconds, and not affected by settimeofday or similar system clock
   * changes.  This is appropriate to use when computing how much longer to
   * wait for an interval to expire.
   * @return a monotonic clock that counts in milliseconds.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryOtherThanRemoteAndSaslException,"org.apache.hadoop.io.retry.RetryPolicies:retryOtherThanRemoteAndSaslException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",194,199,"/**
 * Creates a RetryPolicy excluding Remote and SASL exceptions.
 * @param defaultPolicy Default RetryPolicy to use.
 * @param exceptionToPolicyMap Maps exceptions to RetryPolicies.
 */
","* <p>
   * A retry policy where RemoteException and SaslException are not retried, other individual
   * exception types can have RetryPolicy overrides, and any other exception type without an
   * override is not retried.
   * </p>
   *
   * @param defaultPolicy defaultPolicy.
   * @param exceptionToPolicyMap exceptionToPolicyMap.
   * @return RetryPolicy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/DefaultFailoverProxyProvider.java,getProxy,org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:getProxy(),45,48,"/**
 * Returns a ProxyInfo object containing the proxy.
 * @return ProxyInfo object with the proxy, null details.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.RetryInvocationHandler:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",332,338,"/**
 * Constructs a RetryInvocationHandler with provider, policy, and map.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,close,org.apache.hadoop.io.retry.RetryInvocationHandler:close(),457,460,"/**
 * Closes the proxy descriptor, releasing associated resources.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/FailoverProxyProvider.java,getString,org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:getString(java.lang.String),50,52,"/**
 * Constructs a string describing a method call with proxy info.
 * @param methodName The name of the method being called.
 * @return A formatted string describing the method call.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/FailoverProxyProvider.java,toString,org.apache.hadoop.io.retry.FailoverProxyProvider$ProxyInfo:toString(),54,57,"/**
 * Returns a string representation of the object.
 * Includes the proxy name and proxy info.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,getFailoverCount,org.apache.hadoop.io.retry.RetryInvocationHandler:getFailoverCount(),345,347,"/**
 * Returns the failover count from the proxy descriptor.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invokeMethod,"org.apache.hadoop.io.retry.RetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])",432,443,"/**
 * Invokes a method on a proxy with provided arguments.
 * @param method Method to invoke.
 * @param args Arguments for the method.
 * @return Result of the method invocation.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getCallId,org.apache.hadoop.ipc.Client:getCallId(),137,139,"/**
 * Retrieves the current call ID. Returns next ID if current is null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client$Call:<init>(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable)",287,307,"/**
* Creates a new Call object with the given RPC kind and parameter.
* @param rpcKind The type of RPC call.
* @param param The request parameter for the RPC call.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,getConnectionId,org.apache.hadoop.io.retry.RetryInvocationHandler:getConnectionId(),462,465,"/**
 * Returns the connection ID for the proxy.
 * Uses RPC to retrieve the ID from the proxy descriptor.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProxyCombiner.java,getConnectionId,org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:getConnectionId(),122,125,"/**
 * Returns the connection ID for the first proxy.
 * Uses RPC to fetch the ID.
 */
","* Since this is incapable of returning multiple connection IDs, simply
     * return the first one. In most cases, the connection ID should be the same
     * for all proxies.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,isDone,org.apache.hadoop.io.retry.AsyncCallHandler$1:isDone(),226,228,"/**
 * Checks if the computation is complete.
 * @return True if the result is available, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,hashCode,org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:hashCode(),451,454,"/**
 * Generates a hash code based on the object's string representation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,equals,org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:equals(java.lang.Object),456,464,"/**
 * Checks if two objects are equal based on their string representation.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,parseCommaSeparatedString,org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:parseCommaSeparatedString(java.lang.String),483,514,"/**
 * Parses a comma-separated string into a MultipleLinearRandomRetry.
 * Returns null if parsing fails or input is invalid.
 */","* Parse the given string as a MultipleLinearRandomRetry object.
     * The format of the string is ""t_1, n_1, t_2, n_2, ..."",
     * where t_i and n_i are the i-th pair of sleep time and number of retries.
     * Note that the white spaces in the string are ignored.
     *
     * @param s input string.
     * @return the parsed object, or null if the parsing fails.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryByException,"org.apache.hadoop.io.retry.RetryPolicies:retryByException(org.apache.hadoop.io.retry.RetryPolicy,java.util.Map)",162,165,"/**
 * Creates a retry policy based on exception mapping.
 * @param defaultPolicy Default retry policy.
 * @param exceptionToPolicyMap Maps exceptions to specific policies.
 */
","* <p>
   * Set a default policy with some explicit handlers for specific exceptions.
   * </p>
   *
   * @param exceptionToPolicyMap exceptionToPolicyMap.
   * @param defaultPolicy defaultPolicy.
   * @return RetryPolicy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,calculateExponentialTime,"org.apache.hadoop.io.retry.RetryPolicies:calculateExponentialTime(long,int)",769,771,"/**
 * Calculates exponential backoff time.
 * @param time Initial time in milliseconds.
 * @param retries Number of retry attempts.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,getReason,org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:getReason(),353,356,"/**
 * Returns the reason string based on maxTime and timeUnit.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,getReason,org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:getReason(),293,295,"/**
 * Returns the reason string based on the maximum retry count.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,hashCode,org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:hashCode(),305,308,"/**
 * Generates a hash code based on the object's string representation.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,equals,org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:equals(java.lang.Object),310,318,"/**
 * Checks if two objects are equal based on their string representation.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MultipleIOException.java,createIOException,org.apache.hadoop.io.MultipleIOException:createIOException(java.util.List),50,58,"/**
* Creates an IOException from a list of exceptions.
* Returns null if list is empty, single exception or MultipleIOException.
*/
","* A convenient method to create an {@link IOException}.
   * @param exceptions IOException List.
   * @return IOException.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCompressionAlgorithmByName,org.apache.hadoop.io.file.tfile.Compression:getCompressionAlgorithmByName(java.lang.String),359,370,"/**
 * Retrieves a compression algorithm by its name.
 * @param compressName Name of the algorithm to retrieve.
 * @return Algorithm object or throws IllegalArgumentException.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getSupportedAlgorithms,org.apache.hadoop.io.file.tfile.Compression:getSupportedAlgorithms(),372,382,"/**
 * Returns an array of supported algorithm names.
 * Uses Algorithm enum and filters by isSupported().
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressionName,org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:getCompressionName(),524,526,"/**
 * Returns the name of the compression algorithm being used.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getBlockCount,org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockCount(),687,689,"/**
 * Returns the number of data blocks.
 * Uses dataIndex to determine the block count.
 */
","* Get the number of data blocks.
     * 
     * @return the number of data blocks.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,readAndVerify,org.apache.hadoop.io.file.tfile.BCFile$Magic:readAndVerify(java.io.DataInput),920,929,"/**
 * Reads and verifies the BCFile magic number from the input stream.
 * @param in DataInput stream to read from; throws IOException.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFileDumper.java,format,"org.apache.hadoop.io.file.tfile.TFileDumper$Align:format(long,int,org.apache.hadoop.io.file.tfile.TFileDumper$Align)",73,78,"/**
 * Formats a long value with specified width and alignment.
 * @param l long value to format
 * @param width minimum width of the formatted string
 * @param align alignment type
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,addEntry,org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:addEntry(org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry),782,784,"/**
 * Adds a meta index entry to the index.
 * @param indexEntry The entry to add.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getDefaultCompressionAlgorithm,org.apache.hadoop.io.file.tfile.BCFile$Writer:getDefaultCompressionAlgorithm(),341,343,"/**
 * Returns the default compression algorithm from the data index.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getDefaultCompressionName,org.apache.hadoop.io.file.tfile.BCFile$Reader:getDefaultCompressionName(),652,654,"/**
 * Gets the name of the default compression algorithm.
 * Returns the name as a String.
 */
","* Get the name of the default compression algorithm.
     * 
     * @return the name of the default compression algorithm.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,hashCode,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:hashCode(),1973,1976,"/**
 * Calculates hash code using key buffer.
 * Uses WritableComparator for byte-level hashing.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/CompareUtils.java,compare,"org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator:compare(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",45,49,"/**
 * Compares two RawComparable objects based on their buffers & sizes.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:<init>(java.io.DataInput),948,952,"/**
 * Constructs a BlockRegion from an input stream.
 * @param in Input stream containing block region data.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,readVInt,org.apache.hadoop.io.file.tfile.Utils:readVInt(java.io.DataInput),177,184,"/**
 * Reads a variable-length integer from the input.
 * @param in DataInput to read from.
 * @return Integer value or throws exception if out of range.
 */
","* Decoding the variable-length integer. Synonymous to
   * <code>(int)Utils#readVLong(in)</code>.
   * 
   * @param in
   *          input stream
   * @return the decoded integer
   * @throws IOException raised on errors performing I/O.
   * 
   * @see Utils#readVLong(DataInput)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,makeComparator,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:makeComparator(java.lang.String),2070,2096,"/**
 * Creates a BytesComparator based on the provided comparator string.
 * @param comparator Comparator string, null for unsorted keys.
 * @throws IllegalArgumentException if comparator is unsupported.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion:write(java.io.DataOutput),960,964,"/**
 * Writes offset, compressedSize, and rawSize to the DataOutput.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,writeVInt,"org.apache.hadoop.io.file.tfile.Utils:writeVInt(java.io.DataOutput,int)",55,57,"/**
* Writes an integer using the variable-length encoding.
* @param out DataOutput to write to
* @param n Integer value to write
*/
","* Encoding an integer into a variable-length encoding format. Synonymous to
   * <code>Utils#writeVLong(out, n)</code>.
   * 
   * @param out
   *          output stream
   * @param n
   *          The integer to be encoded
   * @throws IOException raised on errors performing I/O.
   * @see Utils#writeVLong(DataOutput, long)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,isSorted,org.apache.hadoop.io.file.tfile.TFile$Reader:isSorted(),864,866,"/**
 * Checks if the table file is sorted.
 * @return True if sorted, false otherwise.
 */
","* Is the TFile sorted?
     * 
     * @return true if TFile is sorted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCodec,org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:getCodec(),273,273,"/**
* Gets the compression codec.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,equals,org.apache.hadoop.io.file.tfile.Utils$Version:equals(java.lang.Object),395,400,"/**
 * Checks if this version is equal to another version.
 * @param other The other Version object to compare to.
 * @return True if versions are equal, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getEntryCount,org.apache.hadoop.io.file.tfile.TFile$Reader:getEntryCount(),873,875,"/**
 * Returns the number of records in the table file.
 */","* Get the number of key-value pair entries in TFile.
     * 
     * @return the number of key-value pairs in TFile",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Reader:close(),824,827,"/**
 * Closes the reader, releasing associated resources.
 */","* Close the reader. The state of the Reader object is undefined after
     * close. Calling close() for multiple times has no effect.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getComparatorName,org.apache.hadoop.io.file.tfile.TFile$Reader:getComparatorName(),855,857,"/**
 * Returns the comparator name from the file metadata.
 */","* Get the string representation of the comparator.
     * 
     * @return If the TFile is not sorted by keys, an empty string will be
     *         returned. Otherwise, the actual comparator string that is
     *         provided during the TFile creation time will be returned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/ByteArray.java,<init>,org.apache.hadoop.io.file.tfile.ByteArray:<init>(org.apache.hadoop.io.BytesWritable),40,42,"/**
 * Constructs a ByteArray from a BytesWritable.
 * @param other The BytesWritable to copy from.
 */
","* Constructing a ByteArray from a {@link BytesWritable}.
   * 
   * @param other other.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/ByteArray.java,<init>,org.apache.hadoop.io.file.tfile.ByteArray:<init>(byte[]),50,52,"/**
 * Constructs a ByteArray with the specified byte array.
 * @param buffer The byte array to wrap.
 */
","* Wrap a whole byte array as a RawComparable.
   * 
   * @param buffer
   *          the byte array buffer.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getBlockEntryCount,org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockEntryCount(int),2031,2033,"/**
 * Returns the number of entries for a given block ID.
 * @param curBid The block ID to retrieve the entry count for.
 * @return The entry count for the specified block.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,addEntry,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:addEntry(org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry),2262,2266,"/**
* Adds a file index entry to the index and updates sum/record count.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,register,"org.apache.hadoop.io.file.tfile.BCFile$Writer$DataBlockRegister:register(long,long,long)",468,471,"/**
 * Registers a block region with the data index.
 * @param raw Raw data identifier.
 * @param begin Start of the region.
 * @param end End of the region.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getBlockIndexNear,org.apache.hadoop.io.file.tfile.BCFile$Reader:getBlockIndexNear(long),745,756,"/**
 * Finds the index of the block region nearest to the given offset.
 * @param offset The offset to find the nearest block region for.
 * @return Index of the nearest block region, or -1 if not found.
 */
","* Find the smallest Block index whose starting offset is greater than or
     * equal to the specified offset.
     * 
     * @param offset
     *          User-specific offset.
     * @return the index to the data Block if such block exists; or -1
     *         otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,lowerBound,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:lowerBound(org.apache.hadoop.io.file.tfile.RawComparable),2187,2201,"/**
 * Finds the index of the first element not less than the key.
 * @param key the key to search for
 * @return index or -1 if not found
 */
","* @param key
     *          input key.
     * @return the ID of the first block that contains key >= input key. Or -1
     *         if no such block exists.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.java,write,org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:write(int),45,51,"/**
 * Writes a byte to the output stream.
 * @param b The byte to be written.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.java,write,"org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:write(byte[],int,int)",53,65,"/**
 * Writes bytes to the buffer. Flushes if buffer is full.
 * @param b byte array to write
 * @param off offset in the array
 * @param len number of bytes to write
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/SimpleBufferedOutputStream.java,flush,org.apache.hadoop.io.file.tfile.SimpleBufferedOutputStream:flush(),67,71,"/**
 * Flushes the buffer and underlying output stream.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,upperBound,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:upperBound(org.apache.hadoop.io.file.tfile.RawComparable),2209,2223,"/**
 * Finds the upper bound (exclusive) of a key in the index.
 * @param key The key to search for.
 * @return Index of upper bound or -1 if not found.
 */
","* @param key
     *          input key.
     * @return the ID of the first block that contains key > input key. Or -1
     *         if no such block exists.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNumByLocation,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getRecordNumByLocation(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),2244,2248,"/**
 * Calculates record number based on location block and index.
 * @param location Location object containing block and record index.
 * @return The calculated record number.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:compareTo(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),741,744,"/**
 * Compares two Location objects based on block and record indices.
 */",* @see java.lang.Comparable#compareTo(java.lang.Object),,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Location:<init>(int,long)",705,707,"/**
 * Constructs a Location object with the given block and record index.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,set,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:set(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),734,736,"/**
* Sets the location using the provided Location object.
* @param other The Location object to copy values from.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKey,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(byte[]),1775,1777,"/**
 * Retrieves the key from the byte buffer.
 * @param buf The byte buffer to read from.
 * @return The key value.
 */
","* Copy the key into user supplied buffer.
         * 
         * @param buf
         *          The buffer supplied by user. The length of the buffer must
         *          not be shorter than the key length.
         * @return The length of the key.
         * 
         * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getValue,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(byte[],int)",1859,1890,"/**
 * Reads value from stream into buffer.
 * @param buf buffer to read into, @param offset offset within buffer
 * @return number of bytes read, or -1 if error.
 */
","* Copy value into user-supplied buffer. User supplied buffer must be
         * large enough to hold the whole value (starting from the offset). The
         * value part of the key-value pair pointed by the current cursor is not
         * cached and can only be examined once. Calling any of the following
         * functions more than once without moving the cursor will result in
         * exception: {@link #getValue(byte[])}, {@link #getValue(byte[], int)},
         * {@link #getValueStream}.
         *
         * @param buf buf.
         * @param offset offset.
         * @return the length of the value. Does not require
         *         isValueLengthKnown() to be true.
         * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:<init>(org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState),549,552,"/**
 * Constructs a BlockReader with the provided RBlockState.
 * @param rbs The RBlockState providing the input stream.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getRawSize,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getRawSize(),585,587,"/**
 * Returns the raw size of the block region.
 * @return long representing the raw size.
 */
","* Get the uncompressed size of the block.
       * 
       * @return uncompressed size of the block.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressedSize,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getCompressedSize(),594,596,"/**
 * Returns the compressed size of the block region.
 */","* Get the compressed size of the block.
       * 
       * @return compressed size of the block.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getStartPos,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getStartPos(),603,605,"/**
 * Returns the starting position of the block region as a long.
 */","* Get the starting position of the block in the file.
       * 
       * @return the starting position of the block in the file.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:write(byte[]),394,397,"/**
* Writes a byte array to the underlying output stream.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputOutputStream.java,constructOutputStream,org.apache.hadoop.io.DataOutputOutputStream:constructOutputStream(java.io.DataOutput),43,49,"/**
 * Returns an OutputStream, wrapping if necessary.
 * @param out DataOutput to wrap as OutputStream.
 * @return OutputStream.
 */
","* Construct an OutputStream from the given DataOutput. If 'out'
   * is already an OutputStream, simply returns it. Otherwise, wraps
   * it in an OutputStream.
   * @param out the DataOutput to wrap
   * @return an OutputStream instance that outputs to 'out'",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/FloatWritable.java,<init>,org.apache.hadoop.io.FloatWritable:<init>(float),34,34,"/**
 * Constructs a FloatWritable with the given float value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/FastByteComparisons.java,compareTo,"org.apache.hadoop.io.FastByteComparisons$LexicographicalComparerHolder$UnsafeComparer:compareTo(byte[],int,int,byte[],int,int)",188,242,"/**
 * Compares two byte buffers lexicographically.
 * @param buffer1 First byte array and offset/length.
 * @param buffer2 Second byte array and offset/length.
 */
","* Lexicographically compare two arrays.
       *
       * @param buffer1 left operand
       * @param buffer2 right operand
       * @param offset1 Where to start comparing in the left buffer
       * @param offset2 Where to start comparing in the right buffer
       * @param length1 How much to compare from the left buffer
       * @param length2 How much to compare from the right buffer
       * @return 0 if equal, < 0 if left is less than right, etc.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,<init>,org.apache.hadoop.io.DataOutputBuffer:<init>(),89,91,"/**
 * Constructs a DataOutputBuffer with a default buffer.
 */",Constructs a new empty buffer.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,<init>,org.apache.hadoop.io.DataOutputBuffer:<init>(int),93,95,"/**
 * Constructs a DataOutputBuffer with the specified initial size.
 * @param size initial buffer capacity
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,getData,org.apache.hadoop.io.DataOutputBuffer:getData(),108,108,"/**
 * Returns the underlying data buffer.
 * @return byte array containing the data.
 */
","* Returns the current contents of the buffer.
   *  Data is only valid to {@link #getLength()}.
   *
   * @return data byte.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,getLength,org.apache.hadoop.io.DataOutputBuffer:getLength(),114,114,"/**
 * Returns the length of the underlying buffer.
 */","* Returns the length of the valid data currently in the buffer.
   * @return length.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DataOutputBuffer.java,writeInt,"org.apache.hadoop.io.DataOutputBuffer:writeInt(int,int)",154,164,"/**
 * Writes an integer to the buffer at the specified offset.
 * @param v integer to write
 * @param offset offset in the buffer
 */
","* Overwrite an integer into the internal buffer. Note that this call can only
   * be used to overwrite existing data in the buffer, i.e., buffer#count cannot
   * be increased, and DataOutputStream#written cannot be increased.
   *
   * @param v v.
   * @param offset offset.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNSDomainNameResolver.java,getHostnameByIP,org.apache.hadoop.net.DNSDomainNameResolver:getHostnameByIP(java.net.InetAddress),44,62,"/**
 * Gets the hostname for a given IP address, handling cached IPs.
 * @param address InetAddress to resolve
 * @return Hostname string or null on failure.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getDistance,"org.apache.hadoop.net.NetworkTopology:getDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",324,365,"/**
 * Calculates the distance between two nodes in a tree structure.
 * @param node1 first node
 * @param node2 second node
 * @return distance between nodes, or Integer.MAX_VALUE if error
 */
","Return the distance between two nodes
   * It is assumed that the distance from one node to its parent is 1
   * The distance between two nodes is calculated by summing up their distances
   * to their closest common ancestor.
   * @param node1 one node
   * @param node2 another node
   * @return the distance between node1 and node2 which is zero if they are the same
   *  or {@link Integer#MAX_VALUE} if node1 or node2 do not belong to the cluster",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,isNodeInScope,"org.apache.hadoop.net.NetworkTopology:isNodeInScope(org.apache.hadoop.net.Node,java.lang.String)",1023,1029,"/**
 * Checks if a node is within a given scope.
 * @param node The node to check.
 * @param scope The scope string.
 * @return True if node is in scope, false otherwise.
 */
","* Checks whether a node belongs to the scope.
   * @param node  the node to check.
   * @param scope scope to check.
   * @return true if node lies within the scope",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,getPathComponents,org.apache.hadoop.net.NodeBase:getPathComponents(org.apache.hadoop.net.Node),124,126,"/**
 * Splits the node's path into components using the path separator.
 * @param node The node whose path is to be split.
 * @return An array of path components.
 */
","* Get the path components of a node.
   * @param node a non-null node
   * @return the path of a node",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,equals,org.apache.hadoop.net.NodeBase:equals(java.lang.Object),128,137,"/**
 * Checks if two NodeBase objects have the same path.
 * @param to The object to compare to.
 * @return True if paths are equal, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,hashCode,org.apache.hadoop.net.NodeBase:hashCode(),139,142,"/**
 * Returns the hash code, based on the path of this object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,toString,org.apache.hadoop.net.NodeBase:toString(),145,148,"/**
 * Returns a string representation of the object, using getPath.
 */
",@return this node's path as its string representation,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,remove,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:remove(org.apache.hadoop.net.Node),228,254,"/**
 * Removes a node from the cluster map. Throws exception for inner nodes.
 */","Remove a node
   * Update node counter and rack counter if necessary
   * @param node node to be removed; can be null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getDatanodesInRack,org.apache.hadoop.net.NetworkTopology:getDatanodesInRack(java.lang.String),202,215,"/**
 * Retrieves datanodes within a rack.
 * @param loc Rack location string.
 * @return List of Node objects in the rack.
 */
","* Given a string representation of a rack, return its children
   * @param loc a path-like string representation of a rack
   * @return a newly allocated list with all the node's children",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getNode,org.apache.hadoop.net.NetworkTopology:getNode(java.lang.String),271,281,"/**
 * Retrieves a node from the cluster map by its location.
 * @param loc Node location string.
 * @return Node object or null if not found.
 */
","Given a string representation of a node, return its reference
   * 
   * @param loc
   *          a path-like string representation of a node
   * @return a reference to the node; null if the node is not in the tree",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,locationToDepth,org.apache.hadoop.net.NodeBase:locationToDepth(java.lang.String),209,219,"/**
 * Calculates the directory depth of a location string.
 * @param location The location string to analyze.
 * @return The depth of the location (number of path separators).
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,toString,org.apache.hadoop.net.NetworkTopology:toString(),714,732,"/**
 * Returns a string representation of the object's state.
 */",convert a network tree to a string.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,isOnSameRack,"org.apache.hadoop.net.NetworkTopology:isOnSameRack(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",409,415,"/**
* Checks if two nodes are on the same rack.
* @param node1 The first node.
* @param node2 The second node.
* @return True if on the same rack, false otherwise.
*/
","Check if two nodes are on the same rack
   * @param node1 one node (can be null)
   * @param node2 another node (can be null)
   * @return true if node1 and node2 are on the same rack; false otherwise
   * @exception IllegalArgumentException when either node1 or node2 is null, or
   * node1 or node2 do not belong to the cluster",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,"org.apache.hadoop.net.NetworkTopology:chooseRandom(org.apache.hadoop.net.InnerNode,org.apache.hadoop.net.Node,java.util.Collection,int,int)",569,637,"/**
 * Chooses a random valid node from in-scope nodes, excluding specified nodes.
 * @param parentNode Parent node.
 * @return Valid node or null if none found.
 */
","* Randomly choose one node under <i>parentNode</i>, considering the exclude
   * nodes and scope. Should be called with {@link #netlock}'s readlock held.
   *
   * @param parentNode        the parent node
   * @param excludedScopeNode the node corresponding to the exclude scope.
   * @param excludedNodes     a collection of nodes to be excluded from
   * @param totalInScopeNodes total number of nodes under parentNode, excluding
   *                          the excludedScopeNode
   * @param availableNodes    number of available nodes under parentNode that
   *                          could be chosen, excluding excludedNodes
   * @return the chosen node, or null if none can be chosen",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getWeightUsingNetworkLocation,"org.apache.hadoop.net.NetworkTopology:getWeightUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",807,845,"/**
 * Calculates weight between nodes based on network location.
 * @param reader The reader node.
 * @param node The target node.
 * @return Weight representing distance between nodes.
 */
","* Returns an integer weight which specifies how far away <i>node</i> is
   * from <i>reader</i>. A lower value signifies that a node is closer.
   * It uses network location to calculate the weight
   *
   * @param reader Node where data will be read
   * @param node Replica of data
   * @return weight",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,interAddNodeWithEmptyRack,org.apache.hadoop.net.NetworkTopology:interAddNodeWithEmptyRack(org.apache.hadoop.net.Node),1083,1097,"/**
 * Adds a node to a rack in the rackMap, if not decommissioned.
 * @param node The node to add; null or decommissioned nodes are ignored.
 */
","* Internal function for update empty rack number
   * for add or recommission a node.
   * @param node node to be added; can be null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,<init>,"org.apache.hadoop.net.SocketIOWithTimeout:<init>(java.nio.channels.SelectableChannel,long)",58,66,"/**
 * Initializes SocketIO with a channel and timeout.
 * @param channel SelectableChannel for IO operations.
 * @param timeout Timeout duration in milliseconds.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,write,"org.apache.hadoop.net.SocketOutputStream:write(byte[],int,int)",111,129,"/**
 * Writes a byte array to the stream.
 * @param b buffer to write
 * @param off offset in the buffer
 * @param len number of bytes to write
 * @throws IOException if an I/O error occurs
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,transferToFully,"org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int,org.apache.hadoop.io.LongWritable,org.apache.hadoop.io.LongWritable)",202,251,"/**
 * Transfers data to a channel, handling potential writeability issues.
 * @param fileCh destination channel
 * @param position starting position
 * @param count number of bytes to transfer
 * @param waitForWritableTime stores wait time
 * @param transferToTime stores transfer time
 */
","* Transfers data from FileChannel using 
   * {@link FileChannel#transferTo(long, long, WritableByteChannel)}.
   * Updates <code>waitForWritableTime</code> and <code>transferToTime</code>
   * with the time spent blocked on the network and the time spent transferring
   * data from disk to network respectively.
   * 
   * Similar to readFully(), this waits till requested amount of 
   * data is transfered.
   * 
   * @param fileCh FileChannel to transfer data from.
   * @param position position within the channel where the transfer begins
   * @param count number of bytes to transfer.
   * @param waitForWritableTime nanoseconds spent waiting for the socket 
   *        to become writable
   * @param transferToTime nanoseconds spent transferring data
   * 
   * @throws EOFException 
   *         If end of input file is reached before requested number of 
   *         bytes are transfered.
   *
   * @throws SocketTimeoutException 
   *         If this channel blocks transfer longer than timeout for 
   *         this stream.
   *          
   * @throws IOException Includes any exception thrown by 
   *         {@link FileChannel#transferTo(long, long, WritableByteChannel)}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,normalizeHostNames,org.apache.hadoop.net.NetUtils:normalizeHostNames(java.util.Collection),662,668,"/**
 * Normalizes a collection of hostnames.
 * @param names Collection of hostnames to normalize.
 * @return List of normalized hostnames.
 */
","* Given a collection of string representation of hosts, return a list of
   * corresponding IP addresses in the textual representation.
   * 
   * @param names a collection of string representations of hosts
   * @return a list of corresponding IP addresses in the string format
   * @see #normalizeHostName(String)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getHostDetailsAsString,"org.apache.hadoop.net.NetUtils:getHostDetailsAsString(java.lang.String,int,java.lang.String)",977,988,"/**
 * Creates a string with host details: local & destination host, port.
 */","* Get the host details as a string
   * @param destHost destinatioon host (nullable)
   * @param destPort destination port
   * @param localHost local host (nullable)
   * @return a string describing the destination host:port and the local host",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getIPs,"org.apache.hadoop.net.NetUtils:getIPs(java.lang.String,boolean)",1041,1068,"/**
 * Retrieves a list of InetAddresses matching the given subnet.
 * @param subnet Subnet string (e.g., ""192.168.1.0/24"")
 * @param returnSubinterfaces Whether to include subinterfaces
 * @return List of InetAddress objects.
 */
","* Return an InetAddress for each interface that matches the
   * given subnet specified using CIDR notation.
   *
   * @param subnet subnet specified using CIDR notation
   * @param returnSubinterfaces
   *            whether to return IPs associated with subinterfaces
   * @throws IllegalArgumentException if subnet is invalid
   * @return ips.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getFreeSocketPorts,org.apache.hadoop.net.NetUtils:getFreeSocketPorts(int),1098,1113,"/**
 * Acquires a set of free socket ports.
 * @param numOfPorts Number of ports to acquire (1-25).
 * @return Set of free port integers.
 */","* Return free ports. There is no guarantee they will remain free, so
   * ports should be used immediately. The number of free ports returned by
   * this method should match argument {@code numOfPorts}. Num of ports
   * provided in the argument should not exceed 25.
   *
   * @param numOfPorts Number of free ports to acquire.
   * @return Free ports for binding a local socket.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,getConf,org.apache.hadoop.net.TableMapping:getConf(),71,74,"/**
 * Returns the configuration of the raw mapping.
 * @return Configuration object representing the mapping config.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,setConf,org.apache.hadoop.net.TableMapping:setConf(org.apache.hadoop.conf.Configuration),76,80,"/**
 * Sets the configuration for this object and its raw mapping.
 * @param conf The Configuration object to set.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:<init>(),172,172,"/**
 * Default constructor for RawScriptBasedMapping class.
 */
","* Constructor. The mapping is not ready to use until
     * {@link #setConf(Configuration)} has been called",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/CachedDNSToSwitchMapping.java,<init>,org.apache.hadoop.net.CachedDNSToSwitchMapping:<init>(org.apache.hadoop.net.DNSToSwitchMapping),50,52,"/**
 * Constructs a CachedDNSToSwitchMapping with the given raw mapping.
 */","* cache a raw DNS mapping
   * @param rawMapping the raw mapping to cache",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,<init>,org.apache.hadoop.net.NodeBase:<init>(java.lang.String),53,61,"/**
 * Constructs a NodeBase object from a given path, normalizing it.
 */
","Construct a node from its path
   * @param path 
   *   a concatenation of this node's location, the path separator, and its name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,<init>,"org.apache.hadoop.net.NodeBase:<init>(java.lang.String,java.lang.String)",67,69,"/**
 * Constructs a NodeBase with the given name and normalized location.
 */","Construct a node from its name and its location
   * @param name this node's name (can be null, must not contain {@link #PATH_SEPARATOR})
   * @param location this node's location",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NodeBase.java,<init>,"org.apache.hadoop.net.NodeBase:<init>(java.lang.String,java.lang.String,org.apache.hadoop.net.Node,int)",77,81,"/**
 * Constructs a new NodeBase with a name, location, parent, and level.
 */
","Construct a node from its name and its location
   * @param name this node's name (can be null, must not contain {@link #PATH_SEPARATOR})
   * @param location this node's location 
   * @param parent this node's parent node
   * @param level this node's level in the tree",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,getConf,org.apache.hadoop.net.ScriptBasedMapping:getConf(),116,119,"/**
 * Returns the configuration associated with this mapping.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,toString,org.apache.hadoop.net.ScriptBasedMapping:toString(),121,124,"/**
 * Returns a string representation of the mapping.
 * Includes the raw mapping string representation.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,read,org.apache.hadoop.net.unix.DomainSocket$DomainChannel:read(java.nio.ByteBuffer),602,628,"/**
 * Reads data from the socket into the provided ByteBuffer.
 * @param dst Destination buffer to read into.
 * @return Number of bytes read, or 0 if EOF.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,write,org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:write(int),563,575,"/**
 * Writes an integer value as a single byte to the socket.
 * @param val The integer value to write (cast to a byte).
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,write,"org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:write(byte[],int,int)",577,587,"/**
 * Writes a byte array to the socket.
 * @param b buffer to write
 * @param off offset in the buffer
 * @param len number of bytes to write
 * @throws IOException if an I/O error occurs
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,read,org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:read(),507,519,"/**
 * Reads a single byte from the domain socket.
 * @return Byte read or -1 if error. Throws IOException.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,read,"org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:read(byte[],int,int)",521,532,"/**
 * Reads bytes from the socket into the provided byte array.
 * @param b byte array to read into
 * @param off offset in the array to start writing
 * @param len maximum number of bytes to read
 * @return number of bytes read
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,available,org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:available(),534,545,"/**
 * Returns the number of bytes available for reading.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,<init>,"org.apache.hadoop.net.unix.DomainSocket:<init>(java.lang.String,int)",168,172,"/**
 * Constructs a DomainSocket with the given file descriptor and path.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,sendCallback,"org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallback(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)",386,424,"/**
 * Sends a callback, potentially closing the file descriptor.
 * @param caller Caller's name.
 * @param entries TreeMap of entries.
 * @param fdSet File descriptor set.
 * @param fd File descriptor.
 * @return True if callback closed the fd, false otherwise.
 */
","* Send callback and return whether or not the domain socket was closed as a
   * result of processing.
   *
   * @param caller reason for call
   * @param entries mapping of file descriptor to entry
   * @param fdSet set of file descriptors
   * @param fd file descriptor
   * @return true if the domain socket was closed as a result of processing",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,isOpen,org.apache.hadoop.net.unix.DomainSocket:isOpen(),268,270,"/**
 * Checks if the reference count is open.
 * @return True if open, false otherwise.
 */
","* Return true if the file descriptor is currently open.
   *
   * @return                 True if the file descriptor is currently open.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket:close(),344,388,"/**
 * Closes the DomainSocket, releasing resources and closing the FD.
 */",* Close the Socket.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,addNotificationSocket,"org.apache.hadoop.net.unix.DomainSocketWatcher:addNotificationSocket(java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet)",546,561,"/**
 * Adds a notification socket to the entry map and fd set.
 * Adds socket fd to entries and fdSet, increments refCount.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,trimIdleSelectors,org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:trimIdleSelectors(long),426,444,"/**
 * Removes idle selectors from the provider map.
 * @param now Current timestamp to determine idle time.
 */
","* Closes selectors that are idle for IDLE_TIMEOUT (10 sec). It does not
     * traverse the whole list, just over the one that have crossed 
     * the timeout.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,isLeafParent,org.apache.hadoop.net.InnerNodeImpl:isLeafParent(),302,304,"/**
 * Checks if the current node is a leaf parent (is a rack).
 * Returns true if it is a rack, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,getNextAncestorName,org.apache.hadoop.net.InnerNodeImpl:getNextAncestorName(org.apache.hadoop.net.Node),113,127,"/**
 * Gets the name of the next ancestor node.
 * @param n The ancestor node.
 * @return The name of the next ancestor.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.net.InetAddress,int)",70,76,"/**
 * Creates a socket connected to the specified address and port.
 * @param addr The address to connect to.
 * @param port The port to connect to.
 * @return A connected Socket object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",78,86,"/**
 * Creates a socket connected to the specified address and port.
 * @param addr Remote address to connect to.
 * @param port Remote port to connect to.
 * @return A Socket object.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.lang.String,int)",88,95,"/**
 * Creates a socket connected to the specified host and port.
 * @param host The hostname to connect to.
 * @param port The port number to connect to.
 * @return A connected Socket object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,createSocket,"org.apache.hadoop.net.SocksSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",97,106,"/**
 * Creates a socket connected to the specified host and port.
 * @param host Host address.
 * @param port Port number.
 * @return Socket object.
 * @throws IOException, UnknownHostException
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.net.InetAddress,int)",65,71,"/**
 * Creates a socket connected to the specified address and port.
 * @param addr The address to connect to.
 * @param port The port to connect to.
 * @return A connected Socket object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",73,81,"/**
 * Creates a socket connected to the specified address and port.
 * @param addr Remote address to connect to.
 * @param port Remote port to connect to.
 * @return A Socket object.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.lang.String,int)",83,90,"/**
 * Creates a socket connected to the specified host and port.
 * @param host The hostname to connect to.
 * @param port The port number to connect to.
 * @return A connected Socket object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/StandardSocketFactory.java,createSocket,"org.apache.hadoop.net.StandardSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",92,101,"/**
 * Creates a socket connected to the specified host and port.
 * @param host Hostname or IP address.
 * @param port Port number.
 * @return Socket object.
 * @throws IOException, UnknownHostException
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,toString,org.apache.hadoop.net.ScriptBasedMappingWithDependency:toString(),71,74,"/**
 * Returns a string representation of the mapping.
 * Includes the raw mapping string representation.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,getDependency,org.apache.hadoop.net.ScriptBasedMappingWithDependency:getDependency(java.lang.String),96,115,"/**
 * Retrieves dependencies for a given name, using cache if available.
 * @param name The name to retrieve dependencies for.
 * @return List of dependencies or an empty list if not found.
 */
","* Get dependencies in the topology for a given host
   * @param name - host name for which we are getting dependency
   * @return a list of hosts dependent on the provided host name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputWrapper.java,setTimeout,org.apache.hadoop.net.SocketInputWrapper:setTimeout(long),69,75,"/**
 * Sets the timeout value for the socket connection.
 * @param timeoutMs Timeout in milliseconds.
 * @throws SocketException if an error occurs during timeout setting.
 */
","* Set the timeout for reads from this stream.
   * 
   * Note: the behavior here can differ subtly depending on whether the
   * underlying socket has an associated Channel. In particular, if there is no
   * channel, then this call will affect the socket timeout for <em>all</em>
   * readers of this socket. If there is a channel, then this call will affect
   * the timeout only for <em>this</em> stream. As such, it is recommended to
   * only create one {@link SocketInputWrapper} instance per socket.
   * 
   * @param timeoutMs
   *          the new timeout, 0 for no timeout
   * @throws SocketException
   *           if the timeout cannot be set",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getIPs,"org.apache.hadoop.net.DNS:getIPs(java.lang.String,boolean)",175,209,"/**
 * Gets IP addresses for a network interface.
 * @param strInterface Interface name, ""default"" for cached address.
 * @param returnSubinterfaces Whether to include subinterface IPs.
 * @return String array of IP addresses.
 */
","* Returns all the IPs associated with the provided interface, if any, in
   * textual form.
   * 
   * @param strInterface
   *            The name of the network interface or sub-interface to query
   *            (eg eth0 or eth0:0) or the string ""default""
   * @param returnSubinterfaces
   *            Whether to return IPs associated with subinterfaces of
   *            the given interface
   * @return A string vector of all the IPs associated with the provided
   *         interface. The local host IP is returned if the interface
   *         name ""default"" is specified or there is an I/O error looking
   *         for the given interface.
   * @throws UnknownHostException
   *             If the given interface is invalid
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getIPsAsInetAddressList,"org.apache.hadoop.net.DNS:getIPsAsInetAddressList(java.lang.String,boolean)",428,457,"/**
 * Gets a list of InetAddress objects for a network interface.
 * @param strInterface Interface name, ""default"" uses cached address.
 * @param returnSubinterfaces Whether to include subinterface addresses.
 * @return List of InetAddress objects.
 */
","* Returns all the IPs associated with the provided interface, if any, as
   * a list of InetAddress objects.
   *
   * @param strInterface
   *            The name of the network interface or sub-interface to query
   *            (eg eth0 or eth0:0) or the string ""default""
   * @param returnSubinterfaces
   *            Whether to return IPs associated with subinterfaces of
   *            the given interface
   * @return A list of all the IPs associated with the provided
   *         interface. The local host IP is returned if the interface
   *         name ""default"" is specified or there is an I/O error looking
   *         for the given interface.
   * @throws UnknownHostException
   *             If the given interface is invalid
   *",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,read,"org.apache.hadoop.net.SocketInputStream:read(byte[],int,int)",129,132,"/**
 * Reads bytes from the buffer.
 * @param b byte array, off offset, len length
 * @return Number of bytes read
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getRack,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getRack(java.lang.String),57,80,"/**
 * Gets the rack for a given location.
 * @param loc The location string to check.
 * @return The rack string or network location, null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getNodeGroup,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getNodeGroup(java.lang.String),90,118,"/**
 * Retrieves the node group for a given location.
 * @param loc The location string to search for a node group.
 * @return The node group string or null if not found.
 */","* Given a string representation of a node group for a specific network
   * location
   * 
   * @param loc
   *            a path-like string representation of a network location
   * @return a node group string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/AbstractDNSToSwitchMapping.java,dumpTopology,org.apache.hadoop.net.AbstractDNSToSwitchMapping:dumpTopology(),112,133,"/**
 * Generates a string representation of the network topology.
 * Returns topology string or ""No topology information"" if empty.
 */
","* Generate a string listing the switch mapping implementation,
   * the mapping for every known node and the number of nodes and
   * unique switches known about -each entry to a separate line.
   * @return a string that can be presented to the ops team or used in
   * debug messages.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/AbstractDNSToSwitchMapping.java,isMappingSingleSwitch,org.apache.hadoop.net.AbstractDNSToSwitchMapping:isMappingSingleSwitch(org.apache.hadoop.net.DNSToSwitchMapping),150,153,"/**
 * Checks if a DNS mapping represents a single switch.
 * @param mapping The DNS to switch mapping to check.
 * @return True if the mapping is a single switch, false otherwise.
 */
","* Query for a {@link DNSToSwitchMapping} instance being on a single
   * switch.
   * <p>
   * This predicate simply assumes that all mappings not derived from
   * this class are multi-switch.
   * @param mapping the mapping to query
   * @return true if the base class says it is single switch, or the mapping
   * is not derived from this class.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getWeight,"org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getWeight(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",256,271,"/**
 * Calculates node weight based on reader and node location.
 * Returns 0-3, lower values indicate closer proximity.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,writeAttribute,"org.apache.hadoop.jmx.JMXJsonServlet:writeAttribute(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,javax.management.MBeanAttributeInfo)",330,388,"/**
* Writes a bean attribute value to the JSON generator.
* @param jg JSON generator, oname ObjectName, attr attribute info
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,writeObject,"org.apache.hadoop.jmx.JMXJsonServlet:writeObject(com.fasterxml.jackson.core.JsonGenerator,java.lang.Object,java.lang.String)",395,436,"/**
 * Writes an object to a JsonGenerator, handling various data types.
 * @param jg JsonGenerator to write to
 * @param value Object to serialize
 * @param attName Attribute name (unused)
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,getCurrentStats,"org.apache.hadoop.log.LogThrottlingHelper:getCurrentStats(java.lang.String,int)",290,297,"/**
 * Gets current stats for a recorder at a given index.
 * @param recorderName Recorder name.
 * @param idx Index of the statistics.
 * @return SummaryStatistics object or null if not found.
 */
","* Return the summary information for given index.
   *
   * @param recorderName The name of the recorder.
   * @param idx The index value.
   * @return The summary information.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseProtocolArgs,"org.apache.hadoop.log.LogLevel$CLI:parseProtocolArgs(java.lang.String[],int)",209,228,"/**
 * Parses protocol arguments from the array.
 * @param args Command-line arguments.
 * @param index Starting index for parsing.
 * @return Updated index after parsing.
 * @throws HadoopIllegalArgumentException if arguments are invalid.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,printUsage,org.apache.hadoop.log.LogLevel:printUsage(),87,90,"/**
 * Prints the program's usage instructions to the error stream.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ToolRunner.java,printGenericCommandUsage,org.apache.hadoop.util.ToolRunner:printGenericCommandUsage(java.io.PrintStream),105,107,"/**
 * Prints generic command usage information to the specified output stream.
 */","* Prints generic command-line argurments and usage information.
   * 
   *  @param out stream to write usage information to.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericsUtil.java,isLog4jLogger,org.apache.hadoop.util.GenericsUtil:isLog4jLogger(java.lang.Class),95,100,"/**
 * Checks if a class is a Log4j logger.
 * @param clazz Class to check, null if invalid.
 * @return True if it's a Log4j logger, false otherwise.
 */
","* Determine whether the log of <code>clazz</code> is Log4j implementation.
   * @param clazz a class to be determined
   * @return true if the log of <code>clazz</code> is Log4j implementation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,<init>,"org.apache.hadoop.log.LogThrottlingHelper:<init>(long,java.lang.String)",159,161,"/**
 * Constructs a LogThrottlingHelper with a Timer.
 * @param minLogPeriodMs Minimum logging period in milliseconds.
 * @param primaryRecorderName Name of the primary recorder.
 */
","* Create a log helper with a specified primary recorder name; this can be
   * used in conjunction with {@link #record(String, long, double...)} to set up
   * primary and dependent recorders. See
   * {@link #record(String, long, double...)} for more details.
   *
   * @param minLogPeriodMs The minimum period with which to log; do not log
   *                       more frequently than this.
   * @param primaryRecorderName The name of the primary recorder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,record,"org.apache.hadoop.log.LogThrottlingHelper:record(java.lang.String,long,double[])",247,281,"/**
 * Records values for a given recorder, creating new logs if needed.
 * @param recorderName Name of the recorder.
 * @param currentTimeMs Current timestamp in milliseconds.
 * @param values Values to record.
 * @return LoggingAction object or DO_NOT_LOG if not logged.
 */
","* Record some set of values at the specified time into this helper. This can
   * be useful to avoid fetching the current time twice if the caller has
   * already done so for other purposes. This additionally allows the caller to
   * specify a name for this recorder. When multiple names are used, one is
   * denoted as the primary recorder. Only recorders named as the primary
   * will trigger logging; other names not matching the primary can <i>only</i>
   * be triggered by following the primary. This is used to coordinate multiple
   * logging points. A primary can be set via the
   * {@link #LogThrottlingHelper(long, String)} constructor. If no primary
   * is set in the constructor, then the first recorder name used becomes the
   * primary.
   *
   * If multiple names are used, they maintain entirely different sets of values
   * and summary information. For example:
   * <pre>{@code
   *   // Initialize ""pre"" as the primary recorder name
   *   LogThrottlingHelper helper = new LogThrottlingHelper(1000, ""pre"");
   *   LogAction preLog = helper.record(""pre"", Time.monotonicNow());
   *   if (preLog.shouldLog()) {
   *     // ...
   *   }
   *   double eventsProcessed = ... // perform some action
   *   LogAction postLog =
   *       helper.record(""post"", Time.monotonicNow(), eventsProcessed);
   *   if (postLog.shouldLog()) {
   *     // ...
   *     // Can use postLog.getStats(0) to access eventsProcessed information
   *   }
   * }</pre>
   * Since ""pre"" is the primary recorder name, logging to ""pre"" will trigger a
   * log action if enough time has elapsed. This will indicate that ""post""
   * should log as well. This ensures that ""post"" is always logged in the same
   * iteration as ""pre"", yet each one is able to maintain its own summary
   * information.
   *
   * <p>Other behavior is the same as {@link #record(double...)}.
   *
   * @param recorderName The name of the recorder. This is used to check if the
   *                     current recorder is the primary. Other names are
   *                     arbitrary and are only used to differentiate between
   *                     distinct recorders.
   * @param currentTimeMs The current time.
   * @param values The values to log.
   * @return The LogAction for the specified recorder.
   *
   * @see #record(double...)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,fromInternalName,org.apache.hadoop.http.ProfileServlet$Event:fromInternalName(java.lang.String),148,156,"/**
 * Returns the event by its internal name, case-insensitive.
 * @param name The internal name of the event to find.
 * @return The matching Event or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfilerDisabledServlet.java,doGet,"org.apache.hadoop.http.ProfilerDisabledServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",34,48,"/**
 * Handles GET requests, returns an error message indicating servlet disabled.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,<init>,org.apache.hadoop.http.ProfileServlet:<init>(),177,181,"/**
 * Initializes the servlet, setting PID and asyncProfilerHome.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HtmlQuoting.java,needsQuoting,org.apache.hadoop.http.HtmlQuoting:needsQuoting(java.lang.String),68,74,"/**
 * Checks if a string needs quoting based on its byte representation.
 * @param str The string to check.
 * @return True if quoting is needed, false otherwise.
 */
","* Does the given string need to be quoted?
   * @param str the string to check
   * @return does the string contain any of the active html characters?",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HtmlQuoting.java,quoteHtmlChars,org.apache.hadoop.http.HtmlQuoting:quoteHtmlChars(java.lang.String),114,131,"/**
 * Quotes HTML characters in a string. Returns null if input is null.
 */","* Quote the given item to make it html-safe.
   * @param item the string to quote
   * @return the quoted string",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addJerseyResourcePackage,"org.apache.hadoop.http.HttpServer2:addJerseyResourcePackage(java.lang.String,java.lang.String)",1018,1022,"/**
 * Adds a Jersey resource package with optional parameters.
 * @param packageName Package name to add.
 * @param pathSpec Resource path specification.
 */
","* Add a Jersey resource package.
   * @param packageName The Java package name containing the Jersey resource.
   * @param pathSpec The path spec for the servlet",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addServlet,"org.apache.hadoop.http.HttpServer2:addServlet(java.lang.String,java.lang.String,java.lang.Class)",1050,1053,"/**
 * Registers a servlet with the given name and URL pattern.
 * @param name Servlet name.
 * @param pathSpec URL pattern.
 * @param clazz Servlet class.
 */
","* Add a servlet in the server.
   * @param name The name of the servlet (can be passed as null)
   * @param pathSpec The path spec for the servlet
   * @param clazz The servlet class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addInternalServlet,"org.apache.hadoop.http.HttpServer2:addInternalServlet(java.lang.String,java.lang.String,java.lang.Class)",1065,1068,"/**
 * Adds a servlet with the given name, path, and class.
 * @param name Servlet name.
 * @param pathSpec Servlet path spec.
 * @param clazz Servlet class extending HttpServlet.
 */
","* Add an internal servlet in the server.
   * Note: This method is to be used for adding servlets that facilitate
   * internal communication and not for user facing functionality. For
   * servlets added using this method, filters are not enabled.
   *
   * @param name The name of the servlet (can be passed as null)
   * @param pathSpec The path spec for the servlet
   * @param clazz The servlet class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addFilter,"org.apache.hadoop.http.HttpServer2:addFilter(java.lang.String,java.lang.String,java.util.Map)",1170,1193,"/**
 * Adds a filter to the web application context and default contexts.
 * @param name Filter name
 * @param classname Filter class name
 * @param parameters Filter parameters
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addGlobalFilter,"org.apache.hadoop.http.HttpServer2:addGlobalFilter(java.lang.String,java.lang.String,java.util.Map)",1195,1206,"/**
 * Adds a global filter with the given name, class, and parameters.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,defineFilter,"org.apache.hadoop.http.HttpServer2:defineFilter(org.eclipse.jetty.servlet.ServletContextHandler,java.lang.String,java.lang.String,java.util.Map,java.lang.String[])",1217,1222,"/**
 * Defines a filter in the ServletContextHandler.
 * @param ctx Handler, name, classname, params, URLs for filter.
 */
","* Define a filter for a context and set up default url mappings.
   *
   * @param ctx ctx.
   * @param name name.
   * @param classname classname.
   * @param parameters parameters.
   * @param urls urls.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,bindForSinglePort,"org.apache.hadoop.http.HttpServer2:bindForSinglePort(org.eclipse.jetty.server.ServerConnector,int)",1478,1493,"/**
 * Binds a connector to a port, retrying with incremented ports.
 * @param listener The connector to bind.
 * @param port Initial port to try.
 * @throws Exception if binding fails after retries.
 */
","* Bind using single configured port. If findPort is true, we will try to bind
   * after incrementing port till a free port is found.
   * @param listener jetty listener.
   * @param port port which is set in the listener.
   * @throws Exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,toString,org.apache.hadoop.http.HttpServer2:toString(),1631,1642,"/**
 * Returns a string representation of the HttpServer, including listeners.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getEnum,org.apache.hadoop.http.HttpServer2$XFrameOption:getEnum(java.lang.String),1946,1954,"/**
 * Finds an XFrameOption enum by its string representation.
 * @param value The string value of the enum to find.
 * @throws IllegalArgumentException if no matching enum is found.
 */
","* We cannot use valueOf since the AllowFrom enum differs from its value
     * Allow-From. This is a helper method that does exactly what valueof does,
     * but allows us to handle the AllowFrom issue gracefully.
     *
     * @param value - String must be DENY, SAMEORIGIN or ALLOW-FROM.
     * @return XFrameOption or throws IllegalException.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,init,org.apache.hadoop.http.HttpServer2$QuotingInputFilter:init(javax.servlet.FilterConfig),1860,1864,"/**
 * Initializes the filter, storing the config and initializing headers.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,doFilter,"org.apache.hadoop.http.HttpServer2$QuotingInputFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",1870,1893,"/**
 * Filters a request, sets content type, adds headers, and passes to chain.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/lib/StaticUserWebFilter.java,init,org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter:init(javax.servlet.FilterConfig),114,118,"/**
 * Initializes the filter with configuration parameters.
 * @param conf Filter configuration object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileMonitoringTimerTask.java,<init>,"org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<init>(java.util.List,java.util.function.Consumer,java.util.function.Consumer)",75,89,"/**
 * Creates a FileMonitoringTimerTask to monitor file changes.
 * @param filePaths Paths to monitor; onFileChange action; onChangeFailure handler.
 */
","* Create file monitoring task to be scheduled using a standard
   * Java {@link java.util.Timer} instance.
   *
   * @param filePaths The path to the file to monitor.
   * @param onFileChange The function to call when the file has changed.
   * @param onChangeFailure The function to call when an exception is
   *                       thrown during the file change processing.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getNonNegative,"org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getNonNegative(java.lang.String,int)",408,417,"/**
 * Gets property value as int, ensuring it's non-negative.
 * @param key property key, @param defaultValue default int value
 * @return int property value, throws exception if negative.
 */
","* Return the property value if it's non-negative and throw an exception if
   * it's not.
   *
   * @param key the property key
   * @param defaultValue the default value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,checkIfPropertyExists,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkIfPropertyExists(java.lang.String),424,429,"/**
 * Checks if a property exists in the properties map.
 * @param key The property key to check for existence.
 */
","* Throw a {@link MetricsException} if the given property is not set.
   *
   * @param key the key to validate",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,checkForErrors,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkForErrors(java.lang.String),905,910,"/**
 * Throws MetricsException if an error is detected in the stream.
 * @param message Error message to include in the exception.
 */
","* If the sink isn't set to ignore errors, throw a {@link MetricsException}
   * if the stream encountered an exception.  The message parameter will be used
   * as the new exception's message with the current file name
   * ({@link #currentFilePath}) appended to it.
   *
   * @param message the exception message. The message will have a colon and
   * the current file name ({@link #currentFilePath}) appended to it.
   * @throws MetricsException thrown if there was an error and the sink isn't
   * ignoring errors",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,throwMetricsException,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:throwMetricsException(java.lang.String),940,944,"/**
 * Throws a MetricsException if ignoreError is false.
 * @param message Error message to include in the exception.
 */
","* If the sink isn't set to ignore errors, throw a new
   * {@link MetricsException}.  The message parameter will be used  as the
   * new exception's message with the current file name
   * ({@link #currentFilePath}) appended to it.
   *
   * @param message the exception message. The message will have a colon and
   * the current file name ({@link #currentFilePath}) appended to it.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfigException.java,<init>,org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.String),29,31,"/**
 * Constructs a MetricsConfigException with the given error message.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,checkMetricName,org.apache.hadoop.metrics2.lib.MetricsRegistry:checkMetricName(java.lang.String),434,452,"/**
 * Validates metric name: checks for whitespace and existence.
 * @param name The metric name to validate.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,checkTagName,org.apache.hadoop.metrics2.lib.MetricsRegistry:checkTagName(java.lang.String),454,458,"/**
 * Checks if a tag already exists in the tags map.
 * @param name The tag name to check.
 * @throws MetricsException if the tag already exists.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsFactory.java,getInstance,org.apache.hadoop.metrics2.lib.DefaultMetricsFactory:getInstance(java.lang.Class),37,46,"/**
 * Returns an instance of the requested metrics factory class.
 * @param cls The class of the metrics factory to retrieve.
 * @return An instance of the class, or throws MetricsException.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,build,org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:build(),76,92,"/**
 * Builds and returns a MetricsSource object.
 * Returns existing source or creates a new anonymous one.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newTag,org.apache.hadoop.metrics2.lib.MethodMetric:newTag(java.lang.Class),125,139,"/**
 * Creates a MutableMetric for String tags.
 * @param resType The resource type (must be String.class).
 * @throws MetricsException if resType is not String.class.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getRollInterval,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getRollInterval(),341,399,"/**
 * Parses and validates the roll interval from properties, returning millis.
 */","* Extract the roll interval from the configuration and return it in
   * milliseconds.
   *
   * @return the roll interval in millis",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,throwMetricsException,"org.apache.hadoop.metrics2.sink.RollingFileSystemSink:throwMetricsException(java.lang.String,java.lang.Throwable)",924,929,"/**
 * Throws a MetricsException if ignoreError is false.
 * @param message Exception message.
 * @param t The original Throwable.
 */
","* If the sink isn't set to ignore errors, wrap the Throwable in a
   * {@link MetricsException} and throw it.  The message parameter will be used
   * as the new exception's message with the current file name
   * ({@link #currentFilePath}) and the Throwable's string representation
   * appended to it.
   *
   * @param message the exception message. The message will have a colon, the
   * current file name ({@link #currentFilePath}), and the Throwable's string
   * representation (wrapped in square brackets) appended to it.
   * @param t the Throwable to wrap",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/FileSink.java,init,org.apache.hadoop.metrics2.sink.FileSink:init(org.apache.commons.configuration2.SubsetConfiguration),45,55,"/**
 * Initializes the writer using the provided configuration.
 * @param conf SubsetConfiguration object containing filename.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfigException.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.String,java.lang.Throwable)",33,35,"/**
 * Constructs a MetricsConfigException with message and cause.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfigException.java,<init>,org.apache.hadoop.metrics2.impl.MetricsConfigException:<init>(java.lang.Throwable),37,39,"/**
 * Constructs a MetricsConfigException with a given cause.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsTag.java,equals,org.apache.hadoop.metrics2.MetricsTag:equals(java.lang.Object),71,78,"/**
 * Checks if this MetricsTag is equal to another object.
 * @param obj The object to compare to.
 * @return True if equal, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsTag.java,toString,org.apache.hadoop.metrics2.MetricsTag:toString(),84,89,"/**
 * Returns a string representation of the object.
 * Includes class name, info, and value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,appendPrefix,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:appendPrefix(org.apache.hadoop.metrics2.MetricsRecord,java.lang.StringBuilder)",86,106,"/**
 * Appends relevant metrics tags to a StringBuilder.
 * @param record The MetricsRecord to extract tags from.
 * @param sb The StringBuilder to append to.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/AbstractPatternFilter.java,accepts,org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(org.apache.hadoop.metrics2.MetricsTag),102,119,"/**
 * Checks if a tag is accepted based on include/exclude patterns.
 * @param tag The MetricsTag to check. Returns true if accepted.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/AbstractPatternFilter.java,accepts,org.apache.hadoop.metrics2.filter.AbstractPatternFilter:accepts(java.lang.Iterable),121,142,"/**
* Checks if the tags match include/exclude patterns.
* Returns true if tags are accepted, false otherwise.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordImpl.java,context,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:context(),72,80,"/**
 * Returns the context tag value, or DEFAULT_CONTEXT if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,add,"org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.MetricsInfo,java.lang.Object)",61,63,"/**
 * Adds a metric name and its string representation to the builder.
 * @param info Metric info containing the name.
 * @param value Value to be converted to string.
 * @return MetricStringBuilder with the added metric.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,add,org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.MetricsTag),86,89,"/**
 * Adds a metrics tag to the builder.
 * @param tag The metrics tag to add.
 * @return The builder instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,setContext,org.apache.hadoop.metrics2.MetricStringBuilder:setContext(java.lang.String),97,100,"/**
 * Sets the context value for the metrics record.
 * @param value The context string to set.
 * @return MetricsRecordBuilder for chaining.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/AbstractMetric.java,equals,org.apache.hadoop.metrics2.AbstractMetric:equals(java.lang.Object),75,82,"/**
 * Checks if two AbstractMetric objects are equal based on info and value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/FileSink.java,putMetrics,org.apache.hadoop.metrics2.sink.FileSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),57,80,"/**
 * Writes a MetricsRecord to the writer, formatting with tags and metrics.
 * @param record The metrics record to write.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MetricsCache.java,update,"org.apache.hadoop.metrics2.util.MetricsCache:update(org.apache.hadoop.metrics2.MetricsRecord,boolean)",154,177,"/**
 * Updates a record with metrics and tags, caching as needed.
 * @param mr The MetricsRecord to update from.
 * @param includingTags Whether to include tags in the record.
 * @return The updated Record object.
 */
","* Update the cache and return the current cached record
   * @param mr the update record
   * @param includingTags cache tag values (for later lookup by name) if true
   * @return the updated cache record",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java,loadGangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:loadGangliaConf(org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaConfType),185,218,"/**
 * Loads Ganglia configuration properties based on the provided type.
 * @param gtype Ganglia configuration type to load
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java,xdr_string,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:xdr_string(java.lang.String),245,252,"/**
 * Writes a string to the buffer using XDR encoding.
 * @param s The string to encode.
 */
","* Puts a string into the buffer by first writing the size of the string as an
   * int, followed by the bytes of the string, padded if necessary to a multiple
   * of 4.
   * @param s the string to be written to buffer at offset location",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink31.java,emitMetric,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink31:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",47,104,"/**
 * Emits a metric to Ganglia, including metadata and value.
 * @param groupName Metric group name.
 * @param name Metric name.
 * @param type Metric type.
 * @param value Metric value.
 */
","* The method sends metrics to Ganglia servers. The method has been taken from
   * org.apache.hadoop.metrics.ganglia.GangliaContext31 with minimal changes in
   * order to keep it in sync.
   * @param groupName The group name of the metric
   * @param name The metric name
   * @param type The type of the metric
   * @param value The value of the metric
   * @param gConf The GangliaConf for this metric
   * @param gSlope The slope for this metric
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,emitMetric,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:emitMetric(java.lang.String,java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",221,252,"/**
 * Emits a metric to Ganglia, logging warnings for null values.
 * @param groupName Metric group name.
 * @param name Metric name.
 * @param type Metric type.
 * @param value Metric value.
 */
","* The method sends metrics to Ganglia servers. The method has been taken from
   * org.apache.hadoop.metrics.ganglia.GangliaContext30 with minimal changes in
   * order to keep it in sync.
   * @param groupName The group name of the metric
   * @param name The metric name
   * @param type The type of the metric
   * @param value The value of the metric
   * @param gConf The GangliaConf for this metric
   * @param gSlope The slope for this metric
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,calculateSlope,"org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:calculateSlope(org.apache.hadoop.metrics2.sink.ganglia.GangliaConf,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink$GangliaSlope)",196,207,"/**
 * Calculates the Ganglia slope.
 * Uses config, metric slope or default if none are provided.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,flush,org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:flush(),176,180,"/**
 * Flushes the underlying writer if connected.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,connect,org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:connect(),143,165,"/**
* Attempts to establish a connection to the Graphite server.
* Throws MetricsException if already connected or connection fails.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,close,org.apache.hadoop.metrics2.sink.GraphiteSink:close(),124,127,"/**
 * Closes the Graphite connection.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,init,org.apache.hadoop.metrics2.sink.StatsDSink:init(org.apache.commons.configuration2.SubsetConfiguration),78,95,"/**
 * Initializes the StatsD client with configuration from the given object.
 * @param conf Configuration object containing StatsD settings.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,close,org.apache.hadoop.metrics2.sink.StatsDSink:close(),163,166,"/**
 * Closes the StatsD client, releasing associated resources.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java,putMetrics,org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),68,82,"/**
 * Adds metrics to the nextPromMetrics map, filtering by type.
 * @param metricsRecord Record containing metrics to be added.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java,getMetricKey,"org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:getMetricKey(java.lang.String,org.apache.hadoop.metrics2.AbstractMetric,java.util.List)",165,174,"/**
 * Generates a metric key by parsing promMetricKey and adding tags.
 * @param promMetricKey The Prometheus metric key.
 * @param metric The AbstractMetric object.
 * @param extendTags List to add tags to.
 * @return The generated metric key.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/AbstractPatternFilter.java,init,org.apache.hadoop.metrics2.filter.AbstractPatternFilter:init(org.apache.commons.configuration2.SubsetConfiguration),54,84,"/**
 * Initializes the component with configuration, setting include/exclude patterns.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsBufferBuilder.java,add,"org.apache.hadoop.metrics2.impl.MetricsBufferBuilder:add(java.lang.String,java.lang.Iterable)",29,31,"/**
* Adds a new metrics entry to the buffer.
* @param name Entry name.
* @param records Metrics records for the entry.
* @return True if added successfully.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsBufferBuilder.java,get,org.apache.hadoop.metrics2.impl.MetricsBufferBuilder:get(),33,35,"/**
 * Creates a new MetricsBuffer instance, wrapping the current one.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,<init>,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter$WaitableMetricsBuffer:<init>(org.apache.hadoop.metrics2.impl.MetricsBuffer),240,242,"/**
 * Constructs a WaitableMetricsBuffer with the given MetricsBuffer.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,dequeue,org.apache.hadoop.metrics2.impl.SinkQueue:dequeue(),101,108,"/**
 * Dequeues and removes the head of the queue.
 * Blocks if the queue is empty until an element is available.
 * @return The head element of the queue.
 * @throws InterruptedException if interrupted while waiting.
 */
","* Dequeue one element from head of the queue, will block if queue is empty
   * @return  the first element
   * @throws InterruptedException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,clear,org.apache.hadoop.metrics2.impl.SinkQueue:clear(),154,161,"/**
 * Clears the data array, setting size to 0.
 * Ensures consumer is checked before clearing.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,waitForData,org.apache.hadoop.metrics2.impl.SinkQueue:waitForData(),110,118,"/**
 * Waits for data to be available, then retrieves the front element.
 * @return The front element of the queue.
 * @throws InterruptedException If interrupted while waiting.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(org.apache.hadoop.metrics2.MetricsSystem$Callback),308,311,"/**
 * Registers a callback to be invoked later.
 * @param callback The callback to register.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,org.apache.hadoop.metrics2.MetricsSystem$Callback)",313,315,"/**
* Registers a callback with a given name.
* @param name Callback name.
* @param callback The callback to register.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterLong.java,incr,org.apache.hadoop.metrics2.lib.MutableCounterLong:incr(),42,45,"/**
 * Increments the counter by 1. Calls the overloaded incr(int) method.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrSentBytes,org.apache.hadoop.ipc.metrics.RpcMetrics:incrSentBytes(int),256,258,"/**
 * Increments the number of sent bytes by the given count.
 * @param count The number of bytes to add.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrReceivedBytes,org.apache.hadoop.ipc.metrics.RpcMetrics:incrReceivedBytes(int),265,267,"/**
 * Increments the received byte count by the given amount.
 * @param count The number of bytes to add.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordImpl.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsRecordImpl:<init>(org.apache.hadoop.metrics2.MetricsInfo,long,java.util.List,java.lang.Iterable)",47,54,"/**
 * Creates a MetricsRecordImpl with given info, timestamp, tags, and metrics.
 */","* Construct a metrics record
   * @param info  {@link MetricsInfo} of the record
   * @param timestamp of the record
   * @param tags  of the record
   * @param metrics of the record",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.java,toString,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:toString(),46,54,"/**
 * Returns a string representation of the object.
 * Includes timestamp, name, description, tags, and metrics.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.java,hashCode,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:hashCode(),42,44,"/**
 * Generates a hash code based on name, description, and tags.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/AbstractMetricsRecord.java,equals,org.apache.hadoop.metrics2.impl.MetricsRecordImpl:equals(java.lang.Object),29,39,"/**
 * Checks if this record is equal to another MetricsRecord.
 * @param obj The object to compare to.
 * @return True if equal, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,newAttrInfo,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:newAttrInfo(java.lang.String,java.lang.String,java.lang.String)",53,56,"/**
 * Creates a new MBeanAttributeInfo with specified name, type, and description.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,setAttrCacheTag,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttrCacheTag(org.apache.hadoop.metrics2.MetricsTag,int)",279,282,"/**
 * Stores attribute in cache using tag name and record number.
 * @param tag The tag object.
 * @param recNo Record number to use in cache key.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,setAttrCacheMetric,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:setAttrCacheMetric(org.apache.hadoop.metrics2.AbstractMetric,int)",296,299,"/**
 * Stores a metric in the attribute cache with a generated key.
 * @param metric The metric to store.
 * @param recNo Record number for cache key generation.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,refreshQueueSizeGauge,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:refreshQueueSizeGauge(),168,170,"/**
 * Updates the queue size gauge with the current queue size.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/UniqueNames.java,uniqueName,org.apache.hadoop.metrics2.lib.UniqueNames:uniqueName(java.lang.String),47,65,"/**
 * Generates a unique name by appending a counter.
 * @param name The base name to make unique.
 * @return A unique name based on the input.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterInt.java,incr,org.apache.hadoop.metrics2.lib.MutableCounterInt:incr(),41,44,"/**
 * Increments the counter by 1. Delegates to incr(int).
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterInt.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableCounterInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",59,65,"/**
 * Adds a metric to the builder, conditionally based on 'all'.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,<init>,org.apache.hadoop.metrics2.lib.MutableQuantiles$RolloverSample:<init>(org.apache.hadoop.metrics2.lib.MutableQuantiles),236,238,"/**
 * Constructs a RolloverSample with a parent MutableQuantiles object.
 * @param parent The parent MutableQuantiles instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableQuantiles:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",129,146,"/**
 * Records metrics snapshot to the builder, including gauges.
 * @param builder MetricsRecordBuilder to populate with snapshot data
 * @param all If true, always record; otherwise, only if changed
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addGetGroups,org.apache.hadoop.security.UserGroupInformation$UgiMetrics:addGetGroups(long),155,162,"/**
 * Adds latency to getGroups list and quantile objects.
 * @param latency The latency value to add.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcEnQueueTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcEnQueueTime(long),277,284,"/**
 * Adds RPC enqueue time to the list and quantile trackers.
 * @param enQTime The enqueue time to add.
 */
","* Sometimes, the request time observed by the client is much longer than
   * the queue + process time on the RPC server.Perhaps the RPC request
   * 'waiting enQueue' took too long on the RPC server, so we should add
   * enQueue time to RpcMetrics. See HADOOP-18840 for details.
   * Add an RPC enqueue time sample
   * @param enQTime the queue time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcQueueTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcQueueTime(long),290,297,"/**
 * Adds RPC queue time to the list and updates quantiles if enabled.
 * @param qTime The RPC queue time to add.
 */
","* Add an RPC queue time sample
   * @param qTime the queue time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcLockWaitTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcLockWaitTime(long),299,306,"/**
 * Adds RPC lock wait time to the list and quantiles.
 * @param waitTime The RPC lock wait time to add.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcProcessingTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcProcessingTime(long),312,319,"/**
 * Adds RPC processing time to the list and quantiles (if enabled).
 * @param processingTime RPC processing time in milliseconds.
 */
","* Add an RPC processing time sample
   * @param processingTime the processing time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addRpcResponseTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addRpcResponseTime(long),321,328,"/**
 * Adds an RPC response time to the recorded statistics.
 * @param responseTime The RPC response time in milliseconds.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,addDeferredRpcProcessingTime,org.apache.hadoop.ipc.metrics.RpcMetrics:addDeferredRpcProcessingTime(long),330,337,"/**
 * Adds deferred RPC processing time. Updates quantiles if enabled.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,addWriteFileLatency,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:addWriteFileLatency(long),110,116,"/**
 * Adds a write latency value to each quantile object.
 * @param writeLatency The write latency value to add.
 */
","* Add the file write latency to {@link MutableQuantiles} metrics.
   *
   * @param writeLatency file write latency in microseconds",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,addReadFileLatency,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:addReadFileLatency(long),123,129,"/**
* Adds a read latency value to all file read quantile objects.
* @param readLatency The read latency value to add.
*/
","* Add the file read latency to {@link MutableQuantiles} metrics.
   *
   * @param readLatency file read latency in microseconds",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableInverseQuantiles.java,<init>,org.apache.hadoop.metrics2.lib.MutableInverseQuantiles$InversePercentile:<init>(double),41,43,"/**
 * Creates a new InversePercentile object.
 * @param inversePercentile Value used to initialize the object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,initialize,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:initialize(java.lang.String),57,59,"/**
 * Initializes the MetricsSystem with a given prefix.
 * @param prefix Prefix for metrics names.
 * @return MetricsSystem instance.
 */
","* Convenience method to initialize the metrics system
   * @param prefix  for the metrics system configuration
   * @return the metrics system instance",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,instance,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:instance(),68,70,"/**
 * Returns the singleton instance of the MetricsSystem.
 */",* @return the metrics system object,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,shutdown,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:shutdown(),75,77,"/**
 * Shuts down the application instance gracefully.
 */",* Shutdown the metrics system,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,setInstance,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:setInstance(org.apache.hadoop.metrics2.MetricsSystem),87,90,"/**
 * Sets the MetricsSystem instance.
 * @param ms The new MetricsSystem instance.
 * @return The updated MetricsSystem instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,removeMBeanName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeMBeanName(javax.management.ObjectName),113,116,"/**
 * Removes an MBean name from the internal registry.
 * @param name The ObjectName to remove.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,removeSourceName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:removeSourceName(java.lang.String),118,121,"/**
 * Removes a source name from the internal registry.
 * @param name The name of the source to remove.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getTag,org.apache.hadoop.ipc.metrics.RpcMetrics:getTag(java.lang.String),449,452,"/**
 * Retrieves a MetricsTag by its name.
 * @param tagName The name of the tag to retrieve.
 * @return The MetricsTag or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,snapshot,"org.apache.hadoop.metrics2.lib.MetricsRegistry:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",465,472,"/**
 * Records metrics and tags into the provided builder.
 * @param builder MetricsRecordBuilder to populate
 * @param all if true, includes all metrics, otherwise current values
 */
","* Sample all the mutable metrics and put the snapshot in the builder
   * @param builder to contain the metrics snapshot
   * @param all get all the metrics even if the values are not changed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,toString,org.apache.hadoop.metrics2.lib.MetricsRegistry:toString(),474,481,"/**
 * Returns a string representation of the object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,getStats,org.apache.hadoop.metrics2.lib.MutableRollingAverages:getStats(long),292,314,"/**
 * Calculates and returns statistics for entries with at least minSamples.
 * @param minSamples minimum number of samples required for a stat
 * @return Map of statistics (name -> average)
 */
","* Retrieve a map of metric name {@literal ->} (aggregate).
   * Filter out entries that don't have at least minSamples.
   *
   * @param minSamples input minSamples.
   * @return a map of peer DataNode Id to the average latency to that
   *         node seen over the measurement period.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getProcessingSampleCount,org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingSampleCount(),396,398,"/**
 * Returns the number of processing samples recorded.
 */
","* Returns the number of samples that we have seen so far.
   * @return long",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getDeferredRpcProcessingSampleCount,org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingSampleCount(),437,439,"/**
 * Returns the number of samples for deferred RPC processing.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,rollOverAvgs,org.apache.hadoop.metrics2.lib.MutableRollingAverages:rollOverAvgs(),247,274,"/**
 * Rolls over average rates from the current snapshot.
 * Updates the averages deque for each rate.
 */","* Iterates over snapshot to capture all Avg metrics into rolling structure
   * {@link MutableRollingAverages#averages}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableGaugeLong:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",83,89,"/**
 * Adds gauge metric to builder. Adds only if 'all' is true or changed.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeLong:incr(),46,49,"/**
 * Increments the counter by 1. Calls the overloaded incr(int) method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,decr,org.apache.hadoop.metrics2.lib.MutableGaugeLong:decr(),60,63,"/**
 * Decrements the counter by 1. Calls the overloaded decr(int) method.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/Interns.java,info,"org.apache.hadoop.metrics2.lib.Interns:info(java.lang.String,java.lang.String)",117,119,"/**
 * Adds a metric info to the cache.
 * @param name Metric name.
 * @param description Metric description.
 * @return MetricsInfo object.
 */
","* Get a metric info object.
   * @param name Name of metric info object
   * @param description Description of metric info object
   * @return an interned metric info object",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/Interns.java,tag,"org.apache.hadoop.metrics2.lib.Interns:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",151,153,"/**
* Adds a tag to the cache.
* @param info MetricsInfo object
* @param value Tag value string
* @return MetricsTag object
*/
","* Get a metrics tag.
   * @param info  of the tag
   * @param value of the tag
   * @return an interned metrics tag",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableGaugeFloat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",52,58,"/**
 * Adds a gauge metric to the builder if 'all' is true or changed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeFloat:incr(float),70,79,"/**
 * Atomically increments the value by delta.
 * Uses compareAndSet for thread-safe update.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,add,"org.apache.hadoop.metrics2.lib.MutableStat:add(long,long)",123,126,"/**
* Adds sample data to the interval statistic.
* @param numSamples Number of samples added.
* @param sum Sum of the samples.
*/
","* Add a number of samples and their sum to the running stat
   *
   * Note that although use of this method will preserve accurate mean values,
   * large values for numSamples may result in inaccurate variance values due
   * to the use of a single step of the Welford method for variance calculation.
   * @param numSamples  number of samples
   * @param sum of the samples",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,add,org.apache.hadoop.metrics2.util.SampleStat:add(double),68,71,"/**
 * Adds a value to the sample and recursively calls add.
 * @param x The value to add.
 * @return The updated SampleStat object.
 */
","* Add a sample the running stat.
   * @param x the sample number
   * @return  self",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getProcessingMean,org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingMean(),404,406,"/**
 * Returns the mean processing time from the RPC stats.
 */
","* Returns mean of RPC Processing Times.
   * @return double",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getDeferredRpcProcessingMean,org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingMean(),441,443,"/**
 * Returns the mean deferred RPC processing time.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,min,org.apache.hadoop.metrics2.util.SampleStat:min(),134,136,"/**
 * Returns the minimum value from the minmax collection.
 */",* @return  the minimum value of the samples,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,max,org.apache.hadoop.metrics2.util.SampleStat:max(),141,143,"/**
 * Returns the maximum value from the MinMax object.
 */",* @return  the maximum value of the samples,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,reset,org.apache.hadoop.metrics2.util.SampleStat$MinMax:reset(org.apache.hadoop.metrics2.util.SampleStat$MinMax),188,191,"/**
* Sets the min and max values to match the provided MinMax object.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,resetMinMax,org.apache.hadoop.metrics2.lib.MutableStat:resetMinMax(),177,179,"/**
 * Resets the min/max values to their initial state.
 */",* Reset the all time min max of the metric,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,reset,org.apache.hadoop.metrics2.util.SampleStat:reset(),40,45,"/**
 * Resets the internal state of the class to initial values.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,snapshot,"org.apache.hadoop.metrics2.lib.MethodMetric$1:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",141,143,"/**
 * Delegates snapshotting to the underlying implementation.
 * @param builder MetricsRecordBuilder to populate
 * @param all if true, includes all metrics
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newCounter,org.apache.hadoop.metrics2.lib.MethodMetric:newCounter(java.lang.Class),72,87,"/**
* Creates a new counter metric for int or long methods.
* @param type the Class object of the method return type
* @return MutableMetric counter object
* @throws MetricsException if the type is not supported
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,snapshot,"org.apache.hadoop.metrics2.lib.MethodMetric$2:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",141,143,"/**
 * Delegates snapshotting to the underlying implementation.
 * @param builder MetricsRecordBuilder to populate
 * @param all if true, includes all metrics
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newGauge,org.apache.hadoop.metrics2.lib.MethodMetric:newGauge(java.lang.Class),106,123,"/**
 * Creates a MutableMetric gauge for numeric types.
 * @param t Class of the numeric type to gauge.
 * @throws MetricsException if the type is unsupported.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableGaugeInt:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",83,89,"/**
 * Adds a gauge metric to the builder if 'all' is true or changed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getNextTgtRenewalTime,"org.apache.hadoop.security.UserGroupInformation:getNextTgtRenewalTime(long,long,org.apache.hadoop.io.retry.RetryPolicy)",1110,1117,"/**
 * Calculates the next target renewal time based on end time & retry policy.
 * @param tgtEndTime Target end time.
 * @param now Current time.
 * @param rp Retry policy.
 * @return Next renewal time.
 */
","* Get time for next login retry. This will allow the thread to retry with
   * exponential back-off, until tgt endtime.
   * Last retry is {@link #kerberosMinSecondsBeforeRelogin} before endtime.
   *
   * @param tgtEndTime EndTime of the tgt.
   * @param now Current time.
   * @param rp The retry policy.
   * @return Time for next login retry.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeInt:incr(),46,49,"/**
 * Increments the counter by 1. Delegates to incr(int).
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,decr,org.apache.hadoop.metrics2.lib.MutableGaugeInt:decr(),60,63,"/**
 * Decrements the counter by 1. Calls the overloaded decr(int) method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,reattach,"org.apache.hadoop.metrics2.source.JvmMetrics:reattach(org.apache.hadoop.metrics2.MetricsSystem,org.apache.hadoop.metrics2.source.JvmMetrics)",125,127,"/**
 * Registers JvmMetrics with the MetricsSystem.
 * @param ms The MetricsSystem to register with.
 * @param jvmMetrics The JvmMetrics to register.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getMemoryUsage,org.apache.hadoop.metrics2.source.JvmMetrics:getMemoryUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder),157,168,"/**
 * Records JVM memory usage metrics into the provided record builder.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/Metrics2Util.java,equals,org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair:equals(java.lang.Object),56,62,"/**
 * Checks if this NameValuePair is equal to another.
 * @param other The object to compare to.
 * @return True if equal, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,stddev,org.apache.hadoop.metrics2.util.SampleStat:stddev(),127,129,"/**
 * Calculates the standard deviation based on the variance.
 */",* @return  the standard deviation of the samples,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,compress,org.apache.hadoop.metrics2.util.SampleQuantiles:compress(),176,198,"/**
 * Compresses the sample list by merging adjacent items.
 */","* Try to remove extraneous items from the set of sampled items. This checks
   * if an item is unnecessary based on the desired error bounds, and merges it
   * with the adjacent item if it is.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,query,org.apache.hadoop.metrics2.util.SampleQuantiles:query(double),206,228,"/**
* Finds the value at the given quantile.
* @param quantile quantile between 0 and 1
* @return value at the specified quantile
*/
","* Get the estimated value at the specified quantile.
   * 
   * @param quantile Queried quantile, e.g. 0.50 or 0.99.
   * @return Estimated value at that quantile.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,insertBatch,org.apache.hadoop.metrics2.util.SampleQuantiles:insertBatch(),129,169,"/**
 * Inserts buffered samples into the `samples` list, sorted.
 * Resets `bufferCount` to 0 after insertion.
 */
","* Merges items from buffer into the samples array in one pass.
   * This is more efficient than doing an insert on every item.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/Metrics2Util.java,offer,org.apache.hadoop.metrics2.util.Metrics2Util$TopN:offer(org.apache.hadoop.metrics2.util.Metrics2Util$NameValuePair),84,95,"/**
 * Offers a name-value pair to the queue.
 * Maintains size n, removing smallest if necessary.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MetricsCache.java,<init>,org.apache.hadoop.metrics2.util.MetricsCache:<init>(),136,138,"/**
 * Default constructor, initializes with the default max records.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,tag,"org.apache.hadoop.metrics2.MetricsJsonBuilder:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",66,69,"/**
* Adds a tag to the metrics record.
* @param info MetricsInfo object defining the tag name.
* @param value Tag value to be associated with the name.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,add,org.apache.hadoop.metrics2.MetricsJsonBuilder:add(org.apache.hadoop.metrics2.MetricsTag),71,74,"/**
 * Adds a metrics tag to the builder.
 * @param tag The metrics tag to add.
 * @return The current MetricsRecordBuilder instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,add,org.apache.hadoop.metrics2.MetricsJsonBuilder:add(org.apache.hadoop.metrics2.AbstractMetric),76,79,"/**
 * Adds a metric to the builder.
 * @param metric The metric to add.
 * @return The builder with the metric added.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,setContext,org.apache.hadoop.metrics2.MetricsJsonBuilder:setContext(java.lang.String),81,84,"/**
 * Sets the context value for the metrics record.
 * @param value The context string to set.
 * @return MetricsRecordBuilder with the context set.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",86,89,"/**
 * Adds a counter metric with the given info and value.
 * @param info MetricsInfo object
 * @param value The counter value to add
 * @return MetricsRecordBuilder for chaining
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",91,94,"/**
 * Adds a counter metric with the given value.
 * @param info MetricsInfo object describing the metric.
 * @param value The counter value to add.
 * @return A MetricsRecordBuilder with the added counter.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",96,99,"/**
 * Adds a gauge metric with a given name and value.
 * @param info MetricsInfo object containing the metric name.
 * @param value The integer value of the gauge.
 * @return A MetricsRecordBuilder with the added gauge.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",101,104,"/**
 * Adds a gauge metric with the given name and value.
 * @param info MetricsInfo object containing the metric name
 * @param value The gauge value
 * @return MetricsRecordBuilder for chaining
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",106,109,"/**
 * Adds a gauge metric with the given name and value.
 * @param info MetricsInfo object containing the metric name
 * @param value The float value of the gauge
 * @return MetricsRecordBuilder with the added gauge
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricsJsonBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricsJsonBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",111,114,"/**
 * Adds a gauge metric with a given name and value.
 * @param info MetricsInfo object containing the metric name
 * @param value The gauge value to record
 * @return A MetricsRecordBuilder with the added gauge.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolServerSideTranslatorPB.java,getProtocolVersion,"org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:getProtocolVersion(java.lang.String,long)",68,72,"/**
* Gets the protocol version.
* @param protocol protocol name, unused.
* @param clientVersion client version, unused.
* @return Protocol version number.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,getProtocolVersion,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getProtocolVersion(java.lang.String,long)",180,184,"/**
 * Gets the protocol version from RPC.
 * @param protocol Protocol identifier.
 * @param clientVersion Client version.
 * @return Protocol version.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/NetgroupCache.java,getNetgroupNames,org.apache.hadoop.security.NetgroupCache:getNetgroupNames(),63,65,"/**
 * Returns a list of netgroup names.
 * Returns a new list containing the group names.
 */
","* Get the list of cached netgroups
   *
   * @return list of cached groups",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/NetgroupCache.java,isCached,org.apache.hadoop.security.NetgroupCache:isCached(java.lang.String),81,83,"/**
 * Checks if a group is present in the cached groups list.
 * @param group The group name to check.
 * @return True if the group is cached, false otherwise.
 */
","* Returns true if a given netgroup is cached
   *
   * @param group check if this group is cached
   * @return true if group is cached, false otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getDefaults,org.apache.hadoop.security.UserGroupInformation$LoginParams:getDefaults(),2097,2103,"/**
 * Retrieves default Kerberos login parameters from environment variables.
 * Returns a LoginParams object containing KRB5PRINCIPAL, KEYTAB, and CCACHE.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,toLowerCase,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:toLowerCase(javax.servlet.http.HttpServletRequest),132,196,"/**
 * Converts request parameters to lowercase.
 * Wraps the request to lowercase parameter names.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getServerProperties,"org.apache.hadoop.security.SaslPropertiesResolver:getServerProperties(java.net.InetAddress,int)",103,106,"/**
 * Gets server properties for a client address.
 * @param clientAddress Client's InetAddress
 * @param ingressPort Port used for ingress
 * @return Map of server properties.
 */
","* Identify the Sasl Properties to be used for a connection with a  client.
   * @param clientAddress  client's address
   * @param ingressPort the port that the client is connecting
   * @return the sasl properties to be used for the connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getClientProperties,"org.apache.hadoop.security.SaslPropertiesResolver:getClientProperties(java.net.InetAddress,int)",123,126,"/**
 * Retrieves client properties for a given server address.
 * @param serverAddress Server address to fetch properties for.
 * @param ingressPort Port number (unused, for future use).
 * @return Map of client properties.
 */
","* Identify the Sasl Properties to be used for a connection with a server.
   * @param serverAddress server's address
   * @param ingressPort the port that is used to connect to server
   * @return the sasl properties to be used for the connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,getPassword,org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:getPassword(org.apache.hadoop.security.token.TokenIdentifier),289,292,"/**
 * Retrieves and encodes a password using the provided token.
 * @param tokenid Token identifier for password retrieval.
 * @return Encoded password.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMappingWithFallback.java,getGroupsSet,org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback:getGroupsSet(java.lang.String),65,68,"/**
 * Returns a set of group names for the given user.
 * @param user The username to retrieve groups for.
 * @return A set of group names.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMappingWithFallback.java,getGroupsSet,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMappingWithFallback:getGroupsSet(java.lang.String),64,67,"/**
 * Returns a set of group names for the given user.
 * @param user User identifier.
 * @return Set of group names.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.CompositeGroupsMapping:getGroupsSet(java.lang.String),110,131,"/**
 * Retrieves a set of groups for a user from available providers.
 * @param user User identifier.
 * @return Set of group names, or an empty set if none found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HttpCrossOriginFilterInitializer.java,getEnabledConfigKey,org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getEnabledConfigKey(),71,73,"/**
 * Returns the key for the enabled configuration setting.
 * Combines prefix with the ENABLED_SUFFIX.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getCredentialsInternal,org.apache.hadoop.security.UserGroupInformation:getCredentialsInternal(),1759,1770,"/**
 * Retrieves credentials from the subject, creating if none exist.
 * @return Credentials object
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/User.java,<init>,"org.apache.hadoop.security.User:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,javax.security.auth.login.LoginContext)",46,57,"/**
 * Creates a User object with the given name, auth method, and login context.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getHostFromPrincipal,org.apache.hadoop.security.SecurityUtil:getHostFromPrincipal(java.lang.String),353,355,"/**
 * Extracts the hostname from a Kerberos principal name.
 * @param principalName Kerberos principal name string.
 * @return Hostname portion of the principal name.
 */
","* Get the host name from the principal name of format {@literal <}service
   * {@literal >}/host@realm.
   * @param principalName principal name of format as described above
   * @return host name if the the string conforms to the above format, else null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getGroupInternal,org.apache.hadoop.security.Groups:getGroupInternal(java.lang.String),242,264,"/**
* Retrieves group names for a user, using cache and static mappings.
* @param user User identifier.
* @throws IOException if no groups are found or an error occurs.
*/
","* Get the group memberships of a given user.
   * If the user's group is not cached, this method may block.
   * @param user User's name
   * @return the group memberships of the user as Set
   * @throws IOException if user does not exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,refresh,org.apache.hadoop.security.Groups:refresh(),430,441,"/**
 * Refreshes the user-to-groups cache and clears related caches.
 */",* Refresh all user-to-groups mappings.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMapping.java,getGroups,org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroups(java.lang.String),79,82,"/**
 * Retrieves a list of groups for a given user.
 * @param user User identifier.
 * @return List of group names.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.JniBasedUnixGroupsMapping:getGroupsSet(java.lang.String),84,90,"/**
 * Returns a set of groups for a given user.
 * @param user The username to retrieve groups for.
 * @return A set of group names.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,close,org.apache.hadoop.security.KDiag:close(),189,195,"/**
 * Closes the output stream, flushing any buffered data.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,println,"org.apache.hadoop.security.KDiag:println(java.lang.String,java.lang.Object[])",854,863,"/**
 * Prints formatted string to output stream or System.out.
 * @param format Format string for printing.
 * @param args Arguments for formatting.
 */
","* Print a line of output. This goes to any output file, or
   * is logged at info. The output is flushed before and after, to
   * try and stay in sync with JRE logging.
   *
   * @param format format string
   * @param args any arguments",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,usage,org.apache.hadoop.security.KDiag:usage(),246,263,"/**
* Returns a string describing the command-line arguments for usage.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMapInternal,"org.apache.hadoop.security.ShellBasedIdMapping:updateMapInternal(org.apache.hadoop.thirdparty.com.google.common.collect.BiMap,java.lang.String,java.lang.String,java.lang.String,java.util.Map)",224,281,"/**
* Updates a BiMap with data from a command execution.
* @param map BiMap to update, mapName, command, regex, staticMapping
* @return True if the map was updated.
*/","* Get the list of users or groups returned by the specified command,
   * and save them in the corresponding map.
   *
   * @param map map.
   * @param mapName mapName.
   * @param command command.
   * @param staticMapping staticMapping.
   * @param regex regex.
   * @throws IOException raised on errors performing I/O.
   * @return updateMapInternal.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getRunScriptCommand,org.apache.hadoop.util.Shell:getRunScriptCommand(java.io.File),443,448,"/**
 * Generates command to execute a script.
 * @param script File object representing the script.
 * @return String array representing the command.
 */
","* Returns a command to run the given script.  The script interpreter is
   * inferred by platform: cmd on Windows or bash otherwise.
   *
   * @param script File script to run
   * @return String[] command to run the script",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,valueOf,org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod:valueOf(org.apache.hadoop.security.SaslRpcServer$AuthMethod),1504,1512,"/**
 * Returns the AuthenticationMethod corresponding to the given AuthMethod.
 * @param authMethod The AuthMethod to find.
 * @throws IllegalArgumentException if no matching method exists.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,switchBindUser,org.apache.hadoop.security.LdapGroupsMapping:switchBindUser(javax.naming.AuthenticationException),644,651,"/**
 * Switches to the next bind user after an AuthenticationException.
 * Logs the switch if the users are different.
 */
","* Switch to the next available user to bind to.
   * @param e AuthenticationException encountered when contacting LDAP",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,write,"org.apache.hadoop.security.SaslOutputStream:write(byte[],int,int)",166,193,"/**
 * Writes data to the output stream, potentially wrapping with SASL.
 * @param inBuf Input byte array
 * @param off Offset in the array
 * @param len Number of bytes to write
 * @throws IOException if an I/O error occurs
 */
","* Writes <code>len</code> bytes from the specified byte array starting at
   * offset <code>off</code> to this output stream.
   * 
   * @param inBuf
   *          the data.
   * @param off
   *          the start offset in the data.
   * @param len
   *          the number of bytes to write.
   * @exception IOException
   *              if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,close,org.apache.hadoop.security.SaslOutputStream:close(),213,217,"/**
 * Closes the connection, releasing resources.
 * Disposes SASL and closes the output stream.
 */
","* Closes this output stream and releases any system resources associated with
   * this stream.
   * 
   * @exception IOException
   *              if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,init,org.apache.hadoop.security.SaslRpcServer:init(org.apache.hadoop.conf.Configuration),175,182,"/**
 * Initializes the SASL factory.
 * @param conf Configuration object for initialization.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,createSaslServer,"org.apache.hadoop.security.SaslPlainServer$SaslPlainServerFactory:createSaslServer(java.lang.String,java.lang.String,java.lang.String,java.util.Map,javax.security.auth.callback.CallbackHandler)",48,53,"/**
 * Creates a SASL server based on the specified mechanism.
 * @param mechanism SASL mechanism name.
 * @return SaslServer instance or null if unsupported.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,getAuthorizationID,org.apache.hadoop.security.SaslPlainServer:getAuthorizationID(),127,131,"/**
 * Returns the authorization ID. Throws if not complete.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,getNegotiatedProperty,org.apache.hadoop.security.SaslPlainServer:getNegotiatedProperty(java.lang.String),133,137,"/**
 * Returns the negotiated property value, ""auth"" if propName is ""auth"".
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,wrap,"org.apache.hadoop.security.SaslPlainServer:wrap(byte[],int,int)",139,145,"/**
 * Wraps bytes, but throws an exception as PLAIN doesn't support integrity/privacy.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPlainServer.java,unwrap,"org.apache.hadoop.security.SaslPlainServer:unwrap(byte[],int,int)",147,153,"/**
 * Unwraps data (not supported for PLAIN).
 * @param incoming Encrypted data (unused).
 * @param offset Offset in the data (unused).
 * @param len Length of data (unused).
 * @throws SaslException if operation fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,createKeyStore,"org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyStore(java.lang.String,java.lang.String)",1102,1109,"/**
 * Creates a KeyStore from a file.
 * @param location Path to the keystore file.
 * @param password Keystore password.
 * @return Populated KeyStore object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,init,org.apache.hadoop.security.http.RestCsrfPreventionFilter:init(javax.servlet.FilterConfig),71,93,"/**
 * Initializes the filter, reading configuration from FilterConfig.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,handleHttpInteraction,org.apache.hadoop.security.http.RestCsrfPreventionFilter:handleHttpInteraction(org.apache.hadoop.security.http.RestCsrfPreventionFilter$HttpInteraction),193,203,"/**
 * Processes an HTTP interaction, proceeding or sending an error.
 * @param httpInteraction The HTTP interaction to handle.
 * @throws IOException, ServletException if an error occurs.
 */
","* Handles an {@link HttpInteraction} by applying the filtering logic.
   *
   * @param httpInteraction caller's HTTP interaction
   * @throws IOException if there is an I/O error
   * @throws ServletException if the implementation relies on the servlet API
   *     and a servlet API call has failed",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,initializeAllowedMethods,org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedMethods(javax.servlet.FilterConfig),165,174,"/**
 * Initializes allowed HTTP methods from filter config.
 * Uses default if config is missing.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,doCrossFilter,"org.apache.hadoop.security.http.CrossOriginFilter:doCrossFilter(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",107,153,"/**
 * Checks and sets CORS headers based on request headers.
 * @param req HttpServletRequest containing CORS request details.
 * @param res HttpServletResponse to set CORS headers on.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,initializeAllowedHeaders,org.apache.hadoop.security.http.CrossOriginFilter:initializeAllowedHeaders(javax.servlet.FilterConfig),176,185,"/**
 * Initializes allowed headers from filter config or default value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,parsePartialGroupNames,"org.apache.hadoop.security.ShellBasedUnixGroupsMapping:parsePartialGroupNames(java.lang.String,java.lang.String)",242,269,"/**
 * Parses group names from strings, throwing exception on mismatch.
 * @param groupNames Group names string.
 * @param groupIDs Group IDs string.
 * @return Set of parsed group names.
 */
","* Attempt to parse group names given that some names are not resolvable.
   * Use the group id list to identify those that are not resolved.
   *
   * @param groupNames a string representing a list of group names
   * @param groupIDs a string representing a list of group ids
   * @return a linked list of group names
   * @throws PartialGroupNameException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,newLoginContext,"org.apache.hadoop.security.UserGroupInformation:newLoginContext(java.lang.String,javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration)",503,518,"/**
 * Creates a HadoopLoginContext.
 * @param appName Application name.
 * @param subject Subject for authentication.
 * @param loginConf Hadoop configuration.
 * @throws LoginException if login fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,login,org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:login(),2142,2155,"/**
 * Performs login, records success/failure metrics, and sets isLoggedIn.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logout,org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext:logout(),2157,2164,"/**
 * Logs out the user if currently logged in.
 * Uses CAS to ensure thread-safe logout.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,createSecretKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createSecretKey(byte[]),692,694,"/**
 * Creates a SecretKey from the provided byte array.
 * @param key The byte array representing the secret key.
 * @return A SecretKey object.
 */
","* Convert the byte[] to a secret key
   * @param key the byte[] to create the secret key from
   * @return the secret key",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,formatTokenId,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:formatTokenId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),83,90,"/**
 * Formats a TokenIdent object into a string representation.
 * Returns a formatted string or a default if an error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,removeStoredToken,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),204,211,"/**
 * Removes a token from storage, handling potential SQL exceptions.
 */
","* Removes the existing TokenInformation from the SQL database to
   * invalidate it.
   * @param ident TokenInformation to remove from the SQL database.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Daemon.java,newThread,org.apache.hadoop.util.Daemon$DaemonFactory:newThread(java.lang.Runnable),42,45,"/**
 * Creates a new thread to execute the given runnable.
 * @param runnable The runnable to execute in the new thread.
 * @return A new Daemon thread.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,serviceStart,org.apache.hadoop.util.JvmPauseMonitor:serviceStart(),81,86,"/**
 * Starts the monitor thread and calls the superclass's start method.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,reset,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:reset(),182,187,"/**
 * Resets the key ID, clears all keys, and resets token sequence.
 */",* Reset all data structures and mutable state.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,updateDelegationKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),350,352,"/**
* Updates the delegation key in the key store.
* @param key The DelegationKey to update.
* @throws IOException if an I/O error occurs.
*/
","* For subclasses externalizing the storage, for example Zookeeper
   * based implementations.
   *
   * @param key DelegationKey.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,removeStoredMasterKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey),370,377,"/**
 * Removes a stored delegation key.
 * @param key The delegation key to remove.
 */
","* Removes the existing DelegationKey from the SQL database to
   * invalidate it.
   * @param key DelegationKey to remove from the SQL database.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,addKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addKey(org.apache.hadoop.security.token.delegation.DelegationKey),213,220,"/**
 * Adds a delegation key.
 * @param key The key to add. Throws IOException if SecretManager is running.
 */
","* Add a previously used master key to cache (when NN restarts), 
   * should be called before activate().
   *
   * @param key delegation key.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,storeDelegationKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),338,341,"/**
 * Stores a delegation key and its master key.
 * @param key The DelegationKey to store.
 * @throws IOException if an I/O error occurs.
 */
","* For subclasses externalizing the storage, for example Zookeeper
   * based implementations.
   *
   * @param key DelegationKey.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,<init>,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>(long,byte[])",707,709,"/**
 * Constructs a DelegationTokenInformation with a password.
 * @param renewDate Renewal date for the token.
 * @param password Password used for token generation.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeExpiredKeys,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredKeys(),478,491,"/**
 * Removes expired delegation keys from the collection.
 * Iterates and removes keys where expiry date is in the past.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,getTokenTrackingId,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenTrackingId(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),561,567,"/**
 * Gets the tracking ID for a given token identifier.
 * @param identifier TokenIdent object.
 * @return Tracking ID string, or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeExpiredStoredToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),795,797,"/**
* Removes an expired stored token.
* @param ident TokenIdent to remove.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,setExternalDelegationTokenSecretManager,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager),136,141,"/**
 * Sets the external delegation token secret manager.
 * @param secretManager New secret manager instance.
 */
","* Sets an external <code>DelegationTokenSecretManager</code> instance to
   * manage creation and verification of Delegation Tokens.
   * <p>
   * This is useful for use cases where secrets must be shared across multiple
   * services.
   *
   * @param secretManager a <code>DelegationTokenSecretManager</code> instance",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,destroy,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:destroy(),154,158,"/**
 * Stops the managed secret manager threads, if present.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,stopThreads,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:stopThreads(),437,475,"/**
 * Stops various resources: token cache, counters, key cache, and ZK client.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationTokenLoadingCache.java,isEmpty,org.apache.hadoop.security.token.delegation.DelegationTokenLoadingCache:isEmpty(),57,60,"/**
 * Checks if the collection is empty.
 * @return True if the collection contains no elements, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,<init>,"org.apache.hadoop.security.token.delegation.DelegationKey:<init>(int,long,javax.crypto.SecretKey)",51,53,"/**
 * Constructs a DelegationKey with an encoded key or null.
 * @param keyId Delegation key identifier.
 * @param expiryDate Expiry date of the delegation.
 * @param key SecretKey, encoded if not null.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,getConfiguration,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getConfiguration(java.lang.String,javax.servlet.FilterConfig)",114,120,"/**
 * Retrieves configuration properties, extends parent, and sets auth handler.
 */","* It delegates to
   * {@link AuthenticationFilter#getConfiguration(String, FilterConfig)} and
   * then overrides the {@link AuthenticationHandler} to use if authentication
   * type is set to <code>simple</code> or <code>kerberos</code> in order to use
   * the corresponding implementation with delegation token support.
   *
   * @param configPrefix parameter not used.
   * @param filterConfig parameter not used.
   * @return hadoop-auth de-prefixed configuration for the filter and handler.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,initializeAuthHandler,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:initializeAuthHandler(java.lang.String,javax.servlet.FilterConfig)",204,215,"/**
* Initializes auth handler, sets Curator for ZKDelegationTokenSecretManager.
* Uses attribute from ServletContext. Resets Curator afterward.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/HttpUserGroupInformation.java,get,org.apache.hadoop.security.token.delegation.web.HttpUserGroupInformation:get(),36,39,"/**
 * Returns the UserGroupInformation from the current HTTP context.
 */","* Returns the remote {@link UserGroupInformation} in context for the current
   * HTTP request, taking into account proxy user requests.
   *
   * @return the remote {@link UserGroupInformation}, <code>NULL</code> if none.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/MultiSchemeDelegationTokenAuthenticationHandler.java,<init>,org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:<init>(),89,92,"/**
 * Constructs a MultiSchemeDelegationTokenAuthenticationHandler.
 * Delegates to MultiSchemeAuthenticationHandler.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/KerberosDelegationTokenAuthenticationHandler.java,<init>,org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticationHandler:<init>(),49,52,"/**
 * Constructs a KerberosDelegationTokenAuthenticationHandler.
 * Initializes the superclass with a KerberosAuthenticationHandler.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/PseudoDelegationTokenAuthenticationHandler.java,<init>,org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticationHandler:<init>(),50,53,"/**
 * Constructs a PseudoDelegationTokenAuthenticationHandler.
 * Initializes the underlying authentication handler.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,obtainDelegationTokenAuthenticator,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:obtainDelegationTokenAuthenticator(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)",128,140,"/**
 * Obtains a DelegationTokenAuthenticator, creating one if null.
 * @param dta Authenticator, may be null.
 * @param connConfigurator Configurator for the connection.
 * @return DelegationTokenAuthenticator instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/PseudoDelegationTokenAuthenticator.java,<init>,org.apache.hadoop.security.token.delegation.web.PseudoDelegationTokenAuthenticator:<init>(),41,52,"/**
 * Constructs a PseudoDelegationTokenAuthenticator using current user.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/KerberosDelegationTokenAuthenticator.java,<init>,org.apache.hadoop.security.token.delegation.web.KerberosDelegationTokenAuthenticator:<init>(),38,45,"/**
 * Creates a KerberosDelegationTokenAuthenticator with fallback.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,isManagementOperation,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:isManagementOperation(javax.servlet.http.HttpServletRequest),211,218,"/**
 * Checks if the request is a management operation.
 * @param request HttpServletRequest object.
 * @return True if it's a management operation, false otherwise.
 */
","* This method checks if the given HTTP request corresponds to a management
   * operation.
   *
   * @param request The HTTP request
   * @return true if the given HTTP request corresponds to a management
   *         operation false otherwise
   * @throws IOException In case of I/O error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,getDelegationToken,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:getDelegationToken(javax.servlet.http.HttpServletRequest),412,421,"/**
 * Retrieves the delegation token from request header or parameter.
 * @param request HttpServletRequest to check for the token.
 * @return Delegation token string, or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,hasDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:hasDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",115,132,"/**
 * Checks if the URL has a delegation token.
 * @param url The URL to check.
 * @param token The authentication token.
 * @return True if delegation token exists, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,org.apache.hadoop.crypto.key.kms.KMSClientProvider$TokenSelector:<init>(),167,169,"/**
 * Default constructor. Initializes TokenSelector with TOKEN_KIND.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,incrementDelegationTokenSeqNum,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementDelegationTokenSeqNum(),504,531,"/**
 * Increments the delegation token sequence number, fetching a new range if needed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,incrementCurrentKeyId,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:incrementCurrentKeyId(),547,559,"/**
 * Atomically increments the keyId sequence counter and returns the new value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,removeStoredMasterKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredMasterKey(org.apache.hadoop.security.token.delegation.DelegationKey),727,754,"/**
 * Removes a ZKDTSMDelegationKey node from ZooKeeper.
 * @param key The DelegationKey whose node to remove.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,equals,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:equals(java.lang.Object),166,182,"/**
 * Checks if two AbstractDelegationTokenIdentifier objects are equal.
 * Compares sequenceNumber, issueDate, maxDate, masterKeyId, owner, renewer, realUser.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,isManaged,org.apache.hadoop.security.token.Token:isManaged(),487,489,"/**
 * Checks if the resource is managed by the renewer.
 * @return True if managed, false otherwise.
 */
","* Is this token managed so that it can be renewed or cancelled?
   * @return true, if it can be renewed and cancelled.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,renew,org.apache.hadoop.security.token.Token:renew(org.apache.hadoop.conf.Configuration),498,501,"/**
* Renews lease.
* @param conf Hadoop configuration object.
* @return Timestamp of the renewed lease.
*/
","* Renew this delegation token.
   * @param conf configuration.
   * @return the new expiration time
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,cancel,org.apache.hadoop.security.token.Token:cancel(org.apache.hadoop.conf.Configuration),510,513,"/**
 * Cancels the lease renewal process for this lease.
 * @param conf Hadoop configuration object.
 */
","* Cancel this delegation token.
   *
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,validate,org.apache.hadoop.security.token.DtUtilShell$Get:validate(),225,239,"/**
 * Validates the service URL, ensuring it's properly configured.
 * Returns true if valid, false otherwise, logs errors if invalid.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,getCommandUsage,org.apache.hadoop.security.token.DtUtilShell:getCommandUsage(),179,187,"/**
* Returns a formatted string describing command usage.
* Uses DT_USAGE and usages from other commands.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,read,"org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[],int,int)",580,593,"/**
 * Reads data into the buffer.
 * @param buf buffer to fill, off offset, len max bytes to read
 * @return number of bytes read
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,isValidAuthType,org.apache.hadoop.security.SaslRpcClient:isValidAuthType(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),189,199,"/**
 * Checks if the auth type is valid based on method and mechanism.
 * @param authType SaslAuth object to validate.
 * @return True if valid, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getInputStream,org.apache.hadoop.security.SaslRpcClient:getInputStream(java.io.InputStream),533,538,"/**
 * Returns the input stream, wrapping it if useWrap() is true.
 * @param in The input stream to potentially wrap.
 * @return The input stream.
 */
","* Get SASL wrapped InputStream if SASL QoP requires unwrapping,
   * otherwise return original stream.  Can be called only after
   * saslConnect() has been called.
   * 
   * @param in - InputStream used to make the connection
   * @return InputStream that may be using SASL unwrap
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getOutputStream,org.apache.hadoop.security.SaslRpcClient:getOutputStream(java.io.OutputStream),549,558,"/**
 * Wraps the given OutputStream with buffering if configured.
 * @param out The OutputStream to wrap.
 * @return The potentially wrapped OutputStream.
 * @throws IOException if an I/O error occurs.
 */
","* Get SASL wrapped OutputStream if SASL QoP requires wrapping,
   * otherwise return original stream.  Can be called only after
   * saslConnect() has been called.
   * 
   * @param out - OutputStream used to make the connection
   * @return OutputStream that may be using wrapping
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,disposeSasl,org.apache.hadoop.ipc.Client$Connection:disposeSasl(),546,554,"/**
 * Disposes of the SASL RPC client, releasing resources.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,noPasswordWarning,"org.apache.hadoop.security.ProviderUtils:noPasswordWarning(java.lang.String,java.lang.String)",244,247,"/**
 * Generates a warning string including no-password instructions.
 * @param envKey Environment key.
 * @param fileKey File key.
 * @return Combined warning message.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,noPasswordError,"org.apache.hadoop.security.ProviderUtils:noPasswordError(java.lang.String,java.lang.String)",249,251,"/**
 * Returns a no-password error message with instructions.
 * @param envKey Environment key.
 * @param fileKey File key.
 * @return Combined error message.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,readMoreData,org.apache.hadoop.security.SaslInputStream:readMoreData(),95,125,"/**
 * Reads more data from the input stream, handling SASL unwrap.
 * @return Length of unwrap data or -1 on EOF.
 */
","* Read more data and get them processed <br>
   * Entry condition: ostart = ofinish <br>
   * Exit condition: ostart <= ofinish <br>
   * 
   * return (ofinish-ostart) (we have this many bytes for you), 0 (no data now,
   * but could have more later), or -1 (absolutely no more data)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,close,org.apache.hadoop.security.SaslInputStream:close(),341,348,"/**
 * Closes the connection, releasing resources.
 */
","* Closes this input stream and releases any system resources associated with
   * the stream.
   * <p>
   * The <code>close</code> method of <code>SASLInputStream</code> calls the
   * <code>close</code> method of its underlying input stream.
   * 
   * @exception IOException
   *              if an I/O error occurs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,<init>,org.apache.hadoop.security.authorize.AuthorizationException:<init>(),37,39,"/**
 * Constructs a new AuthorizationException with default constructor.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,<init>,org.apache.hadoop.security.authorize.AuthorizationException:<init>(java.lang.Throwable),54,56,"/**
 * Constructs an AuthorizationException with a root cause.
 * @param cause The underlying exception that caused this one.
 */
","* Constructs a new exception with the specified cause and a detail
   * message of <tt>(cause==null ? null : cause.toString())</tt> (which
   * typically contains the class and detail message of <tt>cause</tt>).
   * @param  cause the cause (which is saved for later retrieval by the
   *         {@link #getCause()} method).  (A <tt>null</tt> value is
   *         permitted, and indicates that the cause is nonexistent or
   *         unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,<init>,org.apache.hadoop.security.SaslRpcClient$SaslClientCallbackHandler:<init>(org.apache.hadoop.security.token.Token),664,667,"/**
 * Initializes a SaslClientCallbackHandler with a token.
 * @param token The token containing username and password.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reset,org.apache.hadoop.security.UserGroupInformation:reset(),368,379,"/**
 * Resets the authentication state to its initial configuration.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getLogin,org.apache.hadoop.security.UserGroupInformation:getLogin(),522,526,"/**
 * Retrieves HadoopLoginContext from user login, null otherwise.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isLoginSuccess,org.apache.hadoop.security.UserGroupInformation:isLoginSuccess(),537,542,"/**
 * Checks if the login was successful. Returns true if not HadoopLoginContext.
 */
","This method checks for a successful Kerberos login
    * and returns true by default if it is not using Kerberos.
    *
    * @return true on successful login",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setLogin,org.apache.hadoop.security.UserGroupInformation:setLogin(javax.security.auth.login.LoginContext),528,530,"/**
* Sets the login context for the user.
* @param login The LoginContext to set.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setLastLogin,org.apache.hadoop.security.UserGroupInformation:setLastLogin(long),548,550,"/**
* Sets the user's last login timestamp.
* @param loginTime The timestamp of the last login.
*/
","* Set the last login time for logged in user
   * @param loginTime the number of milliseconds since the beginning of time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,<init>,org.apache.hadoop.security.UserGroupInformation:<init>(javax.security.auth.Subject),559,568,"/**
 * Constructs UserGroupInformation from a Subject.
 * @param subject The Subject to extract user principal from.
 * @throws IllegalStateException if Subject lacks a valid User.
 */
","* Create a UserGroupInformation for the given subject.
   * This does not change the subject or acquire new credentials.
   *
   * The creator of subject is responsible for renewing credentials.
   * @param subject the user's subject",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getUserName,org.apache.hadoop.security.UserGroupInformation:getUserName(),1667,1671,"/**
 * Returns the user's name.
 * @return The user's name as a String.
 */
","* Get the user's full principal name.
   * @return the user's full principal name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,hasKerberosCredentials,org.apache.hadoop.security.UserGroupInformation:hasKerberosCredentials(),574,576,"/**
 * Checks if the user has Kerberos credentials.
 * @return True if authentication method is Kerberos, false otherwise.
 */
","* checks if logged in using kerberos
   * @return true if the subject logged via keytab or has a Kerberos TGT",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:getAuthenticationMethod(),1853,1855,"/**
 * Returns the authentication method associated with the user.
 * Returns null if the user has no authentication method.
 */
","* Get the authentication method from the subject
   * 
   * @return AuthenticationMethod in the subject, null if not present.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,fixKerberosTicketOrder,org.apache.hadoop.security.UserGroupInformation:fixKerberosTicketOrder(),1204,1233,"/**
 * Fixes the order of Kerberos tickets, removing invalid or non-TGT tickets.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,hasSufficientTimeElapsed,org.apache.hadoop.security.UserGroupInformation:hasSufficientTimeElapsed(long),1401,1410,"/**
 * Checks if enough time has elapsed since last login to renew.
 * @param now Current timestamp in milliseconds.
 * @return True if sufficient time has passed, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealUser,org.apache.hadoop.security.UserGroupInformation:getRealUser(),1543,1550,"/**
 * Gets the real user associated with the subject.
 * Returns the real user or null if not found.
 */
","* get RealUser (vs. EffectiveUser)
   * @return realUser running over proxy user",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getShortUserName,org.apache.hadoop.security.UserGroupInformation:getShortUserName(),1651,1653,"/**
 * Returns the short name of the user.
 * @return The user's short name.
 */
","* Get the user's login name.
   * @return the user's name up to the first '/' or '@'.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod),1834,1837,"/**
 * Sets the authentication method for the user.
 * @param authMethod The authentication method to set.
 */
","* Sets the authentication method in the subject
   * 
   * @param authMethod authMethod.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.security.cert.X509Certificate)",349,360,"/**
 * Checks if the certificate matches the provided hostnames.
 * @param host Hostnames to check against the certificate.
 * @param cert X.509 certificate to validate.
 * @throws SSLException if the certificate doesn't match.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],java.lang.String[],java.lang.String[],boolean,boolean)",362,456,"/**
* Validates hostname matches in a certificate.
* @param hosts Array of hostnames to check.
* @param cns Array of CNs, first is primary.
* @param subjectAlts Subject Alternative Names.
* @param ie6 IE6 compatibility flag.
* @param strictWithSubDomains Strict subdomain matching.
* @throws SSLException if hostname validation fails.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509TrustManager.java,<init>,"org.apache.hadoop.security.ssl.ReloadingX509TrustManager:<init>(java.lang.String,java.lang.String,java.lang.String)",72,78,"/**
 * Constructs a ReloadingX509TrustManager with type, location, and password.
 * @param type Trust manager type
 * @param location Trust store location
 * @param password Trust store password
 */
","* Creates a reloadable trustmanager. The trustmanager reloads itself
   * if the underlying trustore file has changed.
   *
   * @param type type of truststore file, typically 'jks'.
   * @param location local path to the truststore file.
   * @param password password of the truststore file.
   * changed, in milliseconds.
   * @throws IOException thrown if the truststore could not be initialized due
   * to an IO error.
   * @throws GeneralSecurityException thrown if the truststore could not be
   * initialized due to a security error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509TrustManager.java,loadFrom,org.apache.hadoop.security.ssl.ReloadingX509TrustManager:loadFrom(java.nio.file.Path),115,123,"/**
 * Loads a trust manager from the given path.
 * @param path Path to the trust store file.
 * @return Returns the current object for chaining.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509KeystoreManager.java,<init>,"org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",69,77,"/**
 * Creates a ReloadingX509KeystoreManager.
 * @param type keystore type, location, passwords for keystore.
 * @throws IOException, GeneralSecurityException on failure.
 */
","* Construct a <code>Reloading509KeystoreManager</code>
   *
   * @param type type of keystore file, typically 'jks'.
   * @param location local path to the keystore file.
   * @param storePassword password of the keystore file.
   * @param keyPassword The password of the key.
   * @throws IOException raised on errors performing I/O.
   * @throws GeneralSecurityException thrown if create encryptor error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/ReloadingX509KeystoreManager.java,loadFrom,org.apache.hadoop.security.ssl.ReloadingX509KeystoreManager:loadFrom(java.nio.file.Path),123,131,"/**
 * Loads a key manager from the given path.
 * @param path Path to the keystore file.
 * @return This object, for chaining.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,getResource,org.apache.hadoop.util.FindClass:getResource(java.lang.String),163,165,"/**
 * Retrieves a resource URL.
 * @param name Resource name to retrieve.
 * @return URL object representing the resource.
 */
","* Get the resource
   * @param name resource name
   * @return URL or null for not found",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getConfResourceAsInputStream,org.apache.hadoop.conf.Configuration:getConfResourceAsInputStream(java.lang.String),2893,2908,"/**
 * Gets a configuration resource as an input stream.
 * @param name Resource name to retrieve.
 * @return InputStream or null if not found/error.
 */
","* Get an input stream attached to the configuration resource with the
   * given <code>name</code>.
   * 
   * @param name configuration resource name.
   * @return an input stream attached to the resource.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getConfResourceAsReader,org.apache.hadoop.conf.Configuration:getConfResourceAsReader(java.lang.String),2917,2932,"/**
 * Reads a configuration resource as a reader.
 * @param name Resource name to load.
 * @return Reader object or null if not found/error.
 */
","* Get a {@link Reader} attached to the configuration resource with the
   * given <code>name</code>.
   * 
   * @param name configuration resource name.
   * @return a reader attached to the resource.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,createSSLEngine,org.apache.hadoop.security.ssl.SSLFactory:createSSLEngine(),256,268,"/**
 * Creates and configures an SSLEngine based on the current mode.
 * @return Configured SSLEngine instance.
 */
","* Returns a configured SSLEngine.
   *
   * @return the configured SSLEngine.
   * @throws GeneralSecurityException thrown if the SSL engine could not
   * be initialized.
   * @throws IOException thrown if and IO error occurred while loading
   * the server keystore.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,configure,org.apache.hadoop.security.ssl.SSLFactory:configure(java.net.HttpURLConnection),358,372,"/**
 * Configures an HttpURLConnection, handling SSL/TLS if applicable.
 * @param conn The HttpURLConnection to configure.
 * @return The configured HttpURLConnection.
 */
","* If the given {@link HttpURLConnection} is an {@link HttpsURLConnection}
   * configures the connection with the {@link SSLSocketFactory} and
   * {@link HostnameVerifier} of this SSLFactory, otherwise does nothing.
   *
   * @param conn the {@link HttpURLConnection} instance to configure.
   * @return the configured {@link HttpURLConnection} instance.
   *
   * @throws IOException if an IO error occurred.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,configureConnection,org.apache.hadoop.crypto.key.kms.KMSClientProvider:configureConnection(java.net.HttpURLConnection),488,500,"/**
 * Configures connection with SSL factory, if provided.
 * @param conn HttpURLConnection to configure
 * @return Configured HttpURLConnection
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,initializeSSLContext,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeSSLContext(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode),153,185,"/**
* Initializes the SSL context based on the preferred channel mode.
* @param preferredChannelMode The desired SSL channel mode.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(),236,239,"/**
 * Creates an SSL socket using the context's socket factory.
 * @return Configured SSL socket.
 * @throws IOException if socket creation fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.Socket,java.lang.String,int,boolean)",241,248,"/**
 * Creates an SSL socket using the provided socket, host, and port.
 * @param s base socket, host, port, autoClose - socket config
 * @return Configured SSLSocket
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.InetAddress,int,java.net.InetAddress,int)",250,257,"/**
 * Creates an SSL socket connected to the given address and port.
 * @param address The remote address.
 * @param port The remote port.
 * @return An SSLSocket object.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.lang.String,int,java.net.InetAddress,int)",259,266,"/**
 * Creates an SSL socket connected to the specified host and port.
 * @param host Hostname or IP address.
 * @param port Port number.
 * @param localHost Local address.
 * @param localPort Local port.
 * @return Socket object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.net.InetAddress,int)",268,273,"/**
 * Creates an SSL socket connected to the specified host and port.
 * @param host The host address.
 * @param port The port number.
 * @return The configured SSL socket.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,createSocket,"org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:createSocket(java.lang.String,int)",275,280,"/**
 * Creates an SSL socket connected to the specified host and port.
 * @param host The hostname.
 * @param port The port number.
 * @return An SSLSocket object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configured.java,<init>,org.apache.hadoop.conf.Configured:<init>(org.apache.hadoop.conf.Configuration),39,41,"/**
 * Constructs a Configured object using the provided configuration.
 * @param conf The configuration object to use.
 */
","Construct a Configured.
   * @param conf the Configuration object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,handleExecutorTimeout,"org.apache.hadoop.security.ShellBasedUnixGroupsMapping:handleExecutorTimeout(org.apache.hadoop.util.Shell$ShellCommandExecutor,java.lang.String)",174,192,"/**
 * Handles shell executor timeout: logs warning & returns true.
 * @param executor Executor instance.
 * @param user User identifier.
 * @return True if timeout occurred, false otherwise.
 */
","* Check if the executor had a timeout and logs the event.
   * @param executor to check
   * @param user user to log
   * @return true if timeout has occurred",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,toString,org.apache.hadoop.util.Shell$ShellCommandExecutor:toString(),1312,1325,"/**
 * Returns a string representation of the command, quoting arguments with spaces.
 */
","* Returns the commands of this instance.
     * Arguments with spaces in are presented with quotes round; other
     * arguments are presented raw
     *
     * @return a string representation of the object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,read,org.apache.hadoop.security.SaslRpcServer$AuthMethod:read(java.io.DataInput),262,264,"/**
 * Reads an AuthMethod from the input stream.
 * @param in DataInput to read from
 * @return AuthMethod value read from the stream
 */
","* Read from in.
     *
     * @param in DataInput.
     * @throws IOException raised on errors performing I/O.
     * @return AuthMethod.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByExactName,org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByExactName(java.lang.String),687,704,"/**
* Resolves a host name to an InetAddress.
* Appends a trailing dot to disable search list usage.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getKerberosEntry,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getKerberosEntry(),2232,2285,"/**
 * Creates an AppConfigurationEntry for Kerberos login.
 * Configures options based on provided parameters.
 * @return AppConfigurationEntry for Kerberos login.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,"org.apache.hadoop.security.ShellBasedIdMapping$StaticMapping:<init>(java.util.Map,java.util.Map)",572,576,"/**
 * Initializes StaticMapping with UID and GID mappings.
 * @param uidMapping Mapping of user IDs to internal IDs.
 * @param gidMapping Mapping of group IDs to internal IDs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,org.apache.hadoop.security.ShellBasedIdMapping$PassThroughMap:<init>(),545,547,"/**
 * Constructs a PassThroughMap with an empty backing map.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,addUser,org.apache.hadoop.security.authorize.AccessControlList:addUser(java.lang.String),152,159,"/**
* Adds a user to the list if not all are allowed, otherwise throws exception.
* @param user The user to add.
* @throws IllegalArgumentException if user is a wildcard.
*/
","* Add user to the names of users allowed for this service.
   * 
   * @param user
   *          The user name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,addGroup,org.apache.hadoop.security.authorize.AccessControlList:addGroup(java.lang.String),167,177,"/**
 * Adds a group to the list, throwing an exception if it's a wildcard.
 * @param group The group to add.
 */
","* Add group to the names of groups allowed for this service.
   * 
   * @param group
   *          The group name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,removeUser,org.apache.hadoop.security.authorize.AccessControlList:removeUser(java.lang.String),185,192,"/**
 * Removes a user if not a wildcard and all users are allowed.
 */","* Remove user from the names of users allowed for this service.
   * 
   * @param user
   *          The user name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,removeGroup,org.apache.hadoop.security.authorize.AccessControlList:removeGroup(java.lang.String),200,208,"/**
 * Removes a group if not a wildcard and all allowed.
 * @param group The group to remove.
 */
","* Remove group from the names of groups allowed for this service.
   * 
   * @param group
   *          The group name",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,getUsersString,org.apache.hadoop.security.authorize.AccessControlList:getUsersString(),337,339,"/**
 * Returns a string representation of the users.
 * @return String containing user data.
 */
","* Returns comma-separated concatenated single String of the set 'users'
   *
   * @return comma separated list of users",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,getGroupsString,org.apache.hadoop.security.authorize.AccessControlList:getGroupsString(),346,348,"/**
 * Returns a comma-separated string of group names.
 * @return String of group names, or an empty string if none.
 */
","* Returns comma-separated concatenated single String of the set 'groups'
   *
   * @return comma separated list of groups",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AuthorizationException.java,printStackTrace,org.apache.hadoop.security.authorize.AuthorizationException:printStackTrace(),65,68,"/**
 * Prints the stack trace to System.err.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,getProxyGroups,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxyGroups(),175,182,"/**
 * Returns a map of proxy groups based on ACL entries.
 * @return Map of proxy group names to collections of strings.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,getProxyHosts,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getProxyHosts(),184,193,"/**
 * Returns a map of proxy host names to their associated host collections.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,innerSetCredential,"org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:innerSetCredential(java.lang.String,char[])",266,281,"/**
 * Stores a credential entry in the keystore.
 * @param alias Credential alias.
 * @param material Credential password (as char array).
 * @throws IOException if storage fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,validate,org.apache.hadoop.security.alias.CredentialShell$CheckCommand:validate(),321,346,"/**
 * Validates the configuration, checking for alias and credential provider.
 * Returns true if valid, false otherwise, printing error messages.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,validate,org.apache.hadoop.security.alias.CredentialShell$CreateCommand:validate(),411,436,"/**
 * Validates the configuration, checks for alias and credentials.
 * Returns true if valid, false otherwise, prints error messages.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,execute,org.apache.hadoop.security.alias.CredentialShell$CheckCommand:execute(),348,386,"/**
 * Executes the credential check process for a given alias.
 * Reads/uses password, compares with stored credential.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,promptForCredential,org.apache.hadoop.security.alias.CredentialShell:promptForCredential(),473,499,"/**
 * Prompts user for alias password, ensuring they match.
 * @return char[] containing the entered password.
 * @throws IOException if no console is available.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,warnIfTransientProvider,org.apache.hadoop.security.alias.CredentialShell$Command:warnIfTransientProvider(),172,176,"/**
 * Warns if the provider is transient. Prints a warning to the output.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,createPermissions,org.apache.hadoop.security.alias.LocalKeyStoreProvider:createPermissions(java.lang.String),80,90,"/**
 * Sets file permissions using a provided octal string.
 * @param perms Octal string representing file permissions.
 * @throws IOException if permissions are invalid.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,isOriginalTGT,org.apache.hadoop.security.SecurityUtil:isOriginalTGT(javax.security.auth.kerberos.KerberosTicket),168,170,"/**
 * Checks if a Kerberos ticket is an original Ticket Granting Ticket.
 * @param ticket The Kerberos ticket to check.
 * @return True if the ticket is an original TGT, false otherwise.
 */
","* Check whether the server principal is the TGS's principal
   * @param ticket the original TGT (the ticket that is obtained when a 
   * kinit is done)
   * @return true or false",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setSslConfiguration,"org.apache.hadoop.security.SecurityUtil:setSslConfiguration(org.apache.zookeeper.client.ZKClientConfig,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore,org.apache.zookeeper.common.ClientX509Util)",825,850,"/**
 * Configures ZK client for SSL/TLS using provided configuration objects.
 * @param zkClientConfig ZK client config to update.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KerberosAuthException.java,<init>,"org.apache.hadoop.security.KerberosAuthException:<init>(java.lang.String,java.lang.Throwable)",51,54,"/**
 * Constructs a KerberosAuthException with a message and cause.
 * @param initialMsg Initial error message.
 * @param cause The underlying Throwable cause.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/WhitelistBasedResolver.java,getServerProperties,org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.net.InetAddress),116,122,"/**
 * Gets server properties based on client address.
 * @param clientAddress Client's InetAddress; null uses default.
 * @return Map of server properties.
 */
","* Identify the Sasl Properties to be used for a connection with a client.
   * @param clientAddress client's address
   * @return the sasl properties to be used for the connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,getIdentifier,"org.apache.hadoop.security.SaslRpcServer:getIdentifier(java.lang.String,org.apache.hadoop.security.token.SecretManager)",192,204,"/**
 * Retrieves a token identifier from a string ID.
 * @param id The token identifier string.
 * @param secretManager Secret manager for identifier creation.
 * @return The token identifier object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,checkCodec,org.apache.hadoop.crypto.CryptoStreamUtils:checkCodec(org.apache.hadoop.crypto.CryptoCodec),75,81,"/**
 * Validates the codec's cipher suite.
 * Throws UnsupportedCodecException if not AES-CTR or SM4-CTR.
 */
","* AES/CTR/NoPadding or SM4/CTR/NoPadding is required.
   *
   * @param codec crypto codec.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,<init>,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:<init>(),35,40,"/**
 * Constructs a new OpensslAesCtrCryptoCodec, throwing exception if loading fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getPos,org.apache.hadoop.crypto.CryptoInputStream:getPos(),580,585,"/**
 * Returns the current position of the stream.
 * @return Current stream position as a long value.
 */
",Get underlying stream position.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,available,org.apache.hadoop.crypto.CryptoInputStream:available(),672,677,"/**
 * Returns the number of bytes available for reading.
 * Includes bytes in the output buffer.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFromUnderlyingStream,org.apache.hadoop.crypto.CryptoInputStream:readFromUnderlyingStream(java.nio.ByteBuffer),223,231,"/**
 * Reads data from the underlying stream into the buffer.
 * @param inBuffer Buffer to read data into.
 * @return Number of bytes read, or -1 if EOF.
 */
",Read data from underlying stream.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,<init>,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:<init>(int,java.lang.String,org.apache.hadoop.crypto.CipherSuite,java.lang.String)",115,126,"/**
 * Creates a JceCtrCipher with specified mode, provider, suite, and name.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CipherSuite.java,convert,org.apache.hadoop.crypto.CipherSuite:convert(java.lang.String),84,92,"/**
 * Converts a cipher suite name to a CipherSuite enum.
 * @param name Cipher suite name to convert.
 * @throws IllegalArgumentException if name is invalid.
 */
","* Convert to CipherSuite from name, {@link #algoBlockSize} is fixed for
   * certain cipher suite, just need to compare the name.
   * @param name cipher suite name
   * @return CipherSuite cipher suite",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,encrypt,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:encrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",140,143,"/**
 * Encrypts data from inBuffer to outBuffer using process().
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,decrypt,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:decrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",145,148,"/**
 * Decrypts data from inBuffer and writes to outBuffer.
 * @param inBuffer Input buffer containing encrypted data.
 * @param outBuffer Output buffer for decrypted data.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoProtocolVersion.java,supports,org.apache.hadoop.crypto.CryptoProtocolVersion:supports(org.apache.hadoop.crypto.CryptoProtocolVersion),54,64,"/**
 * Checks if the current system supports the given protocol version.
 * @param version The protocol version to check.
 * @return True if supported, false otherwise.
 */
","* Returns if a given protocol version is supported.
   *
   * @param version version number
   * @return true if the version is supported, else false",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CipherOption.java,<init>,org.apache.hadoop.crypto.CipherOption:<init>(org.apache.hadoop.crypto.CipherSuite),34,36,"/**
 * Constructs a CipherOption with the provided CipherSuite.
 * @param suite The CipherSuite for this option.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,tokenizeTransformation,org.apache.hadoop.crypto.OpensslCipher:tokenizeTransformation(java.lang.String),155,179,"/**
 * Parses a transformation string into a Transform object.
 * @param transformation Transformation string (algorithm/mode/padding)
 * @throws NoSuchAlgorithmException if transformation is invalid.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,finalize,org.apache.hadoop.crypto.OpensslCipher:finalize(),293,296,"/**
 * Calls the clean() method before the object is garbage collected.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OpensslSecureRandom.java,next,org.apache.hadoop.crypto.random.OpensslSecureRandom:next(int),105,118,"/**
 * Generates the next integer value with specified bits.
 * @param numBits number of bits for the next value (0-32)
 * @return next integer value
 */
","* Generates an integer containing the user-specified number of
   * random bits (right justified, with leading zeros).
   *
   * @param numBits number of random bits to be generated, where
   * 0 {@literal <=} <code>numBits</code> {@literal <=} 32.
   *
   * @return int an <code>int</code> containing the user-specified number
   * of random bits (right justified, with leading zeros).",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeyVersion(java.lang.String),327,350,"/**
 * Retrieves a KeyVersion by versionName. Returns null if not found.
 * @param versionName Version name of the key to retrieve.
 * @throws IOException If key retrieval fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,innerSetKeyVersion,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:innerSetKeyVersion(java.lang.String,java.lang.String,byte[],java.lang.String)",495,506,"/**
 * Stores a key-version in the keystore.
 * @param name Key name, versionName version name, material key material, cipher cipher algorithm.
 * @return KeyVersion object representing the stored key-version.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSKeyVersion:<init>(java.lang.String,java.lang.String,byte[])",611,613,"/**
 * Constructs a KMSKeyVersion with the given key name, version, and material.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,createKeyProviderCryptoExtension,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:createKeyProviderCryptoExtension(org.apache.hadoop.crypto.key.KeyProvider),605,621,"/**
 * Creates a KeyProviderCryptoExtension, using provided KeyProvider.
 * Returns a KeyProviderCryptoExtension object.
 */
","* Creates a <code>KeyProviderCryptoExtension</code> using a given
   * {@link KeyProvider}.
   * <p>
   * If the given <code>KeyProvider</code> implements the
   * {@link CryptoExtension} interface the <code>KeyProvider</code> itself
   * will provide the extension functionality.
   * If the given <code>KeyProvider</code> implements the
   * {@link KeyProviderExtension} interface and the KeyProvider being
   * extended by the <code>KeyProvider</code> implements the
   * {@link CryptoExtension} interface, the KeyProvider being extended will
   * provide the extension functionality. Otherwise, a default extension
   * implementation will be used.
   *
   * @param keyProvider <code>KeyProvider</code> to use to create the
   * <code>KeyProviderCryptoExtension</code> extension.
   * @return a <code>KeyProviderCryptoExtension</code> instance using the
   * given <code>KeyProvider</code>.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,close,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension:close(),623,629,"/**
 * Closes the key provider, if it exists and is not this object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,readObject,org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:readObject(java.io.ObjectInputStream),700,705,"/**
 * Reads object metadata from the input stream.
 * @param in ObjectInputStream to read metadata from.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSMetadata:<init>(java.lang.String,int,java.lang.String,java.util.Map,java.util.Date,int)",647,650,"/**
 * Constructs a KMSMetadata object with the given parameters.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,writeObject,org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:writeObject(java.io.ObjectOutputStream),694,698,"/**
 * Writes object metadata to the output stream.
 * Serializes metadata and writes its length and content.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,printException,org.apache.hadoop.crypto.key.KeyShell:printException(java.lang.Exception),533,537,"/**
 * Logs an exception to the error stream, with a formatted message.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$ListCommand:execute(),252,271,"/**
 * Lists keys from the KeyProvider, optionally with metadata.
 * Throws IOException if listing fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,getKeysMetadata,org.apache.hadoop.crypto.key.KeyProviderExtension:getKeysMetadata(java.lang.String[]),61,64,"/**
 * Retrieves metadata for specified keys.
 * @param names Key names to fetch metadata for.
 * @return Metadata array or empty array if none found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,cleanupNewAndOld,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:cleanupNewAndOld(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",600,605,"/**
 * Renames the new path and deletes the old path.
 * @param newPath Path to the new file.
 * @param oldPath Path to the old file.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,backupToOld,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:backupToOld(org.apache.hadoop.fs.Path),622,630,"/**
* Renames the current path to an old path, returns true on success.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,revertFromOld,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:revertFromOld(org.apache.hadoop.fs.Path,boolean)",632,637,"/**
 * Reverts to the previous version if the file existed.
 * @param oldPath Path to the previous version.
 * @param fileExisted True if the file existed previously.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,deleteKey,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:deleteKey(java.lang.String),462,493,"/**
 * Deletes a key and all its versions from the keystore.
 * @param name Key name to delete.
 * @throws IOException if key doesn't exist or deletion fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,getAlgorithm,org.apache.hadoop.crypto.key.JavaKeyStoreProvider$KeyMetadata:getAlgorithm(),679,682,"/**
 * Returns the algorithm name from the metadata.
 * @return Cipher algorithm name.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,getCurrentKey,org.apache.hadoop.crypto.key.KeyProvider:getCurrentKey(java.lang.String),496,502,"/**
 * Gets the current KeyVersion for a given name.
 * @param name Key name; returns null if not found.
 */
","* Get the current version of the key, which should be used for encrypting new
   * data.
   * @param name the base name of the key
   * @return the version name of the current version of the key or null if the
   *    key version doesn't exist
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,generateKey,"org.apache.hadoop.crypto.key.KeyProvider:generateKey(int,java.lang.String)",545,552,"/**
 * Generates a key of specified size using the given algorithm.
 * @param size Key size in bits.
 * @param algorithm Algorithm to use (e.g., ""AES"").
 * @return Encoded byte representation of the generated key.
 */
","* Generates a key material.
   *
   * @param size length of the key.
   * @param algorithm algorithm to use for generating the key.
   * @return the generated key.
   * @throws NoSuchAlgorithmException no such algorithm exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,<init>,"org.apache.hadoop.crypto.key.CachingKeyProvider:<init>(org.apache.hadoop.crypto.key.KeyProvider,long,long)",91,95,"/**
 * Constructs a CachingKeyProvider with given key provider and timeouts.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,invalidateCache,org.apache.hadoop.crypto.key.CachingKeyProvider:invalidateCache(java.lang.String),156,164,"/**
 * Invalidates caches related to the given key name.
 * @param name The name of the key to invalidate.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,invalidateCache,org.apache.hadoop.crypto.key.KeyProviderExtension:invalidateCache(java.lang.String),120,123,"/**
 * Invalidates cache entry with the given name.
 * @param name The name of the cache entry to invalidate.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$InvalidateCacheCommand:execute(),511,525,"/**
 * Invalidates the cache for a given key on the KeyProvider.
 * Throws IOException if invalidation fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,toJSON,org.apache.hadoop.util.KMSUtil:toJSON(org.apache.hadoop.crypto.key.KeyProvider$KeyVersion),95,108,"/**
 * Converts a KeyVersion object to a JSON Map.
 * @param keyVersion The KeyVersion to convert.
 * @return A Map representing the KeyVersion as JSON.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,generateEncryptedKey,"org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:generateEncryptedKey(org.apache.hadoop.crypto.Encryptor,org.apache.hadoop.crypto.key.KeyProvider$KeyVersion,byte[],byte[])",307,325,"/**
 * Generates an encrypted key version using provided encryptor and key.
 * @param encryptor encryptor instance
 * @param encryptionKey encryption key
 * @param key key to encrypt
 * @param iv initialization vector
 * @return EncryptedKeyVersion object
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,createForDecryption,"org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion:createForDecryption(java.lang.String,java.lang.String,byte[],byte[])",103,110,"/**
 * Creates an EncryptedKeyVersion for decryption.
 * @param keyName Key name.
 * @param encryptionKeyVersionName Version name.
 * @return EncryptedKeyVersion object.
 */
","* Factory method to create a new EncryptedKeyVersion that can then be
     * passed into {@link #decryptEncryptedKey}. Note that the fields of the
     * returned EncryptedKeyVersion will only partially be populated; it is not
     * necessarily suitable for operations besides decryption.
     *
     * @param keyName Key name of the encryption key use to encrypt the
     *                encrypted key.
     * @param encryptionKeyVersionName Version name of the encryption key used
     *                                 to encrypt the encrypted key.
     * @param encryptedKeyIv           Initialization vector of the encrypted
     *                                 key. The IV of the encryption key used to
     *                                 encrypt the encrypted key is derived from
     *                                 this IV.
     * @param encryptedKeyMaterial     Key material of the encrypted key.
     * @return EncryptedKeyVersion suitable for decryption.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,decryptEncryptedKey,"org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.Decryptor,org.apache.hadoop.crypto.key.KeyProvider$KeyVersion,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion)",410,431,"/**
 * Decrypts an encrypted key version using the provided decryptor.
 * @param decryptor Decryption context.
 * @param encryptionKey Encryption key.
 * @param encryptedKeyVersion Encrypted key version to decrypt.
 * @return Decrypted KeyVersion object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderDelegationTokenExtension.java,createKeyProviderDelegationTokenExtension,org.apache.hadoop.crypto.key.KeyProviderDelegationTokenExtension:createKeyProviderDelegationTokenExtension(org.apache.hadoop.crypto.key.KeyProvider),138,148,"/**
 * Creates a KeyProviderDelegationTokenExtension.
 * @param keyProvider The key provider.
 * @return A KeyProviderDelegationTokenExtension.
 */
","* Creates a <code>KeyProviderDelegationTokenExtension</code> using a given 
   * {@link KeyProvider}.
   * <p>
   * If the given <code>KeyProvider</code> implements the 
   * {@link DelegationTokenExtension} interface the <code>KeyProvider</code> 
   * itself will provide the extension functionality, otherwise a default 
   * extension implementation will be used.
   * 
   * @param keyProvider <code>KeyProvider</code> to use to create the 
   * <code>KeyProviderDelegationTokenExtension</code> extension.
   * @return a <code>KeyProviderDelegationTokenExtension</code> instance 
   * using the given <code>KeyProvider</code>.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,validate,org.apache.hadoop.crypto.key.KeyShell$CreateCommand:validate(),431,454,"/**
 * Validates key provider and key name. Returns true if valid, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,writeJson,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:writeJson(java.lang.Object,java.io.OutputStream)",253,257,"/**
 * Writes a Java object to an output stream as JSON.
 * @param obj The object to serialize.
 * @param os The output stream to write to.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,checkNotEmpty,"org.apache.hadoop.util.KMSUtil:checkNotEmpty(java.lang.String,java.lang.String)",133,141,"/**
 * Checks if a string is not empty.
 * @param s string to check, @param name parameter name
 * @throws IllegalArgumentException if string is empty
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,warmUpEncryptedKeys,org.apache.hadoop.crypto.key.kms.KMSClientProvider:warmUpEncryptedKeys(java.lang.String[]),951,959,"/**
 * Initializes queues for specified encrypted keys.
 * @param keyNames Array of encrypted key names to warm up.
 * @throws IOException if initialization fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,close,org.apache.hadoop.crypto.key.kms.KMSClientProvider:close(),1195,1207,"/**
 * Closes the connection, shutting down the queue and destroying SSL factory.
 */",* Shutdown valueQueue executor threads,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,submitRefillTask,"org.apache.hadoop.crypto.key.kms.ValueQueue:submitRefillTask(java.lang.String,java.util.Queue)",401,443,"/**
 * Submits a refill task to the queue, ensuring controlled insertion.
 * @param keyName The key name associated with the refill task.
 * @param keyQueue The queue to be refilled.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,deleteByName,org.apache.hadoop.crypto.key.kms.ValueQueue$UniqueKeyBlockingQueue:deleteByName(java.lang.String),187,194,"/**
 * Removes and cancels a named runnable.
 * @param name The name of the runnable to remove.
 * @return The removed Runnable or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getLock,org.apache.hadoop.crypto.key.kms.ValueQueue:getLock(java.lang.String),136,138,"/**
 * Retrieves a ReadWriteLock from the lock array for the given key.
 * @param keyName The key used to determine the lock's index.
 * @return The ReadWriteLock associated with the key.
 */
","* Get the stripped lock given a key name.
   *
   * @param keyName The key name.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,flush,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:flush(),557,567,"/**
 * Flushes all KMS client providers, handling potential IO errors.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,isTransient,org.apache.hadoop.crypto.key.KeyProviderExtension:isTransient(),56,59,"/**
 * Checks if the key is transient.
 * @return True if the key is transient, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,warnIfTransientProvider,org.apache.hadoop.crypto.key.KeyShell$Command:warnIfTransientProvider(),219,223,"/**
 * Warns if the provider is transient to prevent data loss.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,createKeyProviderFromUri,"org.apache.hadoop.util.KMSUtil:createKeyProviderFromUri(org.apache.hadoop.conf.Configuration,java.net.URI)",81,93,"/**
 * Creates a KeyProvider from a URI.
 * @param conf Configuration object.
 * @param providerUri URI of the KeyProvider.
 * @return KeyProvider instance.
 * @throws IOException if KeyProvider cannot be instantiated.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,append,org.apache.hadoop.ipc.CallerContext$Builder:append(java.lang.String),207,215,"/**
 * Appends a field to the string builder if valid.
 * @param field The field to append.
 * @return This builder instance.
 */
","* Append new field to the context.
     * @param field one of fields to append.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,append,"org.apache.hadoop.ipc.CallerContext$Builder:append(java.lang.String,java.lang.String)",223,231,"/**
 * Appends a key-value pair to the builder.
 * @param key The key to append.
 * @param value The value associated with the key.
 * @return This builder instance.
 */
","* Append new field which contains key and value to the context.
     * @param key the key of field.
     * @param value the value of field.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,appendIfAbsent,"org.apache.hadoop.ipc.CallerContext$Builder:appendIfAbsent(java.lang.String,java.lang.String)",240,251,"/**
 * Appends key-value pair if key is absent, validates input.
 * @param key key to append
 * @param value value to append
 * @return this Builder instance
 */
","* Append new field which contains key and value to the context
     * if the key(""key:"") is absent.
     * @param key the key of field.
     * @param value the value of field.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,"org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,java.lang.String)",148,154,"/**
 * Initializes a Builder with context and field separator.
 * @param context Initial context string (if valid).
 * @param separator Field separator string.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RefreshResponse.java,successResponse,org.apache.hadoop.ipc.RefreshResponse:successResponse(),37,39,"/**
 * Creates a success response with code 0 and message ""Success"".
 */
","* Convenience method to create a response for successful refreshes.
   * @return void response",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,unpack,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:unpack(org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseProto),82,104,"/**
 * Converts a GenericRefreshResponseProto to a RefreshResponse object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolServerSideTranslatorPB.java,pack,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:pack(java.util.Collection),66,83,"/**
* Packs a collection of RefreshResponse objects into a ProtoBuf.
* @param responses Collection of RefreshResponse objects to pack.
* @return GenericRefreshResponseCollectionProto object.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ObserverRetryOnActiveException.java,<init>,org.apache.hadoop.ipc.ObserverRetryOnActiveException:<init>(java.lang.String),32,34,"/**
 * Constructs a new ObserverRetryOnActiveException with the given message.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientId.java,toString,org.apache.hadoop.ipc.ClientId:toString(byte[]),52,62,"/**
 * Converts a byte array to a UUID string representation.
 * @param clientId byte array representing the client ID
 * @return UUID string or empty string if clientId is invalid
 */
","* @return Convert a clientId byte[] to string.
   * @param clientId input clientId.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntry:<init>(byte[],int,long)",72,82,"/**
 * Constructs a CacheEntry with clientId, callId, and expirationTime.
 * @param clientId Client identifier (16 bytes).
 * @param callId Call identifier.
 * @param expirationTime Expiration timestamp.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,isServerFailOverEnabledByQueue,org.apache.hadoop.ipc.CallQueueManager:isServerFailOverEnabledByQueue(),242,249,"/**
 * Checks if server failover is enabled for the queue.
 * Returns true if queue is FairCallQueue, otherwise false.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getPriorityLevel,org.apache.hadoop.ipc.Server:getPriorityLevel(org.apache.hadoop.ipc.Schedulable),722,725,"/**
 * Gets the priority level of a schedulable element.
 * @param e The schedulable element.
 * @return The priority level.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isClientBackoffEnabled,org.apache.hadoop.ipc.Server:isClientBackoffEnabled(),3872,3874,"/**
 * Checks if client backoff is enabled.
 * @return True if backoff is enabled, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,addInternal,"org.apache.hadoop.ipc.CallQueueManager:addInternal(org.apache.hadoop.ipc.Schedulable,boolean)",306,321,"/**
 * Adds an element, optionally checking for backoff conditions.
 * @param e element to add
 * @param checkBackoff whether to check backoff
 * @return true if added, throws exception if backoff needed
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,offer,org.apache.hadoop.ipc.CallQueueManager:offer(java.lang.Object),335,338,"/**
 * Offers the given element to the queue.
 * @param e the element to offer
 * @return true if the element was added, false otherwise
 */
","* Insert e into the backing queue.
   * Return true if e is queued.
   * Return false if the queue is full.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,offer,"org.apache.hadoop.ipc.CallQueueManager:offer(java.lang.Object,long,java.util.concurrent.TimeUnit)",340,344,"/**
 * Offers an element to the queue, waiting up to timeout.
 * @param e the element to offer
 * @param timeout the timeout
 * @param unit the time unit of the timeout
 * @return true if the offer was successful
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getCallQueueLen,org.apache.hadoop.ipc.Server:getCallQueueLen(),3868,3870,"/**
 * Returns the number of calls currently in the queue.
 */","* The number of rpc calls in the queue.
   * @return The number of rpc calls in the queue.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolInterfaces,org.apache.hadoop.ipc.RPC:getProtocolInterfaces(java.lang.Class),144,147,"/**
 * Returns the protocol's interfaces and their superinterfaces.
 * @param protocol The protocol class to inspect.
 * @return An array of Class objects representing interfaces.
 */
","* Get all interfaces that the given protocol implements or extends
   * which are assignable from VersionedProtocol.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getServerAddress,org.apache.hadoop.ipc.RPC:getServerAddress(java.lang.Object),740,742,"/**
 * Gets the InetSocketAddress of the server for a given proxy.
 */","* @return Returns the server address for a given proxy.
   * @param proxy input proxy.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,org.apache.hadoop.ipc.CallerContext:<init>(org.apache.hadoop.ipc.CallerContext$Builder),71,74,"/**
 * Constructs a CallerContext using data from the Builder.
 * @param builder Builder object containing context and signature.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,toString,org.apache.hadoop.ipc.CallerContext:toString(),112,123,"/**
 * Returns a string representation of the object, handling invalid contexts.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,sendPing,org.apache.hadoop.ipc.Client$Connection:sendPing(),1071,1080,"/**
 * Sends a ping request if the activity interval has elapsed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,registerProtocolEngine,"org.apache.hadoop.ipc.Server:registerProtocolEngine(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcInvoker)",288,300,"/**
 * Registers an RPC protocol engine.
 * @param rpcKind RPC kind to register.
 * @param wrapperClass Request wrapper class.
 * @param rpcInvoker Invoker for RPC calls.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ExternalCall.java,get,org.apache.hadoop.ipc.ExternalCall:get(),47,53,"/**
 * Retrieves the result. Throws ExecutionException if an error occurred.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Timer.java,monotonicNowNanos,org.apache.hadoop.util.Timer:monotonicNowNanos(),58,60,"/**
 * Returns the current monotonic time in nanoseconds.
 */","* Same as {@link #monotonicNow()} but returns its result in nanoseconds.
   * Note that this is subject to the same resolution constraints as
   * {@link System#nanoTime()}.
   * @return a monotonic clock that counts in nanoseconds.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getUserGroupInformation,org.apache.hadoop.ipc.Server$Call:getUserGroupInformation(),1112,1115,"/**
 * Returns the UserGroupInformation associated with the remote user.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemoteUser,org.apache.hadoop.ipc.Server:getRemoteUser(),445,448,"/**
 * Gets the remote user's UserGroupInformation from the current call.
 * @return UserGroupInformation or null if no call is active.
 */
","Returns the RPC remote user when invoked inside an RPC.  Note this
   *  may be different than the current user if called within another doAs
   *  @return connection's UGI or null if not an RPC",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doResponse,org.apache.hadoop.ipc.Server$Call:doResponse(java.lang.Throwable),1105,1107,"/**
* Calls doResponse with a fatal status.
* @param t The exception to handle.
* @throws IOException if an I/O error occurs.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,<init>,"org.apache.hadoop.ipc.DecayRpcScheduler$DecayTask:<init>(org.apache.hadoop.ipc.DecayRpcScheduler,java.util.Timer)",210,213,"/**
 * Constructs a DecayTask with a scheduler and timer.
 * @param scheduler DecayRpcScheduler reference.
 * @param timer Timer instance for scheduling.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,putVersionSignatureMap,"org.apache.hadoop.ipc.RpcClientUtil:putVersionSignatureMap(java.net.InetSocketAddress,java.lang.String,java.lang.String,java.util.Map)",86,89,"/**
 * Adds a protocol signature map to the signature map cache.
 * @param addr Socket address, protocol, rpcKind, map to cache.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,getVersionSignatureMap,"org.apache.hadoop.ipc.RpcClientUtil:getVersionSignatureMap(java.net.InetSocketAddress,java.lang.String,java.lang.String)",91,94,"/**
 * Retrieves the version signature map for a given address, protocol, and RPC kind.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getFingerprints,org.apache.hadoop.ipc.ProtocolSignature:getFingerprints(java.lang.reflect.Method[]),121,130,"/**
 * Generates an array of fingerprints for the given methods.
 * @param methods Array of Method objects.
 * @return Array of integer fingerprints.
 */
","* Convert an array of Method into an array of hash codes
   * 
   * @param methods
   * @return array of hash codes",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,convertProtocolSignatureProtos,org.apache.hadoop.ipc.RpcClientUtil:convertProtocolSignatureProtos(java.util.List),149,161,"/**
 * Converts a list of ProtocolSignatureProto to a map.
 * @param protoList List of ProtocolSignatureProto objects
 * @return Map of version to ProtocolSignature
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,methodExists,"org.apache.hadoop.ipc.RpcClientUtil:methodExists(int,long,java.util.Map)",163,174,"/**
 * Checks if a method exists in the protocol signature for a version.
 * @param methodHash Method hash to check.
 * @param version Protocol version.
 * @param versionMap Map of versions to signatures.
 * @return True if the method exists, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RefreshRegistry.java,dispatch,"org.apache.hadoop.ipc.RefreshRegistry:dispatch(java.lang.String,java.lang.String[])",94,131,"/**
 * Dispatches refresh requests to registered handlers.
 * @param identifier Request identifier.
 * @param args Arguments for the handler.
 * @return Collection of RefreshResponse objects.
 */
","* Lookup the responsible handler and return its result.
   * This should be called by the RPC server when it gets a refresh request.
   * @param identifier the resource to refresh
   * @param args the arguments to pass on, not including the program name
   * @throws IllegalArgumentException on invalid identifier
   * @return the response from the appropriate handler",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,<init>,"org.apache.hadoop.ipc.RemoteException:<init>(java.lang.String,java.lang.String)",40,42,"/**
* Constructs a RemoteException with a class name and message.
* @param className The name of the remote class.
* @param msg The error message.
*/
","* @param className wrapped exception, may be null
   * @param msg may be null",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,unwrapRemoteException,org.apache.hadoop.ipc.RemoteException:unwrapRemoteException(java.lang.Class[]),81,96,"/**
 * Unwraps a remote exception of specified types.
 * @param lookupTypes Exception types to check.
 * @return IOException or this if exception not found.
 */
","* If this remote exception wraps up one of the lookupTypes
   * then return this exception.
   * <p>
   * Unwraps any IOException.
   * 
   * @param lookupTypes the desired exception class. may be null.
   * @return IOException, which is either the lookupClass exception or this.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,unwrapRemoteException,org.apache.hadoop.ipc.RemoteException:unwrapRemoteException(),107,115,"/**
 * Unwraps a remote exception, returning IOException or itself.
 */
","* Instantiate and return the exception wrapped up by this remote exception.
   * 
   * <p> This unwraps any <code>Throwable</code> that has a constructor taking
   * a <code>String</code> as a parameter.
   * Otherwise it returns this.
   * 
   * @return <code>Throwable</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getNumInProcessHandler,org.apache.hadoop.ipc.metrics.RpcMetrics:getNumInProcessHandler(),157,160,"/**
 * Returns the number of in-process handlers.
 * Delegates to the server for the actual count.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getTotalRequests,org.apache.hadoop.ipc.metrics.RpcMetrics:getTotalRequests(),175,178,"/**
 * Returns the total number of requests processed by the server.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getTotalRequestsPerSecond,org.apache.hadoop.ipc.metrics.RpcMetrics:getTotalRequestsPerSecond(),180,183,"/**
 * Returns the total requests per second from the server.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$Call:<init>(int,int,org.apache.hadoop.ipc.RPC$RpcKind,byte[],org.apache.hadoop.tracing.Span,org.apache.hadoop.ipc.CallerContext)",1000,1012,"/**
 * Initializes a new RPC call with provided parameters.
 * @param id Call ID, retry count, RPC kind, client ID, span, context.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProcessingDetails.java,get,"org.apache.hadoop.ipc.ProcessingDetails:get(org.apache.hadoop.ipc.ProcessingDetails$Timing,java.util.concurrent.TimeUnit)",73,75,"/**
 * Converts timing duration to specified time unit.
 * @param type Timing type. @param timeUnit Target TimeUnit.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProcessingDetails.java,toString,org.apache.hadoop.ipc.ProcessingDetails:toString(),97,108,"/**
 * Generates a string representation of the timing values.
 * Iterates through Timing values and appends them to a StringBuilder.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedTimeCostProvider.java,getCost,org.apache.hadoop.ipc.WeightedTimeCostProvider:getCost(org.apache.hadoop.ipc.ProcessingDetails),100,109,"/**
 * Calculates the cost based on processing details and weights.
 * @param details ProcessingDetails object containing data.
 * @return The calculated cost as a long value.
 */
","* Calculates a weighted sum of the times stored on the provided processing
   * details to be used as the cost in {@link DecayRpcScheduler}.
   *
   * @param details Processing details
   * @return The weighted sum of the times. The returned unit is the same
   *         as the default unit used by the provided processing details.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProcessingDetails.java,set,"org.apache.hadoop.ipc.ProcessingDetails:set(org.apache.hadoop.ipc.ProcessingDetails$Timing,long,java.util.concurrent.TimeUnit)",81,83,"/**
 * Sets the value for a given timing type, converting time unit.
 * @param type Timing type to set
 * @param value Value to set
 * @param timeUnit Time unit of the value
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getSchedulingDecisionSummary,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getSchedulingDecisionSummary(),904,912,"/**
 * Gets the scheduling decision summary from the delegate scheduler.
 * Returns ""No Active Scheduler"" if the delegate is null.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getUniqueIdentityCount,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getUniqueIdentityCount(),924,932,"/**
 * Gets the count of unique identities from the delegate scheduler.
 * Returns -1 if the scheduler is null.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getTotalCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getTotalCallVolume(),934,942,"/**
 * Gets the total call volume from the underlying scheduler.
 * Returns -1 if the scheduler is not available.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getAverageResponseTime,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getAverageResponseTime(),944,952,"/**
 * Gets the average response time from the scheduler.
 * Returns default if scheduler is null.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getResponseTimeCountInLastWindow,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getResponseTimeCountInLastWindow(),954,961,"/**
 * Gets response time counts from scheduler, or default if null.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getNumDroppedConnections,org.apache.hadoop.ipc.Server:getNumDroppedConnections(),3859,3862,"/**
 * Returns the number of dropped connections.
 * @return The count of dropped connections.
 */
","* The number of RPC connections dropped due to
   * too many connections.
   * @return the number of dropped rpc connections",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isFull,org.apache.hadoop.ipc.Server$ConnectionManager:isFull(),4088,4091,"/**
 * Checks if the connection pool is full.
 * @return True if full, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getNumOpenConnections,org.apache.hadoop.ipc.Server:getNumOpenConnections(),3837,3839,"/**
 * Returns the number of active connections managed.
 */
","* The number of open RPC conections
   * @return the number of open rpc connections",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getConnections,org.apache.hadoop.ipc.Server:getConnections(),756,759,"/**
 * Returns an array of active connections managed by the ConnectionManager.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,startIdleScan,org.apache.hadoop.ipc.Server$ConnectionManager:startIdleScan(),4159,4161,"/**
 * Schedules the idle scan task for periodic execution.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,putQueue,"org.apache.hadoop.ipc.FairCallQueue:putQueue(int,org.apache.hadoop.ipc.Schedulable)",229,233,"/**
 * Adds an element to the queue with the given priority.
 * @param priority Queue priority.
 * @param e Element to add.
 */
","* Put the element in a queue of a specific priority.
   * @param priority - queue priority
   * @param e - element to add",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offerQueue,"org.apache.hadoop.ipc.FairCallQueue:offerQueue(int,org.apache.hadoop.ipc.Schedulable)",241,248,"/**
 * Offers an element to the queue with the given priority.
 * @param priority priority of the element
 * @param e element to offer
 * @return true if offer was successful, false otherwise
 */
","* Offer the element to queue of a specific priority.
   * @param priority - queue priority
   * @param e - element to add
   * @return boolean if added to the given queue",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,"org.apache.hadoop.ipc.FairCallQueue:offer(org.apache.hadoop.ipc.Schedulable,long,java.util.concurrent.TimeUnit)",269,279,"/**
 * Offers an element to the queue with a timeout.
 * @param e element to offer, timeout duration, unit
 * @return True if offer succeeds, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,org.apache.hadoop.ipc.FairCallQueue:offer(org.apache.hadoop.ipc.Schedulable),281,290,"/**
 * Offers the element to the queue with the corresponding priority.
 * @param e element to offer; returns true if successful.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,drainTo,org.apache.hadoop.ipc.FairCallQueue:drainTo(java.util.Collection),366,369,"/**
 * Drains elements from this queue to the given collection.
 * @param c the collection to drain to
 * @return the number of elements drained
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,addTerseExceptions,org.apache.hadoop.ipc.Server:addTerseExceptions(java.lang.Class[]),174,176,"/**
* Adds exception classes for terse logging.
* @param exceptionClass Array of exception classes to add.
*/
","* Add exception classes for which server won't log stack traces.
   *
   * @param exceptionClass exception classes",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,addSuppressedLoggingExceptions,org.apache.hadoop.ipc.Server:addSuppressedLoggingExceptions(java.lang.Class[]),183,185,"/**
* Adds exception classes to suppress logging.
* @param exceptionClass Array of exception classes to suppress.
*/
","* Add exception classes which server won't log at all.
   *
   * @param exceptionClass exception classes",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,logException,"org.apache.hadoop.ipc.Server:logException(org.slf4j.Logger,java.lang.Throwable,org.apache.hadoop.ipc.Server$Call)",3244,3262,"/**
 * Logs an exception with specified severity based on its class.
 * @param logger Logger instance.
 * @param e Throwable exception to log.
 * @param call Call context.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getSupportedProtocolVersions,"org.apache.hadoop.ipc.RPC$Server:getSupportedProtocolVersions(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",1146,1164,"/**
 * Gets supported protocol versions for a given RPC kind and name.
 * @param rpcKind RPC kind.
 * @param protocolName Protocol name.
 * @return Array of VerProtocolImpl or null if none found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getHighestSupportedProtocol,"org.apache.hadoop.ipc.RPC$Server:getHighestSupportedProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",1166,1187,"/**
 * Finds the highest version of a protocol for a given RPC kind.
 * @param rpcKind RPC kind to search for.
 * @param protocolName Protocol name to find.
 * @return VerProtocolImpl object or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/UnexpectedServerException.java,<init>,org.apache.hadoop.ipc.UnexpectedServerException:<init>(java.lang.String),32,34,"/**
 * Constructs an UnexpectedServerException with the given message.
 */
","* Constructs exception with the specified detail message.
   * 
   * @param messages detailed message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcServerException.java,<init>,org.apache.hadoop.ipc.RpcServerException:<init>(java.lang.String),33,35,"/**
 * Constructs a new RpcServerException with the given error message.
 */
","* Constructs exception with the specified detail message.
   * @param message detailed message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientException.java,<init>,org.apache.hadoop.ipc.RpcClientException:<init>(java.lang.String),31,33,"/**
 * Constructs an RpcClientException with the given error message.
 */
","* Constructs exception with the specified detail message.
   * 
   * @param messages detailed message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/UnexpectedServerException.java,<init>,"org.apache.hadoop.ipc.UnexpectedServerException:<init>(java.lang.String,java.lang.Throwable)",45,47,"/**
 * Constructs an UnexpectedServerException with a message and cause.
 */
","* Constructs exception with the specified detail message and cause.
   * 
   * @param message message.
   * @param cause that cause this exception
   * @param cause the cause (can be retried by the {@link #getCause()} method).
   *          (A <tt>null</tt> value is permitted, and indicates that the cause
   *          is nonexistent or unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcServerException.java,<init>,"org.apache.hadoop.ipc.RpcServerException:<init>(java.lang.String,java.lang.Throwable)",45,47,"/**
 * Constructs a new RpcServerException with message and cause.
 */
","* Constructs exception with the specified detail message and cause.
   * 
   * @param message message.
   * @param cause the cause (can be retried by the {@link #getCause()} method).
   *          (A <tt>null</tt> value is permitted, and indicates that the cause
   *          is nonexistent or unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientException.java,<init>,"org.apache.hadoop.ipc.RpcClientException:<init>(java.lang.String,java.lang.Throwable)",44,46,"/**
 * Constructs an RPC client exception with message and cause.
 */
","* Constructs exception with the specified detail message and cause.
   * 
   * @param message message.
   * @param cause that cause this exception
   * @param cause the cause (can be retried by the {@link #getCause()} method).
   *          (A <tt>null</tt> value is permitted, and indicates that the cause
   *          is nonexistent or unknown.)",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,setCapacity,org.apache.hadoop.ipc.ResponseBuffer:setCapacity(int),60,62,"/**
* Sets the capacity of the FramedBuffer.
* @param capacity The new capacity value.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,reset,org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:reset(),98,102,"/**
 * Resets the buffer state to the initial framing byte count.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,getFramedBuffer,org.apache.hadoop.ipc.ResponseBuffer:getFramedBuffer(),42,46,"/**
 * Retrieves the FramedBuffer from the output stream and sets its size.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addCost,"org.apache.hadoop.ipc.DecayRpcScheduler:addCost(java.lang.Object,long)",568,600,"/**
 * Adds a cost delta to the call costs for a given identity.
 * @param identity Object identifying the cost to add.
 * @param costDelta The amount to add to the cost.
 */
","* Adjust the stored cost for a given identity.
   *
   * @param identity the identity of the user whose cost should be adjusted
   * @param costDelta the cost to add for the given identity",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,computePriorityLevel,"org.apache.hadoop.ipc.DecayRpcScheduler:computePriorityLevel(long,java.lang.Object)",609,634,"/**
 * Computes priority level based on cost and user identity.
 * Returns priority level (0-numLevels) or 0 if default.
 */
","* Given the cost for an identity, compute a scheduling decision.
   *
   * @param cost the cost for an identity
   * @param identity the identity of the user
   * @return scheduling decision from 0 to numLevels - 1",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,setPriorityLevel,"org.apache.hadoop.ipc.DecayRpcScheduler:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",693,699,"/**
 * Sets the priority level for a user.
 * @param ugi UserGroupInformation object
 * @param priority The priority level to set (0-based)
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getCallVolumeSummary,org.apache.hadoop.ipc.DecayRpcScheduler:getCallVolumeSummary(),1127,1133,"/**
 * Returns call volume summary as a string.
 * Uses Jackson to serialize decayed call costs.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,wrap,org.apache.hadoop.ipc.RpcWritable$Buffer:wrap(java.nio.ByteBuffer),145,147,"/**
 * Wraps a ByteBuffer in a Buffer object.
 * @param bb The ByteBuffer to wrap.
 * @return A new Buffer instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:<init>(),514,515,"/**
 * Default constructor for RpcProtobufRequest class.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:<init>(org.apache.hadoop.ipc.protobuf.ProtobufRpcEngineProtos$RequestHeaderProto,com.google.protobuf.Message)",517,520,"/**
 * Constructs an RpcProtobufRequest with a header and payload.
 * @param header Request header information.
 * @param payload Protobuf message payload.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:<init>(),652,653,"/**
* Default constructor for the RpcProtobufRequest class.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:<init>(org.apache.hadoop.ipc.protobuf.ProtobufRpcEngine2Protos$RequestHeaderProto,org.apache.hadoop.thirdparty.protobuf.Message)",655,658,"/**
 * Constructs an RpcProtobufRequest with a header and payload.
 * @param header The request header.
 * @param payload The message payload.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getRemoteException,org.apache.hadoop.ipc.ProtobufHelper:getRemoteException(org.apache.hadoop.thirdparty.protobuf.ServiceException),56,58,"/**
 * Converts a ServiceException to a RemoteException.
 * @param se The ServiceException to convert.
 * @return A RemoteException, or null if conversion fails.
 */
","* Return the IOException thrown by the remote server wrapped in
   * ServiceException as cause.
   * @param se ServiceException that wraps IO exception thrown by the server
   * @return Exception wrapped in ServiceException or
   *         a new IOException that wraps the unexpected ServiceException.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,ipc,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:ipc(org.apache.hadoop.ipc.internal.ShadedProtobufHelper$IpcCall),158,164,"/**
 * Executes an IpcCall and returns the result.
 * @param call The IpcCall to execute.
 * @return The result of the IpcCall.
 */
","* Evaluate a protobuf call, converting any ServiceException to an IOException.
   * @param call invocation to make
   * @return the result of the call
   * @param <T> type of the result
   * @throws IOException any translated protobuf exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getFixedByteString,org.apache.hadoop.ipc.ProtobufHelper:getFixedByteString(java.lang.String),94,96,"/**
 * Retrieves a fixed-length ByteString for the given key.
 * @param key The key to look up.
 * @return The corresponding ByteString.
 */
","* Get the ByteString for frequently used fixed and small set strings.
   * @param key string
   * @return ByteString for frequently used fixed and small set strings.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getByteString,org.apache.hadoop.ipc.ProtobufHelper:getByteString(byte[]),104,107,"/**
 * Returns a ByteString from the given byte array.
 * Uses a helper to avoid unnecessary object creation.
 */
","* Get the byte string of a non-null byte array.
   * If the array is 0 bytes long, return a singleton to reduce object allocation.
   * @param bytes bytes to convert.
   * @return a value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,skipRetryCache,"org.apache.hadoop.ipc.RetryCache:skipRetryCache(byte[],int)",206,211,"/**
 * Determines if a retry cache entry should be skipped.
 * @param clientId Client ID (byte array).
 * @param callId Call ID (int).
 * @return True if skipped, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,setState,"org.apache.hadoop.ipc.RetryCache:setState(org.apache.hadoop.ipc.RetryCache$CacheEntry,boolean)",382,387,"/**
 * Sets the completion status of a CacheEntry.
 * @param e CacheEntry to update; null safe.
 * @param success True if successful, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,toString,org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest:toString(),538,547,"/**
 * Returns a string representation of the request, combining class and method names.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setCallIdAndRetryCount,"org.apache.hadoop.ipc.Client:setCallIdAndRetryCount(int,int,java.lang.Object)",122,128,"/**
 * Sets the call ID and retry count for the handler.
 * @param cid Call ID.
 * @param rc Retry count.
 * @param externalHandler Handler object.
 */
","* Set call id and retry count for the next call.
   * @param cid input cid.
   * @param rc input rc.
   * @param externalHandler input externalHandler.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,close,org.apache.hadoop.ipc.Client:close(),1881,1885,"/**
 * Closes the resource by stopping its operation.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,checkAsyncCall,org.apache.hadoop.ipc.Client:checkAsyncCall(),1430,1442,"/**
 * Checks if the async call limit is exceeded.
 * Throws AsyncCallLimitExceededException if limit is reached.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getListenerAddress,org.apache.hadoop.ipc.Server:getListenerAddress(),3751,3753,"/**
 * Returns the address of the listener.
 * @return InetSocketAddress of the listener.
 */
","* Return the socket (ip+port) on which the RPC server is listening to.
   * @return the socket (ip+port) on which the RPC server is listening to.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuxiliaryListenerAddresses,org.apache.hadoop.ipc.Server:getAuxiliaryListenerAddresses(),3762,3770,"/**
 * Returns a set of addresses used by auxiliary listeners.
 * Returns empty set if no auxiliary listeners are configured.
 */
","* Return the set of all the configured auxiliary socket addresses NameNode
   * RPC is listening on. If there are none, or it is not configured at all, an
   * empty set is returned.
   * @return the set of all the auxiliary addresses on which the
   *         RPC server is listening on.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doStop,org.apache.hadoop.ipc.Server$Listener:doStop(),1673,1688,"/**
 * Stops the selector, listener socket, and shuts down readers.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,stopClient,org.apache.hadoop.ipc.ClientCache:stopClient(org.apache.hadoop.ipc.Client),99,120,"/**
 * Decrements client count and stops the client if count reaches zero.
 * @param client The client to stop.
 */
","* Stop a RPC client connection 
   * A RPC client is closed only when its reference count becomes zero.
   *
   * @param client input client.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,clearClientCache,org.apache.hadoop.ipc.ProtobufRpcEngine2:clearClientCache(),392,395,"/**
 * Clears the client cache. Used for testing purposes.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getHostInetAddress,org.apache.hadoop.ipc.Server$RpcCall:getHostInetAddress(),1224,1227,"/**
 * Returns the host's InetAddress from the connection.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemotePort,org.apache.hadoop.ipc.Server$RpcCall:getRemotePort(),1229,1232,"/**
 * Gets the remote port number of the connection.
 * @return The remote port as an integer.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setDeferredResponse,org.apache.hadoop.ipc.Server$RpcCall:setDeferredResponse(org.apache.hadoop.io.Writable),1347,1365,"/**
 * Sets a deferred response, sending it if the server is running.
 * Handles errors by logging and returning.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,toString,org.apache.hadoop.ipc.Server$RpcCall:toString(),1404,1407,"/**
 * Returns a string representation of the object.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,waitForWork,org.apache.hadoop.ipc.Client$Connection:waitForWork(),1036,1062,"/**
 * Waits for work or closes connection based on conditions.
 * Returns true if work is available, false otherwise.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,handleConnectionTimeout,"org.apache.hadoop.ipc.Client$Connection:handleConnectionTimeout(int,int,java.io.IOException)",921,932,"/**
 * Handles connection timeout: closes connection & retries if possible.
 * @param curRetries Current retry count.
 * @param maxRetries Max retry attempts.
 * @param ioe The IOException that occurred.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,handleConnectionFailure,"org.apache.hadoop.ipc.Client$Connection:handleConnectionFailure(int,java.io.IOException)",934,968,"/**
 * Handles connection failure, retries based on policy, or throws exception.
 * @param curRetries Current retry count.
 * @param ioe The IOException that occurred.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,equals,org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:equals(java.lang.Object),168,171,"/**
* Delegates equality check to the superclass implementation.
*/
",Override equals to avoid findbugs warnings,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getQueueSizes,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getQueueSizes(),438,446,"/**
 * Returns an array of queue sizes from the call queue.
 * Returns an empty array if the call queue is null.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getOverflowedCalls,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getOverflowedCalls(),448,456,"/**
 * Returns an array of call IDs that overflowed the queue.
 * Returns an empty array if the queue is null.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufWrapperLegacy.java,<init>,org.apache.hadoop.ipc.ProtobufWrapperLegacy:<init>(java.lang.Object),51,56,"/**
 * Creates a ProtobufWrapperLegacy with the given message.
 * @param message The protobuf message to wrap.
 * @throws IllegalArgumentException if not an unshaded protobuf.
 */
","* Construct.
   * The type of the parameter is Object so as to keep the casting internal
   * to this class.
   * @param message message to wrap.
   * @throws IllegalArgumentException if the class is not a protobuf message.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,switchToSimple,org.apache.hadoop.ipc.Server$Connection:switchToSimple(),2401,2405,"/**
 * Disables SASL authentication and clears any associated server.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,close,org.apache.hadoop.ipc.Server$Connection:close(),3082,3094,"/**
 * Closes the socket and associated resources, releasing them.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processSaslToken,org.apache.hadoop.ipc.Server$Connection:processSaslToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto),2387,2399,"/**
 * Processes a SASL token from a client message.
 * @param saslMessage Incoming SASL message
 * @return Updated SASL response or null if challenge needed
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,checkDataLength,org.apache.hadoop.ipc.Server$Connection:checkDataLength(int),2446,2459,"/**
 * Validates dataLength. Throws IOException if invalid.
 * @param dataLength Length of data to be processed.
 * @throws IOException if dataLength is out of bounds.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponseOldVersionFatal,"org.apache.hadoop.ipc.Server:setupResponseOldVersionFatal(java.io.ByteArrayOutputStream,org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)",3627,3639,"/**
 * Sends a fatal error response to the client for old versions.
 * @param response Response stream, callId, error class, and error message.
 */
","* Setup response for the IPC Call on Fatal Error from a 
   * client that is using old version of Hadoop.
   * The response is serialized using the previous protocol's response
   * layout.
   * 
   * @param response buffer to serialize the response into
   * @param call {@link Call} to which we are setting up the response
   * @param rv return value for the IPC Call, if the call was successful
   * @param errorClass error class, if the the call failed
   * @param error error message, if the call failed
   * @throws IOException",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRpcRequestWrapper,org.apache.hadoop.ipc.Server:getRpcRequestWrapper(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcKindProto),302,308,"/**
 * Returns the RPC request wrapper class for the given RPC kind.
 * @param rpcKind The RPC kind to look up.
 * @return RPC request wrapper class or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,toString,org.apache.hadoop.ipc.ProtobufRpcEngine2$RpcProtobufRequest:toString(),676,685,"/**
 * Returns a string representation of the request method.
 * Returns the declaring class and method name.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,hashCode,org.apache.hadoop.ipc.RetryCache$CacheEntry:hashCode(),94,97,"/**
 * Generates a hash code based on clientId and callId.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.java,advanceIndex,org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:advanceIndex(),121,132,"/**
 * Advances the index when requests are depleted, moving to the next queue.
 */","* Advances the index, which will change the current index
   * if called enough times.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:<init>(),398,403,"/**
 * Constructor: Initializes the callback with server, call, and method details.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:<init>(),430,435,"/**
 * Initializes the ProtobufRpcEngineCallbackImpl with server info.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,error,org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:error(java.lang.Throwable),412,418,"/**
 * Records an error, updates metrics, and sets deferred error.
 * @param t The Throwable object representing the error.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,error,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:error(java.lang.Throwable),444,450,"/**
 * Records an error, updates metrics, and sets deferred error.
 * @param t The exception that occurred.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,capacity,org.apache.hadoop.ipc.ResponseBuffer:capacity(),56,58,"/**
 * Returns the capacity of the underlying FramedBuffer.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,ensureCapacity,org.apache.hadoop.ipc.ResponseBuffer:ensureCapacity(int),64,68,"/**
 * Ensures the FramedBuffer has at least the specified capacity.
 * @param capacity The minimum required capacity.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setException,org.apache.hadoop.ipc.Client$Call:setException(java.io.IOException),341,344,"/**
* Sets the exception and signals completion.
* @param error The exception to set.
*/
","Set the exception when there is an error.
     * Notify the caller the call is done.
     * 
     * @param error exception thrown by the call; either local or remote",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setRpcResponse,org.apache.hadoop.ipc.Client$Call:setRpcResponse(org.apache.hadoop.io.Writable),351,354,"/**
 * Sets the RPC response and signals completion.
 * @param rpcResponse The response to set.
 */
","Set the return value when there is no error. 
     * Notify the caller the call is done.
     * 
     * @param rpcResponse return value of the rpc call.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,read,org.apache.hadoop.ipc.Client$Connection$PingInputStream:read(),513,524,"/**
 * Reads a character from the input stream, handling socket timeouts.
 * Retries read until successful or timeout is exhausted.
 */
","Read a byte from the stream.
       * Send a ping if timeout on read. Retries if no failure is detected
       * until a byte is read.
       * @throws IOException for any IO problem other than socket timeout",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,read,"org.apache.hadoop.ipc.Client$Connection$PingInputStream:read(byte[],int,int)",532,543,"/**
 * Reads bytes from the input stream, handling SocketTimeoutExceptions.
 * @param buf buffer to store read bytes
 * @param off offset in the buffer
 * @param len number of bytes to read
 * @return number of bytes read
 */
","Read bytes into a buffer starting from offset <code>off</code>
       * Send a ping if timeout on read. Retries if no failure is detected
       * until a byte is read.
       * 
       * @return the total number of bytes read; -1 if the connection is closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getHostAddress,org.apache.hadoop.ipc.Server$Call:getHostAddress(),1063,1066,"/**
 * Gets the host address of the system.
 * @return Host address as a String, or null if unavailable.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemoteIp,org.apache.hadoop.ipc.Server:getRemoteIp(),385,388,"/**
 * Gets the remote IP address from the current call.
 * @return InetAddress object or null if no call exists.
 */
","* @return Returns the remote side ip address when invoked inside an RPC
   *  Returns null in case of an error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getServerRpcInvoker,org.apache.hadoop.ipc.Server:getServerRpcInvoker(org.apache.hadoop.ipc.RPC$RpcKind),310,312,"/**
 * Gets the RPC invoker for the given RPC kind.
 * @param rpcKind The type of RPC request.
 * @return The RPC invoker.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemotePort,org.apache.hadoop.ipc.Server:getRemotePort(),394,397,"/**
 * Gets the remote port from the current call, or 0 if no call exists.
 */","* @return Returns the remote side port when invoked inside an RPC
   * Returns 0 in case of an error.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuxiliaryPortEstablishedQOP,org.apache.hadoop.ipc.Server:getAuxiliaryPortEstablishedQOP(),411,423,"/**
 * Gets the established QOP if on auxiliary port, else null.
 */","* Returns the SASL qop for the current call, if the current call is
   * set, and the SASL negotiation is done. Otherwise return null
   * Note this only returns established QOP for auxiliary port, and
   * returns null for primary (non-auxiliary) port.
   *
   * Also note that CurCall is thread local object. So in fact, different
   * handler threads will process different CurCall object.
   *
   * Also, only return for RPC calls, not supported for other protocols.
   * @return the QOP of the current connection.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getProtocol,org.apache.hadoop.ipc.Server:getProtocol(),450,453,"/**
 * Gets the protocol from the current call, or null if no call exists.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getPriorityLevel,org.apache.hadoop.ipc.Server:getPriorityLevel(),465,468,"/**
 * Gets the priority level of the current call.
 * Returns 0 if no call is active.
 */
","* @return Return the priority level assigned by call queue to an RPC
   * Returns 0 in case no priority is assigned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setLogSlowRPCThresholdTime,org.apache.hadoop.ipc.Server:setLogSlowRPCThresholdTime(long),556,560,"/**
 * Sets the threshold for logging slow RPC calls.
 * @param logSlowRPCThresholdMs Threshold in milliseconds.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setClientBackoffEnabled,org.apache.hadoop.ipc.Server:setClientBackoffEnabled(boolean),3876,3878,"/**
 * Sets whether client backoff is enabled on the call queue.
 * @param value True to enable, false to disable.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,addAuxiliaryListener,org.apache.hadoop.ipc.Server:addAuxiliaryListener(int),3427,3443,"/**
 * Adds an auxiliary listener for the specified port.
 * @param auxiliaryPort Port to bind the listener to.
 * @throws IOException if a listener already exists for the port.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponseForProtobuf,"org.apache.hadoop.ipc.Server:setupResponseForProtobuf(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",3585,3607,"/**
 * Creates a byte array representing a protobuf response.
 * @param header Response header.
 * @param rv Writable payload, or null.
 * @return Byte array containing the protobuf response.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getNumOpenConnectionsPerUser,org.apache.hadoop.ipc.Server:getNumOpenConnectionsPerUser(),3844,3852,"/**
 * Returns user-to-connection map as JSON string.
 * Returns null if serialization fails.
 */
",* @return Get the NumOpenConnections/User.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isServerFailOverEnabled,org.apache.hadoop.ipc.Server:isServerFailOverEnabled(),3880,3883,"/**
 * Checks if server failover is enabled.
 * Delegates to the callQueue for the actual check.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processResponse,"org.apache.hadoop.ipc.Server$Responder:processResponse(java.util.LinkedList,boolean)",1844,1922,"/**
 * Processes RPC responses from the queue. Returns true if done.
 * @param responseQueue Queue of RPC calls to process.
 * @param inHandler True if called from a handler thread.
 * @return True if processing is complete for the channel.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,equals,org.apache.hadoop.ipc.Client$ConnectionId:equals(java.lang.Object),1823,1841,"/**
 * Checks if two ConnectionId objects are equal based on their fields.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/Tracer.java,build,org.apache.hadoop.tracing.Tracer$Builder:build(),91,96,"/**
 * Builds and returns the global Tracer instance.
 * Creates a new Tracer if one doesn't already exist.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/Tracer.java,newSpan,"org.apache.hadoop.tracing.Tracer:newSpan(java.lang.String,org.apache.hadoop.tracing.SpanContext)",55,57,"/**
 * Creates a new Span with the given description and context.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/NullTraceScope.java,<init>,org.apache.hadoop.tracing.NullTraceScope:<init>(),23,25,"/**
 * Creates a NullTraceScope with a null parent scope.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tracing/TraceScope.java,close,org.apache.hadoop.tracing.TraceScope:close(),53,57,"/**
 * Closes the span if it exists, releasing associated resources.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,"org.apache.hadoop.util.MachineList:<init>(java.util.Collection,org.apache.hadoop.util.MachineList$InetAddressFactory)",95,136,"/**
 * Creates a MachineList with host entries and an address factory.
 * @param hostEntries Collection of host entries.
 * @param addressFactory Factory for resolving host addresses.
 */
","* Accepts a collection of ip/cidr/host addresses
   * 
   * @param hostEntries hostEntries.
   * @param addressFactory addressFactory to convert host to InetAddress",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,includes,org.apache.hadoop.util.MachineList:includes(java.lang.String),145,160,"/**
 * Checks if the IP address is included.
 * @param ipAddress The IP address to check.
 * @return True if included, false otherwise.
 */
","* Accepts an ip address and return true if ipAddress is in the list.
   * {@link #includes(InetAddress)} should be preferred
   * to avoid possibly re-resolving the ip address.
   *
   * @param ipAddress ipAddress.
   * @return true if ipAddress is part of the list",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfTest.java,checkConf,org.apache.hadoop.util.ConfTest:checkConf(java.io.InputStream),136,216,"/**
 * Checks configuration file for errors.
 * @param in InputStream containing the configuration file
 * @return List of error messages or empty list if valid
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfTest.java,listFiles,org.apache.hadoop.util.ConfTest:listFiles(java.io.File),218,225,"/**
 * Lists XML files in a directory.
 * @param dir The directory to search.
 * @return An array of XML files.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,<init>,"org.apache.hadoop.util.SysInfoLinux:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.lang.String,long)",195,210,"/**
 * Initializes SysInfoLinux with procfs file paths and jiffy length.
 */","* Constructor which allows assigning the /proc/ directories. This will be
   * used only in unit tests.
   * @param procfsMemFile fake file for /proc/meminfo
   * @param procfsCpuFile fake file for /proc/cpuinfo
   * @param procfsStatFile fake file for /proc/stat
   * @param procfsNetFile fake file for /proc/net/dev
   * @param procfsDisksFile fake file for /proc/diskstats
   * @param jiffyLengthInMillis fake jiffy length value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcMemInfoFile,org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile(boolean),238,305,"/**
 * Reads memory information from /proc/memInfo file.
 * Skips if already read and readAgain is false.
 */
","* Read /proc/meminfo, parse and compute memory information.
   * @param readAgain if false, read only on the first time",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNumProcessors,org.apache.hadoop.util.SysInfoLinux:getNumProcessors(),625,629,"/**
 * Reads processor count from file.
 * @return The number of processors.
 */
",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNumCores,org.apache.hadoop.util.SysInfoLinux:getNumCores(),632,636,"/**
 * Reads CPU core count from /proc/cpuinfo.
 * @return The number of CPU cores.
 */
",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getCpuFrequency,org.apache.hadoop.util.SysInfoLinux:getCpuFrequency(),639,643,"/**
 * Gets the CPU frequency in MHz. Reads from proc file.
 */
",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcStatFile,org.apache.hadoop.util.SysInfoLinux:readProcStatFile(),376,421,"/**
 * Reads CPU time data from /proc/stat file and updates tracker.
 */
","* Read /proc/stat file, parse and calculate cumulative CPU.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNetworkBytesRead,org.apache.hadoop.util.SysInfoLinux:getNetworkBytesRead(),675,679,"/**
 * Gets the total number of network bytes read.
 * Reads the file and returns the byte count.
 */
",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNetworkBytesWritten,org.apache.hadoop.util.SysInfoLinux:getNetworkBytesWritten(),682,686,"/**
 * Gets the number of network bytes written.
 * Reads the file and returns the written byte count.
 */
",{@inheritDoc},,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcDisksInfoFile,org.apache.hadoop.util.SysInfoLinux:readProcDisksInfoFile(),483,543,"/**
 * Reads disk statistics from /proc/diskstats file and aggregates bytes read/written.
 */","* Read /proc/diskstats file, parse and calculate amount
   * of bytes read and written from/to disks.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,realloc,org.apache.hadoop.util.IdentityHashStore:realloc(int),74,90,"/**
 * Resizes the internal buffer to the specified new capacity.
 * Rehashes existing entries into the new buffer.
 * @param newCapacity The new capacity for the buffer.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,get,org.apache.hadoop.util.IdentityHashStore:get(java.lang.Object),152,158,"/**
 * Retrieves the value associated with the given key.
 * @param k the key to retrieve the value for
 * @return the value or null if the key is not found
 */
","* Retrieve a value associated with a given key.
   *
   * @param k Generics Type k.
   * @return Generics Type V.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,remove,org.apache.hadoop.util.IdentityHashStore:remove(java.lang.Object),167,177,"/**
 * Removes the element associated with key 'k'.
 * @param k the key of the element to remove
 * @return the removed value, or null if key not found.
 */
","* Retrieve a value associated with a given key, and delete the
   * relevant entry.
   *
   * @param k Generics Type k.
   * @return Generics Type V.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,ensureNext,org.apache.hadoop.util.LightWeightGSet$SetIterator:ensureNext(),312,327,"/**
 * Advances to the next entry, ensuring modification consistency.
 * Updates 'next' to the next nonempty entry if needed.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,remove,"org.apache.hadoop.util.LightWeightGSet:remove(int,java.lang.Object)",188,219,"/**
 * Removes and returns the element at the given index or matching key.
 * @param index Index of the element to remove, or -1 to remove by key.
 * @param key Key of the element to remove.
 * @return The removed element, or null if not found.
 */
","* Remove the element corresponding to the key,
   * given key.hashCode() == index.
   *
   * @param key key.
   * @param index index.
   * @return If such element exists, return it.
   *         Otherwise, return null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MergeSort.java,mergeSort,"org.apache.hadoop.util.MergeSort:mergeSort(int[],int[],int,int)",42,83,"/**
 * Sorts a portion of the array using merge sort.
 * @param src Source array, @param dest Destination array,
 * @param low Start index, @param high End index (exclusive)
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,setOptionalSecureTransformerAttributes,org.apache.hadoop.util.XMLUtils:setOptionalSecureTransformerAttributes(javax.xml.transform.TransformerFactory),180,186,"/**
 * Sets optional secure transformer attributes on the factory.
 * Sets external DTD/stylesheet access flags.
 */
","* These attributes are recommended for maximum security but some JAXP transformers do
   * not support them. If at any stage, we fail to set these attributes, then we won't try again
   * for subsequent transformers.
   *
   * @param transformerFactory to update",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,string2long,org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:string2long(java.lang.String),906,927,"/**
 * Parses a string to a long, handling binary prefixes (k, m, g, etc.).
 */","* Convert a string to long.
     * The input string is first be trimmed
     * and then it is parsed with traditional binary prefix.
     *
     * For example,
     * ""-1230k"" will be converted to -1230 * 1024 = -1259520;
     * ""891g"" will be converted to 891 * 1024^3 = 956703965184;
     *
     * @param s input string
     * @return a long value represented by the input string.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,long2String,"org.apache.hadoop.util.StringUtils$TraditionalBinaryPrefix:long2String(long,java.lang.String,int)",937,977,"/**
* Converts a long value to a formatted string with unit and decimal places.
*/
","* Convert a long integer to a string with traditional binary prefix.
     * 
     * @param n the value to be converted
     * @param unit The unit, e.g. ""B"" for bytes.
     * @param decimalPlaces The number of decimal places.
     * @return a string with traditional binary prefix.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,formatPercent,"org.apache.hadoop.util.StringUtils:formatPercent(double,int)",153,155,"/**
 * Formats a double as a percentage string.
 * @param fraction The value to format (0.0 - 1.0).
 * @param decimalPlaces Number of decimal places.
 */
","* Format a percentage for presentation to the user.
   * @param fraction the percentage as a fraction, e.g. 0.1 = 10%
   * @param decimalPlaces the number of decimal places
   * @return a string representation of the percentage",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteToHexString,"org.apache.hadoop.util.StringUtils:byteToHexString(byte[],int,int)",183,192,"/**
 * Converts a byte array segment to a hexadecimal string.
 * @param bytes byte array to convert
 * @param start start index (inclusive)
 * @param end end index (exclusive)
 * @return Hexadecimal string representation
 */
","* Given an array of bytes it will convert the bytes to a hex string
   * representation of the bytes
   * @param bytes bytes.
   * @param start start index, inclusively
   * @param end end index, exclusively
   * @return hex string representation of the byte array",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,limitDecimalTo2,org.apache.hadoop.util.StringUtils:limitDecimalTo2(double),1033,1036,"/**
 * Formats a double to a string with two decimal places.
 * @param d The double to format.
 * @return String representation of the formatted double.
 */
","* limitDecimalTo2.
   *
   * @param d double param.
   * @return string value (""%.2f"").
   * @deprecated use StringUtils.format(""%.2f"", d).",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HeapSort.java,sort,"org.apache.hadoop.util.HeapSort:sort(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable)",56,74,"/**
 * Sorts a portion of the indexed sortable using a heap sort algorithm.
 * @param s sortable object, p start index, r end index, rep progress reporter
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,subtract,org.apache.hadoop.util.JvmPauseMonitor$GcTimes:subtract(org.apache.hadoop.util.JvmPauseMonitor$GcTimes),166,169,"/**
 * Subtracts another GcTimes object from this one.
 * @param other The GcTimes object to subtract.
 * @return A new GcTimes object representing the difference.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,org.apache.hadoop.util.ExitUtil:terminate(org.apache.hadoop.util.ExitUtil$ExitException),233,272,"/**
 * Terminates the application with the provided exit code.
 * Logs the exit code and message, handles errors during logging.
 */
","* Exits the JVM if exit is enabled, rethrow provided exception or any raised error otherwise.
   * Inner termination: either exit with the exception's exit code,
   * or, if system exits are disabled, rethrow the exception.
   * @param ee exit exception
   * @throws ExitException if {@link System#exit(int)} is disabled and not suppressed by an Error
   * @throws Error if {@link System#exit(int)} is disabled and one Error arise, suppressing
   * anything else, even <code>ee</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,org.apache.hadoop.util.ExitUtil:halt(org.apache.hadoop.util.ExitUtil$HaltException),286,326,"/**
 * Handles HaltException: logs error and halts VM if enabled.
 * @param he HaltException to handle; exit code from it is used.
 * @throws HaltException if halt is disabled and an error occurs.
 */
","* Halts the JVM if halt is enabled, rethrow provided exception or any raised error otherwise.
   * If halt is disabled, this method throws either the exception argument if no
   * error arise, the first error if at least one arise, suppressing <code>he</code>.
   * If halt is enabled, all throwables are caught, even errors.
   *
   * @param he the exception containing the status code, message and any stack
   * trace.
   * @throws HaltException if {@link Runtime#halt(int)} is disabled and not suppressed by an Error
   * @throws Error if {@link Runtime#halt(int)} is disabled and one Error arise, suppressing
   * anyuthing else, even <code>he</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,<init>,org.apache.hadoop.util.SysInfoWindows:<init>(),56,59,"/**
 * Initializes a SysInfoWindows object, resets its state.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,addShutdownHook,"org.apache.hadoop.util.ShutdownHookManager:addShutdownHook(java.lang.Runnable,int,long,java.util.concurrent.TimeUnit)",319,331,"/**
 * Adds a shutdown hook with a priority and timeout.
 * @param shutdownHook Hook to execute during shutdown.
 * @param priority Hook priority.
 * @param timeout Timeout for the hook.
 * @param unit Timeout unit (e.g., TimeUnit.SECONDS)
 */
","*
   * Adds a shutdownHook with a priority and timeout the higher the priority
   * the earlier will run. ShutdownHooks with same priority run
   * in a non-deterministic order. The shutdown hook will be terminated if it
   * has not been finished in the specified period of time.
   *
   * @param shutdownHook shutdownHook <code>Runnable</code>
   * @param priority priority of the shutdownHook
   * @param timeout timeout of the shutdownHook
   * @param unit unit of the timeout <code>TimeUnit</code>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,removeShutdownHook,org.apache.hadoop.util.ShutdownHookManager:removeShutdownHook(java.lang.Runnable),340,350,"/**
 * Removes a shutdown hook. Throws IllegalStateException if shutdown is in progress.
 * @param shutdownHook The shutdown hook to remove.
 * @return True if the hook was removed, false otherwise.
 */
","* Removes a shutdownHook.
   *
   * @param shutdownHook shutdownHook to remove.
   * @return TRUE if the shutdownHook was registered and removed,
   * FALSE otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,hasShutdownHook,org.apache.hadoop.util.ShutdownHookManager:hasShutdownHook(java.lang.Runnable),358,363,"/**
 * Checks if a shutdown hook is already registered.
 * @param shutdownHook The hook to check for.
 * @return True if the hook is registered, false otherwise.
 */
","* Indicates if a shutdownHook is registered or not.
   *
   * @param shutdownHook shutdownHook to check if registered.
   * @return TRUE/FALSE depending if the shutdownHook is is registered.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,compareTo,org.apache.hadoop.util.ComparableVersion:compareTo(org.apache.hadoop.util.ComparableVersion),460,463,"/**
 * Compares this version to another.
 * @param o the other version to compare to
 * @return negative, zero, or a positive value
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ThreadUtil.java,getResourceAsStream,org.apache.hadoop.util.ThreadUtil:getResourceAsStream(java.lang.String),91,99,"/**
 * Gets an input stream for the named resource.
 * @param resourceName Resource path, relative to classpath.
 * @throws IOException If resource is not found or an error occurs.
 */
","* Convenience method that returns a resource as inputstream from the
   * classpath.
   * <p>
   * Uses the Thread's context classloader to load resource.
   *
   * @param resourceName resource to retrieve.
   *
   * @throws IOException thrown if resource cannot be loaded
   * @return inputstream with the resource.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,logWarning,"org.apache.hadoop.util.InstrumentedLock:logWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)",151,161,"/**
 * Logs a warning about a lock held time exceeding the threshold.
 * @param lockHeldTime Lock held time in milliseconds.
 * @param stats Suppression statistics for lock warnings.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,logWaitWarning,"org.apache.hadoop.util.InstrumentedLock:logWaitWarning(long,org.apache.hadoop.util.InstrumentedLock$SuppressedSnapshot)",163,172,"/**
 * Logs a warning when lock wait time exceeds the threshold.
 * @param lockWaitTime Lock wait time in milliseconds.
 * @param stats Suppression statistics for lock wait warnings.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/QuickSort.java,sortInternal,"org.apache.hadoop.util.QuickSort:sortInternal(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable,int)",69,136,"/**
 * Sorts a portion of the indexed sortable using a recursive approach.
 * @param s IndexedSortable to sort, p start, r end, rep progress, depth recursion level.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,<init>,org.apache.hadoop.util.LineReader:<init>(java.io.InputStream),69,71,"/**
 * Constructs a LineReader with the given input stream.
 * Uses the default buffer size.
 */
","* Create a line reader that reads from the given stream using the
   * default buffer-size (64k).
   * @param in The input stream",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,run,org.apache.hadoop.util.Shell$1:run(),951,960,"/**
 * Executes the command if interval has passed.
 * Resets exit code and sets launch mechanism on macOS.
 */","* Check to see if a command needs to be executed and execute if needed.
   *
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,newDaemonThreadFactory,org.apache.hadoop.util.BlockingThreadPoolExecutorService:newDaemonThreadFactory(java.lang.String),86,102,"/**
 * Creates a daemon thread factory with a given prefix.
 * @param prefix Thread name prefix
 * @return ThreadFactory for creating daemon threads
 */
","* Get a named {@link ThreadFactory} that just builds daemon threads.
   *
   * @param prefix name prefix for all threads created from the factory
   * @return a thread factory that creates named, daemon threads with
   * the supplied exception handler and normal priority",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,<init>,"org.apache.hadoop.util.LightWeightResizableGSet:<init>(int,float)",66,81,"/**
 * Constructs a LightWeightResizableGSet with initial capacity & load factor.
 * @param initCapacity Initial capacity of the set.
 * @param loadFactor Load factor for resizing.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,size,org.apache.hadoop.util.LightWeightResizableGSet:size(),108,111,"/**
 * Returns the number of elements in this list.
 * Delegates to the superclass implementation.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,getIterator,org.apache.hadoop.util.LightWeightResizableGSet:getIterator(java.util.function.Consumer),113,115,"/**
 * Calls the consumer with an iterator over the values.
 * @param consumer Functional interface to process the iterator.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,expandIfNecessary,org.apache.hadoop.util.LightWeightResizableGSet:expandIfNecessary(),148,152,"/**
 * Resizes the array if the size exceeds the threshold.
 * Resizes to double the capacity if below MAX_ARRAY_LENGTH.
 */
","* Checks if we need to expand, and expands if necessary.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayList,org.apache.hadoop.util.Lists:newArrayList(java.util.Iterator),109,113,"/**
 * Creates a new ArrayList from an iterator of elements.
 * @param elements Iterator providing the elements for the list.
 */
","* Creates a <i>mutable</i> {@code ArrayList} instance containing the
   * given elements; a very thin shortcut for creating an empty list
   * and then calling Iterators#addAll.
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return ArrayList Generics Type E.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,addAll,"org.apache.hadoop.util.Lists:addAll(java.util.Collection,java.lang.Iterable)",248,258,"/**
 * Adds all elements from an iterable to a collection.
 * @param addTo Collection to add elements to.
 * @param elementsToAdd Iterable containing elements to add.
 * @return True if the collection was modified.
 */
","* Adds all elements in {@code iterable} to {@code collection}.
   *
   * @return {@code true} if {@code collection} was modified as a result of
   *     this operation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayListWithCapacity,org.apache.hadoop.util.Lists:newArrayListWithCapacity(int),128,132,"/**
 * Creates a new ArrayList with the specified initial capacity.
 * @param initialArraySize The initial size of the ArrayList.
 */
","* Creates an {@code ArrayList} instance backed by an array with the
   * specified initial size;
   * simply delegates to {@link ArrayList#ArrayList(int)}.
   *
   * @param <E> Generics Type E.
   * @param initialArraySize the exact size of the initial backing array for
   *     the returned array list
   *     ({@code ArrayList} documentation calls this value the ""capacity"").
   * @return a new, empty {@code ArrayList} which is guaranteed not to
   *     resize itself unless its size reaches {@code initialArraySize + 1}.
   * @throws IllegalArgumentException if {@code initialArraySize} is negative.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,computeArrayListCapacity,org.apache.hadoop.util.Lists:computeArrayListCapacity(int),192,195,"/**
 * Calculates a suitable initial capacity for an ArrayList.
 * @param arraySize The expected number of elements.
 * @return The calculated capacity.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,getResource,org.apache.hadoop.util.ApplicationClassLoader:getResource(java.lang.String),128,153,"/**
 * Finds a resource by name, searching parent if not found.
 * @param name Resource name to locate.
 * @return URL object representing the resource or null.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,loadClass,"org.apache.hadoop.util.ApplicationClassLoader:loadClass(java.lang.String,boolean)",160,204,"/**
 * Loads a class by name, attempting to load from parent if needed.
 * @param name Class name to load
 * @param resolve Whether to resolve the class after loading
 * @return Loaded Class object or throws ClassNotFoundException
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,writeJsonAsBytes,"org.apache.hadoop.util.JsonSerialization:writeJsonAsBytes(java.lang.Object,java.io.OutputStream)",305,312,"/**
 * Writes a JSON representation of an instance to an output stream.
 * @param instance Object to serialize to JSON.
 * @param dataOutputStream Output stream to write to.
 */
","* Write the JSON as bytes, then close the stream.
   * @param instance instance to write
   * @param dataOutputStream an output stream that will always be closed
   * @throws IOException on any failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,<init>,org.apache.hadoop.util.OperationDuration:<init>(),48,51,"/**
 * Initializes an OperationDuration with start and finish times.
 */","* Instantiate.
   * The start time and finished time are both set
   * to the current clock time.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,finished,org.apache.hadoop.util.OperationDuration:finished(),64,66,"/**
 * Records the finish time.
 * Updates the 'finished' field with the current timestamp.
 */
",* Update the finished time with the current system time.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,asDuration,org.apache.hadoop.util.OperationDuration:asDuration(),114,116,"/**
 * Converts the value to a Duration object (in milliseconds).
 */
","* Get the duration of an operation as a java Duration
   * instance.
   * @return a duration.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,getDurationString,org.apache.hadoop.util.OperationDuration:getDurationString(),72,74,"/**
 * Returns a human-readable duration string from the internal value.
 */","* Return the duration as {@link #humanTime(long)}.
   * @return a printable duration.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,iterator,org.apache.hadoop.util.LightWeightCache:iterator(),234,256,"/**
 * Returns an iterator over the elements in this collection.
 * Delegates to the superclass iterator, remove is unsupported.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,iterator,org.apache.hadoop.util.LightWeightGSet$Values:iterator(),240,243,"/**
* Returns an iterator for the set. Delegates to the superclass.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,getMonomial,"org.apache.hadoop.util.CrcUtil:getMonomial(long,int)",52,77,"/**
 * Calculates a monomial based on lengthBytes and a modulus.
 * @param lengthBytes Length in bytes, must be positive.
 * @param mod The modulus to use for calculations.
 * @return The calculated monomial value.
 */
","* Compute x^({@code lengthBytes} * 8) mod {@code mod}, where {@code mod} is
   * in ""reversed"" (little-endian) format such that {@code mod & 1} represents
   * x^31 and has an implicit term x^32.
   *
   * @param lengthBytes lengthBytes.
   * @param mod mod.
   * @return monomial.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,composeWithMonomial,"org.apache.hadoop.util.CrcUtil:composeWithMonomial(int,int,int,int)",88,91,"/**
 * Combines two CRC values using a monomial and modulus.
 * @param crcA First CRC value.
 * @param crcB Second CRC value.
 * @param monomial Monomial to use for multiplication.
 * @param mod Modulus for Galois field arithmetic.
 * @return Combined CRC value.
 */
","* composeWithMonomial.
   *
   * @param crcA crcA.
   * @param crcB crcB.
   * @param monomial Precomputed x^(lengthBInBytes * 8) mod {@code mod}
   * @param mod mod.
   * @return compose with monomial.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,intToBytes,org.apache.hadoop.util.CrcUtil:intToBytes(int),113,125,"/**
 * Converts an integer to a byte array.
 * @param value The integer to convert.
 * @return A byte array representing the integer.
 */
","* @return 4-byte array holding the big-endian representation of
   *     {@code value}.
   *
   * @param value value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,toSingleCrcString,org.apache.hadoop.util.CrcUtil:toSingleCrcString(byte[]),182,190,"/**
 * Converts 4 bytes to a single CRC string in hex format.
 * @param bytes The byte array to convert.
 * @throws IOException if the byte array length is not 4.
 */
","* For use with debug statements; verifies bytes.length on creation,
   * expecting it to represent exactly one CRC, and returns a hex
   * formatted value.
   *
   * @param bytes bytes.
   * @throws IOException raised on errors performing I/O.
   * @return a list of hex formatted values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,toMultiCrcString,org.apache.hadoop.util.CrcUtil:toMultiCrcString(byte[]),201,218,"/**
 * Converts a byte array to a multi-CRC string representation.
 * @param bytes byte array to convert; length must be a multiple of 4.
 * @throws IOException if the byte array length is not a multiple of 4.
 */
","* For use with debug statements; verifies bytes.length on creation,
   * expecting it to be divisible by CRC byte size, and returns a list of
   * hex formatted values.
   *
   * @param bytes bytes.
   * @throws IOException raised on errors performing I/O.
   * @return a list of hex formatted values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJar,"org.apache.hadoop.util.RunJar:unJar(java.io.InputStream,java.io.File,java.util.regex.Pattern)",119,152,"/**
 * Unpacks JAR entries matching a regex to a directory.
 * @param inputStream Input stream for the JAR file.
 * @param toDir Directory to unpack entries to.
 * @param unpackRegex Regex pattern for entries to unpack.
 */
","* Unpack matching files from a jar. Entries inside the jar that do
   * not match the given pattern will be skipped.
   *
   * @param inputStream the jar stream to unpack
   * @param toDir the destination directory into which to unpack the jar
   * @param unpackRegex the pattern to match jar entries against
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJar,"org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File,java.util.regex.Pattern)",191,222,"/**
 * Extracts files from a JAR archive to a directory, filtering by regex.
 * @param jarFile JAR file to extract from.
 * @param toDir Directory to extract files to.
 * @param unpackRegex Regex to filter files to unpack.
 */
","* Unpack matching files from a jar. Entries inside the jar that do
   * not match the given pattern will be skipped.
   *
   * @param jarFile the .jar file to unpack
   * @param toDir the destination directory into which to unpack the jar
   * @param unpackRegex the pattern to match jar entries against
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newCachedThreadPool,org.apache.hadoop.util.concurrent.HadoopExecutors:newCachedThreadPool(java.util.concurrent.ThreadFactory),36,42,"/**
 * Creates a thread pool that creates threads as needed.
 * @param threadFactory ThreadFactory for creating new threads
 * @return ExecutorService using a cached thread pool
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newFixedThreadPool,"org.apache.hadoop.util.concurrent.HadoopExecutors:newFixedThreadPool(int,java.util.concurrent.ThreadFactory)",44,50,"/**
 * Creates a fixed-size thread pool with the given thread factory.
 * @param nThreads Number of threads in the pool.
 * @param threadFactory Factory for creating new threads.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newFixedThreadPool,org.apache.hadoop.util.concurrent.HadoopExecutors:newFixedThreadPool(int),52,56,"/**
 * Creates a fixed-size thread pool with nThreads.
 * @param nThreads the number of threads in the pool
 * @return ExecutorService with a fixed thread count
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newScheduledThreadPool,org.apache.hadoop.util.concurrent.HadoopExecutors:newScheduledThreadPool(int),71,74,"/**
 * Creates a ScheduledExecutorService with a Hadoop-specific thread pool.
 * @param corePoolSize The number of threads in the pool.
 * @return A ScheduledExecutorService.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopExecutors.java,newScheduledThreadPool,"org.apache.hadoop.util.concurrent.HadoopExecutors:newScheduledThreadPool(int,java.util.concurrent.ThreadFactory)",76,79,"/**
 * Creates a Hadoop ScheduledThreadPoolExecutor with given size and factory.
 * @param corePoolSize Core thread pool size.
 * @param threadFactory Thread factory for creating new threads.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopScheduledThreadPoolExecutor.java,afterExecute,"org.apache.hadoop.util.concurrent.HadoopScheduledThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)",66,70,"/**
 * Logs any throwable occurring during task execution.
 * @param r The runnable task.
 * @param t The exception, if any.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/HadoopThreadPoolExecutor.java,afterExecute,"org.apache.hadoop.util.concurrent.HadoopThreadPoolExecutor:afterExecute(java.lang.Runnable,java.lang.Throwable)",87,91,"/**
 * Logs any throwable that occurred during task execution.
 * @param r The runnable task.
 * @param t The exception, if any.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/AsyncGetFuture.java,get,org.apache.hadoop.util.concurrent.AsyncGetFuture:get(),56,60,"/**
 * Retrieves the result of the asynchronous computation.
 * Calls asyncGet and then returns the result from super.get().
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/AsyncGetFuture.java,get,"org.apache.hadoop.util.concurrent.AsyncGetFuture:get(long,java.util.concurrent.TimeUnit)",62,67,"/**
 * Retrieves the result with a timeout.
 * @param timeout Timeout duration.
 * @param unit Time unit of the timeout.
 * @return The result.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/concurrent/AsyncGetFuture.java,isDone,org.apache.hadoop.util.concurrent.AsyncGetFuture:isDone(),69,73,"/**
 * Checks if the task is done, calls async get, and returns result.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,<init>,org.apache.hadoop.util.StopWatch:<init>(),33,35,"/**
 * Constructs a StopWatch with a default Timer.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/UTF8ByteArrayUtils.java,findNthByte,"org.apache.hadoop.util.UTF8ByteArrayUtils:findNthByte(byte[],int,int,byte,int)",78,89,"/**
 * Finds the nth occurrence of a byte within a byte array.
 * @param utf byte array, start index, length, byte to find, nth occurrence
 * @return index of the nth byte or -1 if not found
 */
","* Find the nth occurrence of the given byte b in a UTF-8 encoded string
   * @param utf a byte array containing a UTF-8 encoded string
   * @param start starting offset
   * @param length the length of byte array
   * @param b the byte to find
   * @param n the desired occurrence of the given byte
   * @return position that nth occurrence of the given byte if exists; otherwise -1",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CacheableIPList.java,<init>,"org.apache.hadoop.util.CacheableIPList:<init>(org.apache.hadoop.util.FileBasedIPList,long)",33,37,"/**
 * Constructs a CacheableIPList with a timeout and IP list.
 * @param ipList The FileBasedIPList to cache.
 * @param cacheTimeout Cache expiry time in milliseconds.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,toString,org.apache.hadoop.util.WeakReferenceMap:toString(),108,115,"/**
 * Returns a string representation of this WeakReferenceMap.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,put,"org.apache.hadoop.util.WeakReferenceMap:put(java.lang.Object,java.lang.Object)",246,248,"/**
* Associates the specified key with the given value in the map.
*/","* Put a value under the key.
   * A null value can be put, though on a get() call
   * a new entry is generated
   *
   * @param key key
   * @param value value
   * @return any old non-null reference.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,remove,org.apache.hadoop.util.WeakReferenceMap:remove(java.lang.Object),255,257,"/**
 * Removes the mapping for a key and returns the resolved value.
 * @param key the key whose mapping to be removed
 * @return the resolved value, or null if key was not present
 */
","* Remove any value under the key.
   * @param key key
   * @return any old non-null reference.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,containsKey,org.apache.hadoop.util.WeakReferenceMap:containsKey(java.lang.Object),266,269,"/**
 * Checks if the map contains a key.
 * @param key The key to check for.
 * @return True if the key exists, false otherwise.
 */
","* Does the map have a valid reference for this object?
   * no-side effects: there's no attempt to notify or cleanup
   * if the reference is null.
   * @param key key to look up
   * @return true if there is a valid reference.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,create,org.apache.hadoop.util.WeakReferenceMap:create(java.lang.Object),197,235,"/**
 * Creates a new value for the given key using the factory.
 * @param key The key for which to create a value.
 * @return The created value, resolved from a WeakReference.
 */
","* Create a new instance under a key.
   * <p>
   * The instance is created, added to the map and then the
   * map value retrieved.
   * This ensures that the reference returned is that in the map,
   * even if there is more than one entry being created at the same time.
   * If that race does occur, it will be logged the first time it happens
   * for this specific map instance.
   * <p>
   * HADOOP-18456 highlighted the risk of a concurrent GC resulting a null
   * value being retrieved and so returned.
   * To prevent this:
   * <ol>
   *   <li>A strong reference is retained to the newly created instance
   *       in a local variable.</li>
   *   <li>That variable is used after the resolution process, to ensure
   *       the JVM doesn't consider it ""unreachable"" and so eligible for GC.</li>
   *   <li>A check is made for the resolved reference being null, and if so,
   *       the put() is repeated</li>
   * </ol>
   * @param key key
   * @return the created value",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,prune,org.apache.hadoop.util.WeakReferenceMap:prune(),288,300,"/**
 * Removes entries from the map where the referenced value is null.
 * @return The number of entries removed.
 */
","* Prune all null weak references, calling the referenceLost
   * callback for each one.
   *
   * non-atomic and non-blocking.
   * @return the number of entries pruned.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,snapshot,org.apache.hadoop.util.InstrumentedLock$SuppressedStats:snapshot(),262,268,"/**
 * Creates and returns a snapshot of suppressed exceptions.
 * Resets suppressedCount and maxSuppressedWait to 0.
 */
","* Captures the current value of the counts into a SuppressedSnapshot object
     * and resets the values to zero.
     *
     * @return SuppressedSnapshot containing the current value of the counters",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,formatTimeDiff,"org.apache.hadoop.util.StringUtils:formatTimeDiff(long,long)",294,297,"/**
* Formats the time difference between two timestamps.
* @param finishTime Finish timestamp in milliseconds.
* @param startTime Start timestamp in milliseconds.
*/
","* 
   * Given a finish and start time in long milliseconds, returns a 
   * String in the format Xhrs, Ymins, Z sec, for the time difference between two times. 
   * If finish time comes before start time then negative valeus of X, Y and Z wil return. 
   * 
   * @param finishTime finish time
   * @param startTime start time
   * @return a String in the format Xhrs, Ymins, Z sec,
   *         for the time difference between two times.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getTrimmedStringCollectionSplitByEquals,org.apache.hadoop.util.StringUtils:getTrimmedStringCollectionSplitByEquals(java.lang.String),505,525,"/**
 * Parses string collection into a map of String key-value pairs.
 */","* Splits an ""="" separated value <code>String</code>, trimming leading and
   * trailing whitespace on each value after splitting by comma and new line separator.
   *
   * @param str a comma separated <code>String</code> with values, may be null
   * @return a <code>Map</code> of <code>String</code> keys and values, empty
   * Collection if null String input.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,split,"org.apache.hadoop.util.StringUtils:split(java.lang.String,char,char)",581,601,"/**
 * Splits a string by a separator, respecting an escape character.
 * @param str String to split
 * @param escapeChar Escape character
 * @param separator Separator character
 * @return String array of split parts
 */
","* Split a string using the given separator
   * @param str a string that may have escaped separator
   * @param escapeChar a char that be used to escape the separator
   * @param separator a separator char
   * @return an array of strings",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,escapeString,"org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char[])",701,716,"/**
 * Escapes characters in a string.
 * @param str input string
 * @param escapeChar char to escape with
 * @param charsToEscape chars to escape
 * @return escaped string
 */
","* escapeString.
   *
   * @param str str.
   * @param escapeChar escapeChar.
   * @param charsToEscape array of characters to be escaped
   * @return escapeString.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,unEscapeString,"org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char[])",748,782,"/**
 * Unescapes a string, handling escape characters and specified chars.
 * @param str String to unescape
 * @return Unescaped string
 */
","* unEscapeString.
   * @param str str.
   * @param escapeChar escapeChar.
   * @param charsToEscape array of characters to unescape
   * @return escape string.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getVersion,org.apache.hadoop.util.VersionInfo:getVersion(),105,107,"/**
 * Returns the application version string.
 */","* Get the Hadoop version.
   * @return the Hadoop version string, eg. ""0.6.3-dev""",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getRevision,org.apache.hadoop.util.VersionInfo:getRevision(),113,115,"/**
 * Returns the revision number from the COMMON_VERSION_INFO.
 */","* Get the Git commit hash of the repository when compiled.
   * @return the commit hash, eg. ""18f64065d5db6208daf50b02c1b5ed4ee3ce547a""",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getBranch,org.apache.hadoop.util.VersionInfo:getBranch(),121,123,"/**
 * Retrieves the branch name from the common version info.
 * @return The branch name as a String.
 */
","* Get the branch on which this originated.
   * @return The branch name, e.g. ""trunk"" or ""branches/branch-0.20""",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getDate,org.apache.hadoop.util.VersionInfo:getDate(),129,131,"/**
 * Retrieves the date from the COMMON_VERSION_INFO object.
 */","* The date that Hadoop was compiled.
   * @return the compilation date in unix date format",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getUser,org.apache.hadoop.util.VersionInfo:getUser(),137,139,"/**
 * Retrieves the user identifier from COMMON_VERSION_INFO.
 * @return User identifier string.
 */
","* The user that compiled Hadoop.
   * @return the username of the user",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getUrl,org.apache.hadoop.util.VersionInfo:getUrl(),145,147,"/**
 * Returns the URL from the COMMON_VERSION_INFO object.
 */
","* Get the URL for the Hadoop repository.
   * @return the URL of the Hadoop repository",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,_getBuildVersion,org.apache.hadoop.util.VersionInfo:_getBuildVersion(),85,90,"/**
 * Returns the build version string including revision, user, checksum.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getSrcChecksum,org.apache.hadoop.util.VersionInfo:getSrcChecksum(),153,155,"/**
 * Retrieves the source checksum from the common version info.
 * @return String representing the source checksum.
 */
","* Get the checksum of the source files from which Hadoop was built.
   * @return the checksum of the source files",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getProtocVersion,org.apache.hadoop.util.VersionInfo:getProtocVersion(),170,172,"/**
 * Returns the protocol compiler version.
 */","* Returns the protoc version used for the build.
   * @return the protoc version",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getCompilePlatform,org.apache.hadoop.util.VersionInfo:getCompilePlatform(),178,180,"/**
 * Returns the platform on which the code was compiled.
 */
","* Returns the OS platform used for the build.
   * @return the OS platform",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)",604,610,"/**
 * Creates a FilteringRemoteIterator from a source iterator and filter.
 * @param source Source iterator to filter.
 * @param filter Filter function (raises IOE).
 */
","* An iterator which combines filtering with transformation.
     * All source elements for which filter = true are returned,
     * transformed via the mapper.
     * @param source source iterator.
     * @param filter filter predicate.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$MappingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.FunctionRaisingIOE)",521,526,"/**
 * Creates a MappingRemoteIterator that maps elements from source.
 * @param source The source iterator.
 * @param mapper The function to map elements.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,java.io.Closeable)",677,682,"/**
 * Creates a CloseRemoteIterator, wrapping a source iterator and a Closeable.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,"org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.CallableRaisingIOE)",773,778,"/**
 * Constructs a HaltableRemoteIterator from a source iterator.
 * @param source The iterator to wrap.
 * @param continueWork Callable to determine if iteration continues.
 */
","* Wrap an iterator with one which adds a continuation probe.
     * The probe will be called in the {@link #hasNext()} method, before
     * the source iterator is itself checked and in {@link #next()}
     * before retrieval.
     * That is: it may be called multiple times per iteration.
     * @param source source iterator.
     * @param continueWork predicate which will trigger a fast halt if it returns false.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,org.apache.hadoop.util.functional.RemoteIterators$TypeCastingRemoteIterator:<init>(org.apache.hadoop.fs.RemoteIterator),553,556,"/**
 * Initializes the TypeCastingRemoteIterator with a source iterator.
 * @param source The iterator to wrap.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,hasNext,org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:hasNext(),634,640,"/**
 * Checks if there's a next element available.
 * Returns true if next is not null, otherwise fetches.
 */
","* Trigger a fetch if an entry is needed.
     * @return true if there was already an entry return,
     * or there was not but one could then be retrieved.set
     * @throws IOException failure in fetch operation",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,<init>,org.apache.hadoop.util.functional.RemoteIterators$MaybeClose:<init>(java.lang.Object),723,725,"/**
 * Constructs a MaybeClose with an object and default close flag.
 * @param o The object to be potentially closed.
 */
","* Construct.
     * @param o object to close.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,close,org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:close(),415,419,"/**
 * Closes the underlying source, releasing resources.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,close,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:close(),454,457,"/**
 * Closes the underlying source, releasing resources.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,next,org.apache.hadoop.util.functional.RemoteIterators$RangeExcludingLongIterator:next(),829,837,"/**
 * Returns the next number in the sequence.
 * Throws NoSuchElementException if no more numbers.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,submit,"org.apache.hadoop.util.functional.CommonCallableSupplier:submit(java.util.concurrent.Executor,java.util.concurrent.Callable)",82,87,"/**
 * Submits a Callable to an Executor and returns a CompletableFuture.
 * @param executor Executor to run the Callable
 * @param call Callable to execute
 * @return CompletableFuture that will hold the result
 */
","* Submit a callable into a completable future.
   * RTEs are rethrown.
   * Non RTEs are caught and wrapped; IOExceptions to
   * {@code RuntimeIOException} instances.
   * @param executor executor.
   * @param call     call to invoke
   * @param <T>      type
   * @return the future to wait for",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAutoCloseableReference.java,<init>,org.apache.hadoop.util.functional.LazyAutoCloseableReference:<init>(org.apache.hadoop.util.functional.CallableRaisingIOE),43,45,"/**
 * Constructs a LazyAutoCloseableReference with the given constructor.
 */
","* Constructor for this instance.
   * @param constructor method to invoke to actually construct the inner object.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAtomicReference.java,lazyAtomicReferenceFromSupplier,org.apache.hadoop.util.functional.LazyAtomicReference:lazyAtomicReferenceFromSupplier(java.util.function.Supplier),148,151,"/**
 * Creates a LazyAtomicReference initialized with a supplier.
 * @param supplier Supplier that provides the initial value.
 */
","* Create from a supplier.
   * This is not a constructor to avoid ambiguity when a lambda-expression is
   * passed in.
   * @param supplier supplier implementation.
   * @return a lazy reference.
   * @param <T> type of reference",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAutoCloseableReference.java,eval,org.apache.hadoop.util.functional.LazyAutoCloseableReference:eval(),51,55,"/**
 * Evaluates the reference. Checks if closed; calls super.eval().
 */","* {@inheritDoc}
   * @throws IllegalStateException if the reference is closed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAtomicReference.java,apply,org.apache.hadoop.util.functional.LazyAtomicReference:apply(),104,107,"/**
 * Applies the function and returns the result.
 * @return The result of the evaluation.
 * @throws IOException if an I/O error occurs.
 */
","* Implementation of {@code CallableRaisingIOE.apply()}.
   * Invoke {@link #eval()}.
   * @return the value
   * @throws IOException on any evaluation failure",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FunctionalIO.java,uncheckIOExceptions,org.apache.hadoop.util.functional.FunctionalIO:uncheckIOExceptions(org.apache.hadoop.util.functional.CallableRaisingIOE),45,47,"/**
 * Executes Callable, suppressing IOExceptions.
 * @param call Callable to execute.
 * @return Result of the Callable.
 */
","* Invoke any operation, wrapping IOExceptions with
   * {@code UncheckedIOException}.
   * @param call callable
   * @param <T> type of result
   * @return result
   * @throws UncheckedIOException if an IOE was raised.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FunctionalIO.java,toUncheckedIOExceptionSupplier,org.apache.hadoop.util.functional.FunctionalIO:toUncheckedIOExceptionSupplier(org.apache.hadoop.util.functional.CallableRaisingIOE),55,57,"/**
 * Wraps a CallableRaisingIOE in a Supplier, handling IOEs.
 * @param call Callable that may raise an IOException
 * @return Supplier that returns T, handling IOEs
 */
","* Wrap a {@link CallableRaisingIOE} as a {@link Supplier}.
   * @param call call to wrap
   * @param <T> type of result
   * @return a supplier which invokes the call.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,next,org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:next(),340,349,"/**
 * Returns the next element in the stream. Throws NoSuchElementException if empty.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,foreach,org.apache.hadoop.util.functional.TaskPool:foreach(org.apache.hadoop.fs.RemoteIterator),591,593,"/**
 * Creates a Builder from a RemoteIterator.
 * @param items Iterator to build from.
 * @return New Builder instance.
 */
","* Create a task builder for the remote iterator.
   * @param items item source.
   * @param <I> type of result.
   * @return builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,throwOne,org.apache.hadoop.util.functional.TaskPool:throwOne(java.util.Collection),607,622,"/**
 * Throws the first exception, suppressing others if types differ.
 * @param exceptions Collection of exceptions to process.
 * @throws E The first exception in the collection.
 */
","* Throw one exception, adding the others as suppressed
   * exceptions attached to the one thrown.
   * This method never completes normally.
   * @param exceptions collection of exceptions
   * @param <E> class of exceptions
   * @throws E an extracted exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,<init>,org.apache.hadoop.util.functional.TaskPool$Builder:<init>(java.lang.Iterable),161,163,"/**
 * Creates a builder from an iterable of items.
 * @param items Iterable to initialize the builder.
 */
","* Create the builder.
     * @param items items to process",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,suppressExceptions,org.apache.hadoop.util.functional.TaskPool$Builder:suppressExceptions(),197,199,"/**
 * Suppresses exceptions during operation.
 * @return This builder instance.
 */
","* Suppress exceptions from tasks.
     * RemoteIterator exceptions are not suppressable.
     * @return the builder.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,raiseInnerCause,org.apache.hadoop.util.functional.FutureIO:raiseInnerCause(java.util.concurrent.ExecutionException),254,257,"/**
 * Re-throws the inner exception from an ExecutionException.
 * @param e the exception containing the inner exception
 * @throws IOException if an I/O error occurs during unwrapping
 */
","* From the inner cause of an execution exception, extract the inner cause
   * if it is an IOE or RTE.
   * This will always raise an exception, either the inner IOException,
   * an inner RuntimeException, or a new IOException wrapping the raised
   * exception.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,raiseInnerCause,org.apache.hadoop.util.functional.FutureIO:raiseInnerCause(java.util.concurrent.CompletionException),269,272,"/**
 * Re-throws the inner exception from a CompletionException.
 * @param e The CompletionException to unwrap.
 * @throws IOException If an I/O error occurs during unwrapping.
 */
","* Extract the cause of a completion failure and rethrow it if an IOE
   * or RTE.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,setJobConf,"org.apache.hadoop.util.ReflectionUtils:setJobConf(java.lang.Object,org.apache.hadoop.conf.Configuration)",89,115,"/**
 * Configures an object if JobConf and JobConfigurable are present.
 * @param theObject Object to configure, must extend JobConfigurable.
 * @param conf Configuration object, must extend JobConf.
 */
","* This code is to support backward compatibility and break the compile  
   * time dependency of core on mapred.
   * This should be made deprecated along with the mapred package HADOOP-1230. 
   * Should be removed when mapred package is removed.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClassByName,org.apache.hadoop.conf.Configuration:getClassByName(java.lang.String),2638,2644,"/**
 * Retrieves a Class object by name.
 * @param name Class name to retrieve.
 * @return Class object or throws ClassNotFoundException.
 */
","* Load a class by name.
   * 
   * @param name the class name.
   * @return the class object.
   * @throws ClassNotFoundException if the class is not found.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,printThreadInfo,"org.apache.hadoop.util.ReflectionUtils:printThreadInfo(java.io.PrintStream,java.lang.String)",183,221,"/**
 * Prints thread information to a stream, including state and stack.
 * @param stream Output stream for thread info.
 * @param title Title for the thread dump.
 */
","* Print all of the thread's information and stack traces.
   * 
   * @param stream the stream to
   * @param title a string title for the stack trace",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration:<init>(boolean),830,836,"/**
 * Constructs a Configuration object.
 * @param loadDefaults Whether to load default configurations.
 */
","A new configuration where the behavior of reading from the default 
   * resources can be turned off.
   * 
   * If the parameter {@code loadDefaults} is false, the new instance
   * will not load resources from the default files. 
   * @param loadDefaults specifies whether to load from the default files",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,createServletExceptionResponse,"org.apache.hadoop.util.HttpExceptionUtils:createServletExceptionResponse(javax.servlet.http.HttpServletResponse,int,java.lang.Throwable)",72,86,"/**
 * Sends a JSON error response to the given HttpServletResponse.
 * @param response The response to send the error to.
 * @param status HTTP status code.
 * @param ex The exception that caused the error.
 */
","* Creates a HTTP servlet response serializing the exception in it as JSON.
   *
   * @param response the servlet response
   * @param status the error code to set in the response
   * @param ex the exception to serialize in the response
   * @throws IOException thrown if there was an error while creating the
   * response",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,createJerseyExceptionResponse,"org.apache.hadoop.util.HttpExceptionUtils:createJerseyExceptionResponse(javax.ws.rs.core.Response$Status,java.lang.Throwable)",95,104,"/**
 * Creates a Jersey Response containing exception details in JSON.
 * @param status HTTP status code.
 * @param ex The exception to include in the response.
 * @return A Jersey Response object.
 */
","* Creates a HTTP JAX-RPC response serializing the exception in it as JSON.
   *
   * @param status the error code to set in the response
   * @param ex the exception to serialize in the response
   * @return the JAX-RPC response with the set error and JSON encoded exception",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,throwEx,org.apache.hadoop.util.HttpExceptionUtils:throwEx(java.lang.Throwable),119,121,"/**
 * Re-throws the given Throwable as a RuntimeException.
 * Uses HttpExceptionUtils for exception handling.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PureJavaCrc32C.java,<init>,org.apache.hadoop.util.PureJavaCrc32C:<init>(),41,43,"/**
 * Constructs a PureJavaCrc32C object and resets the CRC value.
 */",Create a new PureJavaCrc32 object.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,remove,org.apache.hadoop.util.IntrusiveCollection:remove(java.lang.Object),326,338,"/**
 * Removes the specified element from this list.
 * @param o element to remove; returns true if removed.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,toArray,org.apache.hadoop.util.IntrusiveCollection:toArray(),256,264,"/**
 * Converts the collection to an array of Objects.
 * @return An array containing all elements of the collection.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,retainAll,org.apache.hadoop.util.IntrusiveCollection:retainAll(java.util.Collection),372,384,"/**
 * Removes all elements not present in the specified collection.
 * @param collection Collection to check against.
 * @return True if the list was modified.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,clear,org.apache.hadoop.util.IntrusiveCollection:clear(),389,395,"/**
 * Removes all elements from the collection.
 */
",* Remove all elements.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,containsAll,org.apache.hadoop.util.IntrusiveCollection:containsAll(java.util.Collection),340,348,"/**
 * Checks if this set contains all elements of the given collection.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CleanerUtil.java,unmapHackImpl,org.apache.hadoop.util.CleanerUtil:unmapHackImpl(),89,160,"/**
 * Attempts to obtain an unmapper for direct ByteBuffers.
 * Supports Java 8/9+ unmapping via sun.misc.Unsafe or Cleaner.
 * @return BufferCleaner or error message if unmapping fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,setIncludesFile,org.apache.hadoop.util.HostsFileReader:setIncludesFile(java.lang.String),313,319,"/**
 * Sets the includes file for the HostDetails.
 * @param includesFile The path to the includes file.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,setExcludesFile,org.apache.hadoop.util.HostsFileReader:setExcludesFile(java.lang.String),321,328,"/**
 * Sets the excludes file for host details.
 * @param excludesFile The path to the excludes file.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,updateFileNames,"org.apache.hadoop.util.HostsFileReader:updateFileNames(java.lang.String,java.lang.String)",330,337,"/**
 * Updates the includes and excludes files for host details.
 * @param includesFile Path to the includes file.
 * @param excludesFile Path to the excludes file.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getExcludedHosts,org.apache.hadoop.util.HostsFileReader:getExcludedHosts(),266,269,"/**
 * Returns a set of excluded hostnames.
 * Fetches from the current HostDetails object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getGroupsForUserCommand,org.apache.hadoop.util.Shell:getGroupsForUserCommand(java.lang.String),229,239,"/**
 * Retrieves group names for a user, platform-dependent.
 * @param user The username to query.
 * @return An array of strings representing group names.
 */
","* A command to get a given user's groups list.
   * If the OS is not WINDOWS, the command will get the user's primary group
   * first and finally get the groups list which includes the primary group.
   * i.e. the user's primary group will be included twice.
   *
   * @param user user.
   * @return groups for user command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getGroupsIDForUserCommand,org.apache.hadoop.util.Shell:getGroupsIDForUserCommand(java.lang.String),251,261,"/**
 * Gets group IDs for a user, using OS-specific commands.
 * @param user The username to retrieve group IDs for.
 * @return An array of strings representing the command.
 */
","* A command to get a given user's group id list.
   * The command will get the user's primary group
   * first and finally get the groups list which includes the primary group.
   * i.e. the user's primary group will be included twice.
   * This command does not support Windows and will only return group names.
   *
   * @param user user.
   * @return groups id for user command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getGetPermissionCommand,org.apache.hadoop.util.Shell:getGetPermissionCommand(),279,282,"/**
 * Returns command to list files, platform-specific.
 * Returns array of strings representing the command.
 */
","* Return a command to get permission information.
   *
   * @return permission command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSetPermissionCommand,"org.apache.hadoop.util.Shell:getSetPermissionCommand(java.lang.String,boolean)",291,301,"/**
 * Constructs chmod command based on permission and recursion.
 * @param perm permission string
 * @param recursive whether to apply recursively
 * @return String array representing the chmod command
 */
","* Return a command to set permission.
   *
   * @param perm permission.
   * @param recursive recursive.
   * @return set permission command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSetOwnerCommand,org.apache.hadoop.util.Shell:getSetOwnerCommand(java.lang.String),326,330,"/**
 * Constructs chown command based on OS.
 * @param owner The user to set as owner.
 * @return String array representing the chown command.
 */
","* Return a command to set owner.
   *
   * @param owner owner.
   * @return set owner command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSymlinkCommand,"org.apache.hadoop.util.Shell:getSymlinkCommand(java.lang.String,java.lang.String)",339,343,"/**
 * Creates a symlink command based on the OS.
 * @param target The target path.
 * @param link The link path.
 * @return String array representing the command.
 */
","* Return a command to create symbolic links.
   *
   * @param target target.
   * @param link link.
   * @return symlink command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getReadlinkCommand,org.apache.hadoop.util.Shell:getReadlinkCommand(java.lang.String),351,355,"/**
 * Generates the readlink command based on the OS.
 * @param link The link to resolve.
 * @return String array representing the command.
 */
","* Return a command to read the target of the a symbolic link.
   *
   * @param link link.
   * @return read link command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSignalKillCommand,"org.apache.hadoop.util.Shell:getSignalKillCommand(int,java.lang.String)",373,394,"/**
 * Generates a signal kill command based on code and PID.
 * @param code signal code (0 for check alive)
 * @param pid process ID to kill
 * @return String array representing the command
 */
","* Return a command to send a signal to a given pid.
   *
   * @param code code.
   * @param pid pid.
   * @return signal kill command.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,appendScriptExtension,"org.apache.hadoop.util.Shell:appendScriptExtension(java.io.File,java.lang.String)",419,421,"/**
 * Appends "".script"" extension to the basename within the parent directory.
 * @param parent Parent directory as a File object.
 * @param basename Basename to append the extension to.
 * @return File object with the appended extension.
 */
","* Returns a File referencing a script with the given basename, inside the
   * given parent directory.  The file extension is inferred by platform:
   * <code>"".cmd""</code> on Windows, or <code>"".sh""</code> otherwise.
   *
   * @param parent File parent directory
   * @param basename String script file basename
   * @return File referencing the script in the directory",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,checkHadoopHome,org.apache.hadoop.util.Shell:checkHadoopHome(),483,493,"/**
 * Locates Hadoop home directory using system properties & env vars.
 * @return Hadoop home directory as a File object.
 * @throws FileNotFoundException if Hadoop home is not found.
 */
","*  Centralized logic to discover and validate the sanity of the Hadoop
   *  home directory.
   *
   *  This does a lot of work so it should only be called
   *  privately for initialization once per process.
   *
   * @return A directory that exists and via was specified on the command line
   * via <code>-Dhadoop.home.dir</code> or the <code>HADOOP_HOME</code>
   * environment variable.
   * @throws FileNotFoundException if the properties are absent or the specified
   * path is not a reference to a valid directory.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getHadoopHomeDir,org.apache.hadoop.util.Shell:getHadoopHomeDir(),620,627,"/**
 * Returns the Hadoop home directory.
 * Throws exception if HADOOP_HOME_DIR_FAILURE_CAUSE is set.
 */
","* Get the Hadoop home directory. If it is invalid,
   * throw an exception.
   * @return a path referring to hadoop home.
   * @throws FileNotFoundException if the directory doesn't exist.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getQualifiedBinInner,"org.apache.hadoop.util.Shell:getQualifiedBinInner(java.io.File,java.lang.String)",656,685,"/**
 * Returns the absolute path to a Hadoop executable.
 * @param hadoopHomeDir Hadoop home directory
 * @param executable executable file name
 * @return File object representing the executable
 */
","* Inner logic of {@link #getQualifiedBin(String)}, accessible
   * for tests.
   * @param hadoopHomeDir home directory (assumed to be valid)
   * @param executable executable
   * @return path to the binary
   * @throws FileNotFoundException if the executable was not found/valid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getWinUtilsFile,org.apache.hadoop.util.Shell:getWinUtilsFile(),800,808,"/**
 * Returns File object for WinUtils. Throws exception if failed.
 */
","* Get a file reference to winutils.
   * Always raises an exception if there isn't one
   * @return the file instance referring to the winutils bin.
   * @throws FileNotFoundException on any failure to locate that file.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,destroyAllShellProcesses,org.apache.hadoop.util.Shell:destroyAllShellProcesses(),1428,1437,"/**
 * Destroys all child shell processes and clears the CHILD_SHELLS map.
 */
","* Static method to destroy all running <code>Shell</code> processes.
   * Iterates through a map of all currently running <code>Shell</code>
   * processes and destroys them one by one. This method is thread safe",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,run,org.apache.hadoop.util.Shell$ShellTimeoutTimerTask:run(),1406,1420,"/**
 * Runs the process, checking for completion and destroying if needed.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownThreadsHelper.java,shutdownThread,org.apache.hadoop.util.ShutdownThreadsHelper:shutdownThread(java.lang.Thread),43,45,"/**
 * Shuts down a thread.
 * @param thread The thread to shut down.
 * @return True if shutdown successful, false otherwise.
 */
","* @param thread {@link Thread to be shutdown}
   * @return <tt>true</tt> if the thread is successfully interrupted,
   * <tt>false</tt> otherwise",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownThreadsHelper.java,shutdownExecutorService,org.apache.hadoop.util.ShutdownThreadsHelper:shutdownExecutorService(java.util.concurrent.ExecutorService),78,81,"/**
 * Shuts down an ExecutorService.
 * @param service ExecutorService to shutdown
 * @return True if shutdown initiated, false otherwise.
 */
","* shutdownExecutorService.
   *
   * @param service {@link ExecutorService to be shutdown}
   * @return <tt>true</tt> if the service is terminated,
   * <tt>false</tt> otherwise
   * @throws InterruptedException if the thread is interrupted.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addNewPhase,org.apache.hadoop.util.Progress:addNewPhase(),80,85,"/**
 * Creates a new phase, adds it to the list, and returns it.
 */",Adds a new phase. Caller needs to set progress weightage,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,org.apache.hadoop.util.Progress:addPhase(float),107,123,"/**
 * Adds a new phase with the given weightage.
 * @param weightage Weightage of the new phase.
 * @return The newly created Progress phase object.
 */
","* Adds a node with a specified progress weightage to the tree.
   *
   * @param weightage weightage.
   * @return Progress.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,getInternal,org.apache.hadoop.util.Progress:getInternal(),244,269,"/**
 * Calculates the internal progress based on phases and weightages.
 * Returns the calculated progress value.
 */",Computes progress in this node.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,toString,org.apache.hadoop.util.Progress:toString(java.lang.StringBuilder),282,288,"/**
 * Appends status and current phase to the StringBuilder.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,complete,org.apache.hadoop.util.Progress:complete(),170,184,"/**
 * Completes the current phase and starts the parent's next phase.
 */","Completes this node, moving the parent node to its next child.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getStringData,org.apache.hadoop.util.curator.ZKCuratorManager:getStringData(java.lang.String),260,266,"/**
 * Reads data from a file path and returns it as a String.
 * @param path file path to read
 * @return String data or null if file is empty/error.
 */
","* Get the data in a ZNode.
   * @param path Path of the ZNode.
   * @return The data in the ZNode.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getStringData,"org.apache.hadoop.util.curator.ZKCuratorManager:getStringData(java.lang.String,org.apache.zookeeper.data.Stat)",275,281,"/**
 * Reads data from a file path as a String, UTF-8 encoded.
 * @param path file path to read
 * @param stat file stat object
 * @return String data or null if read fails
 */
","* Get the data in a ZNode.
   * @param path Path of the ZNode.
   * @param stat Output statistics of the ZNode.
   * @return The data in the ZNode.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,setData,"org.apache.hadoop.util.curator.ZKCuratorManager:setData(java.lang.String,java.lang.String,int)",301,304,"/**
 * Sets data to the specified path with the given version.
 * @param path Path to store data.
 * @param data Data to be stored as a String.
 * @param version Version of the data.
 */
","* Set data into a ZNode.
   * @param path Path of the ZNode.
   * @param data Data to set as String.
   * @param version Version of the data to store.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,create,"org.apache.hadoop.util.curator.ZKCuratorManager:create(java.lang.String,java.util.List)",343,353,"/**
 * Creates a persistent znode with specified ACL.
 * @param path znode path.
 * @param zkAcl ACLs to apply. Returns true if created.
 */
","* Create a ZNode.
   * @param path Path of the ZNode.
   * @param zkAcl ACL for the node.
   * @return If the ZNode was created.
   * @throws Exception If it cannot contact Zookeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,delete,org.apache.hadoop.util.curator.ZKCuratorManager:delete(java.lang.String),392,398,"/**
 * Deletes a node and its children from ZooKeeper.
 * @param path Node path to delete. Returns true on success.
 */
","* Delete a ZNode.
   * @param path Path of the ZNode.
   * @return If the znode was deleted.
   * @throws Exception If it cannot contact ZooKeeper.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,safeCreate,"org.apache.hadoop.util.curator.ZKCuratorManager:safeCreate(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode,java.util.List,java.lang.String)",410,419,"/**
 * Creates a node if it doesn't exist, using a safe transaction.
 * @param path Node path.
 * @param data Node data.
 * @param acl Access control list.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,safeDelete,"org.apache.hadoop.util.curator.ZKCuratorManager:safeDelete(java.lang.String,java.util.List,java.lang.String)",429,437,"/**
 * Deletes a path if it exists, using a safe transaction.
 * @param path Path to delete.
 * @param fencingACL ACLs for fencing.
 * @param fencingNodePath Fencing node path.
 */
","* Deletes the path. Checks for existence of path as well.
   *
   * @param path Path to be deleted.
   * @param fencingNodePath fencingNodePath.
   * @param fencingACL fencingACL.
   * @throws Exception if any problem occurs while performing deletion.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,safeSetData,"org.apache.hadoop.util.curator.ZKCuratorManager:safeSetData(java.lang.String,byte[],int,java.util.List,java.lang.String)",439,446,"/**
 * Sets data safely using a transaction with ACL and fencing.
 * @param path data path, data, version, ACLs, and fencing path.
 * @throws Exception if an error occurs during the transaction.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,addClass,"org.apache.hadoop.util.ProgramDriver:addClass(java.lang.String,java.lang.Class,java.lang.String)",101,104,"/**
 * Adds a program with its class and description.
 * @param name Program name.
 * @param mainClass Main class of the program.
 * @param description Program description.
 */
","* This is the method that adds the classed to the repository.
   * @param name The name of the string you want the class instance to be called with
   * @param mainClass The class that you want to add to the repository
   * @param description The description of the class
   * @throws NoSuchMethodException when a particular method cannot be found.
   * @throws SecurityException security manager to indicate a security violation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,impl,"org.apache.hadoop.util.dynamic.DynConstructors$Builder:impl(java.lang.String,java.lang.Class[])",134,149,"/**
 * Attempts to load an implementation class by name.
 * @param className Class name to load.
 * @param types Constructor parameter types.
 * @return This builder instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,hiddenImpl,org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.Class[]),166,169,"/**
 * Sets hidden implementation types for the base class.
 * @param types Array of classes to mark as hidden implementations.
 * @return This builder instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynConstructors$Builder:hiddenImpl(java.lang.String,java.lang.Class[])",171,186,"/**
 * Tries to find and set a hidden implementation using the given class.
 * @param className Class name of the hidden implementation.
 * @param types Class types for the constructor.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,ctorImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:ctorImpl(java.lang.Class,java.lang.Class[])",348,362,"/**
 * Attempts to find and set a constructor implementation.
 * @param targetClass The class to construct.
 * @param argClasses Constructor argument types.
 * @return The Builder instance.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,newInstanceChecked,org.apache.hadoop.util.dynamic.DynConstructors$Ctor:newInstanceChecked(java.lang.Object[]),56,66,"/**
 * Creates a new instance of C using the constructor.
 * @param args Arguments to pass to the constructor.
 * @throws Exception if instantiation or access fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeChecked,"org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invokeChecked(java.lang.Object,java.lang.Object[])",73,89,"/**
 * Invokes a method on a target object with arguments.
 * @param target object on which to invoke the method
 * @param args arguments to pass to the method
 * @return Result of the method invocation
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.Class,java.lang.String,java.lang.Class[])",320,333,"/**
 * Attempts to set the method implementation.
 * @param targetClass Class containing the method.
 * @param methodName Method name.
 * @param argClasses Method argument classes.
 * @return Builder instance.
 */
","* Checks for a method implementation.
     * @param targetClass the class to check for an implementation
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,<init>,"org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod$1:<init>(java.lang.reflect.Method,java.lang.String)",66,71,"/**
 * Initializes an UnboundMethod with a method and name.
 * @param method The method object.
 * @param name The name of the method.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,<init>,"org.apache.hadoop.util.dynamic.DynConstructors$Ctor:<init>(java.lang.reflect.Constructor,java.lang.Class)",46,50,"/**
 * Initializes a Constructor instance.
 * @param constructor The constructor to be used.
 * @param constructed The class being constructed.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.Class,java.lang.String,java.lang.Class[])",423,438,"/**
 * Finds and makes accessible a hidden method on a class.
 * @param targetClass Class to search for the method.
 * @param methodName Name of the method to find.
 * @param argClasses Method argument types.
 * @return Builder instance.
 */
","* Checks for a method implementation.
     * @param targetClass the class to check for an implementation
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,noop,org.apache.hadoop.util.dynamic.BindingUtils:noop(java.lang.String),158,160,"/**
 * Creates a no-op unbound method with the given name.
 * @param name The name of the no-op method.
 * @return A DynMethods.UnboundMethod representing the no-op.
 */
","* Create a no-op method.
   *
   * @param name method name
   *
   * @return a no-op method.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,implemented,org.apache.hadoop.util.dynamic.BindingUtils:implemented(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod[]),169,176,"/**
 * Checks if all provided methods are not noops.
 * @param methods Array of unbound methods to check.
 * @return True if all methods are implemented, false otherwise.
 */
","* Given a sequence of methods, verify that they are all available.
   *
   * @param methods methods
   *
   * @return true if they are all implemented",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,available,org.apache.hadoop.util.dynamic.BindingUtils:available(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod),195,197,"/**
 * Checks if a method is available (not a noop).
 * @param method The method to check.
 * @return True if the method is available, false otherwise.
 */
","* Is a method available?
   * @param method method to probe
   * @return true iff the method is found and loaded.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,bind,org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:bind(java.lang.Object),118,125,"/**
 * Binds the method to a receiver instance.
 * @param receiver The object to bind the method to.
 * @return A BoundMethod instance.
 */
","* Returns this method as a BoundMethod for the given receiver.
     * @param receiver an Object to receive the method invocation
     * @return a {@link BoundMethod} for this method and the receiver
     * @throws IllegalStateException if the method is static
     * @throws IllegalArgumentException if the receiver's class is incompatible",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,asStatic,org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:asStatic(),146,149,"/**
 * Returns a static wrapper for this dynamic method.
 * Checks if the method is already static.
 */
","* Returns this method as a StaticMethod.
     * @return a {@link StaticMethod} for this method
     * @throws IllegalStateException if the method is not static",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ClassUtil.java,findContainingJar,org.apache.hadoop.util.ClassUtil:findContainingJar(java.lang.Class),38,40,"/**
 * Finds the JAR file containing the given class.
 * @param clazz The class to search for.
 * @return The JAR file name or null if not found.
 */
","* Find a jar that contains a class of the same name, if any.
   * It will return a jar file, even if that is not the first thing
   * on the class path that has a class with the same name.
   * 
   * @param clazz the class to find.
   * @return a jar file that contains the class, or null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ClassUtil.java,findClassLocation,org.apache.hadoop.util.ClassUtil:findClassLocation(java.lang.Class),48,50,"/**
 * Finds the location of a class on the file system.
 * @param clazz The class to locate.
 * @return The class's file path or null if not found.
 */
","* Find the absolute location of the class.
   *
   * @param clazz the class to find.
   * @return the class file with absolute location, or null.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,makeRpcRequestHeader,"org.apache.hadoop.util.ProtoUtil:makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto,int,int,byte[],org.apache.hadoop.ipc.AlignmentContext)",178,212,"/**
 * Creates an RpcRequestHeaderProto with given parameters.
 * @param rpcKind RPC kind
 * @param operation Operation proto
 * @return RpcRequestHeaderProto object
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,getHeader,org.apache.hadoop.util.DataChecksum:getHeader(),226,235,"/**
 * Creates and returns the header as a byte array.
 * Contains type ID and bytes per checksum.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,mapByteToChecksumType,org.apache.hadoop.util.DataChecksum:mapByteToChecksumType(int),204,212,"/**
 * Converts an int to a ChecksumType.
 * @param type int representing the checksum type
 * @throws InvalidChecksumSizeException if type is invalid
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,writeValue,"org.apache.hadoop.util.DataChecksum:writeValue(java.io.DataOutputStream,boolean)",246,263,"/**
 * Writes checksum value to output stream. Resets state if reset is true.
 * @param out Output stream to write to.
 * @param reset Reset state after writing.
 * @return Size of the checksum type.
 */
","* Writes the current checksum to the stream.
   * If <i>reset</i> is true, then resets the checksum.
   *
   * @param out out.
   * @param reset reset.
   * @return number of bytes written. Will be equal to getChecksumSize();
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,writeValue,"org.apache.hadoop.util.DataChecksum:writeValue(byte[],int,boolean)",275,296,"/**
 * Writes checksum to buffer, resets state if specified.
 * @param buf buffer to write to
 * @param offset offset in buffer
 * @param reset resets state after writing
 * @return number of bytes written
 */
","* Writes the current checksum to a buffer.
    * If <i>reset</i> is true, then resets the checksum.
    *
    * @param buf buf.
    * @param offset offset.
    * @param reset reset.
    * @return number of bytes written. Will be equal to getChecksumSize();
    * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RateLimitingFactory.java,create,org.apache.hadoop.util.RateLimitingFactory:create(int),95,100,"/**
 * Creates a RateLimiting instance.
 * @param capacity The rate limit capacity, 0 for unlimited.
 * @return A RateLimiting object.
 */
","* Create an instance.
   * If the rate is 0; return the unlimited rate.
   * @param capacity capacity in permits/second.
   * @return limiter restricted to the given capacity.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SignalLogger.java,register,org.apache.hadoop.util.SignalLogger:register(org.slf4j.Logger),71,92,"/**
 * Registers UNIX signal handlers for TERM, HUP, and INT signals.
 * @param log Logger to record registration status and errors.
 */
","* Register some signal handlers.
   *
   * @param log The log4j logfile to use in the signal handlers.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,parseItem,"org.apache.hadoop.util.ComparableVersion:parseItem(boolean,java.lang.String)",455,458,"/**
 * Parses a string into an Item object based on the isDigit flag.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,<init>,org.apache.hadoop.util.LightWeightGSet:<init>(int),90,97,"/**
 * Constructs a LightWeightGSet with a recommended initial length.
 * @param recommended_length The desired initial capacity.
 */
",* @param recommended_length Recommended size of the internal array.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,get,org.apache.hadoop.util.LightWeightGSet:get(java.lang.Object),126,142,"/**
 * Retrieves the element associated with the given key.
 * @param key the key to search for
 * @return the element or null if not found
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,clear,org.apache.hadoop.util.LightWeightGSet$Values:clear(),256,259,"/**
 * Clears the LightWeightGSet, removing all elements.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,toString,org.apache.hadoop.util.SemaphoredDelegatingExecutor:toString(),200,209,"/**
 * Returns a string representation of this SemaphoredDelegatingExecutor.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readFileToSet,"org.apache.hadoop.util.HostsFileReader:readFileToSet(java.lang.String,java.lang.String,java.util.Set)",77,82,"/**
 * Reads lines from a file and adds them to the provided set.
 * @param type file type, filename file path, set to store lines
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readXmlFileToMapWithFileInputStream,"org.apache.hadoop.util.HostsFileReader:readXmlFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map)",146,183,"/**
 * Reads host data from XML file into a map.
 * @param type host type
 * @param filename XML filename
 * @param fileInputStream input stream for XML file
 * @param map map to store host-timeout pairs
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getHosts,org.apache.hadoop.util.HostsFileReader:getHosts(),261,264,"/**
 * Retrieves the included hosts from the current host details.
 * @return A set of hostnames.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getHostDetails,"org.apache.hadoop.util.HostsFileReader:getHostDetails(java.util.Set,java.util.Set)",278,283,"/**
 * Populates includes/excludes sets with host details.
 * @param includes Set to add included hosts to.
 * @param excludes Set to add excluded hosts to.
 */
","* Retrieve an atomic view of the included and excluded hosts.
   *
   * @param includes set to populate with included hosts
   * @param excludes set to populate with excluded hosts
   * @deprecated use {@link #getHostDetails() instead}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,getHostDetails,"org.apache.hadoop.util.HostsFileReader:getHostDetails(java.util.Set,java.util.Map)",292,298,"/**
 * Populates include/exclude host sets from current host details.
 */
","* Retrieve an atomic view of the included and excluded hosts.
   *
   * @param includeHosts set to populate with included hosts
   * @param excludeHosts map to populate with excluded hosts
   * @deprecated use {@link #getHostDetails() instead}",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/JenkinsHash.java,hash,"org.apache.hadoop.util.hash.JenkinsHash:hash(byte[],int,int)",86,245,"/**
 * Computes a hash value for a byte array.
 * @param key byte array to hash
 * @param nbytes length of the key
 * @param initval initial hash value
 */
","* taken from  hashlittle() -- hash a variable-length key into a 32-bit value
   * 
   * @param key the key (the unaligned variable-length array of bytes)
   * @param nbytes number of bytes to include in hash
   * @param initval can be any integer value
   * @return a 32-bit value.  Every bit of the key affects every bit of the
   * return value.  Two keys differing by one or two bits will have totally
   * different hash values.
   * 
   * <p>The best hash table sizes are powers of 2.  There is no need to do mod
   * a prime (mod is sooo slow!).  If you need less than 32 bits, use a bitmask.
   * For example, if you need only 10 bits, do
   * <code>h = (h &amp; hashmask(10));</code>
   * In which case, the hash table should have hashsize(10) elements.
   * 
   * <p>If you are hashing n strings byte[][] k, do it like this:
   * for (int i = 0, h = 0; i &lt; n; ++i) h = hash( k[i], h);
   * 
   * <p>By Bob Jenkins, 2006.  bob_jenkins@burtleburtle.net.  You may use this
   * code any way you wish, private, educational, or commercial.  It's free.
   * 
   * <p>Use for hash table lookup, or anything where one collision in 2^^32 is
   * acceptable.  Do NOT use for cryptographic purposes.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/Hash.java,getInstance,org.apache.hadoop.util.hash.Hash:getInstance(int),75,84,"/**
 * Returns a Hash instance based on the specified type.
 * @param type Hash type (JENKINS_HASH, MURMUR_HASH)
 * @return Hash instance or null if type is invalid.
 */
","* Get a singleton instance of hash function of a given type.
   * @param type predefined hash type
   * @return hash function instance, or null if type is invalid",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/MurmurHash.java,hash,"org.apache.hadoop.util.hash.MurmurHash:hash(byte[],int,int)",40,43,"/**
 * Calculates hash value for data.
 * @param data byte array to hash
 * @param length length of data to hash
 * @param seed initial hash seed
 * @return hash value
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,<init>,org.apache.hadoop.util.bloom.CountingBloomFilter:<init>(),84,84,"/**
 * Constructs a new CountingBloomFilter with default parameters.
 */",Default constructor - use with readFields,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,<init>,org.apache.hadoop.util.bloom.BloomFilter:<init>(),99,101,"/**
 * Default constructor for the BloomFilter class.
 */",Default constructor - use with readFields,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,<init>,org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>(),113,113,"/**
 * Constructs a new, empty DynamicBloomFilter.
 */",* Zero-args constructor for the serialization.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,and,org.apache.hadoop.util.bloom.CountingBloomFilter:and(org.apache.hadoop.util.bloom.Filter),162,176,"/**
 * Performs a bitwise AND operation with another CountingBloomFilter.
 * @param filter The filter to AND with. Must be compatible.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,or,org.apache.hadoop.util.bloom.CountingBloomFilter:or(org.apache.hadoop.util.bloom.Filter),246,261,"/**
 * ORs the given filter with this filter.
 * @param filter The filter to OR with. Must be a CountingBloomFilter.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,write,org.apache.hadoop.util.bloom.CountingBloomFilter:write(java.io.DataOutput),292,299,"/**
 * Writes the vector's buckets to the DataOutput stream.
 * Writes vector buckets as long values to the output stream.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,and,org.apache.hadoop.util.bloom.DynamicBloomFilter:and(org.apache.hadoop.util.bloom.Filter),155,173,"/**
 * ANDs this filter with the given DynamicBloomFilter.
 * @param filter The filter to AND with. Must be compatible.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,not,org.apache.hadoop.util.bloom.DynamicBloomFilter:not(),190,195,"/**
 * Applies the 'not' operation to each row of the matrix.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,or,org.apache.hadoop.util.bloom.DynamicBloomFilter:or(org.apache.hadoop.util.bloom.Filter),197,214,"/**
 * ORs the given filter with this DynamicBloomFilter.
 * @param filter The filter to OR with, must be a DynamicBloomFilter.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,xor,org.apache.hadoop.util.bloom.DynamicBloomFilter:xor(org.apache.hadoop.util.bloom.Filter),216,233,"/**
 * XORs this filter with another DynamicBloomFilter.
 * @param filter The filter to XOR with. Must be compatible.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,write,org.apache.hadoop.util.bloom.BloomFilter:write(java.io.DataOutput),199,216,"/**
 * Writes the bit vector to a DataOutput.
 * Writes the bit vector as bytes to the output stream.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/HashFunction.java,hash,org.apache.hadoop.util.bloom.HashFunction:hash(org.apache.hadoop.util.bloom.Key),108,122,"/**
 * Computes a hash array from a key.
 * @param k The key to hash.
 * @return An array of integers representing the hash values.
 */
","* Hashes a specified key into several integers.
   * @param k The specified key.
   * @return The array of hashed values.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,compareTo,org.apache.hadoop.util.bloom.Key:compareTo(org.apache.hadoop.util.bloom.Key),172,183,"/**
 * Compares this key to another key based on bytes and weight.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,getWeight,org.apache.hadoop.util.bloom.RetouchedBloomFilter:getWeight(java.util.List),381,387,"/**
 * Calculates the total weight from a list of Key objects.
 * @param keyList List of Key objects, each having a weight.
 * @return The sum of the weights of all keys.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,formatMessage,"org.apache.hadoop.util.JvmPauseMonitor:formatMessage(long,java.util.Map,java.util.Map)",118,143,"/**
 * Formats a message detailing GC activity after a sleep period.
 * @param extraSleepTime Sleep time in ms
 * @param gcTimesAfterSleep GC times after sleep
 * @param gcTimesBeforeSleep GC times before sleep
 * @return Formatted message string.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/AutoCloseableLock.java,<init>,org.apache.hadoop.util.AutoCloseableLock:<init>(),38,40,"/**
 * Constructs an AutoCloseableLock using a ReentrantLock.
 */","* Creates an instance of {@code AutoCloseableLock}, initializes
   * the underlying lock instance with a new {@code ReentrantLock}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/AutoCloseableLock.java,close,org.apache.hadoop.util.AutoCloseableLock:close(),94,97,"/**
 * Closes the resource by releasing associated resources.
 */","* Attempts to release the lock by making a call to {@code release()}.
   *
   * This is to implement {@code close()} method from {@code AutoCloseable}
   * interface. This allows users to user a try-with-resource syntax, where
   * the lock can be automatically released.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,isNull,org.apache.hadoop.util.ComparableVersion$StringItem:isNull(),208,211,"/**
 * Checks if the value is null based on version comparison.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,compareTo,org.apache.hadoop.util.ComparableVersion$StringItem:compareTo(org.apache.hadoop.util.ComparableVersion$Item),232,253,"/**
 * Compares this Item with another Item based on type.
 * @param item The Item to compare to.
 * @return -1, 0, or 1 based on the comparison result.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,printStack,"org.apache.hadoop.util.FindClass:printStack(java.lang.Throwable,java.lang.String,java.lang.Object[])",234,237,"/**
 * Prints an error message and stack trace to the error stream.
 * @param e The throwable to print the stack trace for.
 */","* print a stack trace with text
   * @param e the exception to print
   * @param text text to print",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,explainResult,"org.apache.hadoop.util.FindClass:explainResult(int,java.lang.String)",368,370,"/**
 * Logs an error message with an error code and text.
 * @param errorcode Error code for logging.
 * @param text Error message text.
 */
","* Explain an error code as part of the usage
   * @param errorcode error code returned
   * @param text error text",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,loadedClass,"org.apache.hadoop.util.FindClass:loadedClass(java.lang.String,java.lang.Class)",266,271,"/**
 * Logs class loading information, including source URL.
 * @param name Class name.
 * @param clazz The loaded Class object.
 */
","* Log that a class has been loaded, and where from.
   * @param name classname
   * @param clazz class",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,calculateGCTimePercentageWithinObservedInterval,org.apache.hadoop.util.GcTimeMonitor:calculateGCTimePercentageWithinObservedInterval(),186,224,"/**
 * Calculates GC time percentage within the observed interval.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,getLatestGcData,org.apache.hadoop.util.GcTimeMonitor:getLatestGcData(),182,184,"/**
 * Returns a copy of the latest GC data.
 * @return A clone of the current GC data.
 */
","* Returns a copy of the most recent data measured by this monitor.
   * @return a copy of the most recent data measured by this monitor",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PureJavaCrc32.java,<init>,org.apache.hadoop.util.PureJavaCrc32:<init>(),45,47,"/**
 * Constructs a PureJavaCrc32 object and initializes it.
 */",Create a new PureJavaCrc32 object.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,executeShutdown,org.apache.hadoop.util.ShutdownHookManager:executeShutdown(),117,136,"/**
 * Executes shutdown hooks, tracking timeouts.
 * @return Number of timeout exceptions during shutdown.
 */
","* Execute the shutdown.
   * This is exposed purely for testing: do not invoke it.
   * @return the number of shutdown hooks which timed out.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,put,org.apache.hadoop.util.PriorityQueue:put(java.lang.Object),61,65,"/**
* Adds an element to the heap and maintains heap property.
* @param element The element to add to the heap.
*/
","* Adds an Object to a PriorityQueue in log(size) time.
   * If one tries to add more objects than maxSize from initialize
   * a RuntimeException (ArrayIndexOutOfBound) is thrown.
   * @param element element.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,pop,org.apache.hadoop.util.PriorityQueue:pop(),104,114,"/**
 * Removes and returns the root element of the heap.
 * Returns null if the heap is empty.
 */
","* Removes and returns the least element of the PriorityQueue in log(size)
      time.
   * @return T Generics Type T.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,adjustTop,org.apache.hadoop.util.PriorityQueue:adjustTop(),123,125,"/**
* Moves the root element to its correct position in the heap.
*/
","Should be called when the Object at top changes values.  Still log(n)
   * worst case, but it's at least twice as fast to <pre>
   *  { pq.top().change(); pq.adjustTop(); }
   * </pre> instead of <pre>
   *  { o = pq.pop(); o.change(); pq.push(o); }
   * </pre>",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,addAll,"org.apache.hadoop.util.Sets:addAll(java.util.TreeSet,java.lang.Iterable)",161,171,"/**
 * Adds all elements from the iterable to the TreeSet.
 * @param addTo TreeSet to add elements to.
 * @param elementsToAdd Iterable of elements to add.
 * @return True if the TreeSet was modified.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSet,org.apache.hadoop.util.Sets:newHashSet(java.util.Iterator),191,195,"/**
 * Creates a HashSet from an iterator of elements.
 * @param elements Iterator providing elements for the set.
 */
","* Creates a <i>mutable</i> {@code HashSet} instance containing the given
   * elements. A very thin convenience for creating an empty set and then
   * calling Iterators#addAll.
   *
   * <p><b>Note:</b> if mutability is not required and the elements are
   * non-null, use ImmutableSet#copyOf(Iterator) instead.</p>
   *
   * <p><b>Note:</b> if {@code E} is an {@link Enum} type, you should create
   * an {@link EnumSet} instead.</p>
   *
   * <p>Overall, this method is not very useful and will likely be deprecated
   * in the future.</p>
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return a new, empty thread-safe {@code Set}.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSetWithExpectedSize,org.apache.hadoop.util.Sets:newHashSetWithExpectedSize(int),213,215,"/**
 * Creates a new HashSet with the specified expected size.
 * @param expectedSize The initial capacity for the HashSet.
 */
","* Returns a new hash set using the smallest initial table size that can hold
   * {@code expectedSize} elements without resizing. Note that this is not what
   * {@link HashSet#HashSet(int)} does, but it is what most users want and
   * expect it to do.
   *
   * <p>This behavior can't be broadly guaranteed, but has been tested with
   * OpenJDK 1.7 and 1.8.</p>
   *
   * @param expectedSize the number of elements you expect to add to the
   *     returned set
   * @param <E> Generics Type E.
   * @return a new, empty hash set with enough capacity to hold
   *     {@code expectedSize} elements without resizing
   * @throws IllegalArgumentException if {@code expectedSize} is negative",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SequentialNumber.java,skipTo,org.apache.hadoop.util.SequentialNumber:skipTo(long),78,91,"/**
 * Atomically sets the value to newValue.
 * Throws IllegalStateException if newValue < currentValue.
 */
","* Skip to the new value.
   * @param newValue newValue.
   * @throws IllegalStateException
   *         Cannot skip to less than the current value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,printUsage,org.apache.hadoop.util.ProgramDriver:printUsage(java.util.Map),85,91,"/**
 * Prints a list of valid program names and their descriptions.
 * @param programs Map of program names to their descriptions.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/CommandShell.java,run,org.apache.hadoop.tools.CommandShell:run(java.lang.String[]),63,84,"/**
 * Executes the application logic based on command-line args.
 * Returns exit code; 0 for success, non-zero for failure.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,<init>,"org.apache.hadoop.tools.TableListing$Column:<init>(java.lang.String,org.apache.hadoop.tools.TableListing$Justification,boolean)",52,58,"/**
 * Constructs a new Column with title, justification, and wrap settings.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,build,org.apache.hadoop.tools.TableListing$Builder:build(),194,197,"/**
 * Creates a TableListing object with the configured columns.
 * @return A TableListing instance.
 */
","* Create a new TableListing.
     *
     * @return TableListing.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$7:getDefault(double),522,522,"/**
 * Returns a default value if the input value is invalid.
 * @param value The input value to check.
 * @return A default double value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$3:getDefault(double),522,522,"/**
 * Returns a default value if the input value is invalid.
 * @param value The input value to check.
 * @return A default value if the input is invalid.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,isDeprecated,org.apache.hadoop.conf.Configuration:isDeprecated(java.lang.String),670,672,"/**
 * Checks if a key is deprecated.
 * @param key The key to check.
 * @return True if the key is deprecated, false otherwise.
 */
","* checks whether the given <code>key</code> is deprecated.
   * 
   * @param key the parameter which is to be checked for deprecation
   * @return <code>true</code> if the key is deprecated and 
   *         <code>false</code> otherwise.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getDeprecatedKeyInfo,org.apache.hadoop.conf.Configuration:getDeprecatedKeyInfo(java.lang.String),678,680,"/**
 * Retrieves deprecated key info from the deprecation map.
 * @param key The key to look up.
 * @return DeprecatedKeyInfo object or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,dumpDeprecatedKeys,org.apache.hadoop.conf.Configuration:dumpDeprecatedKeys(),4009,4019,"/**
 * Prints deprecated keys and their replacements to the console.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,hasWarnedDeprecation,org.apache.hadoop.conf.Configuration:hasWarnedDeprecation(java.lang.String),4027,4035,"/**
 * Checks if a deprecation warning has been triggered for the given name.
 */","* Returns whether or not a deprecated name has been warned. If the name is not
   * deprecated then always return false
   * @param name proprties.
   * @return true if name is a warned deprecation.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getDeprecatedKey,org.apache.hadoop.conf.Configuration:getDeprecatedKey(java.lang.String),674,676,"/**
 * Retrieves the reverse-deprecated key for a given key.
 * @param key The key to look up.
 * @return The reverse-deprecated key, or null if not found.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,reloadExistingConfigurations,org.apache.hadoop.conf.Configuration:reloadExistingConfigurations(),880,888,"/**
 * Reloads configurations from the REGISTRY.
 * Logs debug message if enabled.
 */
",* Reload existing configuration instances.,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDefaultResource,org.apache.hadoop.conf.Configuration:addDefaultResource(java.lang.String),895,904,"/**
 * Adds a resource to the default resources list and reloads configs.
 * @param name The name of the resource to add.
 */
","* Add a default resource. Resources are loaded in the order of the resources 
   * added.
   * @param name file name. File should be present in the classpath.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,boolean)",257,259,"/**
 * Constructs a Resource with a resource and a flag.
 * @param resource The resource object.
 * @param useRestrictedParser Flag to use restricted parser.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDurationHelper,"org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",1951,1969,"/**
 * Converts a time duration string to a value in the return unit.
 * @param name name of the duration
 * @param vStr duration string to parse
 * @param defaultUnit default time unit
 * @param returnUnit desired time unit for the result
 * @return Converted time duration value.
 */
","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d).
   *
   * @param name Property name
   * @param vStr The string value with time unit suffix to be converted.
   * @param defaultUnit Unit to convert the stored property, if it exists.
   * @param returnUnit Unit for the returned value.
   * @return time duration in given time unit.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,parse,"org.apache.hadoop.conf.Configuration:parse(java.net.URL,boolean)",3045,3063,"/**
 * Parses an XML stream from a URL.
 * @param url URL to parse; returns null if null.
 * @param restricted parsing restriction flag
 * @return XMLStreamReader or null if URL is null
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleInclude,org.apache.hadoop.conf.Configuration$Parser:handleInclude(),3296,3372,"/**
 * Handles processing of xi:include directives, parsing included resources.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadProperty,"org.apache.hadoop.conf.Configuration:loadProperty(java.util.Properties,java.lang.String,java.lang.String,java.lang.String,boolean,java.lang.String[])",3546,3568,"/**
 * Loads a property into the given properties object, handling overrides.
 * @param properties Properties object to load into.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,toString,org.apache.hadoop.conf.Configuration:toString(),3901,3913,"/**
 * Returns a string representation of the configuration.
 * Uses internal toString method to format resources.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getAllPropertiesByTags,org.apache.hadoop.conf.Configuration:getAllPropertiesByTags(java.util.List),4056,4062,"/**
 * Retrieves all properties associated with the given tags.
 * @param tagList List of tags to retrieve properties for.
 * @return Properties object containing all properties.
 */
","* Get all properties belonging to list of input tags. Calls
   * getAllPropertiesByTag internally.
   * @param tagList list of input tags
   * @return Properties with matching tags",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$6:getDefault(double),522,522,"/**
* Returns a default value, falling back to the provided value.
*/",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$2:getDefault(double),522,522,"/**
 * Returns a default value if the given value is invalid.
 * @param value The input value to check.
 * @return A default value if the input is invalid.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigRedactor.java,redact,"org.apache.hadoop.conf.ConfigRedactor:redact(java.lang.String,java.lang.String)",65,70,"/**
 * Redacts a value if its key is sensitive.
 * @param key The key to check for sensitivity.
 * @param value The value to redact if needed.
 * @return Redacted value or original value.
 */
","* Given a key / value pair, decides whether or not to redact and returns
   * either the original value or text indicating it has been redacted.
   *
   * @param key param key.
   * @param value param value, will return if conditions permit.
   * @return Original value, or text indicating it has been redacted",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigRedactor.java,redactXml,"org.apache.hadoop.conf.ConfigRedactor:redactXml(java.lang.String,java.lang.String)",97,102,"/**
 * Redacts XML value if key is sensitive.
 * @param key XML key to check.
 * @param value XML value to redact.
 * @return Redacted value or original value.
 */
","* Given a key / value pair, decides whether or not to redact and returns
   * either the original value or text indicating it has been redacted.
   *
   * @param key param key.
   * @param value param value, will return if conditions permit.
   * @return Original value, or text indicating it has been redacted",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,getReconfigurationTaskStatus,org.apache.hadoop.conf.ReconfigurableBase:getReconfigurationTaskStatus(),189,196,"/**
 * Gets the status of the reconfiguration task.
 * Returns ReconfigurationTaskStatus object based on thread state.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$4:getDefault(double),522,522,"/**
 * Returns a default value if the given value is invalid.
 * @param value The input value to check.
 * @return A default value if the input is invalid.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,startReconfigurationTask,org.apache.hadoop.conf.ReconfigurableBase:startReconfigurationTask(),169,187,"/**
 * Starts a reconfiguration task, throwing IOException if server is stopped or a task is already running.
 */","* Start a reconfiguration task to reload configuration in background.
   * @throws IOException raised on errors performing I/O.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationContext:<init>(org.apache.hadoop.conf.Configuration$DeprecationContext,org.apache.hadoop.conf.Configuration$DeprecationDelta[])",487,517,"/**
 * Constructs a DeprecationContext, copying from another and applying deltas.
 * @param other The context to copy from.
 * @param deltas DeprecationDelta objects to apply.
 */
","* Create a new DeprecationContext by copying a previous DeprecationContext
     * and adding some deltas.
     *
     * @param other   The previous deprecation context to copy, or null to start
     *                from nothing.
     * @param deltas  The deltas to apply.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationException.java,<init>,"org.apache.hadoop.conf.ReconfigurationException:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.Throwable)",67,74,"/**
 * Constructs a ReconfigurationException with property details and cause.
 */
","* Create a new instance of {@link ReconfigurationException}.
   * @param property property name.
   * @param newVal new value.
   * @param oldVal old value.
   * @param cause original exception.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationException.java,<init>,"org.apache.hadoop.conf.ReconfigurationException:<init>(java.lang.String,java.lang.String,java.lang.String)",82,88,"/**
 * Constructs a ReconfigurationException with property details.
 * @param property Property name. @param newVal New value. @param oldVal Old value.
 */
","* Create a new instance of {@link ReconfigurationException}.
   * @param property property name.
   * @param newVal new value.
   * @param oldVal old value.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleEndProperty,org.apache.hadoop.conf.Configuration$Parser:handleEndProperty(),3415,3446,"/**
 * Handles the end of a property, adding it to results or deprecations.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getWarningMessage,org.apache.hadoop.conf.Configuration$DeprecatedKeyInfo:getWarningMessage(java.lang.String),382,384,"/**
 * Retrieves a warning message by key.
 * Delegates to the overloaded method.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,iterator,org.apache.hadoop.conf.Configuration$IntegerRanges:iterator(),2283,2286,"/**
 * Returns an iterator for the range numbers.
 * Creates a RangeNumberIterator using the ranges.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration$IntegerRanges:<init>(java.lang.String),2195,2217,"/**
 * Parses a comma-separated string of integer ranges and adds them.
 * @param newValue String containing comma-separated integer ranges.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageSize.java,parse,org.apache.hadoop.conf.StorageSize:parse(java.lang.String),50,96,"/**
 * Parses a storage size string (e.g., ""1000MB"") into a StorageSize.
 * @param value The storage size string to parse.
 * @return A StorageSize object representing the parsed value.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$5:getDefault(double),522,522,"/**
 * Returns a default value, falling back to the provided value.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/StorageUnit.java,getDefault,org.apache.hadoop.conf.StorageUnit$1:getDefault(double),522,522,"/**
 * Returns a default value if the input value is invalid.
 * @param value The input value to check.
 * @return A default value if the input is invalid.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reset,org.apache.hadoop.ha.ActiveStandbyElector:reset(),931,934,"/**
 * Resets the state to INIT and terminates the connection.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,convert,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:convert(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),144,162,"/**
 * Converts a StateChangeRequestInfo to an HAStateChangeRequestInfoProto.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,convert,org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:convert(org.apache.hadoop.ha.proto.HAServiceProtocolProtos$HAStateChangeRequestInfoProto),87,105,"/**
 * Converts a HAStateChangeRequestInfoProto to a StateChangeRequestInfo.
 * @param proto The proto object to convert.
 * @return A StateChangeRequestInfo object.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,createReqInfo,org.apache.hadoop.ha.HAAdmin:createReqInfo(),264,266,"/**
 * Creates a StateChangeRequestInfo object using the request source.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,createReqInfo,org.apache.hadoop.ha.ZKFailoverController:createReqInfo(),510,512,"/**
 * Creates a StateChangeRequestInfo with RequestSource.REQUEST_BY_ZKFC.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,createReqInfo,org.apache.hadoop.ha.FailoverController:createReqInfo(),158,160,"/**
 * Creates a StateChangeRequestInfo object using the request source.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,getServiceStatus,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getServiceStatus(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$GetServiceStatusRequestProto)",143,178,"/**
 * Gets the service status and converts it to a response proto.
 * @param controller RPC controller
 * @param request Request proto
 * @return Service status response proto
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,startRPC,org.apache.hadoop.ha.ZKFailoverController:startRPC(),336,338,"/**
 * Starts the RPC server.
 * Throws IOException if an error occurs during startup.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,parseConfiggedPort,org.apache.hadoop.ha.SshFenceByTcpPort$Args:parseConfiggedPort(java.lang.String),258,266,"/**
 * Parses a port string to an integer.
 * @param portStr The port string to parse.
 * @throws BadFencingConfigurationException if parsing fails.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,checkArgs,org.apache.hadoop.ha.ShellCommandFencer:checkArgs(java.lang.String),72,79,"/**
 * Validates arguments passed to the 'shell' fencing method.
 * Throws exception if args is null or empty.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,"org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.lang.String,java.util.Map)",138,149,"/**
 * Prints command usage information to the provided stream.
 * @param pStr PrintStream to write usage to.
 * @param cmd Command name.
 * @param helpEntries Map of command names to UsageInfo.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkManualStateManagementOK,org.apache.hadoop.ha.HAAdmin:checkManualStateManagementOK(org.apache.hadoop.ha.HAServiceTarget),245,262,"/**
 * Checks if manual HA state management is allowed based on auto-failover.
 * @param target The HAServiceTarget to check.
 * @return True if manual management is OK, false otherwise.
 */
","* Ensure that we are allowed to manually manage the HA state of the target
   * service. If automatic failover is configured, then the automatic
   * failover controllers should be doing state management, and it is generally
   * an error to use the HAAdmin command line to do so.
   * 
   * @param target the target to check
   * @return true if manual state management is allowed",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,execCommand,"org.apache.hadoop.ha.SshFenceByTcpPort:execCommand(com.jcraft.jsch.Session,java.lang.String)",177,203,"/**
 * Executes a command on a remote server and returns its exit status.
 * @param session SSH session
 * @param cmd Command to execute
 * @return Exit status of the command
 */
","* Execute a command through the ssh session, pumping its
   * stderr and stdout to our own logs.",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,enteredState,org.apache.hadoop.ha.ZKFailoverController$HealthCallbacks:enteredState(org.apache.hadoop.ha.HealthMonitor$State),988,992,"/**
* Updates last health state and rechecks electability.
* @param newState The new health state observed.
*/
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,badArg,org.apache.hadoop.ha.ZKFailoverController:badArg(java.lang.String),272,276,"/**
 * Throws an exception for invalid arguments, prints usage.
 * @param arg The invalid argument that caused the error.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,checkEligibleForFailover,org.apache.hadoop.ha.ZKFailoverController:checkEligibleForFailover(),763,776,"/**
 * Checks if the service is eligible for failover.
 * Throws ServiceFailedException if not healthy or in observer state.
 */
","* If the local node is an observer or is unhealthy it
   * is not eligible for graceful failover.
   * @throws ServiceFailedException if the node is an observer or unhealthy",,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,fence,org.apache.hadoop.ha.NodeFencer:fence(org.apache.hadoop.ha.HAServiceTarget),92,94,"/**
 * Fences a service target. Overloads fence(fromSvc, null).
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getFencingParameters,org.apache.hadoop.ha.HAServiceTarget:getFencingParameters(),175,179,"/**
 * Retrieves fencing parameters as a map.
 * @return A map containing the fencing parameters.
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,monitorActiveStatus,org.apache.hadoop.ha.ActiveStandbyElector:monitorActiveStatus(),774,781,"/**
 * Checks leader status, resets retry count, and monitors lock node.
 */",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,zkDoWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:zkDoWithRetries(org.apache.hadoop.ha.ActiveStandbyElector$ZKAction,org.apache.zookeeper.KeeperException$Code)",1145,1159,"/**
 * Executes a ZK action with retries based on retry code.
 * @param action ZK action to execute.
 * @param retryCode Retry code to use for retries.
 * @return Result of the action.
 * @throws KeeperException, InterruptedException
 */
",,,,True,2
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readInDirectBuffer,"org.apache.hadoop.fs.VectoredReadUtils:readInDirectBuffer(org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer,org.apache.hadoop.util.functional.Function4RaisingIOE)",189,216,"/**
 * Reads data from a file range into a direct ByteBuffer using an operation.
 * @param range FileRange object defining the read range.
 */
","* Read bytes from stream into a byte buffer using an
   * intermediate byte array.
   *   <pre>
   *     (position, buffer, buffer-offset, length): Void
   *     position:= the position within the file to read data.
   *     buffer := a buffer to read fully `length` bytes into.
   *     buffer-offset := the offset within the buffer to write data
   *     length := the number of bytes to read.
   *   </pre>
   * The passed in function MUST block until the required length of
   * data is read, or an exception is thrown.
   * @param range range to read
   * @param buffer buffer to fill.
   * @param operation operation to use for reading data.
   * @throws IOException any IOE.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,validateAndSortRanges,"org.apache.hadoop.fs.VectoredReadUtils:validateAndSortRanges(java.util.List,java.util.Optional)",292,337,"/**
 * Validates, sorts, and checks FileRange list against file length.
 * @param input List of FileRange objects to validate and sort.
 * @param fileLength Optional file length for range validation.
 * @return Sorted list of validated FileRange objects.
 */
","* Validate a list of ranges (including overlapping checks) and
   * return the sorted list.
   * <p>
   * Two ranges overlap when the start offset
   * of second is less than the end offset of first.
   * End offset is calculated as start offset + length.
   * @param input input list
   * @param fileLength file length if known
   * @return a new sorted list.
   * @throws IllegalArgumentException if there are overlapping ranges or
   * a range element is invalid (other than with negative offset)
   * @throws EOFException if the last range extends beyond the end of the file supplied
   *                          or a range offset is negative",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,readVectored,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:readVectored(java.util.List,java.util.function.IntFunction)",319,345,"/**
 * Reads file ranges using vectored I/O with asynchronous operations.
 * @param ranges List of file ranges to read.
 * @param allocate Allocates ByteBuffer based on range length.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,<init>,org.apache.hadoop.fs.impl.prefetch.BlockManager:<init>(org.apache.hadoop.fs.impl.prefetch.BlockData),49,53,"/**
 * Constructs a BlockManager with the given BlockData.
 * @param blockData The BlockData to use for managing blocks.
 */
","* Constructs an instance of {@code BlockManager}.
   *
   * @param blockData information about each block of the underlying file.
   *
   * @throws IllegalArgumentException if blockData is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,release,org.apache.hadoop.fs.impl.prefetch.BlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData),107,111,"/**
 * Releases buffer data (no-op, new buffer allocated each time).
 * @param data The buffer data to release.
 */
","* Releases resources allocated to the given block.
   *
   * @param data the {@code BufferData} to release.
   *
   * @throws IllegalArgumentException if data is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,release,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:release(java.lang.Object),88,111,"/**
 * Releases an item back to the pool. Checks item ownership.
 * Throws exception if item wasn't created by this pool.
 */","* Releases a previously acquired resource.
   *
   * @throws IllegalArgumentException if item is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,throwIfStateIncorrect,org.apache.hadoop.fs.impl.prefetch.BufferData:throwIfStateIncorrect(org.apache.hadoop.fs.impl.prefetch.BufferData$State[]),259,275,"/**
 * Throws IllegalStateException if buffer state doesn't match given states.
 * @param states Array of expected State values.
 */
","* Helper that asserts the current state is one of the expected values.
   *
   * @param states the collection of allowed states.
   *
   * @throws IllegalArgumentException if states is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.String,java.lang.String)",103,109,"/**
 * Checks if a string is not null and not empty.
 * @param arg The string to check.
 * @param argName Name of the argument for error messages.
 */
","* Validates that the given string is not null and has non-zero length.
   * @param arg the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNumberOfElements,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNumberOfElements(java.util.Collection,int,java.lang.String)",182,192,"/**
 * Checks if collection is null and has the specified number of elements.
 * @param collection Collection to check.
 * @param numElements Expected number of elements.
 * @param argName Name of the argument.
 */
","* Validates that the given set is not null and has an exact number of items.
   * @param <T> the type of collection's elements.
   * @param collection the argument reference to validate.
   * @param numElements the expected number of elements in the collection.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPathExists,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExists(java.nio.file.Path,java.lang.String)",346,350,"/**
 * Checks if the given path exists.
 * @param path The path to check.
 * @param argName Argument name for error message.
 */
","* Validates that the given path exists.
   * @param path the path to check.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,<init>,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:<init>(int),57,65,"/**
 * Creates a bounded resource pool with a specified size.
 * @param size The maximum number of resources in the pool.
 */
","* Constructs a resource pool of the given size.
   *
   * @param size the size of this pool. Cannot be changed post creation.
   *
   * @throws IllegalArgumentException if size is zero or negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,<init>,"org.apache.hadoop.fs.impl.prefetch.BufferPool:<init>(int,int,org.apache.hadoop.fs.impl.prefetch.PrefetchingStatistics)",90,108,"/**
 * Initializes a BufferPool with a specified size and buffer size.
 * @param size Pool size.
 * @param bufferSize Buffer size.
 * @param prefetchingStatistics Prefetching statistics object.
 */
","* Initializes a new instance of the {@code BufferPool} class.
   * @param size number of buffer in this pool.
   * @param bufferSize size in bytes of each buffer.
   * @param prefetchingStatistics statistics for this stream.
   * @throws IllegalArgumentException if size is zero or negative.
   * @throws IllegalArgumentException if bufferSize is zero or negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,requestPrefetch,org.apache.hadoop.fs.impl.prefetch.BlockManager:requestPrefetch(int),120,124,"/**
* Requests a prefetch for the given block number. No-op.
* @param blockNumber The block number to prefetch.
*/
","* Requests optional prefetching of the given block.
   *
   * @param blockNumber the id of the block to prefetch.
   *
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,<init>,"org.apache.hadoop.fs.impl.prefetch.BufferData:<init>(int,java.nio.ByteBuffer)",111,118,"/**
 * Constructs a BufferData object.
 * @param blockNumber Block number.
 * @param buffer ByteBuffer containing data.
 */
","* Constructs an instances of this class.
   *
   * @param blockNumber Number of the block associated with this buffer.
   * @param buffer The buffer associated with this block.
   *
   * @throws IllegalArgumentException if blockNumber is negative.
   * @throws IllegalArgumentException if buffer is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Retryer.java,<init>,"org.apache.hadoop.fs.impl.prefetch.Retryer:<init>(int,int,int)",55,63,"/**
 * Constructs a Retryer with specified delay and update interval.
 * @param perRetryDelay Delay between retries.
 * @param maxDelay Maximum delay value.
 * @param statusUpdateInterval Interval for status updates.
 */
","* Initializes a new instance of the {@code Retryer} class.
   *
   * @param perRetryDelay per retry delay (in ms).
   * @param maxDelay maximum amount of delay (in ms) before retry fails.
   * @param statusUpdateInterval time interval (in ms) at which status update would be made.
   *
   * @throws IllegalArgumentException if perRetryDelay is zero or negative.
   * @throws IllegalArgumentException if maxDelay is less than or equal to perRetryDelay.
   * @throws IllegalArgumentException if statusUpdateInterval is zero or negative.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,throwIfInvalidBlockNumber,org.apache.hadoop.fs.impl.prefetch.BlockData:throwIfInvalidBlockNumber(int),243,245,"/**
 * Validates blockNumber is within the allowed range [0, numBlocks-1].
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,throwIfInvalidOffset,org.apache.hadoop.fs.impl.prefetch.BlockData:throwIfInvalidOffset(long),247,249,"/**
 * Verifies offset is within valid range; throws exception if not.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Object[],java.lang.String)",117,120,"/**
 * Checks if array is not null and has at least one element.
 * @param array The array to check.
 * @param argName Name of the argument being checked.
 */
","* Validates that the given array is not null and has at least one element.
   * @param <T> the type of array's elements.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(byte[],java.lang.String)",127,130,"/**
 * Checks if byte array is not null and not empty.
 * @param array The byte array to validate.
 * @param argName Name of the argument being checked.
 */
","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(short[],java.lang.String)",137,140,"/**
* Checks if array is null and not empty.
* @param array The short array to validate.
* @param argName Name of the argument being checked.
*/
","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(int[],java.lang.String)",147,150,"/**
 * Checks if array is not null and has at least one element.
 * @param array The array to check.
 * @param argName Name of the argument being checked.
 */
","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(long[],java.lang.String)",157,160,"/**
 * Checks if array is not null and has at least one element.
 * @param array The array to check.
 * @param argName Name of the argument being validated.
 */
","* Validates that the given array is not null and has at least one element.
   * @param array the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkNotNullAndNotEmpty,"org.apache.hadoop.fs.impl.prefetch.Validate:checkNotNullAndNotEmpty(java.lang.Iterable,java.lang.String)",168,173,"/**
 * Checks if iterable is not null and contains at least one element.
 * @param iter Iterable to check.
 * @param argName Name of the argument.
 */
","* Validates that the given buffer is not null and has non-zero capacity.
   * @param <T> the type of iterable's elements.
   * @param iter the argument reference to validate.
   * @param argName the name of the argument being validated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/DefaultBulkDeleteOperation.java,bulkDelete,org.apache.hadoop.fs.impl.DefaultBulkDeleteOperation:bulkDelete(java.util.Collection),74,91,"/**
 * Deletes multiple files/directories.
 * @param paths Collection of paths to delete.
 * @return List of deletion results (Path, error message).
 */
","* {@inheritDoc}.
     * The default impl just calls {@code FileSystem.delete(path, false)}
     * on the single path in the list.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,applyToIOStatisticsSnapshot,"org.apache.hadoop.io.wrappedio.WrappedStatistics:applyToIOStatisticsSnapshot(java.io.Serializable,org.apache.hadoop.util.functional.FunctionRaisingIOE)",339,344,"/**
 * Applies a function to an IOStatisticsSnapshot derived from source.
 * @param source Serializable object to extract snapshot from.
 * @param fun Function to apply; returns T.
 */
","* Apply a function to an object which may be an IOStatisticsSnapshot.
   * @param <T> return type
   * @param source statistics snapshot
   * @param fun function to invoke if {@code source} is valid.
   * @return the applied value
   * @throws UncheckedIOException Any IO exception.
   * @throws IllegalArgumentException if the supplied class is not a snapshot",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,<init>,"org.apache.hadoop.fs.impl.FlagSet:<init>(java.lang.Class,java.lang.String,java.util.EnumSet)",83,92,"/**
 * Constructs a FlagSet with an enum class, prefix, and optional flags.
 */","* Create a FlagSet.
   * @param enumClass class of enum
   * @param prefix prefix (with trailing ""."") for path capabilities probe
   * @param flags flags. A copy of these are made.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,seek,org.apache.hadoop.fs.sftp.SFTPInputStream:seek(long),60,67,"/**
 * Seeks to the specified position.
 * @param position The position to seek to.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,available,org.apache.hadoop.fs.sftp.SFTPInputStream:available(),69,77,"/**
 * Returns the number of bytes available for reading.
 * Returns Integer.MAX_VALUE if remaining bytes exceed it.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,close,org.apache.hadoop.fs.FileSystem:close(),2701,2712,"/**
 * Closes the file system, removes from cache, and processes delete-on-exit files.
 */
","* Close this FileSystem instance.
   * Will release any held locks, delete all files queued for deletion
   * through calls to {@link #deleteOnExit(Path)}, and remove this FS instance
   * from the cache, if cached.
   *
   * After this operation, the outcome of any method call on this FileSystem
   * instance, or any input/output stream created by it is <i>undefined</i>.
   * @throws IOException IO failure",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,equals,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:equals(java.lang.Object),153,160,"/**
 * Checks if this node is equal to another MRNflyNode.
 * @param o The object to compare to.
 * @return True if equal, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,listStatus,org.apache.hadoop.fs.ChecksumFs:listStatus(org.apache.hadoop.fs.Path),567,580,"/**
 * Lists file statuses under a path, excluding checksum files.
 * @param f The path to list.
 * @return Array of FileStatus objects.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,compareTo,org.apache.hadoop.fs.FileStatus:compareTo(java.lang.Object),425,429,"/**
 * Compares this file status with another.
 * @param o the FileStatus to compare to
 * @return negative, zero, or a positive value.
 */
","* Compare this FileStatus to another FileStatus based on lexicographical
   * order of path.
   * This method was added back by HADOOP-14683 to keep binary compatibility.
   *
   * @param   o the FileStatus to be compared.
   * @return  a negative integer, zero, or a positive integer as this object
   *   is less than, equal to, or greater than the specified object.
   * @throws ClassCastException if the specified object is not FileStatus",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,compareTo,org.apache.hadoop.fs.LocatedFileStatus:compareTo(org.apache.hadoop.fs.FileStatus),181,184,"/**
 * Compares this file status with another.
 * Delegates to the superclass implementation.
 */
","* Compare this FileStatus to another FileStatus
   * @param   o the FileStatus to be compared.
   * @return  a negative integer, zero, or a positive integer as this object
   *   is less than, equal to, or greater than the specified object.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,stat2Paths,"org.apache.hadoop.fs.FileUtil:stat2Paths(org.apache.hadoop.fs.FileStatus[],org.apache.hadoop.fs.Path)",133,138,"/**
 * Converts FileStatus array to Path array, defaulting to path.
 * @param stats Array of FileStatus objects.
 * @param path Default Path if stats is null.
 * @return Array of Path objects.
 */
","* convert an array of FileStatus to an array of Path.
   * If stats if null, return path
   * @param stats
   *          an array of FileStatus objects
   * @param path
   *          default path to return in stats is null
   * @return an array of paths corresponding to the input",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.HarFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),1300,1303,"/**
 * Gets the default replication factor for a given path.
 * @param f the path to check
 * @return the default replication factor as a short
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.FilterFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),447,450,"/**
 * Gets the default replication factor for a given path.
 * @param f the path to check
 * @return the default replication factor
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,run,org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner:run(),4166,4181,"/**
 * Cleans up statistics data references from the queue.
 * Continues until interrupted, logs errors and continues.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandler.java,openConnection,org.apache.hadoop.fs.FsUrlStreamHandler:openConnection(java.net.URL),46,49,"/**
 * Opens a connection to the specified URL.
 * @param url The URL to connect to.
 * @return A new FsUrlConnection object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,read,"org.apache.hadoop.fs.FSInputStream:read(long,byte[],int,int)",66,89,"/**
 * Reads data from the stream at a specified position.
 * @param position Read position
 * @param buffer Data buffer
 * @param offset Offset in buffer
 * @param length Number of bytes to read
 * @return Number of bytes read
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.HarFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",896,905,"/**
 * Checks if a path has the specified capability.
 * @param path The path to check.
 * @param capability The capability to check for.
 * @return True if the path has the capability, false otherwise.
 */
","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,serializer,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:serializer(),262,264,"/**
 * Creates a JsonSerialization instance for IOStatisticsSnapshot.
 * Configures for field names and ignoring of transient fields.
 */
","* Get a JSON serializer for this class.
   * @return a serializer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,publishAsStorageStatistics,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:publishAsStorageStatistics(java.lang.String,java.lang.String,org.apache.hadoop.fs.statistics.IOStatistics)",701,704,"/**
 * Creates a StorageStatistics object from IOStatistics.
 * @param name Statistics name.
 * @param scheme Statistics scheme.
 * @param source IOStatistics data source.
 * @return StorageStatistics object.
 */
","* Publish the IOStatistics as a set of storage statistics.
   * This is dynamic.
   * @param name storage statistics name.
   * @param scheme FS scheme; may be null.
   * @param source IOStatistics source.
   * @return a dynamic storage statistics object.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStorageStatistics,org.apache.hadoop.fs.FileSystem:getStorageStatistics(),4651,4653,"/**
 * Returns storage statistics for the current URI.
 * Creates an EmptyStorageStatistics object.
 */
","* Get the StorageStatistics for this FileSystem object.  These statistics are
   * per-instance.  They are not shared with any other FileSystem object.
   *
   * <p>This is a default method which is intended to be overridden by
   * subclasses. The default implementation returns an empty storage statistics
   * object.</p>
   *
   * @return    The StorageStatistics for this FileSystem instance.
   *            Will never be null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIsDirectoryException.java,<init>,org.apache.hadoop.fs.PathIsDirectoryException:<init>(java.lang.String),24,26,"/**
 * Constructs a PathIsDirectoryException with the given path.
 * @param path The path that is a directory.
 */
",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIsNotDirectoryException.java,<init>,org.apache.hadoop.fs.PathIsNotDirectoryException:<init>(java.lang.String),24,26,"/**
 * Constructs a PathIsNotDirectoryException with the given path.
 * @param path The path that is not a directory.
 */
",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathOperationException.java,<init>,org.apache.hadoop.fs.PathOperationException:<init>(java.lang.String),24,26,"/**
 * Constructs a PathOperationException with the given path.
 * @param path The path associated with the exception.
 */
",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIsNotEmptyDirectoryException.java,<init>,org.apache.hadoop.fs.PathIsNotEmptyDirectoryException:<init>(java.lang.String),23,25,"/**
 * Constructs a DirectoryIsNotEmptyException with the given path.
 * @param path The path of the directory.
 */
",@param path for the exception,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,bufferSize,org.apache.hadoop.fs.FSDataOutputStreamBuilder:bufferSize(int),175,178,"/**
 * Sets the buffer size for the builder.
 * @param bufSize The desired buffer size.
 * @return This builder instance for chaining.
 */
","* Set the size of the buffer to be used.
   *
   * @param bufSize buffer size.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,replication,org.apache.hadoop.fs.FSDataOutputStreamBuilder:replication(short),190,193,"/**
 * Sets the replica value.
 * @param replica the replica value to set
 * @return this builder instance
 */
","* Set replication factor.
   *
   * @param replica replica.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,blockSize,org.apache.hadoop.fs.FSDataOutputStreamBuilder:blockSize(long),205,208,"/**
 * Sets the block size for the builder.
 * @param blkSize The desired block size.
 * @return This builder instance for chaining.
 */
","* Set block size.
   *
   * @param blkSize block size.
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,recursive,org.apache.hadoop.fs.FSDataOutputStreamBuilder:recursive(),224,227,"/**
 * Marks the builder as recursive and returns the builder itself.
 */
","* Create the parent directory if they do not exist.
   *
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,create,org.apache.hadoop.fs.FSDataOutputStreamBuilder:create(),254,257,"/**
 * Adds the CREATE flag and returns this builder for chaining.
 */
","* Create an FSDataOutputStream at the specified path.
   *
   * @return return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,overwrite,org.apache.hadoop.fs.FSDataOutputStreamBuilder:overwrite(boolean),267,274,"/**
 * Sets the overwrite flag.
 * @param overwrite Whether to enable overwrite functionality.
 * @return This builder instance for chaining.
 */
","* Set to true to overwrite the existing file.
   * Set it to false, an exception will be thrown when calling {@link #build()}
   * if the file exists.
   *
   * @param overwrite overrite.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,append,org.apache.hadoop.fs.FSDataOutputStreamBuilder:append(),281,284,"/**
 * Appends to the builder, adding the APPEND flag.
 * @return This builder instance for chaining.
 */
","* Append to an existing file (optional operation).
   *
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,hashCode,org.apache.hadoop.fs.permission.FsCreateModes:hashCode(),103,108,"/**
 * Generates a hash code based on the object's unmasked value.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],org.apache.hadoop.fs.StorageType[],long,long,boolean)",161,197,"/**
 * Constructs a BlockLocation with provided data.
 * @param names, hosts, etc. block location details.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setHosts,org.apache.hadoop.fs.BlockLocation:setHosts(java.lang.String[]),312,318,"/**
 * Sets the hosts array. Interns strings if provided, otherwise uses empty array.
 */
","* Set the hosts hosting this block.
   * @param hosts hosts array.
   * @throws IOException If an I/O error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setCachedHosts,org.apache.hadoop.fs.BlockLocation:setCachedHosts(java.lang.String[]),324,330,"/**
 * Sets the cached hosts array. Interns strings if provided.
 */","* Set the hosts hosting a cached replica of this block.
   * @param cachedHosts cached hosts.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setNames,org.apache.hadoop.fs.BlockLocation:setNames(java.lang.String[]),337,343,"/**
 * Sets the names array. Interns strings if names is not null.
 */
","* Set the names (host:port) hosting this block.
   * @param names names.
   * @throws IOException If an I/O error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setTopologyPaths,org.apache.hadoop.fs.BlockLocation:setTopologyPaths(java.lang.String[]),351,357,"/**
 * Sets the topology paths. Uses String interning if paths are provided.
 */","* Set the network topology paths of the hosts.
   *
   * @param topologyPaths topology paths.
   * @throws IOException If an I/O error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,setStorageIds,org.apache.hadoop.fs.BlockLocation:setStorageIds(java.lang.String[]),359,365,"/**
* Sets the storage IDs. Uses String interning if provided.
*/",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPathInternal,org.apache.hadoop.fs.shell.Command:processPathInternal(org.apache.hadoop.fs.shell.PathData),382,388,"/**
 * Processes a PathData item, optionally recursing.
 * @param item The PathData to process.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processPath,org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processPath(org.apache.hadoop.fs.shell.PathData),250,272,"/**
 * Processes a PathData item, modifying ACLs based on configuration options.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Stat.java,processPath,org.apache.hadoop.fs.shell.Stat:processPath(org.apache.hadoop.fs.shell.PathData),91,155,"/**
 * Processes a PathData item and formats its status to output.
 * @param item The PathData item to process.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,isFile,org.apache.hadoop.fs.FileSystem:isFile(org.apache.hadoop.fs.Path),1895,1902,"/**
 * Checks if a path represents a file. Returns false if file not found.
 * @param f the path to check
 * @throws IOException if an I/O error occurs
 */
","True iff the named path is a regular file.
   * Note: Avoid using this method. Instead reuse the FileStatus
   * returned by {@link #getFileStatus(Path)} or listStatus() methods.
   *
   * @param f path to check
   * @throws IOException IO failure
   * @deprecated Use {@link #getFileStatus(Path)} instead
   * @return if f is file true, not false.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,isFile,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:isFile(),479,482,"/**
 * Checks if the file is a file.
 * @return True if it's a file, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,isFile,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:isFile(),55,58,"/**
 * Checks if the file system object is a file.
 * @return True if it's a file, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,toString,org.apache.hadoop.fs.FileStatus:toString(),458,488,"/**
 * Returns a string representation of this object's state.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getSymlink,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:getSymlink(),539,542,"/**
 * Returns the path of the symbolic link.
 * @return Path object representing the symlink path.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,getSymlink,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:getSymlink(),115,118,"/**
 * Returns the symbolic link.
 * @return Path representing the symbolic link.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,<init>,"org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type)",64,71,"/**
 * Constructs a FsServerDefaults with default values for some fields.
 * @param blockSize Block size for file system operations
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,<init>,"org.apache.hadoop.fs.FsServerDefaults:<init>(long,int,int,short,int,boolean,long,org.apache.hadoop.util.DataChecksum$Type,java.lang.String)",73,80,"/**
 * Constructs a FsServerDefaults with default encryption key.
 * @param blockSize Block size for file system.
 * @param checksumType Data checksum type.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setXAttr,"org.apache.hadoop.fs.FilterFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",359,363,"/**
 * Sets an extended attribute on a path.
 * @param path The path to set the attribute on.
 * @param name Attribute name.
 * @param value Attribute value.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathAccessDeniedException.java,<init>,"org.apache.hadoop.fs.PathAccessDeniedException:<init>(java.lang.String,java.lang.Throwable)",28,30,"/**
 * Constructs a PathAccessDeniedException with path and cause.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathPermissionException.java,<init>,"org.apache.hadoop.fs.PathPermissionException:<init>(java.lang.String,java.lang.Throwable)",30,32,"/**
* Constructs a PathPermissionException with a path and cause.
* @param path The path associated with the permission error.
* @param cause The underlying Throwable that caused the exception.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathNotFoundException.java,<init>,"org.apache.hadoop.fs.PathNotFoundException:<init>(java.lang.String,java.lang.Throwable)",30,32,"/**
 * Constructs a PathNotFoundException with a path and cause.
 * @param path The path that was not found.
 * @param cause The underlying cause of the exception.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Concat.java,processArguments,org.apache.hadoop.fs.shell.Concat:processArguments(java.util.LinkedList),49,85,"/**
 * Processes arguments to concatenate files. Throws IOException on error.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,wrapException,"org.apache.hadoop.io.IOUtils:wrapException(java.lang.String,java.lang.String,java.io.IOException)",460,480,"/**
 * Wraps an IOException, potentially adding a custom message.
 * @param path file/directory path
 * @param methodName method name
 * @param exception the exception to wrap
 * @return Wrapped IOException
 */
","* Takes an IOException, file/directory path, and method name and returns an
   * IOException with the input exception as the cause and also include the
   * file,method details. The new exception provides the stack trace of the
   * place where the exception is thrown and some extra diagnostics
   * information.
   *
   * Return instance of same exception if exception class has a public string
   * constructor; Otherwise return an PathIOException.
   * InterruptedIOException and PathIOException are returned unwrapped.
   *
   * @param path file/directory path
   * @param methodName method name
   * @param exception the caught exception.
   * @return an exception to throw",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sync,org.apache.hadoop.io.SequenceFile$Reader:sync(long),2831,2864,"/**
 * Synchronizes the input stream to a known sync marker.
 * @param position Starting position to search for sync marker.
 */","* Seek to the next sync mark past a given position.
     * @param position position.
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,reset,org.apache.hadoop.io.MapFile$Reader:reset(),638,640,"/**
 * Resets the data stream position to the initial starting position.
 */","* Re-positions the reader before its first key.
     *
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.java,read,org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(byte[]),82,85,"/**
* Reads data from the input stream into the provided byte array.
* @param b the byte array to read into
* @return the number of bytes read
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,registerExpressions,org.apache.hadoop.fs.shell.find.Find:registerExpressions(org.apache.hadoop.fs.shell.find.ExpressionFactory),102,106,"/**
 * Registers expression classes with the given ExpressionFactory.
 */",Register the expressions with the expression factory.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,registerCommands,org.apache.hadoop.fs.shell.FsCommand:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),52,74,"/**
 * Registers various command classes with the provided command factory.
 */
","* Register the command classes used by the fs subcommand
   * @param factory where to register the class",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,registerCommands,org.apache.hadoop.fs.FsShell:registerCommands(org.apache.hadoop.fs.shell.CommandFactory),111,118,"/**
 * Registers commands with the given factory, conditionally.
 * Registers FsCommand only if this is an FsShell instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobExpander.java,expand,org.apache.hadoop.fs.GlobExpander:expand(java.lang.String),63,77,"/**
 * Expands a file pattern into a list of full file paths.
 * @param filePattern The file pattern to expand.
 * @return List of expanded file paths.
 */
","* Expand globs in the given <code>filePattern</code> into a collection of
   * file patterns so that in the expanded set no file pattern has a slash
   * character (""/"") in a curly bracket pair.
   * <p>
   * Some examples of how the filePattern is expanded:<br>
   * <pre>
   * <b>
   * filePattern         - Expanded file pattern </b>
   * {a/b}               - a/b
   * /}{a/b}             - /}a/b
   * p{a/b,c/d}s         - pa/bs, pc/ds
   * {a/b,c/d,{e,f}}     - a/b, c/d, {e,f}
   * {a/b,c/d}{e,f}      - a/b{e,f}, c/d{e,f}
   * {a,b}/{b,{c/d,e/f}} - {a,b}/b, {a,b}/c/d, {a,b}/e/f
   * {a,b}/{c/\d}        - {a,b}/c/d
   * </pre>
   * 
   * @param filePattern file pattern.
   * @return expanded file patterns
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,fetchMore,org.apache.hadoop.fs.FileSystem$DirListingIterator:fetchMore(),2326,2330,"/**
 * Fetches the next batch of entries using the provided token.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,printXAttr,"org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:printXAttr(java.lang.String,byte[])",120,128,"/**
 * Prints an X attribute to the output stream.
 * @param name Attribute name.
 * @param value Attribute value as bytes.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,"org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2119,2124,"/**
 * Lists status of files/directories under a path, filtered by PathFilter.
 * @param f path to list
 * @param filter filter to apply
 * @return FileStatus array of matching entries
 */
","* Filter files/directories in the given path using the user-supplied path
   * filter.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   *
   * @param f
   *          a path name
   * @param filter
   *          the user-supplied path filter
   * @return an array of FileStatus objects for the files under the given path
   *         after applying the filter
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,"org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)",2161,2168,"/**
 * Lists file statuses for the given paths, filtered by the provided filter.
 * @param files Paths to check.
 * @param filter Filter to apply.
 * @return Array of FileStatus objects.
 */
","* Filter files/directories in the given list of paths using user-supplied
   * path filter.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   *
   * @param files
   *          a list of paths
   * @param filter
   *          the user-supplied path filter
   * @return a list of statuses for the files under the given paths after
   *         applying the filter
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterInt.java,<init>,"org.apache.hadoop.metrics2.lib.MutableCounterInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",36,39,"/**
 * Initializes a MutableCounterInt with given metrics info and initial value.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableCounterLong.java,<init>,"org.apache.hadoop.metrics2.lib.MutableCounterLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",37,40,"/**
 * Initializes a MutableCounterLong with given info and initial value.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeLong.java,<init>,"org.apache.hadoop.metrics2.lib.MutableGaugeLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",37,40,"/**
 * Constructs a MutableGaugeLong with initial value.
 * @param info MetricsInfo object
 * @param initValue Initial gauge value
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,<init>,"org.apache.hadoop.metrics2.lib.MutableGaugeFloat:<init>(org.apache.hadoop.metrics2.MetricsInfo,float)",33,36,"/**
 * Initializes a MutableGaugeFloat with given info and initial value.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeInt.java,<init>,"org.apache.hadoop.metrics2.lib.MutableGaugeInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",37,40,"/**
* Constructs a MutableGaugeInt with initial value.
* @param info MetricsInfo object
* @param initValue Initial integer value for the gauge
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricCounterLong.java,<init>,"org.apache.hadoop.metrics2.impl.MetricCounterLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",29,32,"/**
 * Constructs a MetricCounterLong with the given info and initial value.
 * @param info MetricsInfo object describing the metric
 * @param value Initial long value for the counter
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeLong.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeLong:<init>(org.apache.hadoop.metrics2.MetricsInfo,long)",29,32,"/**
 * Constructs a MetricGaugeLong with given info and value.
 * @param info MetricsInfo object
 * @param value The long value for the gauge
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricCounterInt.java,<init>,"org.apache.hadoop.metrics2.impl.MetricCounterInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",29,32,"/**
 * Constructs a MetricCounterInt with given info and initial value.
 * @param info MetricsInfo object
 * @param value Initial integer value for the counter
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeFloat.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeFloat:<init>(org.apache.hadoop.metrics2.MetricsInfo,float)",29,32,"/**
 * Constructs a MetricGaugeFloat with given info and float value.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeDouble.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeDouble:<init>(org.apache.hadoop.metrics2.MetricsInfo,double)",29,32,"/**
 * Constructs a MetricGaugeDouble with given info and value.
 * @param info MetricsInfo object
 * @param value The double value for the metric
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricGaugeInt.java,<init>,"org.apache.hadoop.metrics2.impl.MetricGaugeInt:<init>(org.apache.hadoop.metrics2.MetricsInfo,int)",29,32,"/**
 * Constructs a MetricGaugeInt with provided info and value.
 * @param info MetricsInfo object
 * @param value The gauge value
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:getDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",369,373,"/**
 * Gets a delegation token.
 * @param url URL of the TGT. @param token TGT. @param renewer renewer.
 * @return Delegation token.
 */
","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @return a delegation token.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:renewDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",416,419,"/**
 * Renews a delegation token.
 * @param url Delegation token URL.
 * @param token Token to renew.
 * @return Renewed token lifetime.
 */
","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",457,460,"/**
 * Cancels a delegation token for the given URL and token.
 * @param url URL of the delegation token to cancel.
 * @param token Token to be canceled.
 * @throws IOException if an I/O error occurs.
 */
","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @throws IOException if an IO error occurred.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,<init>,"org.apache.hadoop.crypto.key.kms.ValueQueue:<init>(int,float,long,int,org.apache.hadoop.crypto.key.kms.ValueQueue$QueueRefiller)",262,266,"/**
 * Constructs a ValueQueue with default generation policy.
 * @param numValues Initial queue size, lowWaterMark, expiry,
 *        numFillerThreads, fetcher.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileEncryptionInfo.java,<init>,"org.apache.hadoop.fs.FileEncryptionInfo:<init>(org.apache.hadoop.crypto.CipherSuite,org.apache.hadoop.crypto.CryptoProtocolVersion,byte[],byte[],java.lang.String,java.lang.String)",57,74,"/**
 * Creates a FileEncryptionInfo object with the given parameters.
 * @param suite CipherSuite, version, edek, iv, keyName, ezKeyVersionName
 */
","* Create a FileEncryptionInfo.
   *
   * @param suite CipherSuite used to encrypt the file
   * @param edek encrypted data encryption key (EDEK) of the file
   * @param iv initialization vector (IV) used to encrypt the file
   * @param keyName name of the key used for the encryption zone
   * @param ezKeyVersionName name of the KeyVersion used to encrypt the
   *                         encrypted data encryption key.
   * @param version version.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,getFS,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getFS(),106,109,"/**
 * Returns the FileSystem instance.
 * @return The FileSystem object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,permission,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:permission(org.apache.hadoop.fs.permission.FsPermission),121,126,"/**
 * Sets the permission for the object.
 * @param perm The permission to set.
 * @return This builder instance for chaining.
 */
",* Set permission for the file.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,checksumOpt,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:checksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt),211,216,"/**
 * Sets the checksum option and returns this builder.
 * @param chksumOpt The checksum option to set.
 * @return This builder instance.
 */
",* Set checksum opt.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WrappedIOException.java,<init>,org.apache.hadoop.fs.impl.WrappedIOException:<init>(java.io.IOException),47,49,"/**
 * Constructs a WrappedIOException with the given IOException as the cause.
 */
","* Construct from a non-null IOException.
   * @param cause inner cause
   * @throws NullPointerException if the cause is null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FsLinkResolution.java,<init>,org.apache.hadoop.fs.impl.FsLinkResolution:<init>(org.apache.hadoop.fs.impl.FsLinkResolution$FsLinkResolutionFunction),50,52,"/**
 * Constructs a FsLinkResolution with the provided function.
 * @param fn The function to resolve links. Must not be null.
 */
","* Construct an instance with the given function.
   * @param fn function to invoke.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,org.apache.hadoop.fs.Globber$GlobBuilder:<init>(org.apache.hadoop.fs.FileContext),437,440,"/**
 * Constructs a GlobBuilder with the given FileContext.
 * @param fc The FileContext to use; must not be null.
 */
","* Construct bonded to a file context.
     * @param fc file context.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,org.apache.hadoop.fs.Globber$GlobBuilder:<init>(org.apache.hadoop.fs.FileSystem),446,449,"/**
 * Constructs a GlobBuilder with the given file system.
 * @param fs The file system to use.
 */
","* Construct bonded to a filesystem.
     * @param fs file system.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,getFS,org.apache.hadoop.fs.FSDataOutputStreamBuilder:getFS(),141,144,"/**
 * Returns the FileSystem instance.
 * @return The FileSystem object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,permission,org.apache.hadoop.fs.FSDataOutputStreamBuilder:permission(org.apache.hadoop.fs.permission.FsPermission),159,163,"/**
 * Sets the permission for the object.
 * @param perm The permission to set.
 * @return This builder object for chaining.
 */
","* Set permission for the file.
   *
   * @param perm permission.
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,progress,org.apache.hadoop.fs.FSDataOutputStreamBuilder:progress(org.apache.hadoop.util.Progressable),239,243,"/**
 * Sets the progressable object.
 * @param prog The progressable object to set.
 * @return This builder instance for chaining.
 */
","* Set the facility of reporting progress.
   *
   * @param prog progress.
   * @return B Generics Type.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,checksumOpt,org.apache.hadoop.fs.FSDataOutputStreamBuilder:checksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt),296,300,"/**
 * Sets the checksum option and returns this builder.
 * @param chksumOpt The ChecksumOpt instance to set.
 * @return This builder instance.
 */
","* Set checksum opt.
   *
   * @param chksumOpt check sum opt.
   * @return Generics Type B.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,validateWriteArgs,"org.apache.hadoop.fs.store.DataBlocks:validateWriteArgs(byte[],int,int)",110,118,"/**
 * Validates write arguments to prevent IndexOutOfBoundsException.
 * @param b byte array
 * @param off offset
 * @param len length
 */
","* Validate args to a write command. These are the same validation checks
   * expected for any implementation of {@code OutputStream.write()}.
   *
   * @param b   byte array containing data.
   * @param off offset in array where to start.
   * @param len number of bytes to be written.
   * @throws NullPointerException      for a null buffer
   * @throws IndexOutOfBoundsException if indices are out of range
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,set,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue:set(java.lang.Object),219,224,"/**
 * Sets the value to the provided value.
 * Ensures value is not null and initially null.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,getLowerLayerAsyncReturn,org.apache.hadoop.io.retry.AsyncCallHandler:getLowerLayerAsyncReturn(),78,83,"/**
 * Retrieves and clears the lower layer async return.
 * Returns the AsyncGet object; ensures it's not null.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,setGcTimeMonitor,org.apache.hadoop.metrics2.source.JvmMetrics:setGcTimeMonitor(org.apache.hadoop.util.GcTimeMonitor),107,110,"/**
 * Sets the GC time monitor.
 * @param gcTimeMonitor The monitor instance to set.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,init,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:init(byte[],byte[])",136,142,"/**
 * Initializes the cipher with the provided key and IV.
 * @param key encryption key
 * @param iv initialization vector
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,init,"org.apache.hadoop.crypto.JceCtrCryptoCodec$JceCtrCipher:init(byte[],byte[])",128,138,"/**
 * Initializes the cipher with the provided key and IV.
 * @param key encryption key
 * @param iv initialization vector
 * @throws IOException if initialization fails
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,equalsIgnoreCase,"org.apache.hadoop.util.StringUtils:equalsIgnoreCase(java.lang.String,java.lang.String)",1259,1264,"/**
 * Compares two strings for equality, ignoring case.
 * @param s1 The first string.
 * @param s2 The second string.
 * @return True if strings are equal ignoring case, false otherwise.
 */
","* Compare strings locale-freely by using String#equalsIgnoreCase.
   *
   * @param s1  Non-null string to be converted
   * @param s2  string to be converted
   * @return     the str, converted to uppercase.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LimitInputStream.java,<init>,"org.apache.hadoop.util.LimitInputStream:<init>(java.io.InputStream,long)",43,48,"/**
 * Creates a LimitInputStream with the given input stream and limit.
 * @param in the input stream
 * @param limit the maximum number of bytes to read
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String[],java.lang.String)",432,439,"/**
 * Constructs a DeprecationDelta with key, new keys, and optional message.
 * @param key The key being deprecated.
 * @param newKeys New keys replacing the old one.
 * @param customMessage Optional custom deprecation message.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,setReconfigurationUtil,org.apache.hadoop.conf.ReconfigurableBase:setReconfigurationUtil(org.apache.hadoop.conf.ReconfigurationUtil),88,91,"/**
* Sets the ReconfigurationUtil instance.
* @param ru The ReconfigurationUtil to set. Must not be null.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,isStaleClient,org.apache.hadoop.ha.ActiveStandbyElector:isStaleClient(java.lang.Object),1173,1181,"/**
 * Checks if the provided ZooKeeper context is stale.
 * @param ctx The ZooKeeper context to check.
 * @return True if the context is stale, false otherwise.
 */
","* The callbacks and watchers pass a reference to the ZK client
   * which made the original call. We don't want to take action
   * based on any callbacks from prior clients after we quit
   * the election.
   * @param ctx the ZK client passed into the watcher
   * @return true if it matches the current client",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatistics,"org.apache.hadoop.fs.FileSystem:getStatistics(java.lang.String,java.lang.Class)",4586,4605,"/**
 * Retrieves statistics for the given file system class.
 * @param scheme file system scheme
 * @param cls file system class
 * @return Statistics object for the file system.
 */
","* Get the statistics for a particular file system.
   * @param scheme scheme.
   * @param cls the class to lookup
   * @return a statistics object
   * @deprecated use {@link #getGlobalStorageStatistics()}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeCompressedByteArray,"org.apache.hadoop.io.WritableUtils:writeCompressedByteArray(java.io.DataOutput,byte[])",61,83,"/**
 * Writes a compressed byte array to the output.
 * @param out DataOutput to write to, -1 if bytes is null
 * @param bytes byte array to compress and write
 * @return Compression ratio (debug only), -1 if bytes is null
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,int,boolean)",64,81,"/**
 * Copies bytes from an input stream to an output stream.
 * @param in Input stream.
 * @param out Output stream.
 * @param buffSize Buffer size.
 * @param close Whether to close streams.
 */
","* Copies from one stream to another.
   *
   * @param in InputStrem to read from
   * @param out OutputStream to write to
   * @param buffSize the size of the buffer 
   * @param close whether or not close the InputStream and 
   * OutputStream at the end. The streams are closed in the finally clause.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,long,boolean)",145,175,"/**
 * Copies a specified number of bytes from an input stream to an output stream.
 * @param in Input stream.
 * @param out Output stream.
 * @param count Number of bytes to copy.
 * @param close Whether to close streams after copying.
 */
","* Copies count bytes from one stream to another.
   *
   * @param in InputStream to read from
   * @param out OutputStream to write to
   * @param count number of bytes to copy
   * @param close whether to close the streams
   * @throws IOException if bytes can not be read or written",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,close,org.apache.hadoop.ipc.Client$IpcStreams:close(),1955,1959,"/**
 * Closes the input and output streams to release resources.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,<init>,org.apache.hadoop.util.VersionInfo:<init>(java.lang.String),41,55,"/**
 * Loads version info from properties file.
 * @param component Component name for version info file.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stopSinks,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSinks(),471,479,"/**
 * Stops all registered metrics sinks and clears the sink registry.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,finalize,org.apache.hadoop.crypto.random.OsSecureRandom:finalize(),128,131,"/**
 * Calls close() in the finalize method to release resources.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,doDiskIo,org.apache.hadoop.util.DiskChecker:doDiskIo(java.io.File),255,274,"/**
 * Checks disk I/O for a directory, retrying a few times.
 * @param dir Directory to check; throws DiskErrorException on failure.
 */
","* Performs some disk IO by writing to a new file in the given directory
   * and sync'ing file contents to disk.
   *
   * This increases the likelihood of catching catastrophic disk/controller
   * failures sooner.
   *
   * @param dir directory to be checked.
   * @throws DiskErrorException if we hit an error while trying to perform
   *         disk IO against the file.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,<init>,"org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,java.util.List)",44,46,"/**
 * Constructs a PartialListing with a path and partial listing.
 * @param listedPath Path of the listing.
 * @param partialListing List of items in the listing.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,<init>,"org.apache.hadoop.fs.PartialListing:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.ipc.RemoteException)",48,50,"/**
 * Constructs a PartialListing with a Path and RemoteException.
 * @param listedPath The Path representing the listing.
 * @param exception The RemoteException that occurred.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Object),50,52,"/**
 * Constructs a Return object with the given return value.
 * @param r The return value.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,org.apache.hadoop.io.retry.CallReturn:<init>(java.lang.Throwable),53,56,"/**
 * Constructs a CallReturn with an exception.
 * @param t The exception that occurred. Must not be null.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/CallReturn.java,<init>,org.apache.hadoop.io.retry.CallReturn:<init>(org.apache.hadoop.io.retry.CallReturn$State),57,59,"/**
 * Constructs a CallReturn object with a State, other fields null.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:calculateIV(byte[],long,byte[])",66,70,"/**
 * Calculates the Initialization Vector (IV) using parent implementation.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:calculateIV(byte[],long,byte[])",52,56,"/**
 * Calculates the Initialization Vector (IV) using parent class logic.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceSm4CtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:calculateIV(byte[],long,byte[])",48,52,"/**
 * Calculates the IV, delegating to the parent class.
 * @param initIV Initial IV byte array
 * @param counter Counter value for IV calculation
 * @param iv Output IV byte array
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java,calculateIV,"org.apache.hadoop.crypto.JceAesCtrCryptoCodec:calculateIV(byte[],long,byte[])",48,52,"/**
 * Calculates the IV, calling superclass implementation.
 * @param initIV Initial IV byte array.
 * @param counter Counter value for IV calculation.
 * @param iv Output IV byte array.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,build,org.apache.hadoop.util.GcTimeMonitor$Builder:build(),95,98,"/**
 * Creates and returns a GcTimeMonitor instance.
 * Uses provided parameters to configure the monitor.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,isTypeQuotaSet,org.apache.hadoop.fs.QuotaUsage:isTypeQuotaSet(),193,202,"/**
 * Checks if any storage type quota is set.
 * Returns true if at least one quota > 0, false otherwise.
 */
","* Return true if any storage type quota has been set.
   *
   * @return if any storage type quota has been set true, not false.
   *",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,isTypeConsumedAvailable,org.apache.hadoop.fs.QuotaUsage:isTypeConsumedAvailable(),210,219,"/**
 * Checks if any storage type has consumed quota.
 * Returns true if at least one type has consumed quota.
 */
","* Return true if any storage type consumption information is available.
   *
   * @return if any storage type consumption information
   * is available, not false.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,getAndCheckStorageTypes,org.apache.hadoop.fs.shell.Count:getAndCheckStorageTypes(java.lang.String),180,193,"/**
 * Parses storage types from a string, returns a list.
 * @param types Comma-separated storage type string or ""all"".
 * @return List of StorageType objects.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,equals,org.apache.hadoop.fs.LocatedFileStatus:equals(java.lang.Object),190,193,"/**
* Checks if this object is equal to another object.
* Delegates to the superclass's equals method.
*/
","Compare if this object is equal to another object
   * @param   o the object to be compared.
   * @return  true if two file status has the same path name; false if not.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,equals,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:equals(java.lang.Object),549,552,"/**
 * Checks if two objects have the same real status.
 * @param o the object to compare to
 * @return true if realStatus is equal; false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,equals,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:equals(java.lang.Object),40,43,"/**
 * Compares this object with another based on superclass equality.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,hashCode,org.apache.hadoop.fs.LocatedFileStatus:hashCode(),201,204,"/**
 * Returns the hash code value for this object. Delegates to super.
 */","* Returns a hash code value for the object, which is defined as
   * the hash code of the path name.
   *
   * @return  a hash code value for the path name.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,hashCode,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:hashCode(),554,557,"/**
 * Returns the hash code, based on the realStatus field.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,hashCode,org.apache.hadoop.fs.viewfs.ViewFsFileStatus:hashCode(),45,48,"/**
 * Returns the hash code value for this object. Delegates to super.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,getFolderUsage,org.apache.hadoop.fs.DUHelper:getFolderUsage(java.lang.String),34,36,"/**
 * Calculates the size of a folder in bytes.
 * @param folder The path to the folder.
 * @return The folder size in bytes.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,clear,org.apache.hadoop.fs.statistics.MeanStatistic:clear(),147,149,"/**
 * Resets the internal samples and sum to zero.
 */
",* Set the values to 0.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,set,org.apache.hadoop.fs.statistics.MeanStatistic:set(org.apache.hadoop.fs.statistics.MeanStatistic),168,170,"/**
* Sets the statistic's samples and sum from another MeanStatistic.
*/
","* Set the statistic to the values of another.
   * Synchronized.
   * @param other the source.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,ioStatisticsToString,org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToString(org.apache.hadoop.fs.statistics.IOStatistics),77,91,"/**
 * Converts IOStatistics to a string representation.
 * @param statistics The IOStatistics object to convert, null for empty string.
 * @return String representation of the statistics or an empty string.
 */
","* Convert IOStatistics to a string form.
   * @param statistics A statistics instance.
   * @return string value or the empty string if null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,mapToSortedString,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:mapToSortedString(java.lang.StringBuilder,java.lang.String,java.util.Map,java.util.function.Predicate)",159,164,"/**
 * Appends a sorted map to a StringBuilder, using isEmpty predicate.
 */
","* Given a map, produce a string with all the values, sorted.
   * Needs to create a treemap and insert all the entries.
   * @param sb string buffer to append to
   * @param type type (for output)
   * @param map map to evaluate
   * @param <E> type of values of the map",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationTrackerFactory.java,trackDuration,org.apache.hadoop.fs.statistics.DurationTrackerFactory:trackDuration(java.lang.String),60,62,"/**
 * Tracks duration with a default weight of 1.
 * @param key Identifier for the duration being tracked.
 * @return DurationTracker object for tracking.
 */
","* Initiate a duration tracking operation by creating/returning
   * an object whose {@code close()} call will
   * update the statistics.
   * The expected use is within a try-with-resources clause.
   * @param key statistic key
   * @return an object to close after an operation completes.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/PairedDurationTrackerFactory.java,trackDuration,"org.apache.hadoop.fs.statistics.impl.PairedDurationTrackerFactory:trackDuration(java.lang.String,long)",50,55,"/**
 * Tracks duration, combining global and local trackers.
 * @param key Identifier for the duration being tracked.
 * @param count The count associated with the duration.
 * @return A PairedDurationTracker instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,getLongStatistics,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:getLongStatistics(),66,78,"/**
 * Returns an iterator over LongStatistic objects from counters & gauges.
 */","* Take a snapshot of the current counter values
   * and return an iterator over them.
   * @return all the counter statistics.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addTimedOperation(java.lang.String,long)",440,445,"/**
 * Adds a timed operation sample for mean, min, and max statistics.
 * @param prefix Operation prefix; used for statistic keys.
 * @param durationMillis Operation duration in milliseconds.
 */
","* Add a duration to the min/mean/max statistics, using the
   * given prefix and adding a suffix for each specific value.
   * <p>
   * The update is non -atomic, even though each individual statistic
   * is updated thread-safely. If two threads update the values
   * simultaneously, at the end of each operation the state will
   * be correct. It is only during the sequence that the statistics
   * may be observably inconsistent.
   * </p>
   * @param prefix statistic prefix
   * @param durationMillis duration in milliseconds.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,build,org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:build(),51,56,"/**
 * Builds and returns the IOStatistics object.
 * Sets instance to null to prevent further building.
 */
","* Build the IOStatistics instance.
   * @return an instance.
   * @throws IllegalStateException if the builder has already been built.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionCounter(java.lang.String,java.util.function.ToLongFunction)",74,78,"/**
 * Adds a long function counter with the given key.
 * @param key Counter key.
 * @param eval Function to apply to a string and return a long.
 */
","* Add a new evaluator to the counter statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionGauge,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionGauge(java.lang.String,java.util.function.ToLongFunction)",125,129,"/**
 * Adds a LongFunction gauge with the given key.
 * @param key Gauge key.
 * @param eval Function to evaluate and get a long value.
 */
","* Add a new evaluator to the gauge statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionMinimum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionMinimum(java.lang.String,java.util.function.ToLongFunction)",163,167,"/**
 * Adds a long function minimum statistic with the given key.
 * @param key Statistic key.
 * @param eval Function to evaluate and get a long value.
 */
","* Add a new evaluator to the minimum statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withLongFunctionMaximum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withLongFunctionMaximum(java.lang.String,java.util.function.ToLongFunction)",202,206,"/**
 * Sets the maximum value for a long function.
 * @param key Identifier for the function.
 * @param eval Function to evaluate.
 * @return This builder instance.
 */
","* Add a new evaluator to the maximum statistics.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withMeanStatisticFunction,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withMeanStatisticFunction(java.lang.String,java.util.function.Function)",242,246,"/**
 * Adds a mean statistic function for a given key.
 * @param key Identifier for the statistic.
 * @param eval Function to calculate the mean statistic.
 * @return This builder instance.
 */
","* Add a new evaluator to the mean statistics.
   *
   * This is a function which must return the mean and the sample count.
   * @param key key of this statistic
   * @param eval evaluator for the statistic
   * @return the builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,register,org.apache.hadoop.service.launcher.InterruptEscalator:register(java.lang.String),142,146,"/**
 * Registers a new interrupt handler for the given signal name.
 */","* Register an interrupt handler.
   * @param signalName signal name
   * @throws IllegalArgumentException if the registration failed",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,unreference,org.apache.hadoop.net.unix.DomainSocket:unreference(boolean),176,182,"/**
 * Decrements the reference count. Checks if closed if checkClosed is true.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EvaluatingStatisticsMap.java,snapshot,org.apache.hadoop.fs.statistics.impl.EvaluatingStatisticsMap:snapshot(),146,148,"/**
 * Returns a snapshot of the statistics as a map.
 * Uses copyFn to create copies of the values.
 */
","* Take a snapshot.
   * @return a map snapshot.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,snapshotMap,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:snapshotMap(java.util.Map),200,204,"/**
 * Creates a snapshot of the source map.
 * @param source The map to snapshot.
 * @return A new map containing the snapshot.
 */
","* Take a snapshot of a supplied map, where the copy option simply
   * uses the existing value.
   *
   * For this to be safe, the map must refer to immutable objects.
   * @param source source map
   * @param <E> type of values.
   * @return a new map referencing the same values.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/EmptyIOStatisticsContextImpl.java,snapshot,org.apache.hadoop.fs.statistics.impl.EmptyIOStatisticsContextImpl:snapshot(),44,47,"/**
 * Creates and returns a new IOStatisticsSnapshot object.
 */
","* Create a new empty snapshot.
   * A new one is always created for isolation.
   *
   * @return a statistics snapshot",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextImpl.java,<init>,"org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:<init>(long,long)",64,67,"/**
 * Constructs an IOStatisticsContextImpl with a thread and ID.
 * @param threadId The ID of the thread.
 * @param id The unique ID for this context.
 */
","* Constructor.
   * @param threadId thread ID
   * @param id instance ID.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSupport.java,snapshotIOStatistics,org.apache.hadoop.fs.statistics.IOStatisticsSupport:snapshotIOStatistics(),59,63,"/**
 * Creates and returns a new IOStatisticsSnapshot object.
 */
","* Create a snapshot statistics instance ready to aggregate data.
   *
   * The instance can be serialized, and its
   * {@code toString()} method lists all the values.
   * @return an empty snapshot",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_enabled,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_enabled(),279,281,"/**
 * Checks if IO statistics context is enabled.
 * Returns true if enabled, false otherwise.
 */
","* Static probe to check if the thread-level IO statistics enabled.
   * @return true if the thread-level IO statistics are enabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/BufferedIOStatisticsOutputStream.java,getIOStatistics,org.apache.hadoop.fs.statistics.BufferedIOStatisticsOutputStream:getIOStatistics(),86,89,"/**
 * Retrieves and returns IO statistics.
 * Delegates to retrieveIOStatistics with the output stream.
 */
","* Ask the inner stream for their IOStatistics.
   * @return any IOStatistics offered by the inner stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/BufferedIOStatisticsInputStream.java,getIOStatistics,org.apache.hadoop.fs.statistics.BufferedIOStatisticsInputStream:getIOStatistics(),64,67,"/**
 * Retrieves IO statistics using the input stream.
 * @return IOStatistics object containing the statistics.
 */
","* Return any IOStatistics offered by the inner stream.
   * @return inner IOStatistics or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,getIOStatistics,org.apache.hadoop.fs.BufferedFSInputStream:getIOStatistics(),156,159,"/**
 * Retrieves IO statistics using the input stream.
 * @return IOStatistics object containing the statistics.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getIOStatistics,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:getIOStatistics(),315,318,"/**
 * Retrieves IO statistics from the data source.
 * @return IOStatistics object containing the data.
 */
","* Get the IO Statistics of the nested stream, falling back to
     * null if the stream does not implement the interface
     * {@link IOStatisticsSource}.
     * @return an IOStatistics instance or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getIOStatistics,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:getIOStatistics(),676,679,"/**
 * Retrieves IO statistics from the data source.
 * @return IOStatistics object containing the statistics.
 */
","* Get the IO Statistics of the nested stream, falling back to
     * null if the stream does not implement the interface
     * {@link IOStatisticsSource}.
     * @return an IOStatistics instance or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,getIOStatistics,org.apache.hadoop.fs.FSDataInputStream:getIOStatistics(),289,292,"/**
 * Retrieves IO statistics using the input stream.
 * @return IOStatistics object containing the statistics.
 */
","* Get the IO Statistics of the nested stream, falling back to
   * null if the stream does not implement the interface
   * {@link IOStatisticsSource}.
   * @return an IOStatistics instance or null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,getIOStatistics,org.apache.hadoop.fs.FSDataOutputStream:getIOStatistics(),167,170,"/**
* Retrieves IO statistics for the wrapped stream.
* @return IOStatistics object containing the statistics.
*/
","* Get the IO Statistics of the nested stream, falling back to
   * empty statistics if the stream does not implement the interface
   * {@link IOStatisticsSource}.
   * @return an IOStatistics instance.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionOutputStream.java,getIOStatistics,org.apache.hadoop.io.compress.CompressionOutputStream:getIOStatistics(),107,110,"/**
* Retrieves IO statistics using the underlying output stream.
*/
","* Return any IOStatistics provided by the underlying stream.
   * @return IO stats from the inner stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionInputStream.java,getIOStatistics,org.apache.hadoop.io.compress.CompressionInputStream:getIOStatistics(),81,84,"/**
 * Retrieves IO statistics using the provided input stream.
 * @return IOStatistics object containing the statistics.
 */
","* Return any IOStatistics provided by the underlying stream.
   * @return IO stats from the inner stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,getIOStatistics,org.apache.hadoop.crypto.CryptoInputStream:getIOStatistics(),882,885,"/**
 * Retrieves IO statistics using the input stream.
 * @return IOStatistics object containing the statistics.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,getIOStatistics,org.apache.hadoop.crypto.CryptoOutputStream:getIOStatistics(),321,324,"/**
 * Retrieves IO statistics using the configured output stream.
 * @return IOStatistics object containing IO metrics.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,getIOStatistics,org.apache.hadoop.util.LineReader:getIOStatistics(),159,162,"/**
 * Retrieves IO statistics for the input stream.
 * @return IOStatistics object containing the statistics.
 */
","* Return any IOStatistics provided by the source.
   * @return IO stats from the input stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,getIOStatistics,org.apache.hadoop.util.functional.RemoteIterators$SingletonIterator:getIOStatistics(),351,354,"/**
 * Retrieves IO statistics from the singleton instance.
 * @return IOStatistics object containing IO metrics.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,getIOStatistics,org.apache.hadoop.util.functional.RemoteIterators$WrappedJavaIterator:getIOStatistics(),405,408,"/**
 * Retrieves IO statistics for the current source.
 * @return IOStatistics object containing the statistics.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,getIOStatistics,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:getIOStatistics(),449,452,"/**
* Retrieves IO statistics for the current source.
* @return IOStatistics object containing the statistics.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,verifyChunked,"org.apache.hadoop.util.DataChecksum:verifyChunked(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,java.nio.ByteBuffer,int,java.nio.ByteBuffer,java.lang.String,long)",429,472,"/**
 * Verifies chunked data integrity using CRC checksums.
 * @param data ByteBuffer containing data to verify.
 * @param crcs ByteBuffer holding expected CRC values.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,verifyChunked,"org.apache.hadoop.util.DataChecksum:verifyChunked(org.apache.hadoop.util.DataChecksum$Type,java.util.zip.Checksum,byte[],int,int,int,byte[],int,java.lang.String,long)",478,512,"/**
 * Verifies chunked checksums against provided data and CRC values.
 * @param type Checksum type, algorithm, data, CRC array, filename, basePos
 */
","* Implementation of chunked verification specifically on byte arrays. This
   * is to avoid the copy when dealing with ByteBuffers that have array backing.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,updateDecryptor,"org.apache.hadoop.crypto.CryptoInputStream:updateDecryptor(org.apache.hadoop.crypto.Decryptor,long,byte[])",297,302,"/**
 * Updates the decryptor with new IV and key based on position.
 */","Calculate the counter and iv, update the decryptor.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,encrypt,org.apache.hadoop.crypto.CryptoOutputStream:encrypt(),178,217,"/**
 * Encrypts data from the input buffer to the output stream.
 */","* Do the encryption, input is {@link #inBuffer} and output is 
   * {@link #outBuffer}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BatchedRemoteIterator.java,hasNext,org.apache.hadoop.fs.BatchedRemoteIterator:hasNext(),98,102,"/**
 * Checks if there are more entries to process.
 * Returns true if entries exist, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BatchedRemoteIterator.java,next,org.apache.hadoop.fs.BatchedRemoteIterator:next(),111,120,"/**
 * Returns the next element in the iteration.
 * Throws NoSuchElementException if no element exists.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32GzipFileChecksum.java,<init>,org.apache.hadoop.fs.MD5MD5CRC32GzipFileChecksum:<init>(),27,29,"/**
 * Default constructor for MD5MD5CRC32GzipFileChecksum.
 */","Same as this(0, 0, null)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32CastagnoliFileChecksum.java,<init>,org.apache.hadoop.fs.MD5MD5CRC32CastagnoliFileChecksum:<init>(),27,29,"/**
 * Default constructor for MD5MD5CRC32CastagnoliFileChecksum.
 */
","Same as this(0, 0, null)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobPattern.java,<init>,org.apache.hadoop.fs.GlobPattern:<init>(java.lang.String),41,43,"/**
 * Constructs a GlobPattern with the given glob pattern string.
 */","* Construct the glob pattern object with a glob pattern string
   * @param globPattern the glob pattern string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,makeShellPath,org.apache.hadoop.fs.FileUtil:makeShellPath(java.io.File),668,670,"/**
 * Creates a shell-style path from a File object.
 * @param file The file to create the path from.
 */
","* Convert a os-native filename to a path that works for the shell.
   * @param file The filename to convert
   * @return The unix pathname
   * @throws IOException on windows, there can be problems with the subprocess",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,makeSecureShellPath,org.apache.hadoop.fs.FileUtil:makeSecureShellPath(java.io.File),679,686,"/**
 * Creates a secure shell path for a file (Unix-like systems).
 * @param file The file for which to create the secure path.
 * @throws IOException if an I/O error occurs.
 */
","* Convert a os-native filename to a path that works for the shell
   * and avoids script injection attacks.
   * @param file The filename to convert
   * @return The unix pathname
   * @throws IOException on windows, there can be problems with the subprocess",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HardLink.java,linkCount,org.apache.hadoop.fs.HardLink$HardLinkCGUnix:linkCount(java.io.File),108,116,"/**
 * Constructs a command array to count links in a file.
 * @param file The file to analyze.
 * @return Command array for link counting.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unZip,"org.apache.hadoop.fs.FileUtil:unZip(java.io.InputStream,java.io.File)",739,775,"/**
 * Extracts a zip file to the specified directory.
 * @param inputStream Input stream of the zip file.
 * @param toDir Directory to extract the zip file to.
 */
","* Given a stream input it will unzip the it in the unzip directory.
   * passed as the second parameter
   * @param inputStream The zip file as input
   * @param toDir The unzip directory where to unzip the zip file.
   * @throws IOException an exception occurred",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unZip,"org.apache.hadoop.fs.FileUtil:unZip(java.io.File,java.io.File)",827,871,"/**
 * Unzips a ZIP file to a specified directory.
 * @param inFile ZIP file to unzip.
 * @param unzipDir Directory to unzip to.
 */
","* Given a File input it will unzip it in the unzip directory.
   * passed as the second parameter
   * @param inFile The zip file as input
   * @param unzipDir The unzip directory where to unzip the zip file.
   * @throws IOException An I/O exception has occurred",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingJava,"org.apache.hadoop.fs.FileUtil:unTarUsingJava(java.io.File,java.io.File,boolean)",1086,1109,"/**
 * Extracts tar archive entries to a directory.
 * @param inFile Input tar file.
 * @param untarDir Target directory for extraction.
 * @param gzipped True if the tar file is gzipped.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingJava,"org.apache.hadoop.fs.FileUtil:unTarUsingJava(java.io.InputStream,java.io.File,boolean)",1111,1128,"/**
 * Extracts tar archive entries to a directory.
 * @param inputStream Input stream of the tar archive.
 * @param untarDir Directory to extract entries to.
 * @param gzipped True if the archive is gzipped.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/PowerShellFencer.java,tryFence,"org.apache.hadoop.ha.PowerShellFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",57,105,"/**
 * Attempts to fence a remote process using a PowerShell script.
 * @param target HAServiceTarget - target address
 * @param argsStr Process name to fence
 * @return True if fencing was successful, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,toString,org.apache.hadoop.fs.permission.FsCreateModes:toString(),82,86,"/**
 * Returns a string representation of the object.
 * Includes masked and unmasked values from the superclass.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,disconnect,org.apache.hadoop.fs.sftp.SFTPFileSystem:disconnect(com.jcraft.jsch.ChannelSftp),165,167,"/**
 * Disconnects an SFTP channel from the connection pool.
 * @param channel The SFTP channel to disconnect.
 * @throws IOException if an I/O error occurs during disconnection.
 */
","* Logout and disconnect the given channel.
   *
   * @param client
   * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPConnectionPool.java,shutdown,org.apache.hadoop.fs.sftp.SFTPConnectionPool:shutdown(),87,113,"/**
 * Shuts down the SFTP server, disconnecting all active connections.
 */",Shutdown the connection pool and close all open connections.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,<init>,"org.apache.hadoop.fs.FSDataOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.fs.FileSystem$Statistics)",82,84,"/**
 * Constructs a FSDataOutputStream with statistics.
 * @param out The underlying OutputStream.
 * @param stats Statistics object for tracking operations.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,<init>,org.apache.hadoop.fs.FSOutputSummer:<init>(org.apache.hadoop.util.DataChecksum),53,58,"/**
 * Initializes the FSOutputSummer with a DataChecksum object.
 * @param sum The DataChecksum object used for initialization.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,setChecksumBufSize,org.apache.hadoop.fs.FSOutputSummer:setChecksumBufSize(int),257,261,"/**
* Sets the checksum buffer size.
* @param size The new size of the checksum buffer.
*/
","* Resets existing buffer with a new one of the specified size.
   *
   * @param size size.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,read,org.apache.hadoop.fs.sftp.SFTPInputStream:read(),108,124,"/**
 * Reads a single byte from the wrapped stream.
 * Returns -1 if end of stream is reached.
 * @throws IOException if an I/O error occurs
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPInputStream.java,read,org.apache.hadoop.fs.ftp.FTPInputStream:read(),70,84,"/**
 * Reads a byte from the stream.
 * @return Byte read or -1 if EOF. Throws IOException if closed.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPInputStream.java,read,"org.apache.hadoop.fs.ftp.FTPInputStream:read(byte[],int,int)",86,101,"/**
 * Reads bytes from the wrapped stream into the provided buffer.
 * @param buf buffer to read into, @param off offset, @param len count
 * @return number of bytes read, or -1 if end of stream
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,read,org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(),217,231,"/**
 * Reads a single byte from the input stream.
 * @return byte value or -1 if end of stream is reached.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,read,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(byte[],int,int)",233,249,"/**
* Reads bytes from the input stream.
* @param b buffer to read into, off offset, len length
* @return number of bytes read, or -1 if EOF
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,read,"org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileInputStream:read(long,byte[],int,int)",251,272,"/**
 * Reads bytes from the stream at a specific position.
 * @param position byte offset
 * @param b buffer
 * @param off offset into buffer
 * @param len number of bytes to read
 * @return number of bytes read
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,write,org.apache.hadoop.fs.FSDataOutputStream$PositionCache:write(int),51,58,"/**
 * Writes a byte to the output stream and updates statistics.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,write,"org.apache.hadoop.fs.FSDataOutputStream$PositionCache:write(byte[],int,int)",60,67,"/**
 * Writes bytes from a byte array to the underlying output stream.
 * @param b The byte array to write.
 * @param off Offset within the array.
 * @param len Number of bytes to write.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,org.apache.hadoop.fs.FileSystem$Statistics:<init>(org.apache.hadoop.fs.FileSystem$Statistics),4110,4125,"/**
 * Creates a Statistics object as a copy of another Statistics object.
 */","* Copy constructor.
     *
     * @param other    The input Statistics object which is cloned.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesRead,org.apache.hadoop.fs.FileSystem$Statistics:getBytesRead(),4308,4321,"/**
 * Returns the total number of bytes read.
 * Uses StatisticsAggregator to sum bytesRead from data.
 */
","* Get the total number of bytes read.
     * @return the number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesWritten,org.apache.hadoop.fs.FileSystem$Statistics:getBytesWritten(),4327,4340,"/**
 * Returns the total number of bytes written.
 * Uses StatisticsAggregator to sum bytesWritten from data.
 */
","* Get the total number of bytes written.
     * @return the number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getReadOps,org.apache.hadoop.fs.FileSystem$Statistics:getReadOps(),4346,4360,"/**
 * Calculates the total number of read operations.
 * Uses StatisticsAggregator to sum readOps and largeReadOps.
 */
","* Get the number of file system read operations such as list files.
     * @return number of read operations",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getLargeReadOps,org.apache.hadoop.fs.FileSystem$Statistics:getLargeReadOps(),4367,4380,"/**
 * Returns the total number of large read operations.
 * Uses StatisticsAggregator to sum largeReadOps from data.
 */
","* Get the number of large file system read operations such as list files
     * under a large directory.
     * @return number of large read operations",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getWriteOps,org.apache.hadoop.fs.FileSystem$Statistics:getWriteOps(),4387,4400,"/**
 * Returns the total number of write operations.
 * Uses StatisticsAggregator to sum writeOps from data.
 */
","* Get the number of file system write operations such as create, append
     * rename etc.
     * @return number of write operations",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getRemoteReadTime,org.apache.hadoop.fs.FileSystem$Statistics:getRemoteReadTime(),4436,4449,"/**
 * Calculates the total remote read time in milliseconds.
 * Uses StatisticsAggregator to sum remoteReadTimeMS from data.
 */
","* Get total time taken in ms for bytes read from remote.
     * @return time taken in ms for remote bytes read.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getData,org.apache.hadoop.fs.FileSystem$Statistics:getData(),4456,4469,"/**
 * Calculates and returns aggregated statistics data.
 * Uses StatisticsAggregator to combine data.
 */
","* Get all statistics data.
     * MR or other frameworks can use the method to get all statistics at once.
     * @return the StatisticsData",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesReadErasureCoded,org.apache.hadoop.fs.FileSystem$Statistics:getBytesReadErasureCoded(),4475,4488,"/**
 * Returns the total bytes read erasure coded.
 * Uses StatisticsAggregator to sum bytesReadErasureCoded.
 */
","* Get the total number of bytes read on erasure-coded files.
     * @return the number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,toString,org.apache.hadoop.fs.FileSystem$Statistics:toString(),4490,4504,"/**
 * Returns a string representation of the statistics.
 * Uses StatisticsAggregator to aggregate and format data.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,reset,org.apache.hadoop.fs.FileSystem$Statistics:reset(),4524,4539,"/**
 * Resets statistics by aggregating and negating data.
 */
","* Resets all statistics to 0.
     *
     * In order to reset, we add up all the thread-local statistics data, and
     * set rootData to the negative of that.
     *
     * This may seem like a counterintuitive way to reset the statistics.  Why
     * can't we just zero out all the thread-local data?  Well, thread-local
     * data can only be modified by the thread that owns it.  If we tried to
     * modify the thread-local data from this thread, our modification might get
     * interleaved with a read-modify-write operation done by the thread that
     * owns the data.  That would result in our update getting lost.
     *
     * The approach used here avoids this problem because it only ever reads
     * (not writes) the thread-local data.  Both reads and writes to rootData
     * are done under the lock, so we're free to modify rootData from any thread
     * that holds the lock.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,toString,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:toString(),113,116,"/**
 * Returns a string representation of the object.
 * Includes algorithm name and MD5 hash.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CreateFlag.java,validate,"org.apache.hadoop.fs.CreateFlag:validate(java.lang.Object,boolean,java.util.EnumSet)",172,187,"/**
 * Validates file creation flags.
 * @param path The file path.
 * @param pathExists Whether the path already exists.
 * @param flag Creation flags.
 */
","* Validate the CreateFlag for create operation
   * @param path Object representing the path; usually String or {@link Path}
   * @param pathExists pass true if the path exists in the file system
   * @param flag set of CreateFlag
   * @throws IOException on error
   * @throws HadoopIllegalArgumentException if the CreateFlag is invalid",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CreateFlag.java,validateForAppend,org.apache.hadoop.fs.CreateFlag:validateForAppend(java.util.EnumSet),195,201,"/**
 * Validates EnumSet for appending; throws exception if missing APPEND.
 */","* Validate the CreateFlag for the append operation. The flag must contain
   * APPEND, and cannot contain OVERWRITE.
   *
   * @param flag enum set flag.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getUri,"org.apache.hadoop.fs.AbstractFileSystem:getUri(java.net.URI,java.lang.String,boolean,int)",316,341,"/**
 * Constructs a URI based on input URI, scheme, authority needs, and default port.
 */","* Get the URI for the file system based on the given URI. The path, query
   * part of the given URI is stripped out and default file system port is used
   * to form the URI.
   * 
   * @param uri FileSystem URI.
   * @param authorityNeeded if true authority cannot be null in the URI. If
   *          false authority must be null.
   * @param defaultPort default port to use if port is not specified in the URI.
   * 
   * @return URI of the file system
   * 
   * @throws URISyntaxException <code>uri</code> has syntax error",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecodeImpl,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecodeImpl(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",85,96,"/**
 * Decodes data using provided inputs, erased indexes, and outputs.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayEncodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,byte[][],byte[][])",36,50,"/**
 * Initializes the ByteArrayEncodingState with encoder, inputs, and outputs.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferEncodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,java.nio.ByteBuffer[],java.nio.ByteBuffer[])",35,47,"/**
 * Initializes the encoding state with encoder, inputs, and outputs.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable:<init>(java.lang.Class),113,116,"/**
 * Creates a new ArrayPrimitiveWritable for the given primitive type.
 * @param componentType The Class object of the primitive type.
 */
","* Construct an instance of known type but no value yet
   * for use with type-specific wrapper classes.
   *
   * @param componentType componentType.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,set,org.apache.hadoop.io.ArrayPrimitiveWritable:set(java.lang.Object),142,150,"/**
 * Sets the component type and value of the array.
 * @param value The array to be set.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/DefaultFailoverProxyProvider.java,close,org.apache.hadoop.io.retry.DefaultFailoverProxyProvider:close(),55,58,"/**
 * Closes the connection by stopping the proxy.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:close(),49,52,"/**
 * Closes the connection by stopping the RPC proxy.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:close(),54,57,"/**
 * Closes the connection by stopping the RPC proxy.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:close(),52,55,"/**
 * Closes the connection by stopping the RPC proxy.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:close(),49,52,"/**
 * Closes the connection by stopping the RPC proxy.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:close(),47,50,"/**
 * Closes the connection by stopping the RPC proxy.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:close(),72,75,"/**
 * Closes the connection by stopping the RPC proxy.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,close,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:close(),165,168,"/**
 * Closes the connection by stopping the RPC proxy.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,getPermFromString,org.apache.hadoop.util.ZKUtil:getPermFromString(java.lang.String),44,71,"/**
 * Parses permission string into an integer representation.
 * @param permString String of permissions (r,w,c,d,a)
 * @return Integer representing combined permissions.
 */
","* Parse ACL permission string, partially borrowed from
   * ZooKeeperMain private method",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,processChecksumOpt,"org.apache.hadoop.fs.Options$ChecksumOpt:processChecksumOpt(org.apache.hadoop.fs.Options$ChecksumOpt,org.apache.hadoop.fs.Options$ChecksumOpt)",339,342,"/**
 * Processes ChecksumOpt, merging default and user options.
 * @param defaultOpt Default ChecksumOpt.
 * @param userOpt User-provided ChecksumOpt.
 */
","* A helper method for processing user input and default value to 
     * create a combined checksum option. 
     *
     * @param defaultOpt Default checksum option
     * @param userOpt User-specified checksum option
     *
     * @return ChecksumOpt.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getUriDefaultPort,org.apache.hadoop.fs.DelegateToFileSystem:getUriDefaultPort(),174,177,"/**
 * Gets the default port for the URI.
 * Delegates to getDefaultPortIfDefined.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,canonicalizeUri,org.apache.hadoop.fs.HarFileSystem:canonicalizeUri(java.net.URI),323,326,"/**
 * Canonicalizes a URI using the underlying filesystem's canonicalizer.
 * @param uri The URI to canonicalize.
 * @return The canonicalized URI.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getCanonicalUri,org.apache.hadoop.fs.FileSystem:getCanonicalUri(),383,385,"/**
 * Returns the canonicalized URI.
 * Canonicalizes the URI obtained from getUri().
 */
","* Return a canonicalized form of this FileSystem's URI.
   *
   * The default implementation simply calls {@link #canonicalizeUri(URI)}
   * on the filesystem's own URI, so subclasses typically only need to
   * implement that method.
   *
   * @see #canonicalizeUri(URI)
   * @return the URI of this filesystem.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,canonicalizeUri,org.apache.hadoop.fs.FilterFileSystem:canonicalizeUri(java.net.URI),118,121,"/**
 * Canonicalizes a URI using the underlying file system.
 * @param uri The URI to canonicalize.
 * @return The canonicalized URI.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,<init>,"org.apache.hadoop.fs.ContentSummary:<init>(long,long,long)",162,165,"/**
 * Constructs a ContentSummary with default values for some fields.
 * @param length Total length of content.
 * @param fileCount Number of files.
 * @param directoryCount Number of directories.
 */
","*  Constructor, deprecated by ContentSummary.Builder
   *  This constructor implicitly set spaceConsumed the same as length.
   *  spaceConsumed and length must be set explicitly with
   *  ContentSummary.Builder.
   *
   * @param length length.
   * @param fileCount file count.
   * @param directoryCount directory count.
   *",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,toString,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:toString(),160,166,"/**
 * Returns a string representation of the token renewal status.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,<init>,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:<init>(org.apache.hadoop.fs.FileSystem),71,75,"/**
* Constructs a RenewAction with a WeakReference to the feeder.
* @param fs Feeder object; stored in a WeakReference.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getStatus,org.apache.hadoop.fs.HarFileSystem:getStatus(org.apache.hadoop.fs.Path),283,286,"/**
 * Gets the status of a file or directory.
 * @param p Path to the file or directory.
 * @return FsStatus object representing the status.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFsStatus,org.apache.hadoop.fs.DelegateToFileSystem:getFsStatus(org.apache.hadoop.fs.Path),153,156,"/**
 * Gets the file system status for the given path.
 * @param f the path to get the status for
 * @return FsStatus object representing the status
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getStatus,org.apache.hadoop.fs.FileSystem:getStatus(),3026,3028,"/**
 * Gets the status of the file system.
 * @return FsStatus object representing the file system status.
 */
","* Returns a status object describing the use and capacity of the
   * filesystem. If the filesystem has multiple partitions, the
   * use and capacity of the root partition is reflected.
   *
   * @return a FsStatus object
   * @throws IOException
   *           see specific implementation",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getStatus,org.apache.hadoop.fs.FilterFileSystem:getStatus(org.apache.hadoop.fs.Path),329,332,"/**
 * Gets the status of a file or directory.
 * @param p Path to the file or directory.
 * @return FsStatus object representing the status.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/CombinedFileRange.java,toString,org.apache.hadoop.fs.impl.CombinedFileRange:toString(),91,96,"/**
 * Returns a string representation of the object.
 * Includes range count and data size.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/audit/HttpReferrerAuditHeader.java,<init>,org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader:<init>(org.apache.hadoop.fs.store.audit.HttpReferrerAuditHeader$Builder),151,180,"/**
 * Constructs an HttpReferrerAuditHeader from a builder, initializing its attributes.
 */
","* Instantiate.
   * <p>
   * All maps/enums passed down are copied into thread safe equivalents.
   * as their origin is unknown and cannot be guaranteed to
   * not be shared.
   * <p>
   * Context and operationId are expected to be well formed
   * numeric/hex strings, at least adequate to be
   * used as individual path elements in a URL.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/WeakReferenceThreadMap.java,<init>,"org.apache.hadoop.fs.impl.WeakReferenceThreadMap:<init>(java.util.function.Function,java.util.function.Consumer)",36,39,"/**
 * Constructs a WeakReferenceThreadMap with a factory and lost ref handler.
 * @param factory Creates values for keys.
 * @param referenceLost Handles lost references.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStream.java,hasCapability,org.apache.hadoop.fs.FSDataOutputStream:hasCapability(java.lang.String),128,131,"/**
 * Checks if the stream has the specified capability.
 * @param capability Capability to check for.
 * @return True if the stream has the capability.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,hasCapability,org.apache.hadoop.crypto.CryptoOutputStream:hasCapability(java.lang.String),316,319,"/**
 * Checks if the store has the specified capability.
 * @param capability Capability to check for.
 * @return True if the store has the capability, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,hasCapability,org.apache.hadoop.fs.FSDataInputStream:hasCapability(java.lang.String),242,245,"/**
 * Checks if the store has the specified capability.
 * @param capability Capability to check for.
 * @return True if the store has the capability, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getPrefetched,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getPrefetched(int),168,172,"/**
 * Retrieves a pre-fetched operation for the given block number.
 * @param blockNumber The block number to prefetch.
 * @return An Operation object representing the prefetch request.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getCached,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getCached(int),174,178,"/**
 * Retrieves a cached operation for the given block number.
 * @param blockNumber The block number to retrieve.
 * @return An Operation object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getRead,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getRead(int),180,184,"/**
 * Retrieves a GET_READ operation for the specified block number.
 * @param blockNumber Block number for the read operation.
 * @return Operation object representing the read operation.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,release,org.apache.hadoop.fs.impl.prefetch.BlockOperations:release(int),186,190,"/**
 * Releases a block.
 * @param blockNumber The block to release.
 * @return The operation representing the release action.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,requestPrefetch,org.apache.hadoop.fs.impl.prefetch.BlockOperations:requestPrefetch(int),192,196,"/**
 * Requests a prefetch operation for the given block number.
 * @param blockNumber The block number to prefetch.
 * @return The created operation object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,prefetch,org.apache.hadoop.fs.impl.prefetch.BlockOperations:prefetch(int),198,202,"/**
 * Prefetches a block.
 * @param blockNumber The block to prefetch.
 * @return An Operation object representing the prefetch.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,cancelPrefetches,org.apache.hadoop.fs.impl.prefetch.BlockOperations:cancelPrefetches(),204,206,"/**
 * Cancels prefetch operations. Returns an Operation object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,close,org.apache.hadoop.fs.impl.prefetch.BlockOperations:close(),208,210,"/**
 * Closes the current operation. Returns the updated operation.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,requestCaching,org.apache.hadoop.fs.impl.prefetch.BlockOperations:requestCaching(int),212,216,"/**
 * Requests caching for a block.
 * @param blockNumber The block number to cache.
 * @return The created operation.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,addToCache,org.apache.hadoop.fs.impl.prefetch.BlockOperations:addToCache(int),218,222,"/**
 * Adds a cache put operation for the given block number.
 * @param blockNumber The block number to cache.
 * @return The created Operation object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,end,org.apache.hadoop.fs.impl.prefetch.BlockOperations:end(org.apache.hadoop.fs.impl.prefetch.BlockOperations$Operation),224,226,"/**
 * Ends the operation sequence by adding an End object.
 * @param op The operation to end.
 * @return The updated operation sequence.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,fromSummary,org.apache.hadoop.fs.impl.prefetch.BlockOperations:fromSummary(java.lang.String),377,424,"/**
 * Parses block operation summary string and creates BlockOperations.
 * @param summary Summary string containing block operation tokens.
 * @return BlockOperations object representing parsed operations.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,release,org.apache.hadoop.fs.impl.prefetch.BufferPool:release(org.apache.hadoop.fs.impl.prefetch.BufferData),236,255,"/**
 * Releases a buffer from the pool.
 * @param data BufferData object to release.
 */
","* Releases a previously acquired resource.
   * @param data the {@code BufferData} instance to release.
   * @throws IllegalArgumentException if data is null.
   * @throws IllegalArgumentException if data cannot be released due to its state.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BoundedResourcePool.java,toString,org.apache.hadoop.fs.impl.prefetch.BoundedResourcePool:toString(),153,158,"/**
 * Returns a string representation of the object's state.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getDurationInfo,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getDurationInfo(java.lang.StringBuilder),252,295,"/**
 * Gathers duration info for each operation kind and appends to sb.
 * @param sb StringBuilder to append duration statistics to.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,createCache,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:createCache(int,org.apache.hadoop.fs.statistics.DurationTrackerFactory)",554,556,"/**
 * Creates a BlockCache instance with specified parameters.
 * @param maxBlocksCount Maximum number of blocks in cache.
 * @param trackerFactory Factory for DurationTracker.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,<init>,"org.apache.hadoop.util.SemaphoredDelegatingExecutor:<init>(java.util.concurrent.ExecutorService,int,boolean)",90,95,"/**
 * Constructs a SemaphoredDelegatingExecutor with null signature.
 * @param executorDelegatee ExecutorService to delegate to.
 * @param permitCount Initial number of permits.
 * @param fair Whether permits are allocated fairly.
 */
","* Instantiate without collecting executor aquisition duration information.
   * @param executorDelegatee Executor to delegate to
   * @param permitCount number of permits into the queue permitted
   * @param fair should the semaphore be ""fair""",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,getEntry,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getEntry(int),297,307,"/**
 * Retrieves an entry from the cache by block number.
 * @param blockNumber Block number to retrieve.
 * @return The Entry object or throws exception if not found.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,releaseReadyBlock,org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseReadyBlock(int),205,224,"/**
* Releases a READY buffer block with the greatest distance.
* @param blockNumber The block number to calculate distance from.
*/
","* If no blocks were released after calling releaseDoneBlocks() a few times,
   * we may end up waiting forever. To avoid that situation, we try releasing
   * a 'ready' block farthest away from the given block.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,toString,org.apache.hadoop.fs.impl.prefetch.BufferPool:toString(),278,292,"/**
 * Generates a string representation of the pool and its data.
 * Includes sorted buffer data, each on a new line.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,buffer,org.apache.hadoop.fs.impl.prefetch.FilePosition:buffer(),130,133,"/**
 * Returns the buffer associated with this object.
 * Throws an exception if the buffer is invalid.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,data,org.apache.hadoop.fs.impl.prefetch.FilePosition:data(),135,138,"/**
 * Returns the buffer data. Throws an exception if buffer is invalid.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,relative,org.apache.hadoop.fs.impl.prefetch.FilePosition:relative(),172,175,"/**
 * Returns the current buffer position.
 * Throws exception if buffer is invalid.
 */
","* Gets the current position within this file relative to the start of the associated buffer.
   *
   * @return the current position within this file relative to the start of the associated buffer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,isWithinCurrentBuffer,org.apache.hadoop.fs.impl.prefetch.FilePosition:isWithinCurrentBuffer(long),183,187,"/**
 * Checks if the given position is within the current buffer.
 * @param pos The position to check.
 * @return True if within buffer, false otherwise.
 */
","* Determines whether the given absolute position lies within the current buffer.
   *
   * @param pos the position to check.
   * @return true if the given absolute position lies within the current buffer, false otherwise.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,bufferStartOffset,org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferStartOffset(),231,234,"/**
 * Returns the start offset of the buffer.
 * @return The buffer's starting offset as a long.
 */
","* Gets the start of the current block's absolute offset.
   *
   * @return the start of the current block's absolute offset.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextIntegration.java,getCurrentIOStatisticsContext,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:getCurrentIOStatisticsContext(),122,126,"/**
 * Returns the current IO statistics context, or an empty one if disabled.
 */
","* Get the current thread's IOStatisticsContext instance. If no instance is
   * present for this thread ID, create one using the factory.
   * @return instance of IOStatisticsContext.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextIntegration.java,setThreadIOStatisticsContext,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:setThreadIOStatisticsContext(org.apache.hadoop.fs.statistics.IOStatisticsContext),133,145,"/**
 * Sets the IO statistics context for the current thread.
 * @param statisticsContext The IOStatisticsContext to set.
 */
","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,mergeSortedRanges,"org.apache.hadoop.fs.VectoredReadUtils:mergeSortedRanges(java.util.List,int,int,int)",380,398,"/**
 * Merges sorted file ranges into combined ranges.
 * @param sortedRanges List of sorted file ranges to merge.
 * @return List of combined file ranges.
 */
","* Merge sorted ranges to optimize the access from the underlying file
   * system.
   * The motivations are that:
   * <ul>
   *   <li>Upper layers want to pass down logical file ranges.</li>
   *   <li>Fewer reads have better performance.</li>
   *   <li>Applications want callbacks as ranges are read.</li>
   *   <li>Some file systems want to round ranges to be at checksum boundaries.</li>
   * </ul>
   *
   * @param sortedRanges already sorted list of ranges based on offset.
   * @param chunkSize round the start and end points to multiples of chunkSize
   * @param minimumSeek the smallest gap that we should seek over in bytes
   * @param maxSize the largest combined file range in bytes
   * @return the list of sorted CombinedFileRanges that cover the input",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,findChecksumRanges,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:findChecksumRanges(java.util.List,int,int,int)",344,362,"/**
 * Finds combined checksum ranges from a list of file ranges.
 * @param dataRanges List of file ranges to process.
 * @param bytesPerSum Checksum size in bytes.
 * @param minSeek Minimum seek distance.
 * @param maxSize Maximum size of a range.
 * @return List of combined checksum ranges.
 */
","* Find the checksum ranges that correspond to the given data ranges.
     * @param dataRanges the input data ranges, which are assumed to be sorted
     *                   and non-overlapping
     * @return a list of AsyncReaderUtils.CombinedFileRange that correspond to
     *         the checksum ranges",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,<init>,org.apache.hadoop.fs.shell.find.Name:<init>(),48,50,"/**
 * Default constructor. Calls the parameterized constructor with true.
 */
",Creates a case sensitive name expression.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,subset,org.apache.hadoop.metrics2.impl.MetricsConfig:subset(java.lang.String),144,147,"/**
 * Creates a new MetricsConfig with a specified prefix.
 * @param prefix The prefix for the new MetricsConfig.
 * @return A new MetricsConfig object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/And.java,apply,"org.apache.hadoop.fs.shell.find.And:apply(org.apache.hadoop.fs.shell.PathData,int)",57,68,"/**
 * Applies the expression to the item, recursively.
 * @param item PathData to apply to; depth is ignored.
 * @return Result of applying expression; fails on child failure.
 */
","* Applies child expressions to the {@link PathData} item. If all pass then
   * returns {@link Result#PASS} else returns the result of the first
   * non-passing expression.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,getOptions,org.apache.hadoop.fs.shell.find.Find:getOptions(),235,241,"/**
 * Returns the FindOptions object, creating it if it's null.
 */
","Returns the current find options, creating them if necessary.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,parse,org.apache.hadoop.fs.shell.CommandFormat:parse(java.util.List),99,140,"/**
 * Parses command-line arguments, handling options and values.
 * @param args List of command-line arguments to parse.
 */
","Parse parameters from the given list of args.  The list is
   *  destructively modified to remove the options.
   * 
   * @param args as a list of input arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,getDescription,org.apache.hadoop.fs.shell.Command:getDescription(),547,551,"/**
 * Returns the command description, indicating deprecation if applicable.
 */","* The long usage suitable for help output
   * @return text of the usage",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,displayWarning,org.apache.hadoop.fs.shell.Command:displayWarning(java.lang.String),510,512,"/**
 * Displays a warning message to the error stream.
 * @param message The warning message to display.
 */
","* Display an warning string prefaced with the command name.
   * @param message warning message to display",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,getUsage,org.apache.hadoop.fs.shell.Command:getUsage(),537,541,"/**
* Returns the command usage string. Combines name if usage is empty.
*/
","* The short usage suitable for the synopsis
   * @return ""name options""",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource,java.lang.Iterable,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,long,boolean)",71,88,"/**
 * Constructs a MetricsSourceAdapter with provided configuration.
 * @param prefix Metric name prefix.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsCollectorImpl.java,addRecord,org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:addRecord(org.apache.hadoop.metrics2.MetricsInfo),42,50,"/**
 * Adds a metrics record builder for the given info.
 * @param info MetricsInfo object containing record details.
 * @return The newly created MetricsRecordBuilderImpl.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,<init>,org.apache.hadoop.util.ChunkedArrayList:<init>(),94,96,"/**
 * Constructs a ChunkedArrayList with default initial chunk capacity.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ScopedAclEntries.java,<init>,org.apache.hadoop.fs.permission.ScopedAclEntries:<init>(java.util.List),47,57,"/**
 * Initializes ScopedAclEntries, splitting entries into access/default.
 * @param aclEntries List of AclEntry objects to be split.
 */
","* Creates a new ScopedAclEntries from the given list.  It is assumed that the
   * list is already sorted such that all access entries precede all default
   * entries.
   *
   * @param aclEntries List&lt;AclEntry&gt; to separate",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,printToStream,org.apache.hadoop.fs.shell.FsUsage$TableBuilder:printToStream(java.io.PrintStream),312,334,"/**
 * Prints the data rows to the given PrintStream, formatted.
 * @param out PrintStream to write the formatted data to.
 */","* Render the table to a stream.
     * @param out PrintStream for output",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,moved,org.apache.hadoop.fs.Options$HandleOpt:moved(boolean),432,434,"/**
 * Creates a new Location object.
 * @param allow boolean value to initialize the Location.
 * @return A new Location object.
 */
","* @param allow If true, resolve references to this entity anywhere in
     *              the namespace.
     * @return Handle option encoding parameter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,changed,org.apache.hadoop.fs.Options$HandleOpt:changed(boolean),423,425,"/**
 * Creates a new Data object with the given allow flag.
 */","* @param allow If true, resolve references to this entity even if it has
     *             been modified.
     * @return Handle option encoding parameter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,<init>,"org.apache.hadoop.fs.DF:<init>(java.io.File,long)",54,59,"/**
 * Constructs a DirectoryFormatter with given path and interval.
 * @param path Directory path.
 * @param dfInterval Interval for directory format.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell:<init>(),901,903,"/**
 * Default constructor. Calls the long-argument constructor with 0.
 */","* Create an instance with no minimum interval between runs; stderr is
   * not merged with stdout.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,<init>,"org.apache.hadoop.fs.CachingGetSpaceUsed$RefreshThread:<init>(org.apache.hadoop.fs.CachingGetSpaceUsed,boolean)",204,207,"/**
 * Constructs a RefreshThread with spaceUsed and runImmediately.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,"org.apache.hadoop.security.token.Token$PrivateToken:<init>(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",255,263,"/**
 * Constructs a PrivateToken by cloning a public token.
 * @param publicToken The public token to clone.
 * @param newService The new service for the private token.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,generateDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:generateDelegationToken(org.apache.hadoop.security.token.Token),1144,1153,"/**
 * Creates a delegation token from a given token.
 * @param dToken The token to create a delegation token from.
 * @return A new DelegationTokenAuthenticatedURL.Token object.
 */
","* Generate a DelegationTokenAuthenticatedURL.Token from the given generic
   * typed delegation token.
   *
   * @param dToken The delegation token.
   * @return The DelegationTokenAuthenticatedURL.Token, with its delegation
   *         token set to the delegation token passed in.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:listLocatedStatus(org.apache.hadoop.fs.Path),847,852,"/**
 * Lists located status for a path.
 * @param f the path to list
 * @return RemoteIterator of LocatedFileStatus
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,listLocatedStatus,org.apache.hadoop.fs.FilterFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),284,288,"/**
 * Lists located status for a given path.
 * @param f the path to list
 * @return RemoteIterator of LocatedFileStatus objects
 */
",List files and its block locations in a directory.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setVerifyChecksum,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setVerifyChecksum(boolean),1368,1372,"/**
 * Sets verify checksum flag. Throws AccessControlException.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFileChecksum,org.apache.hadoop.fs.DelegateToFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),124,128,"/**
 * Gets the file checksum for a given path.
 * @param f Path to the file.
 * @return FileChecksum object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileChecksum,org.apache.hadoop.fs.FilterFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),502,505,"/**
 * Gets the file checksum for a given file path.
 * @param f the path to the file
 * @return FileChecksum object
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processPath,org.apache.hadoop.fs.shell.XAttrCommands$SetfattrCommand:processPath(org.apache.hadoop.fs.shell.PathData),179,186,"/**
 * Processes a PathData item, setting/removing extended attributes.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,setXAttr,"org.apache.hadoop.fs.FilterFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",623,627,"/**
 * Sets an extended attribute on a path.
 * @param path Path to set the attribute on.
 * @param name Attribute name.
 * @param value Attribute value.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.DelegateToFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",282,285,"/**
 * Opens a file with specified options.
 * @param path Path to the file.
 * @param parameters Open file parameters.
 * @return CompletableFuture wrapping the FSDataInputStream.
 */
","* Open a file by delegating to
   * {@link FileSystem#openFileWithOptions(Path, org.apache.hadoop.fs.impl.OpenFileParameters)}.
   * @param path path to the file
   * @param parameters open file parameters from the builder.
   *
   * @return a future which will evaluate to the opened file.ControlAlpha
   * @throws IOException failure to resolve the link.
   * @throws IllegalArgumentException unknown mandatory key",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FilterFileSystem:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",721,726,"/**
 * Opens a file with specified options.
 * @param path Path to the file.
 * @param parameters Open file parameters.
 * @return CompletableFuture wrapping the FSDataInputStream.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,openFileWithOptions,"org.apache.hadoop.fs.FilterFs:openFileWithOptions(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.OpenFileParameters)",444,449,"/**
 * Opens a file with specified options.
 * @param path Path to the file.
 * @param parameters Open file parameters.
 * @return CompletableFuture wrapping the FSDataInputStream.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFileWithOptions,"org.apache.hadoop.fs.FilterFileSystem:openFileWithOptions(org.apache.hadoop.fs.PathHandle,org.apache.hadoop.fs.impl.OpenFileParameters)",728,733,"/**
 * Opens a file with specified options.
 * @param pathHandle Path to the file.
 * @param parameters Open file parameters.
 * @return CompletableFuture wrapping the FSDataInputStream.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDirLink:<init>(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink)",249,252,"/**
 * Constructs a directory link node.
 * @param pathToNode Path to the node.
 * @param aUgi UserGroupInformation.
 * @param link INodeLink object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,addDir,"org.apache.hadoop.fs.viewfs.InodeTree$INodeDir:addDir(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",211,220,"/**
 * Adds a directory to the file system.
 * @param pathComponent Directory name.
 * @param aUgi UserGroupInformation.
 * @return INodeDir representing the new directory.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getChildFileSystems,org.apache.hadoop.fs.viewfs.ViewFileSystem:getChildFileSystems(),1035,1058,"/**
 * Retrieves child FileSystem objects.
 * Returns an array of FileSystem objects.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getFallbackFileSystem,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getFallbackFileSystem(),400,411,"/**
 * Gets the fallback FileSystem, or null if unavailable.
 * Retrieves the FileSystem linked to fsState.
 */
","* @return Gets the fallback file system configured. Usually, this will be the
   * default cluster.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,addCall,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:addCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall),118,124,"/**
 * Adds an AsyncCall to the queue and attempts to start processing.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,update,"org.apache.hadoop.crypto.OpensslCipher:update(java.nio.ByteBuffer,java.nio.ByteBuffer)",231,241,"/**
 * Updates the buffer content.
 * @param input Input buffer.
 * @param output Output buffer.
 * @return Number of bytes written to the output buffer.
 */
","* Continues a multiple-part encryption or decryption operation. The data
   * is encrypted or decrypted, depending on how this cipher was initialized.
   * <p>
   * 
   * All <code>input.remaining()</code> bytes starting at 
   * <code>input.position()</code> are processed. The result is stored in
   * the output buffer.
   * <p>
   * 
   * Upon return, the input buffer's position will be equal to its limit;
   * its limit will not have changed. The output buffer's position will have
   * advanced by n, when n is the value returned by this method; the output
   * buffer's limit will not have changed.
   * <p>
   * 
   * If <code>output.remaining()</code> bytes are insufficient to hold the
   * result, a <code>ShortBufferException</code> is thrown.
   * 
   * @param input the input ByteBuffer
   * @param output the output ByteBuffer
   * @return int number of bytes stored in <code>output</code>
   * @throws ShortBufferException if there is insufficient space in the
   * output buffer",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,doFinal,org.apache.hadoop.crypto.OpensslCipher:doFinal(java.nio.ByteBuffer),270,277,"/**
 * Finalizes the decryption process and writes to the output buffer.
 * @param output ByteBuffer to store the decrypted data
 * @return Number of bytes written to the output buffer
 */
","* Finishes a multiple-part operation. The data is encrypted or decrypted,
   * depending on how this cipher was initialized.
   * <p>
   * The result is stored in the output buffer. Upon return, the output buffer's
   * position will have advanced by n, where n is the value returned by this
   * method; the output buffer's limit will not have changed.
   * </p>
   * If <code>output.remaining()</code> bytes are insufficient to hold the result,
   * a <code>ShortBufferException</code> is thrown.
   * <p>
   * Upon finishing, this method resets this cipher object to the state it was
   * in when previously initialized. That is, the object is available to encrypt
   * or decrypt more data.
   * </p>
   * If any exception is thrown, this cipher object need to be reset before it
   * can be used again.
   *
   * @param output the output ByteBuffer
   * @return int number of bytes stored in <code>output</code>
   * @throws ShortBufferException      if there is insufficient space in the output buffer.
   * @throws IllegalBlockSizeException This exception is thrown when the length
   *                                   of data provided to a block cipher is incorrect.
   * @throws BadPaddingException       This exception is thrown when a particular
   *                                   padding mechanism is expected for the input
   *                                   data but the data is not padded properly.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPointInterceptorFactory.java,create,org.apache.hadoop.fs.viewfs.RegexMountPointInterceptorFactory:create(java.lang.String),41,66,"/**
 * Creates a RegexMountPointInterceptor from settings string.
 * @param interceptorSettingsString settings string to parse
 * @return RegexMountPointInterceptor or null if invalid.
 */
","* interceptorSettingsString string should be like ${type}:${string},
   * e.g. replaceresolveddstpath:word1,word2.
   *
   * @param interceptorSettingsString
   * @return Return interceptor based on setting or null on bad/unknown config.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,toString,org.apache.hadoop.fs.DF:toString(),129,139,"/**
 * Returns a string representation of the filesystem data.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,normalizePath,"org.apache.hadoop.fs.Path:normalizePath(java.lang.String,java.lang.String)",297,318,"/**
 * Normalizes a path string, removing redundant slashes and backslashes.
 * @param scheme URI scheme (e.g., ""file"")
 * @param path  Path string to normalize
 * @return Normalized path string
 */
","* Normalize a path string to use non-duplicated forward slashes as
   * the path separator and remove any trailing path separators.
   *
   * @param scheme the URI scheme. Used to deduce whether we
   * should replace backslashes or not
   * @param path the scheme-specific part
   * @return the normalized path string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isWindowsAbsolutePath,"org.apache.hadoop.fs.Path:isWindowsAbsolutePath(java.lang.String,boolean)",341,348,"/**
 * Checks if a path string is a Windows absolute path.
 * @param pathString Path to check.
 * @param slashed True if path uses forward slashes.
 * @return True if it's a Windows absolute path.
 */
","* Determine whether a given path string represents an absolute path on
   * Windows. e.g. ""C:/a/b"" is an absolute path. ""C:a/b"" is not.
   *
   * @param pathString the path string to evaluate
   * @param slashed true if the given path is prefixed with ""/""
   * @return true if the supplied path looks like an absolute path with a Windows
   * drive-specifier",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isUriPathAbsolute,org.apache.hadoop.fs.Path:isUriPathAbsolute(),388,391,"/**
 * Checks if the URI path is absolute, considering the separator.
 */","* Returns true if the path component (i.e. directory) of this URI is
   * absolute.
   *
   * @return whether this URI's path is absolute",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getHarHash,org.apache.hadoop.fs.HarFileSystem:getHarHash(org.apache.hadoop.fs.Path),488,490,"/**
 * Calculates a hash code for a Path, ensuring a positive value.
 * @param p The Path object to hash.
 * @return The hash code of the Path (positive).
 */
","* the hash of the path p inside  the filesystem
   * @param p the path in the harfilesystem
   * @return the hash code of the path.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,build,org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:build(),4694,4714,"/**
 * Builds and returns an FSDataOutputStream based on flags.
 * Throws exception if create/overwrite/append is not specified.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FileStatus),108,135,"/**
 * Converts a FileStatus to a FileStatusProto.
 * @param stat The FileStatus to convert.
 * @return The equivalent FileStatusProto.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPInputStream.java,<init>,"org.apache.hadoop.fs.sftp.SFTPInputStream:<init>(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem$Statistics)",46,58,"/**
 * Initializes SFTPInputStream with a channel, path, and statistics.
 * @param channel SFTP channel
 * @param path file path
 * @param stats file statistics
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkPath,org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPath(org.apache.hadoop.fs.Path),69,73,"/**
 * Verifies the given path is a child of the base path.
 * @param path The path to check.
 */
","* Validate a path.
   * @param path path to check.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Stat.java,getExecString,org.apache.hadoop.fs.Stat:getExecString(),91,108,"/**
 * Returns the stat command string based on the platform.
 * @return String array representing the stat command.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFileSystem:readOnlyMountTable(java.lang.String,org.apache.hadoop.fs.Path)",103,106,"/**
 * Delegates to the overloaded method with string path.
 * @param operation The operation being performed.
 * @param p The path being accessed.
 * @return AccessControlException if access is denied.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,readOnlyMountTable,"org.apache.hadoop.fs.viewfs.ViewFs:readOnlyMountTable(java.lang.String,org.apache.hadoop.fs.Path)",181,184,"/**
 * Delegates to the overloaded method with String path.
 * @param operation The operation being performed.
 * @param p The path being accessed.
 * @return AccessControlException if access is denied.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,getPathToResolve,"org.apache.hadoop.fs.viewfs.RegexMountPoint:getPathToResolve(java.lang.String,boolean)",217,227,"/**
 * Gets the path to resolve, excluding the last component if specified.
 * @param srcPath The source path string.
 * @param resolveLastComponent Whether to include the last component.
 * @return Path string or null if no slash is found.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,checkDependencies,"org.apache.hadoop.fs.FileContext:checkDependencies(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2284,2298,"/**
 * Checks if copying qualSrc to qualDst would be invalid.
 * Throws IOException if qualDst is a subdirectory or same as qualSrc.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compareTo,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:compareTo(java.lang.Object),3794,3805,"/**
 * Compares this SegmentDescriptor with another based on length, offset, and path.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:equals(java.lang.Object),3807,3820,"/**
 * Checks if this SegmentDescriptor is equal to another object.
 * @param o the object to compare to
 * @return true if objects are equal, false otherwise
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getNextIdToTry,"org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getNextIdToTry(org.apache.hadoop.fs.Path,int)",731,753,"/**
 * Finds the next available ID by iterating through files.
 * @param initial base path to search for files
 * @param lastId last ID used
 * @return next available ID
 */
","* Return the next ID suffix to use when creating the log file. This method
   * will look at the files in the directory, find the one with the highest
   * ID suffix, and 1 to that suffix, and return it. This approach saves a full
   * linear probe, which matters in the case where there are a large number of
   * log files.
   *
   * @param initial the base file path
   * @param lastId the last ID value that was used
   * @return the next ID to try
   * @throws IOException thrown if there's an issue querying the files in the
   * directory",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,getPathAsString,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getPathAsString(),144,146,"/**
 * Converts the path to a String representation.
 * @return String representation of the path.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createServiceURL,org.apache.hadoop.crypto.key.kms.KMSClientProvider:createServiceURL(org.apache.hadoop.fs.Path),447,453,"/**
 * Creates a service URL from a Path, removing trailing slash.
 * @param path Path object representing the base URL.
 * @throws IOException if URL creation fails.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,seek,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:seek(long),313,319,"/**
 * Seeks to the specified position.
 * @param pos The position to seek to.
 * @throws IOException if seeking past EOF.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,skip,org.apache.hadoop.fs.FSInputChecker:skip(long),405,413,"/**
 * Skips n bytes from the current position.
 * @param n number of bytes to skip, must be positive
 * @return the number of skipped bytes
 */
","* Skips over and discards <code>n</code> bytes of data from the
   * input stream.
   *
   * <p>This method may skip more bytes than are remaining in the backing
   * file. This produces no exception and the number of bytes skipped
   * may include some number of bytes that were beyond the EOF of the
   * backing file. Attempting to read from the stream after skipping past
   * the end will result in -1 indicating the end of the file.
   *
   *<p>If <code>n</code> is negative, no bytes are skipped.
   *
   * @param      n   the number of bytes to be skipped.
   * @return     the actual number of bytes skipped.
   * @exception  IOException  if an I/O error occurs.
   *             ChecksumException if the chunk to skip to is corrupted",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ByteBufferUtil.java,fallbackRead,"org.apache.hadoop.fs.ByteBufferUtil:fallbackRead(java.io.InputStream,org.apache.hadoop.io.ByteBufferPool,int)",57,117,"/**
 * Reads data from an InputStream using a ByteBufferPool.
 * @param stream Input stream to read from
 * @param bufferPool Pool for ByteBuffer allocation
 * @param maxLength Maximum number of bytes to read
 * @return ByteBuffer containing read data, or null if error
 * @throws IOException if an I/O error occurs
 */
","* Perform a fallback read.
   *
   * @param stream input stream.
   * @param bufferPool bufferPool.
   * @param maxLength maxLength.
   * @throws IOException raised on errors performing I/O.
   * @return byte buffer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,reset,org.apache.hadoop.fs.audit.CommonAuditContext:reset(),185,188,"/**
 * Resets the evaluated entries and re-initializes the data.
 */
","* Rest the context; will set the standard options again.
   * Primarily for testing.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/audit/CommonAuditContext.java,createInstance,org.apache.hadoop.fs.audit.CommonAuditContext:createInstance(),212,216,"/**
 * Creates and initializes a CommonAuditContext instance.
 * Returns the initialized context object.
 */
","* Demand invoked to create the instance for this thread.
   * @return an instance.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,toString,org.apache.hadoop.tools.TableListing:toString(),229,292,"/**
 * Generates a string representation of the table.
 * Calculates column widths and formats rows for display.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,"org.apache.hadoop.fs.permission.FsPermission:<init>(org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction,org.apache.hadoop.fs.permission.FsAction)",82,84,"/**
* Creates a FsPermission with user, group, and other actions.
* @param u User action
* @param g Group action
* @param o Other action
*/
","* Construct by the given {@link FsAction}.
   * @param u user action
   * @param g group action
   * @param o other action",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission:<init>(short),95,95,"/**
* Constructs a FsPermission object from a short mode value.
*/","* Construct by the given mode.
   * @param mode mode.
   * @see #toShort()",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,readFields,org.apache.hadoop.fs.permission.FsPermission:readFields(java.io.DataInput),185,189,"/**
* Reads a short from the input stream and sets the field.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,read,org.apache.hadoop.fs.permission.FsPermission:read(java.io.DataInput),214,218,"/**
 * Reads a FsPermission from the input stream.
 * @param in Input stream to read permission bits from.
 * @return FsPermission object constructed from the input.
 */
","* Create and initialize a {@link FsPermission} from {@link DataInput}.
   *
   * @param in data input.
   * @throws IOException raised on errors performing I/O.
   * @return FsPermission.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclStatus.java,getEffectivePermission,org.apache.hadoop.fs.permission.AclStatus:getEffectivePermission(org.apache.hadoop.fs.permission.AclEntry),230,232,"/**
* Gets the effective permission for an AclEntry.
* @param entry The AclEntry to evaluate.
* @return The effective FsAction for the entry.
*/
","* Get the effective permission for the AclEntry
   * @param entry AclEntry to get the effective action
   * @return FsAction.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,createImmutable,"org.apache.hadoop.fs.permission.PermissionStatus:createImmutable(java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",49,57,"/**
 * Creates an immutable PermissionStatus object.
 * @param user User name.
 * @param group Group name.
 * @param permission FsPermission object.
 * @return Immutable PermissionStatus.
 */
","* Create an immutable {@link PermissionStatus} object.
   * @param user user.
   * @param group group.
   * @param permission permission.
   * @return PermissionStatus.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,parseAclSpec,"org.apache.hadoop.fs.permission.AclEntry:parseAclSpec(java.lang.String,boolean)",235,245,"/**
 * Parses an ACL specification string into a list of AclEntry objects.
 * @param aclSpec comma-separated ACL specification string.
 * @param includePermission whether to include permission in parsing
 * @return List of AclEntry objects.
 */
","* Parses a string representation of an ACL spec into a list of AclEntry
   * objects. Example: ""user::rwx,user:foo:rw-,group::r--,other::---""
   * The expected format of ACL entries in the string parameter is the same
   * format produced by the {@link #toStringStable()} method.
   * 
   * @param aclSpec
   *          String representation of an ACL spec.
   * @param includePermission
   *          for setAcl operations this will be true. i.e. AclSpec should
   *          include permissions.<br>
   *          But for removeAcl operation it will be false. i.e. AclSpec should
   *          not contain permissions.<br>
   *          Example: ""user:foo,group:bar""
   * @return Returns list of {@link AclEntry} parsed",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,create,"org.apache.hadoop.fs.permission.FsCreateModes:create(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",58,63,"/**
 * Creates a new FsCreateModes object with the given permissions.
 * @param masked The masked permission.
 * @param unmasked The unmasked permission.
 */
","* Create from masked and unmasked modes.
   *
   * @param masked masked.
   * @param unmasked unmasked.
   * @return FsCreateModes.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,printExtendedAclEntry,"org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:printExtendedAclEntry(org.apache.hadoop.fs.permission.AclStatus,org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.AclEntry)",137,152,"/**
 * Prints an extended ACL entry, showing effective permissions if different.
 */","* Prints a single extended ACL entry.  If the mask restricts the
     * permissions of the entry, then also prints the restricted version as the
     * effective permissions.  The mask applies to all named entries and also
     * the unnamed group entry.
     * @param aclStatus AclStatus for the path
     * @param fsPerm FsPermission for the path
     * @param entry AclEntry extended ACL entry to print",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,toString,org.apache.hadoop.fs.permission.AclEntry:toString(),102,108,"/**
 * Returns a string representation of the object, delegating to stable version.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getStrings,org.apache.hadoop.util.StringUtils:getStrings(java.lang.String),407,410,"/**
 * Splits a string by comma.
 * @param str The string to split.
 * @return An array of strings.
 */
","* Returns an arraylist of strings.
   * @param str the comma separated string values
   * @return the arraylist of the comma separated string values",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/UmaskParser.java,<init>,org.apache.hadoop.fs.permission.UmaskParser:<init>(java.lang.String),41,45,"/**
* Constructs a UmaskParser with the given mode string.
* @param modeStr String representing the umask mode.
* @throws IllegalArgumentException if mode string is invalid.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/RawParser.java,<init>,org.apache.hadoop.fs.permission.RawParser:<init>(java.lang.String),35,38,"/**
 * Initializes a RawParser with the given mode string.
 * @param modeStr String representing the parsing mode.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/ChmodParser.java,<init>,org.apache.hadoop.fs.permission.ChmodParser:<init>(java.lang.String),38,40,"/**
 * Constructs a ChmodParser with the given mode string.
 * @param modeStr The chmod mode string to parse.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,create,"org.apache.hadoop.fs.store.DataBlocks$ArrayBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",530,535,"/**
 * Creates a new ByteArrayBlock with specified index and limit.
 * @param index Block index
 * @param limit Maximum block size
 * @param statistics Upload statistics
 * @return New ByteArrayBlock instance
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,available,org.apache.hadoop.fs.store.ByteBufferInputStream:available(),116,120,"/**
 * Returns the number of bytes available in the buffer.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,position,org.apache.hadoop.fs.store.ByteBufferInputStream:position(),126,129,"/**
 * Returns the current position of the byte buffer.
 * @return The current position as an integer.
 */
","* Get the current buffer position.
   * @return the buffer position",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,hasRemaining,org.apache.hadoop.fs.store.ByteBufferInputStream:hasRemaining(),135,138,"/**
 * Checks if the buffer has remaining bytes to read.
 * Returns true if remaining, false otherwise.
 */
","* Check if there is data left.
   * @return true if there is data remaining in the buffer.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,reset,org.apache.hadoop.fs.store.ByteBufferInputStream:reset(),147,152,"/**
 * Resets the buffer to its initial state. Throws IOException if closed.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$DataBlock:startUpload(),454,458,"/**
 * Starts a datablock upload; transitions to the Upload state.
 * @return Null, upload data is handled elsewhere.
 */
","* Switch to the upload state and return a stream for uploading.
     * Base class calls {@link #enterState(DestState, DestState)} to
     * manage the state machine.
     *
     * @return the stream.
     * @throws IOException trouble",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,enterClosedState,org.apache.hadoop.fs.store.DataBlocks$DataBlock:enterClosedState(),466,473,"/**
 * Transitions to the closed state if not already closed.
 * @return True if state changed, false otherwise.
 */
","* Enter the closed state.
     *
     * @return true if the class was in any other state, implying that
     * the subclass should do its close operations.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$DiskBlock:write(byte[],int,int)",878,885,"/**
 * Writes bytes to the underlying output stream.
 * @param b buffer containing bytes to write
 * @param offset offset within the buffer
 * @param len number of bytes to write
 * @return number of bytes actually written
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:write(byte[],int,int)",747,753,"/**
 * Writes data to the buffer.
 * @param b The byte array to write.
 * @param offset Start offset in the byte array.
 * @param len Number of bytes to write.
 * @return Number of bytes actually written.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,flush,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:flush(),940,943,"/**
 * Flushes the output stream, calling super.flush() and out.flush().
 */","* Flush operation will flush to disk.
     *
     * @throws IOException IOE raised on FileOutputStream",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,write,"org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:write(byte[],int,int)",612,618,"/**
 * Writes bytes to the buffer.
 * @param b the byte array to write
 * @param offset offset into the array
 * @param len number of bytes to write
 * @return number of bytes written
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,toString,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:toString(),767,776,"/**
 * Returns a string representation of this ByteBufferBlock.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getStatistics,org.apache.hadoop.fs.FileContext:getStatistics(java.net.URI),2396,2398,"/**
 * Gets statistics for a file system at the given URI.
 * @param uri The URI representing the file system.
 * @return Statistics object for the file system.
 */
","* Get the statistics for a particular file system
   * 
   * @param uri
   *          the uri to lookup the statistics. Only scheme and authority part
   *          of the uri are used as the key to store and lookup.
   * @return a statistics object",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createMultipartUploader,org.apache.hadoop.fs.FilterFs:createMultipartUploader(org.apache.hadoop.fs.Path),457,461,"/**
 * Creates a MultipartUploaderBuilder using the file system.
 * @param basePath Base path for the uploader.
 * @return MultipartUploaderBuilder instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getCurrentDirectoryIndex,org.apache.hadoop.fs.LocalDirAllocator:getCurrentDirectoryIndex(),257,260,"/**
 * Gets the current directory index from the allocator context.
 */","* Get the current directory index for the given configuration item.
   * @return the current directory index for the given configuration item.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,getPos,org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:getPos(),49,52,"/**
 * Returns the current position of the output stream.
 * @return The current position as a long value.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sync,org.apache.hadoop.io.SequenceFile$Writer:sync(),1369,1375,"/**
 * Writes sync data if needed, marking the start with SYNC_ESCAPE.
 */","* create a sync point.
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getLength,org.apache.hadoop.io.SequenceFile$Writer:getLength(),1530,1532,"/**
 * Returns the current length of the output stream in bytes.
 */","@return Returns the current length of the output file.
     *
     * <p>This always returns a synchronized position.  In other words,
     * immediately after calling {@link SequenceFile.Reader#seek(long)} with a position
     * returned by this method, {@link SequenceFile.Reader#next(Writable)} may be called.  However
     * the key may be earlier in the file than key last written when this
     * method was called (e.g., with block-compression, it may be the first key
     * in the block that was being written when this method was called).</p>
     *
     * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCurrentPos,org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getCurrentPos(),156,158,"/**
* Returns the current position in the output stream.
* @return Current position as a long value.
*/
","* Get the current position in file.
       * 
       * @return The current byte offset in underlying file.
       * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getContentSummary,org.apache.hadoop.fs.FileSystem:getContentSummary(org.apache.hadoop.fs.Path),1923,1945,"/**
 * Calculates content summary (length, file/dir count) for a Path.
 * @param f the path to summarize
 * @return ContentSummary object representing the summary
 */
","Return the {@link ContentSummary} of a given {@link Path}.
   * @param f path to use
   * @throws FileNotFoundException if the path does not resolve
   * @throws IOException IO failure
   * @return content summary.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,buildACL,org.apache.hadoop.security.authorize.AccessControlList:buildACL(java.lang.String[]),107,126,"/**
 * Builds access control lists from user/group strings.
 * Populates users/groups sets; allows all if wildcard found.
 */","* Build ACL from the given array of strings.
   * The strings contain comma separated values.
   *
   * @param userGroupStrings build ACL from array of Strings",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfigurationHelper.java,parseEnumSet,"org.apache.hadoop.util.ConfigurationHelper:parseEnumSet(java.lang.String,java.lang.String,java.lang.Class,boolean)",68,99,"/**
 * Parses a comma-separated string into an EnumSet of the specified enum type.
 * @param key Identifier for the parameter.
 * @param valueString Comma-separated string of enum values.
 * @param enumClass Enum class to parse.
 * @param ignoreUnknown Whether to ignore unknown values.
 * @return EnumSet of parsed enum values.
 */
","* Given a comma separated list of enum values,
   * trim the list, map to enum values in the message (case insensitive)
   * and return the set.
   * Special handling of ""*"" meaning: all values.
   * @param key Configuration object key -used in error messages.
   * @param valueString value from Configuration
   * @param enumClass class of enum
   * @param ignoreUnknown should unknown values be ignored?
   * @param <E> enum type
   * @return a mutable set of enum values parsed from the valueString, with any unknown
   * matches stripped if {@code ignoreUnknown} is true.
   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,
   * or there are two entries in the enum which differ only by case.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,ensureCurrentState,org.apache.hadoop.service.ServiceStateModel:ensureCurrentState(org.apache.hadoop.service.Service$STATE),97,104,"/**
 * Verifies the service state matches the expected state.
 * @param expectedState The expected service state.
 * @throws ServiceStateException if states do not match.
 */
","* Verify that that a service is in a given state.
   * @param expectedState the desired state
   * @throws ServiceStateException if the service state is different from
   * the desired state",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,<init>,org.apache.hadoop.service.AbstractService:<init>(java.lang.String),113,116,"/**
 * Constructs a new AbstractService with the given name.
 * @param name The name of the service.
 */
","* Construct the service.
   * @param name service name",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,checkStateTransition,"org.apache.hadoop.service.ServiceStateModel:checkStateTransition(java.lang.String,org.apache.hadoop.service.Service$STATE,org.apache.hadoop.service.Service$STATE)",128,135,"/**
 * Validates state transition; throws exception if invalid.
 * @param name Service name, state, and proposed state.
 */
","* Check that a state tansition is valid and
   * throw an exception if not
   * @param name name of the service (can be null)
   * @param state current state
   * @param proposed proposed new state",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,serviceCreationFailure,org.apache.hadoop.service.launcher.ServiceLauncher:serviceCreationFailure(java.lang.Exception),745,747,"/**
 * Creates a ServiceLaunchException with a failure code and cause.
 * @param exception The exception that caused the failure.
 * @return A ServiceLaunchException representing the failure.
 */
","* Generate an exception announcing a failure to create the service.
   * @param exception inner exception.
   * @return a new exception, with the exit code
   * {@link LauncherExitCodes#EXIT_SERVICE_CREATION_FAILURE}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,verifyConfigurationFilesExist,org.apache.hadoop.service.launcher.ServiceLauncher:verifyConfigurationFilesExist(java.lang.String[]),989,1003,"/**
 * Verifies that the configuration files exist.
 * @param filenames array of configuration file names
 */
","* Verify that all the specified filenames exist.
   * @param filenames a list of files
   * @throws ServiceLaunchException if a file is not found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.String,java.lang.Object[])",1088,1092,"/**
 * Constructs a KerberosDiagsFailure with a formatted message.
 * @param category Failure category.
 * @param message Message format string.
 * @param args Arguments for message formatting.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,convertToExitException,org.apache.hadoop.service.launcher.ServiceLauncher:convertToExitException(java.lang.Throwable),714,737,"/**
 * Converts a Throwable to an ExitUtil.ExitException with exit code.
 * @param thrown The Throwable to convert.
 * @return An ExitUtil.ExitException.
 */
","* Convert an exception to an {@code ExitException}.
   *
   * This process may just be a simple pass through, otherwise a new
   * exception is created with an exit code, the text of the supplied
   * exception, and the supplied exception as an inner cause.
   * 
   * <ol>
   *   <li>If is already the right type, pass it through.</li>
   *   <li>If it implements {@link ExitCodeProvider#getExitCode()},
   *   the exit code is extracted and used in the new exception.</li>
   *   <li>Otherwise, the exit code
   *   {@link LauncherExitCodes#EXIT_EXCEPTION_THROWN} is used.</li>
   * </ol>
   *  
   * @param thrown the exception thrown
   * @return an {@code ExitException} with a status code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,<init>,org.apache.hadoop.service.launcher.ServiceShutdownHook:<init>(org.apache.hadoop.service.Service),52,54,"/**
 * Initializes the shutdown hook with a weak reference to the service.
 * @param service The service to be shut down.
 */
","* Create an instance.
   * @param service the service",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,toString,org.apache.hadoop.service.launcher.InterruptEscalator:toString(),89,101,"/**
 * Returns a string representation of this InterruptEscalator.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,noteFailure,org.apache.hadoop.service.AbstractService:noteFailure(java.lang.Exception),257,272,"/**
 * Records a failure event, logging details and setting failure state.
 * @param exception The exception that caused the failure.
 */
","* Failure handling: record the exception
   * that triggered it -if there was not one already.
   * Services are free to call this themselves.
   * @param exception the exception",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,recordLifecycleEvent,org.apache.hadoop.service.AbstractService:recordLifecycleEvent(),421,426,"/**
 * Records a lifecycle event with timestamp and current state.
 */
",* Add a state change event to the lifecycle history,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,serviceInit,org.apache.hadoop.service.CompositeService:serviceInit(org.apache.hadoop.conf.Configuration),104,113,"/**
 * Initializes all registered services with the given configuration.
 * @param conf Configuration object for service initialization.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,stop,"org.apache.hadoop.service.CompositeService:stop(int,boolean)",147,170,"/**
 * Stops services, optionally stopping only those that started.
 * @param numOfServicesStarted Number of services to stop.
 * @param stopOnlyStartedServices Whether to stop only started services.
 */
","* Stop the services in reverse order
   *
   * @param numOfServicesStarted index from where the stop should work
   * @param stopOnlyStartedServices flag to say ""only start services that are
   * started, not those that are NOTINITED or INITED.
   * @throws RuntimeException the first exception raised during the
   * stop process -<i>after all services are stopped</i>",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceOperations.java,stopQuietly,org.apache.hadoop.service.ServiceOperations:stopQuietly(org.apache.hadoop.service.Service),65,67,"/**
 * Stops a service, suppressing exceptions.
 * @param service The service to stop.
 * @return Exception if any, otherwise null.
 */
","* Stop a service; if it is null do nothing. Exceptions are caught and
   * logged at warn level. (but not Throwables). This operation is intended to
   * be used in cleanup operations
   *
   * @param service a service; may be null
   * @return any exception that was caught; null if none was.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,progressable,org.apache.hadoop.io.SequenceFile$Writer:progressable(org.apache.hadoop.util.Progressable),1036,1038,"/**
 * Creates a ProgressableOption from a given Progressable.
 * @param value The Progressable to wrap in an Option.
 * @return A ProgressableOption instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,blockSize,org.apache.hadoop.io.SequenceFile$Writer:blockSize(long),1032,1034,"/**
 * Creates a BlockSizeOption with the given size value.
 * @param value The block size value.
 * @return A BlockSizeOption object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,syncInterval,org.apache.hadoop.io.SequenceFile$Writer:syncInterval(int),1061,1063,"/**
 * Creates a SyncIntervalOption with the given interval value.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,replication,org.apache.hadoop.io.SequenceFile$Writer:replication(short),1024,1026,"/**
 * Creates a ReplicationOption with the given value.
 * @param value The replication value.
 * @return A new ReplicationOption object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,bufferSize,org.apache.hadoop.io.SequenceFile$Writer:bufferSize(int),1016,1018,"/**
 * Creates a BufferSizeOption with the given size.
 * @param value The buffer size value.
 * @return A BufferSizeOption object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,write,org.apache.hadoop.io.ObjectWritable$NullInstance:write(java.io.DataOutput),126,129,"/**
 * Writes the class name to the DataOutput.
 * Uses UTF8 encoding.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,write,org.apache.hadoop.io.ArrayPrimitiveWritable:write(java.io.DataOutput),171,200,"/**
 * Writes the array data to the DataOutput, type-dependent.
 * @param out DataOutput to write to.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,valueClass,org.apache.hadoop.io.SequenceFile$Writer:valueClass(java.lang.Class),1044,1046,"/**
 * Creates a ValueClassOption for the given class.
 * @param value The class to create an option for.
 * @return A ValueClassOption instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,keyClass,org.apache.hadoop.io.MapFile$Writer:keyClass(java.lang.Class),285,287,"/**
 * Creates a KeyClassOption for the given value class.
 * @param value Class of the key to be used.
 * @return KeyClassOption instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,keyClass,org.apache.hadoop.io.SequenceFile$Writer:keyClass(java.lang.Class),1040,1042,"/**
 * Creates a KeyClassOption for the given class.
 * @param value The class to create the option for.
 * @return A KeyClassOption object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,compareTo,org.apache.hadoop.io.UTF8:compareTo(org.apache.hadoop.io.UTF8),156,160,"/**
 * Compares this UTF8 object to another using byte-wise comparison.
 */
",Compare two UTF8s.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,equals,org.apache.hadoop.io.UTF8:equals(java.lang.Object),193,203,"/**
 * Checks if this UTF8 object is equal to another UTF8 object.
 * @param o the object to compare to
 * @return true if equal, false otherwise
 */
",Returns true iff <code>o</code> is a UTF8 with the same contents.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,compareTo,org.apache.hadoop.io.MD5Hash:compareTo(org.apache.hadoop.io.MD5Hash),241,245,"/**
 * Compares this MD5Hash with another based on their byte digests.
 */
",Compares this object with the specified object for order.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,compareTo,org.apache.hadoop.io.BinaryComparable:compareTo(org.apache.hadoop.io.BinaryComparable),50,56,"/**
 * Compares this binary comparable to another.
 * @param other The other BinaryComparable to compare to.
 * @return -1, 0, or 1 based on comparison.
 */
","* Compare bytes from {#getBytes()}.
   * @see org.apache.hadoop.io.WritableComparator#compareBytes(byte[],int,int,byte[],int,int)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,compareTo,"org.apache.hadoop.io.BinaryComparable:compareTo(byte[],int,int)",66,69,"/**
 * Compares this byte array with another, using specified offset/length.
 */","* Compare bytes from {#getBytes()} to those provided.
   *
   * @param other other.
   * @param off off.
   * @param len len.
   * @return compareBytes.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/CompareUtils.java,compare,"org.apache.hadoop.io.file.tfile.CompareUtils$MemcmpRawComparator:compare(byte[],int,int,byte[],int,int)",89,92,"/**
 * Compares two byte arrays using WritableComparator.
 * @param b1,s1,l1,b2,s2,l2 Byte arrays and their offsets/lengths.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,hashCode,org.apache.hadoop.io.UTF8:hashCode(),205,208,"/**
 * Calculates hash code based on the bytes and length.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,hashCode,org.apache.hadoop.io.BinaryComparable:hashCode(),88,91,"/**
 * Calculates the hash code using bytes and length.
 */
","* Return a hash of the bytes returned from {#getBytes()}.
   * @see org.apache.hadoop.io.WritableComparator#hashBytes(byte[],int)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,hashCode,org.apache.hadoop.security.token.Token:hashCode(),402,405,"/**
 * Calculates the hash code using the identifier's byte representation.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,readDouble,"org.apache.hadoop.io.WritableComparator:readDouble(byte[],int)",311,313,"/**
 * Reads a double value from the byte array, starting at the given index.
 */","* Parse a double from a byte array.
   * @param bytes bytes.
   * @param start start.
   * @return double from a byte array.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,setSize,org.apache.hadoop.io.BytesWritable:setSize(int),131,138,"/**
 * Sets the size of the data structure.
 * Adjusts capacity if new size exceeds current capacity.
 */
","* Change the size of the buffer. The values in the old range are preserved
   * and any new values are undefined. The capacity is changed if it is 
   * necessary.
   * @param size The new number of bytes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MergeSort.java,<init>,org.apache.hadoop.util.MergeSort:<init>(java.util.Comparator),38,40,"/**
 * Initializes a MergeSort with the given comparator.
 * @param comparator Comparator for comparing IntWritable objects.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,canRead,org.apache.hadoop.fs.FileUtil:canRead(java.io.File),1412,1423,"/**
 * Checks if a file is readable. Uses Windows-specific API if on Windows.
 */
","* Platform independent implementation for {@link File#canRead()}
   * @param f input file
   * @return On Unix, same as {@link File#canRead()}
   *         On Windows, true if process has read access on the path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,canWrite,org.apache.hadoop.fs.FileUtil:canWrite(java.io.File),1431,1442,"/**
 * Checks if a file is writable. Uses NativeIO on Windows, f.canWrite() otherwise.
 */
","* Platform independent implementation for {@link File#canWrite()}
   * @param f input file
   * @return On Unix, same as {@link File#canWrite()}
   *         On Windows, true if process has write access on the path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,canExecute,org.apache.hadoop.fs.FileUtil:canExecute(java.io.File),1450,1461,"/**
 * Checks if a file is executable. Uses Windows-specific API if on Windows.
 * @param f The file to check.
 * @return True if executable, false otherwise.
 */
","* Platform independent implementation for {@link File#canExecute()}
   * @param f input file
   * @return On Unix, same as {@link File#canExecute()}
   *         On Windows, true if process has execute access on the path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,assertCodeLoaded,org.apache.hadoop.io.nativeio.NativeIO$POSIX:assertCodeLoaded(),364,368,"/**
 * Throws an IOException if NativeIO hasn't been loaded yet.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ReadaheadPool.java,getInstance,org.apache.hadoop.io.ReadaheadPool:getInstance(),55,62,"/**
 * Returns the singleton instance of the ReadaheadPool.
 * Creates it if it doesn't exist and NativeIO is available.
 */
",* @return Return the singleton instance for the current process.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,verifyCanMlock,org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:verifyCanMlock(),300,302,"/**
 * Checks if native memory locking is available.
 * @return True if memory locking is supported, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/SharedFileDescriptorFactory.java,getLoadingFailureReason,org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:getLoadingFailureReason(),53,61,"/**
 * Returns the reason for loading failure, or null if successful.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getMemlockLimit,org.apache.hadoop.io.nativeio.NativeIO:getMemlockLimit(),884,886,"/**
 * Returns the memlock limit, or 0 if unavailable.
 */
","* Get the maximum number of bytes that can be locked into memory at any
   * given point.
   *
   * @return 0 if no bytes can be locked into memory;
   *         Long.MAX_VALUE if there is no limit;
   *         The number of bytes that can be locked into memory otherwise.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,calculateChunkedSums,"org.apache.hadoop.util.DataChecksum:calculateChunkedSums(byte[],int,int,byte[],int)",576,600,"/**
 * Calculates chunked sums of data, using native or fallback method.
 * @param data Data to process.
 * @param offset Offset into data.
 * @param length Length of data to process.
 * @param sums Array to store sums.
 * @param offset Offset into sums array.
 */
","* Implementation of chunked calculation specifically on byte arrays. This
   * is to avoid the copy when dealing with ByteBuffers that have array backing.
   *
   * @param data data.
   * @param dataOffset dataOffset.
   * @param dataLength dataLength.
   * @param sums sums.
   * @param sumsOffset sumsOffset.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getCreateForWriteFileOutputStream,"org.apache.hadoop.io.nativeio.NativeIO:getCreateForWriteFileOutputStream(java.io.File,int)",1001,1037,"/**
 * Creates a FileOutputStream for writing, creating the file if it doesn't exist.
 * @param f the file
 * @param permissions file permissions
 * @throws IOException if an I/O error occurs
 */
","* @return Create the specified File for write access, ensuring that it does not exist.
   * @param f the file that we want to create
   * @param permissions we want to have on the file (if security is enabled)
   *
   * @throws AlreadyExistsException if the file already exists
   * @throws IOException if any other error occurred",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ReadaheadPool.java,run,org.apache.hadoop.io.ReadaheadPool$ReadaheadRequestImpl:run(),210,232,"/**
 * Attempts a POSIX fadvise readahead operation. Handles cancellation/IO errors.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,cleanBufferPool,org.apache.hadoop.crypto.CryptoInputStream:cleanBufferPool(),816,821,"/**
 * Drains the buffer pool, freeing associated database resources.
 */",Clean direct buffer pool,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,freeBuffers,org.apache.hadoop.crypto.CryptoOutputStream:freeBuffers(),311,314,"/**
* Releases memory occupied by input and output buffers.
*/
",Forcibly free the direct buffers.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getFstat,org.apache.hadoop.io.nativeio.NativeIO$POSIX:getFstat(java.io.FileDescriptor),575,596,"/**
 * Gets file status for a file descriptor.
 * @param fd The file descriptor.
 * @return Stat object containing file status.
 */
","* Returns the file stat for a file descriptor.
     *
     * @param fd file descriptor.
     * @return the file descriptor file stat.
     * @throws IOException thrown if there was an IO error while obtaining the file stat.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getStat,org.apache.hadoop.io.nativeio.NativeIO$POSIX:getStat(java.lang.String),606,627,"/**
 * Gets Stat object for given path. Throws IOException on error.
 */","* Return the file stat for a file path.
     *
     * @param path  file path
     * @return  the file stat
     * @throws IOException  thrown if there is an IO error while obtaining the
     * file stat",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BoundedByteArrayOutputStream.java,<init>,"org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(int,int)",55,57,"/**
 * Constructs BoundedByteArrayOutputStream with given capacity and limit.
 */","* Create a BoundedByteArrayOutputStream with the specified
   * capacity and limit.
   * @param capacity The capacity of the underlying byte array
   * @param limit The maximum limit upto which data can be written",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,<init>,org.apache.hadoop.io.EnumSetWritable:<init>(java.util.EnumSet),80,82,"/**
 * Constructs a Writable from an EnumSet.
 * @param value The EnumSet to wrap.
 */
","* Construct a new EnumSetWritable. Argument <tt>value</tt> should not be null
   * or empty.
   * 
   * @param value enumSet value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/OutputBuffer.java,write,"org.apache.hadoop.io.OutputBuffer:write(java.io.InputStream,int)",107,109,"/**
 * Writes data from an input stream to the buffer.
 * @param in Input stream to read data from
 * @param length Number of bytes to read
 */
","* Writes bytes from a InputStream directly into the buffer.
   * @param in input in.
   * @param length input length.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,nextBytes,org.apache.hadoop.crypto.random.OsSecureRandom:nextBytes(byte[]),97,108,"/**
 * Fills the provided byte array with random bytes from the reservoir.
 * @param bytes The byte array to fill.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,next,org.apache.hadoop.crypto.random.OsSecureRandom:next(int),110,118,"/**
 * Reads the next 4 bytes from the reservoir, returns nbits.
 * @param nbits Number of bits to return from the reservoir.
 * @return Integer value constructed from reservoir bytes.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,fromString,org.apache.hadoop.io.DefaultStringifier:fromString(java.lang.String),75,81,"/**
 * Restores an object from a Base64 encoded string.
 * @param str Base64 encoded string to restore from.
 * @return Restored object of type T.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKeyStream,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKeyStream(),1806,1809,"/**
 * Resets and returns a DataInputStream for key data.
 */","* Streaming access to the key. Useful for desrializing the key into
         * user objects.
         * 
         * @return The input stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,decodeWritable,"org.apache.hadoop.security.token.Token:decodeWritable(org.apache.hadoop.io.Writable,java.lang.String)",355,366,"/**
 * Decodes a Base64 string and populates a Writable object.
 * @param obj Writable object to populate
 * @param newValue Base64 encoded string
 */
","* Modify the writable to the value from the newValue.
   * @param obj the object to read into
   * @param newValue the string with the url-safe base64 encoded bytes
   * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeUncompressedBytes,org.apache.hadoop.io.SequenceFile$CompressedBytes:writeUncompressedBytes(java.io.DataOutputStream),699,715,"/**
 * Writes uncompressed data to the output stream.
 * Uses a decompressed stream to write data.
 * @param outStream DataOutputStream to write to.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,compare,"org.apache.hadoop.io.WritableComparator:compare(byte[],int,int,byte[],int,int)",177,192,"/**
 * Compares two byte array keys using a buffer.
 * @param b1, s1, l1 first byte array and offset/length
 * @param b2, s2, l2 second byte array and offset/length
 */
","Optimization hook.  Override this to make SequenceFile.Sorter's scream.
   *
   * <p>The default implementation reads the data into two {@link
   * WritableComparable}s (using {@link
   * Writable#readFields(DataInput)}, then calls {@link
   * #compare(WritableComparable,WritableComparable)}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/RSErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",34,36,"/**
 * Initializes a new RSErasureCodec with configuration and options.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/HHXORErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",34,36,"/**
 * Constructs a HHXORErasureCodec with configuration and options.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/DummyErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",32,34,"/**
 * Constructs a DummyErasureCodec with configuration and options.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/XORErasureCodec.java,<init>,"org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",34,37,"/**
 * Initializes a XORErasureCodec with the given config and options.
 * Asserts that the schema has exactly one parity unit.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawCoderFactory,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawCoderFactory(java.lang.String,java.lang.String)",154,161,"/**
 * Creates a RawErasureCoderFactory by name.
 * @param coderName Coder name.
 * @param codecName Codec name.
 * @return RawErasureCoderFactory instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/grouper/BlockGrouper.java,anyRecoverable,org.apache.hadoop.io.erasurecode.grouper.BlockGrouper:anyRecoverable(org.apache.hadoop.io.erasurecode.ECBlockGroup),86,90,"/**
 * Checks if a block group has recoverable erased blocks.
 * @param blockGroup BlockGroup object to check.
 * @return True if recoverable, false otherwise.
 */
","* Given a BlockGroup, tell if any of the missing blocks can be recovered,
   * to be called by ECManager
   * @param blockGroup a blockGroup that may contain erased blocks but not sure
   *                   recoverable or not
   * @return true if any erased block recoverable, false otherwise",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getNumErasedBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getNumErasedBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),132,136,"/**
 * Calculates the total number of erased blocks in a block group.
 */","* Get the number of erased blocks in the block group.
   * @param blockGroup blockGroup.
   * @return number of erased blocks",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getErasedIndexes,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getErasedIndexes(org.apache.hadoop.io.erasurecode.ECBlock[]),159,174,"/**
 * Returns indexes of erased blocks in the input array.
 * @param inputBlocks array of ECBlocks
 * @return int[] indexes of erased blocks
 */","* Get indexes of erased blocks from inputBlocks
   * @param inputBlocks inputBlocks.
   * @return indexes of erased blocks from inputBlocks",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,checkInputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:checkInputBuffers(java.nio.ByteBuffer[]),98,122,"/**
 * Validates input buffers, ensuring correct length & directness.
 * @param buffers Array of ByteBuffer objects to validate.
 */","* Check and ensure the buffers are of the desired length and type, direct
   * buffers or not.
   * @param buffers the buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,checkInputBuffers,org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:checkInputBuffers(byte[][]),95,115,"/**
 * Validates input buffers against expected length and count.
 * @param buffers Array of byte buffers to validate.
 */
","* Check and ensure the buffers are of the desired length.
   * @param buffers the buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,<init>,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],int[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",52,65,"/**
 * Initializes erasure decoding step with input blocks, erased indexes, decoders/encoders.
 */","* The constructor with all the necessary info.
   * @param inputBlocks inputBlocks.
   * @param erasedIndexes the indexes of erased blocks in inputBlocks array
   * @param outputBlocks outputBlocks.
   * @param rawDecoder underlying RS decoder for hitchhiker decoding
   * @param rawEncoder underlying XOR encoder for hitchhiker decoding",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DecodingState.java,checkParameters,"org.apache.hadoop.io.erasurecode.rawcoder.DecodingState:checkParameters(java.lang.Object[],int[],java.lang.Object[])",38,54,"/**
 * Validates input array lengths for decoding; throws exceptions if invalid.
 */","* Check and validate decoding parameters, throw exception accordingly. The
   * checking assumes it's a MDS code. Other code  can override this.
   * @param inputs input buffers to check
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,<init>,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:<init>(org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.ECBlock[],org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",48,57,"/**
 * Initializes an HHXORErasureEncodingStep with encoders and block arrays.
 */","* The constructor with all the necessary info.
   *
   * @param inputBlocks inputBlocks.
   * @param outputBlocks outputBlocks.
   * @param rsRawEncoder  underlying RS encoder for hitchhiker encoding
   * @param xorRawEncoder underlying XOR encoder for hitchhiker encoding",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/EncodingState.java,checkParameters,"org.apache.hadoop.io.erasurecode.rawcoder.EncodingState:checkParameters(java.lang.Object[],java.lang.Object[])",36,43,"/**
 * Validates input and output array lengths against encoder counts.
 */","* Check and validate decoding parameters, throw exception accordingly.
   * @param inputs input buffers to check
   * @param outputs output buffers to check",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,40,"/**
 * Creates a RawErasureDecoder instance using the provided options.
 * @param coderOptions ErasureCoderOptions for decoder configuration.
 * @return A RawErasureDecoder object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),36,45,"/**
 * Prepares a decoding step for erasure coding.
 * @param blockGroup The EC block group to decode.
 * @return An ErasureDecodingStep object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,39,"/**
 * Creates a RawErasureDecoder instance.
 * @param coderOptions ErasureCoderOptions for decoder configuration
 * @return A RawErasureDecoder object
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
 * Initializes the NativeXORRawDecoder with provided options.
 * @param coderOptions ErasureCoderOptions object for configuration.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
 * Initializes the NativeRSRawDecoder with erasure coding options.
 * @param coderOptions ErasureCoderOptions object for configuration.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java,getPiggyBackForDecode,"org.apache.hadoop.io.erasurecode.coder.util.HHUtil:getPiggyBackForDecode(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int,int,int,int)",150,201,"/**
* Calculates the piggyback data based on input parity indices.
* @param pbParityIndex Index of the parity unit.
* @return ByteBuffer containing the piggyback data.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,add,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:add(int[],int[])",371,384,"/**
 * Adds corresponding elements of two arrays.
 * @param p First integer array.
 * @param q Second integer array.
 * @return New array with element-wise sums.
 */
","* Compute the sum of two polynomials. The index in the array corresponds to
   * the power of the entry. For example p[0] is the constant term of the
   * polynomial p.
   *
   * @param p input polynomial
   * @param q input polynomial
   * @return polynomial represents p+q",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,multiply,"org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:multiply(int[],int[])",327,340,"/**
 * Multiplies two integer arrays and returns the product array.
 */","* Compute the multiplication of two polynomials. The index in the array
   * corresponds to the power of the entry. For example p[0] is the constant
   * term of the polynomial p.
   *
   * @param p input polynomial
   * @param q input polynomial
   * @return polynomial represents p*q",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,gaussianElimination,org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:gaussianElimination(int[][]),549,588,"/**
 * Performs Gaussian elimination on the input matrix.
 * @param matrix The matrix to be processed.
 */
","* Perform Gaussian elimination on the given matrix. This matrix has to be a
   * fat matrix (number of rows &gt; number of columns).
   *
   * @param matrix matrix.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/RSUtil.java,getPrimitivePower,"org.apache.hadoop.io.erasurecode.rawcoder.util.RSUtil:getPrimitivePower(int,int)",38,45,"/**
 * Computes powers of a primitive root.
 * @param numDataUnits Number of data units.
 * @param numParityUnits Number of parity units.
 * @return Array of primitive powers.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/DumpUtil.java,dumpChunks,"org.apache.hadoop.io.erasurecode.rawcoder.util.DumpUtil:dumpChunks(java.lang.String,org.apache.hadoop.io.erasurecode.ECChunk[])",80,87,"/**
 * Prints a header followed by details of each chunk.
 * @param header Header string to print.
 * @param chunks Array of ECChunk objects to dump.
 */
","* Print data in hex format in an array of chunks.
   * @param header header.
   * @param chunks chunks.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
 * Initializes the XOR raw encoder with specified options.
 * @param coderOptions ErasureCoderOptions for encoder configuration.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,46,"/**
 * Initializes the NativeRSRawEncoder with the provided options.
 * @param coderOptions ErasureCoderOptions object for encoder config.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.XORRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,35,"/**
 * Creates a RawErasureEncoder instance using provided options.
 * @param coderOptions Encoder options to configure the encoder.
 * @return A RawErasureEncoder object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),36,44,"/**
 * Prepares an ErasureCodingStep for encoding.
 * @param blockGroup ECBlockGroup to encode
 * @return An ErasureEncodingStep object
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DummyRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.DummyRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),31,34,"/**
 * Creates a RawErasureEncoder instance.
 * @param coderOptions Encoder options to use.
 * @return A RawErasureEncoder implementation.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetOutputBuffers,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetOutputBuffers(java.nio.ByteBuffer[],int)",87,91,"/**
 * Resets the buffers to their initial state based on dataLen.
 */",* Initialize the output buffers with ZERO bytes.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,toBuffers,org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:toBuffers(org.apache.hadoop.io.erasurecode.ECChunk[]),108,125,"/**
 * Converts an array of ECChunks to an array of ByteBuffers.
 * Handles null chunks and resets buffers if all zero.
 */
","* Convert an array of this chunks to an array of ByteBuffers
   * @param chunks chunks to convertToByteArrayState into buffers
   * @return an array of ByteBuffers",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/CoderUtil.java,resetOutputBuffers,"org.apache.hadoop.io.erasurecode.rawcoder.CoderUtil:resetOutputBuffers(byte[][],int[],int)",96,101,"/**
 * Resets output buffers for each provided buffer and offset.
 */
",* Initialize the output buffers with ZERO bytes.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),83,96,"/**
 * Encodes data, converts to ByteBufferState, then copies results.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/AbstractNativeRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.AbstractNativeRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),86,99,"/**
 * Decodes data using ByteBufferState, logs a performance advisory.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),57,71,"/**
 * Constructs a RSRawDecoder with given options.
 * @param coderOptions ErasureCoderOptions object.
 * @throws HadoopIllegalArgumentException if options are invalid.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),42,61,"/**
 * Initializes the RSRawEncoder with erasure coding options.
 * @param coderOptions ErasureCoderOptions object for configuration.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,generateDecodeMatrix,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:generateDecodeMatrix(int[]),143,176,"/**
 * Generates decode matrix based on erased indexes.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/util/GaloisField.java,getInstance,org.apache.hadoop.io.erasurecode.rawcoder.util.GaloisField:getInstance(),121,123,"/**
* Returns the default GaloisField instance.
* Uses default field size and primitive polynomial.
*/
","* Get the object performs Galois field arithmetic with default setting.
   * @return GaloisField.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsR,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsR(long),616,638,"/**
* Reads n bits from the input stream.
* @param n number of bits to read
* @return the read bits as a long value
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsGetBit,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetBit(),640,658,"/**
 * Gets the least significant bit from the bit buffer.
 * Updates bsLive and bsBuff accordingly.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,init,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:init(),756,772,"/**
 * Initializes the stream by writing magic bytes and block size.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,endCompression,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:endCompression(),833,849,"/**
 * Marks the end of the compression stream and writes the CRC.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,sendMTFValues,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:sendMTFValues(),951,992,"/**
 * Initializes and transmits MTF values for data compression.
 * Uses nMTF to determine number of coding tables.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,mainSort,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:mainSort(),1738,1901,"/**
 * Sorts data using a complex radix sort algorithm.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,<init>,"org.apache.hadoop.io.compress.CompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",49,51,"/**
 * Creates a CompressorStream with a default buffer size.
 * @param out Output stream to write compressed data.
 * @param compressor Compressor implementation.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockCompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int,int)",54,58,"/**
 * Creates a BlockCompressorStream.
 * @param out Output stream.
 * @param compressor Compressor.
 * @param bufferSize Buffer size.
 * @param compressionOverhead Overhead for compression.
 */
","* Create a {@link BlockCompressorStream}.
   * 
   * @param out stream
   * @param compressor compressor to be used
   * @param bufferSize size of buffer
   * @param compressionOverhead maximum 'overhead' of the compression 
   *                            algorithm with given bufferSize",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,write,org.apache.hadoop.io.compress.CompressorStream:write(int),115,119,"/**
 * Writes a single byte to the underlying output stream.
 * @param b The byte to be written.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,"org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",68,72,"/**
 * Constructs a DecompressorStream with default skip buffer size.
 * @param in Input stream to decompress.
 * @param decompressor Decompressor implementation.
 * @param bufferSize Buffer size for decompression.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,<init>,org.apache.hadoop.io.compress.PassthroughCodec$PassthroughDecompressorStream:<init>(java.io.InputStream),162,166,"/**
 * Creates a PassthroughDecompressorStream with the given input stream.
 * @param input The input stream to be decompressed.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,<init>,org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream),64,66,"/**
 * Creates a BlockDecompressorStream with the given input stream.
 * @param in The input stream to decompress.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,decompress,"org.apache.hadoop.io.compress.DecompressorStream:decompress(byte[],int,int)",108,173,"/**
 * Decompresses data from the input stream.
 * @param b buffer, off offset, len length to decompress
 * @return Number of bytes decompressed, or -1 if EOF.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,finish,org.apache.hadoop.io.compress.BlockCompressorStream:finish(),136,145,"/**
 * Completes compression, writes byte count, and flushes remaining data.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<init>(int),78,85,"/**
 * Creates a ZStandardDecompressor with a specified buffer size.
 * @param bufferSize Size of the direct buffers for decompression.
 */
","* Creates a new decompressor.
   * @param bufferSize bufferSize.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,finalize,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:finalize(),248,251,"/**
 * Resets the object's state before garbage collection.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,reset,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:reset(),307,311,"/**
 * Resets the object state, marking the input as finished.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,<init>,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(int,int,int)",94,103,"/**
 * Initializes a ZStandardCompressor with specified parameters.
 * @param level Compression level, input/output buffer sizes.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,decompress,"org.apache.hadoop.io.compress.BlockDecompressorStream:decompress(byte[],int,int)",68,112,"/**
 * Decompresses data from the input stream.
 * @param b buffer to write decompressed data
 * @param off offset in the buffer
 * @param len maximum number of bytes to decompress
 * @return number of decompressed bytes, or -1 if EOF.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/lz4/Lz4Compressor.java,<init>,org.apache.hadoop.io.compress.lz4.Lz4Compressor:<init>(),102,104,"/**
 * Creates a Lz4Compressor with the default direct buffer size.
 */
",* Creates a new compressor with the default buffer size.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodecClassByName,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClassByName(java.lang.String),274,281,"/**
 * Returns the Class object of the specified codec.
 * @param codecName Name of the codec.
 * @return Class of the codec or null if not found.
 */
","* Find the relevant compression codec for the codec's canonical class name
   * or by codec alias and returns its implemetation class.
   * <p>
   * Codec aliases are case insensitive.
   * <p>
   * The code alias is the short class name (without the package name).
   * If the short class name ends with 'Codec', then there are two aliases for
   * the codec, the complete short class name and the short class name without
   * the 'Codec' ending. For example for the 'GzipCodec' codec class name the
   * alias are 'gzip' and 'gzipcodec'.
   *
   * @param codecName the canonical class name of the codec
   * @return the codec class",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getCompressor,"org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration)",149,165,"/**
 * Gets a compressor from the pool or creates a new one.
 * @param codec CompressionCodec object
 * @param conf Hadoop Configuration object
 * @return Compressor object
 */
","* Get a {@link Compressor} for the given {@link CompressionCodec} from the 
   * pool or a new one.
   *
   * @param codec the <code>CompressionCodec</code> for which to get the 
   *              <code>Compressor</code>
   * @param conf the <code>Configuration</code> object which contains confs for creating or reinit the compressor
   * @return <code>Compressor</code> for the given 
   *         <code>CompressionCodec</code> from the pool or a new one",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getDecompressor,org.apache.hadoop.io.compress.CodecPool:getDecompressor(org.apache.hadoop.io.compress.CompressionCodec),180,195,"/**
 * Gets a Decompressor from the pool or creates a new one.
 * @param codec CompressionCodec to use for decompression.
 * @return Decompressor object.
 */
","* Get a {@link Decompressor} for the given {@link CompressionCodec} from the
   * pool or a new one.
   *  
   * @param codec the <code>CompressionCodec</code> for which to get the 
   *              <code>Decompressor</code>
   * @return <code>Decompressor</code> for the given 
   *         <code>CompressionCodec</code> the pool or a new one",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,returnCompressor,org.apache.hadoop.io.compress.CodecPool:returnCompressor(org.apache.hadoop.io.compress.Compressor),202,215,"/**
 * Returns a compressor to the pool, resetting or discarding it.
 * @param compressor Compressor object to return.
 */
","* Return the {@link Compressor} to the pool.
   * 
   * @param compressor the <code>Compressor</code> to be returned to the pool",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,returnDecompressor,org.apache.hadoop.io.compress.CodecPool:returnDecompressor(org.apache.hadoop.io.compress.Decompressor),223,236,"/**
 * Returns a Decompressor to the pool, resetting or discarding it.
 * @param decompressor The Decompressor to return.
 */
","* Return the {@link Decompressor} to the pool.
   * 
   * @param decompressor the <code>Decompressor</code> to be returned to the 
   *                     pool",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getCompressorType,org.apache.hadoop.io.compress.ZStandardCodec:getCompressorType(),151,155,"/**
 * Returns the type of compressor being used (ZStandard).
 */
","* Get the type of {@link Compressor} needed by this {@link CompressionCodec}.
   *
   * @return the type of compressor needed by this codec.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getDecompressorType,org.apache.hadoop.io.compress.ZStandardCodec:getDecompressorType(),209,213,"/**
 * Returns the type of Decompressor this class provides.
 */","* Get the type of {@link Decompressor} needed by
   * this {@link CompressionCodec}.
   *
   * @return the type of decompressor needed by this codec.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,getCompressorType,org.apache.hadoop.io.compress.DefaultCodec:getCompressorType(),69,72,"/**
 * Returns the compressor type based on the configuration.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,getDecompressorType,org.apache.hadoop.io.compress.DefaultCodec:getDecompressorType(),95,98,"/**
 * Returns the type of Decompressor used. Uses conf for lookup.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,<init>,org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:<init>(org.apache.hadoop.conf.Configuration),66,68,"/**
 * Initializes a BuiltInGzipCompressor with the given configuration.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipCompressor.java,reinit,org.apache.hadoop.io.compress.zlib.BuiltInGzipCompressor:reinit(org.apache.hadoop.conf.Configuration),167,175,"/**
 * Reinitializes the object with the given configuration.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,<init>,org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>(),122,126,"/**
 * Constructs a GzipZlibCompressor with default compression settings.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,<init>,org.apache.hadoop.io.compress.GzipCodec$GzipZlibCompressor:<init>(org.apache.hadoop.conf.Configuration),128,133,"/**
 * Constructs a GzipZlibCompressor with configuration settings.
 * @param conf Hadoop configuration object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(),234,239,"/**
 * Creates a ZlibCompressor with default compression settings.
 */","* Creates a new compressor with the default compression level.
   * Compressed data will be generated in ZLIB format.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibCompressor:<init>(org.apache.hadoop.conf.Configuration),245,250,"/**
 * Constructs a ZlibCompressor with default header and buffer size.
 * @param conf Configuration object for compression parameters.
 */
","* Creates a new compressor, taking settings from the configuration.
   * @param conf configuration.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibCompressor.java,reinit,org.apache.hadoop.io.compress.zlib.ZlibCompressor:reinit(org.apache.hadoop.conf.Configuration),283,298,"/**
 * Reinitializes the compressor with a new configuration from conf.
 * @param conf Configuration object containing new compression settings.
 */
","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration. It will reset the compressor's compression level
   * and compression strategy.
   * 
   * @param conf Configuration storing new settings",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:<init>(),352,354,"/**
 * Constructs a ZlibDirectDecompressor with default header and window size.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,"org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:<init>(org.apache.hadoop.io.compress.zlib.ZlibDecompressor$CompressionHeader,int)",356,358,"/**
 * Initializes a ZlibDirectDecompressor with a header and buffer size.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,<init>,org.apache.hadoop.io.compress.zlib.ZlibDecompressor:<init>(),117,119,"/**
 * Constructs a ZlibDecompressor with default header and buffer size.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,<init>,org.apache.hadoop.io.compress.GzipCodec$GzipZlibDecompressor:<init>(),137,139,"/**
 * Constructs a GzipZlibDecompressor with auto-detection and buffer size.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibDecompressor.java,reset,org.apache.hadoop.io.compress.zlib.ZlibDecompressor$ZlibDirectDecompressor:reset(),365,369,"/**
 * Resets the reader, calling superclass reset and marking EOF.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,executeHeaderState,org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:executeHeaderState(),257,358,"/**
 * Processes Gzip header state, advancing to the next stage.
 * Handles basic, extra, filename, comment, and CRC fields.
 */
","* Parse the gzip header (assuming we're in the appropriate state).
   * In order to deal with degenerate cases (e.g., user buffer is one byte
   * long), we copy (some) header bytes to another buffer.  (Filename,
   * comment, and extra-field bytes are simply skipped.)</p>
   *
   * See http://www.ietf.org/rfc/rfc1952.txt for the gzip spec.  Note that
   * no version of gzip to date (at least through 1.4.0, 2010-01-20) supports
   * the FHCRC header-CRC16 flagbit; instead, the implementation treats it
   * as a multi-file continuation flag (which it also doesn't support). :-(
   * Sun's JDK v6 (1.6) supports the header CRC, however, and so do we.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readCompressedStringArray,org.apache.hadoop.io.WritableUtils:readCompressedStringArray(java.io.DataInput),181,189,"/**
 * Reads a compressed string array from the input.
 * @param in DataInput to read from; returns null if len is -1.
 * @return String array read from the input.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VIntWritable.java,write,org.apache.hadoop.io.VIntWritable:write(java.io.DataOutput),54,57,"/**
 * Writes the integer value to the DataOutput stream.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,write,org.apache.hadoop.io.Text:write(java.io.DataOutput),395,399,"/**
 * Writes the data to the DataOutput.
 * @param out DataOutput to write to
 * @throws IOException if an I/O error occurs
 */
","* Serialize. Write this object to out length uses zero-compressed encoding.
   *
   * @see Writable#write(DataOutput)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,write,"org.apache.hadoop.io.Text:write(java.io.DataOutput,int)",401,409,"/**
 * Writes data to the output stream, ensuring it's within maxLength.
 * @param out DataOutput stream to write to.
 * @param maxLength Max length of data to write.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,write,org.apache.hadoop.security.token.delegation.DelegationKey:write(java.io.DataOutput),94,104,"/**
 * Writes the object to the output stream.
 * Writes keyId, expiryDate, and keyBytes (or -1 if null).
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,write,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:write(java.io.DataOutput),737,747,"/**
 * Writes the object to the output stream.
 * Writes renewDate, password length/value, and trackingId.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readVInt,org.apache.hadoop.io.WritableUtils:readVInt(java.io.DataInput),334,340,"/**
 * Reads a VInt from the input stream, casting to int.
 * @param stream DataInput stream to read from.
 * @throws IOException if value is too large for an int.
 */
","* Reads a zero-compressed encoded integer from input stream and returns it.
   * @param stream Binary input stream
   * @throws IOException raised on errors performing I/O.
   * @return deserialized integer from stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readVIntInRange,"org.apache.hadoop.io.WritableUtils:readVIntInRange(java.io.DataInput,int,int)",354,370,"/**
 * Reads a VInt from stream, ensuring it's within [lower, upper].
 * @param stream DataInput stream to read from
 * @param lower Lower bound of the expected integer value
 * @param upper Upper bound of the expected integer value
 * @return Integer value from stream, cast to int
 * @throws IOException if value is out of range
 */
","* Reads an integer from the input stream and returns it.
   *
   * This function validates that the integer is between [lower, upper],
   * inclusive.
   *
   * @param stream Binary input stream
   * @param lower input lower.
   * @param upper input upper.
   * @throws IOException raised on errors performing I/O.
   * @return deserialized integer from stream.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VLongWritable.java,readFields,org.apache.hadoop.io.VLongWritable:readFields(java.io.DataInput),49,52,"/**
 * Reads a long value from the input stream.
 * @param in DataInput stream to read from
 * @throws IOException if an I/O error occurs
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text:<init>(java.lang.String),95,97,"/**
 * Constructs a Text object with the given string.
 * @param string The string to be associated with the Text object.
 */
","* Construct from a string.
   * @param string input string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,find,"org.apache.hadoop.io.Text:find(java.lang.String,int)",187,221,"/**
 * Finds the starting index of 'what' in the byte array, starting from 'start'.
 * Returns -1 if not found.
 */
","* Finds any occurrence of <code>what</code> in the backing
   * buffer, starting as position <code>start</code>. The starting
   * position is measured in bytes and the return value is in
   * terms of byte position in the buffer. The backing buffer is
   * not converted to a string for this operation.
   *
   * @param what input what.
   * @param start input start.
   * @return byte position of the first occurrence of the search
   *         string in the UTF-8 buffer or -1 if not found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,writeString,"org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String)",582,588,"/**
 * Writes a String to a DataOutput.
 * @param out DataOutput to write to
 * @param s String to write
 * @return Length of the written String in bytes
 */
","* Write a UTF8 encoded string to out.
   *
   * @param out input out.
   * @param s input s.
   * @throws IOException raised on errors performing I/O.
   * @return a UTF8 encoded string to out.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,writeString,"org.apache.hadoop.io.Text:writeString(java.io.DataOutput,java.lang.String,int)",598,610,"/**
 * Writes a string to a DataOutput, limiting its length.
 * @param out Output stream.
 * @param s String to write.
 * @param maxLength Max length of the string in bytes.
 * @return Length of the written string.
 */
","* @return Write a UTF8 encoded string with a maximum size to out.
   *
   * @param out input out.
   * @param s input s.
   * @param maxLength input maxLength.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,org.apache.hadoop.io.Text:set(byte[]),246,254,"/**
 * Sets the byte array. Handles empty arrays, otherwise calls set(byte[], int, int).
 */
","* Set to a utf8 byte array. If the length of <code>utf8</code> is
   * <em>zero</em>, actually clear {@link #bytes} and any existing
   * data is lost.
   *
   * @param utf8 input utf8.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,set,org.apache.hadoop.io.Text:set(org.apache.hadoop.io.Text),260,263,"/**
* Sets the text content from another Text object.
* @param other The Text object to copy from.
*/
","* Copy a text.
   * @param other other.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readDefaultLine,"org.apache.hadoop.util.LineReader:readDefaultLine(org.apache.hadoop.io.Text,int,int)",197,263,"/**
 * Reads a line of text from the input stream, handling buffering.
 * @param str destination Text object
 * @param maxLineLength max line length
 * @param maxBytesToConsume max bytes to consume
 * @return number of bytes consumed
 */","* Read a line terminated by one of CR, LF, or CRLF.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readCustomLine,"org.apache.hadoop.util.LineReader:readCustomLine(org.apache.hadoop.io.Text,int,int)",268,371,"/**
 * Reads a custom line from the input stream up to a delimiter.
 * @param str Accumulates the read line.
 * @param maxLineLength Max length of the line to read.
 * @param maxBytesToConsume Max bytes to consume from stream.
 * @return Number of bytes consumed.
 */
",* Read a line terminated by a custom delimiter.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,toString,org.apache.hadoop.io.Text:toString(),337,344,"/**
 * Returns a string representation of the byte array.
 * Converts bytes to string, throws exception if decoding fails.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,<init>,org.apache.hadoop.io.SortedMapWritable:<init>(),45,48,"/**
 * Constructs a new SortedMapWritable.
 * Initializes the underlying TreeMap.
 */",default constructor.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,<init>,org.apache.hadoop.io.MapWritable:<init>(),43,46,"/**
 * Constructs a new MapWritable with an internal HashMap.
 */",Default constructor.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,write,org.apache.hadoop.io.SortedMapWritable:write(java.io.DataOutput),182,198,"/**
 * Writes the map to an output stream. Writes size, then key/value pairs.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,write,org.apache.hadoop.io.MapWritable:write(java.io.DataOutput),147,163,"/**
 * Writes the map to an output stream. Writes size, then key/value pairs.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getDeserializer,"org.apache.hadoop.io.SequenceFile$Reader:getDeserializer(org.apache.hadoop.io.serializer.SerializationFactory,java.lang.Class)",2163,2166,"/**
 * Retrieves a deserializer for the given class using the factory.
 * @param sf SerializationFactory to use.
 * @param c Class to get deserializer for.
 * @return Deserializer instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,<init>,org.apache.hadoop.util.bloom.Key:<init>(byte[]),87,89,"/**
 * Constructs a Key object from a byte array.
 * @param value The byte array representing the key.
 */
","* Constructor.
   * <p>
   * Builds a key with a default weight.
   * @param value The byte value of <i>this</i> key.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,compression,org.apache.hadoop.io.SequenceFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType),1052,1054,"/**
 * Creates a CompressionOption for the given compression type.
 * @param value The CompressionType to create an option for.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,compression,"org.apache.hadoop.io.MapFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",302,306,"/**
 * Sets compression type and codec for SequenceFile writer.
 * @param type CompressionType enum
 * @param codec CompressionCodec instance
 * @return Writer.Option object
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/DeserializerComparator.java,<init>,org.apache.hadoop.io.serializer.DeserializerComparator:<init>(org.apache.hadoop.io.serializer.Deserializer),52,57,"/**
* Initializes the comparator with a deserializer and opens it.
* @param deserializer The deserializer to use.
* @throws IOException if an I/O error occurs during opening.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/DeserializerComparator.java,compare,"org.apache.hadoop.io.serializer.DeserializerComparator:compare(byte[],int,int,byte[],int,int)",59,73,"/**
 * Compares two byte array segments after deserialization.
 * @param b1, s1, l1 first byte array segment
 * @param b2, s2, l2 second byte array segment
 * @return int comparison result
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,readFields,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:readFields(java.io.DataInput),99,104,"/**
 * Reads fields (bytesPerCRC, crcPerBlock, md5) from the input stream.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(byte[]),118,120,"/**
 * Computes the MD5 hash of the input byte array.
 * @param data The byte array to hash.
 * @return MD5Hash object representing the digest.
 */
","* Construct a hash value for a byte array.
   * @param data data.
   * @return MD5Hash.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(org.apache.hadoop.io.UTF8),195,197,"/**
 * Calculates the MD5 hash of a UTF8 string.
 * @param utf8 The UTF8 string to hash.
 * @return MD5Hash object representing the hash.
 */
","* Construct a hash value for a String.
   * @param utf8 utf8.
   * @return MD5Hash.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,<init>,org.apache.hadoop.io.MD5Hash:<init>(java.lang.String),61,63,"/**
* Initializes MD5Hash with the provided hexadecimal string.
* @param hex Hexadecimal representation of the MD5 hash.
*/
","* Constructs an MD5Hash from a hex string.
   * @param hex input hex.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,stream,org.apache.hadoop.io.SequenceFile$Writer:stream(org.apache.hadoop.fs.FSDataOutputStream),1020,1022,"/**
 * Creates a StreamOption from an FSDataOutputStream.
 * @param value The FSDataOutputStream to wrap.
 * @return A StreamOption object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,appendIfExists,org.apache.hadoop.io.SequenceFile$Writer:appendIfExists(boolean),1028,1030,"/**
 * Creates an AppendIfExistsOption if the value is true.
 * @param value boolean value to determine option creation
 * @return AppendIfExistsOption object
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,file,org.apache.hadoop.io.SequenceFile$Writer:file(org.apache.hadoop.fs.Path),994,996,"/**
 * Creates a FileOption representing the given Path.
 * @param value The Path representing the file.
 * @return A FileOption object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$MultipleLinearRandomRetry:shouldRetry(java.lang.Exception,int,int,boolean)",421,436,"/**
 * Determines if a retry should occur and calculates sleep time.
 * @param e Exception that occurred.
 * @return RetryAction object with retry decision and sleep time.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryForeverWithFixedSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryForeverWithFixedSleep(long,java.util.concurrent.TimeUnit)",82,86,"/**
 * Creates a retry policy that retries forever with a fixed sleep.
 * @param sleepTime The sleep time between retries.
 * @param timeUnit The time unit of the sleep time.
 */
","* <p>
   * Keep trying forever with a fixed time between attempts.
   * </p>
   *
   * @param sleepTime sleepTime.
   * @param timeUnit timeUnit.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryUpToMaximumCountWithFixedSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumCountWithFixedSleep(int,long,java.util.concurrent.TimeUnit)",99,101,"/**
 * Creates a RetryPolicy with a fixed sleep time between retries.
 * @param maxRetries Max retries allowed.
 * @param sleepTime Sleep duration.
 * @param timeUnit Time unit for sleepTime.
 * @return RetryPolicy instance.
 */
","* <p>
   * Keep trying a limited number of times, waiting a fixed time between attempts,
   * and then fail by re-throwing the exception.
   * </p>
   *
   * @param maxRetries maxRetries.
   * @param sleepTime sleepTime.
   * @param timeUnit timeUnit.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,<init>,"org.apache.hadoop.io.retry.RetryPolicies$RetryUpToMaximumTimeWithFixedSleep:<init>(long,long,java.util.concurrent.TimeUnit)",346,351,"/**
 * Initializes a retry strategy with max time, sleep time, and unit.
 * @param maxTime Maximum time for retries.
 * @param sleepTime Sleep time between retries.
 * @param timeUnit Time unit for maxTime and sleepTime.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,exponentialBackoffRetry,"org.apache.hadoop.io.retry.RetryPolicies:exponentialBackoffRetry(int,long,java.util.concurrent.TimeUnit)",148,151,"/**
 * Creates an ExponentialBackoffRetry policy.
 * @param maxRetries Max retry attempts.
 * @param sleepTime Initial sleep time.
 * @param timeUnit Time unit for sleepTime.
 */
","* <p>
   * Keep trying a limited number of times, waiting a growing amount of time between attempts,
   * and then fail by re-throwing the exception.
   * The time between attempts is <code>sleepTime</code> mutliplied by a random
   * number in the range of [0, 2 to the number of retries)
   * </p>
   *
   *
   * @param timeUnit timeUnit.
   * @param maxRetries maxRetries.
   * @param sleepTime sleepTime.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryUpToMaximumCountWithProportionalSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumCountWithProportionalSleep(int,long,java.util.concurrent.TimeUnit)",130,132,"/**
 * Creates a retry policy with proportional sleep based on retries.
 * @param maxRetries Max retry attempts.
 * @param sleepTime Initial sleep time.
 * @param timeUnit Time unit for sleepTime.
 */
","* <p>
   * Keep trying a limited number of times, waiting a growing amount of time between attempts,
   * and then fail by re-throwing the exception.
   * The time between attempts is <code>sleepTime</code> mutliplied by the number of tries so far.
   * </p>
   *
   * @param sleepTime sleepTime.
   * @param maxRetries maxRetries.
   * @param timeUnit timeUnit.
   * @return RetryPolicy.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,"org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int,long,long)",210,215,"/**
 * Creates a FailoverOnNetworkExceptionRetry policy.
 * @param fallbackPolicy Fallback policy on network exception.
 * @return RetryPolicy instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,newAsyncCall,"org.apache.hadoop.io.retry.AsyncCallHandler:newAsyncCall(java.lang.reflect.Method,java.lang.Object[],boolean,int,org.apache.hadoop.io.retry.RetryInvocationHandler)",322,327,"/**
 * Creates a new AsyncCall instance.
 * @param method The method to invoke asynchronously.
 * @param args Method arguments.
 * @return A new AsyncCall object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,newRetryInfo,"org.apache.hadoop.io.retry.RetryInvocationHandler$RetryInfo:newRetryInfo(org.apache.hadoop.io.retry.RetryPolicy,java.lang.Exception,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,boolean,long)",273,302,"/**
 * Creates a RetryInfo object based on policy, exception, and counters.
 * Returns RetryInfo with delay, action, failover count, and exception.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,tryStop,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue$Processor:tryStop(org.apache.hadoop.util.Daemon),186,190,"/**
 * Attempts to stop a daemon if the queue is empty for GRACE_PERIOD.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,read,org.apache.hadoop.security.Groups$TimerToTickerAdapter:read(),292,296,"/**
 * Reads the current monotonic time in milliseconds.
 * Returns the time as a long value.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,unlock,org.apache.hadoop.util.InstrumentedWriteLock:unlock(),59,69,"/**
 * Releases the write lock, reporting metrics if held exclusively.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,startLockTiming,org.apache.hadoop.util.InstrumentedWriteLock:startLockTiming(),74,79,"/**
 * Records the start timestamp for write lock timing if held.
 */",* Starts timing for the instrumented write lock.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,<init>,"org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.Lock,long,long,org.apache.hadoop.util.Timer)",87,99,"/**
 * Constructs an InstrumentedLock with provided dependencies.
 * @param name Lock name, logger, lock, timers, thresholds.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,startLockTiming,org.apache.hadoop.util.InstrumentedLock:startLockTiming(),177,179,"/**
 * Records the start time for lock acquisition using the clock.
 */",* Starts timing for the instrumented lock.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,unlock,org.apache.hadoop.util.InstrumentedReadLock:unlock(),65,75,"/**
 * Releases the read lock, reporting timing if it was the last reader.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,startLockTiming,org.apache.hadoop.util.InstrumentedReadLock:startLockTiming(),81,86,"/**
 * Records read lock start time if read lock is already held.
 */","* Starts timing for the instrumented read lock.
   * It records the time to ThreadLocal.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.RetryInvocationHandler:<init>(org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",327,330,"/**
 * Constructs a RetryInvocationHandler with default retry policies.
 * @param proxyProvider Provider for failover proxies.
 * @param retryPolicy The retry policy to use.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,org.apache.hadoop.io.retry.FailoverProxyProvider,java.util.Map,org.apache.hadoop.io.retry.RetryPolicy)",100,110,"/**
 * Creates a dynamic proxy implementing the given interface.
 * @param iface Interface to implement.
 * @param proxyProvider Provider for the interface.
 * @param methodNameToPolicyMap Retry policies for methods.
 * @param defaultPolicy Default retry policy.
 * @return Proxy instance.
 */
","* Create a proxy for an interface of implementations of that interface using
   * the given {@link FailoverProxyProvider} and the a set of retry policies
   * specified by method name. If no retry policy is defined for a method then a
   * default of {@link RetryPolicies#TRY_ONCE_THEN_FAIL} is used.
   * 
   * @param iface the interface that the retry will implement
   * @param proxyProvider provides implementation instances whose methods should be retried
   * @param methodNameToPolicyMap map of method names to retry policies
   * @param defaultPolicy defaultPolicy.
   * @param <T> T.
   * @return the retry proxy",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,failover,"org.apache.hadoop.io.retry.RetryInvocationHandler$ProxyDescriptor:failover(long,java.lang.reflect.Method,int)",217,229,"/**
 * Performs failover if failoverCount matches expected count.
 * Logs warning if failover occurred since call start.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,log,"org.apache.hadoop.io.retry.RetryInvocationHandler:log(java.lang.reflect.Method,boolean,int,int,long,java.lang.Exception)",398,430,"/**
 * Logs a method invocation failure or retry attempt with details.
 * @param method The invoked method.
 * @param isFailover True if a failover attempt.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/LossyRetryInvocationHandler.java,invokeMethod,"org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invokeMethod(java.lang.reflect.Method,java.lang.Object[])",48,65,"/**
 * Invokes method, drops response if retry count is low.
 * @param method The method to invoke.
 * @param args Arguments for the method.
 * @return Result of method invocation, or drops response.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,hashCode,org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:hashCode(),150,153,"/**
 * Returns the hash code, based on the multipleLinearRandomRetry.
 */","* Similarly, remoteExceptionToRetry is ignored as part of hashCode since it
     * does not affect connection failure handling.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,equals,org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:equals(java.lang.Object),135,144,"/**
 * Checks if two WrapperRetryPolicy objects are equal.
 * Compares the multipleLinearRandomRetry field.
 */
","* remoteExceptionToRetry is ignored as part of equals since it does not
     * affect connection failure handling.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$RetryLimited:shouldRetry(java.lang.Exception,int,int,boolean)",283,291,"/**
 * Determines if a retry should occur based on exception details.
 * @param e Exception occurred.
 * @return RetryAction object indicating retry decision.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,mayThrow,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:mayThrow(java.util.List),314,324,"/**
 * Throws a combined IOException if replication is insufficient.
 * @param ioExceptions List of IOException instances to combine.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:getFileStatus(org.apache.hadoop.fs.Path),875,915,"/**
 * Gets the file status for a given path.
 * @param f the path to get the file status for
 * @throws IOException if file status cannot be retrieved
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MultipleIOException.java,build,org.apache.hadoop.io.MultipleIOException$Builder:build(),83,85,"/**
 * Creates and returns an IOException based on collected exceptions.
 */","* @return null if nothing is added to this builder;
     *         otherwise, return an {@link IOException}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$DataIndex:<init>(java.lang.String),878,883,"/**
 * Initializes a DataIndex with a default compression algorithm.
 * @param defaultCompressionAlgorithmName Algorithm name.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getSupportedCompressionAlgorithms,org.apache.hadoop.io.file.tfile.TFile:getSupportedCompressionAlgorithms(),198,200,"/**
 * Returns an array of supported compression algorithms.
 */","* Get names of supported compression algorithms. The names are acceptable by
   * TFile.Writer.
   * 
   * @return Array of strings, each represents a supported compression
   *         algorithm. Currently, the following compression algorithms are
   *         supported.
   *         <ul>
   *         <li>""none"" - No compression.
   *         <li>""lzo"" - LZO compression.
   *         <li>""gz"" - GZIP compression.
   *         </ul>",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressionName,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:getCompressionName(),576,578,"/**
 * Returns the compression name from the rBlkState object.
 */","* Get the name of the compression algorithm used to compress the block.
       * 
       * @return name of the compression algorithm.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,register,"org.apache.hadoop.io.file.tfile.BCFile$Writer$MetaBlockRegister:register(long,long,long)",451,455,"/**
 * Adds a new metadata entry to the index.
 * @param raw raw data size, begin & end define the block region.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,readString,org.apache.hadoop.io.file.tfile.Utils:readString(java.io.DataInput),276,282,"/**
 * Reads a string from the DataInput. Returns null if length is -1.
 */
","* Read a String as a VInt n, followed by n Bytes in Text format.
   * 
   * @param in
   *          The input stream.
   * @return The string
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:<init>(java.io.DataInput),2302,2307,"/**
 * Constructs a TFileIndexEntry from a DataInput stream.
 * @param in Input stream to read the entry data from.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,readLength,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:readLength(),102,109,"/**
 * Reads the length of the data chunk from the input stream.
 * Updates 'remain' and 'lastChunk' based on the value.
 */
","* Reading the length of next chunk.
     * 
     * @throws java.io.IOException
     *           when no more data is available.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:<init>(java.lang.String),2051,2057,"/**
 * Constructs a TFileMeta object with the given comparator.
 * @param comparator Comparator string, empty if null.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,makeComparator,org.apache.hadoop.io.file.tfile.TFile:makeComparator(java.lang.String),176,178,"/**
 * Creates a comparator for RawComparable objects using the given name.
 */
","* Make a raw comparator from a string name.
   * 
   * @param name
   *          Comparator name
   * @return A RawComparable comparator.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,writeChunk,"org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:writeChunk(byte[],int,int,boolean)",253,266,"/**
 * Writes a chunk of data to the output stream.
 * @param chunk Data to write.
 * @param offset Start offset in the chunk.
 * @param len Length of data to write.
 * @param last True if this is the last chunk.
 */
","* Write out a chunk.
     * 
     * @param chunk
     *          The chunk buffer.
     * @param offset
     *          Offset to chunk buffer for the beginning of chunk.
     * @param len
     * @param last
     *          Is this the last call to flushBuffer?",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,writeBufData,"org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:writeBufData(byte[],int,int)",279,287,"/**
 * Writes data to the output stream, possibly preceded by a VInt.
 * @param data Data to write.
 * @param offset Offset in the data array.
 * @param len Length of data to write.
 */
","* Write out a chunk that is a concatenation of the internal buffer plus
     * user supplied data. This will never be the last block.
     * 
     * @param data
     *          User supplied data buffer.
     * @param offset
     *          Offset to user data buffer.
     * @param len
     *          User data buffer size.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,write,org.apache.hadoop.io.file.tfile.TFile$TFileIndexEntry:write(java.io.DataOutput),2335,2339,"/**
 * Writes the key and kvEntries to the DataOutput stream.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,<init>,"org.apache.hadoop.io.file.tfile.Chunk$SingleChunkEncoder:<init>(java.io.DataOutputStream,int)",377,382,"/**
 * Initializes the encoder with an output stream and chunk size.
 * @param out Output stream to write encoded data.
 * @param size Chunk size in bytes.
 */
","* Constructor.
     * 
     * @param out
     *          the underlying output stream.
     * @param size
     *          The total # of bytes to be written as a single chunk.
     * @throws java.io.IOException
     *           if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getEntryComparator,org.apache.hadoop.io.file.tfile.TFile$Reader:getEntryComparator(),925,944,"/**
 * Returns a comparator for Scanner.Entry objects, requiring sorted data.
 */","* Get a Comparator object to compare Entries. It is useful when you want
     * stores the entries in a collection (such as PriorityQueue) and perform
     * sorting or comparison among entries based on the keys without copying out
     * the key.
     * 
     * @return An Entry Comparator..",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareKeys,"org.apache.hadoop.io.file.tfile.TFile$Reader:compareKeys(byte[],int,int,byte[],int,int)",1007,1012,"/**
 * Compares two byte array segments using a predefined comparator.
 * @param a, b byte arrays; o1, o2 offsets; l1, l2 lengths
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareKeys,"org.apache.hadoop.io.file.tfile.TFile$Reader:compareKeys(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1014,1019,"/**
 * Compares two keys using the defined comparator.
 * @param a first key
 * @param b second key
 * @return int result of the comparison
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,setFirstKey,"org.apache.hadoop.io.file.tfile.TFile$TFileIndex:setFirstKey(byte[],int,int)",2250,2253,"/**
 * Copies a portion of a byte array to the firstKey buffer.
 * @param key Source byte array.
 * @param offset Starting offset in the key array.
 * @param length Number of bytes to copy.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLastKey,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLastKey(),2255,2260,"/**
 * Returns the last key in the index. Returns null if empty.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,atEnd,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:atEnd(),1588,1590,"/**
* Checks if the current location is at or beyond the end location.
*/
","* Is cursor at the end location?
       * 
       * @return true if the cursor is at the end location.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLocationNear,org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationNear(long),1031,1035,"/**
 * Gets the location near a given offset.
 * @param offset The offset to find the location near.
 * @return Location object or end if not found.
 */
","* Get the location pointing to the beginning of the first key-value pair in
     * a compressed block whose byte offset in the TFile is greater than or
     * equal to the specified offset.
     * 
     * @param offset
     *          the user supplied offset.
     * @return the location to the corresponding entry; or end() if no such
     *         entry exists.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,clone,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:clone(),759,762,"/**
 * Creates and returns a copy of this Location object.
 */",* @see java.lang.Object#clone(),,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLocationByRecordNum,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:getLocationByRecordNum(long),2238,2242,"/**
 * Finds the location of a record based on its record number.
 * @param recNum The record number to locate.
 * @return A Reader.Location object representing the record's position.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$Reader$Location:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),713,715,"/**
 * Copies data from another Location object.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getValue,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(byte[]),1840,1842,"/**
 * Reads an integer value from a byte array.
 * @param buf byte array to read from
 * @return int value read from the array
 */
","* Copy value into user-supplied buffer. User supplied buffer must be
         * large enough to hold the whole value. The value part of the key-value
         * pair pointed by the current cursor is not cached and can only be
         * examined once. Calling any of the following functions more than once
         * without moving the cursor will result in exception:
         * {@link #getValue(byte[])}, {@link #getValue(byte[], int)},
         * {@link #getValueStream}.
         *
         * @param buf buf.
         * @return the length of the value. Does not require
         *         isValueLengthKnown() to be true.
         * @throws IOException raised on errors performing I/O.
         *",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,set,org.apache.hadoop.io.UTF8:set(java.lang.String),96,118,"/**
 * Sets the string value, truncating if too long, and updates length.
 */","* Set to contain the contents of a string.
   * @param string input string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,toByteArray,org.apache.hadoop.io.WritableUtils:toByteArray(org.apache.hadoop.io.Writable[]),461,472,"/**
 * Converts a set of Writables to a byte array.
 * @param writables Writables to convert.
 * @return Byte array representation of the writables.
 */
","* Convert writables to a byte array.
   * @param writables input writables.
   * @return ByteArray.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,getBytes,org.apache.hadoop.io.UTF8:getBytes(java.lang.String),238,249,"/**
 * Converts a string to a byte array using UTF-8 encoding.
 * @param string The string to convert.
 * @return Byte array representing the string in UTF-8.
 */
","* @return Convert a string to a UTF-8 encoded byte array.
   * @see String#getBytes(String)
   * @param string input string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeBuffer,org.apache.hadoop.io.SequenceFile$BlockCompressWriter:writeBuffer(org.apache.hadoop.io.DataOutputBuffer),1623,1635,"/**
 * Writes uncompressed data to the buffer, compresses, and writes it.
 * @param uncompressedDataBuffer Data to be compressed and written.
 * @throws IOException if an I/O error occurs.
 */
",Workhorse to check and write out compressed data/lengths,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,byteArrayForBloomKey,org.apache.hadoop.io.BloomMapFile:byteArrayForBloomKey(org.apache.hadoop.io.DataOutputBuffer),71,79,"/**
 * Extracts the relevant byte array from a DataOutputBuffer.
 * @param buf The buffer to extract from.
 * @return A byte array containing the relevant data.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,lessThan,"org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:lessThan(java.lang.Object,java.lang.Object)",3537,3548,"/**
 * Compares two SegmentDescriptors based on their key data.
 * @param a, b SegmentDescriptors to compare.
 * @return True if 'a' is less than 'b'.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/AbstractMapWritable.java,copy,org.apache.hadoop.io.AbstractMapWritable:copy(org.apache.hadoop.io.Writable),125,142,"/**
 * Copies data from the given Writable object to this object.
 * @param other The Writable object to copy from.
 * @throws IllegalArgumentException if other is null.
 */
","* Used by child copy constructors.
   * @param other other.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$RecordCompressWriter:append(java.lang.Object,java.lang.Object)",1545,1575,"/**
 * Appends a key-value pair to the buffer, serializing and compressing.
 * @param key The key to append.
 * @param val The value to append.
 */
",Append a key/value pair.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,toString,org.apache.hadoop.io.DefaultStringifier:toString(java.lang.Object),83,90,"/**
 * Serializes object to Base64 string.
 * @param obj Object to serialize
 * @return Base64 encoded string representation
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,checkKey,org.apache.hadoop.io.MapFile$Writer:checkKey(org.apache.hadoop.io.WritableComparable),418,429,"/**
 * Checks key order and updates the lastKey.
 * @param key The key to check and update lastKey with.
 * @throws IOException if the key is out of order.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/TokenIdentifier.java,getBytes,org.apache.hadoop.security.token.TokenIdentifier:getBytes(),60,68,"/**
 * Returns the data as a byte array.
 * Uses a buffer to serialize the object.
 */
","* Get the bytes for the token identifier
   * @return the bytes of the identifier",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,encodeWritable,org.apache.hadoop.security.token.Token:encodeWritable(org.apache.hadoop.io.Writable),340,347,"/**
 * Encodes a Writable object to a Base64 string.
 * @param obj The Writable object to encode.
 * @return Base64 encoded string representation of the object.
 */
","* Generate a string with the url-quoted base64 encoded serialized form
   * of the Writable.
   * @param obj the object to serialize
   * @return the encoded string
   * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,moveData,org.apache.hadoop.util.ReflectionUtils$CopyInCopyOutBuffer:moveData(),314,316,"/**
 * Resets the input buffer with data from the output buffer.
 */",* Move the data from the output buffer to the input buffer.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNSDomainNameResolver.java,getAllResolvedHostnameByDomainName,"org.apache.hadoop.net.DNSDomainNameResolver:getAllResolvedHostnameByDomainName(java.lang.String,boolean)",64,80,"/**
 * Gets resolved hostnames for a domain, using FQDN if specified.
 * @param domainName Domain name to resolve.
 * @param useFQDN Whether to use fully qualified domain names.
 * @return Array of resolved hostnames.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getDistanceByPath,"org.apache.hadoop.net.NetworkTopology:getDistanceByPath(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node)",376,400,"/**
 * Calculates the distance between two nodes based on their paths.
 * @param node1 The first node.
 * @param node2 The second node.
 * @return The distance between the nodes.
 */
","Return the distance between two nodes by comparing their network paths
   * without checking if they belong to the same ancestor node by reference.
   * It is assumed that the distance from one node to its parent is 1
   * The distance between two nodes is calculated by summing up their distances
   * to their closest common ancestor.
   * @param node1 one node
   * @param node2 another node
   * @return the distance between node1 and node2",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,equals,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:equals(java.lang.Object),108,112,"/**
* Checks if this object is equal to another object.
* Delegates to the superclass's equals method.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,equals,org.apache.hadoop.net.InnerNodeImpl:equals(java.lang.Object),316,319,"/**
* Checks if this object is equal to another object.
* Delegates to the superclass's equals method.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,hashCode,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:hashCode(),114,118,"/**
 * Returns the hash code for this object, satisfying FindBugs.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,hashCode,org.apache.hadoop.net.InnerNodeImpl:hashCode(),311,314,"/**
 * Returns the hash code value for this object. Delegates to super.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getNodeForNetworkLocation,org.apache.hadoop.net.NetworkTopology:getNodeForNetworkLocation(org.apache.hadoop.net.Node),193,195,"/**
 * Retrieves a node based on the provided node's network location.
 * @param node The node to get the network location from.
 * @return The node associated with the network location.
 */
","* Return a reference to the node given its string representation.
   * Default implementation delegates to {@link #getNode(String)}.
   * 
   * <p>To be overridden in subclasses for specific NetworkTopology 
   * implementations, as alternative to overriding the full {@link #add(Node)}
   *  method.
   * 
   * @param node The string representation of this node's network location is
   * used to retrieve a Node object. 
   * @return a reference to the node; null if the node is not in the tree
   * 
   * @see #add(Node)
   * @see #getNode(String)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getLeaves,org.apache.hadoop.net.NetworkTopology:getLeaves(java.lang.String),643,655,"/**
 * Gets all leaf nodes within the specified scope.
 * @param scope The scope to search for leaf nodes.
 * @return A list of leaf Node objects.
 */
","return leaves in <i>scope</i>
   * @param scope a path string
   * @return leaves nodes under specific scope",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,countNumOfAvailableNodes,"org.apache.hadoop.net.NetworkTopology:countNumOfAvailableNodes(java.lang.String,java.util.Collection)",664,711,"/**
 * Counts available nodes within a scope, excluding specified nodes.
 * @param scope Node scope identifier.
 * @param excludedNodes Nodes to exclude from the count.
 * @return Number of available nodes.
 */
","return the number of leaves in <i>scope</i> but not in <i>excludedNodes</i>
   * if scope starts with ~, return the number of nodes that are not
   * in <i>scope</i> and <i>excludedNodes</i>; 
   * @param scope a path string that may start with ~
   * @param excludedNodes a list of nodes
   * @return number of available nodes",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,interRemoveNodeWithEmptyRack,org.apache.hadoop.net.NetworkTopology:interRemoveNodeWithEmptyRack(org.apache.hadoop.net.Node),1104,1122,"/**
 * Removes a node from the rack map, updates rack status.
 * @param node The node to remove.
 */
","* Internal function for update empty rack number
   * for remove or decommission a node.
   * @param node node to be removed; can be null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistance,"org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer,boolean)",970,1000,"/**
 * Sorts nodes by distance from a reader, using secondary sort if provided.
 * @param reader The reader node.
 * @param nodes Array of nodes to sort.
 * @param activeLen Length of the active portion of the nodes array.
 */
","* Sort nodes array by network distance to <i>reader</i>.
   * <p>
   * As an additional twist, we also randomize the nodes at each network
   * distance. This helps with load balancing when there is data skew.
   * And it helps choose node with more fast storage type.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array
   * @param nonDataNodeReader True if the reader is not a datanode",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,recommissionNode,org.apache.hadoop.net.NetworkTopology:recommissionNode(org.apache.hadoop.net.Node),1040,1055,"/**
 * Recommissions a node, removing it from decommission list.
 * @param node The node to recommission. Throws exception if inner.
 */","* Update empty rack number when add a node like recommission.
   * @param node node to be added; can be null",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,"org.apache.hadoop.net.SocketInputStream$Reader:<init>(java.nio.channels.ReadableByteChannel,long)",50,53,"/**
 * Constructs a Reader from a ReadableByteChannel with a timeout.
 * @param channel Channel to read from.
 * @param timeout Timeout in milliseconds.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,<init>,"org.apache.hadoop.net.SocketOutputStream$Writer:<init>(java.nio.channels.WritableByteChannel,long)",55,58,"/**
* Initializes a Writer with a channel and timeout.
* @param channel WritableByteChannel to write to
* @param timeout Timeout in milliseconds
* @throws IOException if an I/O error occurs
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,write,org.apache.hadoop.net.SocketOutputStream:write(int),101,109,"/**
 * Writes a single byte to the underlying output stream.
 * @param b The byte to be written.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,transferToFully,"org.apache.hadoop.net.SocketOutputStream:transferToFully(java.nio.channels.FileChannel,long,int)",264,267,"/**
 * Transfers a specified number of bytes to a channel.
 * @param fileCh The destination FileChannel.
 * @param position The offset at which to transfer.
 * @param count The number of bytes to transfer.
 */
","* Call
   * {@link #transferToFully(FileChannel, long, int, LongWritable, LongWritable)
   * }
   * with null <code>waitForWritableTime</code> and <code>transferToTime</code>.
   *
   * @param fileCh input fileCh.
   * @param position input position.
   * @param count input count.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/CachedDNSToSwitchMapping.java,resolve,org.apache.hadoop.net.CachedDNSToSwitchMapping:resolve(java.util.List),106,125,"/**
* Resolves a list of hostnames, caching the results.
* @param names List of hostnames to resolve.
* @return List of resolved hostnames.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,wrapException,"org.apache.hadoop.net.NetUtils:wrapException(java.lang.String,int,java.lang.String,int,java.io.IOException)",850,949,"/**
 * Wraps an IOException with a more informative message.
 * @param exception The IOException to wrap.
 */","* Take an IOException , the local host port and remote host port details and
   * return an IOException with the input exception as the cause and also
   * include the host details. The new exception provides the stack trace of the
   * place where the exception is thrown and some extra diagnostics information.
   * If the exception is of type BindException, ConnectException,
   * UnknownHostException, SocketTimeoutException or has a String constructor,
   * return a new one of the same type; Otherwise return an IOException.
   *
   * @param destHost target host (nullable)
   * @param destPort target port
   * @param localHost local host (nullable)
   * @param localPort local port
   * @param exception the caught exception.
   * @return an exception to throw",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,<init>,org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:<init>(),144,144,"/**
 * Default constructor for RawScriptBasedMappingWithDependency.
 */","* Constructor. The mapping is not ready to use until
     * {@link #setConf(Configuration)} has been called",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,<init>,org.apache.hadoop.net.TableMapping:<init>(),63,65,"/**
 * Constructs a TableMapping with a RawTableMapping as its base.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.net.DNSToSwitchMapping),95,97,"/**
 * Initializes a ScriptBasedMapping with a raw DNSToSwitchMapping.
 */","* Create an instance from the given raw mapping
   * @param rawMap raw DNSTOSwithMapping",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,<init>,org.apache.hadoop.net.InnerNodeImpl:<init>(java.lang.String),48,50,"/**
 * Constructs an InnerNodeImpl with the given path.
 * @param path The path for this node.
 */
","* Construct an InnerNode from a path-like string.
   * @param path input path.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:<init>(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.ChRootedFileSystem)",99,102,"/**
 * Constructs a NflyNode with hostname, rack name, and file system.
 * @param hostName Hostname of the node.
 * @param rackName Rack name of the node.
 * @param fs The ChRootedFileSystem for the node.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,<init>,"org.apache.hadoop.net.InnerNodeImpl:<init>(java.lang.String,java.lang.String,org.apache.hadoop.net.InnerNode,int)",60,63,"/**
 * Constructs an InnerNodeImpl with the given name, location,
 * parent, and level.
 */
","* Construct an InnerNode
   * from its name, its network location, its parent, and its level.
   * @param name input name.
   * @param location input location.
   * @param parent input parent.
   * @param level input level.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,kick,org.apache.hadoop.net.unix.DomainSocketWatcher:kick(),359,374,"/**
 * Kicks the first notification socket, if not already kicked.
 */",* Wake up the DomainSocketWatcher thread.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,handle,org.apache.hadoop.net.unix.DomainSocketWatcher$NotificationHandler:handle(org.apache.hadoop.net.unix.DomainSocket),103,130,"/**
 * Handles a domain socket, reading from it and handling errors.
 * @param sock The domain socket to handle.
 * @return True if socket closed, false otherwise.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,bindAndListen,org.apache.hadoop.net.unix.DomainSocket:bindAndListen(java.lang.String),191,200,"/**
 * Binds and listens on a domain socket path.
 * @param path Socket path to bind to.
 * @return DomainSocket object.
 * @throws IOException if an I/O error occurs.
 */
","* Create a new DomainSocket listening on the given path.
   *
   * @param path         The path to bind and listen on.
   * @return             The new DomainSocket.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,socketpair,org.apache.hadoop.net.unix.DomainSocket:socketpair(),210,216,"/**
 * Creates a domain socket pair.
 * @return An array of two DomainSocket objects.
 * @throws IOException if an error occurs.
 */
","* Create a pair of UNIX domain sockets which are connected to each other
   * by calling socketpair(2).
   *
   * @return                An array of two UNIX domain sockets connected to
   *                        each other.
   * @throws IOException    on error.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,connect,org.apache.hadoop.net.unix.DomainSocket:connect(java.lang.String),255,261,"/**
 * Connects to a domain socket at the given path.
 * @param path socket path
 * @return DomainSocket object
 * @throws IOException if connection fails
 */
","* Create a new DomainSocket connected to the given path.
   *
   * @param path              The path to connect to.
   * @throws IOException      If there was an I/O error performing the connect.
   *
   * @return                  The new DomainSocket.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,sendCallbackAndRemove,"org.apache.hadoop.net.unix.DomainSocketWatcher:sendCallbackAndRemove(java.lang.String,java.util.TreeMap,org.apache.hadoop.net.unix.DomainSocketWatcher$FdSet,int)",435,440,"/**
 * Sends a callback and removes the entry if successful.
 * @param caller Caller ID, entries, fdSet, fd.
 */
","* Send callback, and if the domain socket was closed as a result of
   * processing, then also remove the entry for the file descriptor.
   *
   * @param caller reason for call
   * @param entries mapping of file descriptor to entry
   * @param fdSet set of file descriptors
   * @param fd file descriptor",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,isOpen,org.apache.hadoop.net.unix.DomainSocket$DomainChannel:isOpen(),592,595,"/**
 * Checks if the domain socket is open.
 * Delegates to the underlying DomainSocket's isOpen() method.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket$DomainChannel:close(),597,600,"/**
 * Closes the domain socket, releasing associated resources.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket$DomainOutputStream:close(),558,561,"/**
 * Closes the domain socket, releasing associated resources.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,close,org.apache.hadoop.net.unix.DomainSocketWatcher:close(),266,284,"/**
* Closes the resource, closing a socket and joining the watcher thread.
*/","* Close the DomainSocketWatcher and wait for its thread to terminate.
   *
   * If there is more than one close, all but the first will be ignored.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,close,org.apache.hadoop.net.unix.DomainSocket$DomainInputStream:close(),547,550,"/**
 * Closes this socket, closing the underlying domain socket.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,get,org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:get(java.nio.channels.SelectableChannel),387,403,"/**
* Retrieves a SelectorInfo from the provider's queue.
* Creates a new one if the queue is empty.
* @param channel The SelectableChannel to associate with.
* @return SelectorInfo object.
*/
","* Takes one selector from end of LRU list of free selectors.
     * If there are no selectors awailable, it creates a new selector.
     * Also invokes trimIdleSelectors(). 
     * 
     * @param channel
     * @return 
     * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,release,org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:release(org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool$SelectorInfo),411,417,"/**
 * Releases a SelectorInfo, updating activity time and re-queuing.
 */","* puts selector back at the end of LRU list of free selectos.
     * Also invokes trimIdleSelectors().
     * 
     * @param info",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,getLeaf,"org.apache.hadoop.net.InnerNodeImpl:getLeaf(int,org.apache.hadoop.net.Node)",253,300,"/**
 * Retrieves a leaf node at the given index, excluding a node.
 * @param leafIndex index of the leaf node to retrieve
 * @param excludedNode node to exclude from the search
 * @return LeafNode object or null if not found
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,remove,org.apache.hadoop.net.InnerNodeImpl:remove(org.apache.hadoop.net.Node),189,234,"/**
 * Removes a node from the tree. Throws exception if not a descendant.
 * @param n Node to remove.
 * @return True if node was removed, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getIPs,org.apache.hadoop.net.DNS:getIPs(java.lang.String),152,155,"/**
 * Gets IP addresses for an interface.
 * @param strInterface Interface name.
 * @return Array of IP addresses or null if error.
 */
","* @return Like {@link DNS#getIPs(String, boolean)}, but returns all
   * IPs associated with the given interface and its subinterfaces.
   *
   * @param strInterface input strInterface.
   * @throws UnknownHostException
   * If no IP address for the local host could be found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getHosts,"org.apache.hadoop.net.DNS:getHosts(java.lang.String,java.lang.String,boolean)",248,277,"/**
 * Resolves hostnames for a given network interface.
 * @param strInterface Interface to resolve.
 * @param nameserver Nameserver to use.
 * @param tryfallbackResolution Fallback resolution flag.
 * @return Array of hostnames.
 */
","* Returns all the host names associated by the provided nameserver with the
   * address bound to the specified network interface
   *
   * @param strInterface
   *            The name of the network interface or subinterface to query
   *            (e.g. eth0 or eth0:0)
   * @param nameserver
   *            The DNS host name
   * @param tryfallbackResolution
   *            if true and if reverse DNS resolution fails then attempt to
   *            resolve the hostname with
   *            {@link InetAddress#getCanonicalHostName()} which includes
   *            hosts file resolution.
   * @return A string vector of all host names associated with the IPs tied to
   *         the specified interface
   * @throws UnknownHostException if the given interface is invalid",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,read,org.apache.hadoop.net.SocketInputStream:read(),112,127,"/**
 * Reads a single byte from the stream as an integer.
 * Returns the byte value or -1 if end of stream.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,listBeans,"org.apache.hadoop.jmx.JMXJsonServlet:listBeans(com.fasterxml.jackson.core.JsonGenerator,javax.management.ObjectName,java.lang.String,javax.servlet.http.HttpServletResponse)",235,328,"/**
 * Lists MBeans matching a query, serializing to JSON.
 * @param jg JsonGenerator for output, qry query, attribute, response
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,parseArguments,org.apache.hadoop.log.LogLevel$CLI:parseArguments(java.lang.String[]),141,170,"/**
 * Parses command-line arguments, handling -getlevel, -setlevel, -protocol.
 * Throws HadoopIllegalArgumentException on invalid arguments.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,"org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.util.Map)",117,132,"/**
 * Prints usage information to a PrintStream, including command help.
 * @param pStr PrintStream to write usage to
 * @param helpEntries Map of command names to UsageInfo objects
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,<init>,org.apache.hadoop.log.LogThrottlingHelper:<init>(long),145,147,"/**
 * Creates a LogThrottlingHelper with a minimum logging period.
 * @param minLogPeriodMs Minimum time (ms) between log entries.
 */
","* Create a log helper without any primary recorder.
   *
   * @see #LogThrottlingHelper(long, String)
   * @param minLogPeriodMs input minLogPeriodMs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogThrottlingHelper.java,record,org.apache.hadoop.log.LogThrottlingHelper:record(double[]),195,197,"/**
 * Records a log action with default recorder name and timestamp.
 * @param values Log action values to record.
 * @return LogAction object representing the recorded action.
 */
","* Record some set of values at the current time into this helper. Note that
   * this does <i>not</i> actually write information to any log. Instead, this
   * will return a LogAction indicating whether or not the caller should write
   * to its own log. The LogAction will additionally contain summary information
   * about the values specified since the last time the caller was expected to
   * write to its log.
   *
   * <p>Specifying multiple values will maintain separate summary statistics
   * about each value. For example:
   * <pre>{@code
   *   helper.record(1, 0);
   *   LogAction action = helper.record(3, 100);
   *   action.getStats(0); // == 2
   *   action.getStats(1); // == 50
   * }</pre>
   *
   * @param values The values about which to maintain summary information. Every
   *               time this method is called, the same number of values must
   *               be specified.
   * @return A LogAction indicating whether or not the caller should write to
   *         its log.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,getEvent,org.apache.hadoop.http.ProfileServlet:getEvent(javax.servlet.http.HttpServletRequest),366,373,"/**
 * Retrieves an event from the request, or defaults to CPU.
 * @param req HttpServletRequest containing the event parameter.
 * @return Event object, or Event.CPU if event is not found.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HtmlQuoting.java,main,org.apache.hadoop.http.HtmlQuoting:main(java.lang.String[]),215,224,"/**
 * Processes command-line arguments, quoting and unquoting HTML chars.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getParameter,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameter(java.lang.String),1804,1808,"/**
 * Retrieves a request parameter, quoting HTML characters.
 * @param name Parameter name to retrieve.
 * @return Parameter value or null if not found.
 */
",* Unquote the name and quote the value.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getParameterValues,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterValues(java.lang.String),1810,1822,"/**
 * Retrieves parameter values for a given name, quoting HTML chars.
 * @param name Parameter name to retrieve values for.
 * @return String array of parameter values, or null if none.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getParameterMap,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getParameterMap(),1824,1838,"/**
 * Returns a map of parameter names to quoted HTML character arrays.
 * @return Parameter map with HTML-quoted values.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getRequestURL,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getRequestURL(),1844,1848,"/**
 * Returns the request URL with HTML characters quoted.
 * @return A StringBuffer containing the quoted URL.
 */
","* Quote the url so that users specifying the HOST HTTP header
       * can't inject attacks.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getServerName,org.apache.hadoop.http.HttpServer2$QuotingInputFilter$RequestQuoter:getServerName(),1854,1857,"/**
 * Returns the server name, quoting HTML characters.
 */","* Quote the server name so that users specifying the HOST HTTP header
       * can't inject attacks.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addAsyncProfilerServlet,"org.apache.hadoop.http.HttpServer2:addAsyncProfilerServlet(org.eclipse.jetty.server.handler.ContextHandlerCollection,org.apache.hadoop.conf.Configuration)",797,816,"/**
 * Adds the Async Profiler servlet or a disabled endpoint.
 * @param contexts ServletContextHandlerCollection
 * @param conf Configuration object
 * @throws IOException if file system operations fail
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addNoCacheFilter,org.apache.hadoop.http.HttpServer2:addNoCacheFilter(org.eclipse.jetty.servlet.ServletContextHandler),888,891,"/**
 * Adds a filter to disable caching for the ServletContextHandler.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,makeConfigurationChangeMonitor,"org.apache.hadoop.http.HttpServer2$Builder:makeConfigurationChangeMonitor(long,org.eclipse.jetty.util.ssl.SslContextFactory$Server)",634,663,"/**
 * Creates a Timer to monitor keystore/truststore changes & reload.
 * @param reloadInterval Reload interval in milliseconds
 * @param sslContextFactory SSL context factory for certificate reload
 * @return Timer instance monitoring file changes
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileMonitoringTimerTask.java,<init>,"org.apache.hadoop.security.ssl.FileMonitoringTimerTask:<init>(java.nio.file.Path,java.util.function.Consumer,java.util.function.Consumer)",61,64,"/**
 * Creates a FileMonitoringTimerTask for a single file path.
 * @param filePath Path to monitor.
 * @param onFileChange Callback on file change.
 * @param onChangeFailure Callback on failure.
 */
","* See {@link #FileMonitoringTimerTask(List, Consumer, Consumer)}.
   *
   * @param filePath The file to monitor.
   * @param onFileChange What to do when the file changes.
   * @param onChangeFailure What to do when <code>onFileChange</code>
   *                        throws an exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,close,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:close(),877,892,"/**
 * Closes the current output stream and associated resources.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRatesWithAggregation,org.apache.hadoop.metrics2.lib.MetricsRegistry:newRatesWithAggregation(java.lang.String),331,337,"/**
 * Creates and registers a new MutableRatesWithAggregation.
 * @param name Metric name; used as key in the metrics map.
 * @return The newly created MutableRatesWithAggregation object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,add,"org.apache.hadoop.metrics2.lib.MetricsRegistry:add(java.lang.String,org.apache.hadoop.metrics2.lib.MutableMetric)",348,351,"/**
 * Adds a metric to the map.
 * @param name Metric name.
 * @param metric The metric to add.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsFactory.java,getAnnotatedMetricsFactory,org.apache.hadoop.metrics2.lib.DefaultMetricsFactory:getAnnotatedMetricsFactory(),33,35,"/**
 * Retrieves the annotated MutableMetricsFactory instance.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,flush,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:flush(),863,875,"/**
 * Flushes the underlying output stream.
 * Attempts to flush the stream; throws exception on failure.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,currentConfig,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:currentConfig(),348,358,"/**
 * Stringifies the current configuration to a string.
 * Returns the config as a string or throws exception on error.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getPluginLoader,org.apache.hadoop.metrics2.impl.MetricsConfig:getPluginLoader(),228,264,"/**
 * Gets the plugin class loader, creating one if necessary.
 * Returns the loader or default loader if none is found.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,toString,org.apache.hadoop.metrics2.impl.MetricsConfig:toString(org.apache.commons.configuration2.Configuration),287,298,"/**
 * Converts a Configuration object to a string representation.
 * @param c Configuration object to convert.
 * @return String representation of the configuration.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,consume,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:consume(org.apache.hadoop.metrics2.impl.MetricsBuffer),172,200,"/**
 * Processes metrics from a buffer, filtering based on configured filters.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,tag,"org.apache.hadoop.metrics2.MetricStringBuilder:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",81,84,"/**
 * Adds a tag to the metrics record.
 * @param info MetricsInfo to tag.
 * @param value Tag value.
 * @return MetricsRecordBuilder with the tag added.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,add,org.apache.hadoop.metrics2.MetricStringBuilder:add(org.apache.hadoop.metrics2.AbstractMetric),91,95,"/**
 * Adds a metric to the builder using its info and string representation.
 * @param metric The metric to add.
 * @return The builder instance for chaining.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricStringBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",102,105,"/**
 * Adds a counter value to the metrics record builder.
 * @param info MetricsInfo object
 * @param value The counter value to add
 * @return MetricsRecordBuilder for chaining
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addCounter,"org.apache.hadoop.metrics2.MetricStringBuilder:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",107,110,"/**
 * Adds a counter metric with the given info and value.
 * @param info MetricsInfo object
 * @param value The counter value to add
 * @return MetricsRecordBuilder for chaining
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",112,115,"/**
 * Adds a gauge metric.
 * @param info MetricsInfo object
 * @param value Gauge value
 * @return MetricsRecordBuilder instance
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",117,120,"/**
 * Adds a gauge metric with the given info and value.
 * @param info MetricsInfo object
 * @param value The gauge value
 * @return MetricsRecordBuilder for chaining
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",122,125,"/**
 * Adds a gauge metric with the given info and value.
 * @param info MetricsInfo object
 * @param value The gauge value
 * @return MetricsRecordBuilder for chaining
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/MetricStringBuilder.java,addGauge,"org.apache.hadoop.metrics2.MetricStringBuilder:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",127,130,"/**
 * Adds a gauge metric with the given info and value.
 * @param info MetricsInfo object
 * @param value The gauge value
 * @return MetricsRecordBuilder for chaining.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MetricsCache.java,update,org.apache.hadoop.metrics2.util.MetricsCache:update(org.apache.hadoop.metrics2.MetricsRecord),184,186,"/**
 * Updates a metrics record.
 * @param mr The metrics record to update.
 * @return The updated metrics record.
 */
","* Update the cache and return the current cache record
   * @param mr the update record
   * @return the updated cache record",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,flush,org.apache.hadoop.metrics2.sink.GraphiteSink:flush(),110,122,"/**
 * Flushes metrics to Graphite. Handles errors and closes connection.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,write,org.apache.hadoop.metrics2.sink.GraphiteSink$Graphite:write(java.lang.String),167,174,"/**
 * Writes a message to the output stream.
 * @param msg The message to write.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,init,org.apache.hadoop.metrics2.sink.GraphiteSink:init(org.apache.commons.configuration2.SubsetConfiguration),54,68,"/**
 * Initializes the Graphite connector using the provided configuration.
 * @param conf Configuration object containing Graphite server details.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/PrometheusMetricsSink.java,writeMetrics,org.apache.hadoop.metrics2.sink.PrometheusMetricsSink:writeMetrics(java.io.Writer),111,163,"/**
 * Writes Prometheus metrics to the given writer.
 * @param writer Writer to write the metrics to.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,consume,org.apache.hadoop.metrics2.impl.SinkQueue:consume(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer),65,75,"/**
 * Consumes data using the provided consumer.
 * @param consumer Consumer to process the data.
 * @throws InterruptedException if interrupted while waiting.
 */
","* Consume one element, will block if queue is empty
   * Only one consumer at a time is allowed
   * @param consumer  the consumer callback object",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/SinkQueue.java,consumeAll,org.apache.hadoop.metrics2.impl.SinkQueue:consumeAll(org.apache.hadoop.metrics2.impl.SinkQueue$Consumer),82,94,"/**
 * Consumes all elements in the queue using the provided consumer.
 * @param consumer The consumer to apply to each element.
 * @throws InterruptedException if interrupted while waiting.
 */
","* Consume all the elements, will block if queue is empty
   * @param consumer  the consumer callback object
   * @throws InterruptedException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,incrCacheHit,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheHit(),64,66,"/**
 * Increments the cache hit counter.
 */",* One cache hit event,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,incrCacheCleared,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheCleared(),71,73,"/**
 * Increments the cache cleared counter.
 */",* One cache cleared,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,incrCacheUpdated,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:incrCacheUpdated(),78,80,"/**
 * Increments the cache updated counter.
 */",* One cache updated,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthenticationFailures,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthenticationFailures(),215,217,"/**
 * Increments the authentication failure counter.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthenticationSuccesses,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthenticationSuccesses(),223,225,"/**
 * Increments the authentication successes counter.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthorizationSuccesses,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthorizationSuccesses(),231,233,"/**
 * Increments the authorization successes counter.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrAuthorizationFailures,org.apache.hadoop.ipc.metrics.RpcMetrics:incrAuthorizationFailures(),239,241,"/**
 * Increments the number of authorization failures.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrClientBackoff,org.apache.hadoop.ipc.metrics.RpcMetrics:incrClientBackoff(),343,345,"/**
 * Increments the client backoff counter.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrClientBackoffDisconnected,org.apache.hadoop.ipc.metrics.RpcMetrics:incrClientBackoffDisconnected(),350,352,"/**
 * Increments the backoff counter for disconnected RPC clients.
 */
",* Client was disconnected due to backoff,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrSlowRpc,org.apache.hadoop.ipc.metrics.RpcMetrics:incrSlowRpc(),366,368,"/**
 * Increments the counter for slow RPC calls.
 */",* Increments the Slow RPC counter.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrRequeueCalls,org.apache.hadoop.ipc.metrics.RpcMetrics:incrRequeueCalls(),373,375,"/**
 * Increments the number of requeue calls.
 */",* Increments the Requeue Calls counter.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,incrRpcCallSuccesses,org.apache.hadoop.ipc.metrics.RpcMetrics:incrRpcCallSuccesses(),380,382,"/**
 * Increments the RPC call successes counter.
 */
",* One RPC call success event.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,channelWrite,"org.apache.hadoop.ipc.Server:channelWrite(java.nio.channels.WritableByteChannel,java.nio.ByteBuffer)",3923,3932,"/**
 * Writes data from a buffer to a channel.
 * @param channel WritableByteChannel to write to
 * @param buffer ByteBuffer containing data to write
 * @return Number of bytes written
 */
","* This is a wrapper around {@link WritableByteChannel#write(ByteBuffer)}.
   * If the amount of data is large, it writes to channel in smaller chunks. 
   * This is to avoid jdk from creating many direct buffers as the size of 
   * buffer increases. This also minimizes extra copies in NIO layer
   * as a result of multiple write operations required to write a large 
   * buffer.  
   *
   * @see WritableByteChannel#write(ByteBuffer)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,channelRead,"org.apache.hadoop.ipc.Server:channelRead(java.nio.channels.ReadableByteChannel,java.nio.ByteBuffer)",3943,3952,"/**
 * Reads data from the channel into the buffer.
 * @param channel ReadableByteChannel to read from
 * @param buffer ByteBuffer to store the read data
 * @return Number of bytes read, or -1 if error.
 */
","* This is a wrapper around {@link ReadableByteChannel#read(ByteBuffer)}.
   * If the amount of data is large, it writes to channel in smaller chunks. 
   * This is to avoid jdk from creating many direct buffers as the size of 
   * ByteBuffer increases. There should not be any performance degredation.
   * 
   * @see ReadableByteChannel#read(ByteBuffer)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,getRecord,org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:getRecord(),152,157,"/**
 * Returns a MetricsRecordImpl if conditions are met, otherwise null.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,newAttrInfo,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:newAttrInfo(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",58,60,"/**
 * Creates an MBeanAttributeInfo with name and description from MetricsInfo.
 * @param info MetricsInfo object
 * @param type Attribute type
 * @return New MBeanAttributeInfo object
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,get,org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:get(),96,112,"/**
 * Retrieves MBeanInfo from MetricsRecordImpl objects.
 * Returns an MBeanInfo object containing attribute information.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,updateAttrCache,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateAttrCache(java.lang.Iterable),250,268,"/**
* Updates the attribute cache with metrics from the provided records.
* @param lastRecs Iterable of MetricsRecordImpl objects
* @return The total number of metrics processed.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,newObjectName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newObjectName(java.lang.String),128,137,"/**
 * Creates a new ObjectName.
 * @param name The base name for the ObjectName.
 * @return A unique ObjectName.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,newSourceName,"org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newSourceName(java.lang.String,boolean)",147,156,"/**
 * Gets a unique source name. Returns name if dupOK, otherwise finds a unique name.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,putMetrics,"org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,long)",96,107,"/**
 * Adds metrics to the buffer, enqueues if logicalTime is a multiple of period.
 * @param buffer MetricsBuffer to add
 * @param logicalTimeMs Logical time in milliseconds
 * @return True if added, false if dropped
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,putMetricsImmediate,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:putMetricsImmediate(org.apache.hadoop.metrics2.impl.MetricsBuffer),109,126,"/**
 * Puts metrics into the queue immediately, waits for acknowledgment.
 * @param buffer MetricsBuffer to be put into the queue.
 * @return True if successful, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,diskCheckFailed,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:diskCheckFailed(),146,149,"/**
 * Increments failure count and records the last failure time.
 */
",* Increase the failure count and update the last failure timestamp.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,fetchGroupSet,org.apache.hadoop.security.Groups$GroupCacheLoader:fetchGroupSet(java.lang.String),413,424,"/**
 * Fetches the set of groups for a user, logs performance metrics.
 * @param user The user whose group set is to be fetched.
 * @return A set of group names for the user.
 */
","* Queries impl for groups belonging to the user.
     * This could involve I/O and take awhile.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,shutdown,org.apache.hadoop.metrics2.source.JvmMetrics$Singleton:shutdown(),66,69,"/**
 * Shuts down the JVM metrics collection, unregistering the source.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,registerIfNeeded,org.apache.hadoop.metrics2.source.JvmMetrics:registerIfNeeded(),72,79,"/**
 * Registers the JvmMetrics source if it's not already registered.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,create,org.apache.hadoop.security.UserGroupInformation$UgiMetrics:create(),147,149,"/**
 * Creates and registers a new UgiMetrics instance.
 * Returns the registered UgiMetrics object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,shutdown,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:shutdown(),105,107,"/**
 * Unregisters the metrics source associated with this component.
 */
",* Shutdown the instrumentation process.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,shutdown,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:shutdown(),108,110,"/**
 * Unregisters the metrics source associated with this component.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,shutdown,org.apache.hadoop.ipc.metrics.RpcMetrics:shutdown(),247,249,"/**
 * Unregisters the metrics source.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,registerMetrics2Source,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:registerMetrics2Source(java.lang.String),891,894,"/**
 * Registers metrics for the DecayRpcScheduler with the given namespace.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,unregister,org.apache.hadoop.metrics2.util.MBeans:unregister(javax.management.ObjectName),136,149,"/**
 * Unregisters an MBean from the platform MBean server.
 * @param mbeanName The ObjectName of the MBean to unregister.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2Metrics.java,remove,org.apache.hadoop.http.HttpServer2Metrics:remove(),161,163,"/**
 * Removes the metrics source for the HttpServer2 with the given port.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,snapshot,"org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",228,230,"/**
 * Records a snapshot of metrics into the provided builder.
 * @param rb MetricsRecordBuilder to populate with snapshot data
 * @param all if true, includes all metrics; otherwise, only recent ones
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableRates:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",80,83,"/**
 * Delegates snapshotting to the underlying registry.
 * @param rb MetricsRecordBuilder to populate.
 * @param all Whether to include all metrics.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,setQuantiles,"org.apache.hadoop.metrics2.lib.MutableQuantiles:setQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.text.DecimalFormat)",117,125,"/**
 * Sets quantile information based on provided names, description, and format.
 */","* Sets quantileInfo.
   *
   * @param ucName capitalized name of the metric
   * @param uvName capitalized type of the values
   * @param desc uncapitalized long-form textual description of the metric
   * @param lvName uncapitalized type of the values
   * @param pDecimalFormat Number formatter for percentile value",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableInverseQuantiles.java,setQuantiles,"org.apache.hadoop.metrics2.lib.MutableInverseQuantiles:setQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,java.text.DecimalFormat)",75,83,"/**
 * Sets quantile information based on provided names, description, and format.
 */","* Sets quantileInfo.
   *
   * @param ucName capitalized name of the metric
   * @param uvName capitalized type of the values
   * @param desc uncapitalized long-form textual description of the metric
   * @param lvName uncapitalized type of the values
   * @param df Number formatter for inverse percentile value",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,<init>,org.apache.hadoop.metrics2.lib.MetricsRegistry:<init>(java.lang.String),49,51,"/**
 * Creates a MetricsRegistry with the given name.
 * @param name The name of the metrics registry.
 */
","* Construct the registry with a record name
   * @param name  of the record of the metrics",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableRollingAverages:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",169,197,"/**
 * Records metrics averages to the builder, optionally all.
 * @param builder MetricsRecordBuilder to add gauge values
 * @param all If true, records all averages, otherwise only changed ones
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/Interns.java,tag,"org.apache.hadoop.metrics2.lib.Interns:tag(java.lang.String,java.lang.String,java.lang.String)",162,164,"/**
 * Creates a new MetricsTag with the given name, description, and value.
 */","* Get a metrics tag.
   * @param name  of the tag
   * @param description of the tag
   * @param value of the tag
   * @return an interned metrics tag",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(java.lang.Class,org.apache.hadoop.metrics2.annotation.Metrics)",141,146,"/**
 * Creates a MetricsInfo object from a class and its Metrics annotation.
 * @param cls The class containing the Metrics annotation.
 * @param annotation The Metrics annotation instance.
 * @return A MetricsInfo object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.String)",162,174,"/**
 * Creates a MetricsInfo object based on annotation values.
 * @param annotation Metric annotation
 * @param defaultName Default metric name if annotation is incomplete
 * @return MetricsInfo object
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,<init>,"org.apache.hadoop.metrics2.lib.MutableStat:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)",64,85,"/**
 * Creates a MutableStat object with given name, description, etc.
 * @param name Stat name
 * @param description Stat description
 */
","* Construct a sample statistics metric
   * @param name        of the metric
   * @param description of the metric
   * @param sampleName  of the metric (e.g. ""Ops"")
   * @param valueName   of the metric (e.g. ""Time"", ""Latency"")
   * @param extended    create extended stats (stdev, min/max etc.) by default.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,metricInfo,org.apache.hadoop.metrics2.lib.MethodMetric:metricInfo(java.lang.reflect.Method),145,147,"/**
 * Returns a MetricsInfo object for the given method.
 * @param method The method to get metric info for.
 * @return MetricsInfo object representing the method's metrics.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getGcInfo,org.apache.hadoop.metrics2.source.JvmMetrics:getGcInfo(java.lang.String),209,222,"/**
 * Retrieves GC info for a given name, caching the result.
 * @param gcName Name of the GC for which to retrieve info.
 * @return MetricsInfo[] containing GC count and time metrics.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addUniqueIdentityCount,org.apache.hadoop.ipc.DecayRpcScheduler:addUniqueIdentityCount(org.apache.hadoop.metrics2.MetricsRecordBuilder),1030,1033,"/**
 * Adds the count of unique callers to the metrics record.
 * @param rb MetricsRecordBuilder to add the counter to.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addDecayedCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addDecayedCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1036,1039,"/**
 * Adds decayed call volume counter to the metrics record builder.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addRawCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addRawCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1041,1044,"/**
 * Adds the total raw incoming call volume to the metrics record.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addServiceUserDecayedCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addServiceUserDecayedCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1047,1051,"/**
 * Adds the decayed service user call volume to the metrics record.
 * @param rb MetricsRecordBuilder to add the counter to.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addServiceUserRawCallVolume,org.apache.hadoop.ipc.DecayRpcScheduler:addServiceUserRawCallVolume(org.apache.hadoop.metrics2.MetricsRecordBuilder),1054,1058,"/**
 * Adds service user raw call volume counter to the metrics record.
 * @param rb MetricsRecordBuilder to add the counter to.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addCallVolumePerPriority,org.apache.hadoop.ipc.DecayRpcScheduler:addCallVolumePerPriority(org.apache.hadoop.metrics2.MetricsRecordBuilder),1061,1067,"/**
 * Adds call volume gauges for each priority level to the builder.
 * @param rb MetricsRecordBuilder to add the gauges to.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addAvgResponseTimePerPriority,org.apache.hadoop.ipc.DecayRpcScheduler:addAvgResponseTimePerPriority(org.apache.hadoop.metrics2.MetricsRecordBuilder),1070,1076,"/**
 * Adds average response time per priority to the metrics record.
 * @param rb MetricsRecordBuilder to add the gauge values to.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configureSystem,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSystem(),488,490,"/**
* Adds hostname tag to injected tags.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,tag,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",73,79,"/**
 * Adds a tag to the metrics record if acceptable.
 * @param info MetricsInfo object for the tag.
 * @param value Tag value to add.
 * @return The MetricsRecordBuilderImpl instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String,boolean)",415,420,"/**
 * Adds a tag with a value to the registry.
 * @param info MetricsInfo object, @param value tag value
 * @param override Whether to override existing tags.
 */
","* Add a tag to the metrics
   * @param info  metadata of the tag
   * @param value of the tag
   * @param override existing tag if true
   * @return the registry (for keep adding tags etc.)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,incr,org.apache.hadoop.metrics2.lib.MutableGaugeFloat:incr(),42,45,"/**
 * Increments the counter by 1.0f, calls the overloaded incr method.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableGaugeFloat.java,decr,org.apache.hadoop.metrics2.lib.MutableGaugeFloat:decr(),47,50,"/**
 * Decrements the counter by 1.0, reusing the incr method.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,add,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat:add(double),178,180,"/**
 * Adds a value to the statistics accumulator.
 * @param x The double value to add.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,add,org.apache.hadoop.metrics2.lib.MutableStat:add(long),132,136,"/**
* Adds a value to the interval statistics and min/max trackers.
* @param value The value to add.
*/
","* Add a snapshot to the metric.
   * @param value of the metric",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,reset,"org.apache.hadoop.metrics2.util.SampleStat:reset(long,double,double,org.apache.hadoop.metrics2.util.SampleStat$MinMax)",48,53,"/**
 * Resets the internal state with provided values.
 * @param numSamples Number of samples.
 * @param mean1 The mean value.
 * @param s1 Standard deviation.
 * @param minmax1 MinMax object to reset with.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,snapshotInto,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation$ThreadSafeSampleStat:snapshotInto(org.apache.hadoop.metrics2.lib.MutableRate),182,187,"/**
 * Updates a mutable rate with the current statistics, then resets.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,newImpl,org.apache.hadoop.metrics2.lib.MethodMetric:newImpl(org.apache.hadoop.metrics2.annotation.Metric$Type),55,70,"/**
 * Creates a new mutable metric based on the given type.
 * @param metricType The type of metric to create (COUNTER, GAUGE, etc.)
 * @return A new MutableMetric object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,toString,org.apache.hadoop.metrics2.util.SampleStat:toString(),145,156,"/**
 * Returns a string representation of the object's statistics.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getProcessingStdDev,org.apache.hadoop.ipc.metrics.RpcMetrics:getProcessingStdDev(),412,414,"/**
 * Returns the standard deviation of RPC processing times.
 */
","* Return Standard Deviation of the Processing Time.
   * @return  double",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getDeferredRpcProcessingStdDev,org.apache.hadoop.ipc.metrics.RpcMetrics:getDeferredRpcProcessingStdDev(),445,447,"/**
 * Returns the standard deviation of deferred RPC processing times.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,insert,org.apache.hadoop.metrics2.util.SampleQuantiles:insert(long),113,123,"/**
 * Inserts a value into the buffer. Increments count.
 * May trigger batch insertion and compression.
 */
","* Add a new value from the stream.
   * 
   * @param v v.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,snapshot,org.apache.hadoop.metrics2.util.SampleQuantiles:snapshot(),236,250,"/**
 * Creates a snapshot of quantile values.
 * Returns a map of quantile to count, or null if empty.
 */
","* Get a snapshot of the current values of all the tracked quantiles.
   * 
   * @return snapshot of the tracked quantiles. If no items are added
   * to the estimator, returns null.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,getTopTokenRealOwners,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTopTokenRealOwners(int),880,898,"/**
 * Returns the top N token owners sorted by token count.
 * @param n The number of top owners to retrieve.
 * @return List of NameValuePair objects representing top owners.
 */","* Return top token real owners list as well as the tokens count.
   *
   * @param n top number of users
   * @return map of owners to counts",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getTopCallers,org.apache.hadoop.ipc.DecayRpcScheduler:getTopCallers(int),1099,1112,"/**
 * Finds the top N callers based on cost.
 * @param n The number of top callers to retrieve.
 * @return TopN object containing the top N callers.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java,cacheGroupsAdd,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsAdd(java.util.List),90,103,"/**
 * Adds netgroups to the cache, caching only those starting with '@'.
 * @param groups List of group names to add to the cache.
 */
","* Add a group to cache, only netgroups are cached
   *
   * @param groups list of group names to add to cache",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getTokens,org.apache.hadoop.security.UserGroupInformation:getTokens(),1724,1729,"/**
 * Returns an unmodifiable collection of all tokens.
 */
","* Obtain the collection of tokens associated with this user.
   * 
   * @return an unmodifiable collection of tokens associated with user",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/User.java,<init>,org.apache.hadoop.security.User:<init>(java.lang.String),42,44,"/**
 * Constructs a User object with a name, other fields are null.
 * @param name The name of the user.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getGroups,org.apache.hadoop.security.Groups:getGroups(java.lang.String),213,217,"/**
 * Retrieves a list of groups for the given user.
 * @param user The username to fetch groups for.
 * @return Unmodifiable list of group names.
 */
","* Get the group memberships of a given user.
   * If the user's group is not cached, this method may block.
   * Note this method can be expensive as it involves Set {@literal ->} List
   * conversion. For user with large group membership
   * (i.e., {@literal >} 1000 groups), we recommend using getGroupSet
   * to avoid the conversion and fast membership look up via contains().
   * @param user User's name
   * @return the group memberships of the user as list
   * @throws IOException if user does not exist
   * @deprecated Use {@link #getGroupsSet(String user)} instead.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getGroupsSet,org.apache.hadoop.security.Groups:getGroupsSet(java.lang.String),231,233,"/**
 * Returns a set of group names for the given user.
 * @param user User identifier.
 * @return Unmodifiable set of group names.
 */
","* Get the group memberships of a given user.
   * If the user's group is not cached, this method may block.
   * This provide better performance when user has large group membership via
   * <br>
   * 1) avoid {@literal set->list->set} conversion for the caller
   * UGI/PermissionCheck <br>
   * 2) fast lookup using contains() via Set instead of List
   * @param user User's name
   * @return the group memberships of the user as set
   * @throws IOException if user does not exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java,getGroups,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String),67,73,"/**
 * Retrieves a list of groups for a given user, including netgroups.
 * @param user The username to retrieve groups for.
 * @return List of group names.
 */
","* Gets unix groups and netgroups for the user.
   *
   * It gets all unix groups as returned by id -Gn but it
   * only returns netgroups that are used in ACLs (there is
   * no way to get all netgroups for a given user, see
   * documentation for getent netgroup)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,println,org.apache.hadoop.security.KDiag:println(),868,870,"/**
 * Prints an empty line to the console.
 */",* Print a new line,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printSysprop,org.apache.hadoop.security.KDiag:printSysprop(java.lang.String),897,900,"/**
 * Prints a system property to the console, using ""UNSET"" if absent.
 * @param property The name of the system property to print.
 */
","* Print a system property, or {@link #UNSET} if unset.
   * @param property property to print",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printEnv,org.apache.hadoop.security.KDiag:printEnv(java.lang.String),916,919,"/**
 * Prints the value of an environment variable.
 * @param variable Name of the environment variable to print.
 */
","* Print an environment variable's name and value; printing
   * {@link #UNSET} if it is not set.
   * @param variable environment variable",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dump,org.apache.hadoop.security.KDiag:dump(java.io.File),926,932,"/**
 * Prints the lines of a file to the console.
 * @param file the file to be printed
 * @throws IOException if an I/O error occurs
 */
","* Dump any file to standard out.
   * @param file file to dump
   * @throws IOException IO problems",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,error,"org.apache.hadoop.security.KDiag:error(java.lang.String,java.lang.String,java.lang.Object[])",1011,1013,"/**
 * Logs an error message with a category and formatted message.
 * @param category Error category.
 * @param message Message format string.
 * @param args Arguments for message formatting.
 */
","* Print a message as an error
   * @param category error category
   * @param message format string
   * @param args list of arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,warn,"org.apache.hadoop.security.KDiag:warn(java.lang.String,java.lang.String,java.lang.Object[])",1020,1022,"/**
 * Logs a warning message to the console with a given category.
 * @param category Warning category.
 * @param message Message format string.
 * @param args Arguments for message formatting.
 */
","* Print a message as an warning
   * @param category error category
   * @param message format string
   * @param args list of arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,loadFullUserMap,org.apache.hadoop.security.ShellBasedIdMapping:loadFullUserMap(),359,370,"/**
 * Loads the full user map based on OS.
 * Populates uidNameMap with user IDs and names.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,loadFullGroupMap,org.apache.hadoop.security.ShellBasedIdMapping:loadFullGroupMap(),372,384,"/**
 * Loads group ID to name mapping.
 * Populates gidNameMap based on OS. Updates lastUpdateTime.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:setAuthenticationMethod(org.apache.hadoop.security.SaslRpcServer$AuthMethod),1844,1846,"/**
* Sets the authentication method for the user.
* @param authMethod The authentication method to set.
*/
","* Sets the authentication method in the subject
   * 
   * @param authMethod authMethod.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,write,org.apache.hadoop.security.SaslOutputStream:write(int),123,131,"/**
 * Writes a byte to the output stream, wrapping if enabled.
 * @param b The byte to write.
 * @throws IOException if an I/O error occurs.
 */
","* Writes the specified byte to this output stream.
   * 
   * @param b
   *          the <code>byte</code>.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslOutputStream.java,write,org.apache.hadoop.security.SaslOutputStream:write(byte[]),148,151,"/**
 * Writes a byte array to the underlying output stream.
 * @param b the byte array to be written
 * @throws IOException if an I/O error occurs
 */
","* Writes <code>b.length</code> bytes from the specified byte array to this
   * output stream.
   * <p>
   * The <code>write</code> method of <code>SASLOutputStream</code> calls the
   * <code>write</code> method of three arguments with the three arguments
   * <code>b</code>, <code>0</code>, and <code>b.length</code>.
   * 
   * @param b
   *          the data.
   * @exception NullPointerException
   *              if <code>b</code> is null.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,createKeyManagers,org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createKeyManagers(),1078,1088,"/**
 * Creates KeyManagers from keystore.
 * @return KeyManager array or null if keystore location is empty.
 * @throws IOException, GeneralSecurityException
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,createTrustManagers,org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:createTrustManagers(),1090,1100,"/**
 * Creates TrustManagers from a trust store.
 * @return TrustManager[] or null if trustStoreLocation is empty.
 * @throws IOException, GeneralSecurityException
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,doFilter,"org.apache.hadoop.security.http.RestCsrfPreventionFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",205,212,"/**
 * Filters an HTTP request and response using a handler.
 * @param request HTTP request
 * @param response HTTP response
 * @param chain Filter chain to pass to next filter
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,doFilter,"org.apache.hadoop.security.http.CrossOriginFilter:doFilter(javax.servlet.ServletRequest,javax.servlet.ServletResponse,javax.servlet.FilterChain)",92,98,"/**
 * Filters the request, executes cross-filter logic, and passes to chain.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/CrossOriginFilter.java,init,org.apache.hadoop.security.http.CrossOriginFilter:init(javax.servlet.FilterConfig),84,90,"/**
 * Initializes the filter with configuration from FilterConfig.
 * @param filterConfig Filter configuration object.
 * @throws ServletException if initialization fails.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,getKey,org.apache.hadoop.security.token.delegation.DelegationKey:getKey(),75,82,"/**
 * Retrieves the SecretKey. Returns null if keyBytes is empty.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,checkToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:checkToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),528,546,"/**
 * Checks token validity.
 * @param identifier TokenIdent object to check
 * @return DelegationTokenInformation if valid, throws InvalidToken otherwise
 */
","* Find the DelegationTokenInformation for the given token id, and verify that
   * if the token is expired. Note that this method should be called with 
   * acquiring the secret manager's monitor.
   *
   * @param identifier identifier.
   * @throws InvalidToken invalid token exception.
   * @return DelegationTokenInformation.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:<init>(),703,705,"/**
 * Default constructor for DelegationTokenInformation.
 * Initializes with a token count of 0 and null token.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,logExpireTokens,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:logExpireTokens(java.util.Collection),786,793,"/**
 * Logs and removes expired tokens from the collection.
 * @param expiredTokens Collection of expired TokenIdent objects.
 * @throws IOException if an I/O error occurs during token removal.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,setExternalDelegationTokenSecretManager,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:setExternalDelegationTokenSecretManager(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager),143,146,"/**
 * Sets the external delegation token secret manager.
 * @param secretManager The secret manager to set.
 */
","* Sets an external <code>DelegationTokenSecretManager</code> instance to
   * manage creation and verification of Delegation Tokens.
   * <p>
   * This is useful for use cases where secrets must be shared across multiple
   * services.
   *
   * @param secretManager a <code>DelegationTokenSecretManager</code> instance",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,destroy,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:destroy(),189,193,"/**
 * Releases resources held by the token manager and auth handler.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,updateCurrentKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:updateCurrentKey(),440,456,"/**
 * Updates the current master key for delegation tokens.
 * Generates a new key, logs the update, and stores it.
 */
","* Update the current master key 
   * This is called once by startThreads before tokenRemoverThread is created, 
   * and only by tokenRemoverThread afterwards.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,<init>,org.apache.hadoop.security.token.delegation.DelegationKey:<init>(),47,49,"/**
 * Default constructor for DelegationKey, initializes with default values.
 */
",Default constructore required for Writable,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator,org.apache.hadoop.security.authentication.client.ConnectionConfigurator)",183,188,"/**
 * Constructs a DelegationTokenAuthenticatedURL.
 * @param authenticator DelegationTokenAuthenticator
 * @param connConfigurator ConnectionConfigurator
 */
","* Creates an <code>DelegationTokenAuthenticatedURL</code>.
   *
   * @param authenticator the {@link DelegationTokenAuthenticator} instance to
   * use, if <code>null</code> the default one will be used.
   * @param connConfigurator a connection configurator.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,renew,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:renew(),126,151,"/**
 * Renews the token, handling failures by obtaining a new delegation token.
 * @return True if the filesystem was valid before renewal.
 */
","* Renew or replace the delegation token for this file system.
     * It can only be called when the action is not in the queue.
     * @return
     * @throws IOException",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,cancel,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:cancel(),153,158,"/**
 * Cancels the token, potentially stopping a file system operation.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,read,org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(),568,573,"/**
 * Reads a single byte from the input stream.
 * Returns the byte as an int, or -1 if EOF.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,read,org.apache.hadoop.security.SaslRpcClient$WrappedInputStream:read(byte[]),575,578,"/**
 * Reads bytes from input stream into provided byte array.
 * @param b byte array to read into
 * @return Number of bytes read
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setSaslClient,org.apache.hadoop.ipc.Client$IpcStreams:setSaslClient(org.apache.hadoop.security.SaslRpcClient),1905,1910,"/**
* Sets the Sasl client and associated input/output streams.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,noPasswordWarning,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:noPasswordWarning(),348,352,"/**
 * Returns a warning message if the password is not configured.
 * Uses utility method for message generation.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,noPasswordWarning,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:noPasswordWarning(),315,319,"/**
 * Returns a warning message if the keystore password is not set.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,noPasswordError,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:noPasswordError(),354,358,"/**
 * Returns the error message for missing password credentials.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,noPasswordError,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:noPasswordError(),321,325,"/**
 * Returns the error message for a missing keystore password.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,org.apache.hadoop.security.SaslInputStream:read(),193,207,"/**
 * Reads a single byte from the input stream.
 * Returns -1 if end of stream is reached.
 */","* Reads the next byte of data from this input stream. The value byte is
   * returned as an <code>int</code> in the range <code>0</code> to
   * <code>255</code>. If no byte is available because the end of the stream has
   * been reached, the value <code>-1</code> is returned. This method blocks
   * until input data is available, the end of the stream is detected, or an
   * exception is thrown.
   * <p>
   * 
   * @return the next byte of data, or <code>-1</code> if the end of the stream
   *         is reached.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,"org.apache.hadoop.security.SaslInputStream:read(byte[],int,int)",248,275,"/**
 * Reads bytes from the input stream into the provided byte array.
 * @param b buffer to read into, off offset, len number of bytes
 * @return number of bytes read, or -1 if end of stream
 */
","* Reads up to <code>len</code> bytes of data from this input stream into an
   * array of bytes. This method blocks until some input is available. If the
   * first argument is <code>null,</code> up to <code>len</code> bytes are read
   * and discarded.
   * 
   * @param b
   *          the buffer into which the data is read.
   * @param off
   *          the start offset of the data.
   * @param len
   *          the maximum number of bytes read.
   * @return the total number of bytes read into the buffer, or <code>-1</code>
   *         if there is no more data because the end of the stream has been
   *         reached.
   * @exception IOException
   *              if an I/O error occurs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ImpersonationProvider.java,authorize,"org.apache.hadoop.security.authorize.ImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",51,58,"/**
 * Authorizes a user based on their credentials and remote address.
 * @param user UserGroupInformation object
 * @param remoteAddress Remote address string
 * @throws AuthorizationException if authorization fails
 */
","* Authorize the superuser which is doing doAs.
   * {@link #authorize(UserGroupInformation, InetAddress)} should
   *             be preferred to avoid possibly re-resolving the ip address.
   * @param user ugi of the effective or proxy user which contains a real user.
   * @param remoteAddress the ip address of client.
   * @throws AuthorizationException Authorization Exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getKeytab,org.apache.hadoop.security.UserGroupInformation:getKeytab(),814,819,"/**
 * Retrieves the keytab path from the Hadoop login configuration.
 * @return Keytab path or null if login context is not available.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isHadoopLogin,org.apache.hadoop.security.UserGroupInformation:isHadoopLogin(),825,828,"/**
 * Checks if Hadoop login context is active.
 * @return True if Hadoop login context is managing the UGI.
 */
","* Is the ugi managed by the UGI or an external subject?
   * @return true if managed by UGI.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createProxyUser,"org.apache.hadoop.security.UserGroupInformation:createProxyUser(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",1522,1537,"/**
 * Creates a proxy UserGroupInformation representing a user.
 * @param user The proxy user's name.
 * @param realUser The UserGroupInformation of the real user.
 * @return A UserGroupInformation object representing the proxy.
 */
","* Create a proxy user using username of the effective user and the ugi of the
   * real user.
   * @param user user.
   * @param realUser realUser.
   * @return proxyUser ugi",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getName,org.apache.hadoop.security.UserGroupInformation$RealUser:getName(),472,475,"/**
 * Returns the user's name from the underlying real user.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,shouldBackOff,org.apache.hadoop.ipc.DecayRpcScheduler:shouldBackOff(org.apache.hadoop.ipc.Schedulable),720,745,"/**
 * Determines if backoff is needed based on response times.
 * @param obj The schedulable object to check.
 * @return True if backoff is needed, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealUserOrSelf,org.apache.hadoop.security.UserGroupInformation:getRealUserOrSelf(org.apache.hadoop.security.UserGroupInformation),1558,1564,"/**
 * Returns the real user or the original user if real user is null.
 * @param user The UserGroupInformation to check.
 * @return The real user or the original user.
 */
","* If this is a proxy user, get the real user. Otherwise, return
   * this user.
   * @param user the user to check
   * @return the real user or self",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,toString,org.apache.hadoop.security.UserGroupInformation:toString(),1819,1827,"/**
 * Returns a string representation of the object.
 */
",* Return the username.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:getRealAuthenticationMethod(),1863,1869,"/**
 * Gets the real authentication method of the user.
 * Returns the authentication method from UserGroupInformation.
 */
","* Get the authentication method from the real user's subject.  If there
   * is no real user, return the given user's authentication method.
   * 
   * @return AuthenticationMethod in the subject, null if not present.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getRealAuthenticationMethod,org.apache.hadoop.security.UserGroupInformation:getRealAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation),1878,1885,"/**
 * Gets the real authentication method for a UserGroupInformation.
 * Returns the underlying auth method if using proxy auth.
 */
","* Returns the authentication method of a ugi. If the authentication method is
   * PROXY, returns the authentication method of the real user.
   * 
   * @param ugi ugi.
   * @return AuthenticationMethod",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,makeIpcConnectionContext,"org.apache.hadoop.util.ProtoUtil:makeIpcConnectionContext(java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",91,122,"/**
 * Creates an IpcConnectionContextProto based on provided info.
 * @param protocol Protocol string, may be null.
 * @param ugi UserGroupInformation object, may be null.
 * @param authMethod Authentication method used.
 * @return IpcConnectionContextProto object.
 */
","* This method creates the connection context  using exactly the same logic
   * as the old connection context as was done for writable where
   * the effective and real users are set based on the auth method.
   *
   * @param protocol protocol.
   * @param ugi ugi.
   * @param authMethod authMethod.
   * @return IpcConnectionContextProto.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,close,org.apache.hadoop.ipc.Server$ConnectionManager:close(org.apache.hadoop.ipc.Server$Connection),4110,4126,"/**
 * Closes a connection, removes it, and decrements user connections.
 * @param connection The connection to close and remove.
 * @return True if the connection existed and was removed.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/UserIdentityProvider.java,makeIdentity,org.apache.hadoop.ipc.UserIdentityProvider:makeIdentity(org.apache.hadoop.ipc.Schedulable),28,35,"/**
 * Gets the short username from a schedulable object.
 * @param obj Schedulable object containing user info.
 * @return Short username or null if user info is absent.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,verify,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:verify(java.lang.String,javax.net.ssl.SSLSession)",262,273,"/**
 * Verifies the SSL certificate of a session against a host.
 * @param host The hostname to verify against.
 * @param session The SSL session to verify.
 * @return True if verification succeeds, false otherwise.
 */
","* The javax.net.ssl.HostnameVerifier contract.
         *
         * @param host    'hostname' we used to create our socket
         * @param session SSLSession with the remote server
         * @return true if the host matched the one in the certificate.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,java.security.cert.X509Certificate)",280,284,"/**
 * Checks the certificate against the given host.
 * @param host The hostname to check against.
 * @param cert The X.509 certificate to validate.
 * @throws SSLException if validation fails.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String[],javax.net.ssl.SSLSocket)",292,347,"/**
 * Checks the SSL certificate against the provided hostnames.
 * @param host Hostnames to verify the certificate against.
 * @param ssl The SSLSocket to check.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,loadResource,org.apache.hadoop.util.FindClass:loadResource(java.lang.String),172,180,"/**
 * Loads a resource by name.
 * @param name Resource name.
 * @return SUCCESS or E_NOT_FOUND if resource is missing.
 */
","* Load a resource
   * @param name resource name
   * @return the status code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,<init>,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:<init>(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode),132,151,"/**
 * Initializes the DelegatingSSLSocketFactory with preferred channel mode.
 * @param preferredChannelMode Preferred SSL channel mode.
 * @throws IOException if initialization fails.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,<init>,org.apache.hadoop.fs.shell.Command:<init>(org.apache.hadoop.conf.Configuration),86,88,"/**
 * Constructs a Command object with the given configuration.
 */","* Constructor.
   *
   * @param conf configuration.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,<init>,org.apache.hadoop.fs.shell.CommandFactory:<init>(org.apache.hadoop.conf.Configuration),53,55,"/**
 * Constructs a CommandFactory with the given configuration.
 * @param conf Configuration object for factory setup.
 */
","* Factory constructor for commands
   * @param conf the hadoop configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,org.apache.hadoop.fs.FileSystem:<init>(),777,779,"/**
 * Default constructor. Calls superclass constructor with null.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,<init>,org.apache.hadoop.fs.FsShell:<init>(org.apache.hadoop.conf.Configuration),75,77,"/**
 * Constructs a new FsShell with the given configuration.
 * @param conf Hadoop configuration object.
 */
","* Construct a FsShell with the given configuration.  Commands can be
   * executed via {@link #run(String[])}
   * @param conf the hadoop configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,<init>,org.apache.hadoop.io.ObjectWritable$NullInstance:<init>(),109,109,"/**
 * Constructs a NullInstance with a null superclass constructor.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,<init>,"org.apache.hadoop.io.ObjectWritable$NullInstance:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)",110,113,"/**
 * Constructs a NullInstance with the given class and configuration.
 * @param declaredClass The class this instance represents.
 * @param conf The configuration to use.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag:<init>(org.apache.hadoop.conf.Configuration,java.io.PrintWriter,java.io.File,java.lang.String,long,boolean)",171,184,"/**
 * Constructs a KDiag object with provided configuration and details.
 * @param conf Configuration object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,<init>,org.apache.hadoop.util.FindClass:<init>(org.apache.hadoop.conf.Configuration),131,133,"/**
 * Constructs a FindClass object with the given configuration.
 * @param conf Configuration object for class finding.
 */
","* Create a class with a specified configuration
   * @param conf configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,<init>,"org.apache.hadoop.tools.GetGroupsBase:<init>(org.apache.hadoop.conf.Configuration,java.io.PrintStream)",53,56,"/**
 * Constructs a GetGroupsBase with a configuration and print stream.
 * @param conf Configuration object.
 * @param out PrintStream for output.
 */
","* Used exclusively for testing.
   * 
   * @param conf The configuration to use.
   * @param out The PrintStream to write to, instead of System.out",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configured.java,<init>,org.apache.hadoop.conf.Configured:<init>(),32,34,"/**
 * Default constructor. Calls the parameterized constructor with null.
 */",Construct a Configured.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,<init>,org.apache.hadoop.ha.HAAdmin:<init>(org.apache.hadoop.conf.Configuration),103,105,"/**
 * Constructs a HAAdmin object with the given configuration.
 * @param conf The configuration object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByNameWithSearch,org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByNameWithSearch(java.lang.String),706,718,"/**
 * Resolves a host name, trying exact and search domain matches.
 * @param host The hostname to resolve.
 * @return Resolved InetAddress or null if not found.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getAppConfigurationEntry,org.apache.hadoop.security.UserGroupInformation$HadoopConfiguration:getAppConfigurationEntry(java.lang.String),2211,2230,"/**
 * Retrieves AppConfigurationEntry array for a given app name.
 * @param appName name of the application
 * @return Array of AppConfigurationEntry objects
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,parseStaticMap,org.apache.hadoop.security.ShellBasedIdMapping:parseStaticMap(java.io.File),588,630,"/**
 * Parses a static mapping file.
 * @param staticMapFile File containing uid/gid mappings.
 * @return StaticMapping object with parsed uid/gid mappings.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,getAclString,org.apache.hadoop.security.authorize.AccessControlList:getAclString(),301,312,"/**
 * Generates an ACL string based on allowed users/groups.
 * Returns ""*"" if all are allowed, otherwise combines users/groups.
 */
","* Returns the access control list as a String that can be used for building a
   * new instance by sending it to the constructor of {@link AccessControlList}.
   * @return acl string.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,createCredentialEntry,"org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:createCredentialEntry(java.lang.String,char[])",228,244,"/**
 * Creates a credential entry with the given alias and credential.
 * @param alias Credential alias.
 * @param credential Credential characters.
 * @throws IOException If credential already exists or error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,execute,org.apache.hadoop.security.alias.CredentialShell$CreateCommand:execute(),438,465,"/**
 * Creates a credential entry using provided or prompted credentials.
 * @throws IOException, NoSuchAlgorithmException if an error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getTGT,org.apache.hadoop.security.UserGroupInformation:getTGT(),852,861,"/**
 * Retrieves the original TGT from the subject's private credentials.
 * Returns null if no TGT is found.
 */
","* Get the Kerberos TGT
   * @return the user's TGT or null if none was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setSslConfiguration,"org.apache.hadoop.security.SecurityUtil:setSslConfiguration(org.apache.zookeeper.client.ZKClientConfig,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",819,823,"/**
 * Sets the SSL configuration for the ZKClientConfig.
 * @param zkClientConfig ZKClient configuration object.
 * @param truststoreKeystore Truststore/keystore details.
 * @throws ConfigurationException if configuration fails.
 */
","* Configure ZooKeeper Client with SSL/TLS connection.
   * @param zkClientConfig ZooKeeper Client configuration
   * @param truststoreKeystore truststore keystore, that we use to set the SSL configurations
   * @throws ConfigurationException if the SSL configs are empty",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,unprotectedRelogin,"org.apache.hadoop.security.UserGroupInformation:unprotectedRelogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean)",1347,1377,"/**
 * Performs a relogin, optionally ignoring the last login time.
 * @param login HadoopLoginContext instance
 * @param ignoreLastLoginTime flag to ignore last login time
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/WhitelistBasedResolver.java,getServerProperties,org.apache.hadoop.security.WhitelistBasedResolver:getServerProperties(java.lang.String),124,129,"/**
 * Gets server properties by client address.
 * @param clientAddress Client's IP address or hostname.
 * @return Map of server properties.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,handle,org.apache.hadoop.security.SaslRpcServer$SaslDigestCallbackHandler:handle(javax.security.auth.callback.Callback[]),294,348,"/**
 * Handles authentication callbacks, setting passwords & authorization.
 * @param callbacks Array of authentication callbacks to process.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long,boolean)",97,118,"/**
 * Creates a CryptoOutputStream with specified parameters for encryption.
 * @param out OutputStream, codec, bufferSize, key, iv, offset, close flag
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceSm4CtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:createEncryptor(),54,58,"/**
 * Creates a new JceCtrCipher encryptor.
 * @return A JceCtrCipher instance.
 * @throws GeneralSecurityException if an error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceSm4CtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.JceSm4CtrCryptoCodec:createDecryptor(),60,64,"/**
 * Creates a Decryptor instance for SM4 decryption.
 * @return Decryptor object for decryption.
 * @throws GeneralSecurityException if an error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.JceAesCtrCryptoCodec:createEncryptor(),54,58,"/**
 * Creates and returns a JceCtrCipher encryptor.
 * Uses provider, cipher suite, and ""AES"" for configuration.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceAesCtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.JceAesCtrCryptoCodec:createDecryptor(),60,64,"/**
 * Creates a Decryptor instance using JceCtrCipher.
 * @return Decryptor object for decryption operations.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,getInstance,"org.apache.hadoop.crypto.OpensslCipher:getInstance(java.lang.String,java.lang.String)",131,140,"/**
 * Creates an OpensslCipher instance for the given transformation.
 * @param transformation Cipher transformation string (e.g., ""AES-CBC"")
 * @param engineId Engine identifier, or null for default engine.
 * @return OpensslCipher instance.
 * @throws NoSuchAlgorithmException, NoSuchPaddingException
 */
","* Return an <code>OpensslCipher</code> object that implements the specified
   * transformation.
   * 
   * @param transformation the name of the transformation, e.g., 
   * AES/CTR/NoPadding.
   * @param engineId the openssl engine to use.if not set,
   * defalut engine will be used.
   * @return OpensslCipher an <code>OpensslCipher</code> object
   * @throws NoSuchAlgorithmException if <code>transformation</code> is null, 
   * empty, in an invalid format, or if Openssl doesn't implement the 
   * specified algorithm.
   * @throws NoSuchPaddingException if <code>transformation</code> contains 
   * a padding scheme that is not available.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,isSupported,org.apache.hadoop.crypto.OpensslCipher:isSupported(org.apache.hadoop.crypto.CipherSuite),181,193,"/**
 * Checks if a CipherSuite is supported based on its transformation.
 * @param suite The CipherSuite to check.
 * @return True if supported, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:getKeyVersions(java.lang.String),376,398,"/**
 * Retrieves key versions for a given name.
 * @param name Key name to fetch versions for.
 * @return List of KeyVersion objects.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,createKey,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",433,460,"/**
 * Creates a new key with the given name, material, and options.
 * @param name Key name.
 * @param material Key material.
 * @param options Key options.
 * @return KeyVersion object.
 * @throws IOException if key already exists or length is invalid.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:rollNewVersion(java.lang.String,byte[])",508,527,"/**
 * Creates a new version of the key.
 * @param name Key name.
 * @param material Key material.
 * @return New KeyVersion object.
 * @throws IOException if key not found or incorrect length.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSEncryptedKeyVersion:<init>(java.lang.String,java.lang.String,byte[],java.lang.String,byte[])",246,250,"/**
 * Constructs a KMSEncryptedKeyVersion with key details.
 * @param keyName Key name.
 * @param keyVersionName Version name.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONKeyVersion,org.apache.hadoop.util.KMSUtil:parseJSONKeyVersion(java.util.Map),185,202,"/**
 * Parses a Map to create a KeyProvider.KeyVersion object.
 * @param valueMap Map containing key version data.
 * @return KeyProvider.KeyVersion object or null if empty.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONMetadata,org.apache.hadoop.util.KMSUtil:parseJSONMetadata(java.util.Map),204,218,"/**
 * Parses a Map into a KeyProvider.Metadata object.
 * @param valueMap Map containing metadata values.
 * @return KeyProvider.Metadata object or null if empty.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,getCurrentKey,org.apache.hadoop.crypto.key.KeyProviderExtension:getCurrentKey(java.lang.String),66,69,"/**
 * Gets the current key version for the given name.
 * @param name Key name.
 * @return Current KeyVersion object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,createKey,"org.apache.hadoop.crypto.key.KeyProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",567,571,"/**
 * Creates a new key version with the given name and options.
 * @param name Key name.
 * @param options Key generation options.
 * @return New KeyVersion object.
 */
","* Create a new key generating the material for it.
   * The given key must not already exist.
   * <p>
   * This implementation generates the key material and calls the
   * {@link #createKey(String, byte[], Options)} method.
   *
   * @param name the base name of the key
   * @param options the options for the new key.
   * @return the version name of the first version of the key.
   * @throws IOException raised on errors performing I/O.
   * @throws NoSuchAlgorithmException no such algorithm exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.KeyProvider:rollNewVersion(java.lang.String),612,621,"/**
 * Generates a new version of a key.
 * @param name Key name.
 * @return New KeyVersion object.
 * @throws IOException, NoSuchAlgorithmException
 */
","* Roll a new version of the given key generating the material for it.
   * <p>
   * This implementation generates the key material and calls the
   * {@link #rollNewVersion(String, byte[])} method.
   *
   * @param name the basename of the key
   * @return the name of the new version of the key
   * @throws IOException              raised on errors performing I/O.
   * @throws NoSuchAlgorithmException This exception is thrown when a particular
   *                                  cryptographic algorithm is requested
   *                                  but is not available in the environment.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String,byte[])",140,146,"/**
 * Rolls a new version of a key.
 * @param name Key name.
 * @param material Key material.
 * @return New KeyVersion object.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,toJSON,org.apache.hadoop.util.KMSUtil:toJSON(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),110,122,"/**
 * Converts an EncryptedKeyVersion to a JSON Map.
 * @param encryptedKeyVersion The version to convert.
 * @return A Map representing the version, or empty if null.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,warmUpEncryptedKeys,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:warmUpEncryptedKeys(java.lang.String[]),288,308,"/**
 * Warms up encrypted keys for specified providers.
 * @param keyNames Names of keys to warm up.
 * @throws IOException if warming up keys fails in any provider.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,close,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:close(),544,554,"/**
 * Closes all KMS client providers, logging errors if any occur.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,readLock,org.apache.hadoop.crypto.key.kms.ValueQueue:readLock(java.lang.String),115,117,"/**
 * Acquires a read lock for the specified key.
 * @param keyName The key to acquire the read lock for.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,readUnlock,org.apache.hadoop.crypto.key.kms.ValueQueue:readUnlock(java.lang.String),119,121,"/**
 * Releases the read lock for the given key.
 * @param keyName The name of the key to unlock.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,writeUnlock,org.apache.hadoop.crypto.key.kms.ValueQueue:writeUnlock(java.lang.String),123,125,"/**
 * Releases the write lock for the given key.
 * @param keyName The key for which to release the lock.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,writeLock,org.apache.hadoop.crypto.key.kms.ValueQueue:writeLock(java.lang.String),127,129,"/**
 * Acquires a write lock for the specified key.
 * @param keyName The key to acquire the write lock on.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String),139,141,"/**
 * Constructs a Builder with a context and default separator.
 * @param context The context string.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,unpack,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:unpack(org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshResponseCollectionProto),70,80,"/**
 * Converts a collection of Proto responses to RefreshResponse objects.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolServerSideTranslatorPB.java,refresh,"org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolServerSideTranslatorPB:refresh(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.proto.GenericRefreshProtocolProtos$GenericRefreshRequestProto)",44,62,"/**
 * Refreshes data based on identifier and arguments.
 * @param controller RPC controller
 * @param request Refresh request containing identifier and args
 * @return RefreshResponseCollectionProto containing results
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,newEntry,"org.apache.hadoop.ipc.RetryCache:newEntry(long,byte[],int)",335,339,"/**
 * Creates a new CacheEntry with given expiration, client ID, call ID.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:<init>(byte[],int,java.lang.Object,long)",155,159,"/**
 * Constructs a CacheEntryWithPayload with client ID, call ID,
 * payload, and expiration time.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntry:<init>(byte[],int,long,boolean)",84,88,"/**
 * Constructs a CacheEntry with a success/failure state.
 * @param clientId Client identifier.
 * @param callId Call identifier.
 * @param expirationTime Expiration timestamp.
 * @param success Success indicator.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,isServerFailOverEnabledByQueue,org.apache.hadoop.ipc.Server:isServerFailOverEnabledByQueue(),3885,3888,"/**
* Checks if server failover is enabled by the call queue.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,put,org.apache.hadoop.ipc.CallQueueManager:put(org.apache.hadoop.ipc.Schedulable),289,299,"/**
 * Adds an element to the queue. Throws backoff if needed.
 * @param e element to add
 * @throws InterruptedException if interrupted while adding
 */
","* Insert e into the backing queue or block until we can.  If client
   * backoff is enabled this method behaves like add which throws if
   * the queue overflows.
   * If we block and the queue changes on us, we will insert while the
   * queue is drained.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,add,org.apache.hadoop.ipc.CallQueueManager:add(org.apache.hadoop.ipc.Schedulable),301,304,"/**
 * Adds an element to the collection.
 * @param e the element to add
 * @return true if the addition was successful
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,callQueueLength,org.apache.hadoop.ipc.metrics.RpcMetrics:callQueueLength(),167,169,"/**
 * Returns the length of the call queue.
 * @return The number of calls in the queue.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,initialize,org.apache.hadoop.ipc.WritableRpcEngine:initialize(),80,84,"/**
 * Initializes the RPC protocol engine for writable RPCs.
 * Sets isInitialized to true after registration.
 */
",* Register the rpcRequest deserializer for WritableRpcEngine,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,registerProtocolEngine,org.apache.hadoop.ipc.ProtobufRpcEngine2:registerProtocolEngine(),70,77,"/**
 * Registers the protocol engine for RPC protocol buffers.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,setExpirationTime,"org.apache.hadoop.util.LightWeightCache:setExpirationTime(org.apache.hadoop.util.LightWeightCache$Entry,long)",147,149,"/**
* Sets the expiration time for an entry.
* @param e The entry to set expiration for.
* @param expirationPeriod Expiration time in nanoseconds.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,start,org.apache.hadoop.util.StopWatch:start(),58,65,"/**
 * Starts the StopWatch. Throws exception if already running.
 * Returns this StopWatch instance.
 */
","* Start to measure times and make the state of stopwatch running.
   * @return this instance of StopWatch.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,stop,org.apache.hadoop.util.StopWatch:stop(),71,79,"/**
 * Stops the StopWatch and returns it. Throws exception if already stopped.
 */
","* Stop elapsed time and make the state of stopwatch stop.
   * @return this instance of StopWatch.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,now,org.apache.hadoop.util.StopWatch:now(),105,109,"/**
 * Returns the current elapsed time in nanoseconds.
 */",* @return current elapsed time in nanosecond.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,sendResponse,org.apache.hadoop.ipc.Server$Call:sendResponse(),1086,1094,"/**
 * Sends the response. Decrements a counter; triggers response if 0.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,abortResponse,org.apache.hadoop.ipc.Server$Call:abortResponse(java.lang.Throwable),1096,1103,"/**
 * Aborts the response, sending error details if needed.
 * @param t The exception causing the abort.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getFingerprint,org.apache.hadoop.ipc.ProtocolSignature:getFingerprint(java.lang.reflect.Method[]),140,142,"/**
* Calculates a fingerprint from an array of methods.
* @param methods Array of Method objects to fingerprint.
* @return An integer representing the fingerprint.
*/
","* Get the hash code of an array of methods
   * Methods are sorted before hashcode is calculated.
   * So the returned value is irrelevant of the method order in the array.
   * 
   * @param methods an array of methods
   * @return the hash code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getSigFingerprint,"org.apache.hadoop.ipc.ProtocolSignature:getSigFingerprint(java.lang.Class,long)",186,200,"/**
 * Retrieves a ProtocolSigFingerprint, caching if necessary.
 * @param protocol Protocol class.
 * @param serverVersion Server version.
 * @return ProtocolSigFingerprint object.
 */
","* Return a protocol's signature and finger print from cache
   * 
   * @param protocol a protocol class
   * @param serverVersion protocol version
   * @return its signature and finger print",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RemoteException.java,valueOf,org.apache.hadoop.ipc.RemoteException:valueOf(org.xml.sax.Attributes),131,134,"/**
 * Creates a RemoteException from attributes.
 * @param attrs Attributes object containing class and message.
 */
","* Create RemoteException from attributes.
   * @param attrs may not be null.
   * @return RemoteException.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,readResponse,org.apache.hadoop.ipc.Client$IpcStreams:readResponse(),1922,1944,"/**
 * Reads the RPC response from the input stream.
 * @return ByteBuffer containing the response data.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldFailoverOnException,org.apache.hadoop.io.retry.RetryPolicies:shouldFailoverOnException(java.lang.Exception),773,781,"/**
 * Checks if failover should occur based on the exception type.
 * @param e The exception to check.
 * @return True if it's a StandbyException after unwrapping.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,getWrappedRetriableException,org.apache.hadoop.io.retry.RetryPolicies:getWrappedRetriableException(java.lang.Exception),795,803,"/**
 * Extracts a RetriableException from a RemoteException, or null.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,monitorHealth,"org.apache.hadoop.ha.HAServiceProtocolHelper:monitorHealth(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",34,42,"/**
 * Monitors the health of a service, handling RemoteExceptions.
 * @param svc The HAServiceProtocol to monitor.
 * @param reqInfo Request info associated with the health check.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,transitionToActive,"org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",44,52,"/**
 * Transitions the service to the active state, handling RemoteExceptions.
 * @param svc The service to transition.
 * @param reqInfo Request information for the transition.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,transitionToStandby,"org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",54,62,"/**
 * Transitions the service to standby, handling RemoteExceptions.
 * @param svc The HAServiceProtocol instance.
 * @param reqInfo State change request information.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceProtocolHelper.java,transitionToObserver,"org.apache.hadoop.ha.HAServiceProtocolHelper:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol,org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo)",64,71,"/**
 * Transitions the service to observer state, handling RemoteExceptions.
 * @param svc The service to transition.
 * @param reqInfo Request information for the transition.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,isHealthCheckFailedException,org.apache.hadoop.ha.HealthMonitor:isHealthCheckFailedException(java.lang.Throwable),229,235,"/**
* Checks if the Throwable is a HealthCheckFailedException.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PartialListing.java,get,org.apache.hadoop.fs.PartialListing:get(),67,72,"/**
 * Returns the partial listing. Throws exception if present.
 * @return List of items, or null if an exception occurred.
 */
","* Partial listing of the path being listed. In the case where the path is
   * a file. The list will be a singleton with the file itself.
   *
   * @return Partial listing of the path being listed.
   * @throws IOException if there was an exception getting the listing.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,org.apache.hadoop.ipc.Server$Call:<init>(org.apache.hadoop.ipc.Server$Call),985,988,"/**
 * Constructs a new instance using values from the provided Call object.
 * @param call The Call object whose values are used for initialization.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$Call:<init>(int,int,org.apache.hadoop.ipc.RPC$RpcKind,byte[])",990,992,"/**
 * Calls RPC with given parameters, reusing the constructor.
 * @param id RPC ID, retryCount, kind, clientId
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$Call:<init>(int,int,java.lang.Void,java.lang.Void,org.apache.hadoop.ipc.RPC$RpcKind,byte[])",994,998,"/**
 * Constructs a Call with specified ID, retry count, and RPC kind.
 * @param id Call identifier. @param retryCount Retry attempts.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcScheduler.java,addResponseTime,"org.apache.hadoop.ipc.RpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",65,78,"/**
* Adds response time metrics using the legacy method.
* callName: Name of the call, schedulable: Priority, details: Processing details.
*/
","* Store a processing time value for an RPC call into this scheduler.
   *
   * @param callName The name of the call.
   * @param schedulable The schedulable representing the incoming call.
   * @param details The details of processing time.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,numDroppedConnections,org.apache.hadoop.ipc.metrics.RpcMetrics:numDroppedConnections(),171,173,"/**
 * Returns the number of dropped connections.
 * Fetches the count from the server.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,register,"org.apache.hadoop.ipc.Server$ConnectionManager:register(java.nio.channels.SocketChannel,int,boolean)",4097,4108,"/**
 * Registers a new connection. Returns the connection or null if full.
 * @param channel SocketChannel for the connection
 * @param ingressPort Ingress port number
 * @param isOnAuxiliaryPort Whether the port is auxiliary
 * @return Connection object or null if the server is full
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,numOpenConnections,org.apache.hadoop.ipc.metrics.RpcMetrics:numOpenConnections(),153,155,"/**
 * Returns the number of open connections to the server.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offerQueues,"org.apache.hadoop.ipc.FairCallQueue:offerQueues(int,org.apache.hadoop.ipc.Schedulable,boolean)",257,267,"/**
 * Offers element to queues with priority up to lastPriority.
 * @param priority element priority
 * @param e element to offer
 * @param includeLast whether to include the last queue
 * @return true if offer succeeds, false otherwise
 */
","* Offer the element to queue of the given or lower priority.
   * @param priority - starting queue priority
   * @param e - element to add
   * @param includeLast - whether to attempt last queue
   * @return boolean if added to a queue",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,"org.apache.hadoop.ipc.FairCallQueue:offer(java.lang.Object,long,java.util.concurrent.TimeUnit)",269,279,"/**
 * Offers an element to the queue, waiting up to timeout if needed.
 * @param e element to offer
 * @param timeout wait time
 * @param unit time unit for timeout
 * @return True if offered, false if timeout expired
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,offer,org.apache.hadoop.ipc.FairCallQueue:offer(java.lang.Object),281,290,"/**
 * Offers the element to the queue based on its priority.
 * @param e element to offer; returns true if offered.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,populateResponseParamsOnError,"org.apache.hadoop.ipc.Server$RpcCall:populateResponseParamsOnError(java.lang.Throwable,org.apache.hadoop.ipc.Server$RpcCall$ResponseParams)",1279,1302,"/**
 * Populates response parameters with error details from a Throwable.
 * Sets status, detailed error code, class, and stringified error.
 */","* @param t              the {@link java.lang.Throwable} to use to set
     *                       errorInfo
     * @param responseParams the {@link ResponseParams} instance to populate",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.java,getProtocolVersionForRpcKind,"org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersionForRpcKind(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.String)",106,120,"/**
 * Gets protocol versions for a given RPC kind and protocol.
 * @param rpcKind RPC kind.
 * @param protocol Protocol class name.
 * @return Array of protocol versions or null if none found.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcNoSuchProtocolException.java,<init>,org.apache.hadoop.ipc.RpcNoSuchProtocolException:<init>(java.lang.String),29,31,"/**
 * Constructs a NoSuchProtocolException with the given error message.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcNoSuchMethodException.java,<init>,org.apache.hadoop.ipc.RpcNoSuchMethodException:<init>(java.lang.String),30,32,"/**
 * Constructs a RpcNoSuchMethodException with the given error message.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,<init>,"org.apache.hadoop.ipc.RPC$VersionMismatch:<init>(java.lang.String,long,long)",248,255,"/**
 * Constructs a VersionMismatch exception with interface name & versions.
 * @param interfaceName Interface name.
 * @param clientVersion Client version.
 * @param serverVersion Server version.
 */
","* Create a version mismatch exception
     * @param interfaceName the name of the protocol mismatch
     * @param clientVersion the client's version of the protocol
     * @param serverVersion the server's version of the protocol",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$FatalRpcServerException:<init>(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,java.io.IOException)",1986,1989,"/**
 * Constructs a FatalRpcServerException with error code and IO exception.
 * @param errCode RpcErrorCodeProto - Error code.
 * @param ioe IOException - The underlying IO exception.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,<init>,org.apache.hadoop.ipc.ResponseBuffer$FramedBuffer:<init>(int),78,81,"/**
* Constructs a FramedBuffer with specified capacity.
* @param capacity initial buffer capacity
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,reset,org.apache.hadoop.ipc.ResponseBuffer:reset(),70,74,"/**
 * Resets the buffer to its initial state.
 * Returns a reference to this buffer.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,writeTo,org.apache.hadoop.ipc.ResponseBuffer:writeTo(java.io.OutputStream),48,50,"/**
 * Writes the framed buffer to the specified output stream.
 * @param out the output stream to write to
 * @throws IOException if an I/O error occurs
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,toByteArray,org.apache.hadoop.ipc.ResponseBuffer:toByteArray(),52,54,"/**
 * Converts the framed buffer to a byte array.
 * @return Byte array representation of the framed buffer.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,recomputeScheduleCache,org.apache.hadoop.ipc.DecayRpcScheduler:recomputeScheduleCache(),545,560,"/**
 * Recomputes the schedule cache based on call costs.
 * Updates the scheduleCacheRef with the new computed levels.
 */
",* Update the scheduleCache to match current conditions in callCosts.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,cachedOrComputedPriorityLevel,org.apache.hadoop.ipc.DecayRpcScheduler:cachedOrComputedPriorityLevel(java.lang.Object),642,661,"/**
 * Gets priority level, using cache if available, else computes it.
 * @param identity Object used as key for cache/computation.
 * @return Priority level as an integer.
 */","* Returns the priority level for a given identity by first trying the cache,
   * then computing it.
   * @param identity an object responding to toString and hashCode
   * @return integer scheduling decision from 0 to numLevels - 1",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,setPriorityLevel,"org.apache.hadoop.ipc.CallQueueManager:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",272,276,"/**
 * Sets the priority level for a user via the scheduler.
 * @param user UserGroupInformation object
 * @param priority The priority level to set.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getCallVolumeSummary,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getCallVolumeSummary(),914,922,"/**
 * Gets the call volume summary from the delegate scheduler.
 * Returns ""No Active Scheduler"" if the delegate is null.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,constructRpcRequest,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:constructRpcRequest(java.lang.reflect.Method,com.google.protobuf.Message)",296,299,"/**
 * Creates an RpcProtobufRequest object.
 * @param method RPC method
 * @param theRequest The request message
 * @return An RpcProtobufRequest object
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,constructRpcRequest,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:constructRpcRequest(java.lang.reflect.Method,org.apache.hadoop.thirdparty.protobuf.Message)",306,309,"/**
 * Creates an RpcProtobufRequest with header and request message.
 * @param method RPC method
 * @param theRequest The request message
 * @return RpcProtobufRequest object
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.java,refreshServiceAcl,org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:refreshServiceAcl(),54,58,"/**
 * Refreshes the service ACL using RPC.
 * Uses an internal RPC call with no request data.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,refreshUserToGroupsMappings,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:refreshUserToGroupsMappings(),59,63,"/**
 * Refreshes user-to-groups mappings via RPC call.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,refreshSuperUserGroupsConfiguration,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:refreshSuperUserGroupsConfiguration(),65,69,"/**
 * Refreshes the superuser groups configuration using RPC.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.java,refreshCallQueue,org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:refreshCallQueue(),54,58,"/**
 * Refreshes the call queue using an RPC proxy.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolClientSideTranslatorPB.java,getGroupsForUser,org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:getGroupsForUser(java.lang.String),52,59,"/**
 * Retrieves a list of groups for the given user.
 * @param user The username to fetch groups for.
 * @return String array of group names.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,cedeActive,org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:cedeActive(int),56,63,"/**
 * Cedes active session.
 * @param millisToCede Milliseconds for cession.
 * @throws IOException, AccessControlException on failure.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,gracefulFailover,org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:gracefulFailover(),65,69,"/**
 * Performs a graceful failover operation using the RPC proxy.
 * Throws IOException or AccessControlException on failure.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,monitorHealth,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:monitorHealth(),84,87,"/**
 * Monitors health via RPC, using a predefined request.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,getServiceStatus,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:getServiceStatus(),114,128,"/**
 * Gets the service status.
 * @return HAServiceStatus object representing the service status.
 * @throws IOException if an I/O error occurs during the RPC call.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invokeMethod,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeMethod(),165,171,"/**
 * Invokes the method with arguments, potentially setting call ID.
 * @return Result of the invoked method.
 * @throws Throwable if an error occurs during invocation.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,close,org.apache.hadoop.ipc.WritableRpcEngine$Invoker:close(),264,270,"/**
 * Closes the client, stopping its associated connection.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,close,org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:close(),325,331,"/**
 * Closes the client connection, stopping associated operations.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,close,org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:close(),335,341,"/**
 * Closes the client connection, stopping data transmission.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getMetrics,"org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",462,477,"/**
 * Collects FairCallQueue metrics and adds them to the collector.
 * @param collector MetricsCollector to add the metrics to
 * @param all if true, collect all metrics
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,wrap,org.apache.hadoop.ipc.RpcWritable:wrap(java.lang.Object),40,53,"/**
 * Wraps an object as an RpcWritable, handling RpcWritable, Message,
 * Writable, and unshaded protobuf types. Throws IllegalArgumentException if unwrapped.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,hashCode,org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:hashCode(),174,177,"/**
 * Returns the hash code value for this object. Delegates to super.
 */",Override hashcode to avoid findbugs warnings,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.java,getAndAdvanceCurrentIndex,org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:getAndAdvanceCurrentIndex(),145,149,"/**
 * Gets and advances the current index.
 * @return The current index value.
 */
",* Use the mux by getting and advancing index.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,registerForDeferredResponse,org.apache.hadoop.ipc.ProtobufRpcEngine$Server:registerForDeferredResponse(),421,426,"/**
 * Registers a callback for deferred responses.
 * Creates and sets the current callback, then returns it.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,registerForDeferredResponse2,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:registerForDeferredResponse2(),453,458,"/**
 * Registers and returns a ProtobufRpcEngineCallback2 instance.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,writeTo,org.apache.hadoop.ipc.RpcWritable$ProtobufWrapper:writeTo(org.apache.hadoop.ipc.ResponseBuffer),110,116,"/**
 * Writes the message to the ResponseBuffer.
 * Writes the message in delimited format.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,writeTo,org.apache.hadoop.ipc.RpcWritable$Buffer:writeTo(org.apache.hadoop.ipc.ResponseBuffer),159,163,"/**
 * Writes the buffer's contents to the provided ResponseBuffer.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufWrapperLegacy.java,writeTo,org.apache.hadoop.ipc.ProtobufWrapperLegacy:writeTo(org.apache.hadoop.ipc.ResponseBuffer),63,70,"/**
 * Writes the message to the provided ResponseBuffer.
 * Writes message and its size to the buffer.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,cleanupCalls,org.apache.hadoop.ipc.Client$Connection:cleanupCalls(),1307,1314,"/**
 * Cleans up call entries, removing them and setting an exception.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getRemoteAddress,org.apache.hadoop.ipc.Server:getRemoteAddress(),436,439,"/**
 * Gets the remote IP address as a string.
 * @return Remote IP address string, or null if unavailable.
 */
","@return Returns remote address as a string when invoked inside an RPC.
   *  Returns null in case of an error.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,numOpenConnectionsPerUser,org.apache.hadoop.ipc.metrics.RpcMetrics:numOpenConnectionsPerUser(),162,165,"/**
 * Returns the number of open connections per user.
 * Delegates to the server for the actual value.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doAsyncWrite,org.apache.hadoop.ipc.Server$Responder:doAsyncWrite(java.nio.channels.SelectionKey),1798,1821,"/**
 * Writes data asynchronously for an RPC call.
 * Processes response queue and updates key interest ops.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRespond,org.apache.hadoop.ipc.Server$Responder:doRespond(org.apache.hadoop.ipc.Server$RpcCall),1927,1939,"/**
 * Adds a call to the response queue and processes if it's the first.
 * @param call The RPC call to add.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsTracer.java,get,org.apache.hadoop.fs.FsTracer:get(org.apache.hadoop.conf.Configuration),39,47,"/**
 * Returns the Tracer instance, creating it if it doesn't exist.
 * @param conf Configuration object for Tracer initialization.
 * @return Tracer instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,"org.apache.hadoop.util.MachineList:<init>(java.lang.String,org.apache.hadoop.util.MachineList$InetAddressFactory)",77,79,"/**
 * Constructs a MachineList with trimmed host entries.
 * @param hostEntries Comma-separated host entries string.
 * @param addressFactory Factory for creating InetAddresses.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,org.apache.hadoop.util.MachineList:<init>(java.util.Collection),85,87,"/**
 * Constructs a MachineList with provided host entries and default InetAddressFactory.
 * @param hostEntries Collection of host entries to initialize the list.
 */
","*
   * @param hostEntries collection of separated ip/cidr/host addresses",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FileBasedIPList.java,isIn,org.apache.hadoop.util.FileBasedIPList:isIn(java.lang.String),71,77,"/**
 * Checks if the given IP address is in the address list.
 * @param ipAddress The IP address to check.
 * @return True if the IP is in the list, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,<init>,org.apache.hadoop.util.SysInfoLinux:<init>(),180,183,"/**
 * Default constructor, uses default file paths and jiffy length.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,readProcMemInfoFile,org.apache.hadoop.util.SysInfoLinux:readProcMemInfoFile(),215,217,"/**
 * Reads /proc/meminfo file. Calls overloaded method with flag false.
 */","* Read /proc/meminfo, parse and compute memory information only once.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getAvailablePhysicalMemorySize,org.apache.hadoop.util.SysInfoLinux:getAvailablePhysicalMemorySize(),609,616,"/**
 * Gets the available physical memory size in KB.
 * Uses ramSizeFree and inactiveFileSize/inactiveSize.
 */
",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getCumulativeCpuTime,org.apache.hadoop.util.SysInfoLinux:getCumulativeCpuTime(),646,650,"/**
 * Gets the cumulative CPU time.
 * Reads procstat file and returns tracked CPU time.
 */
",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getCpuUsagePercentage,org.apache.hadoop.util.SysInfoLinux:getCpuUsagePercentage(),653,661,"/**
 * Calculates CPU usage percentage, divided by the number of processors.
 */",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getNumVCoresUsed,org.apache.hadoop.util.SysInfoLinux:getNumVCoresUsed(),664,672,"/**
 * Returns the percentage of vCores used.
 * Reads procstat, calculates usage. Returns float value.
 */
",{@inheritDoc},,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getStorageBytesRead,org.apache.hadoop.util.SysInfoLinux:getStorageBytesRead(),688,692,"/**
 * Returns the total bytes read from storage.
 * Reads the info file and returns the value.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getStorageBytesWritten,org.apache.hadoop.util.SysInfoLinux:getStorageBytesWritten(),694,698,"/**
 * Gets the total storage bytes written.
 * Reads disk info file and returns the bytes written.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,<init>,org.apache.hadoop.util.IdentityHashStore:<init>(int),62,72,"/**
 * Initializes the IdentityHashStore with a given capacity.
 * @param capacity initial capacity of the store, must be non-negative.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IdentityHashStore.java,put,"org.apache.hadoop.util.IdentityHashStore:put(java.lang.Object,java.lang.Object)",118,126,"/**
 * Inserts the specified key-value pair into the buffer.
 * Doubles buffer size if full.
 */
","* Add a new (key, value) mapping.
   *
   * Inserting a new (key, value) never overwrites a previous one.
   * In other words, you can insert the same key multiple times and it will
   * lead to multiple entries.
   *
   * @param k Generics Type k.
   * @param v Generics Type v.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,releaseBuffer,org.apache.hadoop.fs.FSDataInputStream:releaseBuffer(java.nio.ByteBuffer),222,235,"/**
 * Releases a buffer, delegating or returning it to the pool.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,hasNext,org.apache.hadoop.util.LightWeightGSet$SetIterator:hasNext(),329,333,"/**
 * Checks if there's a next element in the iterator.
 * Returns true if next is not null, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,next,org.apache.hadoop.util.LightWeightGSet$SetIterator:next(),335,344,"/**
 * Returns the next element in the iterator.
 * Throws IllegalStateException if no more elements.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,put,org.apache.hadoop.util.LightWeightGSet:put(java.lang.Object),149,177,"/**
* Adds an element to the linked list.
* @param element The element to add.
* @return The previous element at the index, or null if new.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,remove,org.apache.hadoop.util.LightWeightGSet:remove(java.lang.Object),221,228,"/**
 * Removes the value associated with the given key.
 * @param key The key whose value is to be removed.
 * @return The removed value, or null if the key wasn't found.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sort,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:sort(int),3253,3256,"/**
 * Copies and sorts the first 'count' elements of the 'pointers' array.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,newSecureTransformerFactory,org.apache.hadoop.util.XMLUtils:newSecureTransformerFactory(),148,154,"/**
 * Creates a secure TransformerFactory with secure processing enabled.
 * @return TransformerFactory configured for secure XML transformations.
 * @throws TransformerConfigurationException if factory creation fails.
 */
","* This method should be used if you need a {@link TransformerFactory}. Use this method
   * instead of {@link TransformerFactory#newInstance()}. The factory that is returned has
   * secure configuration enabled.
   *
   * @return a {@link TransformerFactory} with secure configuration enabled
   * @throws TransformerConfigurationException if the {@code JAXP} transformer does not
   * support the secure configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,newSecureSAXTransformerFactory,org.apache.hadoop.util.XMLUtils:newSecureSAXTransformerFactory(),165,171,"/**
 * Creates a secure SAX transformer factory.
 * Enables secure processing and sets optional attributes.
 * @return SAXTransformerFactory instance
 * @throws TransformerConfigurationException if creation fails
 */
","* This method should be used if you need a {@link SAXTransformerFactory}. Use this method
   * instead of {@link SAXTransformerFactory#newInstance()}. The factory that is returned has
   * secure configuration enabled.
   *
   * @return a {@link SAXTransformerFactory} with secure configuration enabled
   * @throws TransformerConfigurationException if the {@code JAXP} transformer does not
   * support the secure configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,formatSize,"org.apache.hadoop.fs.ContentSummary:formatSize(long,boolean)",477,481,"/**
 * Formats a size as a string, human-readable or raw.
 * @param size Size in bytes.
 * @param humanReadable Whether to use human-readable format.
 * @return Formatted size string.
 */
","* Formats a size to be human readable or in bytes.
   * @param size value to be formatted
   * @param humanReadable flag indicating human readable or not
   * @return String representation of the size",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,formatSize,org.apache.hadoop.fs.shell.Ls:formatSize(long),126,130,"/**
 * Formats file size as human-readable or raw value.
 * @param size Size in bytes.
 * @return Formatted size string.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,formatSize,org.apache.hadoop.fs.shell.FsUsage:formatSize(long),55,59,"/**
 * Formats a size in human-readable format or as a string.
 * @param size The size in bytes.
 * @return Formatted size string.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,formatSize,"org.apache.hadoop.fs.QuotaUsage:formatSize(long,boolean)",394,398,"/**
 * Formats file size as human-readable or raw value.
 * @param size File size in bytes.
 * @param humanReadable True for human-readable format.
 */
","* Formats a size to be human readable or in bytes.
   * @param size value to be formatted
   * @param humanReadable flag indicating human readable or not
   * @return String representation of the size",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,humanReadableInt,org.apache.hadoop.util.StringUtils:humanReadableInt(long),132,135,"/**
 * Converts a long to a human-readable string representation.
 * Uses TraditionalBinaryPrefix for the conversion.
 */
","* Given an integer, return a string that is in an approximate, but human 
   * readable format. 
   * @param number the number to format
   * @return a human readable form of the integer
   *
   * @deprecated use {@link TraditionalBinaryPrefix#long2String(long, String, int)}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteDesc,org.apache.hadoop.util.StringUtils:byteDesc(long),1022,1024,"/**
 * Converts a byte count to a human-readable binary prefix string.
 * @param len The number of bytes.
 * @return A string representation of the byte count.
 */
","* a byte description of the given long interger value.
   *
   * @param len len.
   * @return a byte description of the given long interger value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,computeCapacity,"org.apache.hadoop.util.LightWeightGSet:computeCapacity(long,double,java.lang.String)",379,417,"/**
 * Computes the capacity based on max memory and percentage.
 * @param maxMemory Maximum memory in bytes.
 * @param percentage Percentage of maxMemory to use.
 * @param mapName Name of the map.
 * @return Computed capacity.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,addToUsagesTable,"org.apache.hadoop.fs.shell.FsUsage$Df:addToUsagesTable(java.net.URI,org.apache.hadoop.fs.FsStatus,java.lang.String)",114,127,"/**
 * Adds filesystem usage data to the usages table.
 * @param uri The URI of the filesystem.
 * @param fsStatus Filesystem status object.
 * @param mountedOnPath Path where filesystem is mounted.
 */
","* Add a new row to the usages table for the given FileSystem URI.
     *
     * @param uri - FileSystem URI
     * @param fsStatus - FileSystem status
     * @param mountedOnPath - FileSystem mounted on path",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,readChecksumChunk,"org.apache.hadoop.fs.FSInputChecker:readChecksumChunk(byte[],int,int)",293,334,"/**
 * Reads a chunk of data, verifies checksum, and retries on error.
 * @param b buffer to read into, off offset, len length
 * @return Number of bytes read, or -1 if an error occurs.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,readChars,"org.apache.hadoop.io.UTF8:readChars(java.io.DataInput,java.lang.StringBuilder,int)",279,332,"/**
 * Reads UTF8 encoded characters from input, appending to buffer.
 * @param in Input stream to read from.
 * @param buffer StringBuilder to append characters to.
 * @param nBytes Number of bytes to read.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteToHexString,org.apache.hadoop.util.StringUtils:byteToHexString(byte[]),199,201,"/**
 * Converts a byte array to a hexadecimal string representation.
 */","* Same as byteToHexString(bytes, 0, bytes.length).
   * @param bytes bytes.
   * @return byteToHexString.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HeapSort.java,sort,"org.apache.hadoop.util.HeapSort:sort(org.apache.hadoop.util.IndexedSortable,int,int)",51,54,"/**
 * Sorts a portion of an indexed sortable using a comparator.
 * @param s sortable object
 * @param p start index
 * @param r end index
 */
","* Sort the given range of items using heap sort.
   * {@inheritDoc}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exit,org.apache.hadoop.service.launcher.ServiceLauncher:exit(org.apache.hadoop.util.ExitUtil$ExitException),874,876,"/**
 * Terminates the application with the provided exit exception.
 */
","* Exit the JVM using an exception for the exit code and message,
   * invoking {@link ExitUtil#terminate(ExitUtil.ExitException)}.
   *
   * This is the standard way a launched service exits.
   * An error code of 0 means success -nothing is printed.
   *
   * If {@link ExitUtil#disableSystemExit()} has been called, this
   * method will throw the exception.
   *
   * The method <i>may</i> be subclassed for testing
   * @param ee exit exception
   * @throws ExitUtil.ExitException if ExitUtil exceptions are disabled",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exitWithMessage,"org.apache.hadoop.service.launcher.ServiceLauncher:exitWithMessage(int,java.lang.String)",1024,1026,"/**
 * Terminates the service with a given status and error message.
 * @param status Exit status code.
 * @param message Error message to include in exception.
 */
","* Exit with a printed message. 
   * @param status status code
   * @param message message message to print before exiting
   * @throws ExitUtil.ExitException if exceptions are disabled",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,"org.apache.hadoop.util.ExitUtil:terminate(int,java.lang.Throwable)",338,344,"/**
 * Terminates the application with a given status and exception.
 * Handles ExitException or wraps other exceptions as ExitException.
 */
","* Like {@link #terminate(int, String)} but uses the given throwable to
   * build the message to display or throw as an
   * {@link ExitException}.
   * <p>
   * @param status exit code to use if the exception is not an ExitException.
   * @param t throwable which triggered the termination. If this exception
   * is an {@link ExitException} its status overrides that passed in.
   * @throws ExitException if {@link System#exit(int)}  is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,"org.apache.hadoop.util.ExitUtil:terminate(int,java.lang.String)",380,382,"/**
* Terminates the application with a given status and message.
* @param status Exit status code.
* @param msg Error message.
*/
","* Terminate the current process. Note that terminate is the *only* method
   * that should be used to terminate the daemon processes.
   *
   * @param status exit code
   * @param msg message used to create the {@code ExitException}
   * @throws ExitException if {@link System#exit(int)} is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,"org.apache.hadoop.util.ExitUtil:halt(int,java.lang.Throwable)",354,360,"/**
 * Halts execution with a status code and optional exception.
 * @param status Halt status code.
 * @param t Optional exception to associate with halt.
 * @throws HaltException if halting fails.
 */
","* Forcibly terminates the currently running Java virtual machine.
   *
   * @param status exit code to use if the exception is not a HaltException.
   * @param t throwable which triggered the termination. If this exception
   * is a {@link HaltException} its status overrides that passed in.
   * @throws HaltException if {@link System#exit(int)}  is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,"org.apache.hadoop.util.ExitUtil:halt(int,java.lang.String)",399,401,"/**
 * Halts execution with a HaltException.
 * @param status Halt status code.
 * @param message Halt message.
 */
","* Forcibly terminates the currently running Java virtual machine.
   * @param status status code
   * @param message message
   * @throws HaltException if {@link Runtime#halt(int)} is disabled.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,unregister,org.apache.hadoop.service.launcher.ServiceShutdownHook:unregister(),69,75,"/**
 * Removes this instance as a shutdown hook. Handles potential errors.
 */",* Unregister the hook.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/QuickSort.java,sort,"org.apache.hadoop.util.QuickSort:sort(org.apache.hadoop.util.IndexedSortable,int,int,org.apache.hadoop.util.Progressable)",63,67,"/**
 * Sorts a portion of the indexed sortable.
 * @param s sortable object, p start index, r end index, rep progress reporter
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,<init>,org.apache.hadoop.util.LightWeightResizableGSet:<init>(),83,85,"/**
 * Constructs a LightWeightResizableGSet with default capacity & load factor.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,<init>,org.apache.hadoop.util.LightWeightResizableGSet:<init>(int),87,89,"/**
 * Constructs a LightWeightResizableGSet with initial capacity.
 * @param initCapacity initial capacity of the set
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayList,org.apache.hadoop.util.Lists:newArrayList(java.lang.Iterable),91,98,"/**
 * Creates an ArrayList from an Iterable.
 * @param elements Iterable to copy elements from.
 * @return New ArrayList containing elements from Iterable.
 */
","* Creates a <i>mutable</i> {@code ArrayList} instance containing the
   * given elements; a very thin shortcut for creating an empty list then
   * calling Iterables#addAll.
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return ArrayList Generics Type E.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newLinkedList,org.apache.hadoop.util.Lists:newLinkedList(java.lang.Iterable),185,190,"/**
 * Creates a new LinkedList from the given iterable elements.
 * @param elements Iterable containing elements for the list.
 * @return A new LinkedList containing the elements.
 */
","* Creates a <i>mutable</i> {@code LinkedList} instance containing the given
   * elements; a very thin shortcut for creating an empty list then calling
   * Iterables#addAll.
   *
   * <p><b>Performance note:</b> {@link ArrayList} and
   * {@link java.util.ArrayDeque} consistently
   * outperform {@code LinkedList} except in certain rare and specific
   * situations. Unless you have spent a lot of time benchmarking your
   * specific needs, use one of those instead.</p>
   *
   * @param elements elements.
   * @param <E> Generics Type E.
   * @return Generics Type E List.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclUtil.java,getAclFromPermAndEntries,"org.apache.hadoop.fs.permission.AclUtil:getAclFromPermAndEntries(org.apache.hadoop.fs.permission.FsPermission,java.util.List)",42,90,"/**
 * Creates an ACL from a permission and a list of entries.
 * @param perm FsPermission object containing permission bits
 * @param entries List of existing AclEntry objects
 * @return Combined ACL list
 */
","* Given permissions and extended ACL entries, returns the full logical ACL.
   *
   * @param perm FsPermission containing permissions
   * @param entries List&lt;AclEntry&gt; containing extended ACL entries
   * @return List&lt;AclEntry&gt; containing full logical ACL",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,addChunk,org.apache.hadoop.util.ChunkedArrayList:addChunk(int),156,160,"/**
 * Adds a new chunk to the list with specified capacity.
 * @param capacity The initial capacity of the new chunk.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayList,org.apache.hadoop.util.Lists:newArrayList(java.lang.Object[]),70,80,"/**
 * Creates an ArrayList from a variable number of elements.
 * @param elements Elements to add to the ArrayList
 * @return An ArrayList containing the provided elements
 */
","* Creates a <i>mutable</i> {@code ArrayList} instance containing the given
   * elements.
   *
   * <p>Note that even when you do need the ability to add or remove,
   * this method provides only a tiny bit of syntactic sugar for
   * {@code newArrayList(}
   * {@link Arrays#asList asList}
   * {@code (...))}, or for creating an empty list then calling
   * {@link Collections#addAll}.
   *
   * @param <E> Generics Type E.
   * @param elements elements.
   * @return ArrayList Generics Type E.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Lists.java,newArrayListWithExpectedSize,org.apache.hadoop.util.Lists:newArrayListWithExpectedSize(int),148,151,"/**
 * Creates a new ArrayList with an estimated initial capacity.
 * @param estimatedSize The expected size of the ArrayList.
 */
","* Creates an {@code ArrayList} instance to hold {@code estimatedSize}
   * elements, <i>plus</i> an unspecified amount of padding;
   * you almost certainly mean to call {@link
   * #newArrayListWithCapacity} (see that method for further advice on usage).
   *
   * @param estimatedSize an estimate of the eventual {@link List#size()}
   *     of the new list.
   * @return a new, empty {@code ArrayList}, sized appropriately to hold the
   *     estimated number of elements.
   * @throws IllegalArgumentException if {@code estimatedSize} is negative.
   *
   * @param <E> Generics Type E.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,loadClass,org.apache.hadoop.util.ApplicationClassLoader:loadClass(java.lang.String),155,158,"/**
 * Loads a class by name.
 * @param name The name of the class to load.
 * @return The Class object or throws ClassNotFoundException.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,save,"org.apache.hadoop.util.JsonSerialization:save(java.io.File,java.lang.Object)",201,204,"/**
 * Saves an instance to a file in JSON format.
 * @param file The file to save to.
 * @param instance The object to save.
 */
","* Save to a local file. Any existing file is overwritten unless
   * the OS blocks that.
   * @param file file
   * @param instance instance
   * @throws IOException IO exception",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,<init>,"org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String,long)",72,81,"/**
 * Initializes a duration tracker with given stats store, key, and count.
 */
","* Constructor.
   * If the supplied count is greater than zero, the counter
   * of the key name is updated.
   * @param iostats statistics to update
   * @param key Key to use as prefix of values.
   * @param count #of times to increment the matching counter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DurationInfo.java,<init>,"org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,boolean,java.lang.String,java.lang.Object[])",69,83,"/**
 * Creates a DurationInfo object with specified logging and format.
 * @param log Logger instance
 * @param logAtInfo Whether to log at info level
 * @param format Format string for duration text
 * @param args Arguments for format string
 */
","* Create the duration text from a {@code String.format()} code call
   * and log either at info or debug.
   * @param log log to write to
   * @param logAtInfo should the log be at info, rather than debug
   * @param format format string
   * @param args list of arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/OperationDuration.java,toString,org.apache.hadoop.util.OperationDuration:toString(),91,94,"/**
 * Returns a string representation of the object's duration.
 */
","* Return the duration as {@link #humanTime(long)}.
   * @return a printable duration.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,newStripedCrcComposer,"org.apache.hadoop.util.CrcComposer:newStripedCrcComposer(org.apache.hadoop.util.DataChecksum$Type,long,long)",83,92,"/**
 * Creates a striped CRC composer with specified parameters.
 * @param type Checksum type, bytesPerCrcHint, stripeLength
 * @return A new CrcComposer instance.
 */
","* Returns a CrcComposer which will collapse CRCs for every combined
   * underlying data size which aligns with the specified stripe boundary. For
   * example, if ""update"" is called with 20 CRCs and bytesPerCrc == 5, and
   * stripeLength == 10, then every two (10 / 5) consecutive CRCs will be
   * combined with each other, yielding a list of 10 CRC ""stripes"" in the
   * final digest, each corresponding to 10 underlying data bytes. Using
   * a stripeLength greater than the total underlying data size is equivalent
   * to using a non-striped CrcComposer.
   *
   * @param type type.
   * @param bytesPerCrcHint bytesPerCrcHint.
   * @param stripeLength stripeLength.
   * @return a CrcComposer which will collapse CRCs for every combined.
   * underlying data size which aligns with the specified stripe boundary.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcUtil.java,compose,"org.apache.hadoop.util.CrcUtil:compose(int,int,long,int)",102,105,"/**
 * Composes two CRCs using a monomial.
 * @param crcA Initial CRC value.
 * @param crcB Current CRC value.
 * @param lengthB Length of the data.
 * @param mod Modulus value.
 */
","* compose.
   *
   * @param crcA crcA.
   * @param crcB crcB.
   * @param lengthB length of content corresponding to {@code crcB}, in bytes.
   * @param mod mod.
   * @return compose result.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CompositeCrcFileChecksum.java,getBytes,org.apache.hadoop.fs.CompositeCrcFileChecksum:getBytes(),64,67,"/**
 * Returns the CRC value as a byte array.
 * Uses CrcUtil to convert the integer CRC to bytes.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,digest,org.apache.hadoop.util.CrcComposer:digest(),204,213,"/**
 * Writes the current CRC value to the digest and resets.
 * @return byte array containing the CRC digest.
 */
","* Returns byte representation of composed CRCs; if no stripeLength was
   * specified, the digest should be of length equal to exactly one CRC.
   * Otherwise, the number of CRCs in the returned array is equal to the
   * total sum bytesPerCrc divided by stripeLength. If the sum of bytesPerCrc
   * is not a multiple of stripeLength, then the last CRC in the array
   * corresponds to totalLength % stripeLength underlying data bytes.
   *
   * @return byte representation of composed CRCs.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJarAndSave,"org.apache.hadoop.util.RunJar:unJarAndSave(java.io.InputStream,java.io.File,java.lang.String,java.util.regex.Pattern)",168,178,"/**
 * Unpacks a JAR file from an InputStream to a directory.
 * @param inputStream Input stream of the JAR file.
 * @param toDir Directory to unpack the JAR to.
 * @param name JAR file name.
 * @param unpackRegex Regex pattern for files to unpack.
 */
","* Unpack matching files from a jar. Entries inside the jar that do
   * not match the given pattern will be skipped. Keep also a copy
   * of the entire jar in the same directory for backward compatibility.
   * TODO remove this feature in a new release and do only unJar
   *
   * @param inputStream the jar stream to unpack
   * @param toDir the destination directory into which to unpack the jar
   * @param unpackRegex the pattern to match jar entries against
   * @param name name.
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,unJar,"org.apache.hadoop.util.RunJar:unJar(java.io.File,java.io.File)",104,106,"/**
 * Extracts files from a JAR archive to a directory.
 * @param jarFile The JAR file to unjar.
 * @param toDir The directory to extract files to.
 */
","* Unpack a jar file into a directory.
   *
   * This version unpacks all files inside the jar regardless of filename.
   *
   * @param jarFile the .jar file to unpack
   * @param toDir the destination directory into which to unpack the jar
   *
   * @throws IOException if an I/O error has occurred or toDir
   * cannot be created and does not already exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/UTF8ByteArrayUtils.java,findNthByte,"org.apache.hadoop.util.UTF8ByteArrayUtils:findNthByte(byte[],byte,int)",98,100,"/**
* Finds the nth occurrence of a byte in a byte array.
* @param utf byte array to search, b byte to find, n occurrence (1-based)
*/
","* Find the nth occurrence of the given byte b in a UTF-8 encoded string
   * @param utf a byte array containing a UTF-8 encoded string
   * @param b the byte to find
   * @param n the desired occurrence of the given byte
   * @return position that nth occurrence of the given byte if exists; otherwise -1",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/WeakReferenceMap.java,get,org.apache.hadoop.util.WeakReferenceMap:get(java.lang.Object),147,171,"/**
 * Retrieves a value associated with the given key.
 * @param key the key to look up
 * @return the value or creates a new one if not found.
 */
","* Get the value, creating if needed.
   * @param key key.
   * @return an instance.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,check,"org.apache.hadoop.util.InstrumentedLock:check(long,long,boolean)",190,226,"/**
 * Logs a lock/wait warning if the holding time exceeds the threshold.
 * @param acquireTime Lock acquisition timestamp.
 * @param releaseTime Lock release timestamp.
 * @param checkLockHeld Whether a lock was held or waited.
 */
","* Log a warning if the lock was held for too long.
   *
   * Should be invoked by the caller immediately AFTER releasing the lock.
   *
   * @param acquireTime  - timestamp just after acquiring the lock.
   * @param releaseTime - timestamp just before releasing the lock.
   * @param checkLockHeld checkLockHeld.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getFormattedTimeWithDiff,"org.apache.hadoop.util.StringUtils:getFormattedTimeWithDiff(java.lang.String,long,long)",390,400,"/**
 * Formats finish time, optionally adding time difference.
 * @param formattedFinishTime Finish time string.
 * @param finishTime Finish time in milliseconds.
 * @param startTime Start time in milliseconds.
 * @return Formatted time string with time difference.
 */
","* Formats time in ms and appends difference (finishTime - startTime)
   * as returned by formatTimeDiff().
   * If finish time is 0, empty string is returned, if start time is 0
   * then difference is not appended to return value.
   * @param formattedFinishTime formattedFinishTime to use
   * @param finishTime finish time
   * @param startTime start time
   * @return formatted value.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,split,org.apache.hadoop.util.StringUtils:split(java.lang.String),570,572,"/**
 * Splits a string by a comma, escaping with ESCAPE_CHAR.
 */","* Split a string using the default separator
   * @param str a string that may have escaped separator
   * @return an array of strings",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,camelize,org.apache.hadoop.util.StringUtils:camelize(java.lang.String),1094,1102,"/**
 * Converts a string to camelCase.
 * @param s The input string to camelize.
 * @return The camelized string.
 */
","* Convert SOME_STUFF to SomeStuff
   *
   * @param s input string
   * @return camelized string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,escapeString,"org.apache.hadoop.util.StringUtils:escapeString(java.lang.String,char,char)",678,681,"/**
 * Escapes a string by replacing a specific char with an escape char.
 * @param str String to escape
 * @param escapeChar Char to use for escaping
 * @param charToEscape Char to be escaped
 * @return Escaped string
 */
","* Escape <code>charToEscape</code> in the string 
   * with the escape char <code>escapeChar</code>
   * 
   * @param str string
   * @param escapeChar escape char
   * @param charToEscape the char to be escaped
   * @return an escaped string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,unEscapeString,"org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String,char,char)",736,739,"/**
 * Unescapes a string, replacing escaped characters.
 * @param str String to unescape
 * @param escapeChar Escape character
 * @param charToEscape Character to escape
 */
","* Unescape <code>charToEscape</code> in the string 
   * with the escape char <code>escapeChar</code>
   * 
   * @param str string
   * @param escapeChar escape char
   * @param charToEscape the escaped char
   * @return an unescaped string",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,createStartupShutdownMessage,"org.apache.hadoop.util.StringUtils:createStartupShutdownMessage(java.lang.String,java.lang.String,java.lang.String[])",837,851,"/**
 * Creates a startup/shutdown message string with class, host, args, version info.
 */
","* Generate the text for the startup/shutdown message of processes.
   * @param classname short classname of the class
   * @param hostname hostname
   * @param args Command arguments
   * @return a string to log.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,getBuildVersion,org.apache.hadoop.util.VersionInfo:getBuildVersion(),162,164,"/**
 * Returns the build version from the COMMON_VERSION_INFO.
 */","* Returns the buildVersion which includes version,
   * revision, user and date.
   * @return the buildVersion",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,next,org.apache.hadoop.util.functional.RemoteIterators$FilteringRemoteIterator:next(),650,658,"/**
 * Returns the next element in the sequence or throws exception.
 */
","* Return the next value.
     * Will retrieve the next elements if needed.
     * This is where the mapper takes place.
     * @return true if there is another data element.
     * @throws IOException failure in fetch operation or the transformation.
     * @throws NoSuchElementException no more data",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,close,org.apache.hadoop.util.functional.RemoteIterators$CloseRemoteIterator:close(),695,707,"/**
 * Closes the resource, preventing further use.
 * Closes superclass and associated resources.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,sourceHasNext,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceHasNext(),466,479,"/**
 * Checks if the source has next element; closes if not.
 * @throws IOException if an I/O error occurs
 * @return true if source has next, false otherwise
 */
","* Check for the source having a next element.
     * If it does not, this object's close() method
     * is called and false returned
     * @return true if there is a new value
     * @throws IOException failure to retrieve next value",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAutoCloseableReference.java,lazyAutoCloseablefromSupplier,org.apache.hadoop.util.functional.LazyAutoCloseableReference:lazyAutoCloseablefromSupplier(java.util.function.Supplier),99,101,"/**
 * Creates a LazyAutoCloseableReference from a supplier.
 * @param supplier Supplier that provides the AutoCloseable resource.
 * @return LazyAutoCloseableReference to the resource.
 */
","* Create from a supplier.
   * This is not a constructor to avoid ambiguity when a lambda-expression is
   * passed in.
   * @param supplier supplier implementation.
   * @return a lazy reference.
   * @param <T> type of reference",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,bulkDelete_pageSize,"org.apache.hadoop.io.wrappedio.WrappedIO:bulkDelete_pageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",72,79,"/**
 * Gets the page size for bulk delete operation.
 * @param fs FileSystem object
 * @param path Path for bulk delete
 * @return Page size for bulk delete
 */
","* Get the maximum number of objects/files to delete in a single request.
   * @param fs filesystem
   * @param path path to delete under.
   * @return a number greater than or equal to zero.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws IllegalArgumentException path not valid.
   * @throws UncheckedIOException if an IOE was raised.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,bulkDelete_delete,"org.apache.hadoop.io.wrappedio.WrappedIO:bulkDelete_delete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Collection)",104,113,"/**
 * Deletes multiple paths using a bulk delete operation.
 * @param fs FileSystem object
 * @param base Base path for deletion
 * @param paths Paths to delete
 * @return List of deleted entries
 */
","* Delete a list of files/objects.
   * <ul>
   *   <li>Files must be under the path provided in {@code base}.</li>
   *   <li>The size of the list must be equal to or less than the page size.</li>
   *   <li>Directories are not supported; the outcome of attempting to delete
   *       directories is undefined (ignored; undetected, listed as failures...).</li>
   *   <li>The operation is not atomic.</li>
   *   <li>The operation is treated as idempotent: network failures may
   *        trigger resubmission of the request -any new objects created under a
   *        path in the list may then be deleted.</li>
   *    <li>There is no guarantee that any parent directories exist after this call.
   *    </li>
   * </ul>
   * @param fs filesystem
   * @param base path to delete under.
   * @param paths list of paths which must be absolute and under the base path.
   * @return a list of all the paths which couldn't be deleted for a reason other
   *          than ""not found"" and any associated error message.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws UncheckedIOException if an IOE was raised.
   * @throws IllegalArgumentException if a path argument is invalid.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,fileSystem_openFile,"org.apache.hadoop.io.wrappedio.WrappedIO:fileSystem_openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.FileStatus,java.lang.Long,java.util.Map)",165,191,"/**
* Opens a file in the file system with specified options.
* @param fs filesystem, path, policy, status, length, options
* @return FSDataInputStream object
*/
","* OpenFile assistant, easy reflection-based access to
   * {@link FileSystem#openFile(Path)} and blocks
   * awaiting the operation completion.
   * @param fs filesystem
   * @param path path
   * @param policy read policy
   * @param status optional file status
   * @param length optional file length
   * @param options nullable map of other options
   * @return stream of the opened file
   * @throws UncheckedIOException if an IOE was raised.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,byteBufferPositionedReadable_readFully,"org.apache.hadoop.io.wrappedio.WrappedIO:byteBufferPositionedReadable_readFully(java.io.InputStream,long,java.nio.ByteBuffer)",213,224,"/**
 * Reads data from InputStream into ByteBuffer at specified position.
 * @param in Input stream.
 * @param position Read position.
 * @param buf Destination buffer.
 */
","* Delegate to {@link ByteBufferPositionedReadable#read(long, ByteBuffer)}.
   * @param in input stream
   * @param position position within file
   * @param buf the ByteBuffer to receive the results of the read operation.
   * Note: that is the default behaviour of {@link FSDataInputStream#readFully(long, ByteBuffer)}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_load,"org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",134,139,"/**
 * Loads an IOStatisticsSnapshot from the given file system path.
 * @param fs FileSystem to load from.
 * @param path Path to the snapshot file.
 * @return IOStatisticsSnapshot object.
 */
","* Load IOStatisticsSnapshot from a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @return the loaded snapshot
   * @throws UncheckedIOException Any IO exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_fromJsonString,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_fromJsonString(java.lang.String),192,196,"/**
 * Parses a JSON string into an IOStatisticsSnapshot object.
 * @param json The JSON string to parse.
 * @return An IOStatisticsSnapshot object.
 */
","* Load IOStatisticsSnapshot from a JSON string.
   * @param json JSON string value.
   * @return deserialized snapshot.
   * @throws UncheckedIOException Any IO/jackson exception.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/LazyAtomicReference.java,get,org.apache.hadoop.util.functional.LazyAtomicReference:get(),120,123,"/**
 * Retrieves the result of the evaluation, handling I/O exceptions.
 * @return The evaluated result of type T.
 * @throws UncheckedIOException if an I/O error occurs.
 */
","* Implementation of {@code Supplier.get()}.
   * <p>
   * Invoke {@link #eval()} and convert IOEs to
   * UncheckedIOException.
   * <p>
   * This is the {@code Supplier.get()} implementation, which allows
   * this class to passed into anything taking a supplier.
   * @return the value
   * @throws UncheckedIOException if the constructor raised an IOException.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,foreach,org.apache.hadoop.util.functional.TaskPool:foreach(java.lang.Iterable),581,583,"/**
 * Creates a Builder for processing items from an Iterable.
 * @param items The Iterable to process. Must not be null.
 * @return A Builder instance.
 */
","* Create a task builder for the iterable.
   * @param items item source.
   * @param <I> type of result.
   * @return builder.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,foreach,org.apache.hadoop.util.functional.TaskPool:foreach(java.lang.Object[]),595,597,"/**
 * Creates a Builder for iterating over an array of items.
 * @param items The array of items to iterate over.
 * @return A Builder object for the items.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,raiseInnerCause,org.apache.hadoop.fs.impl.FutureIOSupport:raiseInnerCause(java.util.concurrent.ExecutionException),106,110,"/**
 * Re-throws the inner cause of an ExecutionException.
 * Delegates to FutureIO.raiseInnerCause(e).
 */
","* From the inner cause of an execution exception, extract the inner cause
   * if it is an IOE or RTE.
   * See {@link FutureIO#raiseInnerCause(ExecutionException)}.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitFuture,org.apache.hadoop.util.functional.FutureIO:awaitFuture(java.util.concurrent.Future),97,110,"/**
 * Awaits the result of a Future, handling exceptions and re-throwing.
 * @param future The Future to await.
 * @return The result of the Future.
 */
","* Given a future, evaluate it.
   * <p>
   * Any exception generated in the future is
   * extracted and rethrown.
   * </p>
   * If this thread is interrupted while waiting for the future to complete,
   * an {@code InterruptedIOException} is raised.
   * However, if the future is cancelled, a {@code CancellationException}
   * is raised in the {code Future.get()} call. This is
   * passed up as is -so allowing the caller to distinguish between
   * thread interruption (such as when speculative task execution is aborted)
   * and future cancellation.
   * @param future future to evaluate
   * @param <T> type of the result.
   * @return the result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitFuture,"org.apache.hadoop.util.functional.FutureIO:awaitFuture(java.util.concurrent.Future,long,java.util.concurrent.TimeUnit)",129,145,"/**
 * Awaits the result of a Future with a timeout.
 * @param future Future to await.
 * @param timeout Timeout duration.
 * @param unit Timeout unit (e.g., SECONDS, MILLS)
 * @return Result of the Future.
 * @throws TimeoutException if timeout expires.
 */
","* Given a future, evaluate it.
   * <p>
   * Any exception generated in the future is
   * extracted and rethrown.
   * </p>
   * @param future future to evaluate
   * @param timeout timeout to wait.
   * @param unit time unit.
   * @param <T> type of the result.
   * @return the result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown
   * @throws TimeoutException the future timed out.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,raiseInnerCause,org.apache.hadoop.fs.impl.FutureIOSupport:raiseInnerCause(java.util.concurrent.CompletionException),123,127,"/**
 * Re-raises the inner cause of a CompletionException.
 * @param e The CompletionException to process.
 * @return The re-raised exception.
 */
","* Extract the cause of a completion failure and rethrow it if an IOE
   * or RTE.
   * See {@link FutureIO#raiseInnerCause(CompletionException)}.
   * @param e exception.
   * @param <T> type of return value.
   * @return nothing, ever.
   * @throws IOException either the inner IOException, or a wrapper around
   * any non-Runtime-Exception
   * @throws RuntimeException if that is the inner cause.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,setConf,"org.apache.hadoop.util.ReflectionUtils:setConf(java.lang.Object,org.apache.hadoop.conf.Configuration)",74,81,"/**
 * Sets the configuration for an object, if configurable.
 * @param theObject Object to set the configuration for.
 * @param conf Configuration object to be set.
 */
","* Check and set 'configuration' if necessary.
   * 
   * @param theObject object for which to set configuration
   * @param conf Configuration",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableName.java,getClass,"org.apache.hadoop.io.WritableName:getClass(java.lang.String,org.apache.hadoop.conf.Configuration)",91,103,"/**
 * Retrieves a Class object by name, using a configuration.
 * @param name Class name to find.
 * @param conf Configuration to use.
 * @throws IOException if class loading fails.
 */
","* Return the class for a name.
   * Default is {@link Class#forName(String)}.
   *
   * @param name input name.
   * @param conf input configuration.
   * @return class for a name.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createCodec,"org.apache.hadoop.io.erasurecode.CodecUtil:createCodec(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",232,254,"/**
 * Creates an ErasureCodec instance using the given class name and options.
 * @param conf Configuration object.
 * @param codecClassName Class name of the ErasureCodec.
 * @param options ErasureCodecOptions object.
 * @return Created ErasureCodec instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,loadClass,"org.apache.hadoop.io.ObjectWritable:loadClass(org.apache.hadoop.conf.Configuration,java.lang.String)",412,424,"/**
 * Loads a class by name, using Configuration if available.
 * @param conf Configuration object (can be null).
 * @param className Name of the class to load.
 * @return The loaded Class object.
 */
","* Find and load the class with given name <tt>className</tt> by first finding
   * it in the specified <tt>conf</tt>. If the specified <tt>conf</tt> is null,
   * try load it directly.
   *
   * @param conf configuration.
   * @param className classname.
   * @return Class.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getProtocolClass,"org.apache.hadoop.ipc.Server:getProtocolClass(java.lang.String,org.apache.hadoop.conf.Configuration)",331,339,"/**
 * Retrieves the protocol class by name, caching for efficiency.
 * @param protocolName Name of the protocol class.
 * @param conf Configuration object.
 * @return The protocol class.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,getClass,org.apache.hadoop.util.FindClass:getClass(java.lang.String),154,156,"/**
 * Retrieves a class by name from the configuration.
 * @param name Class name to retrieve.
 * @return The class object or throws ClassNotFoundException.
 */
","* Get a class fromt the configuration
   * @param name the class name
   * @return the class
   * @throws ClassNotFoundException if the class was not found
   * @throws Error on other classloading problems",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,logThreadInfo,"org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.apache.commons.logging.Log,java.lang.String,long)",232,254,"/**
 * Logs thread info if interval has passed, using provided log.
 * @param log Logger instance
 * @param title Log title
 * @param minInterval Minimum time interval in seconds
 */","* Log the current thread stacks at INFO level.
   * @param log the logger that logs the stack trace
   * @param title a descriptive title for the call stacks
   * @param minInterval the minimum time from the last
   * @deprecated to be removed with 3.4.0. Use {@link #logThreadInfo(Logger, String, long)} instead.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,logThreadInfo,"org.apache.hadoop.util.ReflectionUtils:logThreadInfo(org.slf4j.Logger,java.lang.String,long)",262,283,"/**
 * Logs thread information to the provided logger, throttled by minInterval.
 */","* Log the current thread stacks at INFO level.
   * @param log the logger that logs the stack trace
   * @param title a descriptive title for the call stacks
   * @param minInterval the minimum time from the last",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(java.util.Optional,java.util.Optional)",105,113,"/**
 * Constructs a builder, ensuring only path or pathHandle is provided.
 * @param optionalPath Optional path to use.
 * @param optionalPathHandle Optional path handle to use.
 */
","* Constructor with both optional path and path handle.
   * Either or both argument may be empty, but it is an error for
   * both to be defined.
   * @param optionalPath a path or empty
   * @param optionalPathHandle a path handle/empty
   * @throws IllegalArgumentException if both parameters are set.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration:<init>(),819,821,"/**
 * Default constructor. Initializes with default configuration.
 */
",A new configuration.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HttpExceptionUtils.java,validateResponse,"org.apache.hadoop.util.HttpExceptionUtils:validateResponse(java.net.HttpURLConnection,int)",143,191,"/**
 * Validates HTTP response status. Throws IOException if validation fails.
 * @param conn HttpURLConnection to validate
 * @param expectedStatus Expected HTTP status code
 */
","* Validates the status of an <code>HttpURLConnection</code> against an
   * expected HTTP status code. If the current status code is not the expected
   * one it throws an exception with a detail message using Server side error
   * messages if available.
   * <p>
   * <b>NOTE:</b> this method will throw the deserialized exception even if not
   * declared in the <code>throws</code> of the method signature.
   *
   * @param conn the <code>HttpURLConnection</code>.
   * @param expectedStatus the expected HTTP status code.
   * @throws IOException thrown if the current status code does not match the
   * expected one.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newCrc32C,org.apache.hadoop.util.DataChecksum:newCrc32C(),102,112,"/**
 * Creates a CRC32C checksum object, using Java9 if available.
 * Falls back to PureJavaCrc32C if Java9 fails.
 */
","* The flag is volatile to avoid synchronization here.
   * Re-entrancy is unlikely except in failure mode (and inexpensive).",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,removeAll,org.apache.hadoop.util.IntrusiveCollection:removeAll(java.util.Collection),361,370,"/**
 * Removes all elements from this collection if they are present.
 * @param collection collection of elements to remove
 * @return true if this collection was modified, false otherwise
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/IntrusiveCollection.java,toArray,org.apache.hadoop.util.IntrusiveCollection:toArray(java.lang.Object[]),266,278,"/**
 * Fills the provided array with elements from the collection.
 * @param array Array to fill; if too small, a new array is returned.
 * @return Array filled with collection elements.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroupsForUserCommand,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsForUserCommand(java.lang.String),143,145,"/**
 * Retrieves group names for a user via Shell.
 * @param userName The username to query.
 * @return String array of group names.
 */
","* Returns just the shell command to be used to fetch a user's groups list.
   * This is mainly separate to make some tests easier.
   * @param userName The username that needs to be passed into the command built
   * @return An appropriate shell command with arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroupsIDForUserCommand,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsIDForUserCommand(java.lang.String),164,166,"/**
 * Retrieves group IDs for a user via Shell command.
 * @param userName The username to fetch group IDs for.
 * @return String array of group IDs.
 */
","* Returns just the shell command to be used to fetch a user's group IDs list.
   * This is mainly separate to make some tests easier.
   * @param userName The username that needs to be passed into the command built
   * @return An appropriate shell command with arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getSetPermissionCommand,"org.apache.hadoop.util.Shell:getSetPermissionCommand(java.lang.String,boolean,java.lang.String)",311,318,"/**
 * Constructs a command to set permission for a file.
 * @param perm permission string (e.g., ""rwx"")
 * @param recursive whether to apply recursively
 * @param file file path
 * @return Command array
 */
","* Return a command to set permission for specific file.
   *
   * @param perm String permission to set
   * @param recursive boolean true to apply to all sub-directories recursively
   * @param file String file to set
   * @return String[] containing command and arguments",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getCheckProcessIsAliveCommand,org.apache.hadoop.util.Shell:getCheckProcessIsAliveCommand(java.lang.String),362,364,"/**
 * Returns command to check if process with given PID is alive.
 * @param pid Process ID to check.
 * @return Command array to check process status.
 */
","* Return a command for determining if process with specified pid is alive.
   * @param pid process ID
   * @return a <code>kill -0</code> command or equivalent",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getHadoopHome,org.apache.hadoop.util.Shell:getHadoopHome(),610,612,"/**
 * Returns the canonical path of the Hadoop home directory.
 */","* Get the Hadoop home directory. Raises an exception if not found
   * @return the home dir
   * @throws IOException if the home directory cannot be located.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getQualifiedBin,org.apache.hadoop.util.Shell:getQualifiedBin(java.lang.String),642,646,"/**
 * Gets the qualified Hadoop bin path for the given executable.
 * @param executable Name of the executable.
 * @return File object representing the executable's path.
 */
","*  Fully qualify the path to a binary that should be in a known hadoop
   *  bin location. This is primarily useful for disambiguating call-outs
   *  to executable sub-components of Hadoop to avoid clashes with other
   *  executables that may be in the path.  Caveat:  this call doesn't
   *  just format the path to the bin directory.  It also checks for file
   *  existence of the composed path. The output of this call should be
   *  cached by callers.
   *
   * @param executable executable
   * @return executable file reference
   * @throws FileNotFoundException if the path does not exist",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HardLink.java,linkCount,org.apache.hadoop.fs.HardLink$HardLinkCGWin:linkCount(java.io.File),137,146,"/**
 * Constructs the command array for link count retrieval.
 * @param file The file to get link count for.
 * @return String array representing the command.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell$ShellTimeoutTimerTask:<init>(org.apache.hadoop.util.Shell),1402,1404,"/**
 * Creates a ShellTimeoutTimerTask, associating it with a Shell object.
 * @param shell The Shell instance to associate with the task.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,org.apache.hadoop.util.Progress:addPhase(),71,77,"/**
 * Adds a new phase to the progress tracker.
 * Returns the newly created Progress phase object.
 */
","* Adds a node to the tree. Gives equal weightage to all phases.
   * @return Progress.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhases,org.apache.hadoop.util.Progress:addPhases(int),130,137,"/**
 * Adds specified number of phases and sets equal weightage.
 * @param n The number of phases to add.
 */
","* Adds n nodes to the tree. Gives equal weightage to all phases.
   *
   * @param n n.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,"org.apache.hadoop.util.Progress:addPhase(java.lang.String,float)",94,99,"/**
 * Adds a phase with the given status and weightage.
 * @param status Phase status string.
 * @param weightage Phase weightage (float).
 * @return The newly created Progress phase.
 */
","* Adds a named node with a specified progress weightage to the tree.
   *
   * @param status status.
   * @param weightage weightage.
   * @return Progress.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,get,org.apache.hadoop.util.Progress:get(),225,231,"/**
 * Gets the accumulated progress value from the root node.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,getProgress,org.apache.hadoop.util.Progress:getProgress(),239,241,"/**
 * Returns the progress value. Synchronized for thread safety.
 */","* Returns progress in this node. get() would give overall progress of the
   * root node(not just given current node).
   *
   * @return progress.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,toString,org.apache.hadoop.util.Progress:toString(),275,280,"/**
 * Returns a string representation of the object.
 * Uses a helper method to build the string.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,create,org.apache.hadoop.util.curator.ZKCuratorManager:create(java.lang.String),332,334,"/**
 * Creates a new resource at the given path.
 * @param path The path of the resource to create.
 * @return True if creation was successful, false otherwise.
 */
","* Create a ZNode.
   * @param path Path of the ZNode.
   * @return If the ZNode was created.
   * @throws Exception If it cannot contact Zookeeper.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,createRootDirRecursively,"org.apache.hadoop.util.curator.ZKCuratorManager:createRootDirRecursively(java.lang.String,java.util.List)",372,384,"/**
 * Creates root directory recursively based on the given path.
 * @param path The path to create.
 * @param zkAcl ACLs to apply to the created nodes.
 */
","* Utility function to ensure that the configured base znode exists.
   * This recursively creates the znode as well as all of its parents.
   * @param path Path of the znode to create.
   * @param zkAcl ACLs for ZooKeeper.
   * @throws Exception If it cannot create the file.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,ctorImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:ctorImpl(java.lang.String,java.lang.Class[])",364,378,"/**
 * Attempts to find and set the constructor implementation.
 * @param className Class name of the implementation.
 * @param argClasses Constructor argument classes.
 * @return This builder instance.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,newInstance,org.apache.hadoop.util.dynamic.DynConstructors$Ctor:newInstance(java.lang.Object[]),68,75,"/**
 * Creates a new instance of C, handling exceptions and re-throwing.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,invokeChecked,"org.apache.hadoop.util.dynamic.DynConstructors$Ctor:invokeChecked(java.lang.Object,java.lang.Object[])",84,89,"/**
 * Invokes a checked operation on a target, returning a result.
 * @param target The target object, must be null.
 * @param args Arguments to pass to the operation.
 * @return Result of the operation.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeChecked,org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:invokeChecked(java.lang.Object[]),215,217,"/**
 * Invokes the method with provided arguments, handling exceptions.
 * @param args Arguments to pass to the method.
 * @return Result of the method invocation.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invoke,"org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invoke(java.lang.Object,java.lang.Object[])",91,98,"/**
 * Invokes a method on a target object with provided arguments.
 * Handles exceptions and re-throws as RuntimeException.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeChecked,org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:invokeChecked(java.lang.Object[]),198,200,"/**
 * Invokes the method with given arguments, handling exceptions.
 * @param args Arguments to pass to the method.
 * @return Result of the method invocation.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.String,java.lang.String,java.lang.Class[])",284,298,"/**
 * Attempts to find and set a method implementation by class name.
 * @param className Class name to search for.
 * @param methodName Method name.
 * @param argClasses Argument classes of the method.
 * @return This builder instance.
 */
","* Checks for an implementation, first finding the given class by name.
     * @param className name of a class
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.Class,java.lang.Class[])",343,346,"/**
 * Sets the target class and argument classes for implementation.
 * @param targetClass Target class for the implementation.
 * @param argClasses Argument classes for the implementation.
 */
","* Checks for a method implementation.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param targetClass the class to check for an implementation
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.String,java.lang.String,java.lang.Class[])",387,401,"/**
 * Attempts to find a hidden implementation for a method.
 * @param className Class name to search.
 * @param methodName Method name.
 * @param argClasses Method argument classes.
 * @return Builder instance.
 */
","* Checks for an implementation, first finding the given class by name.
     * @param className name of a class
     * @param methodName name of a method (different from constructor)
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.Class,java.lang.Class[])",448,451,"/**
 * Calls hiddenImpl with targetClass, name, and argClasses.
 * @param targetClass The class to invoke.
 * @param argClasses Argument classes for the hidden method.
 * @return This builder instance.
 */
","* Checks for a method implementation.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param targetClass the class to check for an implementation
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,loadInvocation,"org.apache.hadoop.util.dynamic.BindingUtils:loadInvocation(java.lang.Class,java.lang.Class,java.lang.String,java.lang.Class[])",101,121,"/**
 * Loads an unbound method. Returns a noop method if source is null.
 * @param source Class object
 * @param returnType Class extending T
 * @param name Method name
 * @param parameterTypes Method parameter types
 * @return UnboundMethod object
 */
","* Get an invocation from the source class, which will be unavailable() if
   * the class is null or the method isn't found.
   *
   * @param <T> return type
   * @param source source. If null, the method is a no-op.
   * @param returnType return type class (unused)
   * @param name method name
   * @param parameterTypes parameters
   *
   * @return the method or ""unavailable""",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,requireAllMethodsAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:requireAllMethodsAvailable(),225,242,"/**
 * Verifies that all listed methods are available, throwing an exception if not.
 */
","* For testing: verify that all methods were found.
   * @throws UnsupportedOperationException if the method was not found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,bulkDelete_available,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_available(),249,251,"/**
 * Checks if bulk delete is available, using the configured method.
 */","* Are the bulk delete methods available?
   * @return true if the methods were found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,fileSystem_openFile_available,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:fileSystem_openFile_available(),306,308,"/**
 * Checks if the fileSystemOpenFile method is available.
 * Returns true if available, false otherwise.
 */
","* Is the {@link #fileSystem_openFile(FileSystem, Path, String, FileStatus, Long, Map)}
   * method available.
   * @return true if the optimized open file method can be invoked.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,byteBufferPositionedReadable_available,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_available(),380,382,"/**
 * Checks if the byte buffer is positioned and readable.
 * @return True if readable, false otherwise.
 */
","* Are the ByteBufferPositionedReadable methods loaded?
   * This does not check that a specific stream implements the API;
   * use {@link #byteBufferPositionedReadable_readFullyAvailable(InputStream)}.
   * @return true if the hadoop libraries have the method.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,byteBufferPositionedReadable_readFullyAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_readFullyAvailable(java.io.InputStream),392,400,"/**
 * Checks if the buffer is readable and attempts to read fully.
 * @param in InputStream to read from.
 * @return True if read fully, false otherwise.
 */
","* Probe to see if the input stream is an instance of ByteBufferPositionedReadable.
   * If the stream is an FSDataInputStream, the wrapped stream is checked.
   * @param in input stream
   * @return true if the API is available, the stream implements the interface
   * (including the innermost wrapped stream) and that it declares the stream capability.
   * @throws IOException if the operation was attempted and failed.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,ioStatisticsAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:ioStatisticsAvailable(),360,362,"/**
 * Checks if IO statistics snapshot creation method is available.
 */","* Are the core IOStatistics methods and classes available.
   * @return true if the relevant methods are loaded.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,ioStatisticsContextAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:ioStatisticsContextAvailable(),368,370,"/**
 * Checks if IO statistics context is available.
 * Returns true if enabled, false otherwise.
 */
","* Are the IOStatisticsContext methods and classes available?
   * @return true if the relevant methods are loaded.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,checkAvailable,org.apache.hadoop.util.dynamic.BindingUtils:checkAvailable(org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod),183,188,"/**
 * Checks if a method is available; throws exception if not.
 * @param method The unbound method to check.
 * @throws UnsupportedOperationException if method is unavailable.
 */
","* Require a method to be available.
   * @param method method to probe
   * @throws UnsupportedOperationException if the method was not found.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,buildChecked,org.apache.hadoop.util.dynamic.DynMethods$Builder:buildChecked(java.lang.Object),490,492,"/**
 * Builds a checked method and binds it to the receiver object.
 * @param receiver The object to bind the method to.
 * @return The bound method.
 */
","* Returns the first valid implementation as a BoundMethod or throws a
     * NoSuchMethodException if there is none.
     * @param receiver an Object to receive the method invocation
     * @return a {@link BoundMethod} with a valid implementation and receiver
     * @throws IllegalStateException if the method is static
     * @throws IllegalArgumentException if the receiver's class is incompatible
     * @throws NoSuchMethodException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,build,org.apache.hadoop.util.dynamic.DynMethods$Builder:build(java.lang.Object),503,505,"/**
 * Builds a bound method, binding it to the provided receiver object.
 * @param receiver The object to bind the method to.
 * @return The bound method.
 */
","* Returns the first valid implementation as a BoundMethod or throws a
     * RuntimeError if there is none.
     * @param receiver an Object to receive the method invocation
     * @return a {@link BoundMethod} with a valid implementation and receiver
     * @throws IllegalStateException if the method is static
     * @throws IllegalArgumentException if the receiver's class is incompatible
     * @throws RuntimeException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,buildStaticChecked,org.apache.hadoop.util.dynamic.DynMethods$Builder:buildStaticChecked(),514,516,"/**
 * Builds a static version of a checked method.
 * @return StaticMethod object representing the static method.
 */
","* Returns the first valid implementation as a StaticMethod or throws a
     * NoSuchMethodException if there is none.
     * @return a {@link StaticMethod} with a valid implementation
     * @throws IllegalStateException if the method is not static
     * @throws NoSuchMethodException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,buildStatic,org.apache.hadoop.util.dynamic.DynMethods$Builder:buildStatic(),525,527,"/**
 * Builds a static version of the object.
 * Returns a StaticMethod object.
 */
","* Returns the first valid implementation as a StaticMethod or throws a
     * RuntimeException if there is none.
     * @return a {@link StaticMethod} with a valid implementation
     * @throws IllegalStateException if the method is not static
     * @throws RuntimeException if no implementation was found",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,loadFileSystems,org.apache.hadoop.fs.FileSystem:loadFileSystems(),3516,3545,"/**
 * Loads available file systems using ServiceLoader, caching them.
 */","* Load the filesystem declarations from service resources.
   * This is a synchronized operation.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionInfo.java,main,org.apache.hadoop.util.VersionInfo:main(java.lang.String[]),182,193,"/**
 * Prints version information to the console.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,makeRpcRequestHeader,"org.apache.hadoop.util.ProtoUtil:makeRpcRequestHeader(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto$OperationProto,int,int,byte[])",171,176,"/**
 * Creates an RpcRequestHeaderProto.
 * @param rpcKind RPC kind, operation, callId, retryCount, uuid
 * @return RpcRequestHeaderProto object
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,parseVersion,org.apache.hadoop.util.ComparableVersion:parseVersion(java.lang.String),360,453,"/**
 * Parses a version string into a structured representation.
 * @param version The version string to parse.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,<init>,"org.apache.hadoop.util.LightWeightCache:<init>(int,int,long,long,org.apache.hadoop.util.Timer)",120,145,"/**
 * Constructs a LightWeightCache with specified parameters.
 * @param recommendedLength, sizeLimit, expiration periods, timer
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,get,org.apache.hadoop.util.LightWeightResizableGSet:get(java.lang.Object),98,101,"/**
* Retrieves the value associated with the given key.
* @param key the key whose value is to be retrieved
* @return The value associated with the key, or null if absent.
*/
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,contains,org.apache.hadoop.util.LightWeightGSet:contains(java.lang.Object),144,147,"/**
 * Checks if the map contains the specified key.
 * @param key The key to check for.
 * @return True if the key exists in the map, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,toString,org.apache.hadoop.util.BlockingThreadPoolExecutorService:toString(),158,166,"/**
 * Returns a string representation of this BlockingThreadPoolExecutorService.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readFileToMapWithFileInputStream,"org.apache.hadoop.util.HostsFileReader:readFileToMapWithFileInputStream(java.lang.String,java.lang.String,java.io.InputStream,java.util.Map)",130,144,"/**
 * Reads a file into a map, handling XML or text files.
 * @param type File type, filename, inputStream, map to populate.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/JenkinsHash.java,main,org.apache.hadoop.util.hash.JenkinsHash:main(java.lang.String[]),252,266,"/**
 * Calculates and prints the Jenkins hash of a file.
 * @param args Command line arguments: filename
 */
","* Compute the hash of the specified file
   * @param args name of file to compute hash of.
   * @throws IOException raised on errors performing I/O.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/HashFunction.java,<init>,"org.apache.hadoop.util.bloom.HashFunction:<init>(int,int,int)",83,97,"/**
 * Initializes a HashFunction with max value, number of hashes, and hash type.
 * @param maxValue The maximum value for hashing.
 * @param nbHash The number of hash functions.
 * @param hashType The type of hash function to use.
 */
","* Constructor.
   * <p>
   * Builds a hash function that must obey to a given maximum number of returned values and a highest value.
   * @param maxValue The maximum highest returned value.
   * @param nbHash The number of resulting hashed values.
   * @param hashType type of the hashing function (see {@link Hash}).",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,<init>,org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>(),102,102,"/**
 * Constructs a new RetouchedBloomFilter instance.
 */
",Default constructor - use with readFields,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,write,org.apache.hadoop.util.bloom.DynamicBloomFilter:write(java.io.DataOutput),248,257,"/**
 * Writes the object to the DataOutput.
 * Writes nr, currentNbRecord, matrix length, and matrix data.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,write,org.apache.hadoop.util.bloom.RetouchedBloomFilter:write(java.io.DataOutput),407,427,"/**
 * Writes the fingerprint and key data to the output stream.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,add,org.apache.hadoop.util.bloom.CountingBloomFilter:add(org.apache.hadoop.util.bloom.Key),104,127,"/**
 * Adds a key to the hash structure. Throws NullPointerException if key is null.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,membershipTest,org.apache.hadoop.util.bloom.CountingBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key),178,200,"/**
 * Checks if a key is present in the hash structure.
 * @param key The key to check for membership.
 * @return True if the key is present, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,approximateCount,org.apache.hadoop.util.bloom.CountingBloomFilter:approximateCount(org.apache.hadoop.util.bloom.Key),220,238,"/**
 * Estimates the count associated with a key using multiple hash functions.
 * @param key The key to estimate the count for.
 * @return Approximate count value or 0 if no count found.
 */
","* This method calculates an approximate count of the key, i.e. how many
   * times the key was added to the filter. This allows the filter to be
   * used as an approximate <code>key -&gt; count</code> map.
   * <p>NOTE: due to the bucket size of this filter, inserting the same
   * key more than 15 times will cause an overflow at all filter positions
   * associated with this key, and it will significantly increase the error
   * rate for this and other keys. For this reason the filter can only be
   * used to store small count values <code>0 &lt;= N &lt;&lt; 15</code>.
   * @param key key to be tested
   * @return 0 if the key is not present. Otherwise, a positive value v will
   * be returned such that <code>v == count</code> with probability equal to the
   * error rate of this filter, and <code>v &gt; count</code> otherwise.
   * Additionally, if the filter experienced an underflow as a result of
   * {@link #delete(Key)} operation, the return value may be lower than the
   * <code>count</code> with the probability of the false negative rate of such
   * filter.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,add,org.apache.hadoop.util.bloom.BloomFilter:add(org.apache.hadoop.util.bloom.Key),116,128,"/**
 * Adds a key to the hash.
 * @param key The key to add. Throws NullPointerException if null.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,membershipTest,org.apache.hadoop.util.bloom.BloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key),142,156,"/**
 * Checks if a key is present in the Bloom filter.
 * @param key The key to check. Throws NullPointerException if null.
 * @return True if the key is likely present, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,add,org.apache.hadoop.util.bloom.RetouchedBloomFilter:add(org.apache.hadoop.util.bloom.Key),118,131,"/**
 * Adds a key to the hash table.
 * @param key The key to add. Throws NullPointerException if null.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(org.apache.hadoop.util.bloom.Key),139,150,"/**
 * Adds a key to false positive vectors.
 * @param key The key to add; throws NullPointerException if null.
 */
","* Adds a false positive information to <i>this</i> retouched Bloom filter.
   * <p>
   * <b>Invariant</b>: if the false positive is <code>null</code>, nothing happens.
   * @param key The false positive key to add.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,removeKey,"org.apache.hadoop.util.bloom.RetouchedBloomFilter:removeKey(org.apache.hadoop.util.bloom.Key,java.util.List[])",351,365,"/**
 * Removes a key from all hash buckets in the vector.
 * @param k The key to remove.
 * @param vector Array of lists containing keys.
 */
","* Removes a given key from <i>this</i> filer.
   * @param k The key to remove.
   * @param vector The counting vector associated to the key.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Key.java,equals,org.apache.hadoop.util.bloom.Key:equals(java.lang.Object),137,143,"/**
 * Checks if this key is equal to another key.
 * @param o the object to compare to
 * @return true if keys are equal, false otherwise
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,minimumFnRemove,org.apache.hadoop.util.bloom.RetouchedBloomFilter:minimumFnRemove(int[]),250,265,"/**
 * Finds the index of the element with the minimum key weight.
 * @param h array of hash indices
 * @return Index of the element with the minimum key weight
 */","* Chooses the bit position that minimizes the number of false negative generated.
   * @param h The different bit positions.
   * @return The position that minimizes the number of false negative generated.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,maximumFpRemove,org.apache.hadoop.util.bloom.RetouchedBloomFilter:maximumFpRemove(int[]),272,286,"/**
 * Finds the index of the maximum weight feature in the hash list.
 * @param h array of hash indices
 * @return Index of the feature with the maximum weight.
 */
","* Chooses the bit position that maximizes the number of false positive removed.
   * @param h The different bit positions.
   * @return The position that maximizes the number of false positive removed.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,computeRatio,org.apache.hadoop.util.bloom.RetouchedBloomFilter:computeRatio(),370,379,"/**
 * Calculates the ratio of keyWeight to fpWeight for each element.
 */",* Computes the ratio A/FP.,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,dumpResource,org.apache.hadoop.util.FindClass:dumpResource(java.lang.String),187,209,"/**
 * Dumps resource content to stdout.
 * @param name Resource name to dump.
 * @return SUCCESS or error code if failed.
 */
","* Dump a resource to out
   * @param name resource name
   * @return the status code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,usage,org.apache.hadoop.util.FindClass:usage(java.lang.String[]),342,361,"/**
 * Prints usage instructions and returns E_USAGE.
 * Explains available commands and return codes.
 */
","* Print a usage message
   * @param args the command line arguments
   * @return an exit code",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GcTimeMonitor.java,run,org.apache.hadoop.util.GcTimeMonitor:run(),153,172,"/**
 * Runs the GC monitoring loop, calculating and potentially alerting.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,put,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:put(org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor),3510,3520,"/**
 * Adds a segment to the merged stream.
 * Checks compression settings for consistency.
 * @param stream The segment descriptor to add.
 * @throws IOException if compression settings are inconsistent.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/PriorityQueue.java,insert,org.apache.hadoop.util.PriorityQueue:insert(java.lang.Object),73,85,"/**
 * Inserts an element into the heap. Returns true if successful.
 */","* Adds element to the PriorityQueue in log(size) time if either
   * the PriorityQueue is not full, or not lessThan(element, top()).
   * @param element element.
   * @return true if element is added, false otherwise.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newTreeSet,org.apache.hadoop.util.Sets:newTreeSet(java.lang.Iterable),154,159,"/**
 * Creates a new TreeSet from an Iterable of elements.
 * @param elements Iterable containing elements to add to the set.
 * @return A new TreeSet containing the provided elements.
 */
","* Creates a <i>mutable</i> {@code TreeSet} instance containing the given
   * elements sorted by their natural ordering.
   *
   * <p><b>Note:</b> if mutability is not required, use
   * ImmutableSortedSet#copyOf(Iterable) instead.
   *
   * <p><b>Note:</b> If {@code elements} is a {@code SortedSet} with an
   * explicit comparator, this method has different behavior than
   * {@link TreeSet#TreeSet(SortedSet)}, which returns a {@code TreeSet}
   * with that comparator.
   *
   * <p><b>Note for Java 7 and later:</b> this method is now unnecessary and
   * should be treated as deprecated. Instead, use the {@code TreeSet}
   * constructor directly, taking advantage of the new
   * <a href=""http://goo.gl/iz2Wi"">""diamond"" syntax</a>.
   *
   * <p>This method is just a small convenience for creating an empty set and
   * then calling Iterables#addAll. This method is not very useful and will
   * likely be deprecated in the future.
   *
   * @param <E> Generics Type E.
   * @param elements the elements that the set should contain
   * @return a new {@code TreeSet} containing those elements (minus duplicates)",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSet,org.apache.hadoop.util.Sets:newHashSet(java.lang.Iterable),123,127,"/**
 * Creates a new HashSet from an Iterable or Iterator.
 * @param elements Iterable to populate the HashSet.
 */
","* Creates a <i>mutable</i> {@code HashSet} instance containing the given
   * elements. A very thin convenience for creating an empty set then calling
   * {@link Collection#addAll} or Iterables#addAll.
   *
   * <p><b>Note:</b> if mutability is not required and the elements are
   * non-null, use ImmutableSet#copyOf(Iterable) instead. (Or, change
   * {@code elements} to be a FluentIterable and call {@code elements.toSet()}.)</p>
   *
   * <p><b>Note:</b> if {@code E} is an {@link Enum} type, use
   * newEnumSet(Iterable, Class) instead.</p>
   *
   * @param <E> Generics Type E.
   * @param elements the elements that the set should contain.
   * @return a new, empty thread-safe {@code Set}.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Sets.java,newHashSet,org.apache.hadoop.util.Sets:newHashSet(java.lang.Object[]),100,105,"/**
 * Creates a HashSet from a variable number of elements.
 * @param elements Elements to add to the HashSet.
 * @return A new HashSet containing the provided elements.
 */
","* Creates a <i>mutable</i> {@code HashSet} instance initially containing
   * the given elements.
   *
   * <p><b>Note:</b> if elements are non-null and won't be added or removed
   * after this point, use ImmutableSet#of() or ImmutableSet#copyOf(Object[])
   * instead. If {@code E} is an {@link Enum} type, use
   * {@link EnumSet#of(Enum, Enum[])} instead. Otherwise, strongly consider
   * using a {@code LinkedHashSet} instead, at the cost of increased memory
   * footprint, to get deterministic iteration behavior.</p>
   *
   * <p>This method is just a small convenience, either for
   * {@code newHashSet(}{@link Arrays#asList}{@code (...))}, or for creating an
   * empty set then calling {@link Collections#addAll}.</p>
   *
   * @param <E> Generics Type E.
   * @param elements the elements that the set should contain.
   * @return a new, empty thread-safe {@code Set}",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,run,org.apache.hadoop.util.ProgramDriver:run(java.lang.String[]),120,146,"/**
 * Runs the specified program with given arguments.
 * @param args Command-line arguments, program name is first.
 * @return 0 on success, -1 on failure.
 */
","* This is a driver for the example programs.
   * It looks at the first command line argument and tries to find an
   * example program with that name.
   * If it is found, it calls the main method in that class with the rest 
   * of the command line arguments.
   * @param args The argument from the user. args[0] is the command to run.
   * @return -1 on error, 0 on success
   * @throws NoSuchMethodException  when a particular method cannot be found.
   * @throws SecurityException security manager to indicate a security violation.
   * @throws IllegalAccessException for backward compatibility.
   * @throws IllegalArgumentException if the arg is invalid.
   * @throws Throwable Anything thrown by the example program's main",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,"org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,org.apache.hadoop.tools.TableListing$Justification,boolean)",151,155,"/**
 * Adds a column with title, justification, and wrap flag.
 * @param title Column title.
 * @param justification Text justification.
 * @param wrap Wrap text or not.
 * @return This builder instance.
 */
","* Add a new field to the Table under construction.
     *
     * @param title Field title.
     * @param justification Right or left justification. Defaults to left.
     * @param wrap Width at which to auto-wrap the content of the cell.
     *        Defaults to Integer.MAX_VALUE.
     * @return This Builder object",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,logDeprecationOnce,"org.apache.hadoop.conf.Configuration:logDeprecationOnce(java.lang.String,java.lang.String)",1465,1470,"/**
 * Logs a deprecation warning once for a given name and source.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDurationHelper,"org.apache.hadoop.conf.Configuration:getTimeDurationHelper(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)",1936,1938,"/**
 * Delegates to the overloaded method with default unit.
 * @param name Identifier name.
 * @param vStr Value string.
 * @param unit Time unit.
 */
","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d).
   *
   * @param name Property name
   * @param vStr The string value with time unit suffix to be converted.
   * @param unit Unit to convert the stored property, if it exists.
   * @return time duration in given time unit.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStreamReader,"org.apache.hadoop.conf.Configuration:getStreamReader(org.apache.hadoop.conf.Configuration$Resource,boolean)",3149,3177,"/**
 * Retrieves an XMLStreamReader2 from a resource wrapper.
 * @param wrapper Resource wrapper object.
 * @param quiet  Quiet mode flag.
 * @return XMLStreamReader2 or null if parsing fails.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleStartElement,org.apache.hadoop.conf.Configuration$Parser:handleStartElement(),3240,3265,"/**
 * Handles start element events, routing to appropriate handlers.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,appendXMLProperty,"org.apache.hadoop.conf.Configuration:appendXMLProperty(org.w3c.dom.Document,org.w3c.dom.Element,java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",3695,3733,"/**
 * Appends an XML property to the document, redacting if needed.
 * @param doc XML document, conf Element, propertyName, redactor
 */
","*  Append a property with its attributes to a given {#link Document}
   *  if the property is found in configuration.
   *
   * @param doc
   * @param conf
   * @param propertyName",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecations,org.apache.hadoop.conf.Configuration:addDeprecations(org.apache.hadoop.conf.Configuration$DeprecationDelta[]),566,572,"/**
 * Atomically applies deprecation deltas to the deprecation context.
 */
","* Adds a set of deprecated keys to the global deprecations.
   *
   * This method is lockless.  It works by means of creating a new
   * DeprecationContext based on the old one, and then atomically swapping in
   * the new context.  If someone else updated the context in between us reading
   * the old context and swapping in the new one, we try again until we win the
   * race.
   *
   * @param deltas   The deprecations to add.",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleEndElement,org.apache.hadoop.conf.Configuration$Parser:handleEndElement(),3374,3413,"/**
 * Handles the end of an XML element, processing its value.
 */",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,bindForPortRange,"org.apache.hadoop.http.HttpServer2:bindForPortRange(org.eclipse.jetty.server.ServerConnector,int)",1502,1531,"/**
 * Attempts to bind the listener to a port within the defined range.
 */","* Bind using port ranges. Keep on looking for a free port in the port range
   * and throw a bind exception if no port in the configured range binds.
   * @param listener jetty listener.
   * @param startPort initial port which is set in the listener.
   * @throws Exception",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,fatalError,org.apache.hadoop.ha.ActiveStandbyElector:fatalError(java.lang.String),768,772,"/**
 * Logs error, resets state, and notifies client of a fatal error.
 * @param errorMessage The error message to log and notify.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,transitionToActive,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToActive(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),89,95,"/**
 * Transitions to the active state using the provided request info.
 * @param reqInfo StateChangeRequestInfo object.
 * @throws IOException if RPC call fails.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,transitionToStandby,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToStandby(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),97,103,"/**
 * Transitions the device to standby mode.
 * @param reqInfo StateChangeRequestInfo object.
 * @throws IOException if RPC call fails.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,transitionToObserver,org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:transitionToObserver(org.apache.hadoop.ha.HAServiceProtocol$StateChangeRequestInfo),105,112,"/**
 * Transitions to observer state using provided request info.
 * @param reqInfo StateChangeRequestInfo object
 * @throws IOException if RPC call fails
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,transitionToActive,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToActive(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToActiveRequestProto)",107,117,"/**
 * Transitions a resource to active state and returns a response.
 * @param controller RPC controller; @param request Transition request
 * @return TransitionToActiveResponseProto object
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,transitionToStandby,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToStandby(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToStandbyRequestProto)",119,129,"/**
 * Transitions the server to standby.
 * @param controller RPC controller
 * @param request TransitionToStandbyRequestProto
 * @return TransitionToStandbyResponseProto
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,transitionToObserver,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:transitionToObserver(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ha.proto.HAServiceProtocolProtos$TransitionToObserverRequestProto)",131,141,"/**
 * Transitions to observer state.
 * Converts request, calls server, returns response or throws exception.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,<init>,org.apache.hadoop.ha.SshFenceByTcpPort$Args:<init>(java.lang.String),237,256,"/**
 * Parses arguments to configure user and SSH port.
 * @param arg Argument string containing user and port (optional).
 * @throws BadFencingConfigurationException if parsing fails.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,"org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream,java.lang.String)",151,153,"/**
 * Prints command usage information to a PrintStream.
 * @param pStr PrintStream to output usage to
 * @param cmd Command name being used
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,parseOpts,"org.apache.hadoop.ha.HAAdmin:parseOpts(java.lang.String,org.apache.commons.cli.Options,java.lang.String[],java.util.Map)",491,503,"/**
 * Parses command-line arguments using GnuParser.
 * @param cmdName Command name, @param opts Options to parse,
 * @param argv Arguments, @param helpEntries Help entries.
 * @return Parsed CommandLine or null on error.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,doFence,"org.apache.hadoop.ha.SshFenceByTcpPort:doFence(com.jcraft.jsch.Session,java.net.InetSocketAddress)",130,171,"/**
 * Attempts to fence a service by killing processes on a port.
 * @param session SSH session
 * @param serviceAddr Service address (host and port)
 * @return True if fencing was successful, false otherwise.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,addTargetInfoAsEnvVars,"org.apache.hadoop.ha.ShellCommandFencer:addTargetInfoAsEnvVars(org.apache.hadoop.ha.HAServiceTarget,java.util.Map)",220,243,"/**
 * Adds target fencing parameters to the environment variables map.
 * Uses TARGET_PREFIX for active/null state, SOURCE_PREFIX otherwise.
 */","* Add information about the target to the the environment of the
   * subprocess.
   * 
   * @param target
   * @param environment",,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,setAclsWithRetries,org.apache.hadoop.ha.ActiveStandbyElector:setAclsWithRetries(java.lang.String),1124,1138,"/**
 * Sets ACL for a given path, retrying on version mismatch.
 * @param path ZNode path to set ACL for.
 * @throws KeeperException, InterruptedException on ZooKeeper error.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,zkDoWithRetries,org.apache.hadoop.ha.ActiveStandbyElector:zkDoWithRetries(org.apache.hadoop.ha.ActiveStandbyElector$ZKAction),1140,1143,"/**
 * Executes a ZK action with retries.
 * @param action ZK action to execute; returns result of type T.
 * @return Result of type T from the ZK action.
 */
",,,,True,3
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readNonByteBufferPositionedReadable,"org.apache.hadoop.fs.VectoredReadUtils:readNonByteBufferPositionedReadable(org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.FileRange,java.nio.ByteBuffer)",151,170,"/**
 * Reads data from a positioned readable stream into a ByteBuffer.
 * Uses direct buffer optimization if available.
 */
","* Read into a direct tor indirect buffer using {@code PositionedReadable.readFully()}.
   * @param stream stream
   * @param range file range
   * @param buffer destination buffer
   * @throws IOException IO problems.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,validateVectoredReadRanges,org.apache.hadoop.fs.VectoredReadUtils:validateVectoredReadRanges(java.util.List),82,85,"/**
 * Validates and sorts a list of file ranges.
 * @param ranges List of FileRange objects to validate.
 * @throws EOFException if validation fails.
 */
","* Validate a list of vectored read ranges.
   * @param ranges list of ranges.
   * @throws EOFException any EOF exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setCaching,org.apache.hadoop.fs.impl.prefetch.BufferData:setCaching(java.util.concurrent.Future),195,201,"/**
 * Sets the caching action future.
 * @param actionFuture Future representing the caching operation.
 */
","* Indicates that a caching operation is in progress.
   *
   * @param actionFuture the {@code Future} of a caching action.
   *
   * @throws IllegalArgumentException if actionFuture is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,updateState,"org.apache.hadoop.fs.impl.prefetch.BufferData:updateState(org.apache.hadoop.fs.impl.prefetch.BufferData$State,org.apache.hadoop.fs.impl.prefetch.BufferData$State[])",243,250,"/**
 * Updates the state to the new state, validating expected current state.
 */
","* Updates the current state to the specified value.
   * Asserts that the current state is as expected.
   * @param newState the state to transition to.
   * @param expectedCurrentState the collection of states from which
   *        transition to {@code newState} is allowed.
   *
   * @throws IllegalArgumentException if newState is null.
   * @throws IllegalArgumentException if expectedCurrentState is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPathExistsAsDir,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsDir(java.nio.file.Path,java.lang.String)",357,364,"/**
 * Checks if the given path exists and is a directory.
 * @param path Path to check.
 * @param argName Argument name for error message.
 */
","* Validates that the given path exists and is a directory.
   * @param path the path to check.
   * @param argName the name of the argument being validated.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/Validate.java,checkPathExistsAsFile,"org.apache.hadoop.fs.impl.prefetch.Validate:checkPathExistsAsFile(java.nio.file.Path,java.lang.String)",371,375,"/**
 * Checks if the path exists and is a file.
 * @param path The path to check.
 * @param argName Name of the argument being validated.
 */
","* Validates that the given path exists and is a file.
   * @param path the path to check.
   * @param argName the name of the argument being validated.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,isLastBlock,org.apache.hadoop.fs.impl.prefetch.BlockData:isLastBlock(int),127,135,"/**
 * Checks if a given block is the last block in the file.
 * @param blockNumber The block number to check.
 * @return True if the block is the last one, false otherwise.
 */
","* Indicates whether the given block is the last block in the associated file.
   * @param blockNumber the id of the desired block.
   * @return true if the given block is the last block in the associated file, false otherwise.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getStartOffset,org.apache.hadoop.fs.impl.prefetch.BlockData:getStartOffset(int),181,185,"/**
 * Calculates the start offset for a given block number.
 * @param blockNumber The block number to calculate offset for.
 * @return The start offset in bytes.
 */
","* Gets the start offset of the given block.
   * @param blockNumber the id of the given block.
   * @return the start offset of the given block.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getState,org.apache.hadoop.fs.impl.prefetch.BlockData:getState(int),206,210,"/**
 * Retrieves the state of a block.
 * @param blockNumber The block's index.
 * @return The state of the specified block.
 */
","* Gets the state of the given block.
   * @param blockNumber the id of the given block.
   * @return the state of the given block.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,setState,"org.apache.hadoop.fs.impl.prefetch.BlockData:setState(int,org.apache.hadoop.fs.impl.prefetch.BlockData$State)",218,222,"/**
 * Sets the state of a block.
 * @param blockNumber Block index to update.
 * @param blockState The new state for the block.
 */
","* Sets the state of the given block to the given value.
   * @param blockNumber the id of the given block.
   * @param blockState the target state.
   * @throws IllegalArgumentException if blockNumber is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getBlockNumber,org.apache.hadoop.fs.impl.prefetch.BlockData:getBlockNumber(long),143,147,"/**
 * Calculates the block number given an offset.
 * @param offset The offset into the data.
 * @return The corresponding block number.
 */
","* Gets the id of the block that contains the given absolute offset.
   * @param offset the absolute offset to check.
   * @return the id of the block that contains the given absolute offset.
   * @throws IllegalArgumentException if offset is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_aggregate,"org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_aggregate(java.io.Serializable,java.lang.Object)",94,107,"/**
 * Aggregates IOStatistics to a snapshot.
 * @param snapshot The snapshot to aggregate to.
 * @param statistics The IOStatistics to aggregate. Returns true on success.
 */
","* Aggregate an existing {@link IOStatisticsSnapshot} with
   * the supplied statistics.
   * @param snapshot snapshot to update
   * @param statistics IOStatistics to add
   * @return true if the snapshot was updated.
   * @throws IllegalArgumentException if the {@code statistics} argument is not
   * null but not an instance of IOStatistics, or if  {@code snapshot} is invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_save,"org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_save(java.io.Serializable,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",162,171,"/**
 * Saves an IO statistics snapshot to a file system path.
 * @param snapshot Snapshot data, FS, path, overwrite flag.
 */
","* Save IOStatisticsSnapshot to a Hadoop filesystem as a JSON file.
   * @param snapshot statistics
   * @param fs filesystem
   * @param path path
   * @param overwrite should any existing file be overwritten?
   * @throws UncheckedIOException Any IO exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_counters,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_counters(java.io.Serializable),203,206,"/**
 * Retrieves IO statistics counters from a source.
 * @param source The source of IO statistics.
 * @return A map of counter names to their values.
 */
","* Get the counters of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of counters.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_gauges,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_gauges(java.io.Serializable),213,216,"/**
 * Retrieves gauges from an IOStatisticsSnapshot.
 * @param source Source object containing the snapshot.
 * @return Map of gauge names to their values.
 */
","* Get the gauges of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of gauges.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_minimums,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_minimums(java.io.Serializable),223,226,"/**
 * Retrieves minimum IO statistics from a snapshot.
 * @param source Serializable snapshot source.
 * @return Map of minimum IO statistics.
 */
","* Get the minimums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of minimums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_maximums,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_maximums(java.io.Serializable),233,236,"/**
 * Retrieves maximum IO statistics from a source.
 * @param source Serializable object containing IO statistics.
 * @return Map of statistic names to their maximum values.
 */
","* Get the maximums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of maximums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_means,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_means(java.io.Serializable),245,253,"/**
 * Calculates mean statistics from a snapshot.
 * @param source IOStatisticsSnapshot source
 * @return Map of statistic names to (samples, sum) pairs.
 */
","* Get the means of an IOStatisticsSnapshot.
   * Each value in the map is the (sample, sum) tuple of the values;
   * the mean is then calculated by dividing sum/sample wherever sample count is non-zero.
   * @param source source of statistics.
   * @return a map of mean key to (sample, sum) tuples.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,copy,org.apache.hadoop.fs.impl.FlagSet:copy(),253,255,"/**
 * Creates a copy of this FlagSet with the same flags.
 * @return A new FlagSet instance.
 */
","* Create a copy of the FlagSet.
   * @return a new mutable instance with a separate copy of the flags",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,createFlagSet,"org.apache.hadoop.fs.impl.FlagSet:createFlagSet(java.lang.Class,java.lang.String,java.util.EnumSet)",276,281,"/**
 * Creates a FlagSet instance with the given enum class, prefix, and flags.
 */
","* Create a FlagSet.
   * @param enumClass class of enum
   * @param prefix prefix (with trailing ""."") for path capabilities probe
   * @param flags flags
   * @param <E> enum type
   * @return a mutable FlagSet",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,close,org.apache.hadoop.fs.HarFileSystem:close(),733,744,"/**
 * Closes this stream and any underlying FileSystem object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,close,org.apache.hadoop.fs.RawLocalFileSystem:close(),895,898,"/**
 * Closes this stream and releases associated resources.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,closeAll,org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:closeAll(),155,163,"/**
 * Closes all FileSystem instances managed by the map.
 * Handles potential IOExceptions during closing.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,close,org.apache.hadoop.fs.FsShell:close(),371,376,"/**
 * Closes the file system, releasing resources.
 * Closes the underlying 'fs' if it exists.
 */
","*  Performs any necessary cleanup
   * @throws IOException upon error",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,close,org.apache.hadoop.fs.FilterFileSystem:close(),527,531,"/**
 * Closes this stream and the underlying file system.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem$Cache:closeAll(boolean),3799,3831,"/**
 * Closes all FileSystem resources, optionally only automatic ones.
 * @param onlyAutomatic if true, closes only automatically registered resources.
 * @throws IOException if any of the close operations fail.
 */
","* Close all FileSystem instances in the Cache.
     * @param onlyAutomatic only close those that are marked for automatic closing
     * @throws IOException a problem arose closing one or more FileSystem.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem$Cache:closeAll(org.apache.hadoop.security.UserGroupInformation),3844,3868,"/**
 * Closes FileSystems associated with the given UserGroupInformation.
 * @param ugi UserGroupInformation to filter FileSystems by.
 * @throws IOException if any FileSystem close operation fails.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,compareTo,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:compareTo(org.apache.hadoop.fs.FileStatus),117,120,"/**
 * Compares this file status with another. Delegates to super class.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,readFully,"org.apache.hadoop.fs.FSInputStream:readFully(long,byte[],int,int)",118,133,"/**
 * Reads data into the buffer from the specified position.
 * @param position starting position
 * @param buffer buffer to fill
 * @param offset offset in the buffer
 * @param length number of bytes to read
 * @throws IOException if an I/O error occurs
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,read,"org.apache.hadoop.fs.BufferedFSInputStream:read(long,byte[],int,int)",115,118,"/**
 * Reads data from the stream at a specified position.
 * @param position offset from the beginning of the stream
 * @param buffer buffer to store read data
 * @param offset offset within the buffer
 * @param length number of bytes to read
 * @return number of bytes read
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_toJsonString,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_toJsonString(java.io.Serializable),180,184,"/**
 * Converts an IOStatisticsSnapshot to a JSON string.
 * @param snapshot The snapshot to convert; null allowed.
 * @return JSON string representation of the snapshot.
 */
","* Save IOStatisticsSnapshot to a JSON string.
   * @param snapshot statistics; may be null or of an incompatible type
   * @return JSON string value
   * @throws UncheckedIOException Any IO/jackson exception.
   * @throws IllegalArgumentException if the supplied class is not a snapshot",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,byte[])",1878,1890,"/**
 * Writes byte array to a file in the file system.
 * @param fileContext Hadoop FileContext
 * @param path Path to write to
 * @param bytes Data to write
 * @return FileContext
 */
","* Writes bytes to a file. This utility method opens the file for writing,
   * creating the file if it does not exist, or overwrites an existing file. All
   * bytes in the byte array are written to the file.
   *
   * @param fileContext the file context with which to create the file
   * @param path the path to the file
   * @param bytes the byte array with the bytes to write
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)",1948,1966,"/**
 * Writes lines to a file in the file system.
 * @param fileContext Hadoop FileContext
 * @param path Path to write to
 * @param lines Lines to write
 * @param cs Charset to use
 * @return FileContext
 */
","* Write lines of text to a file. Each line is a char sequence and is written
   * to the file in sequence with each line terminated by the platform's line
   * separator, as defined by the system property {@code
   * line.separator}. Characters are encoded into bytes using the specified
   * charset. This utility method opens the file for writing, creating the file
   * if it does not exist, or overwrites an existing file.
   *
   * @param fileContext the file context with which to create the file
   * @param path the path to the file
   * @param lines a Collection to iterate over the char sequences
   * @param cs the charset to use for encoding
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence,java.nio.charset.Charset)",2014,2028,"/**
 * Writes a CharSequence to a file using the given Charset.
 * @param fs FileContext, the file system context
 * @param path Path, the file path
 * @param charseq CharSequence to write
 * @param cs Charset, character encoding
 * @return FileContext
 */
","* Write a line of text to a file. Characters are encoded into bytes using the
   * specified charset. This utility method opens the file for writing, creating
   * the file if it does not exist, or overwrites an existing file.
   *
   * @param fs the file context with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   * @param cs the charset to use for encoding
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createFile,org.apache.hadoop.fs.FileSystem:createFile(org.apache.hadoop.fs.Path),4732,4735,"/**
 * Creates a data output stream builder for the given path.
 * @param path Path to create the output stream for.
 * @return FSDataOutputStreamBuilder for the created stream.
 */
","* Create a new FSDataOutputStreamBuilder for the file with path.
   * Files are overwritten by default.
   *
   * @param path file path
   * @return a FSDataOutputStreamBuilder object to build the file
   *
   * HADOOP-14384. Temporarily reduce the visibility of method before the
   * builder interface becomes stable.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,createFile,org.apache.hadoop.fs.ChecksumFileSystem:createFile(org.apache.hadoop.fs.Path),1109,1112,"/**
 * Creates a data output stream builder for the given path.
 * @param path Path to create the output stream for.
 * @return FSDataOutputStreamBuilder for the specified path.
 */
","* This is overridden to ensure that this class's create() method is
   * ultimately called.
   *
   * {@inheritDoc}",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,appendFile,org.apache.hadoop.fs.FileSystem:appendFile(org.apache.hadoop.fs.Path),4742,4744,"/**
 * Creates an appendable FSDataOutputStreamBuilder for the given path.
 * @param path the path to append to
 * @return FSDataOutputStreamBuilder for appending to the path
 */
","* Create a Builder to append a file.
   * @param path file path.
   * @return a {@link FSDataOutputStreamBuilder} to build file append request.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,appendFile,org.apache.hadoop.fs.ChecksumFileSystem:appendFile(org.apache.hadoop.fs.Path),1120,1122,"/**
 * Appends a file to the Hadoop file system.
 * @param path Path to the file to append.
 * @return FSDataOutputStreamBuilder for appending.
 */
","* This is overridden to ensure that this class's create() method is
   * ultimately called.
   *
   * {@inheritDoc}",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],java.lang.String[],long,long,boolean)",155,159,"/**
 * Constructs a BlockLocation with default metadata.
 * @param names Block names, hosts, cachedHosts, topologyPaths, offset, length, corrupt.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,toString,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:toString(),559,562,"/**
 * Returns a string representation of the status.
 * Delegates to the internal {@code realStatus}.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpConfigKeys.java,getServerDefaults,org.apache.hadoop.fs.ftp.FtpConfigKeys:getServerDefaults(),60,71,"/**
 * Returns default HDFS server settings.
 * Creates and returns a FsServerDefaults object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/LocalConfigKeys.java,getServerDefaults,org.apache.hadoop.fs.local.LocalConfigKeys:getServerDefaults(),60,71,"/**
 * Returns the default FileSystem server settings.
 * Returns a new FsServerDefaults object with default values.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BoundedRangeFileInputStream.java,read,org.apache.hadoop.io.file.tfile.BoundedRangeFileInputStream:read(),75,80,"/**
 * Reads a single byte from the input stream.
 * @return The byte value as an integer, or -1 if EOF.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,next,org.apache.hadoop.fs.FileSystem$DirListingIterator:next(),2332,2342,"/**
 * Returns the next element in the iteration.
 * @throws NoSuchElementException if no more elements exist.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/XAttrCommands.java,processPath,org.apache.hadoop.fs.shell.XAttrCommands$GetfattrCommand:processPath(org.apache.hadoop.fs.shell.PathData),102,118,"/**
 * Processes a PathData item, printing file info and xattrs.
 * @param item PathData object containing file and FS info.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,listStatus,org.apache.hadoop.fs.ChecksumFileSystem:listStatus(org.apache.hadoop.fs.Path),959,962,"/**
 * Lists status of files and directories under the given path.
 * @param f Path to list status for.
 * @return Array of FileStatus objects.
 */
","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   *
   * @param f
   *          given path
   * @return the statuses of the files/directories in the given path
   * @throws IOException if an I/O error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,listStatus,org.apache.hadoop.fs.FileSystem:listStatus(org.apache.hadoop.fs.Path[]),2140,2143,"/**
 * Lists the status of multiple files.
 * @param files Array of file paths to check.
 * @return FileStatus array for the given files.
 */
","* Filter files/directories in the given list of paths using default
   * path filter.
   * <p>
   * Does not guarantee to return the List of files/directories status in a
   * sorted order.
   *
   * @param files
   *          a list of paths
   * @return a list of statuses for the files under the given paths after
   *         applying the filter default Path filter
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException see specific implementation",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",103,108,"/**
 * Creates a new MutableCounterInt with given info and initial value.
 * @param info MetricsInfo object
 * @param iVal Initial counter value
 * @return New MutableCounterInt object
 */
","* Create a mutable integer counter
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new counter object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",127,133,"/**
 * Creates a new MutableCounterLong with given info and initial value.
 * @param info MetricsInfo object
 * @param iVal Initial counter value
 * @return New MutableCounterLong object
 */
","* Create a mutable long integer counter
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new counter object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",176,181,"/**
 * Creates a new MutableGaugeLong metric.
 * @param info Metric info.
 * @param iVal Initial value.
 * @return The new MutableGaugeLong metric.
 */
","* Create a mutable long integer gauge
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new gauge object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",200,205,"/**
 * Creates a new MutableGaugeFloat with given info and initial value.
 * @param info MetricsInfo object
 * @param iVal Initial float value
 * @return New MutableGaugeFloat object
 */
","* Create a mutable float gauge
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new gauge object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",152,157,"/**
 * Creates a new MutableGaugeInt with given info and initial value.
 * @param info MetricsInfo object
 * @param iVal Initial integer value
 * @return New MutableGaugeInt object
 */
","* Create a mutable integer gauge
   * @param info  metadata of the metric
   * @param iVal  initial value
   * @return a new gauge object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addCounter,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addCounter(org.apache.hadoop.metrics2.MetricsInfo,long)",102,109,"/**
 * Adds a counter metric with the given value.
 * @param info MetricsInfo object
 * @param value The counter value
 * @return This builder object
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,long)",120,127,"/**
 * Adds a gauge metric.
 * @param info MetricsInfo object.
 * @param value The gauge value.
 * @return This builder.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addCounter,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addCounter(org.apache.hadoop.metrics2.MetricsInfo,int)",93,100,"/**
 * Adds an integer counter metric.
 * @param info MetricsInfo object
 * @param value The counter value
 * @return This builder object
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,float)",129,136,"/**
 * Adds a float gauge metric.
 * @param info MetricsInfo object.
 * @param value The gauge value.
 * @return This builder.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,double)",138,145,"/**
 * Adds a gauge metric with the given info and value.
 * @param info MetricsInfo object describing the metric.
 * @param value The gauge value.
 * @return This builder object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,addGauge,"org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:addGauge(org.apache.hadoop.metrics2.MetricsInfo,int)",111,118,"/**
 * Adds an integer gauge metric.
 * @param info MetricsInfo object.
 * @param value The integer value of the gauge.
 * @return This builder object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getFS,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getFS(),54,57,"/**
 * Returns the FileSystem object from the superclass.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FsLinkResolution.java,resolve,"org.apache.hadoop.fs.impl.FsLinkResolution:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.impl.FsLinkResolution$FsLinkResolutionFunction)",91,96,"/**
 * Resolves a file path using a provided resolution function.
 * @param fileContext File context for resolution.
 * @param path Path to resolve.
 * @param fn Resolution function.
 * @return Resolved value of type T.
 */
","* Apply the given function to the resolved path under the the supplied
   * FileContext.
   * @param fileContext file context to resolve under
   * @param path path to resolve
   * @param fn function to invoke
   * @param <T> return type.
   * @return the return value of the function as revoked against the resolved
   * path.
   * @throws UnresolvedLinkException link resolution failure
   * @throws IOException other IO failure.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,createGlobber,org.apache.hadoop.fs.Globber:createGlobber(org.apache.hadoop.fs.FileContext),413,415,"/**
 * Creates a new GlobBuilder instance with the given FileContext.
 * @param fileContext The file context for globbing operations.
 * @return A new GlobBuilder object.
 */
","* Create a builder for a Globber, bonded to the specific file
   * context.
   * @param fileContext file context.
   * @return the builder to finish configuring.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,createGlobber,org.apache.hadoop.fs.Globber:createGlobber(org.apache.hadoop.fs.FileSystem),403,405,"/**
 * Creates a new GlobBuilder instance for the given filesystem.
 * @param filesystem The filesystem to use for globbing.
 * @return A new GlobBuilder object.
 */
","* Create a builder for a Globber, bonded to the specific filesystem.
   * @param filesystem filesystem
   * @return the builder to finish configuring.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,isDone,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:isDone(),246,266,"/**
 * Checks if the asynchronous call is done.
 * Returns true if returned or exception occurred, false otherwise.
 */
","@return true if the call is done; otherwise, return false.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,getAsyncReturn,org.apache.hadoop.io.retry.AsyncCallHandler:getAsyncReturn(),56,66,"/**
 * Retrieves an AsyncGet instance, or fetches from a lower layer.
 */","* @return the async return value from {@link AsyncCallHandler}.
   * @param <T> T.
   * @param <R> R.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String,java.lang.String)",441,443,"/**
 * Constructs a DeprecationDelta with a single replacement key.
 * @param key Original key.
 * @param newKey Replacement key.
 * @param customMessage Optional custom deprecation message.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$DeprecationDelta:<init>(java.lang.String,java.lang.String)",445,447,"/**
 * Creates a DeprecationDelta with a single replacement key.
 * @param key The deprecated key.
 * @param newKey The replacement key.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeCompressedString,"org.apache.hadoop.io.WritableUtils:writeCompressedString(java.io.DataOutput,java.lang.String)",94,96,"/**
 * Writes a string to the output, compressed as a byte array.
 * @param out DataOutput to write to.
 * @param s String to write.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,concat,"org.apache.hadoop.fs.RawLocalFileSystem:concat(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path[])",612,622,"/**
 * Concatenates multiple paths into a target path.
 * @param trg Target path to write to.
 * @param psrcs Paths to concatenate.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,invoke,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall:invoke(),279,316,"/**
 * Executes the call, either from a previous async call or a new one.
 * @return CallReturn status: ASYNC_INVOKED or ASYNC_CALL_IN_PROGRESS
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,equals,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:equals(java.lang.Object),122,125,"/**
* Delegates equality check to the superclass.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,hashCode,org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:hashCode(),127,130,"/**
 * Returns the hash code value for this object. Delegates to super.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DUHelper.java,main,org.apache.hadoop.fs.DUHelper:main(java.lang.String[]),85,90,"/**
 * Prints folder usage. Uses DUHelper.getFolderUsage, OS dependent.
 * @param args Command line arguments, folder path.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/WindowsGetSpaceUsed.java,refresh,org.apache.hadoop.fs.WindowsGetSpaceUsed:refresh(),45,48,"/**
* Updates the 'used' attribute with folder usage.
*/
",* Override to hook in DUHelper class.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,<init>,org.apache.hadoop.fs.statistics.MeanStatistic:<init>(org.apache.hadoop.fs.statistics.MeanStatistic),107,111,"/**
 * Constructs a new MeanStatistic by copying another one.
 * @param that The MeanStatistic to copy.
 */
","* Create from another statistic.
   * @param that source",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,setMeanStatistic,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:setMeanStatistic(java.lang.String,org.apache.hadoop.fs.statistics.MeanStatistic)",245,251,"/**
 * Updates the MeanStatistic for the given key.
 * @param key statistic key
 * @param value new MeanStatistic value
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,ioStatisticsSourceToString,org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsSourceToString(java.lang.Object),63,70,"/**
 * Converts IO statistics source to a string representation.
 * @param source The source object for IO statistics.
 * @return String representation or """" if an error occurs.
 */
","* Extract the statistics from a source object -or """"
   * if it is not an instance of {@link IOStatistics},
   * {@link IOStatisticsSource} or the retrieved
   * statistics are null.
   * <p>
   * Exceptions are caught and downgraded to debug logging.
   * @param source source of statistics.
   * @return a string for logging.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/WrappedIOStatistics.java,toString,org.apache.hadoop.fs.statistics.impl.WrappedIOStatistics:toString(),103,106,"/**
 * Returns a string representation of the IO statistics.
 */
","* Return the statistics dump of the wrapped statistics.
   * @return the statistics for logging.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,toString,org.apache.hadoop.fs.statistics.IOStatisticsLogging$StatisticsToString:toString(),325,330,"/**
 * Returns a string representation of the IO statistics.
 * Returns NULL_SOURCE if statistics are null.
 */
","* Evaluate and stringify the statistics.
     * @return a string value.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,toString,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:toString(),253,256,"/**
 * Returns a string representation of the object.
 * Uses ioStatisticsToString to format the output.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,ioStatisticsToPrettyString,org.apache.hadoop.fs.statistics.IOStatisticsLogging:ioStatisticsToPrettyString(org.apache.hadoop.fs.statistics.IOStatistics),102,121,"/**
 * Converts IOStatistics to a formatted string representation.
 * @param statistics The IOStatistics object to format.
 * @return Formatted string or empty string if statistics is null.
 */
","* Convert IOStatistics to a string form, with all the metrics sorted
   * and empty value stripped.
   * This is more expensive than the simple conversion, so should only
   * be used for logging/output where it's known/highly likely that the
   * caller wants to see the values. Not for debug logging.
   * @param statistics A statistics instance.
   * @return string value or the empty string if null",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,createTracker,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:createTracker(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String)",672,678,"/**
 * Creates a DurationTracker, using factory if provided.
 * @param factory Factory to create tracker, or null.
 * @param statistic Statistic name for the tracker.
 * @return DurationTracker instance.
 */
","* Create the tracker. If the factory is null, a stub
   * tracker is returned.
   * @param factory tracker factory
   * @param statistic statistic to track
   * @return a duration tracker.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,deleteBlockFileAndEvictCache,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:deleteBlockFileAndEvictCache(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry),446,471,"/**
 * Deletes a block file and evicts it from the cache.
 * @param elementToPurge Entry object representing the file to purge.
 */
","* Delete cache file as part of the block cache LRU eviction.
   *
   * @param elementToPurge Block entry to evict.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,submit,org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.util.concurrent.Callable),128,138,"/**
 * Submits a Callable task, acquiring a permit and releasing it.
 * @param task The Callable task to execute.
 * @return Future representing the task's execution.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,submit,"org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable,java.lang.Object)",140,150,"/**
 * Submits a task to the executor, acquiring a permit.
 * @param task Runnable task to execute
 * @param result Result value to be returned on completion
 * @return Future representing the task's execution
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,submit,org.apache.hadoop.util.SemaphoredDelegatingExecutor:submit(java.lang.Runnable),152,162,"/**
 * Submits a task to the executor, acquiring a permit.
 * @param task The runnable task to execute.
 * @return A Future representing the task execution.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SemaphoredDelegatingExecutor.java,execute,org.apache.hadoop.util.SemaphoredDelegatingExecutor:execute(java.lang.Runnable),164,173,"/**
 * Executes a command, tracking duration and releasing a permit.
 * @param command The runnable to execute.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StorageStatisticsFromIOStatistics.java,iterator,org.apache.hadoop.fs.statistics.impl.StorageStatisticsFromIOStatistics:iterator(),56,59,"/**
 * Returns an iterator for LongStatistic objects.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,addTimedOperation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:addTimedOperation(java.lang.String,java.time.Duration)",447,450,"/**
 * Adds a timed operation with duration in milliseconds.
 * Delegates to the overload using milliseconds.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,fromStorageStatistics,org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:fromStorageStatistics(org.apache.hadoop.fs.StorageStatistics),72,83,"/**
 * Creates IOStatistics from StorageStatistics.
 * @param storageStatistics Source for IO statistics.
 * @return IOStatistics object populated from StorageStatistics.
 */
","* Create  IOStatistics from a storage statistics instance.
   *
   * This will be updated as the storage statistics change.
   * @param storageStatistics source data.
   * @return an IO statistics source.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongCounter(java.lang.String,java.util.concurrent.atomic.AtomicLong)",87,91,"/**
 * Adds an AtomicLong counter to the statistics with the given key.
 * @param key Counter key
 * @param source AtomicLong to track
 * @return This builder instance
 */
","* Add a counter statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long counter
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerCounter(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",100,104,"/**
 * Adds an AtomicInteger counter to the statistics with the given key.
 * @param key Counter key
 * @param source AtomicInteger to track
 * @return this
 */
","* Add a counter statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int counter
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withMutableCounter,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withMutableCounter(java.lang.String,org.apache.hadoop.metrics2.lib.MutableCounterLong)",113,117,"/**
 * Adds a mutable counter to the statistics with the given key.
 * @param key counter key
 * @param source mutable counter source
 * @return this builder
 */
","* Build a dynamic counter statistic from a
   * {@link MutableCounterLong}.
   * @param key key of this statistic
   * @param source mutable long counter
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongGauge,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongGauge(java.lang.String,java.util.concurrent.atomic.AtomicLong)",138,142,"/**
 * Adds an AtomicLong gauge with the given key.
 * @param key Gauge key.
 * @param source AtomicLong source.
 * @return this
 */
","* Add a gauge statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long gauge
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerGauge,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerGauge(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",151,155,"/**
 * Adds an AtomicInteger gauge with the given key.
 * @param key Gauge key.
 * @param source AtomicInteger to track.
 * @return This builder.
 */
","* Add a gauge statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int gauge
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongMinimum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongMinimum(java.lang.String,java.util.concurrent.atomic.AtomicLong)",176,180,"/**
 * Sets the minimum value for a given key using an AtomicLong.
 * @param key Identifier for the statistic.
 * @param source AtomicLong source for the minimum value.
 */
","* Add a minimum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long minimum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerMinimum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerMinimum(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",189,193,"/**
 * Sets the minimum value for a key using an AtomicInteger.
 * @param key Key to associate with the minimum value.
 * @param source AtomicInteger providing the minimum value.
 */
","* Add a minimum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int minimum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicLongMaximum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicLongMaximum(java.lang.String,java.util.concurrent.atomic.AtomicLong)",215,219,"/**
 * Sets the maximum value for a key using an AtomicLong.
 * @param key key for the statistic
 * @param source AtomicLong source for the maximum value
 */
","* Add a maximum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic long maximum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatisticsBuilder.java,withAtomicIntegerMaximum,"org.apache.hadoop.fs.statistics.impl.DynamicIOStatisticsBuilder:withAtomicIntegerMaximum(java.lang.String,java.util.concurrent.atomic.AtomicInteger)",228,232,"/**
 * Sets the maximum value for a key using an AtomicInteger.
 * @param key Key for the statistic.
 * @param source AtomicInteger to track the maximum.
 * @return The builder instance.
 */
","* Add a maximum statistic to dynamically return the
   * latest value of the source.
   * @param key key of this statistic
   * @param source atomic int maximum
   * @return the builder.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,registerFailureHandling,org.apache.hadoop.service.launcher.ServiceLauncher:registerFailureHandling(),760,772,"/**
 * Registers interrupt handling and sets uncaught exception handler.
 */","* Override point: register this class as the handler for the control-C
   * and SIGINT interrupts.
   *
   * Subclasses can extend this with extra operations, such as
   * an exception handler:
   * <pre>
   *  Thread.setDefaultUncaughtExceptionHandler(
   *     new YarnUncaughtExceptionHandler());
   * </pre>",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,accept,org.apache.hadoop.net.unix.DomainSocket:accept(),233,243,"/**
 * Accepts a connection on the domain socket.
 * @return A new DomainSocket representing the accepted connection.
 * @throws IOException if an I/O error occurs.
 */
","* Accept a new UNIX domain connection.
   *
   * This method can only be used on sockets that were bound with bind().
   *
   * @return                The new connection.
   * @throws IOException    If there was an I/O error performing the accept--
   *                        such as the socket being closed from under us.
   *                        Particularly when the accept is timed out, it throws
   *                        SocketTimeoutException.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,setAttribute,"org.apache.hadoop.net.unix.DomainSocket:setAttribute(int,int)",308,317,"/**
 * Sets an attribute on the file descriptor.
 * @param type Attribute type.
 * @param size Attribute size.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,getAttribute,org.apache.hadoop.net.unix.DomainSocket:getAttribute(int),321,332,"/**
 * Gets an attribute from the file descriptor.
 * @param type Attribute type to retrieve.
 * @return Attribute value.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,shutdown,org.apache.hadoop.net.unix.DomainSocket:shutdown(),395,404,"/**
 * Shuts down the resource, ensuring proper cleanup and reference management.
 */","* Call shutdown(SHUT_RDWR) on the UNIX domain socket.
   *
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,sendFileDescriptors,"org.apache.hadoop.net.unix.DomainSocket:sendFileDescriptors(java.io.FileDescriptor[],byte[],int,int)",421,431,"/**
 * Sends file descriptors using a buffer.
 * @param descriptors File descriptors to send.
 * @param jbuf Buffer for data.
 * @param offset Start offset in jbuf.
 * @param length Length of data to send.
 */
","* Send some FileDescriptor objects to the process on the other side of this
   * socket.
   * 
   * @param descriptors       The file descriptors to send.
   * @param jbuf              Some bytes to send.  You must send at least
   *                          one byte.
   * @param offset            The offset in the jbuf array to start at.
   * @param length            Length of the jbuf array to use.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocket.java,recvFileInputStreams,"org.apache.hadoop.net.unix.DomainSocket:recvFileInputStreams(java.io.FileInputStream[],byte[],int,int)",448,487,"/**
 * Receives file descriptors from streams into a buffer.
 * @param streams Input streams to receive from.
 * @param buf Buffer to store received data.
 * @param offset Offset in buffer.
 * @param length Length of data to receive.
 * @return Number of bytes received.
 * @throws IOException If an I/O error occurs.
 */
","* Receive some FileDescriptor objects from the process on the other side of
   * this socket, and wrap them in FileInputStream objects.
   *
   * @param streams input stream.
   * @param buf input buf.
   * @param offset input offset.
   * @param length input length.
   * @return wrap them in FileInputStream objects.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextIntegration.java,createNewInstance,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextIntegration:createNewInstance(java.lang.Long),102,107,"/**
 * Creates a new IOStatisticsContext instance for the given key.
 * @param key The key to associate with the new context.
 * @return A new IOStatisticsContextImpl instance.
 */
","* Creating a new IOStatisticsContext instance for a FS to be used.
   * @param key Thread ID that represents which thread the context belongs to.
   * @return an instance of IOStatisticsContext.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,verifyChunkedSums,"org.apache.hadoop.util.DataChecksum:verifyChunkedSums(java.nio.ByteBuffer,java.nio.ByteBuffer,java.lang.String,long)",401,427,"/**
 * Verifies chunked sums using native or fallback method.
 * @param data Data buffer to verify.
 * @param checksums Checksum buffer.
 * @param fileName File name.
 * @param basePos Base position.
 */
","* Verify that the given checksums match the given data.
   * 
   * The 'mark' of the ByteBuffer parameters may be modified by this function,.
   * but the position is maintained.
   *  
   * @param data the DirectByteBuffer pointing to the data to verify.
   * @param checksums the DirectByteBuffer pointing to a series of stored
   *                  checksums
   * @param fileName the name of the file being read, for error-reporting
   * @param basePos the file position to which the start of 'data' corresponds
   * @throws ChecksumException if the checksums do not match",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,afterDecryption,"org.apache.hadoop.crypto.CryptoInputStream:afterDecryption(org.apache.hadoop.crypto.Decryptor,java.nio.ByteBuffer,long,byte[])",271,286,"/**
 * Returns padding byte. Re-initializes decryptor if needed.
 * @param decryptor Decryptor object
 * @param inBuffer Input buffer
 * @param position Current position
 * @param iv Initialization vector
 * @return Padding byte
 */
","* This method is executed immediately after decryption. Check whether 
   * decryptor should be updated and recalculate padding if needed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,resetStreamOffset,org.apache.hadoop.crypto.CryptoInputStream:resetStreamOffset(long),309,317,"/**
 * Resets the stream offset and clears buffers.
 * @param offset The new offset for the stream.
 * @throws IOException if an I/O error occurs.
 */
","* Reset the underlying stream offset; clear {@link #inBuffer} and 
   * {@link #outBuffer}. This Typically happens during {@link #seek(long)} 
   * or {@link #skip(long)}.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,write,"org.apache.hadoop.crypto.CryptoOutputStream:write(byte[],int,int)",151,172,"/**
 * Writes data to the stream.
 * @param b The byte array to write.
 * @param off Offset within the array.
 * @param len Number of bytes to write.
 */","* Encryption is buffer based.
   * If there is enough room in {@link #inBuffer}, then write to this buffer.
   * If {@link #inBuffer} is full, then do encryption and write data to the
   * underlying stream.
   * @param b the data.
   * @param off the start offset in the data.
   * @param len the number of bytes to write.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,flush,org.apache.hadoop.crypto.CryptoOutputStream:flush(),262,269,"/**
 * Flushes the buffer, encrypting data before calling super.flush().
 */
","* To flush, we need to encrypt the data in the buffer and write to the 
   * underlying stream, then do the flush.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobPattern.java,compile,org.apache.hadoop.fs.GlobPattern:compile(java.lang.String),57,59,"/**
 * Compiles a glob pattern into a Pattern object for matching.
 * @param globPattern The glob pattern string to compile.
 * @return A Pattern object representing the compiled glob.
 */
","* Compile glob pattern string
   * @param globPattern the glob pattern
   * @return the pattern object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,init,"org.apache.hadoop.fs.GlobFilter:init(java.lang.String,org.apache.hadoop.fs.PathFilter)",64,73,"/**
 * Initializes the filter with a file pattern and PathFilter.
 * @param filePattern glob pattern for file matching.
 * @param filter PathFilter to apply.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Name.java,prepare,org.apache.hadoop.fs.shell.find.Name:prepare(),73,80,"/**
 * Prepares the glob pattern based on the argument,
 * converting to lowercase if case-insensitive.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingTar,"org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.InputStream,java.io.File,boolean)",1035,1051,"/**
 * Untars a stream using tar, optionally decompressing with gzip.
 * @param inputStream Input stream containing the tar archive.
 * @param untarDir Directory to extract files to.
 * @param gzipped True if the archive is gzipped.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,close,org.apache.hadoop.fs.sftp.SFTPFileSystem:close(),710,722,"/**
 * Closes the connection pool and releases resources.
 * Shuts down the connection pool gracefully.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,create,"org.apache.hadoop.fs.viewfs.NflyFSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",723,729,"/**
 * Creates a data output stream for writing to a file.
 * @param f Path to the file, permission, overwrite flag, etc.
 * @return FSDataOutputStream object
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,resetChecksumBufSize,org.apache.hadoop.fs.FSOutputSummer:resetChecksumBufSize(),263,265,"/**
* Resets the checksum buffer size based on bytes per checksum.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getAllStatistics,org.apache.hadoop.fs.AbstractFileSystem:getAllStatistics(),234,244,"/**
 * Returns a copy of all statistics entries from the statistics table.
 * @return A map containing all statistics entries.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,<init>,"org.apache.hadoop.fs.FileSystemStorageStatistics:<init>(java.lang.String,org.apache.hadoop.fs.FileSystem$Statistics)",118,125,"/**
 * Constructs a FileSystemStorageStatistics object.
 * @param name Name of the storage.
 * @param stats FileSystem statistics object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,getLongStatistics,org.apache.hadoop.fs.FileSystemStorageStatistics:getLongStatistics(),132,135,"/**
 * Returns an iterator for LongStatistic objects.
 * Uses the underlying data for iteration.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,getLong,org.apache.hadoop.fs.FileSystemStorageStatistics:getLong(java.lang.String),137,140,"/**
 * Retrieves a Long value associated with the given key.
 * @param key The key to look up.
 * @return The Long value or null if not found.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getBytesReadByDistance,org.apache.hadoop.fs.FileSystem$Statistics:getBytesReadByDistance(int),4411,4430,"/**
 * Gets bytes read based on distance.
 * @param distance Distance value to determine bytes read.
 * @return Number of bytes read.
 */
","* In the common network topology setup, distance value should be an even
     * number such as 0, 2, 4, 6. To make it more general, we group distance
     * by {1, 2}, {3, 4} and {5 and beyond} for accounting. So if the caller
     * ask for bytes read for distance 2, the function will return the value
     * for group {1, 2}.
     * @param distance the network distance
     * @return the total number of bytes read by the network distance",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemStorageStatistics.java,reset,org.apache.hadoop.fs.FileSystemStorageStatistics:reset(),157,160,"/**
 * Resets the internal statistics by calling stats.reset().
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,clearStatistics,org.apache.hadoop.fs.AbstractFileSystem:clearStatistics(),218,222,"/**
 * Resets all statistics stored in the STATISTICS_TABLE to their initial state.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,primitiveCreate,"org.apache.hadoop.fs.FileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",1334,1359,"/**
 * Creates a primitive data output stream.
 * @param f Path to create.
 * @throws IOException on failure.
 */
","* This create has been added to support the FileContext that processes
   * the permission with umask before calling this method.
   * This a temporary method added to support the transition from FileSystem
   * to FileContext for user applications.
   *
   * @param f path.
   * @param absolutePermission permission.
   * @param flag create flag.
   * @param bufferSize buffer size.
   * @param replication replication.
   * @param blockSize block size.
   * @param progress progress.
   * @param checksumOpt check sum opt.
   * @return output stream.
   * @throws IOException IO failure",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,<init>,"org.apache.hadoop.fs.AbstractFileSystem:<init>(java.net.URI,java.lang.String,boolean,int)",278,283,"/**
 * Constructs a FileSystem with the given URI, scheme, and port.
 */
","* Constructor to be called by subclasses.
   * 
   * @param uri for this file system.
   * @param supportedScheme the scheme supported by the implementor
   * @param authorityNeeded if true then theURI must have authority, if false
   *          then the URI must have null authority.
   * @param defaultPort default port to use if port is not specified in the URI.
   * @throws URISyntaxException <code>uri</code> has syntax error",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,encode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(byte[][],byte[][])",117,127,"/**
 * Encodes input byte arrays to output arrays.
 * @param inputs Input byte arrays to encode.
 * @param outputs Output byte arrays to store encoded data.
 */
","* Encode with inputs and generates outputs. More see above.
   *
   * @param inputs input buffers to read data from
   * @param outputs output buffers to put the encoded data into, read to read
   *                after the call
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,encode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",68,99,"/**
 * Encodes data from input ByteBuffers to output ByteBuffers.
 * @param inputs Input ByteBuffers to encode.
 * @param outputs Output ByteBuffers to write encoded data.
 */
","* Encode with inputs and generates outputs.
   *
   * Note, for both inputs and outputs, no mixing of on-heap buffers and direct
   * buffers are allowed.
   *
   * If the coder option ALLOW_CHANGE_INPUTS is set true (false by default), the
   * content of input buffers may change after the call, subject to concrete
   * implementation. Anyway the positions of input buffers will move forward.
   *
   * @param inputs input buffers to read data from. The buffers' remaining will
   *               be 0 after encoding
   * @param outputs output buffers to put the encoded data into, ready to read
   *                after the call
   * @throws IOException if the encoder is closed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable:<init>(java.lang.Object),122,124,"/**
 * Constructs a new ArrayPrimitiveWritable with the given value.
 */","* Wrap an existing array of primitives
   * @param value - array of primitives",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getCanonicalUri,org.apache.hadoop.fs.HarFileSystem:getCanonicalUri(),318,321,"/**
 * Returns the canonical URI of the file system.
 * Delegates to the underlying file system's method.
 */
","* Used for delegation token related functionality. Must delegate to
   * underlying file system.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getCanonicalUri,org.apache.hadoop.fs.FilterFileSystem:getCanonicalUri(),113,116,"/**
 * Returns the canonical URI of the file system.
 * Delegates to the underlying file system's method.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFsStatus,org.apache.hadoop.fs.DelegateToFileSystem:getFsStatus(),148,151,"/**
 * Gets the FileSystem status.
 * @return FsStatus object representing the status.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,hasCapability,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:hasCapability(java.lang.String),687,693,"/**
 * Checks if the data source has the specified capability.
 * @param capability Capability to check for.
 * @return True if the data source has the capability.
 */
","* Probe the inner stream for a capability.
     * Syncable operations are rejected before being passed down.
     * @param capability string to query the stream support for.
     * @return true if a capability is known to be supported.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,hasCapability,org.apache.hadoop.io.SequenceFile$Writer:hasCapability(java.lang.String),1410,1416,"/**
 * Checks if the underlying object has the given capability.
 * @param capability Capability to check for.
 * @return True if capability exists, false otherwise.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,hasCapability,org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:hasCapability(java.lang.String),484,487,"/**
 * Checks if the data source has the specified capability.
 * @param capability Capability to check for.
 * @return True if the data source has the capability.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,addToCacheAndRelease,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:addToCacheAndRelease(org.apache.hadoop.fs.impl.prefetch.BufferData,java.util.concurrent.Future,java.time.Instant)",484,552,"/**
 * Adds data to cache after block future completion, handles errors.
 * @param data BufferData to cache
 * @param blockFuture Future representing block operation
 * @param taskQueuedStartTime Start time of the task
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,release,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:release(org.apache.hadoop.fs.impl.prefetch.BufferData),215,226,"/**
 * Releases a buffer data object, releasing it to the pool.
 * @param data The buffer data to release.
 */
","* Releases resources allocated to the given block.
   *
   * @throws IllegalArgumentException if data is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,releaseDoneBlocks,org.apache.hadoop.fs.impl.prefetch.BufferPool:releaseDoneBlocks(),192,198,"/**
 * Releases buffers in the 'DONE' state. Iterates through all buffers.
 */",* Releases resources for any blocks marked as 'done'.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockOperations.java,getSummary,org.apache.hadoop.fs.impl.prefetch.BlockOperations:getSummary(boolean),232,250,"/**
 * Generates a summary string of operations, with optional debug info.
 * @param showDebugInfo flag to include debug information
 * @return Summary string of operations.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,<init>,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:<init>(org.apache.hadoop.fs.impl.prefetch.BlockManagerParameters),113,137,"/**
 * Constructs a CachingBlockManager with provided parameters.
 * @param blockManagerParameters Configuration parameters for the manager.
 */
","* Constructs an instance of a {@code CachingBlockManager}.
   *
   * @param blockManagerParameters params for block manager.
   * @throws IllegalArgumentException if bufferPoolSize is zero or negative.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,<init>,"org.apache.hadoop.util.BlockingThreadPoolExecutorService:<init>(int,java.util.concurrent.ThreadPoolExecutor)",104,108,"/**
 * Initializes the BlockingThreadPoolExecutorService.
 * @param permitCount Number of permits.
 * @param eventProcessingExecutor Executor for event processing.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,get,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:get(int,java.nio.ByteBuffer)",265,283,"/**
 * Reads data from a block into the provided buffer.
 * @param blockNumber Block number to read.
 * @param buffer ByteBuffer to store the data.
 */
","* Gets the block having the given {@code blockNumber}.
   *
   * @throws IllegalArgumentException if buffer is null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,toString,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:toString(),635,647,"/**
 * Returns a string representation of the object's state.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,absolute,org.apache.hadoop.fs.impl.prefetch.FilePosition:absolute(),145,148,"/**
 * Calculates the absolute position based on buffer offset.
 * Throws an exception if the buffer is invalid.
 */
","* Gets the current absolute position within this file.
   *
   * @return the current absolute position within this file.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,bufferFullyRead,org.apache.hadoop.fs.impl.prefetch.FilePosition:bufferFullyRead(),241,246,"/**
 * Checks if the buffer has been fully read.
 * Returns true if all data has been processed.
 */
","* Determines whether the current buffer has been fully read.
   *
   * @return true if the current buffer has been fully read, false otherwise.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,setAbsolute,org.apache.hadoop.fs.impl.prefetch.FilePosition:setAbsolute(long),157,165,"/**
 * Sets the buffer position to the absolute position.
 * @param pos The absolute position to set.
 * @return True if successful, false otherwise.
 */
","* If the given {@code pos} lies within the current buffer, updates the current position to
   * the specified value and returns true; otherwise returns false without changing the position.
   *
   * @param pos the absolute position to change the current position to if possible.
   * @return true if the given current position was updated, false otherwise.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsContext.java,getCurrentIOStatisticsContext,org.apache.hadoop.fs.statistics.IOStatisticsContext:getCurrentIOStatisticsContext(),71,77,"/**
 * Returns the current IO statistics context.
 * Returns null if the context is not available.
 */
","* Get the context's IOStatisticsContext.
   *
   * @return instance of IOStatisticsContext for the context.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsContext.java,setThreadIOStatisticsContext,org.apache.hadoop.fs.statistics.IOStatisticsContext:setThreadIOStatisticsContext(org.apache.hadoop.fs.statistics.IOStatisticsContext),84,88,"/**
 * Sets the thread's IO statistics context.
 * @param statisticsContext The context to set.
 */
","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getInstanceConfigs,org.apache.hadoop.metrics2.impl.MetricsConfig:getInstanceConfigs(java.lang.String),157,171,"/**
 * Retrieves instance configurations based on the given type.
 * @param type config type to filter by
 * @return Map of instance name to MetricsConfig
 */
","* Return sub configs for instance specified in the config.
   * Assuming format specified as follows:<pre>
   * [type].[instance].[option] = [value]</pre>
   * Note, '*' is a special default instance, which is excluded in the result.
   * @param type  of the instance
   * @return  a map with [instance] as key and config object as value",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,applyItem,org.apache.hadoop.fs.shell.find.Find:applyItem(org.apache.hadoop.fs.shell.PathData),412,419,"/**
 * Applies an item if the current depth meets the minimum depth.
 * @param item The PathData item to apply.
 * @throws IOException If an I/O error occurs during application.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,processArguments,org.apache.hadoop.fs.shell.find.Find:processArguments(java.util.LinkedList),421,429,"/**
 * Processes arguments, configures expression, and calls super.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,processOptions,org.apache.hadoop.fs.shell.Test:processOptions(java.util.LinkedList),59,75,"/**
 * Processes command-line arguments to extract a test flag.
 * Throws IllegalArgumentException if no or multiple flags are given.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Rm:processOptions(java.util.LinkedList),81,90,"/**
 * Parses command-line arguments and sets processing options.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processOptions,org.apache.hadoop.fs.shell.Display$Checksum:processOptions(java.util.LinkedList),189,195,"/**
 * Parses command-line arguments and sets display block size.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processOptions,org.apache.hadoop.fs.shell.FsUsage$Df:processOptions(java.util.LinkedList),85,92,"/**
 * Processes command-line arguments, sets human-readable flag.
 * Parses arguments, sets 'h' option, adds separator if empty.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,processOptions,org.apache.hadoop.fs.shell.Head:processOptions(java.util.LinkedList),50,54,"/**
 * Parses command-line arguments using a predefined format.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processOptions,org.apache.hadoop.fs.shell.Ls:processOptions(java.util.LinkedList),132,153,"/**
 * Parses command-line arguments and initializes processing options.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,processOptions,org.apache.hadoop.fs.shell.Tail:processOptions(java.util.LinkedList),63,78,"/**
 * Processes command-line arguments; sets followDelay if 's' is provided.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Expunge:processOptions(java.util.LinkedList),235,242,"/**
 * Processes command-line arguments to configure options.
 * Parses arguments, extracts 'immediate' and filesystem values.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Rmdir:processOptions(java.util.LinkedList),197,203,"/**
 * Parses command-line arguments and sets the ignoreNonEmpty flag.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Get:processOptions(java.util.LinkedList),234,249,"/**
 * Parses command-line arguments and configures processing options.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,processOptions,org.apache.hadoop.fs.shell.Count:processOptions(java.util.LinkedList),124,178,"/**
 * Processes command-line arguments, parses options, and sets flags.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,processOptions,org.apache.hadoop.fs.shell.Mkdir:processOptions(java.util.LinkedList),51,56,"/**
 * Parses command-line arguments and sets createParents flag.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processOptions,org.apache.hadoop.fs.shell.TouchCommands$Touch:processOptions(java.util.LinkedList),134,146,"/**
 * Processes command-line options from the provided argument list.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Put:processOptions(java.util.LinkedList),276,292,"/**
 * Parses command-line arguments, configures options, and sets defaults.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Cp:processOptions(java.util.LinkedList),175,189,"/**
 * Processes command-line options from the provided argument list.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processOptions(java.util.LinkedList),377,390,"/**
 * Processes command-line options, parsing arguments and setting flags.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processOptions,org.apache.hadoop.fs.shell.Display$Cat:processOptions(java.util.LinkedList),80,86,"/**
 * Parses command-line arguments and sets verification flag.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processOptions,org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processOptions(java.util.LinkedList),64,75,"/**
 * Parses command-line arguments, sets options, and validates input.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Stat.java,processOptions,org.apache.hadoop.fs.shell.Stat:processOptions(java.util.LinkedList),82,89,"/**
 * Processes command-line options from the provided argument list.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processOptions,org.apache.hadoop.fs.shell.TouchCommands$Touchz:processOptions(java.util.LinkedList),61,65,"/**
 * Parses command-line arguments using a CommandFormat.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFormat.java,parse,"org.apache.hadoop.fs.shell.CommandFormat:parse(java.lang.String[],int)",87,92,"/**
 * Parses arguments, removes initial elements, and returns modified list.
 */
","Parse parameters starting from the given position
   * Consider using the variant that directly takes a List
   * 
   * @param args an array of input arguments
   * @param pos the position at which starts to parse
   * @return a list of parameters",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processOptions,org.apache.hadoop.fs.shell.MoveCommands$Rename:processOptions(java.util.LinkedList),104,109,"/**
 * Processes command-line arguments and determines remote destination.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,processOptions,org.apache.hadoop.fs.shell.Truncate:processOptions(java.util.LinkedList),51,66,"/**
 * Processes command-line options, parses arguments, and sets length.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,processOptions,org.apache.hadoop.fs.shell.SetReplication:processOptions(java.util.LinkedList),56,72,"/**
 * Processes command-line arguments, parses options, and sets replication.
 * @param args Command-line arguments to parse.
 * @throws IOException, NumberFormatException, IllegalArgumentException
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processOptions,org.apache.hadoop.fs.shell.FsUsage$Du:processOptions(java.util.LinkedList),183,192,"/**
 * Processes command-line options.
 * Parses arguments, sets flags, and adds default path if needed.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processOptions,org.apache.hadoop.fs.FsShellPermissions$Chown:processOptions(java.util.LinkedList),144,150,"/**
 * Processes command-line arguments, sets options, and parses group.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,displayError,org.apache.hadoop.fs.shell.Command:displayError(java.lang.String),501,504,"/**
 * Displays an error message and increments the error count.
 * @param message The error message to display.
 */
","* Display an error string prefaced with the command name.  Also increments
   * the error count for the command which will result in a non-zero exit
   * code.
   * @param message error message to display",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printInstanceUsage,"org.apache.hadoop.fs.FsShell:printInstanceUsage(java.io.PrintStream,org.apache.hadoop.fs.shell.Command)",249,251,"/**
 * Prints the usage information for a given command to the output stream.
 * @param out PrintStream to write the usage to.
 * @param instance Command instance providing usage string.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsCollectorImpl.java,addRecord,org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:addRecord(java.lang.String),52,55,"/**
 * Adds a record with the given name.
 * @param name The name of the record to add.
 * @return A MetricsRecordBuilderImpl instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processArguments,org.apache.hadoop.fs.shell.FsUsage$Df:processArguments(java.util.LinkedList),94,105,"/**
 * Processes arguments, builds & prints filesystem usage table.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processArguments,org.apache.hadoop.fs.shell.FsUsage$Du:processArguments(java.util.LinkedList),194,207,"/**
 * Processes argument list, sets up table, and prints usage data.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createPathHandle,"org.apache.hadoop.fs.RawLocalFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",1148,1172,"/**
 * Creates a PathHandle for a file, throwing exceptions if invalid.
 * @param stat FileStatus object
 * @param opts Handle options
 * @return LocalFileSystemPathHandle object
 */
","* Hook to implement support for {@link PathHandle} operations.
   * @param stat Referent in the target FileSystem
   * @param opts Constraints that determine the validity of the
   *            {@link PathHandle} reference.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,exact,org.apache.hadoop.fs.Options$HandleOpt:exact(),384,386,"/**
 * Returns an array containing 'changed' and 'moved' HandleOpt objects.
 */","* Handle is valid iff the referent is neither moved nor changed.
     * Equivalent to changed(false), moved(false).
     * @return Options requiring that the content and location of the entity
     * be unchanged between calls.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,content,org.apache.hadoop.fs.Options$HandleOpt:content(),394,396,"/**
 * Returns an array of HandleOpt objects, representing content changes.
 */","* Handle is valid iff the content of the referent is the same.
     * Equivalent to changed(false), moved(true).
     * @return Options requiring that the content of the entity is unchanged,
     * but it may be at a different location.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,path,org.apache.hadoop.fs.Options$HandleOpt:path(),404,406,"/**
 * Returns an array containing changed and moved HandleOpt objects.
 */","* Handle is valid iff the referent is unmoved in the namespace.
     * Equivalent to changed(true), moved(false).
     * @return Options requiring that the referent exist in the same location,
     * but its content may have changed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,reference,org.apache.hadoop.fs.Options$HandleOpt:reference(),414,416,"/**
 * Returns an array containing 'changed' and 'moved' HandleOpt objects.
 */","* Handle is valid iff the referent exists in the namespace.
     * Equivalent to changed(true), moved(true).
     * @return Options requiring that the implementation resolve a reference
     * to this entity regardless of changes to content or location.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map,long,boolean)",1248,1259,"/**
 * Constructs a ShellCommandExecutor with command, directory, env, timeout.
 */","* Create a new instance of the ShellCommandExecutor to execute a command.
     *
     * @param execString The command to execute with arguments
     * @param dir If not-null, specifies the directory which should be set
     *            as the current working directory for the command.
     *            If null, the current working directory is not modified.
     * @param env If not-null, environment of the command will include the
     *            key-value pairs specified in the map. If null, the current
     *            environment is not modified.
     * @param timeout Specifies the time in milliseconds, after which the
     *                command will be killed and the status marked as timed-out.
     *                If 0, the command will not be timed out.
     * @param inheritParentEnv Indicates if the process should inherit the env
     *                         vars from the parent process or not.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,initRefreshThread,org.apache.hadoop.fs.CachingGetSpaceUsed:initRefreshThread(boolean),108,118,"/**
 * Starts/stops the refresh thread based on refreshInterval.
 * @param runImmediately Whether to start immediately.
 */
","* RunImmediately should set true, if we skip the first refresh.
   * @param runImmediately The param default should be false.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,privateClone,org.apache.hadoop.security.token.Token:privateClone(org.apache.hadoop.io.Text),243,245,"/**
 * Creates a private clone of the token with a new service.
 * @param newService The new service for the cloned token.
 * @return A new PrivateToken instance.
 */
","* Create a private clone of a public token.
   * @param newService the new service name
   * @return a private token",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAllStoragePolicies,org.apache.hadoop.fs.viewfs.ViewFileSystem:getAllStoragePolicies(),1125,1139,"/**
 * Retrieves all BlockStoragePolicySpi instances from child file systems.
 * Returns a Collection of BlockStoragePolicySpi objects.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,initAsyncCall,"org.apache.hadoop.io.retry.AsyncCallHandler:initAsyncCall(org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCall,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncValue)",333,353,"/**
 * Initializes an asynchronous call and its associated return value.
 * @param asyncCall The async call object.
 * @param asyncCallReturn AsyncValue holding the call's return.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,process,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:process(java.nio.ByteBuffer,java.nio.ByteBuffer)",164,182,"/**
 * Processes data using the cipher, updating the output buffer.
 * @param inBuffer Input buffer containing data to encrypt/decrypt.
 * @param outBuffer Output buffer for the processed data.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,initializeInterceptors,org.apache.hadoop.fs.viewfs.RegexMountPoint:initializeInterceptors(),98,115,"/**
 * Initializes RegexMountPointInterceptors from settings string.
 * @throws IOException if settings are invalid.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,main,org.apache.hadoop.fs.DF:main(java.lang.String[]),216,223,"/**
 * Prints a FileSystem object's string representation, using path from args.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,initialize,"org.apache.hadoop.fs.Path:initialize(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",258,266,"/**
 * Initializes the URI with provided scheme, authority, path, fragment.
 * @param scheme URI scheme (e.g., ""http"")
 * @param authority URI authority (e.g., ""example.com"")
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,uriToString,"org.apache.hadoop.fs.shell.PathData:uriToString(java.net.URI,boolean)",466,487,"/**
 * Converts a URI to a String, optionally removing the scheme.
 * @param uri The URI to convert.
 * @param inferredSchemeFromPath Whether scheme was inferred from path.
 * @return String representation of the URI.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,checkNotSchemeWithRelative,org.apache.hadoop.fs.Path:checkNotSchemeWithRelative(),85,90,"/**
 * Checks if URI has a scheme but a relative path. Throws exception if true.
 */","* Test whether this Path uses a scheme and is relative.
   * Pathnames with scheme and relative path are illegal.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isAbsoluteAndSchemeAuthorityNull,org.apache.hadoop.fs.Path:isAbsoluteAndSchemeAuthorityNull(),377,380,"/**
 * Checks if the URI is absolute and has null scheme/authority.
 */","* Returns true if the path component (i.e. directory) of this URI is
   * absolute <strong>and</strong> the scheme is null, <b>and</b> the authority
   * is null.
   *
   * @return whether the path is absolute and the URI has no scheme nor
   * authority parts",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isAbsolute,org.apache.hadoop.fs.Path:isAbsolute(),399,401,"/**
 * Checks if the URI path is absolute.
 * @return True if absolute, false otherwise.
 */
","* Returns true if the path component (i.e. directory) of this URI is
   * absolute.  This method is a wrapper for {@link #isUriPathAbsolute()}.
   *
   * @return whether this URI's path is absolute",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,checkPath,org.apache.hadoop.fs.AbstractFileSystem:checkPath(org.apache.hadoop.fs.Path),369,413,"/**
 * Checks if the given path is valid for this file system.
 * @param path The path to check.
 */
","* Check that a Path belongs to this FileSystem.
   * 
   * If the path is fully qualified URI, then its scheme and authority
   * matches that of this file system. Otherwise the path must be 
   * slash-relative name.
   * @param path the path.
   * @throws InvalidPathException if the path is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,write,org.apache.hadoop.fs.FileStatus:write(java.io.DataOutput),530,537,"/**
 * Writes the object to a DataOutput stream as a serialized proto.
 * @param out Output stream to write to.
 * @throws IOException if an I/O error occurs.
 */
","* Write instance encoded as protobuf to stream.
   * @param out Output stream
   * @see PBHelper#convert(FileStatus)
   * @deprecated Use the {@link PBHelper} and protobuf serialization directly.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,checkPutArguments,"org.apache.hadoop.fs.impl.AbstractMultipartUploader:checkPutArguments(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long)",114,124,"/**
 * Validates arguments for a part upload operation.
 * @param filePath Path to the file.
 * @param inputStream Input stream for the part.
 * @param partNumber Part number.
 * @param uploadId Upload handle.
 * @param lengthInBytes Length of the part in bytes.
 */
","* Check all the arguments to the
   * {@link MultipartUploader#putPart(UploadHandle, int, Path, InputStream, long)}
   * operation.
   * @param filePath Target path for upload (as {@link #startUpload(Path)}).
   * @param inputStream Data for this part. Implementations MUST close this
   * stream after reading in the data.
   * @param partNumber Index of the part relative to others.
   * @param uploadId Identifier from {@link #startUpload(Path)}.
   * @param lengthInBytes Target length to read from the stream.
   * @throws IllegalArgumentException invalid argument",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractMultipartUploader.java,abortUploadsUnderPath,org.apache.hadoop.fs.impl.AbstractMultipartUploader:abortUploadsUnderPath(org.apache.hadoop.fs.Path),132,138,"/**
 * Aborts all uploads under the given path. Returns a CompletableFuture.
 * @param path Path to check for uploads to abort.
 * @throws IOException if path validation fails.
 */
","* {@inheritDoc}.
   * @param path path to abort uploads under.
   * @return a future to -1.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,append,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",1452,1456,"/**
 * Appends data to a file. Throws an exception as append is not supported.
 * @param f Path to the file
 * @param bufferSize Buffer size for appending
 * @param progress Progressable object for tracking progress
 * @throws IOException if append operation is not supported
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path,boolean)",1498,1503,"/**
* Deletes a file or directory.
* @param f Path to delete.
* @param recursive If true, deletes directories recursively.
* @throws AccessControlException, IOException on failure.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,rename,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",1739,1745,"/**
* Renames a file or directory from src to dst.
* @param src Source path
* @param dst Destination path
* @throws AccessControlException, IOException
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:truncate(org.apache.hadoop.fs.Path,long)",1747,1750,"/**
 * Throws a read-only exception when truncate is called.
 * @param f The file to truncate (not used).
 * @param newLength The new length (not used).
 * @throws IOException if the file is read-only.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setOwner,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1752,1757,"/**
* Sets the owner of a file path, throws exceptions if invalid.
* @param f Path to the file.
* @param username Username of the new owner.
* @param groupname Groupname of the new owner.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setPermission,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1759,1764,"/**
* Sets the permission for a path.
* @param f the path to set permission on
* @param permission the new permission to set
* @throws AccessControlException, IOException on failure
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setReplication,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setReplication(org.apache.hadoop.fs.Path,short)",1766,1771,"/**
 * Sets replication factor for a path. Throws exception if read-only.
 * @param f Path to set replication for.
 * @param replication New replication factor.
 * @return false, as replication cannot be set.
 * @throws AccessControlException, IOException
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setTimes,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)",1773,1778,"/**
* Sets file modification and access times.
* @param f Path to file. Throws exceptions if not possible.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1800,1805,"/**
 * Throws an exception, preventing ACL modifications on the path.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1807,1812,"/**
 * Removes ACL entries from a path. Throws an exception as it's read-only.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path),1814,1818,"/**
 * Removes the default ACL for a path.
 * @param path The path to remove the ACL from.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeAcl(org.apache.hadoop.fs.Path),1820,1824,"/**
* Removes ACL entries for a given path.
* @param path The path for which to remove ACLs.
* @throws IOException if an I/O error occurs.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",1826,1830,"/**
 * Sets ACL for a path, throws exception if read-only.
 * @param path The path to set ACL on.
 * @param aclSpec The ACL entries to apply.
 * @throws IOException if the mount is read-only.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",1841,1846,"/**
 * Sets an extended attribute on a path. Throws exception if read-only.
 * @param path Path to set attribute on.
 * @param name Attribute name.
 * @param value Attribute value.
 * @param flag Attribute set flags.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1869,1873,"/**
 * Removes an extended attribute by name from a path.
 * @param path The path to remove the attribute from.
 * @param name The name of the attribute to remove.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1875,1880,"/**
 * Creates a snapshot of the given path.
 * @param path Path to snapshot.
 * @param snapshotName Snapshot name.
 * @throws IOException If snapshot creation fails.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1882,1887,"/**
* Renames a snapshot.
* @param path Snapshot path.
* @param snapshotOldName Old snapshot name.
* @param snapshotNewName New snapshot name.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1889,1894,"/**
* Deletes a snapshot at the given path.
* @param path Path to the snapshot.
* @param snapshotName Name of the snapshot to delete.
* @throws IOException if an I/O error occurs.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),1901,1905,"/**
* Enforces storage policy; throws exception if not writable.
* @param src Path to the source; must be a directory.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",1907,1912,"/**
* Sets storage policy for a path, throws exception if read-only.
* @param src The path to set the policy on.
* @param policyName The name of the storage policy.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),1914,1918,"/**
* Prevents unsetting a storage policy on a path.
* @param src The path where unsetting is attempted.
* @throws IOException if the operation is disallowed.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,delete,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path,boolean)",1044,1049,"/**
* Deletes a file or directory.
* @param f Path to delete.
* @param recursive If true, deletes directories recursively.
* @throws AccessControlException, IOException
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:truncate(org.apache.hadoop.fs.Path,long)",1308,1313,"/**
 * Truncates a file to the specified length. Throws exceptions if needed.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",1315,1321,"/**
 * Renames a file or directory from src to dst.
 * @param src Source path
 * @param dst Destination path
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSymlink,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1328,1332,"/**
 * Creates a symbolic link. Throws exception if link is on read-only mount.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setOwner,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1340,1345,"/**
 * Sets the owner of a file path, throwing exceptions on failure.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setPermission,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1347,1352,"/**
* Sets the permission for a given file path.
* @param f the file path
* @param permission the permission to set
* @throws AccessControlException, IOException
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setReplication,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setReplication(org.apache.hadoop.fs.Path,short)",1354,1359,"/**
* Attempts to set replication factor. Throws exception if read-only.
* @param f Path to set replication on. @param replication New value.
* @throws AccessControlException, IOException
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setTimes,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setTimes(org.apache.hadoop.fs.Path,long,long)",1361,1366,"/**
* Sets modification and access times on a file path.
* @param f The file path.
* @param mtime Modification time.
* @param atime Access time.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1374,1379,"/**
* Throws an exception, preventing ACL modifications on the path.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",1381,1386,"/**
 * Removes ACL entries from a path. Throws an exception as it's read-only.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeDefaultAcl(org.apache.hadoop.fs.Path),1388,1392,"/**
 * Removes the default ACL for a given path.
 * @param path The path for which to remove the ACL.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeAcl(org.apache.hadoop.fs.Path),1394,1398,"/**
* Removes ACL entries for a given path.
* @param path the path for which to remove ACLs
* @throws IOException if an I/O error occurs
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",1400,1404,"/**
* Sets ACL on a path, throws exception as it's read-only.
* @param path The path to set the ACL on.
* @param aclSpec The ACL entries to apply.
* @throws IOException If an I/O error occurs.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",1415,1420,"/**
 * Sets an extended attribute on a path.
 * @param path Path to set attribute on.
 * @param name Attribute name.
 * @param value Attribute value.
 * @param flag Set of flags.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",1443,1447,"/**
 * Removes an extended attribute by name from a path.
 * @param path Path to remove attribute from.
 * @param name Attribute name to remove.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1449,1454,"/**
 * Creates a snapshot of the given path.
 * @param path The path to snapshot.
 * @param snapshotName Snapshot name.
 * @throws IOException If snapshot creation fails.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1456,1461,"/**
 * Renames a snapshot.
 * @param path Path to snapshot.
 * @param snapshotOldName Old snapshot name.
 * @param snapshotNewName New snapshot name.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1463,1468,"/**
 * Deletes a snapshot at the given path.
 * @param path The path to the snapshot.
 * @param snapshotName The name of the snapshot to delete.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:satisfyStoragePolicy(org.apache.hadoop.fs.Path),1470,1473,"/**
* Throws an exception indicating storage policy satisfaction is not allowed.
* @param path The path being accessed.
* @throws IOException If storage policy satisfaction is not allowed.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",1475,1479,"/**
* Throws an exception; setting storage policy is not allowed.
* @param path The path.
* @param policyName The policy name.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createInternal,"org.apache.hadoop.fs.viewfs.ViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",340,364,"/**
 * Creates a data output stream.
 * @param f Path to create.
 * @return FSDataOutputStream object.
 * @throws IOException if creation fails.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createSymlink,"org.apache.hadoop.fs.viewfs.ViewFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",651,667,"/**
* Creates a symbolic link.
* @param target Link target.
* @param link Link path.
* @param createParent Creates parent directories if needed.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Sorter$LinkedSegmentsDescriptor:equals(java.lang.Object),3923,3929,"/**
 * Checks if this descriptor is equal to another LinkedSegmentsDescriptor.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,getCredentialEntry,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getCredentialEntry(java.lang.String),173,198,"/**
 * Retrieves a credential entry by alias.
 * @param alias credential alias
 * @return CredentialEntry object or null if not found
 * @throws IOException if an I/O error occurs
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,getAliases,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:getAliases(),206,226,"/**
 * Retrieves a list of aliases from the keystore.
 * @return List of aliases or an empty list if none exist.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,skip,org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:skip(long),291,299,"/**
 * Skips specified number of bytes.
 * @param n number of bytes to skip, capped by file length.
 * @return actual number of bytes skipped.
 */
","* Skips over and discards <code>n</code> bytes of data from the
     * input stream.
     *
     * The <code>skip</code> method skips over some smaller number of bytes
     * when reaching end of file before <code>n</code> bytes have been skipped.
     * The actual number of bytes skipped is returned.  If <code>n</code> is
     * negative, no bytes are skipped.
     *
     * @param      n   the number of bytes to be skipped.
     * @return     the actual number of bytes skipped.
     * @exception  IOException  if an I/O error occurs.
     *             ChecksumException if the chunk to skip to is corrupted",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getPermissions,org.apache.hadoop.fs.ftp.FTPFileSystem:getPermissions(org.apache.commons.net.ftp.FTPFile),455,461,"/**
 * Creates FsPermission object from FTPFile access types.
 * @param ftpFile The FTPFile containing access permissions.
 * @return FsPermission object representing file permissions.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,applyUMask,org.apache.hadoop.fs.permission.FsPermission:applyUMask(org.apache.hadoop.fs.permission.FsPermission),297,301,"/**
 * Applies a umask to the current permissions.
 * @param umask The umask to apply.
 * @return A new FsPermission with the umask applied.
 */
","* Apply a umask to this permission and return a new one.
   *
   * The umask is used by create, mkdir, and other Hadoop filesystem operations.
   * The mode argument for these operations is modified by removing the bits
   * which are set in the umask.  Thus, the umask limits the permissions which
   * newly created files and directories get.
   *
   * @param umask              The umask to use
   * 
   * @return                   The effective permission",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FSProtos$FsPermissionProto),38,41,"/**
 * Converts a FsPermissionProto to a FsPermission object.
 * @param proto the FsPermissionProto to convert
 * @return the equivalent FsPermission object
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getPermissions,org.apache.hadoop.fs.sftp.SFTPFileSystem:getPermissions(com.jcraft.jsch.ChannelSftp$LsEntry),305,307,"/**
 * Gets the file permissions from an LsEntry.
 * @param sftpFile Entry containing file attributes.
 * @return FsPermission object representing the permissions.
 */
","* Return file permission.
   *
   * @param sftpFile
   * @return file permission",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processPath,org.apache.hadoop.fs.FsShellPermissions$Chmod:processPath(org.apache.hadoop.fs.shell.PathData),98,110,"/**
 * Updates file permissions if they have changed.
 * @param item PathData object containing file information.
 * @throws IOException if an error occurs while setting permissions.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission:<init>(int),127,129,"/**
 * Constructs a FsPermission with the given numeric mode.
 * @param mode Numeric permission mode (0-17777)
 */
","* Construct by the given mode.
   *
   * octal mask is applied.
   *
   *<pre>
   *              before mask     after mask    file type   sticky bit
   *
   *    octal     100644            644         file          no
   *    decimal    33188            420
   *
   *    octal     101644           1644         file          yes
   *    decimal    33700           1420
   *
   *    octal      40644            644         directory     no
   *    decimal    16804            420
   *
   *    octal      41644           1644         directory     yes
   *    decimal    17316           1420
   *</pre>
   *
   * 100644 becomes 644 while 644 remains as 644
   *
   * @param mode Mode is supposed to come from the result of native stat() call.
   *             It contains complete permission information: rwxrwxrwx, sticky
   *             bit, whether it is a directory or a file, etc. Upon applying
   *             mask, only permission and sticky bit info will be kept because
   *             they are the only parts to be used for now.
   * @see #FsPermission(short mode)",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getDefault,org.apache.hadoop.fs.permission.FsPermission:getDefault(),416,418,"/**
 * Returns the default file system permission (00777).
 * Returns a new FsPermission object.
 */
","* Get the default permission for directory and symlink.
   * In previous versions, this default permission was also used to
   * create files, so files created end up with ugo+x permission.
   * See HADOOP-9155 for detail. 
   * Two new methods are added to solve this, please use 
   * {@link FsPermission#getDirDefault()} for directory, and use
   * {@link FsPermission#getFileDefault()} for file.
   * This method is kept for compatibility.
   *
   * @return Default FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getDirDefault,org.apache.hadoop.fs.permission.FsPermission:getDirDefault(),425,427,"/**
 * Returns the default directory permission (00777).
 */","* Get the default permission for directory.
   *
   * @return DirDefault FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getFileDefault,org.apache.hadoop.fs.permission.FsPermission:getFileDefault(),434,436,"/**
* Returns the default file permissions (00666).
*/
","* Get the default permission for file.
   *
   * @return FileDefault FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getCachePoolDefault,org.apache.hadoop.fs.permission.FsPermission:getCachePoolDefault(),443,445,"/**
 * Returns the default cache pool permission.
 * Returns a new FsPermission object with (00755).
 */
","* Get the default permission for cache pools.
   *
   * @return CachePoolDefault FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,valueOf,org.apache.hadoop.fs.permission.FsPermission:valueOf(java.lang.String),452,475,"/**
 * Parses a Unix symbolic permission string into an FsPermission.
 * @param unixSymbolicPermission Unix permission string (e.g., ""rwxrwxrwx"")
 * @return FsPermission object representing the parsed permission.
 */
","* Create a FsPermission from a Unix symbolic permission string
   * @param unixSymbolicPermission e.g. ""-rw-rw-rw-""
   * @return FsPermission.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission$ImmutableFsPermission:<init>(short),479,481,"/**
 * Constructs a new ImmutableFsPermission with the given permission value.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processOptions,org.apache.hadoop.fs.shell.AclCommands$SetfaclCommand:processOptions(java.util.LinkedList),188,248,"/**
 * Processes command-line options and validates arguments.
 * Parses arguments, checks for conflicting flags, and validates input.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,printAclEntriesForSingleScope,"org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:printAclEntriesForSingleScope(org.apache.hadoop.fs.permission.AclStatus,org.apache.hadoop.fs.permission.FsPermission,java.util.List)",112,126,"/**
 * Prints ACL entries for a scope, handling minimal/extended formats.
 * @param aclStatus ACL status object
 * @param fsPerm FileSystem permission
 * @param entries List of ACL entries to print
 */
","* Prints all the ACL entries in a single scope.
     * @param aclStatus AclStatus for the path
     * @param fsPerm FsPermission for the path
     * @param entries List<AclEntry> containing ACL entries of file",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclEntry.java,aclSpecToString,org.apache.hadoop.fs.permission.AclEntry:aclSpecToString(java.util.List),330,337,"/**
 * Converts a list of AclEntry objects to a comma-separated string.
 */","* Convert a List of AclEntries into a string - the reverse of parseAclSpec.
   * @param aclSpec List of AclEntries to convert
   * @return String representation of aclSpec",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,<init>,org.apache.hadoop.fs.permission.FsPermission:<init>(java.lang.String),148,150,"/**
 * Creates a FsPermission with the given mode string.
 * @param mode String representing file permissions (e.g., ""rwxrwxrwx"")
 */
","* Construct by given mode, either in octal or symbolic format.
   * @param mode mode as a string, either in octal or symbolic format
   * @throws IllegalArgumentException if <code>mode</code> is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShellPermissions.java,processOptions,org.apache.hadoop.fs.FsShellPermissions$Chmod:processOptions(java.util.LinkedList),81,96,"/**
* Parses command-line arguments and initializes the ChmodParser.
* @param args Command-line arguments to parse.
* @throws IOException if an I/O error occurs.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,read,org.apache.hadoop.fs.store.ByteBufferInputStream:read(),94,100,"/**
 * Reads a byte from the buffer. Returns -1 if no data is available.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,skip,org.apache.hadoop.fs.store.ByteBufferInputStream:skip(long),102,114,"/**
 * Skips bytes from the current position.
 * @param offset Number of bytes to skip.
 * @return New position after skipping.
 * @throws IOException if offset is negative or exceeds file size.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,mark,org.apache.hadoop.fs.store.ByteBufferInputStream:mark(int),140,145,"/**
 * Marks the current position in the byte buffer.
 * @param readlimit Unused parameter, kept for compatibility.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/ByteBufferInputStream.java,read,"org.apache.hadoop.fs.store.ByteBufferInputStream:read(byte[],int,int)",169,189,"/**
 * Reads up to 'length' bytes from the stream into 'b', starting at 'offset'.
 * Returns the number of bytes read, or -1 if EOF.
 */","* Read in data.
   * @param b destination buffer.
   * @param offset offset within the buffer.
   * @param length length of bytes to read.
   * @throws EOFException if the position is negative
   * @throws IndexOutOfBoundsException if there isn't space for the
   * amount of data requested.
   * @throws IllegalArgumentException other arguments are invalid.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$DiskBlock:startUpload(),887,897,"/**
 * Starts the upload process and returns BlockUploadData.
 * Closes and nulls the output stream.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$ByteArrayBlock:startUpload(),593,600,"/**
 * Starts the upload process, returning BlockUploadData with input stream.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,startUpload,org.apache.hadoop.fs.store.DataBlocks$ByteBufferBlockFactory$ByteBufferBlock:startUpload(),722,731,"/**
 * Starts a block upload.
 * Returns BlockUploadData with input stream for block data.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,close,org.apache.hadoop.fs.store.DataBlocks$DataBlock:close(),475,481,"/**
 * Closes the resource, logging closure and invoking innerClose.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,checkAndWriteSync,org.apache.hadoop.io.SequenceFile$Writer:checkAndWriteSync(),1445,1450,"/**
 * Checks if a sync is needed and performs it if so.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressedSize,org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:getCompressedSize(),170,173,"/**
 * Calculates the compressed size from the starting position.
 * @return The compressed size in bytes.
 */
","* Current size of compressed data.
       * 
       * @return
       * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getFileLength,org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:getFileLength(),507,512,"/**
 * Gets the file length. Caches the length for efficiency.
 * @return File length in bytes.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processPathArgument,org.apache.hadoop.fs.shell.Ls:processPathArgument(org.apache.hadoop.fs.shell.PathData),232,246,"/**
 * Processes a path argument, throwing an exception if EC policy is missing.
 * Recursively processes directories if dirRecurse is true.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,adjustColumnWidths,org.apache.hadoop.fs.shell.Ls:adjustColumnWidths(org.apache.hadoop.fs.shell.PathData[]),327,355,"/**
 * Adjusts column widths based on PathData items' stats.
 * Updates maxRepl, maxLen, maxOwner, maxGroup, maxEC.
 */","* Compute column widths and rebuild the format string
   * @param items to find the max field width for each column",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processPath,org.apache.hadoop.fs.shell.FsUsage$Du:processPath(org.apache.hadoop.fs.shell.PathData),219,230,"/**
 * Processes a PathData item, updates usage table with size info.
 * @param item PathData object containing file system and path.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getQuotaUsage,org.apache.hadoop.fs.FileSystem:getQuotaUsage(org.apache.hadoop.fs.Path),1952,1954,"/**
 * Gets quota usage for a file path.
 * @param f the file path
 * @return QuotaUsage object
 */
","Return the {@link QuotaUsage} of a given {@link Path}.
   * @param f path to use
   * @return the quota usage
   * @throws IOException IO failure",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getUsed,org.apache.hadoop.fs.FileSystem:getUsed(org.apache.hadoop.fs.Path),2730,2732,"/**
 * Gets the size of the content at the given path.
 * @param path Path to the content.
 * @return Size of the content in bytes.
 */
","* Return the total size of all files from a specified path.
   * @param path the path.
   * @throws IOException IO failure
   * @return the number of path content summary.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/AbstractLaunchableService.java,<init>,org.apache.hadoop.service.launcher.AbstractLaunchableService:<init>(java.lang.String),48,50,"/**
 * Constructs a new AbstractLaunchableService with the given name.
 */","* Construct an instance with the given name.
   *
   * @param name input name.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,<init>,org.apache.hadoop.service.CompositeService:<init>(java.lang.String),53,55,"/**
 * Constructs a CompositeService with the given name.
 * @param name The name of the composite service.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,<init>,org.apache.hadoop.util.JvmPauseMonitor:<init>(),70,72,"/**
 * Constructs a JvmPauseMonitor with the class name as the name.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/ServiceStateModel.java,enterState,org.apache.hadoop.service.ServiceStateModel:enterState(org.apache.hadoop.service.Service$STATE),113,119,"/**
 * Transitions the service to a new state, returning the old state.
 * @param proposed The desired new state.
 * @return The previous state of the service.
 */
","* Enter a state -thread safe.
   *
   * @param proposed proposed new state
   * @return the original state
   * @throws ServiceStateException if the transition is not permitted",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,instantiateService,org.apache.hadoop.service.launcher.ServiceLauncher:instantiateService(org.apache.hadoop.conf.Configuration),658,694,"/**
 * Instantiates a Service class using the provided configuration.
 * @param conf Configuration object for service instantiation.
 * @return Service instance, cast to type S.
 */
","* @return Instantiate the service defined in {@code serviceClassName}.
   *
   * Sets the {@code configuration} field
   * to the the value of {@code conf},
   * and the {@code service} field to the service created.
   *
   * @param conf configuration to use",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,"org.apache.hadoop.security.KDiag$KerberosDiagsFailure:<init>(java.lang.String,java.lang.Throwable,java.lang.String,java.lang.Object[])",1094,1098,"/**
 * Constructs a KerberosDiagsFailure with a cause and message.
 * @param category Failure category
 * @param throwable The underlying Throwable
 * @param message Failure message
 * @param args Arguments for message formatting
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,serviceStop,org.apache.hadoop.service.CompositeService:serviceStop(),128,136,"/**
 * Stops all services started by this component.
 * Calls superclass to complete service stop process.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/CompositeService.java,run,org.apache.hadoop.service.CompositeService$CompositeServiceShutdownHook:run(),184,187,"/**
* Stops the composite service gracefully.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,progressable,org.apache.hadoop.io.MapFile$Writer:progressable(org.apache.hadoop.util.Progressable),308,310,"/**
 * Sets the progressable object for the SequenceFile writer.
 * @param value Progressable object to track progress.
 * @return Writer.Option object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,valueClass,org.apache.hadoop.io.MapFile$Writer:valueClass(java.lang.Class),293,295,"/**
 * Sets the class of values to be written to the SequenceFile.
 * @param value The class of the values.
 * @return SequenceFile.Writer.Option object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BinaryComparable.java,equals,org.apache.hadoop.io.BinaryComparable:equals(java.lang.Object),74,82,"/**
 * Checks if this BinaryComparable object is equal to another.
 * @param other The object to compare to.
 * @return True if equal, false otherwise.
 */
",* Return true if bytes from {#getBytes()} match.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,hashCode,org.apache.hadoop.io.BytesWritable:hashCode(),207,210,"/**
 * Returns the hash code value for this object.
 * Delegates to the superclass's hashCode implementation.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,hashCode,org.apache.hadoop.io.Text:hashCode(),422,425,"/**
 * Returns the hash code value for this object. Delegates to super.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,hashCode,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:hashCode(),95,98,"/**
 * Returns the hash code, based on the token's hash code.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,set,"org.apache.hadoop.io.BytesWritable:set(byte[],int,int)",178,182,"/**
* Copies data to the internal byte array.
* @param newData Source byte array.
* @param offset Starting offset in newData.
* @param length Number of bytes to copy.
*/
","* Set the value to a copy of the given byte range.
   *
   * @param newData the new values to copy in
   * @param offset the offset in newData to start at
   * @param length the number of bytes to copy",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,readFields,org.apache.hadoop.io.BytesWritable:readFields(java.io.DataInput),184,189,"/**
 * Reads data from input stream, populating the object's data.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKey,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getKey(org.apache.hadoop.io.BytesWritable),1690,1694,"/**
 * Retrieves key length from BytesWritable.
 * @param key The BytesWritable object to process.
 * @return The length of the key.
 */
","* Copy the key into BytesWritable. The input BytesWritable will be
         * automatically resized to the actual key size.
         * 
         * @param key
         *          BytesWritable to hold the key.
         * @throws IOException raised on errors performing I/O.
         * @return the key into BytesWritable.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,list,org.apache.hadoop.fs.FileUtil:list(java.io.File),1619,1630,"/**
 * Lists files in a directory.
 * @param dir The directory to list.
 * @throws IOException if access is denied or an I/O error occurs.
 */
","* A wrapper for {@link File#list()}. This java.io API returns null
   * when a dir is not a directory or for any I/O error. Instead of having
   * null check everywhere File#list() is used, we will add utility API
   * to get around this problem. For the majority of cases where we prefer
   * an IOException to be thrown.
   * @param dir directory for which listing should be performed
   * @return list of file names or empty string list
   * @exception AccessDeniedException for unreadable directory
   * @exception IOException for invalid directory or for bad disk",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkAccessByFileMethods,org.apache.hadoop.util.DiskChecker:checkAccessByFileMethods(java.io.File),153,174,"/**
 * Checks if a directory has read, write, and execute permissions.
 * @param dir The directory to check.
 * @throws DiskErrorException if any permission check fails.
 */
","* Checks that the current running process can read, write, and execute the
   * given directory by using methods of the File object.
   * 
   * @param dir File to check
   * @throws DiskErrorException if dir is not readable, not writable, or not
   *   executable",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,mlock,"org.apache.hadoop.io.nativeio.NativeIO$POSIX:mlock(java.nio.ByteBuffer,long)",470,477,"/**
 * Locks a direct ByteBuffer in memory to prevent swapping.
 * @param buffer The direct ByteBuffer to lock.
 * @param len The number of bytes to lock.
 */
","* Locks the provided direct ByteBuffer into memory, preventing it from
     * swapping out. After a buffer is locked, future accesses will not incur
     * a page fault.
     * 
     * See the mlock(2) man page for more information.
     * 
     * @throws NativeIOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/SharedFileDescriptorFactory.java,create,"org.apache.hadoop.io.nativeio.SharedFileDescriptorFactory:create(java.lang.String,java.lang.String[])",74,100,"/**
 * Creates a SharedFileDescriptorFactory, throws IOException on failure.
 * @param prefix Prefix for file descriptor creation.
 * @param paths Array of paths to try.
 */
","* Create a new SharedFileDescriptorFactory.
   *
   * @param prefix       The prefix to prepend to all the file names created
   *                       by this factory.
   * @param paths        An array of paths to use.  We will try each path in 
   *                       succession, and return a factory using the first 
   *                       usable path.
   * @return             The factory.
   * @throws IOException If a factory could not be created for any reason.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getMemlockLimit,org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:getMemlockLimit(),285,287,"/**
 * Gets the maximum amount of memory a process can lock into RAM.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,writeChecksumChunks,"org.apache.hadoop.fs.FSOutputSummer:writeChecksumChunks(byte[],int,int)",212,228,"/**
* Writes checksummed chunks of data.
* @param b byte array to write
* @param off start offset in the array
* @param len number of bytes to write
* @throws IOException if an I/O error occurs
*/
","Generate checksums for the given data chunks and output chunks & checksums
   * to the underlying output stream.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,calculateChunkedSums,"org.apache.hadoop.util.DataChecksum:calculateChunkedSums(java.nio.ByteBuffer,java.nio.ByteBuffer)",534,564,"/**
 * Calculates chunked sums for data, using native or fallback methods.
 */","* Calculate checksums for the given data.
   * 
   * The 'mark' of the ByteBuffer parameters may be modified by this function,
   * but the position is maintained.
   * 
   * @param data the DirectByteBuffer pointing to the data to checksum.
   * @param checksums the DirectByteBuffer into which checksums will be
   *                  stored. Enough space must be available in this
   *                  buffer to put the checksums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,freeBuffers,org.apache.hadoop.crypto.CryptoInputStream:freeBuffers(),809,813,"/**
* Releases allocated buffers and cleans the buffer pool.
*/
",Forcibly free the direct buffers.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,unbuffer,org.apache.hadoop.crypto.CryptoInputStream:unbuffer(),853,858,"/**
 * Releases resources held by the buffer and decryptor pools.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BoundedByteArrayOutputStream.java,<init>,org.apache.hadoop.io.BoundedByteArrayOutputStream:<init>(int),45,47,"/**
 * Constructs a BoundedByteArrayOutputStream with specified capacity.
 * @param capacity Initial size of the underlying byte array.
 */
","* Create a BoundedByteArrayOutputStream with the specified
   * capacity
   * @param capacity The capacity of the underlying byte array",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,decodeFromUrlString,org.apache.hadoop.security.token.Token:decodeFromUrlString(java.lang.String),382,384,"/**
 * Decodes a URL string and writes the decoded value.
 * @param newValue The URL string to decode.
 * @throws IOException if an I/O error occurs during decoding.
 */
","* Decode the given url safe string into this token.
   * @param newValue the encoded string
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,getOutputBlocks,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:getOutputBlocks(org.apache.hadoop.io.erasurecode.ECBlockGroup),89,107,"/**
 * Extracts erased blocks from a block group.
 * @param blockGroup The block group to extract from.
 * @return Array of erased ECBlocks.
 */
","* Which blocks were erased ?
   * @param blockGroup blockGroup.
   * @return output blocks to recover",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteBufferDecodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",36,49,"/**
 * Initializes a ByteBufferDecodingState with provided decoding data.
 * @param decoder Erasure decoder
 * @param inputs Input buffers
 * @param outputs Output buffers
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/ByteArrayDecodingState.java,<init>,"org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState:<init>(org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder,byte[][],int[],byte[][])",37,52,"/**
 * Initializes a ByteArrayDecodingState with provided decoding data.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),38,41,"/**
 * Creates a RawErasureDecoder using the provided options.
 * @param coderOptions ErasureCoderOptions for decoder creation.
 * @return A RawErasureDecoder instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),38,41,"/**
 * Creates a RawErasureDecoder instance using the provided options.
 * @param coderOptions ErasureCoderOptions for decoder configuration
 * @return A RawErasureDecoder object
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),43,53,"/**
 * Constructs a RSLegacyRawDecoder with provided options.
 * Throws exception if units are invalid.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawEncoder.java,<init>,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,53,"/**
 * Initializes the RSLegacyRawEncoder with erasure coding options.
 * Computes the generating polynomial based on data/parity units.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeXORRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeXORRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),33,36,"/**
 * Creates a RawErasureEncoder using the provided options.
 * @param coderOptions Encoder options to use.
 * @return A RawErasureEncoder instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/NativeRSRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.NativeRSRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),33,36,"/**
 * Creates a RawErasureEncoder using the provided options.
 * @param coderOptions Encoder options to use.
 * @return A RawErasureEncoder instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState),39,62,"/**
 * Decodes data using XOR operation, skipping the erased index.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState),39,60,"/**
 * Encodes input data into the output buffer using XOR operations.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState),167,217,"/**
 * Decodes data, handling erased indexes and output buffers.
 * Uses caller-provided buffers where possible to avoid copying.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState),55,88,"/**
 * Encodes input data using Reed-Solomon coding.
 * Uses provided encoding state and generating polynomial.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferEncodingState),63,68,"/**
 * Encodes input data using Galois field tables.
 * @param encodingState State object containing inputs & outputs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),64,86,"/**
 * Decodes data using XOR operations based on the decoding state.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/XORRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.XORRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),62,85,"/**
 * Encodes data using XOR operations into the output buffer.
 * Uses provided encoding state for input/output buffers.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),111,165,"/**
 * Decodes data using provided or temporary buffers, adjusting parameters.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),90,128,"/**
 * Encodes data using Reed-Solomon encoding.
 * Uses encodingState for inputs, outputs, and offsets.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawEncoder.java,doEncode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawEncoder:doEncode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayEncodingState),70,79,"/**
 * Encodes input data using Galois Field tables.
 * @param encodingState State object containing input/output buffers.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,40,"/**
 * Creates a RawErasureDecoder instance using provided options.
 * @param coderOptions ErasureCoderOptions for decoder configuration
 * @return RawErasureDecoder instance
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RSRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,35,"/**
 * Creates a RawErasureEncoder instance using the given options.
 * @param coderOptions Encoder options to configure the encoder.
 * @return A RawErasureEncoder instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,processErasures,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:processErasures(int[]),117,140,"/**
 * Processes erasure indexes, initializes matrices & tables.
 * @param erasedIndexes array of erased unit indexes
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,skipToNextMarker,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextMarker(long,int)",217,257,"/**
 * Skips to the next marker in the compressed stream.
 * @param marker target marker value
 * @param markerBitLength marker's bit length
 * @return true if marker found, false otherwise.
 */
","* This method tries to find the marker (passed to it as the first parameter)
   * in the stream. It can find bit patterns of length &lt;= 63 bits.
   * Specifically this method is used in CBZip2InputStream to find the end of
   * block (EOB) delimiter in the stream, starting from the current position
   * of the stream. If marker is found, the stream position will be at the
   * byte containing the starting bit of the marker.
   * @param marker The bit pattern to be found in the stream
   * @param markerBitLength No of bits in the marker
   * @return true if the marker was found otherwise false
   * @throws IOException raised on errors performing I/O.
   * @throws IllegalArgumentException if marketBitLength is greater than 63",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsGetUByte,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetUByte(),660,662,"/**
 * Reads and returns the next unsigned byte as a character.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,bsGetInt,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:bsGetInt(),664,666,"/**
 * Reads an integer (4 bytes) from the bit stream.
 * @return The integer value read from the bit stream.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,getAndMoveToFrontDecode0,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getAndMoveToFrontDecode0(int),1006,1037,"/**
 * Decodes a value from the bit stream and moves the group to front.
 * @param groupNo group number to decode
 * @return decoded value
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,recvDecodingTables,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:recvDecodingTables(),709,788,"/**
 * Receives and initializes decoding tables from the compressed data.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,<init>,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:<init>(java.io.OutputStream,int)",627,643,"/**
 * Creates a CBZip2OutputStream writing to the given OutputStream.
 * @param out Output stream to write to.
 * @param blockSize Block size in 100k units (1-9).
 * @throws IOException If an I/O error occurs.
 */
","* Constructs a new <tt>CBZip2OutputStream</tt> with specified blocksize.
  *
  * <p>
  * <b>Attention: </b>The caller is resonsible to write the two BZip2 magic
  * bytes <tt>""BZ""</tt> to the specified stream prior to calling this
  * constructor.
  * </p>
  *
  *
  * @param out
  *            the destination stream.
  * @param blockSize
  *            the blockSize as 100k units.
  *
  * @throws IOException
  *             if an I/O error occurs in the specified stream.
  * @throws IllegalArgumentException
  *             if {@code (blockSize < 1) || (blockSize > 9)}
  * @throws NullPointerException
  *             if {@code out == null}.
  *
  * @see #MIN_BLOCKSIZE
  * @see #MAX_BLOCKSIZE",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,moveToFrontCodeAndSend,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:moveToFrontCodeAndSend(),1391,1395,"/**
* Moves code to front and sends MTF values.
* Writes original pointer, generates, and sends MTF data.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,blockSort,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:blockSort(),1605,1629,"/**
 * Sorts the data block, potentially randomizing it if needed.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockCompressorStream:<init>(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",69,71,"/**
 * Creates a BlockCompressorStream with default buffer size and compression level.
 * @param out The output stream to write compressed data.
 * @param compressor The compressor to use for compression.
 */
","* Create a {@link BlockCompressorStream} with given output-stream and 
   * compressor.
   * Use default of 512 as bufferSize and compressionOverhead of 
   * (1% of bufferSize + 12 bytes) =  18 bytes (zlib algorithm).
   * 
   * @param out stream
   * @param compressor compressor to be used",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,<init>,"org.apache.hadoop.io.compress.DecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",74,77,"/**
 * Constructs a DecompressorStream with default buffer size.
 * @param in Input stream to decompress.
 * @param decompressor Decompressor to use.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",48,51,"/**
 * Creates a BlockDecompressorStream with given input, decompressor, and buffer size.
 */
","* Create a {@link BlockDecompressorStream}.
   * 
   * @param in input stream
   * @param decompressor decompressor to use
   * @param bufferSize size of buffer
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,createInputStream,"org.apache.hadoop.io.compress.PassthroughCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",138,142,"/**
 * Creates a PassthroughDecompressorStream using the given input stream.
 * @param in Input stream to wrap.
 * @param decompressor Decompressor object (unused).
 * @return PassthroughDecompressorStream instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,read,"org.apache.hadoop.io.compress.DecompressorStream:read(byte[],int,int)",95,106,"/**
 * Reads up to {@code len} bytes from the stream into the buffer.
 * @param b buffer to read into, off offset, len number of bytes
 * @return number of bytes read, or -1 if end of stream reached
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockCompressorStream.java,write,"org.apache.hadoop.io.compress.BlockCompressorStream:write(byte[],int,int)",81,134,"/**
 * Writes data to the compressor, handling size limits and flushing.
 * @param b The byte array to write.
 * @param off Offset into the array.
 * @param len Number of bytes to write.
 * @throws IOException if stream is finished or an error occurs.
 */
","* Write the data provided to the compression codec, compressing no more
   * than the buffer size less the compression overhead as specified during
   * construction for each block.
   *
   * Each block contains the uncompressed length for the block, followed by
   * one or more length-prefixed blocks of compressed data.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor:<init>(),70,72,"/**
 * Default constructor. Initializes with the default stream size.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardDecompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardDecompressor$ZStandardDirectDecompressor:<init>(int),298,300,"/**
 * Creates a ZStandardDirectDecompressor with specified buffer size.
 * @param directBufferSize Size of the direct buffer to use.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,<init>,"org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(int,int)",90,92,"/**
* Constructs a ZStandardCompressor with specified level and buffer sizes.
*/
","* Creates a new compressor with the default compression level.
   * Compressed data will be generated in ZStandard format.
   * @param level level.
   * @param bufferSize bufferSize.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CodecPool.java,getCompressor,org.apache.hadoop.io.compress.CodecPool:getCompressor(org.apache.hadoop.io.compress.CompressionCodec),167,169,"/**
 * Gets a compressor for the given codec.
 * @param codec CompressionCodec to use.
 * @return A Compressor instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getDecompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:getDecompressor(),319,343,"/**
 * Gets a Decompressor from the CodecPool, resets it, and returns.
 * Returns null if no Codec is available.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodec.java,createOutputStreamWithCodecPool,"org.apache.hadoop.io.compress.CompressionCodec$Util:createOutputStreamWithCodecPool(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration,java.io.OutputStream)",128,143,"/**
 * Creates a CompressionOutputStream using the provided codec and compressor.
 * @param codec Compression codec to use
 * @param conf Hadoop configuration
 * @param out Output stream to compress
 * @return CompressionOutputStream
 */
","* Create an output stream with a codec taken from the global CodecPool.
     *
     * @param codec       The codec to use to create the output stream.
     * @param conf        The configuration to use if we need to create a new codec.
     * @param out         The output stream to wrap.
     * @return            The new output stream
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionOutputStream.java,close,org.apache.hadoop.io.compress.CompressionOutputStream:close(),61,75,"/**
 * Closes the resource, finishing processing and releasing resources.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Writer:close(),1422,1443,"/**
 * Closes the codec and output stream, releasing resources.
 */",Close the file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,returnCompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:returnCompressor(org.apache.hadoop.io.compress.Compressor),310,317,"/**
 * Returns a compressor to the CodecPool for reuse.
 * @param compressor The compressor to return.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodec.java,createInputStreamWithCodecPool,"org.apache.hadoop.io.compress.CompressionCodec$Util:createInputStreamWithCodecPool(org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.conf.Configuration,java.io.InputStream)",154,169,"/**
 * Creates a CompressionInputStream using the provided codec and stream.
 * @param codec Compression codec to use.
 * @param conf Hadoop configuration.
 * @param in Input stream to decompress.
 * @return CompressionInputStream or null if creation fails.
 */
","* Create an input stream with a codec taken from the global CodecPool.
     *
     * @param codec       The codec to use to create the input stream.
     * @param conf        The configuration to use if we need to create a new codec.
     * @param in          The input stream to wrap.
     * @return            The new input stream
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionInputStream.java,close,org.apache.hadoop.io.compress.CompressionInputStream:close(),65,75,"/**
 * Closes the input stream and returns the decompressor to the pool.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Reader:close(),2169,2188,"/**
 * Releases resources: decompressors, deserializers, and input stream.
 */",Close the file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,returnDecompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:returnDecompressor(org.apache.hadoop.io.compress.Decompressor),345,352,"/**
 * Returns a decompressor to the CodecPool for reuse.
 * @param decompressor The decompressor to return.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createCompressor,org.apache.hadoop.io.compress.GzipCodec:createCompressor(),62,67,"/**
 * Creates a compressor, using native Zlib if available.
 * @return A Compressor instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibCompressor,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibCompressor(org.apache.hadoop.conf.Configuration),109,113,"/**
 * Returns a ZlibCompressor, using native or built-in Zlib.
 * @param conf Hadoop configuration object
 * @return ZlibCompressor instance
 */
","* Return the appropriate implementation of the zlib compressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the zlib compressor.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibDirectDecompressor,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDirectDecompressor(org.apache.hadoop.conf.Configuration),144,147,"/**
 * Returns a ZlibDirectDecompressor if native Zlib is loaded, else null.
 * @param conf Hadoop configuration object.
 */
","* Return the appropriate implementation of the zlib direct decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the zlib decompressor.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createDirectDecompressor,org.apache.hadoop.io.compress.GzipCodec:createDirectDecompressor(),109,114,"/**
 * Creates a DirectDecompressor, using native Zlib if available, else null.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/ZlibFactory.java,getZlibDecompressor,org.apache.hadoop.io.compress.zlib.ZlibFactory:getZlibDecompressor(org.apache.hadoop.conf.Configuration),133,136,"/**
 * Gets a ZlibDecompressor, using native Zlib if available.
 * @param conf Hadoop configuration object.
 * @return ZlibDecompressor instance.
 */
","* Return the appropriate implementation of the zlib decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the zlib decompressor.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createDecompressor,org.apache.hadoop.io.compress.GzipCodec:createDecompressor(),95,100,"/**
 * Creates a Decompressor instance based on Zlib availability.
 * @return Decompressor instance (GzipZlibDecompressor or BuiltInGzipDecompressor)
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zlib/BuiltInGzipDecompressor.java,decompress,"org.apache.hadoop.io.compress.zlib.BuiltInGzipDecompressor:decompress(byte[],int,int)",189,243,"/**
 * Decompresses data from the input buffer into the output buffer.
 * @param b output buffer
 * @param off offset in output buffer
 * @param len maximum number of bytes to decompress
 * @return number of bytes decompressed
 * @throws IOException if an I/O error occurs
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,write,org.apache.hadoop.io.SequenceFile$Metadata:write(java.io.DataOutput),759,769,"/**
 * Writes the metadata to the DataOutput stream.
 * Writes the size of the metadata map, then each key-value pair.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,writeImpl,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:writeImpl(java.io.DataOutput),205,215,"/**
 * Writes the object to the output stream.
 * Writes version, owner, renewer, realUser, dates, and IDs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,write,org.apache.hadoop.security.token.Token:write(java.io.DataOutput),323,331,"/**
 * Writes the object to the DataOutput stream.
 * Writes identifier, password, kind, and service data.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,storeDelegationKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),333,345,"/**
 * Stores a delegation key by writing to SQL and cache.
 * @param key The delegation key to store.
 * @throws IOException If storage fails.
 */
","* Persists a DelegationKey into the SQL database. The delegation keyId
   * is expected to be unique and any duplicate key attempts will result
   * in an IOException.
   * @param key DelegationKey to persist into the SQL database.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,updateDelegationKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),351,363,"/**
* Updates a delegation key by writing it to the database and cache.
* @param key The delegation key to update.
* @throws IOException if updating the key fails.
*/
","* Updates an existing DelegationKey in the SQL database.
   * @param key Updated DelegationKey.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,addOrUpdateDelegationKey,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:addOrUpdateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey,boolean)",691,725,"/**
 * Adds or updates a delegation key in Zookeeper.
 * @param key The delegation key to add/update.
 * @param isUpdate True to update if exists, false otherwise.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/VIntWritable.java,readFields,org.apache.hadoop.io.VIntWritable:readFields(java.io.DataInput),49,52,"/**
 * Reads a VInt value from the input stream and assigns it to 'value'.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readStringSafely,"org.apache.hadoop.io.WritableUtils:readStringSafely(java.io.DataInput,int)",484,497,"/**
 * Reads a string safely from DataInput, up to maxLength.
 * @param in DataInput to read from
 * @param maxLength Max string length
 * @return Decoded string
 */
","* Read a string, but check it for sanity. The format consists of a vint
   * followed by the given number of bytes.
   * @param in the stream to read from
   * @param maxLength the largest acceptable length of the encoded string
   * @return the bytes as a string
   * @throws IOException if reading from the DataInput fails
   * @throws IllegalArgumentException if the encoded byte size for string 
             is negative or larger than maxSize. Only the vint is read.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readFields,org.apache.hadoop.io.Text:readFields(java.io.DataInput),346,350,"/**
 * Reads fields from the input stream.
 * Uses readVInt to determine the length to read.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readFields,"org.apache.hadoop.io.Text:readFields(java.io.DataInput,int)",352,362,"/**
 * Reads fields from input, validating length against maxLength.
 * @param in DataInput stream to read from.
 * @param maxLength Maximum allowed length of data to read.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,skip,org.apache.hadoop.io.Text:skip(java.io.DataInput),369,372,"/**
* Skips a variable-length integer's worth of data from the input.
* @param in DataInput stream to skip data from.
*/
","* Skips over one Text in the input.
   * @param in input in.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readBuffer,"org.apache.hadoop.io.SequenceFile$Reader:readBuffer(org.apache.hadoop.io.DataInputBuffer,org.apache.hadoop.io.compress.CompressionInputStream)",2274,2291,"/**
 * Reads data from input stream into buffer and resets codec.
 * @param buffer DataInputBuffer to populate.
 * @param filter CompressionInputStream to reset.
 */",Read a compressed buffer,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,readFields,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenInformation:readFields(java.io.DataInput),749,758,"/**
 * Reads fields from DataInput, populating object's data.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readString,"org.apache.hadoop.io.Text:readString(java.io.DataInput,int)",566,572,"/**
 * Reads a string from the input stream.
 * @param in DataInput to read from
 * @param maxLength Max string length
 * @return Decoded string
 */
","* @return Read a UTF8 encoded string with a maximum size.
   * @param in input datainput.
   * @param maxLength input maxLength.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/DelegationKey.java,readFields,org.apache.hadoop.security.token.delegation.DelegationKey:readFields(java.io.DataInput),108,119,"/**
 * Reads fields from a DataInput stream.
 * keyId, expiryDate are read; keyBytes read conditionally.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Utils.java,writeString,"org.apache.hadoop.io.file.tfile.Utils:writeString(java.io.DataOutput,java.lang.String)",256,266,"/**
 * Writes a string to the DataOutput. Writes -1 if string is null.
 */","* Write a String as a VInt n, followed by n Bytes as in Text format.
   * 
   * @param out out.
   * @param s s.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,getCredentialEntry,org.apache.hadoop.security.alias.UserProvider:getCredentialEntry(java.lang.String),54,62,"/**
 * Retrieves a CredentialEntry by its alias.
 * @param alias credential alias
 * @return CredentialEntry object or null if not found
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,createCredentialEntry,"org.apache.hadoop.security.alias.UserProvider:createCredentialEntry(java.lang.String,char[])",64,75,"/**
* Creates a new credential entry with the given name and credential.
* @param name Credential name.
* @param credential Credential characters.
* @throws IOException if credential already exists.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,deleteCredentialEntry,org.apache.hadoop.security.alias.UserProvider:deleteCredentialEntry(java.lang.String),77,87,"/**
 * Deletes a credential entry by name.
 * @param name Credential name to delete.
 * @throws IOException if credential does not exist.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,buildTokenService,org.apache.hadoop.security.SecurityUtil:buildTokenService(java.net.InetSocketAddress),474,487,"/**
 * Creates a Text object representing the token service address.
 * @param addr InetSocketAddress of the token service
 * @return Text object with host:port string
 */
","* Construct the service key for a token
   * @param addr InetSocketAddress of remote connection with a token
   * @return ""ip:port"" or ""host:port"" depending on the value of
   *          hadoop.security.token.service.use_ip",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.UserProvider:getKeyVersion(java.lang.String),58,66,"/**
 * Retrieves a KeyVersion by version name.
 * @param versionName Version name to retrieve.
 * @return KeyVersion object or null if not found.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getMetadata,org.apache.hadoop.crypto.key.UserProvider:getMetadata(java.lang.String),68,80,"/**
 * Retrieves metadata by name, caching the result.
 * @param name Metadata name.
 * @return Metadata object or null if not found.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,createKey,"org.apache.hadoop.crypto.key.UserProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",82,100,"/**
 * Creates a new key with the given name, material, and options.
 * @param name Key name
 * @param material Key material
 * @param options Key options
 * @return KeyVersion object
 * @throws IOException if key already exists or length is wrong
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getDtService,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDtService(java.net.URI),430,441,"/**
 * Creates a Text object representing the URI, removing fragment.
 * @param uri The URI to convert.
 * @return Text object representing the URI.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,tokenFromProto,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:tokenFromProto(org.apache.hadoop.security.proto.SecurityProtos$TokenProto),123,131,"/**
 * Creates a Token object from a TokenProto.
 * @param tokenProto TokenProto object to create Token from.
 * @return Token object.
 */
","* Create a hadoop token from a protobuf token.
   * @param tokenProto token
   * @return a new token",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,find,org.apache.hadoop.io.Text:find(java.lang.String),171,173,"/**
 * Finds the index of 'what' in the data structure.
 * Overloads the method to start search from index 0.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeEnum,"org.apache.hadoop.io.WritableUtils:writeEnum(java.io.DataOutput,java.lang.Enum)",432,435,"/**
 * Writes the enum's name as a string to the output stream.
 * @param out DataOutput to write to
 * @param enumVal Enum value to write
 */
","* writes String value of enum to DataOutput. 
   * @param out Dataoutput stream
   * @param enumVal enum value
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,write,"org.apache.hadoop.fs.permission.PermissionStatus:write(java.io.DataOutput,java.lang.String,java.lang.String,org.apache.hadoop.fs.permission.FsPermission)",128,135,"/**
 * Writes username, groupname, and permission to the output stream.
 * @param out DataOutput to write to
 * @param username Username string
 * @param groupname Groupname string
 * @param permission FsPermission object
 */
","* Serialize a {@link PermissionStatus} from its base components.
   * @param out out.
   * @param username username.
   * @param groupname groupname.
   * @param permission FsPermission.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text:<init>(byte[]),112,114,"/**
 * Constructs a Text object from a byte array.
 * @param utf8 The byte array representing UTF-8 encoded text.
 */
","* Construct from a byte array.
   *
   * @param utf8 input utf8.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text:<init>(org.apache.hadoop.io.Text),103,105,"/**
 * Constructs a new Text object by copying from an existing Text.
 */","* Construct from another text.
   * @param utf8 input utf8.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readLine,"org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text,int,int)",180,187,"/**
 * Reads a line of text.
 * @param str Buffer to read into.
 * @param maxLineLength Max line length.
 * @param maxBytesToConsume Max bytes to consume.
 */
","* Read one line from the InputStream into the given Text.
   *
   * @param str the object to store the given line (without newline)
   * @param maxLineLength the maximum number of bytes to store into str;
   *  the rest of the line is silently discarded.
   * @param maxBytesToConsume the maximum number of bytes to consume
   *  in this call.  This is only a hint, because if the line cross
   *  this threshold, we allow it to happen.  It can overshoot
   *  potentially by as much as one buffer length.
   *
   * @return the number of bytes read including the (longest) newline
   * found.
   *
   * @throws IOException if the underlying stream throws",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,getTextLength,org.apache.hadoop.io.Text:getTextLength(),146,151,"/**
 * Returns the length of the text. Calculates if not already set.
 */
","* @return Returns the length of this text. The length is equal to the number of
   * Unicode code units in the text.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,toString,org.apache.hadoop.io.SequenceFile$Metadata:toString(),828,840,"/**
 * Returns a string representation of the metadata.
 * Iterates through entries and appends key-value pairs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,setRenewer,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:setRenewer(org.apache.hadoop.io.Text),105,116,"/**
 * Sets the renewer Text object, creating a new one if null.
 * @param renewer The renewer Text object to set.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,getAliases,org.apache.hadoop.security.alias.UserProvider:getAliases(),111,119,"/**
 * Retrieves a list of aliases (secret keys) as strings.
 * @return List of aliases, or an empty list if none exist.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getCanonicalServiceName,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getCanonicalServiceName(),1020,1023,"/**
 * Returns the canonical service name as a String.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getCanonicalServiceName,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCanonicalServiceName(),246,249,"/**
 * Returns the canonical service name as a String.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,<init>,org.apache.hadoop.io.SortedMapWritable:<init>(org.apache.hadoop.io.SortedMapWritable),55,58,"/**
* Constructs a new SortedMapWritable by copying from another.
* @param other The SortedMapWritable to copy from.
*/
","* Copy constructor.
   * 
   * @param other the map to copy from",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,<init>,org.apache.hadoop.io.MapWritable:<init>(org.apache.hadoop.io.MapWritable),53,56,"/**
 * Creates a new MapWritable by copying another MapWritable.
 * @param other The MapWritable to copy from.
 */
","* Copy constructor.
   * 
   * @param other the map to copy from",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,compression,org.apache.hadoop.io.MapFile$Writer:compression(org.apache.hadoop.io.SequenceFile$CompressionType),297,300,"/**
 * Sets the compression type for the SequenceFile writer.
 * @param type CompressionType to use.
 * @return Option object for chaining.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/JavaSerializationComparator.java,<init>,org.apache.hadoop.io.serializer.JavaSerializationComparator:<init>(),42,45,"/**
 * Constructs a JavaSerializationComparator using a deserializer.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,retryUpToMaximumTimeWithFixedSleep,"org.apache.hadoop.io.retry.RetryPolicies:retryUpToMaximumTimeWithFixedSleep(long,long,java.util.concurrent.TimeUnit)",114,116,"/**
 * Creates a RetryPolicy with max time and fixed sleep duration.
 * @param maxTime Max time to retry, in specified time unit.
 * @param sleepTime Sleep time between retries.
 * @param timeUnit Time unit for maxTime and sleepTime.
 * @return RetryPolicy instance.
 */
","* <p>
   * Keep trying for a maximum time, waiting a fixed time between attempts,
   * and then fail by re-throwing the exception.
   * </p>
   *
   * @param timeUnit timeUnit.
   * @param sleepTime sleepTime.
   * @param maxTime maxTime.
   * @return RetryPolicy.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,run,org.apache.hadoop.security.UserGroupInformation$AutoRenewalForUserCredsRunnable:run(),977,1053,"/**
 * Continuously renews Kerberos ticket until the renewal loop stops.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,"org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(org.apache.hadoop.io.retry.RetryPolicy,int)",205,208,"/**
 * Creates a failover RetryPolicy with a fallback and maxFailovers.
 * @param fallbackPolicy Fallback policy to use on network errors
 * @param maxFailovers Max number of failover attempts.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,newCall,"org.apache.hadoop.io.retry.RetryInvocationHandler:newCall(java.lang.reflect.Method,java.lang.Object[],boolean,int)",349,356,"/**
 * Creates a new Call object based on async mode.
 * @param method Method to be invoked.
 * @param args Arguments for the method.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,<init>,"org.apache.hadoop.util.InstrumentedWriteLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long,org.apache.hadoop.util.Timer)",50,57,"/**
 * Constructs an InstrumentedWriteLock with provided dependencies.
 * @param name Lock name, logger, lock, gap, threshold, clock.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,<init>,"org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.Lock,long,long)",81,85,"/**
 * Constructs an InstrumentedLock with a default Timer.
 * @param name Lock name, logger, lock, gap, threshold.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,<init>,"org.apache.hadoop.util.InstrumentedReadLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long,org.apache.hadoop.util.Timer)",56,63,"/**
 * Constructs an InstrumentedReadLock with provided dependencies.
 * @param name Lock name, logger, lock, timer for instrumentation.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,tryLock,org.apache.hadoop.util.InstrumentedLock:tryLock(),117,124,"/**
 * Attempts to acquire the lock. Returns true if successful.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",58,65,"/**
 * Creates a dynamic proxy instance for the given interface.
 * @param iface Interface to proxy, proxyProvider & retryPolicy used.
 * @return Proxy instance of the specified interface.
 */
","* Create a proxy for an interface of implementations of that interface using
   * the given {@link FailoverProxyProvider} and the same retry policy for each
   * method in the interface.
   * 
   * @param iface the interface that the retry will implement
   * @param proxyProvider provides implementation instances whose methods should be retried
   * @param retryPolicy the policy for retrying or failing over method call failures
   * @param <T> T.
   * @return the retry proxy",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/LossyRetryInvocationHandler.java,<init>,"org.apache.hadoop.io.retry.LossyRetryInvocationHandler:<init>(int,org.apache.hadoop.io.retry.FailoverProxyProvider,org.apache.hadoop.io.retry.RetryPolicy)",35,39,"/**
 * Constructs a LossyRetryInvocationHandler.
 * @param numToDrop Number of retries to drop.
 * @param proxyProvider Provider for failover proxies.
 * @param retryPolicy Policy for retry attempts.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,java.lang.Object,java.util.Map)",79,85,"/**
 * Creates a proxy object for the given interface and implementation.
 * @param iface Interface to proxy.
 * @param implementation Implementation of the interface.
 * @param methodNameToPolicyMap Retry policies for methods.
 * @return Proxy object.
 */
","* Create a proxy for an interface of an implementation class
   * using the a set of retry policies specified by method name.
   * If no retry policy is defined for a method then a default of
   * {@link RetryPolicies#TRY_ONCE_THEN_FAIL} is used.
   * 
   * @param iface the interface that the retry will implement
   * @param <T> T.
   * @param implementation the instance whose methods should be retried
   * @param methodNameToPolicyMap a map of method names to retry policies
   * @return the retry proxy",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,processRetryInfo,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processRetryInfo(),151,159,"/**
 * Processes retry information, increments counters, and handles failovers.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,handleException,"org.apache.hadoop.io.retry.RetryInvocationHandler:handleException(java.lang.reflect.Method,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.io.retry.RetryInvocationHandler$Counters,long,java.lang.Exception)",376,396,"/**
 * Handles exceptions, determines retry status, and returns RetryInfo.
 * @param method Method being invoked
 * @return RetryInfo object containing retry details
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,write,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(int),327,340,"/**
 * Writes an integer to each output stream; handles exceptions.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,write,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:write(byte[],int,int)",348,361,"/**
 * Writes bytes to multiple output streams, handling potential IOExceptions.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,flush,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:flush(),363,376,"/**
 * Flushes all output streams, handling and potentially throwing exceptions.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProxyCombiner.java,close,org.apache.hadoop.ipc.ProxyCombiner$CombinedProxyInvocationHandler:close(),133,149,"/**
 * Closes all resources managed by this object, throwing a combined IOException.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Writer:<init>(org.apache.hadoop.fs.FSDataOutputStream,java.lang.String,org.apache.hadoop.conf.Configuration)",285,297,"/**
 * Initializes a Writer with an output stream, compression name, and config.
 */","* Constructor
     * 
     * @param fout
     *          FS output stream.
     * @param compressionName
     *          Name of the compression algorithm, which will be used for all
     *          data blocks.
     * @throws IOException
     * @see Compression#getSupportedAlgorithms",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:<init>(java.io.DataInput),809,822,"/**
 * Constructs a MetaIndexEntry from a DataInput.
 * @param in Input stream to read entry data from.
 * @throws IOException if input is corrupted.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:<init>(java.io.DataInput),2060,2068,"/**
 * Constructs a TFileMeta from a DataInput, validating file version.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$DataIndex:<init>(java.io.DataInput),864,875,"/**
 * Constructs a DataIndex from a DataInput stream.
 * Reads compression algorithm and block regions.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$TFileIndex:<init>(int,java.io.DataInput,org.apache.hadoop.io.file.tfile.CompareUtils$BytesComparator)",2145,2179,"/**
 * Constructs a TFileIndex with entries from the input stream.
 * @param entryCount Number of entries to read.
 * @param in Input stream for reading index data.
 * @param comparator Comparator for byte array comparison.
 */
","* For reading from file.
     * 
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,checkEOF,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:checkEOF(),119,126,"/**
 * Checks for end-of-file. Returns true if EOF is reached.
 */","* Check whether we reach the end of the stream.
     * 
     * @return false if the chunk encoded stream has more data to read (in which
     *         case available() will be greater than 0); true otherwise.
     * @throws java.io.IOException
     *           on I/O errors.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,flushBuffer,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:flushBuffer(),296,301,"/**
 * Flushes the internal buffer to the underlying output stream.
 */","* Flush the internal buffer.
     * 
     * Is this the last call to flushBuffer?
     * 
     * @throws java.io.IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,close,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:close(),338,348,"/**
 * Closes the stream, flushing any buffered data.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,"org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(byte[],int,int)",316,330,"/**
 * Writes data to the buffer. If full, flushes and writes directly.
 * @param b byte array to write
 * @param off offset in the array
 * @param len number of bytes to write
 * @throws IOException if an I/O error occurs
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,write,org.apache.hadoop.io.file.tfile.TFile$TFileIndex:write(java.io.DataOutput),2272,2290,"/**
 * Writes the index to the provided DataOutput.
 * @param out DataOutput to write to.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(org.apache.hadoop.io.file.tfile.RawComparable),1957,1961,"/**
 * Compares this key with another key using the reader.
 * @param key The key to compare to.
 * @return An integer indicating the comparison result.
 */
","* Compare an entry with a RawComparable object. This is useful when
         * Entries are stored in a collection, and we want to compare a user
         * supplied key.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Writer$KeyRegister:close(),432,473,"/**
 * Closes the current key buffer and writes it to the block appender.
 * Updates key buffers and checks for sorted order.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,<init>,org.apache.hadoop.io.UTF8:<init>(java.lang.String),70,72,"/**
 * Initializes a UTF8 object with the given string.
 * @param string The string to be encoded.
 */
","* Construct from a given string.
   * @param string input string.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/MD5MD5CRC32FileChecksum.java,getBytes,org.apache.hadoop.fs.MD5MD5CRC32FileChecksum:getBytes(),80,83,"/**
 * Converts the Writable object to a byte array.
 * Uses WritableUtils.toByteArray() for conversion.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,digest,org.apache.hadoop.io.MD5Hash:digest(java.lang.String),186,188,"/**
 * Calculates the MD5 hash of a string.
 * @param string The string to hash.
 * @return MD5Hash object representing the hash.
 */
","* Construct a hash value for a String.
   * @param string string.
   * @return MD5Hash.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sync,org.apache.hadoop.io.SequenceFile$BlockCompressWriter:sync(),1638,1665,"/**
 * Synchronizes buffered records to the output stream.
 * Writes buffered keys/values and resets internal buffers.
 */",Compress and flush contents to dfs,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/TokenIdentifier.java,getTrackingId,org.apache.hadoop.security.token.TokenIdentifier:getTrackingId(),78,83,"/**
 * Returns the tracking ID. Generates if null.
 * @return The tracking ID string.
 */
","* Returns a tracking identifier that can be used to associate usages of a
   * token across multiple client sessions.
   *
   * Currently, this function just returns an MD5 of {{@link #getBytes()}.
   *
   * @return tracking identifier",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,encodeToUrlString,org.apache.hadoop.security.token.Token:encodeToUrlString(),373,375,"/**
 * Encodes the object to a URL-safe string.
 * @return URL-encoded string representation.
 */
","* Encode this token as a url safe string.
   * @return the encoded string
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,cloneWritableInto,"org.apache.hadoop.util.ReflectionUtils:cloneWritableInto(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",363,371,"/**
 * Copies data from src Writable to dst Writable.
 * @param dst Destination Writable object.
 * @param src Source Writable object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,hashCode,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:hashCode(),162,166,"/**
* Returns the hash code for this object. Satisfies findbugs.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,add,org.apache.hadoop.net.NetworkTopology:add(org.apache.hadoop.net.Node),133,169,"/**
 * Adds a node to the topology. Throws IllegalArgumentException if invalid.
 * @param node The node to add.
 */
","Add a leaf node
   * Update node counter &amp; rack counter if necessary
   * @param node node to be added; can be null
   * @exception IllegalArgumentException if add a node to a leave 
                                         or node to be added is not a leaf",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,"org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.lang.String,java.util.Collection)",497,554,"/**
 * Chooses a random node based on scope, exclusions, and availability.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,remove,org.apache.hadoop.net.NetworkTopology:remove(org.apache.hadoop.net.Node),221,241,"/**
 * Removes a node from the cluster map. Throws exception for inner nodes.
 * @param node The node to remove.
 */
","Remove a node
   * Update node counter and rack counter if necessary
   * @param node node to be removed; can be null",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,decommissionNode,org.apache.hadoop.net.NetworkTopology:decommissionNode(org.apache.hadoop.net.Node),1061,1076,"/**
* Decommissions a node, adding it to the decommissioning list.
* @param node The node to decommission.
*/
","* Update empty rack number when remove a node like decommission.
   * @param node node to be added; can be null",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistance,"org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer)",912,915,"/**
 * Sorts nodes by distance from a reader, then applies secondary sort.
 * @param reader The reference node for distance calculation.
 * @param nodes Array of nodes to sort.
 * @param activeLen Length of the active portion of the nodes array.
 * @param secondarySort Consumer to apply after distance sorting.
 */
","* Sort nodes array by network distance to <i>reader</i> with secondary sort.
   * <p>
   * In a three-level topology, a node can be either local, on the same rack,
   * or on a different rack from the reader. Sorting the nodes based on network
   * distance from the reader reduces network traffic and improves
   * performance.
   * </p>
   * As an additional twist, we also randomize the nodes at each network
   * distance. This helps with load balancing when there is data skew.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array
   * @param secondarySort a secondary sorting strategy which can inject into
   *     that point from outside to help sort the same distance.
   * @param <T> Generics Type T",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistanceUsingNetworkLocation,"org.apache.hadoop.net.NetworkTopology:sortByDistanceUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int,java.util.function.Consumer)",953,956,"/**
 * Sorts nodes by distance from a reader, using a secondary sort.
 * @param reader The starting node.
 * @param nodes Array of nodes to sort.
 * @param activeLen Length of the active portion of the array.
 * @param secondarySort Consumer for secondary sorting.
 */
","* Sort nodes array by network distance to <i>reader</i>.
   * <p> using network location. This is used when the reader
   * is not a datanode. Sorting the nodes based on network distance
   * from the reader reduces network traffic and improves
   * performance.
   * </p>
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array
   * @param secondarySort a secondary sorting strategy which can inject into
   *     that point from outside to help sort the same distance.
   * @param <T> Generics Type T.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,"org.apache.hadoop.net.SocketInputStream:<init>(java.nio.channels.ReadableByteChannel,long)",72,76,"/**
 * Creates a SocketInputStream with a timeout for reading.
 * @param channel ReadableByteChannel to read from.
 * @param timeout Timeout in milliseconds.
 * @throws IOException if an I/O error occurs.
 */
","* Create a new input stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @param channel 
   *        Channel for reading, should also be a {@link SelectableChannel}.
   *        The channel will be configured to be non-blocking.
   * @param timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,<init>,"org.apache.hadoop.net.SocketOutputStream:<init>(java.nio.channels.WritableByteChannel,long)",77,81,"/**
 * Creates a SocketOutputStream with a specified timeout.
 * @param channel WritableByteChannel to write to.
 * @param timeout Timeout in milliseconds.
 */
","* Create a new ouput stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @param channel 
   *        Channel for writing, should also be a {@link SelectableChannel}.  
   *        The channel will be configured to be non-blocking.
   * @param timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,createSocket,org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:createSocket(),184,196,"/**
 * Creates a DatagramSocket and initial packet to connect to server.
 * @throws IOException if socket creation fails.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getRpcResponse,"org.apache.hadoop.ipc.Client:getRpcResponse(org.apache.hadoop.ipc.Client$Call,org.apache.hadoop.ipc.Client$Connection,long,java.util.concurrent.TimeUnit)",1566,1598,"/**
 * Retrieves RPC response from a call, waiting up to timeout.
 * @param call The RPC call object.
 * @return Writable RPC response or null on timeout/error.
 */
","@return the rpc response or, in case of timeout, null.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping:<init>(),87,89,"/**
 * Default constructor, uses RawScriptBasedMapping internally.
 */
","* Create an instance with the default configuration.
   * <p>
   * Calling {@link #setConf(Configuration)} will trigger a
   * re-evaluation of the configuration settings and so be used to
   * set up the mapping script.
   *",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,<init>,org.apache.hadoop.net.ScriptBasedMappingWithDependency:<init>(),59,61,"/**
 * Constructs a ScriptBasedMappingWithDependency, using a RawScriptBasedMapping.
 */","* Create an instance with the default configuration.
   * <p>
   * Calling {@link #setConf(Configuration)} will trigger a
   * re-evaluation of the configuration settings and so be used to
   * set up the mapping script.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,newInnerNode,org.apache.hadoop.net.InnerNodeImpl$Factory:newInnerNode(java.lang.String),32,35,"/**
 * Creates a new inner node with the given path.
 * @param path The path for the new inner node.
 * @return A new InnerNodeImpl instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,<init>,org.apache.hadoop.net.NetworkTopologyWithNodeGroup$InnerNodeWithNodeGroup:<init>(java.lang.String),306,308,"/**
 * Constructs an InnerNodeWithNodeGroup with the given path.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:<init>(org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode),127,129,"/**
 * Constructs MRNflyNode using data from NflyNode.
 * @param n The NflyNode to copy data from.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,createParentNode,org.apache.hadoop.net.InnerNodeImpl:createParentNode(java.lang.String),184,187,"/**
 * Creates a parent node with the given name and updated path/level.
 */","* Creates a parent node to be added to the list of children.
   * Creates a node using the InnerNode four argument constructor specifying
   * the name, location, parent, and level of this node.
   *
   * <p>To be overridden in subclasses for specific InnerNode implementations,
   * as alternative to overriding the full {@link #add(Node)} method.
   *
   * @param parentName The name of the parent node
   * @return A new inner node
   * @see InnerNodeImpl(String, String, InnerNode, int)",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,add,"org.apache.hadoop.net.unix.DomainSocketWatcher:add(org.apache.hadoop.net.unix.DomainSocket,org.apache.hadoop.net.unix.DomainSocketWatcher$Handler)",304,332,"/**
 * Adds a domain socket and handler for processing. Handles closed sockets.
 */","* Add a socket.
   *
   * @param sock     The socket to add.  It is an error to re-add a socket that
   *                   we are already watching.
   * @param handler  The handler to associate with this socket.  This may be
   *                   called any time after this function is called.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,remove,org.apache.hadoop.net.unix.DomainSocketWatcher:remove(org.apache.hadoop.net.unix.DomainSocket),339,354,"/**
 * Removes a DomainSocket from the managed set, triggering processing.
 * @param sock The DomainSocket to remove.
 */
","* Remove a socket.  Its handler will be called.
   *
   * @param sock     The socket to remove.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/unix/DomainSocketWatcher.java,<init>,"org.apache.hadoop.net.unix.DomainSocketWatcher:<init>(int,java.lang.String)",241,259,"/**
 * Initializes a DomainSocketWatcher.
 * @param interruptCheckPeriodMs period for interrupt checks
 * @param src name of the watcher thread
 * @throws IOException if socketpair creation fails
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,select,"org.apache.hadoop.net.SocketIOWithTimeout$SelectorPool:select(java.nio.channels.SelectableChannel,int,long)",321,376,"/**
 * Selects on a channel with specified operations and timeout.
 * @param channel SelectableChannel to select on
 * @param ops Operations to monitor
 * @param timeout Timeout in milliseconds
 * @return Number of keys selected, or 0 if timeout
 */
","* Waits on the channel with the given timeout using one of the 
     * cached selectors. It also removes any cached selectors that are
     * idle for a few seconds.
     * 
     * @param channel
     * @param ops
     * @param timeout
     * @return
     * @throws IOException",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultIP,org.apache.hadoop.net.DNS:getDefaultIP(java.lang.String),224,228,"/**
 * Gets the default IP address for a given network interface.
 * @param strInterface Interface name, returns first IP.
 */
","* Returns the first available IP address associated with the provided
   * network interface or the local host IP if ""default"" is given.
   *
   * @param strInterface
   *            The name of the network interface or subinterface to query
   *             (e.g. eth0 or eth0:0) or the string ""default""
   * @return The IP address in text form, the local host IP is returned
   *         if the interface name ""default"" is specified
   * @throws UnknownHostException
   *             If the given interface is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getHosts,org.apache.hadoop.net.DNS:getHosts(java.lang.String),340,343,"/**
 * Gets hostnames for a given interface.
 * @param strInterface Interface to resolve hosts for.
 * @return Array of hostnames or empty array if none found.
 */
","* Returns all the host names associated by the default nameserver with the
   * address bound to the specified network interface
   * 
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0)
   * @return The list of host names associated with IPs bound to the network
   *         interface
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface
   *",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultHost,"org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String,java.lang.String,boolean)",360,374,"/**
 * Gets the default host based on interface, nameserver, and fallback.
 * @param strInterface Interface string, or null/default for default.
 * @param nameserver Nameserver string, or null/default.
 * @return Hostname string.
 */
","* Returns the default (first) host name associated by the provided
   * nameserver with the address bound to the specified network interface
   * 
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0)
   * @param nameserver
   *            The DNS host name
   * @param tryfallbackResolution
   *            Input tryfallbackResolution.
   * @return The default host names associated with IPs bound to the network
   *         interface
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,printUsage,org.apache.hadoop.ha.HAAdmin:printUsage(java.io.PrintStream),134,136,"/**
 * Prints usage information to the provided PrintStream.
 * Delegates to overloaded method with default usage.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkParameterValidity,"org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[],java.util.Map)",363,385,"/**
 * Checks if the command line arguments are valid.
 * @param argv Command line arguments.
 * @param helpEntries Map of command help information.
 * @return True if arguments are valid, false otherwise.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,help,"org.apache.hadoop.ha.HAAdmin:help(java.lang.String[],java.util.Map)",512,537,"/**
 * Displays help information based on command-line arguments.
 * @param argv Command-line arguments.
 * @param helpEntries Map of command to help information.
 * @return 0 on success, -1 on error.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addContext,"org.apache.hadoop.http.HttpServer2:addContext(org.eclipse.jetty.servlet.ServletContextHandler,boolean)",997,1001,"/**
 * Adds a ServletContextHandler to the handlers and default contexts.
 * @param ctxt Handler to add; @param isFiltered Filter flag.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getPlugin,org.apache.hadoop.metrics2.impl.MetricsConfig:getPlugin(java.lang.String),202,216,"/**
 * Retrieves a plugin of type T by name.
 * @param name Plugin name.
 * @return Plugin instance or null if not found.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,loadFirst,"org.apache.hadoop.metrics2.impl.MetricsConfig:loadFirst(java.lang.String,java.lang.String[])",113,142,"/**
 * Loads a MetricsConfig from the first found file.
 * @param prefix config prefix
 * @param fileNames file names to try
 * @return MetricsConfig object or default if none found
 */
","* Load configuration from a list of files until the first successful load
   * @param conf  the configuration object
   * @param files the list of filenames to try
   * @return  the configuration object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,toString,org.apache.hadoop.metrics2.impl.MetricsConfig:toString(),282,285,"/**
 * Returns a string representation of the object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,putMetrics,org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),108,193,"/**
 * Puts metrics record, handles dense or sparse metric publishing.
 * @param record MetricsRecord to be published.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/GraphiteSink.java,putMetrics,org.apache.hadoop.metrics2.sink.GraphiteSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),70,108,"/**
 * Sends a MetricsRecord to Graphite, constructing the metric path.
 * @param record The metrics record to send.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/PrometheusServlet.java,doGet,"org.apache.hadoop.http.PrometheusServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",40,46,"/**
 * Publishes metrics to Prometheus and flushes the response writer.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,publishMetricsFromQueue,org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:publishMetricsFromQueue(),128,166,"/**
 * Consumes metrics from the queue, retries on failure, stops when stopping is true.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,incrCacheClearedCounter,org.apache.hadoop.ipc.RetryCache:incrCacheClearedCounter(),221,223,"/**
 * Increments the cache cleared counter in the retry metrics.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,requeueCall,org.apache.hadoop.ipc.Server$Handler:requeueCall(org.apache.hadoop.ipc.Server$Call),3232,3240,"/**
 * Requeues a call, handling potential RpcServerExceptions.
 * @param call The call to requeue.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsCollectorImpl.java,getRecords,org.apache.hadoop.metrics2.impl.MetricsCollectorImpl:getRecords(),57,66,"/**
 * Retrieves a list of MetricsRecordImpl objects from the builders.
 * @return List of MetricsRecordImpl objects.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,int)",62,65,"/**
 * Records a metric value for a given info.
 * @param info MetricsInfo object
 * @param value Integer value to record
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,long)",67,70,"/**
 * Records a metric value for the given info.
 * @param info MetricsInfo object
 * @param value The value to record
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,float)",72,75,"/**
 * Records a metric value for the given info.
 * @param info MetricsInfo object
 * @param value The value to record
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,gauge,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:gauge(org.apache.hadoop.metrics2.MetricsInfo,double)",77,80,"/**
 * Records a metric value with associated information.
 * @param info MetricsInfo object describing the metric.
 * @param value The numeric value of the metric.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,counter,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:counter(org.apache.hadoop.metrics2.MetricsInfo,int)",82,85,"/**
 * Records a metric with the given info and value.
 * @param info MetricsInfo object
 * @param value Integer value to record
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MBeanInfoBuilder.java,counter,"org.apache.hadoop.metrics2.impl.MBeanInfoBuilder:counter(org.apache.hadoop.metrics2.MetricsInfo,long)",87,90,"/**
 * Records a metric with provided info and value as a Long.
 * @param info MetricsInfo object
 * @param value The value of the metric
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,updateInfoCache,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateInfoCache(java.lang.Iterable),243,248,"/**
 * Updates the info cache with data from the provided records.
 * @param lastRecs Iterable of MetricsRecordImpl objects.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,newMBeanName,org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:newMBeanName(java.lang.String),108,111,"/**
 * Creates a new ObjectName with the given name.
 * @param name The name of the ObjectName to create.
 * @return A new ObjectName instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/DefaultMetricsSystem.java,sourceName,"org.apache.hadoop.metrics2.lib.DefaultMetricsSystem:sourceName(java.lang.String,boolean)",123,126,"/**
 * Generates a source name.
 * @param name The base name.
 * @param dupOK Whether duplicates are acceptable.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,load,org.apache.hadoop.security.Groups$GroupCacheLoader:load(java.lang.String),342,370,"/**
 * Loads group names for a user.
 * @param user The user to fetch groups for.
 * @return Set of group names. Throws if no groups are found.
 */
","* This method will block if a cache entry doesn't exist, and
     * any subsequent requests for the same user will wait on this
     * request to return. If a user already exists in the cache,
     * and when the key expires, the first call to reload the key
     * will block, but subsequent requests will return the old
     * value until the blocking thread returns.
     * If reloadGroupsInBackground is true, then the thread that
     * needs to refresh an expired key will not block either. Instead
     * it will return the old cache value and schedule a background
     * refresh
     * @param user key of cache
     * @return List of groups belonging to user
     * @throws IOException to prevent caching negative entries",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,shutdownSingleton,org.apache.hadoop.metrics2.source.JvmMetrics:shutdownSingleton(),139,141,"/**
 * Shuts down the Singleton instance gracefully.
 */","* Shutdown the JvmMetrics singleton. This is not necessary if the JVM itself
   * is shutdown, but may be necessary for scenarios where JvmMetrics instance
   * needs to be re-created while the JVM is still around. One such scenario
   * is unit-testing.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reattach,org.apache.hadoop.security.UserGroupInformation$UgiMetrics:reattach(),151,153,"/**
 * Re-initializes the UgiMetrics instance.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,stop,org.apache.hadoop.ipc.Server:stop(),3696,3719,"/**
 * Stops the server, interrupts handlers, and shuts down associated components.
 */",Stops the service.  No new calls will be handled after this is called.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,stopMBeans,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:stopMBeans(),226,231,"/**
 * Unregisters the JMX MBean if registered. Resets mbeanName to null.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,unregisterSource,org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:unregisterSource(java.lang.String),896,902,"/**
 * Unregisters a metrics source and associated MBean.
 * @param namespace Source namespace to unregister.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2Metrics.java,create,"org.apache.hadoop.http.HttpServer2Metrics:create(org.eclipse.jetty.server.handler.StatisticsHandler,int)",151,159,"/**
 * Creates and registers an HttpServer2Metrics instance.
 * @param handler StatisticsHandler to use.
 * @param port Port number for metrics registration.
 * @return Registered HttpServer2Metrics object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,stop,org.apache.hadoop.http.HttpServer2:stop(),1559,1609,"/**
 * Stops the web application context, listeners, and server.
 * Handles exceptions and aggregates them into a MultiException.
 */","* stop the server.
   *
   * @throws Exception exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,getMetrics,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",569,581,"/**
 * Collects metrics and adds them to the provided collector.
 * @param builder MetricsCollector to add metrics to
 * @param all If true, collects all metrics, otherwise only active ones
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableQuantiles.java,<init>,"org.apache.hadoop.metrics2.lib.MutableQuantiles:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",87,106,"/**
 * Initializes a MutableQuantiles object with provided parameters.
 * @param interval The interval in seconds for data rollover.
 */","* Instantiates a new {@link MutableQuantiles} for a metric that rolls itself
   * over on the specified time interval.
   * 
   * @param name
   *          of the metric
   * @param description
   *          long-form textual description of the metric
   * @param sampleName
   *          type of items in the stream (e.g., ""Ops"")
   * @param valueName
   *          type of the values
   * @param interval
   *          rollover interval (in seconds) of the estimator",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:<init>(),993,1000,"/**
 * Initializes the metrics registry and IO statistics for tracking.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,<init>,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:<init>(org.apache.hadoop.ipc.RetryCache),42,48,"/**
 * Initializes RetryCacheMetrics with the given RetryCache.
 * @param retryCache The RetryCache instance to associate metrics with.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.reflect.Field)",129,131,"/**
 * Delegates to a more specific getInfo method with field name.
 * @param annotation Metric annotation
 * @param field Field to get info for
 * @return MetricsInfo object
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,getInfo,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:getInfo(org.apache.hadoop.metrics2.annotation.Metric,java.lang.reflect.Method)",137,139,"/**
 * Delegates to a more specific getInfo method.
 * @param annotation Metric annotation
 * @param method The method being annotated
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newStat,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newStat(java.lang.String,java.lang.String,java.lang.String,java.lang.String,boolean)",262,269,"/**
 * Creates a new MutableStat with given parameters and adds it.
 * @param name Stat name
 * @return The created MutableStat object
 */
","* Create a mutable metric with stats
   * @param name  of the metric
   * @param desc  metric description
   * @param sampleName  of the metric (e.g., ""Ops"")
   * @param valueName   of the metric (e.g., ""Time"" or ""Latency"")
   * @param extended    produce extended stat (stdev, min/max etc.) if true.
   * @return a new mutable stat metric object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,<init>,"org.apache.hadoop.metrics2.lib.MutableStat:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",94,97,"/**
 * Constructs a MutableStat with a flag set to false.
 * @param name Stat name.
 * @param description Stat description.
 * @param sampleName Sample name.
 * @param valueName Value name.
 */
","* Construct a snapshot stat metric with extended stat off by default
   * @param name        of the metric
   * @param description of the metric
   * @param sampleName  of the metric (e.g. ""Ops"")
   * @param valueName   of the metric (e.g. ""Time"", ""Latency"")",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRate.java,<init>,"org.apache.hadoop.metrics2.lib.MutableRate:<init>(java.lang.String,java.lang.String,boolean)",31,33,"/**
* Constructs a MutableRate object.
* @param name Rate name.
* @param description Rate description.
* @param extended Whether the rate is extended.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getGcUsage,org.apache.hadoop.metrics2.source.JvmMetrics:getGcUsage(org.apache.hadoop.metrics2.MetricsRecordBuilder),180,207,"/**
 * Records garbage collection usage metrics to the provided builder.
 * @param rb MetricsRecordBuilder to record GC usage data.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsRecordBuilderImpl.java,setContext,org.apache.hadoop.metrics2.impl.MetricsRecordBuilderImpl:setContext(java.lang.String),147,150,"/**
 * Sets the context value as a tag.
 * @param value The context value to set.
 * @return This builder instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,setContext,org.apache.hadoop.metrics2.lib.MetricsRegistry:setContext(java.lang.String),380,382,"/**
 * Sets the context for metrics.
 * @param name Context name to set.
 * @return MetricsRegistry with the context set.
 */
","* Set the metrics context tag
   * @param name of the context
   * @return the registry itself as a convenience",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(java.lang.String,java.lang.String,java.lang.String,boolean)",403,406,"/**
* Tags a metric with a name, description, and value.
* @param name Metric name.
* @param description Metric description.
* @param value Metric value.
* @param override Whether to override existing tags.
* @return MetricsRegistry object.
*/
","* Add a tag to the metrics
   * @param name  of the tag
   * @param description of the tag
   * @param value of the tag
   * @param override  existing tag if true
   * @return the registry (for keep adding tags)",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(org.apache.hadoop.metrics2.MetricsInfo,java.lang.String)",422,424,"/**
 * Tags a metric with the provided info and value.
 * @param info MetricsInfo object
 * @param value Tag value
 * @return This MetricsRegistry instance
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,add,"org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:add(java.lang.String,long)",100,114,"/**
 * Adds elapsed time to a sample statistic for the given name.
 * @param name Statistic name
 * @param elapsed Elapsed time in milliseconds
 */
","* Add a rate sample for a rate metric.
   * @param name of the rate metric
   * @param elapsed time",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,publishMetrics,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:publishMetrics(org.apache.hadoop.metrics2.impl.MetricsBuffer,boolean)",435,449,"/**
 * Publishes metrics from a buffer to registered sinks.
 * @param buffer MetricsBuffer to publish.
 * @param immediate If true, publishes immediately.
 */
","* Publish a metrics snapshot to all the sinks
   * @param buffer  the metrics snapshot to publish
   * @param immediate  indicates that we should publish metrics immediately
   *                   instead of using a separate thread.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleStat.java,copyTo,org.apache.hadoop.metrics2.util.SampleStat:copyTo(org.apache.hadoop.metrics2.util.SampleStat),59,61,"/**
* Copies the statistics to another SampleStat object.
*/
","* Copy the values to other (saves object creation and gc.)
   * @param other the destination to hold our values",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MethodMetric.java,<init>,"org.apache.hadoop.metrics2.lib.MethodMetric:<init>(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.MetricsInfo,org.apache.hadoop.metrics2.annotation.Metric$Type)",46,53,"/**
 * Constructs a MethodMetric with given object, method, info, and type.
 * @param obj The object on which the method is invoked.
 * @param method The method to be measured.
 * @param info Metrics info.
 * @param type Metric type.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,toString,org.apache.hadoop.metrics2.lib.MutableStat:toString(),187,190,"/**
 * Returns a string representation of the object, based on the last stat.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,logSlowRpcCalls,"org.apache.hadoop.ipc.Server:logSlowRpcCalls(java.lang.String,org.apache.hadoop.ipc.Server$Call,org.apache.hadoop.ipc.ProcessingDetails)",593,615,"/**
 * Logs slow RPC calls if processing time exceeds defined thresholds.
 * @param methodName RPC method name
 * @param call RPC call object
 * @param details Processing details including timing information
 */
","* Logs a Slow RPC Request.
   *
   * @param methodName - RPC Request method name
   * @param details - Processing Detail.
   *
   * If a request took significant more time than other requests,
   * and its processing time is at least `logSlowRPCThresholdMs` we consider that as a slow RPC.
   *
   * The definition rules for calculating whether the current request took too much time
   * compared to other requests are as follows:
   * 3 is a magic number that comes from 3 sigma deviation.
   * A very simple explanation can be found by searching for 68-95-99.7 rule.
   * We flag an RPC as slow RPC if and only if it falls above 99.7% of requests.
   * We start this logic only once we have enough sample size.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/SampleQuantiles.java,toString,org.apache.hadoop.metrics2.util.SampleQuantiles:toString(),280,288,"/**
 * Returns a string representation of the quantile snapshot.
 * Returns ""[no samples]"" if the snapshot is empty.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addTopNCallerSummary,org.apache.hadoop.ipc.DecayRpcScheduler:addTopNCallerSummary(org.apache.hadoop.metrics2.MetricsRecordBuilder),1079,1096,"/**
 * Adds top N caller summary to the metrics record builder.
 * @param rb MetricsRecordBuilder to add caller data to.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/JniBasedUnixGroupsNetgroupMapping.java,cacheGroupsRefresh,org.apache.hadoop.security.JniBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh(),78,83,"/**
 * Refreshes the netgroup cache by fetching and adding groups.
 */",* Refresh the netgroup cache,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroupsSet,org.apache.hadoop.security.UserGroupInformation$TestingGroups:getGroupsSet(java.lang.String),1585,1592,"/**
 * Retrieves a set of group names for a given user.
 * @param user The username to lookup.
 * @return Set of group names, or empty set if user not found.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,endln,org.apache.hadoop.security.KDiag:endln(),875,878,"/**
 * Prints a newline followed by a line of dashes (""-----"").
 */
",* Print something at the end of a section,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,title,"org.apache.hadoop.security.KDiag:title(java.lang.String,java.lang.Object[])",886,891,"/**
 * Prints a formatted title to the console with surrounding newlines.
 * @param format Format string for the title.
 * @param args Arguments to format into the title.
 */
","* Print a title entry.
   *
   * @param format format string
   * @param args any arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,fail,"org.apache.hadoop.security.KDiag:fail(java.lang.String,java.lang.String,java.lang.Object[])",942,946,"/**
* Logs an error and throws a KerberosDiagsFailure.
* @param category Error category.
* @param message Error message.
* @param args Arguments for message formatting.
*/
","* Format and raise a failure.
   *
   * @param category category for exception
   * @param message string formatting message
   * @param args any arguments for the formatting
   * @throws KerberosDiagsFailure containing the formatted text",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,loadFullMaps,org.apache.hadoop.security.ShellBasedIdMapping:loadFullMaps(),386,389,"/**
 * Loads full user and group maps.
 * Synchronized to prevent concurrent map loading.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createRemoteUser,"org.apache.hadoop.security.UserGroupInformation:createRemoteUser(java.lang.String,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",1451,1462,"/**
 * Creates a remote UserGroupInformation object for a user.
 * @param user The username.
 * @param authMethod Authentication method used.
 * @return A UserGroupInformation object.
 */
","* Create a user from a login name. It is intended to be used for remote
   * users in RPC, since it won't have any credentials.
   * @param user the full user principal name, must not be empty or null
   * @param authMethod authMethod.
   * @return the UserGroupInformation for the remote user.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getDefault,org.apache.hadoop.security.LdapGroupsMapping$LdapSslSocketFactory:getDefault(),1052,1067,"/**
* Returns the default SocketFactory, creating it if necessary.
* Uses TLS protocol and key/trust stores for SSL.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,retrievePassword,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:retrievePassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),548,552,"/**
 * Retrieves the password associated with the provided token.
 * @param identifier Token to validate and retrieve password from.
 * @return Password as a byte array.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,startThreads,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:startThreads(),169,177,"/**
 * Starts the token remover thread.
 * Checks state, updates key, and initiates thread execution.
 */
","* should be called before this object is used.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,rollMasterKey,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:rollMasterKey(),463,476,"/**
 * Rolls the master key, extending its expiry date and updating delegations.
 */","* Update the current master key for generating delegation tokens 
   * It should be called only by tokenRemoverThread.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(),150,152,"/**
 * Constructs a DelegationTokenAuthenticatedURL with null parameters.
 */","* Creates an <code>DelegationTokenAuthenticatedURL</code>.
   * <p>
   * An instance of the default {@link DelegationTokenAuthenticator} will be
   * used.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator),160,163,"/**
 * Constructs a DelegationTokenAuthenticatedURL with an authenticator.
 * @param authenticator The DelegationTokenAuthenticator to use.
 */
","* Creates an <code>DelegationTokenAuthenticatedURL</code>.
   *
   * @param authenticator the {@link DelegationTokenAuthenticator} instance to
   * use, if <code>null</code> the default one will be used.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:<init>(org.apache.hadoop.security.authentication.client.ConnectionConfigurator),171,174,"/**
 * Constructs a DelegationTokenAuthenticatedURL with a null base URL.
 * @param connConfigurator Configures the connection.
 */
","* Creates an <code>DelegationTokenAuthenticatedURL</code> using the default
   * {@link DelegationTokenAuthenticator} class.
   *
   * @param connConfigurator a connection configurator.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,org.apache.hadoop.security.SaslInputStream:read(byte[]),225,228,"/**
 * Reads data from the input stream into the provided byte array.
 * @param b byte array to hold the read data
 * @return number of bytes read
 */
","* Reads up to <code>b.length</code> bytes of data from this input stream into
   * an array of bytes.
   * <p>
   * The <code>read</code> method of <code>InputStream</code> calls the
   * <code>read</code> method of three arguments with the arguments
   * <code>b</code>, <code>0</code>, and <code>b.length</code>.
   * 
   * @param b
   *          the buffer into which the data is read.
   * @return the total number of bytes read into the buffer, or <code>-1</code>
   *         is there is no more data because the end of the stream has been
   *         reached.
   * @exception IOException
   *              if an I/O error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isFromKeytab,org.apache.hadoop.security.UserGroupInformation:isFromKeytab(),834,838,"/**
 * Checks if authentication is performed using a keytab file.
 * Returns true if Kerberos credentials, Hadoop login, and keytab exist.
 */
","* Is this user logged in from a keytab file managed by the UGI?
   * @return true if the credentials are from a keytab file.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isFromTicket,org.apache.hadoop.security.UserGroupInformation:isFromTicket(),844,846,"/**
 * Checks if the request originates from a Kerberos ticket.
 * Requires Kerberos credentials, Hadoop login, and no keytab.
 */
","*  Is this user logged in from a ticket (but no keytab) managed by the UGI?
   * @return true if the credentials are from a ticket cache.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,shouldRelogin,org.apache.hadoop.security.UserGroupInformation:shouldRelogin(),869,873,"/**
 * Checks if a relogin is needed based on Kerberos credentials.
 * @return True if relogin is required, false otherwise.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,toString,org.apache.hadoop.fs.FileSystem$Cache$Key:toString(),3915,3918,"/**
 * Returns a string representation of the URI.
 * Combines user info, scheme, and authority.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,toString,org.apache.hadoop.security.UserGroupInformation$RealUser:toString(),497,500,"/**
 * Returns a string representation of the object, using realUser.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,closeIdle,org.apache.hadoop.ipc.Server$ConnectionManager:closeIdle(boolean),4130,4149,"/**
 * Closes idle connections. Scans all if scanAll is true.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,closeAll,org.apache.hadoop.ipc.Server$ConnectionManager:closeAll(),4151,4157,"/**
 * Closes all connections in the pool to release resources.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,closeConnection,org.apache.hadoop.ipc.Server:closeConnection(org.apache.hadoop.ipc.Server$Connection),3493,3495,"/**
 * Closes the provided database connection.
 * @param connection The connection to close.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLHostnameVerifier.java,check,"org.apache.hadoop.security.ssl.SSLHostnameVerifier$AbstractVerifier:check(java.lang.String,javax.net.ssl.SSLSocket)",275,278,"/**
* Checks a host using an SSLSocket, delegating to the array version.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/DelegatingSSLSocketFactory.java,initializeDefaultFactory,org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory:initializeDefaultFactory(org.apache.hadoop.security.ssl.DelegatingSSLSocketFactory$SSLChannelMode),103,108,"/**
 * Initializes the default SSL socket factory.
 * @param preferredMode Preferred SSL channel mode.
 * @throws IOException if initialization fails.
 */
","* Initialize a singleton SSL socket factory.
   *
   * @param preferredMode applicable only if the instance is not initialized.
   * @throws IOException if an error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,<init>,org.apache.hadoop.fs.shell.FsCommand:<init>(org.apache.hadoop.conf.Configuration),78,80,"/**
 * Constructs a FsCommand with the given configuration.
 * @param conf The Hadoop configuration object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,<init>,org.apache.hadoop.fs.shell.CommandFactory:<init>(),45,47,"/**
 * Default constructor. Calls the parameterized constructor with null.
 */
",Factory constructor for commands,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,org.apache.hadoop.fs.HarFileSystem:<init>(),83,85,"/**
 * Constructs a HarFileSystem instance.
 * Must initialize the underlying file system.
 */
",* public construction of harfilesystem,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,org.apache.hadoop.fs.HarFileSystem:<init>(org.apache.hadoop.fs.FileSystem),103,106,"/**
 * Constructs a HarFileSystem with the given FileSystem.
 * @param fs The FileSystem to associate with this HarFileSystem.
 */
","* Constructor to create a HarFileSystem with an
   * underlying filesystem.
   * @param fs underlying file system",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,<init>,org.apache.hadoop.fs.FilterFileSystem:<init>(),71,72,"/**
 * Constructs a new FilterFileSystem object.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,<init>,org.apache.hadoop.fs.FilterFileSystem:<init>(org.apache.hadoop.fs.FileSystem),74,77,"/**
 * Constructs a FilterFileSystem with the given FileSystem and stats.
 * @param fs The FileSystem to filter.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,<init>,org.apache.hadoop.fs.FsShell:<init>(),66,68,"/**
 * Default constructor for FsShell, calls the parameterized constructor.
 */","* Default ctor with no configuration.  Be sure to invoke
   * {@link #setConf(Configuration)} with a valid configuration prior
   * to running commands.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,<init>,org.apache.hadoop.tools.GetGroupsBase:<init>(org.apache.hadoop.conf.Configuration),43,45,"/**
 * Constructs a GetGroupsBase with a Configuration and default output.
 * @param conf Configuration object for group retrieval.
 */
","* Create an instance of this tool using the given configuration.
   * @param conf configuration.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,<init>,org.apache.hadoop.fs.shell.Command:<init>(),76,79,"/**
 * Default constructor. Initializes standard output and error streams.
 */
",Constructor,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.ErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),39,43,"/**
 * Constructs an ErasureEncoder with specified options.
 * @param options ErasureCoderOptions object configuring the encoder.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.ErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),38,42,"/**
 * Initializes ErasureDecoder with provided options.
 * @param options ErasureCoderOptions object defining parameters.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/WritableSerialization.java,<init>,"org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)",48,51,"/**
 * Constructs a WritableDeserializer for the given class.
 * @param conf Configuration object.
 * @param c Writable class to deserialize.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,<init>,org.apache.hadoop.log.LogLevel$CLI:<init>(org.apache.hadoop.conf.Configuration),105,107,"/**
 * Constructor. Initializes the CLI with the provided configuration.
 * @param conf Configuration object for CLI setup.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,<init>,org.apache.hadoop.security.KDiag:<init>(),186,187,"/**
 * Default constructor for the KDiag class.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,<init>,org.apache.hadoop.ha.HAAdmin:<init>(),99,101,"/**
 * Default constructor. Calls the superclass constructor.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByName,org.apache.hadoop.security.SecurityUtil$QualifiedHostResolver:getByName(java.lang.String),647,685,"/**
 * Resolves a host name to an InetAddress. Throws UnknownHostException if not found.
 */
","* Create an InetAddress with a fully qualified hostname of the given
     * hostname.  InetAddress does not qualify an incomplete hostname that
     * is resolved via the domain search list.
     * {@link InetAddress#getCanonicalHostName()} will fully qualify the
     * hostname, but it always return the A record whereas the given hostname
     * may be a CNAME.
     * 
     * @param host a hostname or ip address
     * @return InetAddress with the fully qualified hostname or ip
     * @throws UnknownHostException if host does not exist",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateStaticMapping,org.apache.hadoop.security.ShellBasedIdMapping:updateStaticMapping(),304,336,"/**
 * Updates the static mapping based on file existence and modification.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,write,org.apache.hadoop.security.authorize.AccessControlList:write(java.io.DataOutput),317,321,"/**
 * Writes the ACL string to the DataOutput.
 * @param out DataOutput to write the ACL string to.
 */
",* Serializes the AccessControlList object,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,createZooKeeper,org.apache.hadoop.ha.ActiveStandbyElector:createZooKeeper(),752,762,"/**
 * Creates and configures a ZooKeeper client.
 * Uses truststore if provided, otherwise uses default config.
 */
","* Get a new zookeeper client instance. protected so that test class can
   * inherit and pass in a mock object for zookeeper
   *
   * @return new zookeeper client instance
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,relogin,"org.apache.hadoop.security.UserGroupInformation:relogin(org.apache.hadoop.security.UserGroupInformation$HadoopLoginContext,boolean)",1334,1345,"/**
 * Relogins the HadoopLoginContext, ensuring atomicity.
 * @param login The HadoopLoginContext to relogin.
 * @param ignoreLastLoginTime Ignores the last login time.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],boolean)",34,40,"/**
 * Creates a CryptoFSDataOutputStream.
 * @param out Output stream to wrap, codec, buffer size, key, iv.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long)",91,95,"/**
 * Creates a CryptoOutputStream with default authentication.
 * @param out Output stream.
 * @param codec CryptoCodec.
 * @param bufferSize Buffer size.
 * @param key Encryption key.
 * @param iv Initialization vector.
 * @param streamOffset Stream offset.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,<init>,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:<init>(int,org.apache.hadoop.crypto.CipherSuite,java.lang.String)",124,128,"/**
 * Initializes an OpensslCtrCipher with mode, suite, and engine ID.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCipher.java,getInstance,org.apache.hadoop.crypto.OpensslCipher:getInstance(java.lang.String),111,114,"/**
 * Gets an OpenSSL cipher instance for the given transformation.
 * @param transformation Cipher transformation string (e.g., ""AES-128-CBC"")
 * @throws NoSuchAlgorithmException, NoSuchPaddingException if algo/padding not found
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,<init>,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:<init>(),39,48,"/**
 * Initializes the codec, throwing exception if OpenSSL fails to load or SM4 CTR is unsupported.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONEncKeyVersion,"org.apache.hadoop.util.KMSUtil:parseJSONEncKeyVersion(java.lang.String,java.util.Map)",157,183,"/**
 * Parses a JSON map to create an EncryptedKeyVersion object.
 * @param keyName Key name.
 * @param valueMap Map containing key version data.
 * @return EncryptedKeyVersion object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$CreateCommand:execute(),456,474,"/**
 * Creates a key using the provider, handling exceptions and printing status.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,createKey,"org.apache.hadoop.crypto.key.KeyProviderExtension:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",71,75,"/**
 * Creates a new key with the given name and options.
 * @param name Key name.
 * @param options Key options.
 * @return The created KeyVersion.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/CachingKeyProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.CachingKeyProvider:rollNewVersion(java.lang.String),148,154,"/**
 * Rolls a new version of the key with the given name.
 * @param name Key name.
 * @return New KeyVersion object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderExtension.java,rollNewVersion,org.apache.hadoop.crypto.key.KeyProviderExtension:rollNewVersion(java.lang.String),77,81,"/**
 * Rolls a new version of the key with the given name.
 * @param name Key name.
 * @return New KeyVersion object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,execute,org.apache.hadoop.crypto.key.KeyShell$RollCommand:execute(),308,328,"/**
* Rolls a new version of the specified key using the KeyProvider.
* Handles potential NoSuchAlgorithmException and IOException.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getSize,org.apache.hadoop.crypto.key.kms.ValueQueue:getSize(java.lang.String),323,340,"/**
 * Returns the size of the queue associated with the key.
 * @param keyName The key for which to retrieve the queue size.
 * @return The size of the queue, or 0 if the key is not found.
 */
","* Get size of the Queue for keyName. This is only used in unit tests.
   * @param keyName the key name
   * @return int queue size. Zero means the queue is empty or the key does not exist.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getAtMost,"org.apache.hadoop.crypto.key.kms.ValueQueue:getAtMost(java.lang.String,int)",354,399,"/**
 * Retrieves at most {@code num} elements from the queue for {@code keyName}.
 * @param keyName Queue key
 * @param num Maximum number of elements to retrieve
 * @return List of retrieved elements
 */
","* This removes the ""num"" values currently at the head of the Queue for the
   * provided key. Will immediately fire the Queue filler function if key
   * does not exist
   * How many values are actually returned is governed by the
   * <code>SyncGenerationPolicy</code> specified by the user.
   * @param keyName String key name
   * @param num Minimum number of values to return.
   * @return {@literal List<E>} values returned
   * @throws IOException raised on errors performing I/O.
   * @throws ExecutionException execution exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,drain,org.apache.hadoop.crypto.key.kms.ValueQueue:drain(java.lang.String),302,316,"/**
 * Drains all tasks from the queue for a given key.
 * @param keyName The key for which to drain the queue.
 */
","* Drains the Queue for the provided key.
   *
   * @param keyName the key to drain the Queue for",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,refresh,"org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:refresh(java.lang.String,java.lang.String[])",57,68,"/**
 * Refreshes data based on identifier and arguments.
 * @param identifier Refresh identifier.
 * @param args Arguments for refresh operation.
 * @return Collection of RefreshResponse objects.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,newEntry,"org.apache.hadoop.ipc.RetryCache:newEntry(java.lang.Object,long,byte[],int)",341,345,"/**
 * Creates a new CacheEntryWithPayload.
 * @param payload The data to be cached.
 * @param expirationTime Expiration time in nanoseconds.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache$CacheEntryWithPayload:<init>(byte[],int,java.lang.Object,long,boolean)",161,165,"/**
 * Constructs a CacheEntryWithPayload.
 * @param clientId Client ID, callId, expirationTime, success, payload
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,put,org.apache.hadoop.ipc.CallQueueManager:put(java.lang.Object),289,299,"/**
 * Adds an element to the queue. Throws backoff if enabled.
 * @param e element to add
 * @throws InterruptedException if interrupted while waiting
 */
","* Insert e into the backing queue or block until we can.  If client
   * backoff is enabled this method behaves like add which throws if
   * the queue overflows.
   * If we block and the queue changes on us, we will insert while the
   * queue is drained.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,add,org.apache.hadoop.ipc.CallQueueManager:add(java.lang.Object),301,304,"/**
 * Adds an element to the collection.
 * @param e the element to add
 * @return true if the element was added, false otherwise
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,internalQueueCall,"org.apache.hadoop.ipc.Server:internalQueueCall(org.apache.hadoop.ipc.Server$Call,boolean)",3111,3141,"/**
 * Queues a call to the call queue, blocking if specified.
 * @param call The call to queue.
 * @param blocking Whether to block the queue.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,ensureInitialized,org.apache.hadoop.ipc.WritableRpcEngine:ensureInitialized(),71,75,"/**
 * Initializes the class if it hasn't been initialized yet.
 */
",* Initialize this class if it isn't already.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,get,org.apache.hadoop.util.LightWeightCache:get(java.lang.Object),186,199,"/**
 * Retrieves the value associated with the given key.
 * Updates expiration if enabled.
 * @param key the key to retrieve
 * @return the value or null if not found
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,close,org.apache.hadoop.util.StopWatch:close(),116,121,"/**
 * Closes the resource by stopping it if it's started.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,now,org.apache.hadoop.util.StopWatch:now(java.util.concurrent.TimeUnit),97,100,"/**
 * Returns the current time in the specified time unit.
 * @param timeUnit The TimeUnit to convert to.
 * @return Current time in the given time unit.
 */
","* now.
   *
   * @param timeUnit timeUnit.
   * @return current elapsed time in specified timeunit.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StopWatch.java,toString,org.apache.hadoop.util.StopWatch:toString(),111,114,"/**
 * Returns a string representation of the current time.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Invocation:<init>(java.lang.reflect.Method,java.lang.Object[])",104,120,"/**
 * Creates an Invocation object with method and parameters.
 * Sets client version and fingerprint based on declaring class.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(int,long,java.lang.Class)",210,223,"/**
 * Gets the protocol signature based on client/server versions.
 * @param clientMethodsHashCode Client methods hash code
 * @param serverVersion Server version
 * @param protocol Protocol class
 * @return ProtocolSignature object
 */
","* Get a server protocol's signature
   * 
   * @param clientMethodsHashCode client protocol methods hashcode
   * @param serverVersion server protocol version
   * @param protocol protocol
   * @return the server's protocol signature",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(java.lang.String,long)",225,229,"/**
 * Retrieves the protocol signature by name and version.
 * @param protocolName Name of the protocol class.
 * @param version Protocol version.
 * @return ProtocolSignature object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,shouldRetry,"org.apache.hadoop.io.retry.RetryPolicies$FailoverOnNetworkExceptionRetry:shouldRetry(java.lang.Exception,int,int,boolean)",697,750,"/**
 * Determines retry action based on exception, retries, and flags.
 * @param e Exception occurred
 * @return RetryAction object with retry decision.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,shouldRetry,"org.apache.hadoop.io.retry.RetryUtils$WrapperRetryPolicy:shouldRetry(java.lang.Exception,int,int,boolean)",98,129,"/**
 * Determines if a method should be retried based on the exception.
 * @param e The exception that occurred.
 * @return RetryAction object indicating retry behavior.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,doHealthChecks,org.apache.hadoop.ha.HealthMonitor:doHealthChecks(),195,227,"/**
 * Performs health checks on the service, handling failures and updates.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,org.apache.hadoop.ipc.Server$Call:<init>(),980,983,"/**
 * Constructs a new Call with default invalid values.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,addResponseTime,"org.apache.hadoop.ipc.CallQueueManager:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",256,258,"/**
 * Adds response time data for a schedulable event.
 * @param name Event name, Schedulable event, ProcessingDetails
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doAccept,org.apache.hadoop.ipc.Server$Listener:doAccept(java.nio.channels.SelectionKey),1616,1639,"/**
 * Accepts incoming connections on a server socket channel.
 * Handles connection registration and attachment to the key.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,add,org.apache.hadoop.ipc.FairCallQueue:add(org.apache.hadoop.ipc.Schedulable),194,213,"/**
 * Adds an element to the queue. Throws exception if queue overflows.
 * @param e element to add
 * @return true if added successfully
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,put,org.apache.hadoop.ipc.FairCallQueue:put(org.apache.hadoop.ipc.Schedulable),215,222,"/**
 * Adds an element to the queues, prioritizing based on level.
 * @param e element to add
 * @throws InterruptedException if interrupted while waiting
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$RpcCall:run(),1234,1272,"/**
 * Executes an RPC call, handles errors, and sends the response.
 * Returns null.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setDeferredError,org.apache.hadoop.ipc.Server$RpcCall:setDeferredError(java.lang.Throwable),1367,1391,"/**
 * Sets a deferred error response, handling null exceptions.
 * Populates response params and sends the deferred response.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.java,getProtocolVersions,"org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolVersions(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolVersionsRequestProto)",44,68,"/**
 * Retrieves protocol versions for a given protocol.
 * @param controller RPC controller
 * @param request Request containing the protocol
 * @return Response containing protocol versions
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolProxy.java,fetchServerMethods,org.apache.hadoop.ipc.ProtocolProxy:fetchServerMethods(java.lang.reflect.Method),57,78,"/**
 * Fetches server methods and verifies protocol version compatibility.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProtocolImpl,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:getProtocolImpl(org.apache.hadoop.ipc.RPC$Server,java.lang.String,long)",510,527,"/**
 * Retrieves ProtocolImpl for given protocol name and client version.
 * @param server RPC server, protocol map
 * @param protoName protocol name
 * @param clientVersion client's protocol version
 * @return ProtocolImpl object
 * @throws RpcServerException if protocol is not found
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server$FatalRpcServerException:<init>(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,java.lang.String)",1990,1992,"/**
 * Constructs a FatalRpcServerException with error code and message.
 * @param errCode The error code.
 * @param message The exception message.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,<init>,org.apache.hadoop.ipc.ResponseBuffer:<init>(int),37,39,"/**
 * Constructs a ResponseBuffer with the given capacity.
 * @param capacity the initial capacity of the buffer
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,run,org.apache.hadoop.ipc.Client$Connection$RpcRequestSender:run(),1115,1150,"/**
 * Processes RPC requests from the queue and sends them via IPC.
 * Exits on interruption or unrecoverable I/O error.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,decayCurrentCosts,org.apache.hadoop.ipc.DecayRpcScheduler:decayCurrentCosts(),479,540,"/**
 * Decays current costs, updates totals, and recomputes the schedule cache.
 */","* Decay the stored costs for each user and clean as necessary.
   * This method should be called periodically in order to keep
   * costs current.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getPriorityLevel,org.apache.hadoop.ipc.DecayRpcScheduler:getPriorityLevel(org.apache.hadoop.ipc.Schedulable),677,684,"/**
 * Gets the priority level of a Schedulable object, ensuring >= 0.
 * @param obj The Schedulable object to get the priority level from.
 * @return The priority level (non-negative).
 */
","* Compute the appropriate priority for a schedulable based on past requests.
   * @param obj the schedulable obj to query and remember
   * @return the level index which we recommend scheduling in",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getPriorityLevel,org.apache.hadoop.ipc.DecayRpcScheduler:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation),686,691,"/**
 * Gets the priority level for a user.
 * @param ugi UserGroupInformation object.
 * @return Priority level integer.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setPriorityLevel,"org.apache.hadoop.ipc.Server:setPriorityLevel(org.apache.hadoop.security.UserGroupInformation,int)",732,735,"/**
 * Sets the priority level for a user group.
 * @param ugi UserGroupInformation object
 * @param priority The priority level to set.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invoke,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invoke(),161,163,"/**
 * Invokes the method and wraps the result in a CallReturn object.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,getValue,org.apache.hadoop.ipc.RpcWritable$Buffer:getValue(java.lang.Object),190,192,"/**
 * Reads a value from the buffer.
 * @param value The value to wrap and read.
 * @return The read value.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,setResponse,org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtobufRpcEngineCallbackImpl:setResponse(com.google.protobuf.Message),405,410,"/**
 * Sets the response message, updates metrics, and wraps it.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,setResponse,org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtobufRpcEngineCallbackImpl:setResponse(org.apache.hadoop.thirdparty.protobuf.Message),437,442,"/**
 * Sets the response message and updates deferred metrics.
 * @param message The response message to be set.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponseForWritable,"org.apache.hadoop.ipc.Server:setupResponseForWritable(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",3564,3580,"/**
 * Creates a byte array response containing header and writable data.
 * @param header RpcResponseHeaderProto
 * @param rv Writable data, may be null
 * @return Byte array representing the response
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,removeNextElement,org.apache.hadoop.ipc.FairCallQueue:removeNextElement(),165,178,"/**
 * Removes and returns the next element from the priority queues.
 * Loops to handle potential race conditions and ensures an element is returned.
 */
","* Returns an element first non-empty queue equal to the priority returned
   * by the multiplexer or scans from highest to lowest priority queue.
   *
   * Caller must always acquire a semaphore permit before invoking.
   *
   * @return the first non-empty queue with less priority, or null if
   * everything was empty",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,close,org.apache.hadoop.ipc.Client$Connection:close(),1266,1304,"/**
 * Closes the connection, removing it and cleaning up resources.
 */",Close the connection.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRunLoop,org.apache.hadoop.ipc.Server$Responder:doRunLoop(),1727,1796,"/**
 * Runs the main loop for handling asynchronous writes and purging old calls.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,sendResponse,org.apache.hadoop.ipc.Server$Connection:sendResponse(org.apache.hadoop.ipc.Server$RpcCall),3062,3064,"/**
 * Sends a response to an RPC call using the responder.
 * @param call The RPC call to respond to.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",56,63,"/**
 * Constructs a Globber with a file system, pattern, and filter.
 * @param fs FileSystem instance
 * @param pathPattern Path pattern to match
 * @param filter Path filter to apply
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,<init>,"org.apache.hadoop.fs.Globber:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter,boolean)",81,91,"/**
 * Constructs a Globber with the provided FileSystem, path pattern, filter, and symlink resolution flag.
 */
","* Filesystem constructor for use by {@link GlobBuilder}.
   * @param fs filesystem
   * @param pathPattern path pattern
   * @param filter optional filter
   * @param resolveSymlinks should symlinks be resolved.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/MachineList.java,<init>,org.apache.hadoop.util.MachineList:<init>(java.lang.String),73,75,"/**
 * Constructs a MachineList with the given host entries and default InetAddressFactory.
 * @param hostEntries Comma-separated string of host entries.
 */
","* 
   * @param hostEntries comma separated ip/cidr/host addresses",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FileBasedIPList.java,<init>,org.apache.hadoop.util.FileBasedIPList:<init>(java.lang.String),52,65,"/**
 * Initializes IP list from file.
 * @param fileName File containing IP addresses, one per line.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfo.java,newInstance,org.apache.hadoop.util.SysInfo:newInstance(),36,44,"/**
 * Creates a SysInfo instance based on the operating system.
 * @return SysInfo object for the detected OS; throws exception if OS is unknown.
 */
","* Return default OS instance.
   * @throws UnsupportedOperationException If cannot determine OS.
   * @return Default instance for the detected OS.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getPhysicalMemorySize,org.apache.hadoop.util.SysInfoLinux:getPhysicalMemorySize(),594,600,"/**
 * Gets the physical memory size in kilobytes.
 * Reads meminfo and calculates available memory.
 */
",{@inheritDoc},,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getAvailableVirtualMemorySize,org.apache.hadoop.util.SysInfoLinux:getAvailableVirtualMemorySize(),619,622,"/**
 * Returns the available virtual memory size in KB.
 * Sums physical memory and free swap space.
 */",{@inheritDoc},,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,<init>,org.apache.hadoop.fs.FSDataInputStream:<init>(java.io.InputStream),58,64,"/**
* Creates a FSDataInputStream from an InputStream.
* Requires InputStream to implement Seekable & PositionedReadable.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,read,"org.apache.hadoop.fs.FSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)",196,212,"/**
 * Reads data into a buffer from the input stream.
 * @param bufferPool Buffer pool for allocations.
 * @param maxLength Max buffer size.
 * @param opts Read options.
 * @return ByteBuffer or null if fallback read fails.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,put,org.apache.hadoop.util.LightWeightResizableGSet:put(java.lang.Object),91,96,"/**
 * Adds an element to the queue and expands if needed.
 * @param element The element to add.
 * @return The previous element, or null if none.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,remove,org.apache.hadoop.util.LightWeightGSet$SetIterator:remove(),346,357,"/**
 * Removes the current element from the set.
 * Throws IllegalStateException if no current element exists.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightResizableGSet.java,remove,org.apache.hadoop.util.LightWeightResizableGSet:remove(java.lang.Object),103,106,"/**
 * Removes the specified key's mapping.
 * @param key the key whose mapping to be removed
 * @return the removed value, or null if key not present
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,evict,org.apache.hadoop.util.LightWeightCache:evict(),155,161,"/**
 * Removes and returns the head of the queue.
 * Returns the element or null if the queue is empty.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/XMLUtils.java,transform,"org.apache.hadoop.util.XMLUtils:transform(java.io.InputStream,java.io.InputStream,java.io.Writer)",79,95,"/**
 * Transforms XML using a stylesheet and writes output to a Writer.
 * @param styleSheet Stylesheet input stream.
 * @param xml XML input stream.
 * @param out Writer for transformed output.
 */
","* Transform input xml given a stylesheet.
   * 
   * @param styleSheet the style-sheet
   * @param xml input xml data
   * @param out output
   * @throws TransformerConfigurationException synopsis signals a problem
   *         creating a transformer object.
   * @throws TransformerException this is used for throwing processor
   *          exceptions before the processing has started.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean,boolean,java.util.List)",446,469,"/**
 * Generates a formatted string summarizing storage usage.
 * @param q,h,t,x Options to control output format.
 * @param types List of storage types.
 * @return Formatted string representing storage usage.
 */
","Return the string representation of the object in the output format.
   * if qOption is false, output directory count, file count, and content size;
   * if qOption is true, output quota and remaining quota as well.
   * if hOption is false, file sizes are returned in bytes
   * if hOption is true, file sizes are returned in human readable
   * if tOption is true, display the quota by storage types
   * if tOption is false, same logic with #toString(boolean,boolean)
   * if xOption is false, output includes the calculation from snapshots
   * if xOption is true, output excludes the calculation from snapshots
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output is to be used
   * @param tOption a flag indicating if display quota by storage types
   * @param xOption a flag indicating if calculation from snapshots is to be
   *                included in the output
   * @param types Storage types to display
   * @return the string representation of the object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toSnapshot,org.apache.hadoop.fs.ContentSummary:toSnapshot(boolean),498,503,"/**
 * Creates a snapshot string with formatted sizes based on hOption.
 * @param hOption boolean flag for human-readable size format
 * @return Snapshot string with formatted size information.
 */
","* Return the string representation of the snapshot counts in the output
   * format.
   * @param hOption flag indicating human readable or not
   * @return String representation of the snapshot counts",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,getQuotaUsage,org.apache.hadoop.fs.QuotaUsage:getQuotaUsage(boolean),329,346,"/**
 * Returns quota usage string.
 * @param hOption Whether to use human-readable size format.
 * @return Formatted quota usage string.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,getTypesQuotaUsage,"org.apache.hadoop.fs.QuotaUsage:getTypesQuotaUsage(boolean,java.util.List)",348,366,"/**
 * Generates quota usage summary for storage types.
 * @param hOption flag for human-readable size format
 * @param types list of storage types to summarize
 * @return String containing quota usage summary
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,computeCapacity,"org.apache.hadoop.util.LightWeightGSet:computeCapacity(double,java.lang.String)",374,377,"/**
 * Computes capacity using given percentage and map name.
 * Delegates to overloaded method with max memory.
 */","* Let t = percentage of max memory.
   * Let e = round(log_2 t).
   * Then, we choose capacity = 2^e/(size of reference),
   * unless it is outside the close interval [1, 2^30].
   *
   * @param mapName mapName.
   * @param percentage percentage.
   * @return compute capacity.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,fill,org.apache.hadoop.fs.FSInputChecker:fill(),217,222,"/**
* Fills the internal buffer with data from the input stream.
*/","* Fills the buffer with a chunk data. 
   * No mark is supported.
   * This method assumes that all data in the buffer has already been read in,
   * hence pos > count.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,readAndDiscard,org.apache.hadoop.fs.FSInputChecker:readAndDiscard(int),231,245,"/**
 * Reads and discards up to 'len' bytes from the input stream.
 * @param len Number of bytes to read and discard.
 * @return Number of bytes actually read.
 */
","* Like read(byte[], int, int), but does not provide a dest buffer,
   * so the read data is discarded.
   * @param      len maximum number of bytes to read.
   * @return     the number of bytes read.
   * @throws     IOException  if an I/O error occurs.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,toString,org.apache.hadoop.io.UTF8:toString(),163,175,"/**
 * Returns a string representation of the character array.
 * Uses a synchronized buffer to read and append characters.
 */
",Convert to a String.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,toStringChecked,org.apache.hadoop.io.UTF8:toStringChecked(),183,190,"/**
 * Reads characters from the buffer and returns them as a string.
 * @throws IOException if an I/O error occurs during reading.
 */
","* Convert to a string, checking for valid UTF8.
   * @return the converted string
   * @throws UTFDataFormatException if the underlying bytes contain invalid
   * UTF8 data.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,fromBytes,org.apache.hadoop.io.UTF8:fromBytes(byte[]),257,263,"/**
 * Converts a byte array to a String using DataInputBuffer.
 * @param bytes byte array to convert
 * @return String representation of the byte array
 */
","* @return Convert a UTF-8 encoded byte array back into a string.
   *
   * @param bytes input bytes.
   * @throws IOException if the byte array is invalid UTF8",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,readString,org.apache.hadoop.io.UTF8:readString(java.io.DataInput),272,277,"/**
 * Reads a string from the DataInput.
 * @param in DataInput to read from
 * @return String read from the input
 */
","* @return Read a UTF-8 encoded string.
   *
   * @see DataInput#readUTF()
   * @param in DataInput.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,checkResponse,org.apache.hadoop.ipc.Client:checkResponse(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto),252,267,"/**
 * Checks the response header for client ID matching.
 * @param header The RpcResponseHeaderProto to validate.
 * @throws IOException if client IDs do not match.
 */
",Check the rpc response header.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,toString,org.apache.hadoop.ipc.Client:toString(),1353,1357,"/**
 * Returns a string representation of the object.
 * Includes class name and client ID in hex format.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,byteToHexString,org.apache.hadoop.util.StringUtils:byteToHexString(byte),210,212,"/**
 * Converts a single byte to its hexadecimal string representation.
 */","* Convert a byte to a hex string.
   * @see #byteToHexString(byte[])
   * @see #byteToHexString(byte[], int, int)
   * @param b byte
   * @return byte's hex value as a String",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,toString,org.apache.hadoop.ha.ActiveStandbyElector:toString(),1274,1280,"/**
 * Returns a string representation of the elector.
 * Includes identity hash, appData (hex string), and appClient.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,uncaughtException,"org.apache.hadoop.service.launcher.ServiceLauncher:uncaughtException(java.lang.Thread,java.lang.Throwable)",779,783,"/**
 * Handles uncaught exceptions, logs error, and exits application.
 * @param thread The thread where exception occurred.
 * @param exception The uncaught exception.
 */
","* Handler for uncaught exceptions: terminate the service.
   * @param thread thread
   * @param exception exception",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exitWithUsageMessage,org.apache.hadoop.service.launcher.ServiceLauncher:exitWithUsageMessage(),1033,1035,"/**
 * Exits the program with a usage message.
 * Calls exitWithMessage with predefined constants.
 */
","* Exit with the usage exit code {@link #EXIT_USAGE}
   * and message {@link #USAGE_MESSAGE}.
   * @throws ExitUtil.ExitException if exceptions are disabled",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/HadoopUncaughtExceptionHandler.java,uncaughtException,"org.apache.hadoop.service.launcher.HadoopUncaughtExceptionHandler:uncaughtException(java.lang.Thread,java.lang.Throwable)",83,127,"/**
 * Handles uncaught exceptions in a thread. Logs error and terminates if Error.
 * @param thread The thread where exception occurred.
 * @param exception The thrown exception.
 */
","* Uncaught exception handler.
   * If an error is raised: shutdown
   * The state of the system is unknown at this point -attempting
   * a clean shutdown is dangerous. Instead: exit
   * @param thread thread that failed
   * @param exception the raised exception",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,exit,"org.apache.hadoop.service.launcher.ServiceLauncher:exit(int,java.lang.String)",856,858,"/**
 * Terminates the application with a given exit code and message.
 */
","* Exit the JVM.
   *
   * This is method can be overridden for testing, throwing an 
   * exception instead. Any subclassed method MUST raise an 
   * {@code ExitException} instance/subclass.
   * The service launcher code assumes that after this method is invoked,
   * no other code in the same method is called.
   * @param exitCode code to exit
   * @param message input message.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,terminate,org.apache.hadoop.util.ExitUtil:terminate(int),368,370,"/**
* Terminates the application with a given status code.
* @param status the exit status code
* @throws ExitException if termination fails
*/
","* Like {@link #terminate(int, Throwable)} without a message.
   *
   * @param status exit code
   * @throws ExitException if {@link System#exit(int)} is disabled.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Classpath.java,terminate,"org.apache.hadoop.util.Classpath:terminate(int,java.lang.String)",121,124,"/**
 * Terminates the application with a given status and message.
 * @param status Exit status code.
 * @param msg Error message to display.
 */
","* Prints a message to stderr and exits with a status code.
   *
   * @param status exit code
   * @param msg message",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/InterruptEscalator.java,interrupted,org.apache.hadoop.service.launcher.InterruptEscalator:interrupted(org.apache.hadoop.service.launcher.IrqHandler$InterruptData),103,135,"/**
 * Handles service interruption, logs, and initiates forced shutdown.
 * Logs warning and terminates JVM if interrupt is repeated.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ExitUtil.java,halt,org.apache.hadoop.util.ExitUtil:halt(int),389,391,"/**
 * Halts execution with a specified status and default message.
 * @param status The exit status code.
 * @throws HaltException if halting fails.
 */
","* Forcibly terminates the currently running Java virtual machine.
   * @param status status code
   * @throws HaltException if {@link Runtime#halt(int)} is disabled.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/QuickSort.java,sort,"org.apache.hadoop.util.QuickSort:sort(org.apache.hadoop.util.IndexedSortable,int,int)",58,61,"/**
 * Sorts a portion of the IndexedSortable using a custom comparator.
 * @param s sortable object, p start index, r end index
 */
","* Sort the given range of items using quick sort.
   * {@inheritDoc} If the recursion depth falls below {@link #getMaxDepth},
   * then switch to {@link HeapSort}.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclStatus.java,<init>,"org.apache.hadoop.fs.permission.AclStatus:<init>(java.lang.String,java.lang.String,boolean,java.lang.Iterable,org.apache.hadoop.fs.permission.FsPermission)",216,223,"/**
 * Constructs an AclStatus object.
 * @param owner The owner of the file/directory.
 * @param group The group of the file/directory.
 */
","* Private constructor.
   *
   * @param file Path file associated to this ACL
   * @param owner String file owner
   * @param group String file group
   * @param stickyBit the sticky bit
   * @param entries the ACL entries
   * @param permission permission of the path",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,parseACLs,org.apache.hadoop.util.ZKUtil:parseACLs(java.lang.String),95,122,"/**
 * Parses a comma-separated ACL string into a List of ACL objects.
 * @param aclString ACL string to parse.
 * @return List of ACL objects.
 * @throws BadAclFormatException if ACL format is invalid.
 */
","* Parse comma separated list of ACL entries to secure generated nodes, e.g.
   * <code>sasl:hdfs/host1@MY.DOMAIN:cdrwa,sasl:hdfs/host2@MY.DOMAIN:cdrwa</code>
   *
   * @param aclString aclString.
   * @return ACL list
   * @throws BadAclFormatException if an ACL is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ZKUtil.java,parseAuth,org.apache.hadoop.util.ZKUtil:parseAuth(java.lang.String),133,154,"/**
 * Parses auth string into a list of ZKAuthInfo objects.
 * @param authString Comma-separated auth string.
 * @return List of ZKAuthInfo objects.
 */
","* Parse a comma-separated list of authentication mechanisms. Each
   * such mechanism should be of the form 'scheme:auth' -- the same
   * syntax used for the 'addAuth' command in the ZK CLI.
   * 
   * @param authString the comma-separated auth mechanisms
   * @return a list of parsed authentications
   * @throws BadAuthFormatException if the auth format is invalid",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,preserveAttributes,"org.apache.hadoop.fs.shell.CommandWithDestination:preserveAttributes(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData,boolean)",445,490,"/**
* Copies file attributes from src to target, respecting preservation flags.
*/
","* Preserve the attributes of the source to the target.
   * The method calls {@link #shouldPreserve(FileAttribute)} to check what
   * attribute to preserve.
   * @param src source to preserve
   * @param target where to preserve attributes
   * @param preserveRawXAttrs true if raw.* xattrs should be preserved
   * @throws IOException if fails to preserve attributes",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ChunkedArrayList.java,add,org.apache.hadoop.util.ChunkedArrayList:add(java.lang.Object),132,146,"/**
 * Adds an element to the list.
 * @param e the element to add
 * @return true if successful, false otherwise.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/AclUtil.java,getMinimalAcl,org.apache.hadoop.fs.permission.AclUtil:getMinimalAcl(org.apache.hadoop.fs.permission.FsPermission),99,116,"/**
 * Creates a minimal AclEntry list based on the provided FsPermission.
 */","* Translates the given permission bits to the equivalent minimal ACL.
   *
   * @param perm FsPermission to translate
   * @return List&lt;AclEntry&gt; containing exactly 3 entries representing the
   *         owner, group and other permissions",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,trackDuration,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:trackDuration(java.lang.String,long)",461,468,"/**
 * Tracks duration for a given key and count.
 * @param key Identifier for the duration being tracked.
 * @param count The duration count to record.
 * @return A DurationTracker instance.
 */
","* If the store is tracking the given key, return the
   * duration tracker for it. If not tracked, return the
   * stub tracker.
   * @param key statistic key prefix
   * @param count  #of times to increment the matching counter in this
   * operation.
   * @return a tracker.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,<init>,"org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:<init>(org.apache.hadoop.fs.statistics.impl.IOStatisticsStore,java.lang.String)",58,62,"/**
 * Constructs a DurationTracker with a default interval.
 * @param iostats IOStatisticsStore to track durations.
 * @param key Identifier for the duration tracker.
 */
","* Constructor -increments the counter by 1.
   * @param iostats statistics to update
   * @param key prefix of values.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DurationInfo.java,<init>,"org.apache.hadoop.util.DurationInfo:<init>(org.slf4j.Logger,java.lang.String,java.lang.Object[])",57,59,"/**
 * Creates a DurationInfo with logging enabled.
 * @param log Logger instance for logging.
 * @param format Format string for the duration message.
 * @param args Arguments to format into the message.
 */
","* Create the duration text from a {@code String.format()} code call;
   * log output at info level.
   * @param log log to write to
   * @param format format string
   * @param args list of arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,waitForCompletion,org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.concurrent.CompletableFuture),116,126,"/**
 * Waits for the completion of a CompletableFuture.
 * Re-throws CancellationException as IOException.
 */
","* Wait for a single of future to complete, extracting IOEs afterwards.
   *
   * @param <T> Generics Type T.
   * @param future future to wait for.
   * @throws IOException      if one of the called futures raised an IOE.
   * @throws RuntimeException if one of the futures raised one.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,waitForCompletionIgnoringExceptions,org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletionIgnoringExceptions(java.util.concurrent.CompletableFuture),133,143,"/**
 * Waits for CompletableFuture completion, ignoring any exceptions.
 * @param future The CompletableFuture to wait for.
 */
","* Wait for a single of future to complete, ignoring exceptions raised.
   * @param future future to wait for.
   * @param <T> Generics Type T.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/StatisticDurationTracker.java,toString,org.apache.hadoop.fs.statistics.impl.StatisticDurationTracker:toString(),107,112,"/**
 * Returns a string representation of the duration statistic.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DurationInfo.java,toString,org.apache.hadoop.util.DurationInfo:toString(),89,92,"/**
 * Returns a string representation of the object, including formatted text and duration.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,newCrcComposer,"org.apache.hadoop.util.CrcComposer:newCrcComposer(org.apache.hadoop.util.DataChecksum$Type,long)",60,64,"/**
 * Creates a new CrcComposer with specified checksum type and hint.
 * @param type Checksum type.
 * @param bytesPerCrcHint Hint for CRC calculation.
 * @return A new CrcComposer instance.
 */
","* Returns a CrcComposer which will collapse all ingested CRCs into a single
   * value.
   *
   * @param type type.
   * @param bytesPerCrcHint bytesPerCrcHint.
   * @throws IOException raised on errors performing I/O.
   * @return a CrcComposer which will collapse all ingested CRCs into a single value.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,update,"org.apache.hadoop.util.CrcComposer:update(int,long)",167,192,"/**
 * Updates the composite CRC based on CRC value and bytes.
 * @param crcB CRC value to compose.
 * @param bytesPerCrc Number of bytes processed.
 * @throws IOException if position exceeds stripe length.
 */
","* Updates with a single additional CRC which corresponds to an underlying
   * data size of {@code bytesPerCrc}.
   *
   * @param crcB crcB.
   * @param bytesPerCrc bytesPerCrc.
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,lock,org.apache.hadoop.util.InstrumentedLock:lock(),101,107,"/**
 * Acquires the lock and records timing information.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,lockInterruptibly,org.apache.hadoop.util.InstrumentedLock:lockInterruptibly(),109,115,"/**
 * Acquires the lock, interrupting if another thread holds it.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,tryLock,"org.apache.hadoop.util.InstrumentedLock:tryLock(long,java.util.concurrent.TimeUnit)",126,136,"/**
 * Attempts to acquire the lock for a specified time.
 * @param time Acquisition time.
 * @param unit Time unit for the duration.
 * @return True if lock acquired, false otherwise.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,unlock,org.apache.hadoop.util.InstrumentedLock:unlock(),138,144,"/**
 * Releases the lock and records lock acquisition/release times.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,getFormattedTimeWithDiff,"org.apache.hadoop.util.StringUtils:getFormattedTimeWithDiff(org.apache.commons.lang3.time.FastDateFormat,long,long)",375,379,"/**
 * Formats finish time with time difference from start time.
 * @param dateFormat DateFormat instance
 * @param finishTime Finish time in milliseconds
 * @param startTime Start time in milliseconds
 */
","* Formats time in ms and appends difference (finishTime - startTime)
   * as returned by formatTimeDiff().
   * If finish time is 0, empty string is returned, if start time is 0
   * then difference is not appended to return value.
   *
   * @param dateFormat date format to use
   * @param finishTime finish time
   * @param startTime  start time
   * @return formatted value.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,escapeString,org.apache.hadoop.util.StringUtils:escapeString(java.lang.String),665,667,"/**
 * Escapes a string using a specified escape character and delimiter.
 */
","* Escape commas in the string using the default escape char
   * @param str a string
   * @return an escaped string",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,unEscapeString,org.apache.hadoop.util.StringUtils:unEscapeString(java.lang.String),723,725,"/**
 * Unescapes a string using default escape character and comma.
 */
","* Unescape commas in the string using the default escape char
   * @param str a string
   * @return an unescaped string",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,startupShutdownMessage,"org.apache.hadoop.service.launcher.ServiceLauncher:startupShutdownMessage(java.lang.String,java.util.List)",1010,1016,"/**
 * Creates a startup/shutdown message with class name, hostname, and args.
 */
","* @return Build a log message for starting up and shutting down.
   * @param classname the class of the server
   * @param args arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,sourceNext,org.apache.hadoop.util.functional.RemoteIterators$WrappingRemoteIterator:sourceNext(),489,499,"/**
 * Retrieves the next element from the source, throwing exception if empty.
 */
","* Get the next source value.
     * This calls {@link #sourceHasNext()} first to verify
     * that there is data.
     * @return the next value
     * @throws IOException failure
     * @throws NoSuchElementException no more data",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,sourceHasNext,org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:sourceHasNext(),790,793,"/**
 * Checks if the source has more data, considering continueWork.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,awaitFuture,org.apache.hadoop.fs.impl.FutureIOSupport:awaitFuture(java.util.concurrent.Future),65,69,"/**
 * Awaits the result of a Future.
 * @param future The Future to await.
 * @return The result of the Future.
 */
","* Given a future, evaluate it. Raised exceptions are
   * extracted and handled.
   * See {@link FutureIO#awaitFuture(Future, long, TimeUnit)}.
   * @param future future to evaluate
   * @param <T> type of the result.
   * @return the result, if all went well.
   * @throws InterruptedIOException future was interrupted
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitAllFutures,org.apache.hadoop.util.functional.FutureIO:awaitAllFutures(java.util.Collection),162,169,"/**
 * Awaits completion of all futures in the collection.
 * @param collection Futures to await.
 * @return List of results from completed futures.
 */
","* Evaluates a collection of futures and returns their results as a list.
   * <p>
   * This method blocks until all futures in the collection have completed.
   * If any future throws an exception during its execution, this method
   * extracts and rethrows that exception.
   * </p>
   * @param collection collection of futures to be evaluated
   * @param <T> type of the result.
   * @return the list of future's result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,awaitFuture,"org.apache.hadoop.fs.impl.FutureIOSupport:awaitFuture(java.util.concurrent.Future,long,java.util.concurrent.TimeUnit)",86,93,"/**
 * Awaits a future's completion within a specified timeout.
 * @param future Future to await.
 * @param timeout Timeout duration.
 * @param unit Timeout unit (e.g., SECONDS, MILLS).
 * @return Result of the future, or throws TimeoutException.
 */
","* Given a future, evaluate it. Raised exceptions are
   * extracted and handled.
   * See {@link FutureIO#awaitFuture(Future, long, TimeUnit)}.
   * @param future future to evaluate
   * @param <T> type of the result.
   * @param timeout timeout.
   * @param unit unit.
   * @return the result, if all went well.
   * @throws InterruptedIOException future was interrupted
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown
   * @throws TimeoutException the future timed out.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,awaitAllFutures,"org.apache.hadoop.util.functional.FutureIO:awaitAllFutures(java.util.Collection,java.time.Duration)",188,197,"/**
 * Awaits completion of all futures in the collection.
 * @param collection Futures to await.
 * @param duration Timeout duration for each future.
 * @return List of results from completed futures.
 */
","* Evaluates a collection of futures and returns their results as a list,
   * but only waits up to the specified timeout for each future to complete.
   * <p>
   * This method blocks until all futures in the collection have completed or
   * the timeout expires, whichever happens first. If any future throws an
   * exception during its execution, this method extracts and rethrows that exception.
   * @param collection collection of futures to be evaluated
   * @param duration timeout duration
   * @param <T> type of the result.
   * @return the list of future's result, if all went well.
   * @throws InterruptedIOException waiting for future completion was interrupted
   * @throws CancellationException if the future itself was cancelled
   * @throws IOException if something went wrong
   * @throws RuntimeException any nested RTE thrown
   * @throws TimeoutException the future timed out.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,cancelAllFuturesAndAwaitCompletion,"org.apache.hadoop.util.functional.FutureIO:cancelAllFuturesAndAwaitCompletion(java.util.Collection,boolean,java.time.Duration)",211,239,"/**
 * Cancels futures and awaits completion, returning results.
 * @param collection Futures to cancel and await.
 * @param interruptIfRunning Whether to interrupt running threads.
 * @param duration Timeout duration for awaiting futures.
 * @return List of results from completed futures.
 */
","* Cancels a collection of futures and awaits the specified duration for their completion.
   * <p>
   * This method blocks until all futures in the collection have completed or
   * the timeout expires, whichever happens first.
   * All exceptions thrown by the futures are ignored. as is any TimeoutException.
   * @param collection collection of futures to be evaluated
   * @param interruptIfRunning should the cancel interrupt any active futures?
   * @param duration total timeout duration
   * @param <T> type of the result.
   * @return all futures which completed successfully.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,newInstance,"org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration,java.lang.Class[],java.lang.Object[])",138,161,"/**
 * Creates a new instance of the given class with provided arguments.
 * @param theClass Class to instantiate
 * @param conf Configuration object
 * @param argTypes Argument types for the constructor
 * @param values Values to pass to the constructor
 * @return New instance of the class
 */
","Create an object for the given class and initialize it from conf
   *
   * @param theClass class of which an object is created
   * @param conf Configuration
   * @param argTypes the types of the arguments
   * @param values the values of the arguments
   * @param <T> Generics Type.
   * @return a new object",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getKeyClass,org.apache.hadoop.io.SequenceFile$Reader:getKeyClass(),2196,2205,"/**
 * Returns the Class object for the key class, caching the result.
 */",@return Returns the class of keys in this file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getValueClass,org.apache.hadoop.io.SequenceFile$Reader:getValueClass(),2213,2222,"/**
 * Returns the class of the value. Lazily initializes if null.
 * @return Class object representing the value's class.
 */
",@return Returns the class of values in this file.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,readFields,org.apache.hadoop.io.EnumSetWritable:readFields(java.io.DataInput),117,133,"/**
* Reads fields from DataInput, populating the EnumSet value.
*/",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,loadClass,org.apache.hadoop.util.FindClass:loadClass(java.lang.String),244,259,"/**
 * Loads a class by name. Returns SUCCESS, E_NOT_FOUND, or E_LOAD_FAILED.
 */
","* Loads the class of the given name
   * @param name classname
   * @return outcome code",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,createClassInstance,org.apache.hadoop.util.FindClass:createClassInstance(java.lang.String),278,302,"/**
 * Creates an instance of the class with the given name.
 * @param name Class name to instantiate.
 * @return SUCCESS, E_NOT_FOUND, or E_CREATE_FAILED.
 */
","* Create an instance of a class
   * @param name classname
   * @return the outcome",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,<init>,org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(org.apache.hadoop.fs.Path),115,117,"/**
 * Constructs a builder with the given path, no parent.
 * @param path The path for the filesystem being built.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,<init>,org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:<init>(org.apache.hadoop.fs.PathHandle),119,121,"/**
 * Constructs an AbstractFSBuilderImpl with an empty root.
 * @param pathHandle The initial path handle for the builder.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandler.java,<init>,org.apache.hadoop.fs.FsUrlStreamHandler:<init>(),42,44,"/**
 * Constructs a new FsUrlStreamHandler, initializing the configuration.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,createConfiguration,org.apache.hadoop.service.launcher.ServiceLauncher:createConfiguration(),399,401,"/**
 * Creates and returns a new Configuration object.
 */","* Override point: create the base configuration for the service.
   *
   * Subclasses can override to create HDFS/YARN configurations etc.
   * @return the configuration to use as the service initializer.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,loadConf,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:loadConf(),437,449,"/**
 * Returns a Configuration object, using suppliedConf if available.
 * Otherwise, creates a new Configuration.
 */
","* Return the supplied configuration for testing or otherwise load a new
   * configuration.
   *
   * @return the configuration to use",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,<init>,org.apache.hadoop.util.FindClass:<init>(),123,125,"/**
 * Constructs a FindClass instance with a default Configuration.
 */","* Empty constructor; passes a new Configuration
   * object instance to its superclass's constructor",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,<init>,org.apache.hadoop.conf.ReconfigurableBase:<init>(),75,77,"/**
 * Constructs a ReconfigurableBase object with a default configuration.
 */",* Construct a ReconfigurableBase.,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,<init>,org.apache.hadoop.conf.ReconfigurableBase:<init>(org.apache.hadoop.conf.Configuration),84,86,"/**
 * Constructs a ReconfigurableBase with the given configuration.
 * Uses a default config if none is provided.
 */
","* Construct a ReconfigurableBase with the {@link Configuration}
   * conf.
   * @param conf configuration.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newDataChecksum,"org.apache.hadoop.util.DataChecksum:newDataChecksum(org.apache.hadoop.util.DataChecksum$Type,int)",135,150,"/**
 * Creates a new DataChecksum instance based on the given type.
 * @param type checksum type (NULL, CRC32, CRC32C)
 * @param bytesPerChecksum bytes per checksum
 * @return DataChecksum object or null if type is invalid
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,getQualifiedBinPath,org.apache.hadoop.util.Shell:getQualifiedBinPath(java.lang.String),701,704,"/**
 * Returns the absolute, canonical path of the executable's bin directory.
 * @param executable The executable file name.
 * @return The absolute path to the bin directory.
 */
","*  Fully qualify the path to a binary that should be in a known hadoop
   *  bin location. This is primarily useful for disambiguating call-outs
   *  to executable sub-components of Hadoop to avoid clashes with other
   *  executables that may be in the path.  Caveat:  this call doesn't
   *  just format the path to the bin directory.  It also checks for file
   *  existence of the composed path. The output of this call should be
   *  cached by callers.
   *
   * @param executable executable
   * @return executable file reference
   * @throws FileNotFoundException if the path does not exist
   * @throws IOException on path canonicalization failures",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,runCommand,org.apache.hadoop.util.Shell:runCommand(),967,1098,"/**
 * Executes a shell command using ProcessBuilder, handling timeouts & errors.
 */","* Run the command.
   *
   * @throws IOException raised on errors performing I/O.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Progress.java,addPhase,org.apache.hadoop.util.Progress:addPhase(java.lang.String),61,65,"/**
* Adds a phase with the given status.
* @param status The status of the new phase.
* @return The newly created Progress phase.
*/
","* Adds a named node to the tree.
   * @param status status.
   * @return Progress.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,createRootDirRecursively,org.apache.hadoop.util.curator.ZKCuratorManager:createRootDirRecursively(java.lang.String),361,363,"/**
 * Creates the root directory recursively.
 * @param path The path of the directory to create.
 */
","* Utility function to ensure that the configured base znode exists.
   * This recursively creates the znode as well as all of its parents.
   * @param path Path of the znode to create.
   * @throws Exception If it cannot create the file.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynConstructors.java,invoke,"org.apache.hadoop.util.dynamic.DynConstructors$Ctor:invoke(java.lang.Object,java.lang.Object[])",77,82,"/**
 * Invokes a constructor with provided arguments.
 * @param target Must be null; used for constructor invocation.
 * @param args Arguments to pass to the constructor.
 * @return Result of the constructor invocation.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,pathCapabilities_hasPathCapability,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:pathCapabilities_hasPathCapability(java.lang.Object,org.apache.hadoop.fs.Path,java.lang.String)",349,356,"/**
 * Checks if a path has a specific capability.
 * @param fs filesystem, path, capability - details of the check
 * @return True if the path has the capability, false otherwise.
 */
","* Does a path have a given capability?
   * Calls {@code PathCapabilities#hasPathCapability(Path, String)},
   * mapping IOExceptions to false.
   * @param fs filesystem
   * @param path path to query the capability of.
   * @param capability non-null, non-empty string to query the path for support.
   * @return true if the capability is supported
   * under that part of the FS
   * false if the method is not loaded or the path lacks the capability.
   * @throws IllegalArgumentException invalid arguments",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,streamCapabilities_hasCapability,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:streamCapabilities_hasCapability(java.lang.Object,java.lang.String)",367,372,"/**
 * Checks if an object has a specific capability.
 * @param object The object to check.
 * @param capability Capability string to check for.
 * @return True if the object has the capability, false otherwise.
 */
","* Does an object implement {@code StreamCapabilities} and, if so,
   * what is the result of the probe for the capability?
   * Calls {@code StreamCapabilities#hasCapability(String)},
   * @param object object to probe
   * @param capability capability string
   * @return true iff the object implements StreamCapabilities and the capability is
   * declared available.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_counters,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_counters(java.io.Serializable),611,614,"/**
 * Retrieves counters from an iostatistics method, given a source.
 * @param source The source object for the counters.
 * @return A map of string counters.
 */
","* Get the counters of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of counters.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_gauges,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_gauges(java.io.Serializable),621,625,"/**
 * Invokes iostatisticsGaugesMethod with the given source.
 * @param source The source object for the invocation.
 * @return A map of string keys to long values.
 */
","* Get the gauges of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of gauges.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_minimums,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_minimums(java.io.Serializable),632,635,"/**
 * Retrieves minimum statistics from a source using reflection.
 * @param source The source object to extract statistics from.
 * @return A map of string keys to long values representing minimums.
 */
","* Get the minimums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of minimums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_maximums,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_maximums(java.io.Serializable),642,645,"/**
 * Retrieves maximum statistics from a source using reflection.
 * @param source The source object to extract statistics from.
 * @return A map of string keys to long values representing maximums.
 */
","* Get the maximums of an IOStatisticsSnapshot.
   * @param source source of statistics.
   * @return the map of maximums.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_means,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_means(java.io.Serializable),654,657,"/**
 * Invokes iostatisticsMeansMethod with the given source.
 * @param source Serializable object to process.
 * @return Map of statistics means.
 */
","* Get the means of an IOStatisticsSnapshot.
   * Each value in the map is the (sample, sum) tuple of the values;
   * the mean is then calculated by dividing sum/sample wherever sample is non-zero.
   * @param source source of statistics.
   * @return a map of mean key to (sample, sum) tuples.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invoke,org.apache.hadoop.util.dynamic.DynMethods$StaticMethod:invoke(java.lang.Object[]),219,221,"/**
 * Invokes the method with the given arguments.
 * @param args Arguments to pass to the method.
 * @return The result of the method invocation.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invokeStatic,org.apache.hadoop.util.dynamic.DynMethods$UnboundMethod:invokeStatic(java.lang.Object[]),106,109,"/**
 * Invokes a static method with the given arguments.
 * @param args Arguments to pass to the static method.
 * @return The result of the static method invocation.
 */
","* Invoke a static method.
     * @param args arguments.
     * @return result.
     * @param <R> type of result.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,invoke,org.apache.hadoop.util.dynamic.DynMethods$BoundMethod:invoke(java.lang.Object[]),202,204,"/**
 * Invokes the method with the given arguments.
 * @param args Arguments to pass to the method.
 * @return The result of the method invocation.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,impl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:impl(java.lang.String,java.lang.Class[])",308,311,"/**
 * Sets implementation class and argument classes.
 * @param className Class name of the implementation.
 * @param argClasses Argument class types.
 * @return This builder instance.
 */
","* Checks for an implementation, first finding the given class by name.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param className name of a class
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/DynMethods.java,hiddenImpl,"org.apache.hadoop.util.dynamic.DynMethods$Builder:hiddenImpl(java.lang.String,java.lang.Class[])",411,414,"/**
 * Calls hiddenImpl with provided class name and argument classes.
 * @param className Class name to use.
 * @param argClasses Argument classes to pass.
 * @return This builder instance.
 */
","* Checks for an implementation, first finding the given class by name.
     * <p>
     * The name passed to the constructor is the method name used.
     * @param className name of a class
     * @param argClasses argument classes for the method
     * @return this Builder for method chaining",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/dynamic/BindingUtils.java,loadStaticMethod,"org.apache.hadoop.util.dynamic.BindingUtils:loadStaticMethod(java.lang.Class,java.lang.Class,java.lang.String,java.lang.Class[])",139,149,"/**
 * Loads a static method from a class.
 * @param source Class containing the method.
 * @param returnType Method's return type.
 * @param name Method name.
 * @param parameterTypes Method parameter types.
 * @return UnboundMethod object.
 */
","* Load a static method from the source class, which will be a noop() if
   * the class is null or the method isn't found.
   * If the class and method are not found, then an {@code IllegalStateException}
   * is raised on the basis that this means that the binding class is broken,
   * rather than missing/out of date.
   *
   * @param <T> return type
   * @param source source. If null, the method is a no-op.
   * @param returnType return type class (unused)
   * @param name method name
   * @param parameterTypes parameters
   *
   * @return the method or a no-op.
   * @throws IllegalStateException if the method is not static.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,isIOStatisticsSource,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatisticsSource(java.lang.Object),394,397,"/**
 * Checks if the given object is a source of I/O statistics.
 * Uses ioStatisticsAvailable() and reflection.
 */
","* Probe for an object being an instance of {@code IOStatisticsSource}.
   * @param object object to probe
   * @return true if the object is the right type, false if the classes
   * were not found or the object is null/of a different type",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,isIOStatistics,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatistics(java.lang.Object),405,408,"/**
 * Checks if the object has IO statistics available.
 * Returns true if ioStatisticsAvailable() is true and invoke succeeds.
 */
","* Probe for an object being an instance of {@code IOStatisticsSource}.
   * @param object object to probe
   * @return true if the object is the right type, false if the classes
   * were not found or the object is null/of a different type",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,isIOStatisticsSnapshot,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:isIOStatisticsSnapshot(java.io.Serializable),416,419,"/**
 * Checks if the object is an IO statistics snapshot.
 * @param object Serializable object to check.
 * @return True if object is an IO statistics snapshot.
 */
","* Probe for an object being an instance of {@code IOStatisticsSnapshot}.
   * @param object object to probe
   * @return true if the object is the right type, false if the classes
   * were not found or the object is null/of a different type",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_enabled,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_enabled(),427,430,"/**
 * Checks if IO statistics context is enabled.
 * Returns true if IO statistics are available and enabled.
 */
","* Probe to check if the thread-level IO statistics enabled.
   * If the relevant classes and methods were not found, returns false
   * @return true if the IOStatisticsContext API was found
   * and is enabled.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,toString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:toString(),671,677,"/**
 * Returns a string representation of the DynamicWrappedStatistics object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,bulkDelete_pageSize,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_pageSize(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",263,268,"/**
 * Deletes files in bulk, returning the page size.
 * @param fileSystem The file system to operate on.
 * @param path The path to delete files from.
 * @return Page size of the bulk delete operation.
 */
","* Get the maximum number of objects/files to delete in a single request.
   * @param fileSystem filesystem
   * @param path path to delete under.
   * @return a number greater than or equal to zero.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws IllegalArgumentException path not valid.
   * @throws IOException problems resolving paths
   * @throws RuntimeException invocation failure.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,bulkDelete_delete,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:bulkDelete_delete(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.util.Collection)",293,299,"/**
 * Deletes multiple paths in a file system.
 * @param fs FileSystem, base Path, paths Paths to delete.
 * @return List of deleted entries.
 */
","* Delete a list of files/objects.
   * <ul>
   *   <li>Files must be under the path provided in {@code base}.</li>
   *   <li>The size of the list must be equal to or less than the page size.</li>
   *   <li>Directories are not supported; the outcome of attempting to delete
   *       directories is undefined (ignored; undetected, listed as failures...).</li>
   *   <li>The operation is not atomic.</li>
   *   <li>The operation is treated as idempotent: network failures may
   *        trigger resubmission of the request -any new objects created under a
   *        path in the list may then be deleted.</li>
   *    <li>There is no guarantee that any parent directories exist after this call.
   *    </li>
   * </ul>
   * @param fs filesystem
   * @param base path to delete under.
   * @param paths list of paths which must be absolute and under the base path.
   * @return a list of all the paths which couldn't be deleted for a reason other than
   *          ""not found"" and any associated error message.
   * @throws UnsupportedOperationException bulk delete under that path is not supported.
   * @throws IllegalArgumentException if a path argument is invalid.
   * @throws IOException IO problems including networking, authentication and more.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,fileSystem_openFile,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:fileSystem_openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.String,org.apache.hadoop.fs.FileStatus,java.lang.Long,java.util.Map)",323,335,"/**
 * Opens a file using the FileSystem, returning an FSDataInputStream.
 * @param fs Filesystem object
 * @param path Path to the file
 * @param policy Policy for file opening
 * @return FSDataInputStream or null on failure.
 */
","* OpenFile assistant, easy reflection-based access to
   * {@code FileSystem#openFile(Path)} and blocks
   * awaiting the operation completion.
   * @param fs filesystem
   * @param path path
   * @param policy read policy
   * @param status optional file status
   * @param length optional file length
   * @param options nullable map of other options
   * @return stream of the opened file
   * @throws IOException if the operation was attempted and failed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,byteBufferPositionedReadable_readFully,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:byteBufferPositionedReadable_readFully(java.io.InputStream,long,java.nio.ByteBuffer)",412,419,"/**
 * Reads fully from an InputStream at a specified position to a ByteBuffer.
 * @param in Input stream to read from.
 * @param position Read position.
 * @param buf Destination buffer.
 */
","* Delegate to {@code ByteBufferPositionedReadable#read(long, ByteBuffer)}.
   * @param in input stream
   * @param position position within file
   * @param buf the ByteBuffer to receive the results of the read operation.
   * @throws UnsupportedOperationException if the input doesn't implement
   * the interface or, if when invoked, it is raised.
   * Note: that is the default behaviour of {@code FSDataInputStream#readFully(long, ByteBuffer)}.
   * @throws IOException if the operation was attempted and failed.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,checkIoStatisticsAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:checkIoStatisticsAvailable(),376,378,"/**
 * Checks if the I/O statistics snapshot creation method is available.
 */","* Require a IOStatistics to be available.
   * @throws UnsupportedOperationException if the method was not found.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,checkIoStatisticsContextAvailable,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:checkIoStatisticsContextAvailable(),384,386,"/**
 * Checks if the IO statistics context enablement method is available.
 */","* Require IOStatisticsContext methods to be available.
   * @throws UnsupportedOperationException if the classes/methods were not found",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ComparableVersion.java,<init>,org.apache.hadoop.util.ComparableVersion:<init>(java.lang.String),355,358,"/**
 * Parses a version string and initializes the ComparableVersion object.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,<init>,"org.apache.hadoop.util.LightWeightCache:<init>(int,int,long,long)",112,118,"/**
 * Constructs a LightWeightCache with default Timer.
 * @param recommendedLength Initial capacity suggestion.
 * @param sizeLimit Maximum number of entries.
 * @param creationExpiration Period for creation expiration.
 * @param accessExpiration Period for access expiration.
 */
","* @param recommendedLength Recommended size of the internal array.
   * @param sizeLimit the limit of the size of the cache.
   *            The limit is disabled if it is &lt;= 0.
   * @param creationExpirationPeriod the time period C &gt; 0 in nanoseconds
   *            that the creation of an entry is expired if it is added to the
   *            cache longer than C.
   * @param accessExpirationPeriod the time period A &gt;= 0 in nanoseconds that
   *            the access of an entry is expired if it is not accessed
   *            longer than A.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightGSet.java,contains,org.apache.hadoop.util.LightWeightGSet$Values:contains(java.lang.Object),250,254,"/**
 * Checks if the set contains the specified element.
 * @param o element to check for
 * @return true if found, false otherwise
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,readFileToMap,"org.apache.hadoop.util.HostsFileReader:readFileToMap(java.lang.String,java.lang.String,java.util.Map)",123,128,"/**
 * Reads a file and populates a map with key-value pairs.
 * @param type file type, filename, map to store results
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refresh,"org.apache.hadoop.util.HostsFileReader:refresh(java.io.InputStream,java.io.InputStream)",236,259,"/**
 * Refreshes the host include/exclude lists from input streams.
 * @param inFileInputStream Include list input stream.
 * @param exFileInputStream Exclude list input stream.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Filter.java,<init>,"org.apache.hadoop.util.bloom.Filter:<init>(int,int,int)",102,107,"/**
 * Constructs a Filter with specified vector size, hash count, and type.
 */","* Constructor.
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash functions to consider.
   * @param hashType type of the hashing function (see {@link Hash}).",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/Filter.java,readFields,org.apache.hadoop.util.bloom.Filter:readFields(java.io.DataInput),204,218,"/**
* Reads fields from a DataInput, handling different versions.
* @param in DataInput to read from.
* @throws IOException if an unsupported version is encountered.
*/
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,delete,org.apache.hadoop.util.bloom.CountingBloomFilter:delete(org.apache.hadoop.util.bloom.Key),135,160,"/**
 * Removes a key from the hash. Throws exceptions for null/invalid keys.
 */","* Removes a specified key from <i>this</i> counting Bloom filter.
   * <p>
   * <b>Invariant</b>: nothing happens if the specified key does not belong to <i>this</i> counter Bloom filter.
   * @param key The key to remove.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,membershipTest,org.apache.hadoop.util.bloom.DynamicBloomFilter:membershipTest(org.apache.hadoop.util.bloom.Key),175,188,"/**
 * Checks if the key is present in the matrix.
 * @param key The key to check for membership.
 * @return True if key is found, false otherwise.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(java.util.Collection),156,164,"/**
 * Adds a collection of keys as false positives.
 * @param coll Collection of keys to add.
 */
","* Adds a collection of false positive information to <i>this</i> retouched Bloom filter.
   * @param coll The collection of false positive.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(java.util.List),170,178,"/**
 * Adds a list of keys as false positives.
 * @param keys List of keys to add; throws NullPointerException if null.
 */
","* Adds a list of false positive information to <i>this</i> retouched Bloom filter.
   * @param keys The list of false positive.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,addFalsePositive,org.apache.hadoop.util.bloom.RetouchedBloomFilter:addFalsePositive(org.apache.hadoop.util.bloom.Key[]),184,192,"/**
 * Adds multiple false positives. Throws NullPointerException if keys is null.
 */
","* Adds an array of false positive information to <i>this</i> retouched Bloom filter.
   * @param keys The array of false positive.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,clearBit,org.apache.hadoop.util.bloom.RetouchedBloomFilter:clearBit(int),313,344,"/**
 * Clears the bit vector entry at the given index, removing associated keys.
 * @param index index of the bit to clear
 */
","* Clears a specified bit in the bit vector and keeps up-to-date the KeyList vectors.
   * @param index The position of the bit to clear.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,ratioRemove,org.apache.hadoop.util.bloom.RetouchedBloomFilter:ratioRemove(int[]),294,307,"/**
 * Finds the index of the smallest ratio value in the array.
 * @param h array of hash indices
 * @return index with the smallest ratio value
 */
","* Chooses the bit position that minimizes the number of false negative generated while maximizing.
   * the number of false positive removed.
   * @param h The different bit positions.
   * @return The position that minimizes the number of false negative generated while maximizing.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProgramDriver.java,driver,org.apache.hadoop.util.ProgramDriver:driver(java.lang.String[]),155,159,"/**
 * Main driver method; executes provided arguments.
 * Exits with error code if run() returns -1.
 */
","* API compatible with Hadoop 1.x.
   *
   * @param argv argv.
   * @throws Throwable Anything thrown
   *                   by the example program's main",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String),130,132,"/**
 * Adds a field with the given title, left-justified and not bold.
 * @param title The title of the field to add.
 * @return This builder instance.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,"org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,org.apache.hadoop.tools.TableListing$Justification)",134,136,"/**
 * Adds a field with the given title and justification.
 * Overloads addField to use default visibility.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/TableListing.java,addField,"org.apache.hadoop.tools.TableListing$Builder:addField(java.lang.String,boolean)",138,140,"/**
 * Adds a field with the given title and default justification.
 * @param title The title of the field.
 * @param wrap Whether to wrap the text.
 * @return This builder.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getCredentialEntry,"org.apache.hadoop.conf.Configuration:getCredentialEntry(org.apache.hadoop.security.alias.CredentialProvider,java.lang.String)",2439,2469,"/**
* Retrieves a CredentialEntry by name, handling deprecations.
* @param provider CredentialProvider to fetch from.
* @param name Credential entry name. Returns entry or null.
*/
","* Get the credential entry by name from a credential provider.
   *
   * Handle key deprecation.
   *
   * @param provider a credential provider
   * @param name alias of the credential
   * @return the credential entry or null if not found",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadResource,"org.apache.hadoop.conf.Configuration:loadResource(java.util.Properties,org.apache.hadoop.conf.Configuration$Resource,boolean)",3102,3147,"/**
 * Loads and parses a resource, optionally overlaying properties.
 * @param properties Base properties to overlay.
 * @param wrapper Resource wrapper object.
 * @param quiet Suppress error messages.
 * @return Resource object or null.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String[],java.lang.String)",594,600,"/**
 * Adds a single deprecation to be processed later.
 * @param key Key of the deprecated element.
 * @param newKeys New keys replacing the old.
 * @param customMessage Custom deprecation message.
 */
","* Adds the deprecated key to the global deprecation map.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If a key is deprecated in favor of multiple keys, they are all treated as 
   * aliases of each other, and setting any one of them resets all the others 
   * to the new value.
   *
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   * 
   * @param key to be deprecated
   * @param newKeys list of keys that take up the values of deprecated key
   * @param customMessage depcrication message
   * @deprecated use {@link #addDeprecation(String key, String newKey,
      String customMessage)} instead",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,parseNext,org.apache.hadoop.conf.Configuration$Parser:parseNext(),3448,3466,"/**
 * Parses the next XML stream event and handles it accordingly.
 */",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,openListeners,org.apache.hadoop.http.HttpServer2:openListeners(),1537,1552,"/**
 * Opens server listeners, binding them to ports or ranges.
 */","* Open the main listener for the server
   * @throws Exception",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,checkArgs,org.apache.hadoop.ha.SshFenceByTcpPort:checkArgs(java.lang.String),73,78,"/**
 * Parses the argument string.
 * @param argStr The argument string to parse.
 * @throws BadFencingConfigurationException if parsing fails.
 */
","* Verify that the argument, if given, in the conf is parseable.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,parseOpts,"org.apache.hadoop.ha.HAAdmin:parseOpts(java.lang.String,org.apache.commons.cli.Options,java.lang.String[])",505,507,"/**
 * Delegates to overloaded method with default USAGE string.
 * @param cmdName Command name
 * @param opts Options
 * @param argv Arguments
 * @return Parsed CommandLine object
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,clearParentZNode,org.apache.hadoop.ha.ActiveStandbyElector:clearParentZNode(),407,427,"/**
 * Recursively deletes the parent ZNode and its children from ZK.
 * Requires wantToBeInElection to be false.
 */","* Clear all of the state held within the parent ZNode.
   * This recursively deletes everything within the znode as well as the
   * parent znode itself. It should only be used when it's certain that
   * no electors are currently participating in the election.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException interrupted exception.",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,fenceOldActive,org.apache.hadoop.ha.ActiveStandbyElector:fenceOldActive(),1016,1047,"/**
 * Checks for an old active node and fences it if necessary.
 * Returns Stat object or null if no old node exists.
 */
","* If there is a breadcrumb node indicating that another node may need
   * fencing, try to fence that node.
   * @return the Stat of the breadcrumb node that was read, or null
   * if no breadcrumb node existed",,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,createWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:createWithRetries(java.lang.String,byte[],java.util.List,org.apache.zookeeper.CreateMode)",1082,1091,"/**
 * Creates a ZNode with retries.
 * @param path ZNode path.
 * @param data ZNode data.
 * @param acl Access control list.
 * @param mode Create mode.
 * @return ZNode's path.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,getDataWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:getDataWithRetries(java.lang.String,boolean,org.apache.zookeeper.data.Stat)",1093,1101,"/**
 * Retrieves data from Zookeeper with retries.
 * @param path ZNode path. @param watch Watch flag. @param stat Stat object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,setDataWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:setDataWithRetries(java.lang.String,byte[],int)",1103,1111,"/**
 * Sets data at the given path with retries.
 * @param path ZNode path.
 * @param data Data to set.
 * @param version Version to set.
 * @return Stat object.
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,deleteWithRetries,"org.apache.hadoop.ha.ActiveStandbyElector:deleteWithRetries(java.lang.String,int)",1113,1122,"/**
 * Deletes a znode with retries.
 * @param path znode path to delete
 * @param version znode version
 * @throws KeeperException, InterruptedException
 */
",,,,True,4
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readRangeFrom,"org.apache.hadoop.fs.VectoredReadUtils:readRangeFrom(org.apache.hadoop.fs.PositionedReadable,org.apache.hadoop.fs.FileRange,java.util.function.IntFunction)",117,142,"/**
 * Reads a specified range from a stream into a ByteBuffer.
 * @param stream Stream to read from
 * @param range Range to read
 * @param allocate Allocates ByteBuffer
 * @return CompletableFuture<ByteBuffer> containing the data
 */
","* Synchronously reads a range from the stream dealing with the combinations
   * of ByteBuffers buffers and PositionedReadable streams.
   * @param stream the stream to read from
   * @param range the range to read
   * @param allocate the function to allocate ByteBuffers
   * @return the CompletableFuture that contains the read data or an exception.
   * @throws IllegalArgumentException the range is invalid other than by offset or being null.
   * @throws EOFException the range offset is negative
   * @throws NullPointerException if the range is null.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,requestCaching,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestCaching(org.apache.hadoop.fs.impl.prefetch.BufferData),435,482,"/**
 * Requests caching for the given buffer data.
 * @param data BufferData object to be cached.
 */
","* Requests that the given block should be copied to the local cache.
   * The block must not be accessed by the caller after calling this method
   * because it will released asynchronously relative to the caller.
   *
   * @throws IllegalArgumentException if data is null.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setPrefetch,org.apache.hadoop.fs.impl.prefetch.BufferData:setPrefetch(java.util.concurrent.Future),181,186,"/**
 * Sets the prefetch action future.
 * @param actionFuture Future representing the prefetch action.
 */
","* Indicates that a prefetch operation is in progress.
   *
   * @param actionFuture the {@code Future} of a prefetch action.
   *
   * @throws IllegalArgumentException if actionFuture is null.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferData.java,setReady,org.apache.hadoop.fs.impl.prefetch.BufferData:setReady(org.apache.hadoop.fs.impl.prefetch.BufferData$State[]),209,218,"/**
 * Sets the buffer as read-only, calculates checksum, and updates state.
 */","* Marks the completion of reading data into the buffer.
   * The buffer cannot be modified once in this state.
   *
   * @param expectedCurrentState the collection of states from which transition to READY is allowed.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getSize,org.apache.hadoop.fs.impl.prefetch.BlockData:getSize(int),154,164,"/**
 * Gets the size of a block. Returns blockSize if not last,
 * otherwise calculates remaining size.
 */","* Gets the size of the given block.
   * @param blockNumber the id of the desired block.
   * @return the size of the given block.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getRelativeOffset,"org.apache.hadoop.fs.impl.prefetch.BlockData:getRelativeOffset(int,long)",194,198,"/**
 * Calculates the relative offset from a block's start.
 * @param blockNumber Block identifier.
 * @param offset Offset within the block.
 * @return Relative offset as an integer.
 */
","* Gets the relative offset corresponding to the given block and the absolute offset.
   * @param blockNumber the id of the given block.
   * @param offset absolute offset in the file.
   * @return the relative offset corresponding to the given block and the absolute offset.
   * @throws IllegalArgumentException if either blockNumber or offset is invalid.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,getStateString,org.apache.hadoop.fs.impl.prefetch.BlockData:getStateString(),225,241,"/**
 * Generates a string representation of states within blocks.
 * Returns a formatted string describing consecutive identical states.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockData.java,<init>,"org.apache.hadoop.fs.impl.prefetch.BlockData:<init>(long,int)",75,95,"/**
 * Initializes a BlockData object with file size and block size.
 * @param fileSize Size of the file in bytes.
 * @param blockSize Size of each block.
 */
","* Constructs an instance of {@link BlockData}.
   * @param fileSize the size of a file.
   * @param blockSize the file is divided into blocks of this size.
   * @throws IllegalArgumentException if fileSize is negative.
   * @throws IllegalArgumentException if blockSize is negative.
   * @throws IllegalArgumentException if blockSize is zero or negative.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,blockNumber,org.apache.hadoop.fs.impl.prefetch.FilePosition:blockNumber(),194,197,"/**
 * Retrieves the block number at the buffer's offset.
 * @return The block number as an integer.
 */
","* Gets the id of the current block.
   *
   * @return the id of the current block.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,run,org.apache.hadoop.fs.FileSystem$Cache$ClientFinalizer:run(),3834,3841,"/**
* Closes all resources in the cache.
* Attempts to close resources, logs exceptions.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem$Cache:closeAll(),3790,3792,"/**
 * Closes all resources managed by this component.
 * Throws IOException if any resource fails to close.
 */
","* Close all FileSystems in the cache, whether they are marked for
     * automatic closing or not.
     * @throws IOException a problem arose closing one or more FileSystem.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAllForUGI,org.apache.hadoop.fs.FileSystem:closeAllForUGI(org.apache.hadoop.security.UserGroupInformation),653,657,"/**
 * Closes all resources associated with the given UserGroupInformation.
 * @param ugi UserGroupInformation for which to close resources.
 * @throws IOException if an I/O error occurs during closing.
 */
","* Close all cached FileSystem instances for a given UGI.
   * Be sure those filesystems are not used anymore.
   * @param ugi user group info to close
   * @throws IOException a problem arose closing one or more filesystem.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,readFully,"org.apache.hadoop.fs.FSInputStream:readFully(long,byte[])",135,139,"/**
 * Reads data from the channel into the provided byte buffer.
 * @param position Starting position in the channel.
 * @param buffer The buffer to fill with data.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,readFully,"org.apache.hadoop.fs.BufferedFSInputStream:readFully(long,byte[],int,int)",120,123,"/**
 * Reads data from the stream fully.
 * @param position Starting position.
 * @param buffer Byte array to fill.
 * @param offset Offset in the buffer.
 * @param length Number of bytes to read.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path,java.lang.CharSequence)",2063,2066,"/**
 * Writes a CharSequence to a file.
 * @param fileContext file context
 * @param path file path
 * @param charseq data to write
 * @return FileContext object
 */
","* Write a line of text to a file. Characters are encoded into bytes using
   * UTF-8. This utility method opens the file for writing, creating the file if
   * it does not exist, or overwrites an existing file.
   *
   * @param fileContext the files system with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   *
   * @return the file context
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,createFile,org.apache.hadoop.fs.HarFileSystem:createFile(org.apache.hadoop.fs.Path),1305,1308,"/**
 * Creates a file output stream builder for the given path.
 * @param path Path to the file to be created.
 * @return FSDataOutputStreamBuilder object.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,byte[])",1851,1862,"/**
 * Writes byte array to a file in the given FileSystem.
 * @param fs FileSystem to write to
 * @param path Path of the file to write
 * @param bytes Data to write
 * @return The FileSystem used.
 */
","* Writes bytes to a file. This utility method opens the file for writing,
   * creating the file if it does not exist, or overwrites an existing file. All
   * bytes in the byte array are written to the file.
   *
   * @param fs the file system with which to create the file
   * @param path the path to the file
   * @param bytes the byte array with the bytes to write
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Iterable,java.nio.charset.Charset)",1910,1928,"/**
 * Writes lines to a file in the filesystem.
 * @param fs filesystem, path to write to, lines, charset
 * @return filesystem
 */
","* Write lines of text to a file. Each line is a char sequence and is written
   * to the file in sequence with each line terminated by the platform's line
   * separator, as defined by the system property {@code
   * line.separator}. Characters are encoded into bytes using the specified
   * charset. This utility method opens the file for writing, creating the file
   * if it does not exist, or overwrites an existing file.
   *
   * @param fs the file system with which to create the file
   * @param path the path to the file
   * @param lines a Collection to iterate over the char sequences
   * @param cs the charset to use for encoding
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence,java.nio.charset.Charset)",1983,1997,"/**
 * Writes a CharSequence to a file in the FileSystem.
 * @param fs FileSystem to write to
 * @param path Path to write to
 * @param charseq Data to write
 * @param cs Charset to use for encoding
 * @return The FileSystem
 */
","* Write a line of text to a file. Characters are encoded into bytes using the
   * specified charset. This utility method opens the file for writing, creating
   * the file if it does not exist, or overwrites an existing file.
   *
   * @param fs the file system with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   * @param cs the charset to use for encoding
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createFile,org.apache.hadoop.fs.FilterFileSystem:createFile(org.apache.hadoop.fs.Path),699,702,"/**
 * Creates a file output stream builder for the given path.
 * @param path Path to the file to be created.
 * @return FSDataOutputStreamBuilder object.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,appendFile,org.apache.hadoop.fs.HarFileSystem:appendFile(org.apache.hadoop.fs.Path),1310,1313,"/**
 * Delegates append file operation to the underlying file system.
 * @param path Path to append to.
 * @return FSDataOutputStreamBuilder for appending.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,appendFile,org.apache.hadoop.fs.FilterFileSystem:appendFile(org.apache.hadoop.fs.Path),704,707,"/**
 * Delegates to the underlying file system's append file builder.
 * @param path Path to append to.
 * @return FSDataOutputStreamBuilder for appending.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],long,long,boolean)",150,153,"/**
 * Constructs a BlockLocation with null topology.
 * @param names Block names. @param hosts Hostnames.
 * @param offset Offset. @param length Length. @param corrupt Corrupt flag.
 */
","* Constructor with host, name, network topology, offset, length 
   * and corrupt flag.
   * @param names names.
   * @param hosts hosts.
   * @param topologyPaths topologyPaths.
   * @param offset offset.
   * @param length length.
   * @param corrupt corrupt.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpFs.java,getServerDefaults,org.apache.hadoop.fs.ftp.FtpFs:getServerDefaults(),60,64,"/**
 * Returns the server defaults from FtpConfigKeys.
 * @return FsServerDefaults object containing server defaults.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpFs.java,getServerDefaults,org.apache.hadoop.fs.ftp.FtpFs:getServerDefaults(org.apache.hadoop.fs.Path),66,69,"/**
 * Returns the default server configuration.
 * Uses FtpConfigKeys to retrieve the defaults.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,getServerDefaults,org.apache.hadoop.fs.local.RawLocalFs:getServerDefaults(org.apache.hadoop.fs.Path),66,70,"/**
 * Retrieves server defaults from LocalConfigKeys.
 * @return FsServerDefaults object
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,getServerDefaults,org.apache.hadoop.fs.local.RawLocalFs:getServerDefaults(),72,76,"/**
 * Returns the server defaults from the local configuration.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getServerDefaults(),1135,1139,"/**
* Returns the server defaults from the local configuration.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getServerDefaults(org.apache.hadoop.fs.Path),1141,1144,"/**
 * Retrieves the server defaults.
 * Returns the server defaults from LocalConfigKeys.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults(),292,296,"/**
 * Returns the server defaults from the local configuration.
 * @return FsServerDefaults object
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFs:getServerDefaults(org.apache.hadoop.fs.Path),298,307,"/**
 * Retrieves server defaults for a given path.
 * @param f Path to resolve; returns defaults on failure.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(java.lang.String,java.lang.String,int)",93,95,"/**
 * Creates a new counter with given name, description, and initial value.
 * @param name Counter name
 * @param desc Counter description
 * @param iVal Initial counter value
 * @return New MutableCounterInt object
 */
","* Create a mutable integer counter
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new counter object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newCounter,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newCounter(java.lang.String,java.lang.String,long)",117,119,"/**
 * Creates a new MutableCounterLong with given name, description, and initial value.
 * @param name Counter name
 * @param desc Counter description
 * @param iVal Initial counter value
 * @return New MutableCounterLong instance
 */
","* Create a mutable long integer counter
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new counter object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,long)",166,168,"/**
* Creates a new MutableGaugeLong with given name, description, and initial value.
* @param name Gauge name
* @param desc Gauge description
* @param iVal Initial gauge value
* @return A new MutableGaugeLong instance
*/
","* Create a mutable long integer gauge
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new gauge object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,float)",190,192,"/**
 * Creates a new MutableGaugeFloat with provided name, description, and initial value.
 * @param name Gauge name
 * @param desc Gauge description
 * @param iVal Initial gauge value
 * @return New MutableGaugeFloat instance
 */
","* Create a mutable float gauge
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new gauge object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newGauge,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newGauge(java.lang.String,java.lang.String,int)",142,144,"/**
 * Creates a new MutableGaugeInt with given name, description, and initial value.
 * @param name Gauge name
 * @param desc Gauge description
 * @param iVal Initial integer value
 * @return New MutableGaugeInt instance
 */
","* Create a mutable integer gauge
   * @param name  of the metric
   * @param desc  metric description
   * @param iVal  initial value
   * @return a new gauge object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/AsyncCallHandler.java,checkCalls,org.apache.hadoop.io.retry.AsyncCallHandler$AsyncCallQueue:checkCalls(),126,143,"/**
 * Checks async calls, removes done calls, and finds min wait time.
 * Returns the minimum wait time found or Processor.MAX_WAIT_PERIOD.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,writeCompressedStringArray,"org.apache.hadoop.io.WritableUtils:writeCompressedStringArray(java.io.DataOutput,java.lang.String[])",149,158,"/**
 * Writes a string array to a DataOutput, compressing each string.
 * @param out DataOutput to write to
 * @param s String array to write
 * @throws IOException if an I/O error occurs
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,copy,org.apache.hadoop.fs.statistics.MeanStatistic:copy(),281,283,"/**
 * Creates a copy of this MeanStatistic object.
 */
","* Create a copy of this instance.
   * @return copy.
   *",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,toString,org.apache.hadoop.fs.statistics.IOStatisticsLogging$SourceToString:toString(),297,302,"/**
 * Returns a string representation of the IOStatisticsBinding.
 * Uses source's toString if available, otherwise returns NULL_SOURCE.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,logIOStatisticsAtDebug,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(org.slf4j.Logger,java.lang.String,java.lang.Object)",227,238,"/**
 * Logs IO statistics at debug level if enabled.
 * @param log Logger instance.
 * @param message Debug message.
 * @param source Source object for stats.
 */
","* Extract any statistics from the source and log at debug, if
   * the log is set to log at debug.
   * No-op if logging is not at debug or the source is null/of
   * the wrong type/doesn't provide statistics.
   * @param log log to log to
   * @param message message for log -this must contain ""{}"" for the
   * statistics report to actually get logged.
   * @param source source object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputStream.java,toString,org.apache.hadoop.fs.FSInputStream:toString(),148,158,"/**
 * Returns a string representation of the object, including IO stats.
 */
","* toString method returns the superclass toString, but if the subclass
   * implements {@link IOStatisticsSource} then those statistics are
   * extracted and included in the output.
   * That is: statistics of subclasses are automatically reported.
   * @return a string value.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatistics_toPrettyString,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatistics_toPrettyString(java.lang.Object),324,328,"/**
 * Converts IOStatistics object to a pretty string representation.
 * @param statistics The IOStatistics object to convert.
 * @return A string representation or empty string if null.
 */
","* Convert IOStatistics to a string form, with all the metrics sorted
   * and empty value stripped.
   * @param statistics A statistics instance; may be null
   * @return string value or the empty string if null",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,measureDurationOfInvocation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:measureDurationOfInvocation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)",484,507,"/**
 * Measures the duration of an invocation, tracking success/failure.
 * @param factory DurationTrackerFactory
 * @param statistic Statistic name
 * @param input Invocation to measure
 * @return Duration of the invocation
 */
","* Given an IOException raising callable/lambda expression,
   * execute it and update the relevant statistic,
   * returning the measured duration.
   *
   * {@link #trackDurationOfInvocation(DurationTrackerFactory, String, InvocationRaisingIOE)}
   * with the duration returned for logging etc.; added as a new
   * method to avoid linking problems with any code calling the existing
   * method.
   *
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @return the duration of the operation, as measured by the duration tracker.
   * @throws IOException IO failure.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,trackDurationOfSupplier,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfSupplier(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,java.util.function.Supplier)",642,663,"/**
 * Tracks duration of a Supplier, records success/failure, then closes tracker.
 * @param input Supplier to execute and track.
 * @return Result of the Supplier.
 */
","* Given a Java supplier, evaluate it while
   * tracking the duration of the operation and success/failure.
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @param <B> return type.
   * @return the output of the supplier.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,addToLinkedListAndEvictIfRequired,org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:addToLinkedListAndEvictIfRequired(org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache$Entry),421,439,"/**
 * Adds an entry to the linked list and evicts if size exceeds limit.
 */","* Add the given entry to the head of the linked list and if the LRU cache size
   * exceeds the max limit, evict tail of the LRU linked list.
   *
   * @param entry Block entry to add.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreImpl.java,<init>,"org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreImpl:<init>(java.util.List,java.util.List,java.util.List,java.util.List,java.util.List)",92,140,"/**
 * Initializes the store with counters, gauges, mins, maxes, means.
 * @param counters List of counter keys
 * @param gauges List of gauge keys
 * @param minimums List of minimum keys
 * @param maximums List of maximum keys
 * @param meanStatistics List of mean statistic keys
 */
","* Constructor invoked via the builder.
   * @param counters keys to use for the counter statistics.
   * @param gauges names of gauges
   * @param minimums names of minimums
   * @param maximums names of maximums
   * @param meanStatistics names of mean statistics.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(byte[],int,int)",162,220,"/**
 * Reads up to 'len' bytes from the stream into the provided byte array.
 * @param b buffer to read into, off offset, len number of bytes
 * @return number of bytes read, or -1 if end of stream
 */
","* Decryption is buffer based.
   * If there is data in {@link #outBuffer}, then read it out of this buffer.
   * If there is no data in {@link #outBuffer}, then read more from the 
   * underlying stream and do the decryption.
   * @param b the buffer into which the decrypted data is read.
   * @param off the buffer offset.
   * @param len the maximum number of decrypted data bytes to read.
   * @return int the total number of decrypted data bytes read into the buffer.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(long,byte[],int,int)",395,425,"/**
 * Decrypts data at the given position into the provided buffer.
 * @param position starting position for decryption
 * @param buffer buffer to hold decrypted data
 * @param offset offset within buffer
 * @param length number of bytes to decrypt
 */
","* Decrypt length bytes in buffer starting at offset. Output is also put 
   * into buffer starting at offset. It is thread-safe.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(long,java.nio.ByteBuffer,int,int)",456,499,"/**
 * Decrypts data from a buffer, updating decryption state.
 * @param filePosition File position for decryption context.
 */
","* Decrypts the given {@link ByteBuffer} in place. {@code length} bytes are
   * decrypted from {@code buf} starting at {@code start}.
   * {@code buf.position()} and {@code buf.limit()} are unchanged after this
   * method returns. This method is thread-safe.
   *
   * <p>
   *   This method decrypts the input buf chunk-by-chunk and writes the
   *   decrypted output back into the input buf. It uses two local buffers
   *   taken from the {@link #bufferPool} to assist in this process: one is
   *   designated as the input buffer and it stores a single chunk of the
   *   given buf, the other is designated as the output buffer, which stores
   *   the output of decrypting the input buffer. Both buffers are of size
   *   {@link #bufferSize}.
   * </p>
   *
   * <p>
   *   Decryption is done by using a {@link Decryptor} and the
   *   {@link #decrypt(Decryptor, ByteBuffer, ByteBuffer, byte)} method. Once
   *   the decrypted data is written into the output buffer, is is copied back
   *   into buf. Both buffers are returned back into the pool once the entire
   *   buf is decrypted.
   * </p>
   *
   * @param filePosition the current position of the file being read
   * @param buf the {@link ByteBuffer} to decrypt
   * @param length the number of bytes in {@code buf} to decrypt
   * @param start the position in {@code buf} to start decrypting data from",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,decrypt,"org.apache.hadoop.crypto.CryptoInputStream:decrypt(java.nio.ByteBuffer,int,int)",649,670,"/**
 * Decrypts a portion of the input buffer into the output buffer.
 */","* Decrypts the given {@link ByteBuffer} in place. {@code length} bytes are
   * decrypted from {@code buf} starting at {@code start}.
   * {@code buf.position()} and {@code buf.limit()} are unchanged after this
   * method returns.
   *
   * @see #decrypt(long, ByteBuffer, int, int)",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,<init>,"org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[],long)",124,140,"/**
 * Creates a CryptoInputStream with the given input stream, codec, buffer size, key, IV, and offset.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,seek,org.apache.hadoop.crypto.CryptoInputStream:seek(long),523,546,"/**
* Seeks to the specified position in the stream.
* @param pos the position to seek to
* @throws IOException if an I/O error occurs
*/
",Seek to a position.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,skip,org.apache.hadoop.crypto.CryptoInputStream:skip(long),549,577,"/**
 * Skips specified number of bytes from the stream.
 * @param n number of bytes to skip, must be non-negative
 * @return number of bytes actually skipped
 */
",Skip n bytes,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,seekToNewSource,org.apache.hadoop.crypto.CryptoInputStream:seekToNewSource(long),693,705,"/**
 * Seeks to a new source position.
 * @param targetPos The target position to seek to.
 * @return True if seek was successful, false otherwise.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,write,org.apache.hadoop.crypto.CryptoOutputStream:write(int),271,275,"/**
 * Writes a single byte to the underlying stream.
 * @param b The byte to write.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,close,org.apache.hadoop.crypto.CryptoOutputStream:close(),238,256,"/**
 * Closes this stream, flushing and releasing resources.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,hflush,org.apache.hadoop.crypto.CryptoOutputStream:hflush(),294,300,"/**
 * Flushes the output stream, and calls hflush on the underlying stream.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,hsync,org.apache.hadoop.crypto.CryptoOutputStream:hsync(),302,308,"/**
 * Performs a half-synchronization. Flushes and calls out's hsync().
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/filter/GlobFilter.java,compile,org.apache.hadoop.metrics2.filter.GlobFilter:compile(java.lang.String),36,39,"/**
 * Compiles a glob string into a Pattern.
 * @param s The glob string to compile.
 * @return A Pattern object representing the glob.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,<init>,org.apache.hadoop.fs.GlobFilter:<init>(java.lang.String),49,51,"/**
* Constructs a GlobFilter with the given file pattern.
* @param filePattern The glob pattern for file filtering.
*/
","* Creates a glob filter with the specified file pattern.
   *
   * @param filePattern the file pattern.
   * @throws IOException thrown if the file pattern is incorrect.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GlobFilter.java,<init>,"org.apache.hadoop.fs.GlobFilter:<init>(java.lang.String,org.apache.hadoop.fs.PathFilter)",60,62,"/**
 * Initializes a GlobFilter with a file pattern and PathFilter.
 * @param filePattern Pattern to match files against.
 * @param filter Filter to apply to matched paths.
 */
","* Creates a glob filter with the specified file pattern and an user filter.
   *
   * @param filePattern the file pattern.
   * @param filter user filter in addition to the glob pattern.
   * @throws IOException thrown if the file pattern is incorrect.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTar,"org.apache.hadoop.fs.FileUtil:unTar(java.io.InputStream,java.io.File,boolean)",985,1003,"/**
 * Extracts a tar archive to the specified directory.
 * @param inputStream Input stream for the tar archive.
 * @param untarDir Directory to extract the archive to.
 * @param gzipped Whether the archive is gzipped.
 */
","* Given a Tar File as input it will untar the file in a the untar directory
   * passed as the second parameter
   *
   * This utility will untar "".tar"" files and "".tar.gz"",""tgz"" files.
   *
   * @param inputStream The tar file as input.
   * @param untarDir The untar directory where to untar the tar file.
   * @param gzipped The input stream is gzipped
   *                TODO Use magic number and PusbackInputStream to identify
   * @throws IOException an exception occurred
   * @throws InterruptedException command interrupted
   * @throws ExecutionException task submit failed",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAllStatistics,org.apache.hadoop.fs.FileContext:getAllStatistics(),2420,2422,"/**
 * Retrieves all file system statistics.
 * @return A map of URI to Statistics objects.
 */
","* @return Map of uri and statistics for each filesystem instantiated. The uri
   *         consists of scheme and authority for the filesystem.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,clearStatistics,org.apache.hadoop.fs.FileContext:clearStatistics(),2404,2406,"/**
 * Clears the file system statistics.
 */","* Clears all the statistics stored in AbstractFileSystem, for all the file
   * systems.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,primitiveCreate,"org.apache.hadoop.fs.FilterFileSystem:primitiveCreate(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",551,559,"/**
 * Delegates file creation to the underlying filesystem.
 * @param f Path to create, permissions, flags, etc.
 * @return FSDataOutputStream created, or null on failure.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:<init>(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir,long,org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.fs.viewfs.InodeTree,org.apache.hadoop.conf.Configuration)",977,988,"/**
 * Constructs a ViewFileSystem directory.
 * @param dir Directory inode, cTime, ugi, URI, fsState, config.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,<init>,org.apache.hadoop.fs.FilterFs:<init>(org.apache.hadoop.fs.AbstractFileSystem),63,66,"/**
 * Constructs a FilterFs with the given AbstractFileSystem.
 * @param fs the AbstractFileSystem to filter
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/util/HHUtil.java,getPiggyBacksFromInput,"org.apache.hadoop.io.erasurecode.coder.util.HHUtil:getPiggyBacksFromInput(java.nio.ByteBuffer[],int[],int,int,org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder)",64,120,"/**
 * Generates piggyback data blocks from input buffers using erasure coding.
 * @param inputs Input buffers.
 * @return Array of piggyback ByteBuffer objects.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureEncoder.java,encode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureEncoder:encode(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",146,150,"/**
 * Encodes input chunks to output chunks using internal encoding.
 * @param inputs Input ECChunk array.
 * @param outputs Output ECChunk array.
 */
","* Encode with inputs and generates outputs. More see above.
   *
   * @param inputs input buffers to read data from
   * @param outputs output buffers to put the encoded data into, read to read
   *                after the call
   * @throws IOException if the encoder is closed.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,<init>,org.apache.hadoop.io.ArrayPrimitiveWritable$Internal:<init>(java.lang.Object),163,165,"/**
 * Internal constructor, used for writing values.
 * @param value The value to be set.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,acquireHelper,"org.apache.hadoop.fs.impl.prefetch.BufferPool:acquireHelper(int,boolean)",161,187,"/**
 * Acquires buffer data for a block. Blocks if `canBlock` is true.
 * @param blockNumber Block number to acquire.
 * @param canBlock Whether to block while acquiring.
 * @return BufferData object or null if acquisition fails.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,numAvailable,org.apache.hadoop.fs.impl.prefetch.BufferPool:numAvailable(),300,303,"/**
 * Returns the number of available objects in the pool.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BlockingThreadPoolExecutorService.java,newInstance,"org.apache.hadoop.util.BlockingThreadPoolExecutorService:newInstance(int,int,long,java.util.concurrent.TimeUnit,java.lang.String)",122,148,"/**
 * Creates a BlockingThreadPoolExecutorService with specified parameters.
 * @param activeTasks Active thread count.
 * @param waitingTasks Waiting task queue capacity.
 * @return BlockingThreadPoolExecutorService instance.
 */
","* A thread pool that that blocks clients submitting additional tasks if
   * there are already {@code activeTasks} running threads and {@code
   * waitingTasks} tasks waiting in its queue.
   *
   * @param activeTasks maximum number of active tasks
   * @param waitingTasks maximum number of waiting tasks
   * @param keepAliveTime time until threads are cleaned up in {@code unit}
   * @param unit time unit
   * @param prefixName prefix of name for threads
   * @return BlockingThreadPoolExecutorService.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,setData,"org.apache.hadoop.fs.impl.prefetch.FilePosition:setData(org.apache.hadoop.fs.impl.prefetch.BufferData,long,long)",109,128,"/**
 * Sets data, buffer, offsets and initializes read state.
 * @param bufferData Data to be set.
 * @param startOffset Start offset for data.
 * @param readOffset Read offset.
 */
","* Associates a buffer with this file.
   *
   * @param bufferData the buffer associated with this file.
   * @param startOffset Start offset of the buffer relative to the start of a file.
   * @param readOffset Offset where reading starts relative to the start of a file.
   *
   * @throws IllegalArgumentException if bufferData is null.
   * @throws IllegalArgumentException if startOffset is negative.
   * @throws IllegalArgumentException if readOffset is negative.
   * @throws IllegalArgumentException if readOffset is outside the range [startOffset, buffer end].",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_getCurrent,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_getCurrent(),261,263,"/**
 * Returns the current IO statistics context.
 */
","* Get the context's {@link IOStatisticsContext} which
   * implements {@link IOStatisticsSource}.
   * This is either a thread-local value or a global empty context.
   * @return instance of {@link IOStatisticsContext}.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_reset,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_reset(),287,289,"/**
 * Resets the current IO statistics context.
 */
","* Reset the context's IOStatistics.
   * {@link IOStatisticsContext#reset()}",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_snapshot,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_snapshot(),296,298,"/**
 * Returns a snapshot of the current IO statistics context.
 * Returns a Serializable object representing the snapshot.
 */
","* Take a snapshot of the context IOStatistics.
   * {@link IOStatisticsContext#snapshot()}
   * @return an instance of {@link IOStatisticsSnapshot}.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_aggregate,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_aggregate(java.lang.Object),308,316,"/**
 * Aggregates IO statistics from the provided source.
 * @param source The source object containing IO statistics.
 * @return True if aggregation was successful, false otherwise.
 */
","* Aggregate into the IOStatistics context the statistics passed in via
   * IOStatistics/source parameter.
   * <p>
   * Returns false if the source is null or does not contain any statistics.
   * @param source implementation of {@link IOStatisticsSource} or {@link IOStatistics}
   * @return true if the the source object was aggregated.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,runParallel,org.apache.hadoop.util.functional.TaskPool$Builder:runParallel(org.apache.hadoop.util.functional.TaskPool$Task),385,519,"/**
 * Runs a task in parallel for each item from the iterator.
 * @param task The task to run, may throw an exception.
 */","* Parallel execution.
     * All tasks run within the same IOStatisticsContext as the
     * thread calling this method.
     * @param task task to execute
     * @param <E> exception which may be raised in execution.
     * @return true if the operation executed successfully
     * @throws E any exception raised.
     * @throws IOException IOExceptions raised by remote iterator or in execution.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsContext_setThreadIOStatisticsContext,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsContext_setThreadIOStatisticsContext(java.lang.Object),270,273,"/**
 * Sets the thread's IO statistics context.
 * @param statisticsContext The context to set, may be null.
 */
","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,setStatisticsContext,org.apache.hadoop.util.functional.TaskPool$Builder:setStatisticsContext(),524,528,"/**
 * Sets the thread's IO statistics context if one exists.
 */",* Set the statistics context for this thread.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,resetStatisticsContext,org.apache.hadoop.util.functional.TaskPool$Builder:resetStatisticsContext(),535,539,"/**
 * Resets the thread's IO statistics context to null.
 */","* Reset the statistics context if it was set earlier.
     * This unbinds the current thread from any statistics
     * context.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,processPath,org.apache.hadoop.fs.shell.find.Find:processPath(org.apache.hadoop.fs.shell.PathData),394,401,"/**
 * Applies the PathData item if not depth-first.
 * @param item PathData item to apply.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,postProcessPath,org.apache.hadoop.fs.shell.find.Find:postProcessPath(org.apache.hadoop.fs.shell.PathData),403,410,"/**
 * Applies the PathData item if depth-first search is enabled.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processOptions,org.apache.hadoop.fs.shell.Delete$Rmr:processOptions(java.util.LinkedList),174,178,"/**
 * Processes command-line options, adding ""-r"" and calling super.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processOptions,org.apache.hadoop.fs.shell.Ls$Lsr:processOptions(java.util.LinkedList),411,416,"/**
 * Processes command-line options, adding ""-R"" and calling super.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processOptions,org.apache.hadoop.fs.shell.FsUsage$Dus:processOptions(java.util.LinkedList),236,240,"/**
 * Adds ""-s"" to the argument list and processes options.
 * @param args List of command-line arguments.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,displayError,org.apache.hadoop.fs.shell.Command:displayError(java.lang.Exception),474,493,"/**
 * Logs an error, potentially wrapping InterruptedIOExceptions.
 * Logs the error message to the display, using localized message.
 */","* Display an exception prefaced with the command name.  Also increments
   * the error count for the command which will result in a non-zero exit
   * code.
   * @param e exception to display",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getPathHandle,"org.apache.hadoop.fs.FileSystem:getPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",1050,1057,"/**
 * Creates a PathHandle from a FileStatus, using default options if none provided.
 * @param stat FileStatus object
 * @param opt HandleOpt array, options for path creation
 * @return PathHandle object
 */
","* Create a durable, serializable handle to the referent of the given
   * entity.
   * @param stat Referent in the target FileSystem
   * @param opt If absent, assume {@link HandleOpt#path()}.
   * @throws IllegalArgumentException If the FileStatus does not belong to
   *         this FileSystem
   * @throws UnsupportedOperationException If {@link #createPathHandle}
   *         not overridden by subclass.
   * @throws UnsupportedOperationException If this FileSystem cannot enforce
   *         the specified constraints.
   * @return path handle.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map,long)",1227,1230,"/**
 * Constructs a ShellCommandExecutor with default shell=true.
 * @param execString Command to execute.
 * @param dir Working directory.
 * @param env Environment variables.
 * @param timeout Timeout in milliseconds.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,init,org.apache.hadoop.fs.CachingGetSpaceUsed:init(),90,102,"/**
 * Initializes the refresh process, correcting 'used' if needed.
 * If 'used' is negative, resets it and initiates refresh.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,addToken,"org.apache.hadoop.security.Credentials:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)",121,137,"/**
 * Adds a token to the map, updating private clones if necessary.
 * @param alias Text alias for the token
 * @param t Token to add
 */
","* Add a token in the storage (in memory).
   * @param alias the alias for the key
   * @param t the token object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,encrypt,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:encrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",148,152,"/**
 * Encrypts data from input buffer to output buffer.
 * @param inBuffer Input buffer containing data to encrypt.
 * @param outBuffer Output buffer for encrypted data.
 */
","* AES-CTR will consume all of the input data. It requires enough space in
     * the destination buffer to encrypt entire input buffer.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,decrypt,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:decrypt(java.nio.ByteBuffer,java.nio.ByteBuffer)",158,162,"/**
* Decrypts data from input buffer to output buffer.
* @param inBuffer Input buffer containing encrypted data.
* @param outBuffer Output buffer for decrypted data.
*/
","*  AES-CTR will consume all of the input data. It requires enough space in
     * the destination buffer to decrypt entire input buffer.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,initialize,org.apache.hadoop.fs.viewfs.RegexMountPoint:initialize(),86,96,"/**
 * Initializes resources, including regex pattern and interceptors.
 * Throws IOException if regex compilation fails.
 */
","* Initialize regex mount point.
   *
   * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",149,164,"/**
 * Constructs a Path by resolving a child path relative to a parent.
 * @param parent The parent Path.
 * @param child The child Path to resolve.
 */
","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,org.apache.hadoop.fs.Path:<init>(java.lang.String),184,223,"/**
 * Creates a Path from a string representation.
 * @param pathString The path string to represent.
 * @throws IllegalArgumentException if the path string is invalid.
 */
","* Construct a path from a String.  Path strings are URIs, but with
   * unescaped elements and some additional normalization.
   *
   * @param pathString the path string",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String,java.lang.String)",241,256,"/**
 * Constructs a Path with the given scheme, authority, and path.
 * Normalizes the path by adding prefixes as needed.
 */","* Construct a Path from components.
   *
   * @param scheme the scheme
   * @param authority the authority
   * @param path the path",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,toString,org.apache.hadoop.fs.shell.PathData:toString(),461,464,"/**
 * Returns a string representation of the URI.
 * Uses uriToString to format the URI.
 */
","* Returns the printable version of the path that is either the path
   * as given on the commandline, or the full path
   * @return String of the path",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,checkNotRelative,org.apache.hadoop.fs.Path:checkNotRelative(),92,96,"/**
 * Throws exception if path is relative and has no scheme.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getUriPath,org.apache.hadoop.fs.AbstractFileSystem:getUriPath(org.apache.hadoop.fs.Path),423,431,"/**
 * Extracts and validates the path from a Path object.
 * @param p The Path object to extract the path from.
 * @return The path string, after validation.
 */
","* Get the path-part of a pathname. Checks that URI matches this file system
   * and that the path-part is a valid name.
   * 
   * @param p path
   * 
   * @return path-part of the Path p",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,resolvePath,org.apache.hadoop.fs.AbstractFileSystem:resolvePath(org.apache.hadoop.fs.Path),506,510,"/**
 * Resolves a path, throwing exceptions on failure.
 * @param p The path to resolve.
 * @return The resolved Path object.
 */
","* Return the fully-qualified path of path f resolving the path
   * through any internal symlinks or mount point
   * @param p path to be resolved
   * @return fully qualified path 
   * @throws FileNotFoundException when file not find throw.
   * @throws AccessControlException when accees control error throw.
   * @throws IOException raised on errors performing I/O.
   * @throws UnresolvedLinkException if symbolic link on path cannot be
   * resolved internally",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,create,"org.apache.hadoop.fs.AbstractFileSystem:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",530,640,"/**
 * Creates a new data output stream.
 * @param f path of the file to create
 * @param createFlag flags for create operation
 * @param opts options for file creation
 * @return FSDataOutputStream created output stream
 */","* The specification of this method matches that of
   * {@link FileContext#create(Path, EnumSet, Options.CreateOpts...)} except
   * that the Path f must be fully qualified and the permission is absolute
   * (i.e. umask has been applied).
   *
   * @param f the path.
   * @param createFlag create_flag.
   * @param opts create ops.
   * @throws AccessControlException access controll exception.
   * @throws FileAlreadyExistsException file already exception.
   * @throws FileNotFoundException file not found exception.
   * @throws ParentNotDirectoryException parent not dir exception.
   * @throws UnsupportedFileSystemException unsupported file system exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.
   * @return output stream.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,checkPath,org.apache.hadoop.fs.FilterFs:checkPath(org.apache.hadoop.fs.Path),184,187,"/**
 * Delegates path check to the underlying FileSystem object.
 * @param path The path to check.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:delete(org.apache.hadoop.fs.Path),1505,1510,"/**
 * Deletes a file or directory.
 * @param f Path to the file/directory to delete.
 * @return True if successful, false otherwise.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsCreateModes.java,applyUMask,"org.apache.hadoop.fs.permission.FsCreateModes:applyUMask(org.apache.hadoop.fs.permission.FsPermission,org.apache.hadoop.fs.permission.FsPermission)",43,49,"/**
 * Applies umask to a file mode, returning the modified mode.
 */","* Create from unmasked mode and umask.
   *
   * @param mode mode.
   * @param umask umask.
   * @return If the mode is already
   * an FsCreateModes object, return it.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,loadPermissionInfoByNativeIO,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfoByNativeIO(),1063,1085,"/**
 * Loads permission info using native IO calls.
 * Throws IOException if an error occurs during the process.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,mkdirs,org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.Path),2495,2497,"/**
* Creates directory recursively.
* @param f the path of the directory to create
* @throws IOException if an I/O error occurs
*/
","* Call {@link #mkdirs(Path, FsPermission)} with default permission.
   * @param f path
   * @return true if the directory was created
   * @throws IOException IO failure",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set)",160,190,"/**
 * Constructs a FileStatus object with the provided file attributes.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,setPermission,org.apache.hadoop.fs.FileStatus:setPermission(org.apache.hadoop.fs.permission.FsPermission),367,370,"/**
 * Sets the file permission. Uses default if permission is null.
 */
","* Sets permission.
   * @param permission if permission is null, default value is set",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,getPermission,org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:getPermission(),111,116,"/**
 * Returns the file system permission. Lazily initializes if null.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createNonRecursive,"org.apache.hadoop.fs.FileSystem:createNonRecursive(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1432,1438,"/**
 * Creates a non-recursive output stream.
 * @param f Path to create stream on.
 * @param overwrite Whether to overwrite existing file.
 * @param bufferSize Stream buffer size.
 * @return FSDataOutputStream
 */
","* Opens an FSDataOutputStream at the indicated Path with write-progress
   * reporting. Same as create(), except fails if parent directory doesn't
   * already exist.
   * @param f the file name to open
   * @param overwrite if a file with this name already exists, then if true,
   * the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize block size
   * @param progress the progress reporter
   * @throws IOException IO failure
   * @see #setPermission(Path, FsPermission)
   * @return output stream.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,getPermission,org.apache.hadoop.fs.FSDataOutputStreamBuilder:getPermission(),146,151,"/**
 * Returns the file permission. Lazily initializes if null.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,createImmutable,org.apache.hadoop.fs.permission.FsPermission:createImmutable(short),64,66,"/**
 * Creates an immutable FsPermission object from a short value.
 * @param permission The permission value to use.
 * @return An ImmutableFsPermission object.
 */
","* Create an immutable {@link FsPermission} object.
   * @param permission permission.
   * @return FsPermission.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/AclCommands.java,processPath,org.apache.hadoop.fs.shell.AclCommands$GetfaclCommand:processPath(org.apache.hadoop.fs.shell.PathData),77,104,"/**
 * Processes a PathData item, printing file metadata and ACLs.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,createPermissions,org.apache.hadoop.security.alias.KeyStoreProvider:createPermissions(java.lang.String),68,71,"/**
 * Sets file system permissions from a string representation.
 * @param perms String representing the file system permissions.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$Writer:append(java.lang.Object,java.lang.Object)",1469,1502,"/**
 * Appends a key-value pair to the buffer, serializing and writing.
 * @param key The key to serialize.
 * @param val The value to serialize.
 */
","* Append a key/value pair.
     * @param key input Object key.
     * @param val input Object val.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,appendRaw,"org.apache.hadoop.io.SequenceFile$Writer:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)",1504,1517,"/**
 * Appends a key-value pair to the output stream.
 * @param keyData Key data as bytes.
 * @param keyOffset Key offset in keyData.
 * @param keyLength Key length.
 * @param val Value to be appended.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getCompressedSize,org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:getCompressedSize(),242,244,"/**
 * Returns the compressed size of the block.
 * @return Compressed size as a long value.
 */
","* Get the compressed size of the block in progress.
       * 
       * @return the number of compressed bytes written to the underlying FS
       *         file. The size may be smaller than actual need to compress the
       *         all data written due to internal buffering inside the
       *         compressor.
       * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,skip,org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:skip(long),528,536,"/**
 * Skips specified bytes, ensuring not to exceed file length.
 * @param n number of bytes to skip
 * @return number of bytes actually skipped
 */
","* Skips over and discards <code>n</code> bytes of data from the
     * input stream.
     *
     *The <code>skip</code> method skips over some smaller number of bytes
     * when reaching end of file before <code>n</code> bytes have been skipped.
     * The actual number of bytes skipped is returned.  If <code>n</code> is
     * negative, no bytes are skipped.
     *
     * @param      n   the number of bytes to be skipped.
     * @return     the actual number of bytes skipped.
     * @exception  IOException  if an I/O error occurs.
     *             ChecksumException if the chunk to skip to is corrupted",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,seek,org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:seek(long),550,556,"/**
 * Seeks to the specified position.
 * @param pos The position to seek to.
 * @throws EOFException if pos exceeds file length.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processPaths,"org.apache.hadoop.fs.shell.Ls:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])",270,283,"/**
 * Processes path data, sorts items, and adjusts column widths.
 * @param parent Parent PathData, items to process.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getUsed,org.apache.hadoop.fs.HarFileSystem:getUsed(org.apache.hadoop.fs.Path),1277,1280,"/**
 * Returns the number of bytes used by the file or directory.
 * @param path Path to the file or directory.
 * @return Number of bytes used.
 */
",Return the total size of all files from a specified path.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getUsed,org.apache.hadoop.fs.FilterFileSystem:getUsed(org.apache.hadoop.fs.Path),421,424,"/**
 * Gets the number of bytes used by a given path.
 * @param path The path to get the used space for.
 * @return The number of bytes used.
 */
",Return the total size of all files from a specified path.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,main,org.apache.hadoop.util.JvmPauseMonitor:main(java.lang.String[]),221,231,"/**
 * Starts the JvmPauseMonitor and continuously adds strings to a list.
 */","* Simple 'main' to facilitate manual testing of the pause monitor.
   * 
   * This main function just leaks memory into a list. Running this class
   * with a 1GB heap will very quickly go into ""GC hell"" and result in
   * log messages about the GC pauses.
   *
   * @param args args.
   * @throws Exception Exception.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,start,org.apache.hadoop.service.AbstractService:start(),185,208,"/**
 * Starts the service. Transitions to STARTED state, starts service.
 */","* {@inheritDoc}
   * @throws ServiceStateException if the current service state does not permit
   * this action",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,enterState,org.apache.hadoop.service.AbstractService:enterState(org.apache.hadoop.service.Service$STATE),440,449,"/**
 * Transitions to a new state, logging and recording lifecycle.
 * @param newState The state to transition to.
 * @return The previous state.
 */
","* Enter a state; record this via {@link #recordLifecycleEvent}
   * and log at the info level.
   * @param newState the proposed new state
   * @return the original state
   * it wasn't already in that state, and the state model permits state re-entrancy.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printDefaultRealm,org.apache.hadoop.security.KDiag:printDefaultRealm(),488,512,"/**
 * Prints the default Kerberos realm, or warns if none is found.
 */","* Get the default realm.
   * <p>
   * Not having a default realm may be harmless, so is noted at info.
   * All other invocation failures are downgraded to warn, as
   * follow-on actions may still work.
   * Failure to invoke the method via introspection is considered a failure,
   * as it's a sign of JVM compatibility issues that may have other 
   * consequences",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,equals,org.apache.hadoop.io.BytesWritable:equals(java.lang.Object),200,205,"/**
 * Compares to another object, returns true if BytesWritable.
 */",* Are the two byte sequences equal?,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,equals,org.apache.hadoop.io.Text:equals(java.lang.Object),415,420,"/**
 * Compares this Text object with another. Returns true if same type.
 */","* Returns true iff <code>o</code> is a Text with the same length and same
   * contents.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,hashCode,org.apache.hadoop.security.token.Token$PrivateToken:hashCode(),299,304,"/**
 * Calculates the hash code for the object, incorporating publicService.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,set,org.apache.hadoop.io.BytesWritable:set(org.apache.hadoop.io.BytesWritable),167,169,"/**
 * Sets the value of the object to the provided bytes.
 * @param newData BytesWritable containing the new data.
 */
","* Set the BytesWritable to the contents of the given newData.
   *
   * @param newData the value to set this BytesWritable to.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirInternal,org.apache.hadoop.util.DiskChecker:checkDirInternal(java.io.File),94,101,"/**
 * Creates a directory and checks access.
 * @param dir The directory to create and check.
 * @throws DiskErrorException if directory creation fails.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,mlock,"org.apache.hadoop.io.nativeio.NativeIO$POSIX$CacheManipulator:mlock(java.lang.String,java.nio.ByteBuffer,long)",280,283,"/**
 * Locks a memory region in the buffer.
 * @param identifier Lock identifier.
 * @param buffer The memory buffer to lock.
 * @param len Length of the buffer to lock.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,flushBuffer,"org.apache.hadoop.fs.FSOutputSummer:flushBuffer(boolean,boolean)",159,176,"/**
* Flushes data from the buffer to the output stream.
* @param keep Whether to keep partial data.
* @param flushPartial Flush partial buffer if true.
* @return Number of bytes left in the buffer.
*/",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,close,org.apache.hadoop.crypto.CryptoInputStream:close(),319,329,"/**
 * Closes this stream, releasing resources.
 * Calls super.close(), frees buffers, and closes the codec.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",85,116,"/**
 * Decodes data from input ByteBuffers, handling erasure, to outputs.
 */","* Decode with inputs and erasedIndexes, generates outputs.
   * How to prepare for inputs:
   * 1. Create an array containing data units + parity units. Please note the
   *    data units should be first or before the parity units.
   * 2. Set null in the array locations specified via erasedIndexes to indicate
   *    they're erased and no data are to read from;
   * 3. Set null in the array locations for extra redundant items, as they're
   *    not necessary to read when decoding. For example in RS-6-3, if only 1
   *    unit is really erased, then we have 2 extra items as redundant. They can
   *    be set as null to indicate no data will be used from them.
   *
   * For an example using RS (6, 3), assuming sources (d0, d1, d2, d3, d4, d5)
   * and parities (p0, p1, p2), d2 being erased. We can and may want to use only
   * 6 units like (d1, d3, d4, d5, p0, p2) to recover d2. We will have:
   *     inputs = [null(d0), d1, null(d2), d3, d4, d5, p0, null(p1), p2]
   *     erasedIndexes = [2] // index of d2 into inputs array
   *     outputs = [a-writable-buffer]
   *
   * Note, for both inputs and outputs, no mixing of on-heap buffers and direct
   * buffers are allowed.
   *
   * If the coder option ALLOW_CHANGE_INPUTS is set true (false by default), the
   * content of input buffers may change after the call, subject to concrete
   * implementation.
   *
   * @param inputs input buffers to read data from. The buffers' remaining will
   *               be 0 after decoding
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to put decoded data into according to
   *                erasedIndexes, ready for read after the call
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(byte[][],int[],byte[][])",135,145,"/**
 * Decodes data from input byte arrays to output arrays.
 * @param inputs Input byte arrays.
 * @param erasedIndexes Indexes of erased data.
 * @param outputs Output byte arrays.
 */
","* Decode with inputs and erasedIndexes, generates outputs. More see above.
   *
   * @param inputs input buffers to read data from
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to put decoded data into according to
   *                erasedIndexes, ready for read after the call
   * @throws IOException if the decoder is closed.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawErasureCoderFactory.java,createDecoder,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:createDecoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,40,"/**
 * Creates a RawErasureDecoder instance using the provided options.
 * @param coderOptions ErasureCoderOptions object
 * @return RawErasureDecoder instance
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawErasureCoderFactory.java,createEncoder,org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawErasureCoderFactory:createEncoder(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,35,"/**
 * Creates a RawErasureEncoder instance using the provided options.
 * @param coderOptions Encoder options to configure the encoder.
 * @return A RawErasureEncoder object.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,prepareDecoding,"org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:prepareDecoding(java.lang.Object[],int[])",103,115,"/**
 * Prepares decoding by checking cached erasure data.
 * @param inputs Input array, @param erasedIndexes erasure indices
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,skipToNextBlockMarker,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:skipToNextBlockMarker(),328,332,"/**
 * Skips to the next block marker in the CBZip2 stream.
 * Returns true if successful, false otherwise.
 */
","* Skips bytes in the stream until the start marker of a block is reached
   * or end of stream is reached. Used for testing purposes to identify the
   * start offsets of blocks.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,complete,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:complete(),591,599,"/**
 * Completes processing: stores CRC, sets state to EOF, and checks CRC.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,getAndMoveToFrontDecode,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:getAndMoveToFrontDecode(),821,1004,"/**
 * Decodes and moves data to the front based on run lengths.
 * Reads bit streams and updates internal data structures.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,<init>,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:<init>(java.io.OutputStream),598,600,"/**
 * Creates a CBZip2OutputStream with default block size.
 * @param out The OutputStream to write compressed data to.
 */
","* Constructs a new <tt>CBZip2OutputStream</tt> with a blocksize of 900k.
  *
  * <p>
  * <b>Attention: </b>The caller is resonsible to write the two BZip2 magic
  * bytes <tt>""BZ""</tt> to the specified stream prior to calling this
  * constructor.
  * </p>
  *
  * @param out *
  *            the destination stream.
  *
  * @throws IOException
  *             if an I/O error occurs in the specified stream.
  * @throws NullPointerException
  *             if <code>out == null</code>.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,endBlock,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:endBlock(),788,831,"/**
 * Ends the current block, calculating CRCs and writing headers.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BlockDecompressorStream.java,<init>,"org.apache.hadoop.io.compress.BlockDecompressorStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",60,62,"/**
 * Creates a BlockDecompressorStream with an InputStream and Decompressor.
 */","* Create a {@link BlockDecompressorStream}.
   * 
   * @param in input stream
   * @param decompressor decompressor to use
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,createInputStream,org.apache.hadoop.io.compress.PassthroughCodec:createInputStream(java.io.InputStream),132,136,"/**
 * Creates a compression input stream from an input stream.
 * @param in The input stream to compress.
 * @return A CompressionInputStream object.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,read,org.apache.hadoop.io.compress.DecompressorStream:read(),89,93,"/**
 * Reads a single byte from the stream.
 * Returns -1 if EOF, otherwise returns the byte value.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,skip,org.apache.hadoop.io.compress.DecompressorStream:skip(long),193,213,"/**
 * Skips the specified number of bytes from the stream.
 * @param n number of bytes to skip
 * @return the actual number of skipped bytes
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,<init>,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:<init>(),78,82,"/**
 * Default constructor, uses default compression level and buffer size.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCompressor,org.apache.hadoop.io.file.tfile.Compression$Algorithm:getCompressor(),285,308,"/**
 * Gets a compressor from the CodecPool, resets it, and returns.
 * Returns null if no codec is available.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createOutputStream,org.apache.hadoop.io.compress.DefaultCodec:createOutputStream(java.io.OutputStream),53,58,"/**
 * Creates a compression output stream using the codec pool.
 * @param out The underlying output stream.
 * @return A CompressionOutputStream.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createOutputStream,org.apache.hadoop.io.compress.Lz4Codec:createOutputStream(java.io.OutputStream),66,71,"/**
 * Creates a compression output stream using the codec pool.
 * @param out The underlying output stream.
 * @return CompressionOutputStream instance.
 * @throws IOException if an I/O error occurs.
 */
","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out the location for the final output stream
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createOutputStream,org.apache.hadoop.io.compress.BZip2Codec:createOutputStream(java.io.OutputStream),105,110,"/**
 * Creates a compression output stream using the codec pool.
 * @param out The underlying output stream.
 * @return CompressionOutputStream instance.
 * @throws IOException if an I/O error occurs.
 */
","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out        the location for the final output stream
   * @return a stream the user can write uncompressed data to, to have it 
   *         compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createOutputStream,org.apache.hadoop.io.compress.ZStandardCodec:createOutputStream(java.io.OutputStream),121,126,"/**
 * Creates a compression output stream using the codec pool.
 * @param out The underlying output stream.
 * @return A CompressionOutputStream.
 * @throws IOException if an I/O error occurs.
 */
","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out the location for the final output stream
   * @return a stream the user can write uncompressed data to have compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createOutputStream,org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream),44,49,"/**
 * Creates a compression output stream using the codec pool.
 * @param out The underlying output stream.
 * @return CompressionOutputStream instance.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createOutputStream,org.apache.hadoop.io.compress.SnappyCodec:createOutputStream(java.io.OutputStream),66,71,"/**
 * Creates a compression output stream using the codec pool.
 * @param out the underlying output stream
 * @return CompressionOutputStream instance
 */
","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream}.
   *
   * @param out the location for the final output stream
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressorStream.java,close,org.apache.hadoop.io.compress.CompressorStream:close(),102,112,"/**
 * Closes the resource, preventing further use.
 * Throws IOException if an error occurs during closing.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,close,org.apache.hadoop.io.MapFile$Writer:close(),385,389,"/**
 * Closes the data and index resources, releasing associated resources.
 */",Close the map.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,finish,org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:finish(),178,188,"/**
 * Flushes output and releases compressor resources.
 */",* Finishing up the current block.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createInputStream,org.apache.hadoop.io.compress.DefaultCodec:createInputStream(java.io.InputStream),79,84,"/**
 * Creates a compression input stream using the codec pool.
 * @param in Input stream to be compressed.
 * @return CompressionInputStream object.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createInputStream,org.apache.hadoop.io.compress.Lz4Codec:createInputStream(java.io.InputStream),130,135,"/**
 * Creates a compression input stream using the codec pool.
 * @param in Input stream to compress
 * @return CompressionInputStream
 */
","* Create a {@link CompressionInputStream} that will read from the given
   * input stream.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createInputStream,org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream),160,165,"/**
 * Creates a compression input stream using the codec pool.
 * @param in The input stream to compress.
 * @return A CompressionInputStream.
 * @throws IOException if an I/O error occurs.
 */
","* Create a {@link CompressionInputStream} that will read from the given
   * input stream and return a stream for uncompressed data.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createInputStream,org.apache.hadoop.io.compress.ZStandardCodec:createInputStream(java.io.InputStream),178,183,"/**
 * Creates a compression input stream using a codec pool.
 * @param in The input stream to compress.
 * @return CompressionInputStream object.
 */
","* Create a {@link CompressionInputStream} that will read from the given
   * input stream.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createInputStream,org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream),76,81,"/**
 * Creates a compression input stream using the codec pool.
 * @param in The input stream to compress.
 * @return A CompressionInputStream.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createInputStream,org.apache.hadoop.io.compress.SnappyCodec:createInputStream(java.io.InputStream),127,132,"/**
 * Creates a compression input stream using the codec pool.
 * @param in The input stream to compress.
 * @return A CompressionInputStream.
 */
","* Create a {@link CompressionInputStream} that will read from the given
   * input stream.
   *
   * @param in the stream to read compressed bytes from
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DecompressorStream.java,close,org.apache.hadoop.io.compress.DecompressorStream:close(),221,230,"/**
 * Closes the resource, preventing further operations.
 * Throws IOException if an error occurs during closing.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,close,org.apache.hadoop.fs.shell.Display$TextRecordInputStream:close(),259,263,"/**
 * Closes this stream and any associated resources.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:close(),3181,3191,"/**
 * Closes input, output, and index output streams, if they exist.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,close,org.apache.hadoop.io.MapFile$Reader:close(),881,887,"/**
 * Closes the index and data resources.
 * Closes index if not already closed.
 */
","* Close the map.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:close(),3877,3880,"/**
 * Closes the input stream and sets it to null.
 */
",closes the underlying reader,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,finish,org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:finish(),532,539,"/**
 * Releases resources and closes the input stream.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createCompressor,org.apache.hadoop.io.compress.DefaultCodec:createCompressor(),74,77,"/**
 * Creates a Zlib compressor using the provided configuration.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createDirectDecompressor,org.apache.hadoop.io.compress.DefaultCodec:createDirectDecompressor(),108,111,"/**
 * Creates a DirectDecompressor using the provided configuration.
 */",* {@inheritDoc},,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createDecompressor,org.apache.hadoop.io.compress.DefaultCodec:createDecompressor(),100,103,"/**
 * Creates and returns a Zlib decompressor using the configuration.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeFileHeader,org.apache.hadoop.io.SequenceFile$Writer:writeFileHeader(),1274,1289,"/**
 * Writes the file header to the output stream.
 * Writes version, class names, compression flags, metadata, and sync bytes.
 */",Write and flush the file header.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,write,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:write(java.io.DataOutput),217,229,"/**
 * Writes the object to the output stream, validating lengths.
 * Throws IOException if any string exceeds Text.DEFAULT_MAX_LEN.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,write,org.apache.hadoop.security.Credentials:write(java.io.DataOutput),354,371,"/**
 * Writes the data to the provided DataOutput, including tokens and secrets.
 */","* Stores all the keys to DataOutput.
   * @param out DataOutput.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,storeDelegationKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:storeDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),681,684,"/**
 * Stores a delegation key.
 * @param key The delegation key to store.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,updateDelegationKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:updateDelegationKey(org.apache.hadoop.security.token.delegation.DelegationKey),686,689,"/**
* Updates a delegation key.
* @param key The delegation key to update.
* @throws IOException if an I/O error occurs.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readFields,org.apache.hadoop.io.SequenceFile$Metadata:readFields(java.io.DataInput),771,783,"/**
 * Reads file metadata from the input stream.
 * @param in DataInput stream containing metadata
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,readFields,org.apache.hadoop.security.token.Token:readFields(java.io.DataInput),307,321,"/**
 * Reads fields from DataInput, populating identifier, password, kind, service.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,readFields,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:readFields(java.io.DataInput),189,203,"/**
 * Reads delegation token fields from the input stream.
 * Throws IOException if version is unknown.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,readBlock,org.apache.hadoop.io.SequenceFile$Reader:readBlock(),2294,2330,"/**
 * Reads a block of data from the input stream.
 * Handles lazy decompression and sync checks.
 */",Read the next 'compressed' block,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,seekToCurrentValue,org.apache.hadoop.io.SequenceFile$Reader:seekToCurrentValue(),2336,2369,"/**
 * Seeks to the current value, handling compressed/decompressed blocks.
 */","* Position valLenIn/valIn to the 'value' 
     * corresponding to the 'current' key",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,createTokenInfo,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:createTokenInfo(byte[]),262,271,"/**
 * Creates a DelegationTokenInformation object from byte array.
 * @param tokenInfoBytes byte array containing token information
 * @return DelegationTokenInformation object
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,readFields,org.apache.hadoop.fs.permission.PermissionStatus:readFields(java.io.DataInput),96,101,"/**
 * Reads fields from the DataInput stream.
 * Populates username, groupname, and permission.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,readString,org.apache.hadoop.io.Text:readString(java.io.DataInput),556,558,"/**
 * Reads a string from the DataInput.
 * Uses Integer.MAX_VALUE as the length limit.
 */
","* @return Read a UTF8 encoded string from in.
   * @param in input in.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,getDelegationKey,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getDelegationKey(int),385,412,"/**
 * Retrieves a delegation key by ID, caching if not found.
 * @param keyId The ID of the delegation key to retrieve.
 * @return The DelegationKey object or null if not found.
 */
","* Obtains the DelegationKey from the SQL database.
   * @param keyId KeyId of the DelegationKey to obtain.
   * @return DelegationKey that matches the given keyId or null
   *         if it doesn't exist in the database.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,processKeyAddOrUpdate,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processKeyAddOrUpdate(byte[]),391,397,"/**
 * Processes key addition or update from byte array.
 * @param data byte array containing key data
 * @throws IOException if an I/O error occurs
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getKeyFromZK,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getKeyFromZK(int),579,598,"/**
 * Retrieves a DelegationKey from Zookeeper by keyId.
 * @param keyId The ID of the delegation key.
 * @return DelegationKey object or null if not found.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$MetaIndexEntry:write(java.io.DataOutput),843,848,"/**
 * Writes the data to the output stream.
 * Writes prefix, algorithm name, and region data.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,write,org.apache.hadoop.io.file.tfile.TFile$TFileMeta:write(java.io.DataOutput),2098,2102,"/**
 * Writes the data to the provided DataOutput stream.
 * Writes API version, record count, and string comparator.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$DataIndex:write(java.io.DataOutput),897,905,"/**
 * Writes the configuration to the given DataOutput.
 * @param out Output stream to write to.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,selectDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:selectDelegationToken(java.net.URL,org.apache.hadoop.security.Credentials)",344,354,"/**
 * Retrieves a delegation token from the specified service.
 * @param url URL of the delegation token service.
 * @param creds Credentials to use for token retrieval.
 * @return Delegation token or null if not found.
 */
","* Select a delegation token from all tokens in credentials, based on url.
   *
   * @param url url.
   * @param creds credentials.
   * @return token.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getServerToken,org.apache.hadoop.security.SaslRpcClient:getServerToken(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),279,293,"/**
 * Retrieves a server token based on the provided authentication type.
 * @param authType SASL authentication type
 * @return Token object or null if no token is supported
 */
","* Try to locate the required token for the server.
   * 
   * @param authType of the SASL client
   * @return Token for server, or null if no token available
   * @throws IOException - token selector cannot be instantiated",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setTokenService,"org.apache.hadoop.security.SecurityUtil:setTokenService(org.apache.hadoop.security.token.Token,java.net.InetSocketAddress)",456,466,"/**
 * Sets the service on the provided token, or logs a warning.
 * @param token The token to set the service on.
 * @param addr The address of the service.
 */
","* Set the given token's service to the format expected by the RPC client 
   * @param token a delegation token
   * @param addr the socket for the rpc connection",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,deleteKey,org.apache.hadoop.crypto.key.UserProvider:deleteKey(java.lang.String),102,113,"/**
 * Deletes a key and all its versions from the store and cache.
 * @param name Key name to delete.
 * @throws IOException if the key does not exist.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.UserProvider:rollNewVersion(java.lang.String,byte[])",115,131,"/**
 * Creates a new version of a key.
 * @param name Key name.
 * @param material Key material.
 * @return KeyVersion object representing the new version.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.UserProvider:getKeyVersions(java.lang.String),167,181,"/**
 * Retrieves key versions for a given name.
 * @param name Key name.
 * @return List of KeyVersion objects.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,tokenFromProto,org.apache.hadoop.ipc.ProtobufHelper:tokenFromProto(org.apache.hadoop.security.proto.SecurityProtos$TokenProto),114,117,"/**
 * Creates a Token from a TokenProto.
 * @param tokenProto The TokenProto to convert.
 * @return A Token object.
 */
","* Get a token from a TokenProto payload.
   * @param tokenProto marshalled token
   * @return the token.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,getKeys,org.apache.hadoop.crypto.key.UserProvider:getKeys(),155,165,"/**
 * Retrieves secret keys, excluding those with ""@"" symbol.
 * @return List of secret keys.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,write,org.apache.hadoop.fs.FsServerDefaults:write(java.io.DataOutput),163,173,"/**
 * Writes block metadata to the DataOutput stream.
 * Writes blockSize, bytesPerChecksum, etc.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,write,org.apache.hadoop.fs.permission.PermissionStatus:write(java.io.DataOutput),103,106,"/**
 * Writes data to the output stream, delegating to a helper method.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,getFixedByteString,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:getFixedByteString(org.apache.hadoop.io.Text),82,89,"/**
 * Gets a fixed ByteString from cache or computes and caches it.
 * @param key The key to look up or compute the ByteString for.
 * @return The ByteString value.
 */
","* Get the ByteString for frequently used fixed and small set strings.
   * @param key Hadoop Writable Text string
   * @return the ByteString for frequently used fixed and small set strings.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,<init>,org.apache.hadoop.security.token.Token:<init>(org.apache.hadoop.security.token.Token),107,112,"/**
 * Creates a new Token by copying values from another Token.
 */
","* Clone a token.
   * @param other the token to clone",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readLine,"org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text,int)",380,382,"/**
 * Reads a line from the input stream.
 * @param str Input stream to read from.
 * @param maxLineLength Maximum line length.
 * @return Number of characters read.
 */
","* Read from the InputStream into the given Text.
   * @param str the object to store the given line
   * @param maxLineLength the maximum number of bytes to store into str.
   * @return the number of bytes read including the newline
   * @throws IOException if the underlying stream throws",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,readLine,org.apache.hadoop.util.LineReader:readLine(org.apache.hadoop.io.Text),390,392,"/**
 * Reads a line from the input stream into the provided Text object.
 */","* Read from the InputStream into the given Text.
   * @param str the object to store the given line
   * @return the number of bytes read including the newline
   * @throws IOException if the underlying stream throws",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.lang.String)",166,188,"/**
 * Creates a delegation token with specified user, renewer, service.
 * @param ugi UserGroupInformation
 * @param renewer Renewer principal name
 * @param service Service name
 * @return Delegation token
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,<init>,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",55,61,"/**
 * Constructs a DelegationTokenIdentifier with owner, renewer, realUser.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryPolicies.java,failoverOnNetworkException,org.apache.hadoop.io.retry.RetryPolicies:failoverOnNetworkException(int),201,203,"/**
 * Creates a RetryPolicy that retries on NetworkException.
 * @param maxFailovers max number of retry attempts
 * @return RetryPolicy instance
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedWriteLock.java,<init>,"org.apache.hadoop.util.InstrumentedWriteLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long)",43,48,"/**
 * Constructs an InstrumentedWriteLock with a default Timer.
 * @param name Lock name, logger, readWriteLock, gaps, threshold.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedLock.java,<init>,"org.apache.hadoop.util.InstrumentedLock:<init>(java.lang.String,org.slf4j.Logger,long,long)",75,79,"/**
 * Constructs an InstrumentedLock with a ReentrantLock.
 * @param name Lock name, logger, gap/threshold durations.
 */
","* Create a instrumented lock instance which logs a warning message
   * when lock held time is above given threshold.
   *
   * @param name the identifier of the lock object
   * @param logger this class does not have its own logger, will log to the
   *               given logger instead
   * @param minLoggingGapMs  the minimum time gap between two log messages,
   *                         this is to avoid spamming to many logs
   * @param lockWarningThresholdMs the time threshold to view lock held
   *                               time as being ""too long""",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadLock.java,<init>,"org.apache.hadoop.util.InstrumentedReadLock:<init>(java.lang.String,org.slf4j.Logger,java.util.concurrent.locks.ReentrantReadWriteLock,long,long)",49,54,"/**
 * Constructs an InstrumentedReadLock with a default Timer.
 * @param name Lock name, logger, readWriteLock, gaps, threshold
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryProxy.java,create,"org.apache.hadoop.io.retry.RetryProxy:create(java.lang.Class,java.lang.Object,org.apache.hadoop.io.retry.RetryPolicy)",40,45,"/**
 * Creates a proxy with retry for the given interface and implementation.
 * @param iface Interface to proxy.
 * @param implementation Implementation of the interface.
 * @param retryPolicy Retry policy to use.
 * @return Proxy object.
 */
","* <p>
   * Create a proxy for an interface of an implementation class
   * using the same retry policy for each method in the interface. 
   * </p>
   * @param iface the interface that the retry will implement
   * @param implementation the instance whose methods should be retried
   * @param retryPolicy the policy for retrying method call failures
   * @param <T> T.
   * @return the retry proxy",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,processWaitTimeAndRetryInfo,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:processWaitTimeAndRetryInfo(),129,149,"/**
 * Waits specified time, retries, then processes retry info.
 * @return CallReturn.RETRY after wait and processing.
 */
","* It first processes the wait time, if there is any,
     * and then invokes {@link #processRetryInfo()}.
     *
     * If the wait time is positive, it either sleeps for synchronous calls
     * or immediately returns for asynchronous calls.
     *
     * @return {@link CallReturn#RETRY} if the retryInfo is processed;
     *         otherwise, return {@link CallReturn#WAIT_RETRY}.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Writer:<init>(org.apache.hadoop.fs.FSDataOutputStream,int,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)",281,292,"/**
 * Constructs a Writer with provided stream, size, compression, comparator, and config.
 */","* Constructor
     * 
     * @param fsdos
     *          output stream for writing. Must be at position 0.
     * @param minBlockSize
     *          Minimum compressed block size in bytes. A compression block will
     *          not be closed until it reaches this size except for the last
     *          block.
     * @param compressName
     *          Name of the compression algorithm. Must be one of the strings
     *          returned by {@link TFile#getSupportedCompressionAlgorithms()}.
     * @param comparator
     *          Leave comparator as null or empty string if TFile is not sorted.
     *          Otherwise, provide the string name for the comparison algorithm
     *          for keys. Two kinds of comparators are supported.
     *          <ul>
     *          <li>Algorithmic comparator: binary comparators that is language
     *          independent. Currently, only ""memcmp"" is supported.
     *          <li>Language-specific comparator: binary comparators that can
     *          only be constructed in specific language. For Java, the syntax
     *          is ""jclass:"", followed by the class name of the RawComparator.
     *          Currently, we only support RawComparators that can be
     *          constructed through the default constructor (with no
     *          parameters). Parameterized RawComparators such as
     *          {@link WritableComparator} or
     *          {@link JavaSerializationComparator} may not be directly used.
     *          One should write a wrapper class that inherits from such classes
     *          and use its default constructor to perform proper
     *          initialization.
     *          </ul>
     * @param conf
     *          The configuration object.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:<init>(java.io.DataInput),772,780,"/**
 * Constructs MetaIndex from DataInput.
 * Reads entry count, then builds index from MetaIndexEntry objects.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,isLastChunk,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:isLastChunk(),80,83,"/**
 * Checks if this is the last chunk of data.
 * @return True if last chunk, false otherwise.
 */
","* Have we reached the last chunk.
     * 
     * @return true if we have reached the last chunk.
     * @throws java.io.IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,getRemain,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:getRemain(),91,94,"/**
 * Returns the remaining bytes in the internal buffer.
 */","* How many bytes remain in the current chunk?
     * 
     * @return remaining bytes left in the current chunk.
     * @throws java.io.IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,read,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(),137,144,"/**
 * Reads the next byte from the input stream.
 * Returns -1 if end of stream is reached.
 * Throws IOException on corrupted stream.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,read,"org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[],int,int)",151,165,"/**
 * Reads up to {@code len} bytes from the input stream.
 * @param b buffer to read into
 * @param off offset in buffer
 * @param len number of bytes to read
 * @return number of bytes read, or -1 if EOF.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,skip,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:skip(long),167,175,"/**
 * Skips specified number of bytes.
 * @param n number of bytes to skip, returns bytes skipped.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(int),303,309,"/**
 * Writes a byte to the output stream.
 * @param b the byte to be written
 * @throws IOException if an I/O error occurs
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,flush,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:flush(),332,336,"/**
 * Flushes the buffer and underlying output stream.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,write,org.apache.hadoop.io.file.tfile.Chunk$ChunkEncoder:write(byte[]),311,314,"/**
* Writes a byte array to the output stream.
* @param b the byte array to be written
* @throws IOException if an I/O error occurs
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[],int,int)",1948,1950,"/**
 * Compares this byte array with another byte array.
 * @param buf Byte array to compare with.
 * @param offset Offset within the byte array.
 * @param length Length of the byte array segment.
 */
","* Compare the entry key to another key. Synonymous to compareTo(new
         * ByteArray(buf, offset, length)
         * 
         * @param buf
         *          The key buffer
         * @param offset
         *          offset into the key buffer.
         * @param length
         *          the length of the key.
         * @return comparison result between the entry key with the input key.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayWritable.java,<init>,org.apache.hadoop.io.ArrayWritable:<init>(java.lang.String[]),62,67,"/**
 * Constructs an ArrayWritable from an array of strings.
 * @param strings array of strings to be stored in the array
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$BlockCompressWriter:close(),1668,1674,"/**
 * Closes the writer, syncing if necessary, then calls super.close().
 */",Close the file.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$BlockCompressWriter:append(java.lang.Object,java.lang.Object)",1677,1707,"/**
 * Appends a key-value pair to the buffer, validating types.
 * @param key The key to append.
 * @param val The value to append.
 * @throws IOException if key or value types are incorrect.
 */
",Append a key/value pair.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,appendRaw,"org.apache.hadoop.io.SequenceFile$BlockCompressWriter:appendRaw(byte[],int,int,org.apache.hadoop.io.SequenceFile$ValueBytes)",1710,1733,"/**
 * Appends key/value data to the buffer.
 * @param keyData Key data as bytes.
 * @param val Value data.
 * @throws IOException If an I/O error occurs.
 */
",Append a key/value pair.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,delegationTokenToJSON,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:delegationTokenToJSON(org.apache.hadoop.security.token.Token),357,365,"/**
 * Converts a delegation token to a JSON map.
 * @param token The delegation token to encode.
 * @return A map containing the token as a JSON string.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,doDelegationTokenOperation,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:doDelegationTokenOperation(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator$DelegationTokenOperation,java.lang.String,org.apache.hadoop.security.token.Token,boolean,java.lang.String)",288,356,"/**
 * Executes a delegation token operation and returns a map.
 * @param url URL to call, token, operation, renewer, dToken, hasResponse, doAsUser
 * @return Map containing the result or null if no response.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,cloneInto,"org.apache.hadoop.io.WritableUtils:cloneInto(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",236,239,"/**
 * Copies data from src Writable to dst Writable using reflection.
 * @param dst Destination Writable object
 * @param src Source Writable object
 */
","* Make a copy of the writable object using serialization to a buffer.
   * @param dst the object to copy from
   * @param src the object to copy into, which is destroyed
   * @throws IOException raised on errors performing I/O.
   * @deprecated use ReflectionUtils.cloneInto instead.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,"org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String,java.util.Collection)",483,495,"/**
 * Chooses a random node within a scope, excluding specified nodes.
 * @param scope Node scope string.
 * @param excludedNodes Nodes to exclude from selection.
 */
","* Randomly choose one node from <i>scope</i>.
   *
   * If scope starts with ~, choose one from the all nodes except for the
   * ones in <i>scope</i>; otherwise, choose one from <i>scope</i>.
   * If excludedNodes is given, choose a node that's not in excludedNodes.
   *
   * @param scope range of nodes from which a node will be chosen
   * @param excludedNodes nodes to be excluded from
   * @return the chosen node",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistance,"org.apache.hadoop.net.NetworkTopology:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",886,892,"/**
 * Sorts nodes by distance from the reader. Calls overloaded method.
 */
","* Sort nodes array by network distance to <i>reader</i>.
   * <p>
   * In a three-level topology, a node can be either local, on the same rack,
   * or on a different rack from the reader. Sorting the nodes based on network
   * distance from the reader reduces network traffic and improves
   * performance.
   * <p>
   * As an additional twist, we also randomize the nodes at each network
   * distance. This helps with load balancing when there is data skew.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,sortByDistanceUsingNetworkLocation,"org.apache.hadoop.net.NetworkTopology:sortByDistanceUsingNetworkLocation(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",929,936,"/**
 * Sorts nodes by distance using network location, calls overloaded method.
 */","* Sort nodes array by network distance to <i>reader</i> with secondary sort.
   * <p> using network location. This is used when the reader
   * is not a datanode. Sorting the nodes based on network distance
   * from the reader reduces network traffic and improves
   * performance.
   * </p>
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,"org.apache.hadoop.net.SocketInputStream:<init>(java.net.Socket,long)",91,94,"/**
 * Creates a SocketInputStream with a timeout.
 * @param socket The Socket to read from.
 * @param timeout Timeout in milliseconds.
 * @throws IOException If an I/O error occurs.
 */
","* Same as SocketInputStream(socket.getChannel(), timeout): <br><br>
   * 
   * Create a new input stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @see SocketInputStream#SocketInputStream(ReadableByteChannel, long)
   *  
   * @param socket should have a channel associated with it.
   * @param timeout timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketInputStream.java,<init>,org.apache.hadoop.net.SocketInputStream:<init>(java.net.Socket),108,110,"/**
* Constructs a SocketInputStream using the given Socket.
* Uses the Socket's channel and timeout for initialization.
*/
","* Same as SocketInputStream(socket.getChannel(), socket.getSoTimeout())
   * :<br><br>
   * 
   * Create a new input stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * @see SocketInputStream#SocketInputStream(ReadableByteChannel, long)
   *  
   * @param socket should have a channel associated with it.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketOutputStream.java,<init>,"org.apache.hadoop.net.SocketOutputStream:<init>(java.net.Socket,long)",96,99,"/**
 * Creates a SocketOutputStream with a specified timeout.
 * @param socket The Socket instance.
 * @param timeout Timeout in milliseconds.
 */
","* Same as SocketOutputStream(socket.getChannel(), timeout):<br><br>
   * 
   * Create a new ouput stream with the given timeout. If the timeout
   * is zero, it will be treated as infinite timeout. The socket's
   * channel will be configured to be non-blocking.
   * 
   * @see SocketOutputStream#SocketOutputStream(WritableByteChannel, long)
   *  
   * @param socket should have a channel associated with it.
   * @param timeout timeout timeout in milliseconds. must not be negative.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,write,org.apache.hadoop.metrics2.sink.StatsDSink$StatsD:write(java.lang.String),198,205,"/**
 * Sends a metric message via the socket.
 * @param msg The metric message to send.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,<init>,org.apache.hadoop.net.NetworkTopology:<init>(),122,125,"/**
 * Initializes the NetworkTopology with a root node.
 * Sets the factory and creates the cluster map.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,getNodeForNetworkLocation,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:getNodeForNetworkLocation(org.apache.hadoop.net.Node),42,55,"/**
 * Retrieves a Node for a given network location, adding default nodegroup if needed.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,add,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:add(org.apache.hadoop.net.Node),176,222,"/**
 * Adds a node to the network topology. Throws exception if inner node.
 * @param node The node to add.
 */
","Add a leaf node
   * Update node counter &amp; rack counter if necessary
   * @param node node to be added; can be null
   * @exception IllegalArgumentException if add a node to a leave 
   *                                     or node to be added is not a leaf",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/InnerNodeImpl.java,add,org.apache.hadoop.net.InnerNodeImpl:add(org.apache.hadoop.net.Node),129,170,"/**
* Adds a node to this node's children, or a subtree if needed.
* @param n the node to add
* @return true if added, false if already present
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,doIO,"org.apache.hadoop.net.SocketIOWithTimeout:doIO(java.nio.ByteBuffer,int)",124,170,"/**
* Performs I/O operations on a ByteBuffer. Returns bytes read/written, or -1 if closed.
*/
","* Performs one IO and returns number of bytes read or written.
   * It waits up to the specified timeout. If the channel is 
   * not read before the timeout, SocketTimeoutException is thrown.
   * 
   * @param buf buffer for IO
   * @param ops Selection Ops used for waiting. Suggested values: 
   *        SelectionKey.OP_READ while reading and SelectionKey.OP_WRITE while
   *        writing. 
   *        
   * @return number of bytes read or written. negative implies end of stream.
   * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,connect,"org.apache.hadoop.net.SocketIOWithTimeout:connect(java.nio.channels.SocketChannel,java.net.SocketAddress,int)",182,228,"/**
 * Connects a SocketChannel to an endpoint with a specified timeout.
 * @param channel SocketChannel to connect.
 * @param endpoint Remote address to connect to.
 * @param timeout Connection timeout in milliseconds.
 */
","* The contract is similar to {@link SocketChannel#connect(SocketAddress)} 
   * with a timeout.
   * 
   * @see SocketChannel#connect(SocketAddress)
   * 
   * @param channel - this should be a {@link SelectableChannel}
   * @param endpoint
   * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocketIOWithTimeout.java,waitForIO,org.apache.hadoop.net.SocketIOWithTimeout:waitForIO(int),242,248,"/**
 * Waits for I/O operations on a channel. Throws IOException on timeout.
 */
","* This is similar to {@link #doIO(ByteBuffer, int)} except that it
   * does not perform any I/O. It just waits for the channel to be ready
   * for I/O as specified in ops.
   * 
   * @param ops Selection Ops used for waiting
   * 
   * @throws SocketTimeoutException 
   *         if select on the channel times out.
   * @throws IOException
   *         if any other I/O error occurs.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultHost,org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String),388,391,"/**
 * Gets the default host for a given interface.
 * @param strInterface Interface name, can be null.
 * @throws UnknownHostException if host cannot be resolved.
 */
","* Returns the default (first) host name associated by the default
   * nameserver with the address bound to the specified network interface
   * 
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0).
   *            Must not be null.
   * @return The default host name associated with IPs bound to the network
   *         interface
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DNS.java,getDefaultHost,"org.apache.hadoop.net.DNS:getDefaultHost(java.lang.String,java.lang.String)",404,408,"/**
 * Gets the default host, using interface and nameserver if provided.
 * @param strInterface Network interface, may be null.
 * @param nameserver Nameserver, may be null.
 * @return Default host string.
 */
","* @return Returns the default (first) host name associated by the provided
   * nameserver with the address bound to the specified network interface.
   *
   * @param strInterface
   *            The name of the network interface to query (e.g. eth0)
   * @param nameserver
   *            The DNS host name
   * @throws UnknownHostException
   *             If one is encountered while querying the default interface",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkParameterValidity,org.apache.hadoop.ha.HAAdmin:checkParameterValidity(java.lang.String[]),387,389,"/**
* Checks if parameters are valid using a predefined usage string.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,help,org.apache.hadoop.ha.HAAdmin:help(java.lang.String[]),508,510,"/**
 * Displays help information. Delegates to the overloaded method.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,getFilter,org.apache.hadoop.metrics2.impl.MetricsConfig:getFilter(java.lang.String),266,280,"/**
 * Retrieves a metrics filter based on the provided prefix.
 * @param prefix filter prefix, used to configure the filter.
 * @return MetricsFilter object or null if no filter exists.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,create,org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String),98,101,"/**
 * Creates a MetricsConfig using the given prefix.
 * @param prefix Prefix for the metrics configuration file.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsConfig.java,create,"org.apache.hadoop.metrics2.impl.MetricsConfig:create(java.lang.String,java.lang.String[])",103,105,"/**
 * Creates a MetricsConfig object by loading from specified files.
 * @param prefix Prefix for metrics names.
 * @param fileNames Files to load configuration from.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,clear,org.apache.hadoop.ipc.RetryCache:clear(org.apache.hadoop.ipc.RetryCache),398,403,"/**
 * Clears the retry cache.
 * @param cache The cache to clear.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getMetrics,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMetrics(org.apache.hadoop.metrics2.impl.MetricsCollectorImpl,boolean)",196,210,"/**
 * Retrieves metrics records from the source, applying filters and tags.
 * @param builder MetricsCollectorImpl to populate with records.
 * @param all if true, retrieves all metrics; otherwise, filtered.
 * @return Iterable of MetricsRecordImpl objects.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,getMBeanName,"org.apache.hadoop.metrics2.util.MBeans:getMBeanName(java.lang.String,java.lang.String,java.util.Map)",151,169,"/**
 * Creates an MBean object name with service and name, plus params.
 * @param serviceName MBean service name.
 * @param nameName MBean name.
 * @param additionalParameters Additional parameters as key-value pairs.
 * @return MBean object name or null on error.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reattachMetrics,org.apache.hadoop.security.UserGroupInformation:reattachMetrics(),259,261,"/**
 * Reattaches Ugi metrics. Delegates to UgiMetrics.reattach().
 */",* Reattach the class's metrics to a new metric system.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stopMetricsMBeans,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopMetricsMBeans(),341,346,"/**
 * Stops the metrics MBeans for each registered source adapter.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,stop,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:stop(),212,214,"/**
 * Stops the service by stopping associated MBeans.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,stop,org.apache.hadoop.ipc.DecayRpcScheduler:stop(),1156,1161,"/**
 * Stops the metrics collection process. Unregisters source and shuts down scheduler.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableInverseQuantiles.java,<init>,"org.apache.hadoop.metrics2.lib.MutableInverseQuantiles:<init>(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",61,64,"/**
 * Constructs a MutableInverseQuantiles object.
 * @param intervalSecs Interval in seconds.
 */","* Instantiates a new {@link MutableInverseQuantiles} for a metric that rolls itself
   * over on the specified time interval.
   *
   * @param name          of the metric
   * @param description   long-form textual description of the metric
   * @param sampleName    type of items in the stream (e.g., ""Ops"")
   * @param valueName     type of the values
   * @param intervalSecs  rollover interval (in seconds) of the estimator",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newQuantiles,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",217,228,"/**
 * Creates and registers a new MutableQuantiles object.
 * @param name Quantiles name, used as key in metricsMap.
 * @param interval Interval for quantiles calculation.
 * @return The newly created MutableQuantiles object.
 */
","* Create a mutable metric that estimates quantiles of a stream of values
   * @param name of the metric
   * @param desc metric description
   * @param sampleName of the metric (e.g., ""Ops"")
   * @param valueName of the metric (e.g., ""Time"" or ""Latency"")
   * @param interval rollover interval of estimator in seconds
   * @return a new quantile estimator object
   * @throws MetricsException if interval is not a positive integer",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,create,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:create(),989,991,"/**
 * Creates and registers a DelegationTokenSecretManagerMetrics instance.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RetryCacheMetrics.java,create,org.apache.hadoop.ipc.metrics.RetryCacheMetrics:create(org.apache.hadoop.ipc.RetryCache),52,55,"/**
 * Creates and registers a RetryCacheMetrics instance.
 * @param cache The RetryCache to track.
 * @return Registered RetryCacheMetrics instance.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newStat,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newStat(java.lang.String,java.lang.String,java.lang.String,java.lang.String)",279,282,"/**
 * Creates a new MutableStat with the provided details.
 * @param name Stat name
 * @param desc Description
 * @param sampleName Sample name
 * @param valueName Value name
 * @return New MutableStat object
 */
","* Create a mutable metric with stats
   * @param name  of the metric
   * @param desc  metric description
   * @param sampleName  of the metric (e.g., ""Ops"")
   * @param valueName   of the metric (e.g., ""Time"" or ""Latency"")
   * @return a new mutable metric object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,addMetricIfNotExists,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:addMetricIfNotExists(java.lang.String),163,172,"/**
 * Adds a metric if it doesn't exist, returns the metric.
 * @param name Metric name.
 * @return The MutableRate object.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String,boolean,boolean)",314,329,"/**
 * Creates a new mutable rate. Returns existing if {@code returnExisting} is true.
 * @param name Rate name.
 * @param desc Rate description.
 * @param extended Whether the rate is extended.
 * @param returnExisting Whether to return existing rate.
 * @return New MutableRate object.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,getMetrics,"org.apache.hadoop.metrics2.source.JvmMetrics:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",143,155,"/**
 * Collects JVM metrics and adds them to the provided collector.
 * @param collector MetricsCollector to add the metrics to
 * @param all If true, collects all metrics.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,initRegistry,org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:initRegistry(java.lang.Object),98,127,"/**
 * Retrieves or creates a MetricsRegistry for the given source object.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,tag,"org.apache.hadoop.metrics2.lib.MetricsRegistry:tag(java.lang.String,java.lang.String,java.lang.String)",391,393,"/**
 * Adds a tag with a boolean flag.
 * @param name tag name
 * @param description tag description
 * @param value tag value
 * @return this
 */
","* Add a tag to the metrics
   * @param name  of the tag
   * @param description of the tag
   * @param value of the tag
   * @return the registry (for keep adding tags)",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,add,"org.apache.hadoop.metrics2.lib.MutableRollingAverages:add(java.lang.String,long)",212,214,"/**
 * Adds a metric with a given name and value.
 * @param name Metric name.
 * @param value Metric value.
 */
","* @param name
   *          name of metric
   * @param value
   *          value of metric",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,addQueueTime,"org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:addQueueTime(int,long)",88,90,"/**
* Adds queue time for a given priority level.
* @param priority Level of the queue (index).
* @param queueTime Time spent in the queue (in ms).
*/
","* Instrument a Call queue time based on its priority.
   *
   * @param priority of the RPC call
   * @param queueTime of the RPC call in the queue of the priority",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,addProcessingTime,"org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:addProcessingTime(int,long)",98,100,"/**
 * Adds processing time for a given priority level.
 * @param priority Processing priority level.
 * @param processingTime Processing time in milliseconds.
 */
","* Instrument a Call processing time based on its priority.
   *
   * @param priority of the RPC call
   * @param processingTime of the RPC call in the queue of the priority",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,addProcessingTime,"org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addProcessingTime(java.lang.String,long)",87,89,"/**
* Adds processing time for an RPC call.
* @param rpcCallName Name of the RPC call.
* @param processingTime Processing time in milliseconds.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,addDeferredProcessingTime,"org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addDeferredProcessingTime(java.lang.String,long)",91,93,"/**
 * Adds deferred processing time for a given name.
 * @param name Identifier for the processing task.
 * @param processingTime Time in milliseconds to defer.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,addOverallProcessingTime,"org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:addOverallProcessingTime(java.lang.String,long)",100,102,"/**
 * Adds overall processing time for a given RPC call.
 * @param rpcCallName Name of the RPC call.
 * @param overallProcessingTime Processing time in milliseconds.
 */
","* Add an overall RPC processing time sample.
   * @param rpcCallName of the RPC call
   * @param overallProcessingTime  the overall RPC processing time",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableStat.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableStat:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",138,163,"/**
 * Records metrics to the builder, optionally including all data.
 * @param builder MetricsRecordBuilder to populate with data.
 * @param all If true, includes all metrics, otherwise only changed ones.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,newForMethod,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForMethod(java.lang.Object,java.lang.reflect.Method,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)",94,105,"/**
 * Creates and registers a MutableMetric for the given method.
 * @param source Object source, method, annotation, registry
 * @return The created MutableMetric.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getMetrics,"org.apache.hadoop.ipc.DecayRpcScheduler:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",1009,1027,"/**
 * Collects and adds metrics to the provided collector.
 * @param collector MetricsCollector to add metrics to
 * @param all if true, collect all metrics.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroups,org.apache.hadoop.security.UserGroupInformation$TestingGroups:getGroups(java.lang.String),1580,1583,"/**
 * Retrieves a list of groups for the given user.
 * @param user The username to retrieve groups for.
 * @return A list of group names.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateKrb5File,org.apache.hadoop.security.KDiag:validateKrb5File(),561,588,"/**
 * Locates and validates the Kerberos configuration file.
 * Uses sysprops/env vars to find the file path.
 */
","* Locate the {@code krb5.conf} file and dump it.
   *
   * No-op on windows.
   * @throws IOException problems reading the file.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,verify,"org.apache.hadoop.security.KDiag:verify(boolean,java.lang.String,java.lang.String,java.lang.Object[])",963,981,"/**
 * Checks a condition; fails/errors if false, otherwise returns true.
 * @param condition boolean to verify; @param category error category
 */
","* Assert that a condition must hold.
   *
   * If not, an exception is raised, or, if {@link #nofail} is set,
   * an error will be logged and the method return false.
   *
   * @param condition condition which must hold
   * @param category category for exception
   * @param message string formatting message
   * @param args any arguments for the formatting
   * @return true if the verification succeeded, false if it failed but
   * an exception was not raised.
   * @throws KerberosDiagsFailure containing the formatted text
   *         if the condition was met",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,failif,"org.apache.hadoop.security.KDiag:failif(boolean,java.lang.String,java.lang.String,java.lang.Object[])",1034,1042,"/**
 * Fails if the condition is true, logs an error with category & message.
 */","* Conditional failure with string formatted arguments.
   * There is no chek for the {@link #nofail} value.
   * @param condition failure condition
   * @param category category for exception
   * @param message string formatting message
   * @param args any arguments for the formatting
   * @throws KerberosDiagsFailure containing the formatted text
   *         if the condition was met",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createRemoteUser,org.apache.hadoop.security.UserGroupInformation:createRemoteUser(java.lang.String),1438,1442,"/**
 * Creates a UserGroupInformation for a remote user.
 * @param user The username of the remote user.
 */
","* Create a user from a login name. It is intended to be used for remote
   * users in RPC, since it won't have any credentials.
   * @param user the full user principal name, must not be empty or null
   * @return the UserGroupInformation for the remote user.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuthorizedUgi,org.apache.hadoop.ipc.Server$Connection:getAuthorizedUgi(java.lang.String),2161,2176,"/**
 * Retrieves UserGroupInformation based on the authorized ID.
 * @param authorizedId Identifier for the authorized user.
 * @return UserGroupInformation object.
 * @throws InvalidToken, AccessControlException
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,verifyToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:verifyToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,byte[])",575,582,"/**
 * Verifies a token against a stored password.
 * @param identifier Token identifier.
 * @param password Password to verify.
 * @throws InvalidToken if password doesn't match.
 */
","* Verifies that the given identifier and password are valid and match.
   * @param identifier Token identifier.
   * @param password Password in the token.
   * @throws InvalidToken InvalidToken.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,init,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:init(),143,152,"/**
 * Initializes the secret manager threads if enabled.
 * Throws exception if thread start fails.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,startThreads,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:startThreads(),243,348,"/**
 * Starts threads and initializes Curator Framework and counters.
 * Handles external clients and creates ZK nodes/caches.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,run,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$ExpiredTokenRemover:run(),829,859,"/**
 * Removes expired delegation tokens and rolls the master key periodically.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslInputStream.java,read,org.apache.hadoop.security.SaslInputStream:read(java.nio.ByteBuffer),367,384,"/**
* Reads bytes from the input stream into the destination buffer.
* @param dst Destination buffer to read into.
* @return Number of bytes read, or -1 if an I/O error occurs.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,spawnAutoRenewalThreadForKeytab,org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForKeytab(),905,918,"/**
 * Spawns a thread to renew Keytab credentials if needed.
 * Checks if relogin is needed or from ticket, returns if so.
 */","* Spawn a thread to do periodic renewals of kerberos credentials from a
   * keytab file.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,<init>,org.apache.hadoop.fs.shell.Ls:<init>(org.apache.hadoop.conf.Configuration),122,124,"/**
 * Constructs a new Ls object with the given configuration.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,<init>,"org.apache.hadoop.fs.shell.Count:<init>(java.lang.String[],int,org.apache.hadoop.conf.Configuration)",118,122,"/**
 * Constructs a Count object.
 * @param cmd Command line arguments.
 * @param pos Starting position in the arguments.
 * @param conf Configuration object.
 */
","Constructor
   * @deprecated invoke via {@link FsShell}
   * @param cmd the count command
   * @param pos the starting index of the arguments
   * @param conf configuration",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,<init>,org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:<init>(org.apache.hadoop.fs.FileSystem),495,497,"/**
 * Constructs a TargetFileSystem with the given FileSystem.
 * @param fs The FileSystem to associate with this TargetFileSystem.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,org.apache.hadoop.fs.ChecksumFileSystem:<init>(org.apache.hadoop.fs.FileSystem),79,81,"/**
 * Initializes a ChecksumFileSystem with the given FileSystem.
 * @param fs The FileSystem to associate with this ChecksumFileSystem.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,newShellInstance,org.apache.hadoop.fs.FsShell:newShellInstance(),398,400,"/**
 * Creates and returns a new FsShell instance.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,<init>,org.apache.hadoop.fs.shell.FsCommand:<init>(),76,76,"/**
 * Default constructor, protected to prevent external instantiation.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,39,"/**
 * Constructs a RSErasureEncoder with the given options.
 * @param options ErasureCoderOptions configuration.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),44,46,"/**
 * Constructs an HHXORErasureEncoder with provided options.
 * @param options ErasureCoderOptions object for configuration.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.DummyErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Constructs a DummyErasureEncoder with provided options.
 * @param options ErasureCoderOptions object for configuration.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureEncoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,38,"/**
 * Constructs a XORErasureEncoder with the given options.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/DummyErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.DummyErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),32,34,"/**
 * Constructs a DummyErasureDecoder with provided options.
 * @param options ErasureCoderOptions object for configuration.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),36,38,"/**
 * Constructs a XORErasureDecoder with specified options.
 * @param options ErasureCoderOptions object for configuration.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),37,39,"/**
 * Constructs an RSErasureDecoder with provided options.
 * @param options ErasureCoderOptions configuring the decoder.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,<init>,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:<init>(org.apache.hadoop.io.erasurecode.ErasureCoderOptions),45,47,"/**
 * Initializes the HHXORErasureDecoder with provided options.
 * @param options ErasureCoderOptions configuring the decoder.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/WritableSerialization.java,getDeserializer,org.apache.hadoop.io.serializer.WritableSerialization:getDeserializer(java.lang.Class),120,124,"/**
 * Returns a Deserializer for the given Writable class.
 * @param c The Writable class to deserialize.
 * @return A Deserializer instance.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMaps,org.apache.hadoop.security.ShellBasedIdMapping:updateMaps(),343,357,"/**
 * Updates internal maps based on platform support and init state.
 * Loads full maps initially, then updates incrementally.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMapIncr,"org.apache.hadoop.security.ShellBasedIdMapping:updateMapIncr(java.lang.String,boolean)",465,504,"/**
 * Updates the user/group mapping based on platform.
 * @param name The name to update.
 * @param isGrp True for group, false for user.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,updateMapIncr,"org.apache.hadoop.security.ShellBasedIdMapping:updateMapIncr(int,boolean)",506,540,"/**
 * Updates the internal mapping for a given ID (user/group).
 * @param id The ID to update.
 * @param isGrp True for group, false for user.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,connectToZooKeeper,org.apache.hadoop.ha.ActiveStandbyElector:connectToZooKeeper(),723,743,"/**
 * Connects to ZooKeeper, adds auth info, and waits for connection.
 * @return ZooKeeper instance
 * @throws IOException, KeeperException on connection failure
 */
","* Get a new zookeeper client instance. protected so that test class can
   * inherit and mock out the zookeeper instance
   * 
   * @return new zookeeper client instance
   * @throws IOException raised on errors performing I/O.
   * @throws KeeperException zookeeper connectionloss exception",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromKeytab,"org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(boolean,boolean)",1272,1289,"/**
 * Relogins using keytab, optionally checking TGT and ignoring last login time.
 * @param checkTGT Whether to check the Ticket Granting Ticket.
 * @param ignoreLastLoginTime Ignores the last login time.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromTicketCache,org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache(boolean),1322,1332,"/**
 * Relogins using cached ticket if needed, ignoring last login time.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",29,32,"/**
 * Creates a CryptoFSDataOutputStream.
 * @param out Output stream.
 * @param codec Crypto codec.
 * @param bufferSize Buffer size.
 * @param key Encryption key.
 * @param iv Initialization vector.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",86,89,"/**
 * Creates a CryptoOutputStream with default update mode.
 * @param out Output stream.
 * @param codec CryptoCodec.
 * @param bufferSize Buffer size.
 * @param key Encryption key.
 * @param iv Initialization vector.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:createEncryptor(),72,76,"/**
 * Creates and returns an OpensslCtrCipher instance for encryption.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:createDecryptor(),78,82,"/**
 * Creates a Decryptor instance using OpensslCtrCipher.
 * @return Decryptor object for decryption.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,<init>,"org.apache.hadoop.crypto.OpensslCtrCryptoCodec$OpensslCtrCipher:<init>(int,org.apache.hadoop.crypto.CipherSuite)",130,134,"/**
 * Initializes an OpensslCtrCipher with the given mode and CipherSuite.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,parseJSONEncKeyVersions,"org.apache.hadoop.util.KMSUtil:parseJSONEncKeyVersions(java.lang.String,java.util.List)",143,155,"/**
 * Parses a list of JSON key versions.
 * @param keyName Key name.
 * @param valueList List of JSON key version maps.
 * @return List of EncryptedKeyVersion objects.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getEncKeyQueueSize,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getEncKeyQueueSize(java.lang.String),966,969,"/**
 * Gets the size of the encryption key version queue for a key.
 * @param keyName Name of the encryption key.
 * @return Queue size.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/ValueQueue.java,getNext,org.apache.hadoop.crypto.key.kms.ValueQueue:getNext(java.lang.String),292,295,"/**
 * Retrieves the next element associated with the given key.
 * @param keyName The key to retrieve the next element for.
 * @return The next element or null if none exists.
 */
","* This removes the value currently at the head of the Queue for the
   * provided key. Will immediately fire the Queue filler function if key
   * does not exist.
   * If Queue exists but all values are drained, It will ask the generator
   * function to add 1 value to Queue and then drain it.
   * @param keyName String key name
   * @return E the next value in the Queue
   * @throws IOException raised on errors performing I/O.
   * @throws ExecutionException executionException.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,drain,org.apache.hadoop.crypto.key.kms.KMSClientProvider:drain(java.lang.String),961,964,"/**
* Drains the encKeyVersionQueue for the given key name.
* @param keyName The name of the key to drain.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,internalQueueCall,org.apache.hadoop.ipc.Server:internalQueueCall(org.apache.hadoop.ipc.Server$Call),3106,3109,"/**
 * Queues a call internally.
 * @param call The call object to queue.
 * @throws IOException, InterruptedException if an error occurs.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getByName,org.apache.hadoop.security.SecurityUtil:getByName(java.lang.String),569,588,"/**
 * Resolves a hostname to an InetAddress. Logs slow lookups.
 * @param hostname The hostname to resolve.
 * @return An InetAddress object.
 * @throws UnknownHostException if resolution fails.
 */
","* Resolves a host subject to the security requirements determined by
   * hadoop.security.token.service.use_ip. Optionally logs slow resolutions.
   * 
   * @param hostname host or ip to resolve
   * @return a resolved host
   * @throws UnknownHostException if the host doesn't exist",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,run,org.apache.hadoop.util.JvmPauseMonitor$Monitor:run(),181,208,"/**
 * Monitors JVM pauses, logs warnings/info if pause time exceeds thresholds.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolSignature.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolSignature:getProtocolSignature(org.apache.hadoop.ipc.VersionedProtocol,java.lang.String,long,int)",241,254,"/**
 * Gets a ProtocolSignature based on protocol details.
 * @param server Protocol implementation, protocol name, client version, hash.
 * @return ProtocolSignature object.
 */
","* Get a server protocol's signature
   *
   * @param server server implementation
   * @param protocol server protocol
   * @param clientVersion client's version
   * @param clientMethodsHash client's protocol's hash code
   * @return the server protocol's signature
   * @throws IOException if any error occurs",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolServerSideTranslatorPB.java,getProtocolSignature,"org.apache.hadoop.ha.protocolPB.ZKFCProtocolServerSideTranslatorPB:getProtocolSignature(java.lang.String,long,int)",74,86,"/**
 * Gets the protocol signature for the given client version.
 * @param protocol Protocol name
 * @param clientVersion Client version
 * @param clientMethodsHash Client methods hash
 * @throws IOException if protocol is unknown
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolServerSideTranslatorPB.java,getProtocolSignature,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolServerSideTranslatorPB:getProtocolSignature(java.lang.String,long,int)",186,198,"/**
 * Gets the protocol signature for the given client version.
 * @param protocol Protocol name.
 * @param clientVersion Client version.
 * @param clientMethodsHash Hash of client methods.
 * @throws IOException if the protocol is unknown.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolMetaInfoServerSideTranslatorPB.java,getProtocolSignature,"org.apache.hadoop.ipc.ProtocolMetaInfoServerSideTranslatorPB:getProtocolSignature(org.apache.hadoop.thirdparty.protobuf.RpcController,org.apache.hadoop.ipc.protobuf.ProtocolInfoProtos$GetProtocolSignatureRequestProto)",70,104,"/**
 * Gets protocol signature for a given protocol and RPC kind.
 * @param controller RPC controller
 * @param request Request containing protocol and RPC kind
 * @return Protocol signature response or empty response.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ExternalCall.java,<init>,org.apache.hadoop.ipc.ExternalCall:<init>(java.security.PrivilegedExceptionAction),36,38,"/**
 * Constructs an ExternalCall with the given privileged action.
 * @param action The action to perform in a privileged context.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Listener:run(),1551,1600,"/**
 * Runs the server's main loop, handling connections and events.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,add,org.apache.hadoop.ipc.FairCallQueue:add(java.lang.Object),194,213,"/**
 * Adds an element to the queue. Throws exception if queue overflows.
 * @param e element to add
 * @return true if added successfully
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,put,org.apache.hadoop.ipc.FairCallQueue:put(java.lang.Object),215,222,"/**
 * Adds an element to the queues, prioritizing based on level.
 * @param e element to add
 * @throws InterruptedException if interrupted while waiting
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtocolProxy.java,isMethodSupported,"org.apache.hadoop.ipc.ProtocolProxy:isMethodSupported(java.lang.String,java.lang.Class[])",95,117,"/**
 * Checks if a method is supported by the server.
 * @param methodName Method name to check.
 * @param parameterTypes Method parameter types.
 * @return True if method is supported, false otherwise.
 */
","* Check if a method is supported by the server or not.
   * 
   * @param methodName a method's name in String format
   * @param parameterTypes a method's parameter types
   * @return true if the method is supported by the server
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,checkRpcHeaders,org.apache.hadoop.ipc.Server$Connection:checkRpcHeaders(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto),2830,2852,"/**
 * Validates RPC request headers, throwing exceptions on failure.
 */","* Verify RPC header is valid
     * @param header - RPC request header
     * @throws RpcServerException - header contains invalid values",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ResponseBuffer.java,<init>,org.apache.hadoop.ipc.ResponseBuffer:<init>(),33,35,"/**
 * Constructs a ResponseBuffer with a default capacity of 1024.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,forceDecay,org.apache.hadoop.ipc.DecayRpcScheduler:forceDecay(),822,825,"/**
 * Forces the decay of current costs.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,getPriorityLevel,org.apache.hadoop.ipc.CallQueueManager:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation),265,270,"/**
 * Gets the priority level for a user.
 * @param user User for whom to determine priority.
 * @return Priority level, 0 if scheduler is not DecayRpcScheduler.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getReturnMessage,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)",301,323,"/**
 * Retrieves the return message from the buffer.
 * @param method The method being invoked.
 * @param buf The buffer containing the message.
 * @return The return message.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcWritable.java,newInstance,"org.apache.hadoop.ipc.RpcWritable$Buffer:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",175,188,"/**
 * Creates a new instance of the given class and configures it.
 * @param valueClass Class to instantiate.
 * @param conf Configuration object to set.
 * @return New instance of the class.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getMessage,"org.apache.hadoop.ipc.Server$Connection:getMessage(org.apache.hadoop.thirdparty.protobuf.Message,org.apache.hadoop.ipc.RpcWritable$Buffer)",3046,3057,"/**
 * Retrieves a message from the buffer, casting to type T.
 * @param message The message to decode.
 * @param buffer The buffer containing the message data.
 * @return The decoded message of type T.
 */
","* Decode the a protobuf from the given input stream 
     * @return Message - decoded protobuf
     * @throws RpcServerException - deserialization failed",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getReturnMessage,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:getReturnMessage(java.lang.reflect.Method,org.apache.hadoop.ipc.RpcWritable$Buffer)",311,333,"/**
 * Retrieves the return message from the buffer.
 * @param method The method to get the return message for.
 * @param buf The buffer containing the message.
 * @return The return message.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponse,"org.apache.hadoop.ipc.Server:setupResponse(org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto,org.apache.hadoop.io.Writable)",3549,3562,"/**
 * Sets the response for the RPC call.
 * @param call The RPC call object.
 * @param header Response header.
 * @param rv Writable response data.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,take,org.apache.hadoop.ipc.FairCallQueue:take(),292,296,"/**
 * Removes and returns the next element from the queue.
 * Decrements the semaphore count.
 * @return The next element or null if the queue is empty.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,poll,"org.apache.hadoop.ipc.FairCallQueue:poll(long,java.util.concurrent.TimeUnit)",298,301,"/**
 * Attempts to retrieve and remove an element within the given timeout.
 * @param timeout Timeout duration.
 * @param unit Time unit of the timeout.
 * @return The removed element or null if timeout expires.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,poll,org.apache.hadoop.ipc.FairCallQueue:poll(),307,310,"/**
 * Retrieves and removes the head of the queue, if available.
 * Returns null if no element is available.
 */
","* poll() provides no strict consistency: it is possible for poll to return
   * null even though an element is in the queue.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Responder:run(),1711,1725,"/**
 * Starts the server's main event loop and closes resources.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doSaslReply,org.apache.hadoop.ipc.Server$Connection:doSaslReply(org.apache.hadoop.thirdparty.protobuf.Message),2420,2426,"/**
 * Sends a SASL reply message.
 * @param message The SASL reply message to send.
 * @throws IOException if sending fails.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doSaslReply,org.apache.hadoop.ipc.Server$Connection:doSaslReply(java.lang.Exception),2428,2433,"/**
 * Sends an authentication failure response with error details.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupBadVersionResponse,org.apache.hadoop.ipc.Server$Connection:setupBadVersionResponse(int),2636,2665,"/**
 * Handles version mismatch response based on client version.
 * Sends appropriate error response to the client.
 */","* Try to set up the response to indicate that the client version
     * is incompatible with the server. This can contain special-case
     * code to speak enough of past IPC protocols to pass back
     * an exception to the caller.
     * @param clientVersion the version the caller is using 
     * @throws IOException",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupHttpRequestOnIpcPortResponse,org.apache.hadoop.ipc.Server$Connection:setupHttpRequestOnIpcPortResponse(),2667,2672,"/**
 * Simulates an HTTP request response on the IPC port.
 * Creates a fake RPC call and sends a pre-defined response.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CombinedIPWhiteList.java,<init>,"org.apache.hadoop.util.CombinedIPWhiteList:<init>(java.lang.String,java.lang.String,long)",31,43,"/**
 * Creates a CombinedIPWhiteList with fixed/variable IP lists.
 * @param fixedWhiteListFile Fixed IP list file.
 * @param variableWhiteListFile Optional variable IP list file.
 * @param cacheExpiryInSeconds Cache expiry time in seconds.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CombinedIPList.java,<init>,"org.apache.hadoop.util.CombinedIPList:<init>(java.lang.String,java.lang.String,long)",33,44,"/**
 * Creates a CombinedIPList with fixed and variable IP lists.
 * @param fixedBlackListFile Fixed IP list file.
 * @param variableBlackListFile Variable IP list file (optional).
 * @param cacheExpiryInSeconds Cache expiry time in seconds.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FileBasedIPList.java,reload,org.apache.hadoop.util.FileBasedIPList:reload(),67,69,"/**
 * Reloads the IP list from the file.
 * @return A new FileBasedIPList object.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getVirtualMemorySize,org.apache.hadoop.util.SysInfoLinux:getVirtualMemorySize(),603,606,"/**
 * Returns the virtual memory size, sum of physical memory and swap.
 */",{@inheritDoc},,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$FSDataBoundedInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.InputStream)",495,499,"/**
 * Constructs a bounded input stream for reading from a file.
 * @param fs FileSystem object
 * @param file Path to the file
 * @param in Input stream
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,open,"org.apache.hadoop.fs.http.AbstractHttpFileSystem:open(org.apache.hadoop.fs.Path,int)",61,67,"/**
 * Opens an input stream for a path.
 * @param path Path to open.
 * @param bufferSize Buffer size for input stream.
 * @return FSDataInputStream for reading data from the path.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,<init>,"org.apache.hadoop.fs.HarFileSystem$HarFSDataInputStream:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,long,long,int)",1109,1112,"/**
 * Creates a HarFSDataInputStream, wrapping a HarFSInputStream.
 * @param fs Filesystem, path, start, length, bufsize for stream.
 */
","* constructors for har input stream.
     * @param fs the underlying filesystem
     * @param p The path in the underlying filesystem
     * @param start the start position in the part file
     * @param length the length of valid data in the part file
     * @param bufsize the buffer size
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,read,"org.apache.hadoop.fs.FSDataInputStream:read(org.apache.hadoop.io.ByteBufferPool,int)",217,220,"/**
* Reads data into a buffer from the input.
* @param bufferPool Buffer pool for allocating buffers.
* @param maxLength Maximum number of bytes to read.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,evictExpiredEntries,org.apache.hadoop.util.LightWeightCache:evictExpiredEntries(),164,175,"/**
 * Evicts expired entries from the queue up to EVICTION_LIMIT.
 */",Evict expired entries.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,evictEntries,org.apache.hadoop.util.LightWeightCache:evictEntries(),178,184,"/**
 * Evicts entries until the cache size is within the limit.
 */",Evict entries in order to enforce the size limit of the cache.,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean)",408,410,"/**
 * Overloads toString, passing default value for the 'i' parameter.
 */","Return the string representation of the object in the output format.
   * For description of the options,
   * @see #toString(boolean, boolean, boolean, boolean, List)
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output is to be used
   * @param xOption a flag indicating if calculation from snapshots is to be
   *                included in the output
   * @return the string representation of the object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean,boolean,java.util.List)",423,426,"/**
 * Generates a string representation with specified options.
 * @param qOption, hOption, tOption, types - Options for string format.
 */
","* Return the string representation of the object in the output format.
   * For description of the options,
   * @see #toString(boolean, boolean, boolean, boolean, List)
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output if to be used
   * @param tOption a flag indicating if display quota by storage types
   * @param types Storage types to display
   * @return the string representation of the object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,toString,"org.apache.hadoop.fs.QuotaUsage:toString(boolean,boolean,java.util.List)",321,327,"/**
 * Returns quota usage string. Uses getTypesQuotaUsage if tOption is true.
 * @param hOption boolean option
 * @param tOption boolean option
 * @param types List of storage types
 * @return Quota usage string
 */
","* Return the string representation of the object in the output format.
   * if hOption is false file sizes are returned in bytes
   * if hOption is true file sizes are returned in human readable
   *
   * @param hOption a flag indicating if human readable output if to be used
   * @param tOption type option.
   * @param types storage types.
   * @return the string representation of the object.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,read,org.apache.hadoop.fs.FSInputChecker:read(),150,159,"/**
 * Reads a byte from the input stream. Returns -1 if EOF.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,read1,"org.apache.hadoop.fs.FSInputChecker:read1(byte[],int,int)",251,275,"/**
 * Reads up to `len` bytes from the internal buffer to `b`.
 * @param b buffer to read to
 * @param off offset in buffer to start writing
 * @param len maximum number of bytes to read
 * @return number of bytes read
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,readFields,org.apache.hadoop.io.ObjectWritable$NullInstance:readFields(java.io.DataInput),114,125,"/**
 * Reads class name from input and resolves it to a Class object.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayPrimitiveWritable.java,readFields,org.apache.hadoop.io.ArrayPrimitiveWritable:readFields(java.io.DataInput),205,251,"/**
 * Reads array fields from input.
 * Sets component type and length, then populates the array.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,waitForCompletion,org.apache.hadoop.util.functional.CommonCallableSupplier:waitForCompletion(java.util.List),98,106,"/**
 * Waits for all CompletableFutures in the list to complete.
 * @param futures List of CompletableFutures to wait for.
 */
","* Wait for a list of futures to complete. If the list is empty,
   * return immediately.
   *
   * @param futures list of futures.
   * @param <T> Generics Type T.
   * @throws IOException      if one of the called futures raised an IOE.
   * @throws RuntimeException if one of the futures raised one.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/CommonCallableSupplier.java,maybeAwaitCompletion,org.apache.hadoop.util.functional.CommonCallableSupplier:maybeAwaitCompletion(java.util.concurrent.CompletableFuture),152,157,"/**
 * Awaits completion of a CompletableFuture, if it exists.
 * @param future CompletableFuture to await, or null.
 */
","* Block awaiting completion for any non-null future passed in;
   * No-op if a null arg was supplied.
   * @param future future
   * @throws IOException      if one of the called futures raised an IOE.
   * @throws RuntimeException if one of the futures raised one.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,update,"org.apache.hadoop.util.CrcComposer:update(byte[],int,int,long)",123,138,"/**
 * Updates CRC value using data from buffer.
 * @param crcBuffer Byte array containing CRC data.
 * @param offset Start offset in the buffer.
 * @param length Number of bytes to process.
 * @param bytesPerCrc Bytes per CRC update.
 */
","* Composes length / CRC_SIZE_IN_BYTES more CRCs from crcBuffer, with
   * each CRC expected to correspond to exactly {@code bytesPerCrc} underlying
   * data bytes.
   *
   * @param crcBuffer crcBuffer.
   * @param offset offset.
   * @param length must be a multiple of the expected byte-size of a CRC.
   * @param bytesPerCrc bytesPerCrc.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CrcComposer.java,update,"org.apache.hadoop.util.CrcComposer:update(java.io.DataInputStream,long,long)",150,157,"/**
 * Updates the CRC32 state with checksums from the input stream.
 * @param checksumIn Input stream of checksums.
 * @param numChecksumsToRead Number of checksums to read.
 * @param bytesPerCrc Bytes per CRC value.
 */
","* Composes {@code numChecksumsToRead} additional CRCs into the current digest
   * out of {@code checksumIn}, with each CRC expected to correspond to exactly
   * {@code bytesPerCrc} underlying data bytes.
   *
   * @param checksumIn checksumIn.
   * @param numChecksumsToRead numChecksumsToRead.
   * @param bytesPerCrc bytesPerCrc.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,hasNext,org.apache.hadoop.util.functional.RemoteIterators$HaltableRemoteIterator:hasNext(),780,783,"/**
 * Checks if there is another element available from the source.
 * @throws IOException if an I/O error occurs
 * @return True if another element exists, false otherwise.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,newInstance,"org.apache.hadoop.util.ReflectionUtils:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",124,127,"/**
 * Creates a new instance of the specified class using the config.
 * @param theClass Class to instantiate.
 * @param conf Configuration object.
 * @return New instance of the class.
 */
","Create an object for the given class and initialize it from conf
   * 
   * @param theClass class of which an object is created
   * @param conf Configuration
   * @param <T> Generics Type T.
   * @return a new object",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getKeyClass,org.apache.hadoop.io.MapFile$Reader:getKeyClass(),465,465,"/**
 * Returns the key class of the underlying data structure.
 */","* Returns the class of keys in this file.
     *
     * @return keyClass.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getValueClass,org.apache.hadoop.io.MapFile$Reader:getValueClass(),472,472,"/**
 * Returns the value class of the underlying data.
 */","* Returns the class of values in this file.
     *
     * @return Value Class.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,run,org.apache.hadoop.util.FindClass:run(java.lang.String[]),310,335,"/**
 * Executes a command based on provided arguments.
 * @param args Command-line arguments: action and name.
 * @return Result code indicating success or failure.
 */
","* Run the class/resource find or load operation
   * @param args command specific arguments.
   * @return the outcome
   * @throws Exception if something went very wrong",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",78,84,"/**
 * Constructs a FutureDataInputStreamBuilderImpl.
 * @param fc FileContext; @param path Path to build from; throws IOException.
 */
","* Construct from a {@link FileContext}.
   *
   * @param fc FileContext
   * @param path path.
   * @throws IOException failure",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newDataChecksum,"org.apache.hadoop.util.DataChecksum:newDataChecksum(byte[],int)",160,181,"/**
 * Creates a new DataChecksum from byte array at given offset.
 * @param bytes Byte array containing checksum data.
 * @param offset Starting offset in the byte array.
 * @return New DataChecksum object.
 * @throws IOException if checksum size is invalid.
 */
","* Creates a DataChecksum from HEADER_LEN bytes from arr[offset].
   *
   * @param bytes bytes.
   * @param offset offset.
   * @return DataChecksum of the type in the array or null in case of an error.
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DataChecksum.java,newDataChecksum,org.apache.hadoop.util.DataChecksum:newDataChecksum(java.io.DataInputStream),192,202,"/**
 * Creates a new DataChecksum object based on input stream data.
 * @param in Input stream containing checksum type and size.
 * @return DataChecksum object or throws InvalidChecksumSizeException.
 */
","* This constructs a DataChecksum by reading HEADER_LEN bytes from input
   * stream <i>in</i>.
   *
   * @param in data input stream.
   * @throws IOException raised on errors performing I/O.
   * @return DataChecksum by reading HEADER_LEN
   *         bytes from input stream.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,run,org.apache.hadoop.util.Shell:run(),951,960,"/**
 * Executes the command if enough time has passed.
 * Resets exitCode and sets launch mechanism on macOS.
 */
","* Check to see if a command needs to be executed and execute if needed.
   *
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:<init>(java.lang.String),148,210,"/**
 * Constructs a DynamicWrappedIO instance, loading methods from classname.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:<init>(java.lang.String),228,346,"/**
 * Initializes DynamicWrappedStatistics by loading methods from classname.
 * @param classname Fully qualified name of the class to wrap.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_aggregate,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_aggregate(java.io.Serializable,java.lang.Object)",502,507,"/**
 * Aggregates statistics for a snapshot using reflection.
 * @param snapshot The snapshot object.
 * @param statistics Statistics to aggregate.
 * @throws UnsupportedOperationException if aggregation fails.
 */
","* Aggregate an existing {@code IOStatisticsSnapshot} with
   * the supplied statistics.
   * @param snapshot snapshot to update
   * @param statistics IOStatistics to add
   * @return true if the snapshot was updated.
   * @throws IllegalArgumentException if the {@code statistics} argument is not
   * null but not an instance of IOStatistics, or if  {@code snapshot} is invalid.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_create(),514,518,"/**
 * Creates an I/O statistics snapshot.
 * @throws UnsupportedOperationException if I/O statistics are unavailable
 */
","* Create a new {@code IOStatisticsSnapshot} instance.
   * @return an empty IOStatisticsSnapshot.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_create(java.lang.Object),527,532,"/**
 * Creates an I/O statistics snapshot from a source object.
 * @param source Object used to create the snapshot.
 * @return IostatisticsSnapshot object.
 */
","* Create a new {@code IOStatisticsSnapshot} instance.
   * @param source optional source statistics
   * @return an IOStatisticsSnapshot.
   * @throws ClassCastException if the {@code source} is not valid.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_toJsonString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_toJsonString(java.io.Serializable),541,545,"/**
 * Converts an IO statistics snapshot to a JSON string.
 * @param snapshot The IO statistics snapshot.
 * @return JSON string representation of the snapshot.
 */
","* Save IOStatisticsSnapshot to a JSON string.
   * @param snapshot statistics; may be null or of an incompatible type
   * @return JSON string value or null if source is not an IOStatisticsSnapshot
   * @throws UncheckedIOException Any IO/jackson exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_fromJsonString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_fromJsonString(java.lang.String),554,558,"/**
 * Parses a JSON string into an IostatisticsSnapshot object.
 * @param json JSON string to parse.
 * @return IostatisticsSnapshot object.
 */
","* Load IOStatisticsSnapshot from a JSON string.
   * @param json JSON string value.
   * @return deserialized snapshot.
   * @throws UncheckedIOException Any IO/jackson exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_load,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",568,573,"/**
 * Loads an I/O statistics snapshot from the given file system path.
 * @param fs FileSystem to load from.
 * @param path Path to the snapshot file.
 * @return IostatisticsSnapshot object.
 */
","* Load IOStatisticsSnapshot from a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @return the loaded snapshot
   * @throws UncheckedIOException Any IO exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_retrieve,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_retrieve(java.lang.Object),581,585,"/**
 * Retrieves an I/O statistics snapshot using reflection.
 * @param source Source object for snapshot retrieval.
 * @return IostatisticsSnapshot object.
 */
","* Extract the IOStatistics from an object in a serializable form.
   * @param source source object, may be null/not a statistics source/instance
   * @return {@code IOStatisticsSnapshot} or null if the object is null/doesn't have statistics
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsSnapshot_save,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsSnapshot_save(java.io.Serializable,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",596,604,"/**
 * Saves an IO statistics snapshot to the specified path.
 * @param snapshot Snapshot data, FS, path, overwrite flag.
 */
","* Save IOStatisticsSnapshot to a Hadoop filesystem as a JSON file.
   * @param snapshot statistics
   * @param fs filesystem
   * @param path path
   * @param overwrite should any existing file be overwritten?
   * @throws UncheckedIOException Any IO exception.
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatistics_toPrettyString,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatistics_toPrettyString(java.lang.Object),666,669,"/**
 * Converts statistics object to a human-readable string.
 * @param statistics The statistics object to convert.
 */
","* Convert IOStatistics to a string form, with all the metrics sorted
   * and empty value stripped.
   * @param statistics A statistics instance.
   * @return string value or the empty string if null
   * @throws UnsupportedOperationException if the IOStatistics classes were not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_getCurrent,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_getCurrent(),439,443,"/**
 * Gets the current IO statistics context.
 * @throws UnsupportedOperationException if unavailable
 */
","* Get the context's {@code IOStatisticsContext} which
   * implements {@code IOStatisticsSource}.
   * This is either a thread-local value or a global empty context.
   * @return instance of {@code IOStatisticsContext}.
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_setThreadIOStatisticsContext,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_setThreadIOStatisticsContext(java.lang.Object),451,455,"/**
 * Sets the thread IO statistics context.
 * @param statisticsContext The context object to set.
 * @throws UnsupportedOperationException if not supported.
 */
","* Set the IOStatisticsContext for the current thread.
   * @param statisticsContext IOStatistics context instance for the
   * current thread. If null, the context is reset.
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_reset,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_reset(),462,466,"/**
 * Resets the IO statistics context. Throws UnsupportedOperationException if unavailable.
 */","* Reset the context's IOStatistics.
   * {@code IOStatisticsContext#reset()}
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_snapshot,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_snapshot(),474,478,"/**
 * Retrieves an I/O statistics context snapshot.
 * @throws UnsupportedOperationException if unavailable.
 */
","* Take a snapshot of the context IOStatistics.
   * {@code IOStatisticsContext#snapshot()}
   * @return an instance of {@code IOStatisticsSnapshot}.
   * @throws UnsupportedOperationException if the IOStatisticsContext API was not found",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,iostatisticsContext_aggregate,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:iostatisticsContext_aggregate(java.lang.Object),487,490,"/**
 * Aggregates statistics context using reflection.
 * @param source The object to aggregate statistics from.
 * @return True if aggregation was successful, false otherwise.
 */
","* Aggregate into the IOStatistics context the statistics passed in via
   * IOStatistics/source parameter.
   * <p>
   * Returns false if the source is null or does not contain any statistics.
   * @param source implementation of {@link IOStatisticsSource} or {@link IOStatistics}
   * @return true if the the source object was aggregated.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/VersionUtil.java,compareVersions,"org.apache.hadoop.util.VersionUtil:compareVersions(java.lang.String,java.lang.String)",39,43,"/**
 * Compares two version strings.
 * @param version1 First version string.
 * @param version2 Second version string.
 * @return -1, 0, or 1 based on version comparison.
 */
","* Compares two version name strings using maven's ComparableVersion class.
   *
   * @param version1
   *          the first version to compare
   * @param version2
   *          the second version to compare
   * @return a negative integer if version1 precedes version2, a positive
   *         integer if version2 precedes version1, and 0 if and only if the two
   *         versions are equal.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refreshInternal,"org.apache.hadoop.util.HostsFileReader:refreshInternal(java.lang.String,java.lang.String,boolean)",200,225,"/**
 * Refreshes host details from include/exclude files (lazy option).
 * @param includesFile Path to includes file.
 * @param excludesFile Path to excludes file.
 * @param lazy If true, performs a lazy refresh.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,<init>,"org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.io.InputStream,java.lang.String,java.io.InputStream)",67,75,"/**
 * Initializes the HostsFileReader with files and input streams.
 * @param includesFile Includes file path.
 * @param inFileInputStream FileInputStream for includes file.
 * @param excludesFile Excludes file path.
 * @param exFileInputStream FileInputStream for excludes file.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,<init>,"org.apache.hadoop.util.bloom.CountingBloomFilter:<init>(int,int,int)",93,96,"/**
 * Initializes a CountingBloomFilter with specified size, hashes, and type.
 * @param vectorSize Size of the filter.
 * @param nbHash Number of hash functions.
 * @param hashType Type of hash function to use.
 */
","* Constructor
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,<init>,"org.apache.hadoop.util.bloom.BloomFilter:<init>(int,int,int)",110,114,"/**
 * Initializes a Bloom filter with specified size, hash count, and type.
 */","* Constructor
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/CountingBloomFilter.java,readFields,org.apache.hadoop.util.bloom.CountingBloomFilter:readFields(java.io.DataInput),301,309,"/**
 * Reads bucket data from input stream.
 * Reads 'vectorSize' number of longs into the 'buckets' array.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/BloomFilter.java,readFields,org.apache.hadoop.util.bloom.BloomFilter:readFields(java.io.DataInput),218,233,"/**
* Reads bit vector data from input stream.
* Populates the internal bits representation.
*/
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,probablyHasKey,org.apache.hadoop.io.BloomMapFile$Reader:probablyHasKey(org.apache.hadoop.io.WritableComparable),264,272,"/**
 * Checks if key probably exists in the Bloom filter.
 * @param key The key to check.
 * @return True if probably exists, false otherwise.
 */
","* Checks if this MapFile has the indicated key. The membership test is
     * performed using a Bloom filter, so the result has always non-zero
     * probability of false positives.
     * @param key key to check
     * @return  false iff key doesn't exist, true if key probably exists.
     * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,selectiveClearing,"org.apache.hadoop.util.bloom.RetouchedBloomFilter:selectiveClearing(org.apache.hadoop.util.bloom.Key,short)",199,235,"/**
 * Selectively clears a bit based on the provided scheme.
 * @param k The key used for membership testing.
 * @param scheme The clearing scheme to use.
 */
","* Performs the selective clearing for a given key.
   * @param k The false positive key to remove from <i>this</i> retouched Bloom filter.
   * @param scheme The selective clearing scheme to apply.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,createOptionTableListing,org.apache.hadoop.fs.FsShell:createOptionTableListing(),292,295,"/**
 * Creates a TableListing object with predefined fields and width.
 */",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadResources,"org.apache.hadoop.conf.Configuration:loadResources(java.util.Properties,java.util.ArrayList,int,boolean,boolean)",3082,3100,"/**
 * Loads resources into the provided list, optionally reloads defaults.
 * @param properties Properties object to load from.
 * @param resources List to load resources into.
 * @param startIdx Starting index for loading resources.
 */
",,,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String,java.lang.String)",616,619,"/**
 * Adds a deprecation entry.
 * @param key Original key.
 * @param newKey Replacement key.
 * @param customMessage Custom deprecation message.
 */
","* Adds the deprecated key to the global deprecation map.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   *
   * @param key to be deprecated
   * @param newKey key that take up the values of deprecated key
   * @param customMessage deprecation message",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String[])",640,643,"/**
 * Adds a deprecation, forwarding to the other addDeprecation method.
 */","* Adds the deprecated key to the global deprecation map when no custom
   * message is provided.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If a key is deprecated in favor of multiple keys, they are all treated as 
   * aliases of each other, and setting any one of them resets all the others 
   * to the new value.
   * 
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   *
   * @param key Key that is to be deprecated
   * @param newKeys list of keys that take up the values of deprecated key
   * @deprecated use {@link #addDeprecation(String key, String newKey)} instead",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addDeprecation,"org.apache.hadoop.conf.Configuration:addDeprecation(java.lang.String,java.lang.String)",659,661,"/**
* Adds a deprecation entry, specifying a replacement key.
* @param key The key to deprecate.
* @param newKey The replacement key.
*/
","* Adds the deprecated key to the global deprecation map when no custom
   * message is provided.
   * It does not override any existing entries in the deprecation map.
   * This is to be used only by the developers in order to add deprecation of
   * keys, and attempts to call this method after loading resources once,
   * would lead to <tt>UnsupportedOperationException</tt>
   * 
   * If you have multiple deprecation entries to add, it is more efficient to
   * use #addDeprecations(DeprecationDelta[] deltas) instead.
   *
   * @param key Key that is to be deprecated
   * @param newKey key that takes up the value of deprecated key",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,start,org.apache.hadoop.http.HttpServer2:start(),1382,1434,"/**
 * Starts the HTTP server, registers metrics, and initializes context.
 * Throws IOException if startup fails.
 */","* Start the server. Does not wait for the server to start.
   *
   * @throws IOException raised on errors performing I/O.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,writeBreadCrumbNode,org.apache.hadoop.ha.ActiveStandbyElector:writeBreadCrumbNode(org.apache.zookeeper.data.Stat),963,977,"/**
 * Writes or updates a znode to indicate the local node is active.
 * @param oldBreadcrumbStat Previous breadcrumb stat, used for update.
 * @throws KeeperException, InterruptedException on ZooKeeper errors.
 */
","* Write the ""ActiveBreadCrumb"" node, indicating that this node may need
   * to be fenced on failover.
   * @param oldBreadcrumbStat",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,tryDeleteOwnBreadCrumbNode,org.apache.hadoop.ha.ActiveStandbyElector:tryDeleteOwnBreadCrumbNode(),985,1008,"/**
 * Deletes the breadcrumb node for the active node, with retries.
 */","* Try to delete the ""ActiveBreadCrumb"" node when gracefully giving up
   * active status.
   * If this fails, it will simply warn, since the graceful release behavior
   * is only an optimization.",,,True,5
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/VectoredReadUtils.java,readVectored,"org.apache.hadoop.fs.VectoredReadUtils:readVectored(org.apache.hadoop.fs.PositionedReadable,java.util.List,java.util.function.IntFunction)",98,104,"/**
 * Reads data from a stream into file ranges using provided allocator.
 * @param stream Stream to read from
 * @param ranges Ranges to populate
 * @param allocate Allocates ByteBuffer for each range
 */
","* This is the default implementation which iterates through the ranges
   * to read each synchronously, but the intent is that subclasses
   * can make more efficient readers.
   * The data or exceptions are pushed into {@link FileRange#getData()}.
   * @param stream the stream to read the data from
   * @param ranges the byte ranges to read
   * @param allocate the byte buffer allocation
   * @throws IllegalArgumentException if there are overlapping ranges or a range is invalid
   * @throws EOFException the range offset is negative",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,cancelPrefetches,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:cancelPrefetches(),294,306,"/**
 * Cancels prefetching operations and caches prefetched buffers.
 */",* Requests cancellation of any previously issued prefetch requests.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BlockManager.java,get,org.apache.hadoop.fs.impl.prefetch.BlockManager:get(int),77,86,"/**
 * Retrieves a buffer data object for the given block number.
 * @param blockNumber Block number to retrieve.
 * @return BufferData object containing block data.
 */
","* Gets the block having the given {@code blockNumber}.
   *
   * The entire block is read into memory and returned as a {@code BufferData}.
   * The blocks are treated as a limited resource and must be released when
   * one is done reading them.
   *
   * @param blockNumber the number of the block to be read and returned.
   * @return {@code BufferData} having data from the given block.
   *
   * @throws IOException if there an error reading the given block.
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,readBlock,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:readBlock(org.apache.hadoop.fs.impl.prefetch.BufferData,boolean,org.apache.hadoop.fs.impl.prefetch.BufferData$State[])",331,395,"/**
 * Reads a block of data into the buffer, potentially from cache.
 * @param data BufferData object
 * @param isPrefetch Whether to perform a prefetch operation
 * @param expectedState Expected state of the BufferData
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,<init>,"org.apache.hadoop.fs.impl.prefetch.FilePosition:<init>(long,int)",83,95,"/**
 * Initializes a FilePosition with file size and block size.
 * @param fileSize Size of the file in bytes.
 * @param blockSize Size of each block.
 */
","* Constructs an instance of {@link FilePosition}.
   *
   * @param fileSize size of the associated file.
   * @param blockSize size of each block within the file.
   *
   * @throws IllegalArgumentException if fileSize is negative.
   * @throws IllegalArgumentException if blockSize is zero or negative.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,isLastBlock,org.apache.hadoop.fs.impl.prefetch.FilePosition:isLastBlock(),204,206,"/**
 * Checks if the current block is the last block.
 * Uses blockData and blockNumber to determine this.
 */
","* Determines whether the current block is the last block in this file.
   *
   * @return true if the current block is the last block in this file, false otherwise.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/FilePosition.java,toString,org.apache.hadoop.fs.impl.prefetch.FilePosition:toString(),275,296,"/**
 * Generates a string representation of the object's state.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,closeAll,org.apache.hadoop.fs.FileSystem:closeAll(),642,645,"/**
 * Closes all resources managed by the cache, logging the action.
 */
","* Close all cached FileSystem instances. After this operation, they
   * may not be used in any operations.
   *
   * @throws IOException a problem arose closing one or more filesystem.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,readFully,"org.apache.hadoop.fs.BufferedFSInputStream:readFully(long,byte[])",125,128,"/**
 * Reads data from the stream fully into the provided buffer.
 * @param position Starting position in the stream.
 * @param buffer Buffer to read data into.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,write,"org.apache.hadoop.fs.FileUtil:write(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.CharSequence)",2044,2047,"/**
 * Writes a CharSequence to a Path in a FileSystem.
 * @param fs FileSystem to write to
 * @param path Path to write to
 * @param charseq CharSequence to write
 * @return The updated FileSystem.
 * @throws IOException if an I/O error occurs
 */
","* Write a line of text to a file. Characters are encoded into bytes using
   * UTF-8. This utility method opens the file for writing, creating the file if
   * it does not exist, or overwrites an existing file.
   *
   * @param fs the files system with which to create the file
   * @param path the path to the file
   * @param charseq the char sequence to write to the file
   *
   * @return the file system
   *
   * @throws NullPointerException if any of the arguments are {@code null}
   * @throws IOException if an I/O error occurs creating or writing to the file",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],long,long,boolean)",122,125,"/**
 * Constructs a BlockLocation with null block details.
 * @param names Block names, hosts, offset, length, corrupt flag.
 */
","* Constructor with host, name, offset, length and corrupt flag.
   * @param names names.
   * @param hosts hosts.
   * @param offset offset.
   * @param length length.
   * @param corrupt corrupt.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],java.lang.String[],long,long)",135,138,"/**
 * Constructs a BlockLocation with default flag.
 * @param names Block names, hosts, topology paths, offset, length.
 */
","* Constructor with host, name, network topology, offset and length.
   * @param names names.
   * @param hosts hosts.
   * @param topologyPaths topologyPaths.
   * @param offset offset.
   * @param length length.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/MeanStatistic.java,clone,org.apache.hadoop.fs.statistics.MeanStatistic:clone(),271,274,"/**
 * Creates and returns a deep copy of this MeanStatistic object.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/DynamicIOStatistics.java,<init>,org.apache.hadoop.fs.statistics.impl.DynamicIOStatistics:<init>(),58,59,"/**
 * Initializes DynamicIOStatistics with default values.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,aggregateMeanStatistics,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:aggregateMeanStatistics(org.apache.hadoop.fs.statistics.MeanStatistic,org.apache.hadoop.fs.statistics.MeanStatistic)",312,317,"/**
 * Combines two MeanStatistic objects.
 * @param l Left MeanStatistic to aggregate.
 * @param r Right MeanStatistic to aggregate.
 * @return Combined MeanStatistic.
 */
","* Aggregate the mean statistics.
   * This returns a new instance.
   * @param l left value
   * @param r right value
   * @return aggregate value",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,snapshot,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:snapshot(org.apache.hadoop.fs.statistics.IOStatistics),160,168,"/**
 * Creates a snapshot of IO statistics.
 * @param source The source IOStatistics object to snapshot.
 */
","* Take a snapshot.
   *
   * This completely overwrites the map data with the statistics
   * from the source.
   * @param source statistics source.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,logIOStatisticsAtDebug,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtDebug(java.lang.String,java.lang.Object)",250,254,"/**
 * Logs IO statistics at debug level, using provided message and source.
 */
","* Extract any statistics from the source and log to
   * this class's log at debug, if
   * the log is set to log at debug.
   * No-op if logging is not at debug or the source is null/of
   * the wrong type/doesn't provide statistics.
   * @param message message for log -this must contain ""{}"" for the
   * statistics report to actually get logged.
   * @param source source object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsLogging.java,logIOStatisticsAtLevel,"org.apache.hadoop.fs.statistics.IOStatisticsLogging:logIOStatisticsAtLevel(org.slf4j.Logger,java.lang.String,java.lang.Object)",263,281,"/**
 * Logs IO statistics at the specified level.
 * @param log Logger instance.
 * @param level Logging level (INFO, ERROR, WARN).
 * @param source Source object for statistics.
 */
","* A method to log IOStatistics from a source at different levels.
   *
   * @param log    Logger for logging.
   * @param level  LOG level.
   * @param source Source to LOG.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,cleanupRemoteIterator,org.apache.hadoop.util.functional.RemoteIterators:cleanupRemoteIterator(org.apache.hadoop.fs.RemoteIterator),297,304,"/**
 * Closes a RemoteIterator, logging statistics if it's Closeable.
 */
","* Clean up after an iteration.
   * If the log is at debug, calculate and log the IOStatistics.
   * If the iterator is closeable, cast and then cleanup the iterator
   * @param source iterator source
   * @param <T> type of source",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsBinding.java,trackDurationOfInvocation,"org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding:trackDurationOfInvocation(org.apache.hadoop.fs.statistics.DurationTrackerFactory,java.lang.String,org.apache.hadoop.util.functional.InvocationRaisingIOE)",460,466,"/**
 * Tracks invocation duration using a provided factory.
 * @param factory DurationTrackerFactory instance
 * @param statistic Statistic name
 * @param input InvocationRaisingIOE to track
 */
","* Given an IOException raising callable/lambda expression,
   * execute it and update the relevant statistic.
   * @param factory factory of duration trackers
   * @param statistic statistic key
   * @param input input callable.
   * @throws IOException IO failure.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsStoreBuilderImpl.java,build,org.apache.hadoop.fs.statistics.impl.IOStatisticsStoreBuilderImpl:build(),107,111,"/**
 * Builds and returns an IOStatisticsStore instance.
 * Uses internal statistics counters and gauges.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,org.apache.hadoop.crypto.CryptoInputStream:read(),779,782,"/**
 * Reads a single byte from the input stream.
 * Returns -1 if EOF, otherwise returns the byte value.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(long,byte[],int,int)",332,348,"/**
 * Reads data from the input stream at a specific position.
 * @param position byte offset in the stream
 * @param buffer buffer to store read data
 * @param offset offset within the buffer
 * @param length number of bytes to read
 * @return number of bytes read
 */
",Positioned read. It is thread-safe,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFully,"org.apache.hadoop.crypto.CryptoInputStream:readFully(long,byte[],int,int)",502,515,"/**
 * Reads bytes from the input stream at a specific position.
 * @param position Start position
 * @param buffer Byte array to fill
 * @param offset Offset in buffer
 * @param length Number of bytes to read
 * @throws IOException If an I/O error occurs
 */
",Positioned read fully. It is thread-safe,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(long,java.nio.ByteBuffer)",353,369,"/**
 * Reads data from the input stream into the provided buffer.
 * @param position byte offset in the input stream
 * @param buf buffer to read into
 * @return number of bytes read
 */
",* Positioned read using {@link ByteBuffer}s. This method is thread-safe.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFully,"org.apache.hadoop.crypto.CryptoInputStream:readFully(long,java.nio.ByteBuffer)",374,389,"/**
 * Reads data from the stream into the provided ByteBuffer.
 * @param position Starting position for reading.
 * @param buf ByteBuffer to read into.
 */
",* Positioned readFully using {@link ByteBuffer}s. This method is thread-safe.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,org.apache.hadoop.crypto.CryptoInputStream:read(java.nio.ByteBuffer),588,639,"/**
 * Reads data from the stream into the provided ByteBuffer.
 * @param buf ByteBuffer to read data into.
 * @return Number of bytes read, or -1 if EOF.
 */
",ByteBuffer read.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,read,"org.apache.hadoop.crypto.CryptoInputStream:read(org.apache.hadoop.io.ByteBufferPool,int,java.util.EnumSet)",707,736,"/**
 * Reads data into a buffer from the input stream.
 * @param bufferPool Buffer pool for allocating buffers
 * @param maxLength Max buffer size
 * @param opts Read options
 * @return ByteBuffer containing read data
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,<init>,"org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",118,122,"/**
 * Creates a CryptoInputStream with specified input, codec, buffer, key, and IV.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,doEncode,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:doEncode(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][])",101,118,"/**
 * Encodes input sub-packets using Reed-Solomon and adds piggybacks.
 * @param inputs Input sub-packet data.
 * @param outputs Encoded sub-packet data.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureEncodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.ErasureEncodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",50,54,"/**
 * Encodes input chunks to output chunks using the raw encoder.
 * @param inputChunks Input chunks to be encoded.
 * @param outputChunks Output chunks to store encoded data.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,writeObject,"org.apache.hadoop.io.ObjectWritable:writeObject(java.io.DataOutput,java.lang.Object,java.lang.Class,org.apache.hadoop.conf.Configuration,boolean)",165,234,"/**
* Writes an object to a DataOutput stream, handling various types.
* @param out Output stream, instance object to write, class type, config, compact arrays flag
* @throws IOException if writing fails
*/
","* Write a {@link Writable}, {@link String}, primitive type, or an array of
     * the preceding.  
     * 
     * @param allowCompactArrays - set true for RPC and internal or intra-cluster
     * usages.  Set false for inter-cluster, File, and other persisted output 
     * usages, to preserve the ability to interchange files with other clusters 
     * that may not be running the same version of software.  Sometime in ~2013 
     * we can consider removing this parameter and always using the compact format.
     *
     * @param conf configuration.
     * @param out dataoutput.
     * @param declaredClass declaredClass.
     * @param instance instance.
     * @throws IOException raised on errors performing I/O.
     *",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,tryAcquire,org.apache.hadoop.fs.impl.prefetch.BufferPool:tryAcquire(int),157,159,"/**
 * Attempts to acquire a buffer data block.
 * @param blockNumber The block number to acquire.
 * @return BufferData object or null if acquisition fails.
 */
","* Acquires a buffer if one is immediately available. Otherwise returns null.
   * @param blockNumber the id of the block to try acquire.
   * @return the acquired block's {@code BufferData} or null.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,numAvailable,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:numAvailable(),600,602,"/**
 * Returns the number of available objects in the buffer pool.
 */
","* Number of ByteBuffers available to be acquired.
   *
   * @return the number of available buffers.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/TaskPool.java,run,org.apache.hadoop.util.functional.TaskPool$Builder:run(org.apache.hadoop.util.functional.TaskPool$Task),268,282,"/**
 * Executes a task, either single-threaded or in parallel.
 * @param task The task to execute.
 * @return True if successful, throws E or IOException on failure.
 */
","* Execute the task across the data.
     * @param task task to execute
     * @param <E> exception which may be raised in execution.
     * @return true if the operation executed successfully
     * @throws E any exception raised.
     * @throws IOException IOExceptions raised by remote iterator or in execution.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPaths,"org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData[])",342,351,"/**
 * Processes a list of path data items, handling potential IO errors.
 * @param parent parent PathData
 * @param items PathData items to process
 * @throws IOException if an I/O error occurs during processing
 */
","*  Iterates over the given expanded paths and invokes
   *  {@link #processPath(PathData)} on each element.  If ""recursive"" is true,
   *  will do a post-visit DFS on directories.
   *  @param parent if called via a recurse, will be the parent dir, else null
   *  @param items a list of {@link PathData} objects to process
   *  @throws IOException if anything goes wrong...",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,getPathHandle,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:getPathHandle(org.apache.hadoop.fs.Path),163,166,"/**
 * Retrieves a PathHandle for the given file path.
 * @param filePath The path to retrieve the handle for.
 * @return PathHandle object.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Options.java,resolve,"org.apache.hadoop.fs.Options$HandleOpt:resolve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Options$HandleOpt[])",359,362,"/**
 * Returns a Function to resolve FileStatus to a PathHandle.
 * @param fs FileSystem to use for resolution
 * @param opt Handle options to apply
 */
","* Utility function for mapping {@link FileSystem#getPathHandle} to a
     * fixed set of handle options.
     * @param fs Target filesystem
     * @param opt Options to bind in partially evaluated function
     * @return Function reference with options fixed",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,createPathHandle,"org.apache.hadoop.fs.FilterFileSystem:createPathHandle(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Options$HandleOpt[])",177,180,"/**
* Creates a PathHandle from a FileStatus and HandleOpts.
* @param stat FileStatus object
* @param opts Handle options
* @return PathHandle object
*/
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,createGroupExecutor,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:createGroupExecutor(java.lang.String),132,135,"/**
 * Creates a ShellCommandExecutor for retrieving user groups.
 * @param userName The username for which to fetch groups.
 */
","* Create a ShellCommandExecutor object using the user's name.
   *
   * @param userName user's name
   * @return a ShellCommandExecutor object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,createGroupIDExecutor,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:createGroupIDExecutor(java.lang.String),153,156,"/**
 * Creates a ShellCommandExecutor for fetching group IDs for a user.
 * @param userName The username to fetch group IDs for.
 */
","* Create a ShellCommandExecutor object for fetch a user's group id list.
   *
   * @param userName the user's name
   * @return a ShellCommandExecutor object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File,java.util.Map)",1222,1225,"/**
 * Constructs a ShellCommandExecutor with environment variables.
 * @param execString Command to execute.
 * @param dir Working directory.
 * @param env Environment variables.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execCommand,"org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[],long)",1373,1379,"/**
 * Executes a command with given environment and timeout.
 * @param env Environment variables.
 * @param cmd Command to execute.
 * @param timeout Timeout in milliseconds.
 * @return Command output as a string.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readProto,org.apache.hadoop.security.Credentials:readProto(java.io.DataInput),403,413,"/**
 * Reads credentials from a delimited protobuf stream.
 * @param in DataInput stream containing credentials proto.
 */
","* Populates keys/values from proto buffer storage.
   * @param in - stream ready to read a serialized proto buffer message",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,addAll,"org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials,boolean)",463,476,"/**
 * Adds all secrets and tokens from another Credentials object.
 * @param other The Credentials object to copy from.
 * @param overwrite If true, overwrites existing entries.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DelegationTokenIssuer.java,collectDelegationTokens,"org.apache.hadoop.security.token.DelegationTokenIssuer:collectDelegationTokens(org.apache.hadoop.security.token.DelegationTokenIssuer,java.lang.String,org.apache.hadoop.security.Credentials,java.util.List)",98,138,"/**
 * Collects delegation tokens from issuer and its children.
 * @param issuer Token issuer
 * @param renewer Token renewer
 * @param credentials Credentials to add tokens to
 * @param tokens List to add tokens to
 */
","* NEVER call this method directly.
   *
   * @param issuer issuer.
   * @param renewer renewer.
   * @param credentials cache in which to add new delegation tokens.
   * @param tokens list of new delegation tokens.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addToken,"org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.io.Text,org.apache.hadoop.security.token.Token)",1712,1717,"/**
 * Adds a token associated with an alias.
 * @param alias Token alias
 * @param token Token to add
 * @return True upon successful addition
 */
","* Add a named token to this UGI
   * 
   * @param alias Name of the token
   * @param token Token to be added
   * @return true on successful add of new token",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,addRegexMountEntry,org.apache.hadoop.fs.viewfs.InodeTree:addRegexMountEntry(org.apache.hadoop.fs.viewfs.InodeTree$LinkEntry),807,816,"/**
 * Adds a regex mount entry to the list, logging details.
 * @param le LinkEntry containing regex mount configuration.
 * @throws IOException if initialization fails.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,makeAbsolute,"org.apache.hadoop.fs.sftp.SFTPFileSystem:makeAbsolute(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",176,181,"/**
 * Resolves a path to be absolute, using workDir if needed.
 * @param workDir Base directory for relative paths.
 * @param path Path to resolve.
 * @return Absolute Path object.
 */
","* Resolve against given working directory.
   *
   * @param workDir
   * @param path
   * @return absolute path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,makeAbsolute,"org.apache.hadoop.fs.ftp.FTPFileSystem:makeAbsolute(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",269,274,"/**
 * Resolves a path to be absolute, using workDir if needed.
 * @param workDir Base directory for relative paths.
 * @param path The path to resolve.
 * @return Absolute path.
 */
","* Resolve against given working directory. *
   * 
   * @param workDir
   * @param path
   * @return",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,makeAbsolute,org.apache.hadoop.fs.RawLocalFileSystem:makeAbsolute(org.apache.hadoop.fs.Path),105,111,"/**
 * Resolves a Path to an absolute path, using workingDir if needed.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,pathToFile,org.apache.hadoop.fs.RawLocalFileSystem:pathToFile(org.apache.hadoop.fs.Path),119,125,"/**
 * Converts a Path object to a File object.
 * @param path The Path to convert.
 * @return A File object representing the path.
 */
","* Convert a path to a File.
   *
   * @param path the path.
   * @return file.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,fixRelativePart,org.apache.hadoop.fs.FileSystem:fixRelativePart(org.apache.hadoop.fs.Path),2884,2890,"/**
 * Resolves a path, returning it if absolute, otherwise relative to working dir.
 */","* See {@link FileContext#fixRelativePart}.
   * @param p the path.
   * @return relative part.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,makeAbsolute,org.apache.hadoop.fs.viewfs.ViewFileSystem:makeAbsolute(org.apache.hadoop.fs.Path),269,271,"/**
 * Returns an absolute path. If not absolute, resolves against working dir.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),188,191,"/**
 * Sets the working directory.
 * Uses absolute path if provided, otherwise combines with current.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,makeQualified,"org.apache.hadoop.fs.Path:makeQualified(java.net.URI,org.apache.hadoop.fs.Path)",562,598,"/**
 * Makes a path absolute, using default URI if necessary.
 * @param defaultUri Default URI to use if path is relative.
 * @param workingDir Working directory to resolve relative paths.
 * @return Qualified Path object.
 */
","* Returns a qualified path object.
   *
   * @param defaultUri if this path is missing the scheme or authority
   * components, borrow them from this URI
   * @param workingDir if this path isn't absolute, treat it as relative to this
   * working directory
   * @return this path if it contains a scheme and authority and is absolute, or
   * a new path that includes a path and authority and is fully qualified",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,fixRelativePart,org.apache.hadoop.fs.FileContext:fixRelativePart(org.apache.hadoop.fs.Path),284,291,"/**
 * Resolves a path, returning it if absolute, otherwise relative to working dir.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.HarFileSystem:getWorkingDirectory(),273,276,"/**
 * Returns the working directory as a Path object.
 * Uses the URI to create the Path.
 */
",* return the top level archive.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.HarFileSystem:getHomeDirectory(),809,812,"/**
 * Returns the home directory as a Path object.
 * Uses the URI's string representation for the path.
 */
",* return the top level archive path.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getHomeDirectory(com.jcraft.jsch.ChannelSftp),680,686,"/**
 * Gets the current working directory from the SFTP channel.
 * @param channel SFTP channel to get the directory from
 * @return Path object representing the directory, or null on error.
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,org.apache.hadoop.fs.ChecksumFs:<init>(org.apache.hadoop.fs.AbstractFileSystem),58,63,"/**
 * Constructs a ChecksumFS with the given AbstractFileSystem.
 * Initializes default bytes per checksum from server defaults.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.RawLocalFileSystem:getHomeDirectory(),842,845,"/**
 * Returns the home directory as a qualified Path object.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.RawLocalFileSystem:getInitialWorkingDirectory(),861,864,"/**
 * Returns the initial working directory, qualified for the job.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,abort,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:abort(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path)",245,261,"/**
 * Aborts an upload by deleting the associated file.
 * @param uploadId Upload handle to abort.
 * @param filePath Path to the file being uploaded.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,lookupStat,"org.apache.hadoop.fs.shell.PathData:lookupStat(org.apache.hadoop.fs.FileSystem,java.lang.String,boolean)",175,186,"/**
 * Retrieves FileStatus for a given path.
 * @param fs FileSystem object
 * @param pathString path to check
 * @param ignoreFNF ignore FileNotFoundException if true
 * @return FileStatus object or null if not found
 */
","* Get the FileStatus info
   * @param ignoreFNF if true, stat will be null if the path doesn't exist
   * @return FileStatus for the given path
   * @throws IOException if anything goes wrong",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,getPath,org.apache.hadoop.fs.PathIOException:getPath(),107,107,"/**
 * Returns a new Path object based on the internal path.
 */",@return Path that generated the exception,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PathIOException.java,getTargetPath,org.apache.hadoop.fs.PathIOException:getTargetPath(),110,112,"/**
 * Returns a copy of the target path, or null if it's null.
 */
","@return Path if the operation involved copying or moving, else null",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getUsed,org.apache.hadoop.fs.FileSystem:getUsed(),2719,2722,"/**
 * Gets the used space of the root directory.
 * @return Used space in bytes.
 * @throws IOException if an I/O error occurs.
 */
","* Return the total size of all files in the filesystem.
   * @throws IOException IO failure
   * @return the number of path used.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.viewfs.ViewFileSystem:getHomeDirectory(),422,434,"/**
 * Gets the user's home directory path.
 * Returns a Path object representing the home directory.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getMountPoints,org.apache.hadoop.fs.viewfs.ViewFileSystem:getMountPoints(),1060,1070,"/**
 * Returns an array of MountPoint objects from the file system state.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>(org.apache.hadoop.fs.FileSystem,java.net.URI)",105,116,"/**
 * Creates a ChRootedFileSystem with the given filesystem and URI.
 * @param fs the filesystem
 * @param uri the URI representing the chroot location
 * @throws IOException if an I/O error occurs
 */
","* Constructor
   * @param fs base file system
   * @param uri base uri
   * @throws IOException",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getResolvedQualifiedPath,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getResolvedQualifiedPath(org.apache.hadoop.fs.Path),177,181,"/**
 * Resolves a qualified path by prepending the root path.
 * @param f Path to resolve; returns a resolved Path.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getHomeDirectory,org.apache.hadoop.fs.viewfs.ViewFs:getHomeDirectory(),314,326,"/**
 * Gets the user's home directory path.
 * Returns Path object representing the home directory.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getMountPoints,org.apache.hadoop.fs.viewfs.ViewFs:getMountPoints(),720,730,"/**
 * Returns an array of MountPoint objects.
 * Converts InodeTree.MountPoint to MountPoint array.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,getRemainingPathStr,"org.apache.hadoop.fs.viewfs.RegexMountPoint:getRemainingPathStr(java.lang.String,java.lang.String)",207,215,"/**
 * Calculates the remaining path string after resolving a path.
 * @param srcPath The original path string.
 * @param resolvedPathStr The resolved path string.
 * @return Path object representing the remaining path.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,createLink,"org.apache.hadoop.fs.viewfs.InodeTree:createLink(java.lang.String,java.lang.String,org.apache.hadoop.fs.viewfs.InodeTree$LinkType,java.lang.String,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration)",423,505,"/**
* Creates a link in the file system.
* @param src Source path of the link.
* @param target Target URI or string.
* @param linkType Type of the link.
*/",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getRemainingPath,"org.apache.hadoop.fs.viewfs.InodeTree:getRemainingPath(java.lang.String[],int)",996,1008,"/**
 * Returns the remaining path from the given path array.
 * @param path Array of path segments.
 * @param startIndex Starting index for the remaining path.
 * @return Path object representing the remaining path.
 */
","* Return remaining path from specified index to the end of the path array.
   * @param path An array of path components split by slash
   * @param startIndex the specified start index of the path array
   * @return remaining path.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,getTargetLink,org.apache.hadoop.fs.viewfs.InodeTree$INodeLink:getTargetLink(),370,377,"/**
 * Constructs a Path representing the target link(s), comma-separated if merged.
 * @return Path object representing the target link(s)
 */
","* Get the target of the link. If a merge link then it returned
     * as "","" separated URI list.
     *
     * @return the path.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(java.lang.String,java.lang.String)",119,121,"/**
 * Creates a Path object from parent and child Path objects.
 */","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(org.apache.hadoop.fs.Path,java.lang.String)",129,131,"/**
 * Creates a child path under the given parent path.
 * @param parent The parent path.
 * @param child The name of the child path.
 */
","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,<init>,"org.apache.hadoop.fs.Path:<init>(java.lang.String,org.apache.hadoop.fs.Path)",139,141,"/**
 * Constructs a Path with a parent Path and a child string.
 */","* Create a new Path based on the child path resolved against the parent path.
   *
   * @param parent the parent path
   * @param child the child path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,rename,"org.apache.hadoop.io.MapFile:rename(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.String)",898,905,"/**
 * Renames a file or directory within the given file system.
 * @param fs The file system to operate on.
 * @param oldName The old name of the file/directory.
 * @param newName The new name for the file/directory.
 */
","* Renames an existing map directory.
   * @param fs fs.
   * @param oldName oldName.
   * @param newName newName.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,insecureCreateForWrite,"org.apache.hadoop.io.SecureIOUtils:insecureCreateForWrite(java.io.File,int)",243,262,"/**
 * Creates a file for writing, checking for existence and setting permissions.
 * @param f the file to create
 * @param permissions file permissions
 * @throws IOException if file exists or an error occurs
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,fileToPath,org.apache.hadoop.security.token.DtFileOperations:fileToPath(java.io.File),93,95,"/**
 * Converts a File object to a Path object.
 * @param f The File object to convert.
 * @return A Path object representing the file.
 */
",Add the service prefix for a local filesystem.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,unnestUri,org.apache.hadoop.security.ProviderUtils:unnestUri(java.net.URI),80,101,"/**
 * Extracts a Path from a nested URI, handling authority components.
 */","* Convert a nested URI to decode the underlying path. The translation takes
   * the authority and parses it into the underlying scheme and authority.
   * For example, ""myscheme://hdfs@nn/my/path"" is converted to
   * ""hdfs://nn/my/path"".
   * @param nestedUri the URI from the nested URI
   * @return the unnested path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,constructNewPath,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:constructNewPath(org.apache.hadoop.fs.Path),300,302,"/**
 * Creates a new Path by appending ""_NEW"" to the original path.
 * @param path The original Path object.
 * @return A new Path object with the modified string.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,constructOldPath,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:constructOldPath(org.apache.hadoop.fs.Path),304,306,"/**
 * Creates a new Path by appending ""_OLD"" to the input path.
 * @param path The original Path object.
 * @return A new Path object with the modified name.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,stringToPath,org.apache.hadoop.util.StringUtils:stringToPath(java.lang.String[]),273,282,"/**
 * Converts a string array to a Path array.
 * @param str String array to convert.
 * @return Path array or null if input is null.
 */
","* stringToPath.
   * @param str str.
   * @return path array.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,makeQualified,org.apache.hadoop.fs.HarFileSystem:makeQualified(org.apache.hadoop.fs.Path),400,412,"/**
 * Creates a qualified Path based on the input path.
 * @param path The path to qualify.
 * @return A qualified Path object.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getPathWithoutSchemeAndAuthority,org.apache.hadoop.fs.Path:getPathWithoutSchemeAndAuthority(org.apache.hadoop.fs.Path),104,111,"/**
 * Returns a Path with the scheme and authority removed.
 * Handles absolute URI paths correctly.
 */
","* Return a version of the given Path without the scheme information.
   *
   * @param path the source Path
   * @return a copy of this Path without the scheme information",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,mergePaths,"org.apache.hadoop.fs.Path:mergePaths(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",277,286,"/**
 * Merges two paths, handling Windows drive letters correctly.
 * @param path1 The first path.
 * @param path2 The second path to merge.
 * @return A new Path object representing the merged path.
 */
","* Merge 2 paths such that the second path is appended relative to the first.
   * The returned path has the scheme and authority of the first path.  On
   * Windows, the drive specification in the second path is discarded.
   * 
   * @param path1 the first path
   * @param path2 the second path, to be appended relative to path1
   * @return the merged path",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getParentUtil,org.apache.hadoop.fs.Path:getParentUtil(),444,459,"/**
 * Gets the parent path component of the URI.
 * Returns null if the path is empty or at the root.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Print.java,apply,"org.apache.hadoop.fs.shell.find.Print:apply(org.apache.hadoop.fs.shell.PathData,int)",59,63,"/**
 * Applies transformation to PathData, prints to output.
 * @param item PathData to transform, @param depth transformation depth
 * @return PASS result
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processPath,org.apache.hadoop.fs.shell.Display$Checksum:processPath(org.apache.hadoop.fs.shell.PathData),197,214,"/**
 * Processes a PathData item, handling directories and generating checksum output.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,checkIfExists,org.apache.hadoop.fs.shell.PathData:checkIfExists(org.apache.hadoop.fs.shell.PathData$FileTypeRequirement),220,233,"/**
 * Checks if the path exists and satisfies the given type requirement.
 * @param typeRequirement FileTypeRequirement to check against.
 */
","* Ensure that the file exists and if it is or is not a directory
   * @param typeRequirement Set it to the desired requirement.
   * @throws PathIOException if file doesn't exist or the type does not match
   * what was specified in typeRequirement.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getStringForChildPath,org.apache.hadoop.fs.shell.PathData:getStringForChildPath(org.apache.hadoop.fs.Path),319,328,"/**
 * Constructs a string representation of a child path.
 * @param childPath The Path object representing the child path.
 * @return String representation of the child path.
 */
","* Given a child of this directory, use the directory's path and the child's
   * basename to construct the string to the child.  This preserves relative
   * paths since Path will fully qualify.
   * @param childPath a path contained within this directory
   * @return String of the path relative to this directory",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,processPath,org.apache.hadoop.fs.shell.Ls:processPath(org.apache.hadoop.fs.shell.PathData),285,321,"/**
 * Processes a PathData item, displaying its details or just the path.
 * @param item The PathData to process.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,rename,"org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:rename(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",546,563,"/**
* Renames a PathData object to a target PathData, deleting target if needed.
*/
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processPath,org.apache.hadoop.fs.shell.Delete$Rmdir:processPath(org.apache.hadoop.fs.shell.PathData),205,217,"/**
 * Processes a PathData item, deleting if empty or throwing exception.
 * @param item PathData object containing path and filesystem info.
 * @throws IOException if an error occurs during processing.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,processPath,org.apache.hadoop.fs.shell.Mkdir:processPath(org.apache.hadoop.fs.shell.PathData),58,67,"/**
 * Processes a path data item, throwing exceptions if needed.
 * Checks if the item is a directory and throws exceptions.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processPath,org.apache.hadoop.fs.shell.SnapshotCommands$RenameSnapshot:processPath(org.apache.hadoop.fs.shell.PathData),143,148,"/**
 * Processes a PathData item, throwing exception if not a directory.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processNonexistentPath,org.apache.hadoop.fs.shell.Command:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),330,332,"/**
 * Throws a PathNotFoundException when the path does not exist.
 * @param item The PathData object representing the missing path.
 * @throws IOException if the path is not found.
 */
","*  Provides a hook for handling paths that don't exist.  By default it
   *  will throw an exception.  Primarily overriden by commands that create
   *  paths such as mkdir or touch.
   *  @param item the {@link PathData} that doesn't exist
   *  @throws FileNotFoundException if arg is a path and it doesn't exist
   *  @throws IOException if anything else goes wrong...",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processPath,org.apache.hadoop.fs.shell.SnapshotCommands$CreateSnapshot:processPath(org.apache.hadoop.fs.shell.PathData),57,62,"/**
 * Processes a PathData item, throwing exception if not a directory.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processPath,"org.apache.hadoop.fs.shell.MoveCommands$Rename:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",111,128,"/**
 * Renames a path to the target location, validating filesystem matches.
 * @param src Source PathData
 * @param target Target PathData
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SnapshotCommands.java,processPath,org.apache.hadoop.fs.shell.SnapshotCommands$DeleteSnapshot:processPath(org.apache.hadoop.fs.shell.PathData),102,107,"/**
 * Processes a PathData item, throwing exception if not a directory.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,processPath,org.apache.hadoop.fs.shell.Truncate:processPath(org.apache.hadoop.fs.shell.PathData),75,97,"/**
 * Truncates a file to the specified length. Throws exception if directory.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,processPath,org.apache.hadoop.fs.shell.SetReplication:processPath(org.apache.hadoop.fs.shell.PathData),81,103,"/**
 * Processes a PathData item, handling symlinks and setting replication.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,processPath,"org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",61,68,"/**
* Processes a path, throwing exception if target directory exists.
* @param src Source PathData
* @param target Target PathData
*/
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/MoveCommands.java,postProcessPath,org.apache.hadoop.fs.shell.MoveCommands$MoveFromLocal:postProcessPath(org.apache.hadoop.fs.shell.PathData),70,78,"/**
 * Deletes the path data. Throws PathIOException if deletion fails.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFSofPath,org.apache.hadoop.fs.FileContext:getFSofPath(org.apache.hadoop.fs.Path),325,337,"/**
 * Retrieves the AbstractFileSystem for the given path.
 * @param absOrFqPath Path to check; must be absolute/FQ.
 * @return AbstractFileSystem associated with the path.
 */
","* Get the file system of supplied path.
   * 
   * @param absOrFqPath - absolute or fully qualified path
   * @return the file system of the path
   * 
   * @throws UnsupportedFileSystemException If the file system for
   *           <code>absOrFqPath</code> is not supported.
   * @throws IOException If the file system for <code>absOrFqPath</code> could
   *         not be instantiated.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,<init>,"org.apache.hadoop.fs.viewfs.ChRootedFs:<init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.fs.Path)",102,122,"/**
 * Creates a ChRootedFs instance.
 * @param fs AbstractFileSystem instance.
 * @param theRoot Root path for the chroot.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getUriPath,org.apache.hadoop.fs.FilterFs:getUriPath(org.apache.hadoop.fs.Path),189,192,"/**
 * Gets the URI path for a given path.
 * @param p the path to get the URI path for
 * @return the URI path as a String
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,resolvePath,org.apache.hadoop.fs.viewfs.ViewFs:resolvePath(org.apache.hadoop.fs.Path),328,338,"/**
 * Resolves a path within the file system.
 * @param f The path to resolve.
 * @return Resolved Path object.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,resolvePath,org.apache.hadoop.fs.FilterFs:resolvePath(org.apache.hadoop.fs.Path),168,172,"/**
 * Resolves a path using the underlying file system.
 * @param p The path to resolve.
 * @return The resolved Path object.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,createInternal,"org.apache.hadoop.fs.FilterFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",88,97,"/**
 * Creates an internal data output stream.
 * @param f Path to create stream on
 * @return FSDataOutputStream
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,delete,"org.apache.hadoop.fs.FilterFs:delete(org.apache.hadoop.fs.Path,boolean)",99,104,"/**
 * Deletes a file or directory.
 * @param f Path to delete.
 * @param recursive If true, deletes recursively.
 * @return True if successful, false otherwise.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileBlockLocations,"org.apache.hadoop.fs.FilterFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",106,111,"/**
 * Gets block locations for a file within a given range.
 * @param f Path to the file.
 * @param start Start offset.
 * @param len Length of the range.
 * @return BlockLocation array.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileChecksum,org.apache.hadoop.fs.FilterFs:getFileChecksum(org.apache.hadoop.fs.Path),113,118,"/**
 * Gets the file checksum for a given path.
 * @param f Path to the file.
 * @return FileChecksum object.
 * @throws IOException, UnresolvedLinkException
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileStatus,org.apache.hadoop.fs.FilterFs:getFileStatus(org.apache.hadoop.fs.Path),120,125,"/**
 * Gets the file status for the given path.
 * @param f the path to get the status for
 * @return FileStatus object representing the path
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getFileLinkStatus,org.apache.hadoop.fs.FilterFs:getFileLinkStatus(org.apache.hadoop.fs.Path),139,144,"/**
 * Gets the status of a file link.
 * @param f Path to the file link.
 * @return FileStatus object or throws exception.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listStatus,org.apache.hadoop.fs.FilterFs:listStatus(org.apache.hadoop.fs.Path),194,199,"/**
 * Lists the status of a file or directory.
 * @param f Path to list status for.
 * @return FileStatus array.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,listLocatedStatus,org.apache.hadoop.fs.FilterFs:listLocatedStatus(org.apache.hadoop.fs.Path),201,207,"/**
 * Lists located status for a given path.
 * @param f the path to list
 * @return RemoteIterator of LocatedFileStatus objects
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,mkdir,"org.apache.hadoop.fs.FilterFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",215,221,"/**
 * Creates a directory with specified permissions and parent flags.
 * @param dir Path of the directory to create.
 * @param permission FsPermission for the directory.
 * @param createParent Whether to create parent directories.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,open,org.apache.hadoop.fs.FilterFs:open(org.apache.hadoop.fs.Path),223,228,"/**
 * Opens a file for input stream.
 * @param f Path to the file.
 * @return FSDataInputStream for reading.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,open,"org.apache.hadoop.fs.FilterFs:open(org.apache.hadoop.fs.Path,int)",230,235,"/**
 * Opens a file for input.
 * @param f Path to the file.
 * @param bufferSize I/O buffer size.
 * @return FSDataInputStream for reading.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,truncate,"org.apache.hadoop.fs.FilterFs:truncate(org.apache.hadoop.fs.Path,long)",237,243,"/**
 * Truncates a file to the specified length.
 * @param f The file to truncate.
 * @param newLength The new length of the file.
 * @return True if successful, false otherwise.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setOwner,"org.apache.hadoop.fs.FilterFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",261,267,"/**
 * Sets the owner of a file system path.
 * @param f path to the file; username, groupname - new owner details.
 * @throws IOException, UnresolvedLinkException on failure.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setPermission,"org.apache.hadoop.fs.FilterFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",269,274,"/**
 * Sets the permission for a given path.
 * @param f The path to set permission on.
 * @param permission The permission to apply.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setReplication,"org.apache.hadoop.fs.FilterFs:setReplication(org.apache.hadoop.fs.Path,short)",276,281,"/**
 * Sets the replication factor for a file.
 * @param f Path to the file. @param replication New replication count.
 * @return True if successful, false otherwise.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,setTimes,"org.apache.hadoop.fs.FilterFs:setTimes(org.apache.hadoop.fs.Path,long,long)",283,288,"/**
 * Sets modification and access times for a file.
 * @param f The Path object representing the file.
 * @param mtime Modification time in milliseconds.
 * @param atime Access time in milliseconds.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,mkdirs,"org.apache.hadoop.fs.FileSystem:mkdirs(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",764,771,"/**
 * Creates directories recursively and sets their permission.
 * @param fs FileSystem object
 * @param dir Path to create
 * @param permission FsPermission to set
 * @return True if directories were created, false otherwise.
 */
","* Create a directory with the provided permission.
   * The permission of the directory is set to be the provided permission as in
   * setPermission, not permission{@literal &~}umask
   *
   * @see #create(FileSystem, Path, FsPermission)
   *
   * @param fs FileSystem handle
   * @param dir the name of the directory to be created
   * @param permission the permission of the directory
   * @return true if the directory creation succeeds; false otherwise
   * @throws IOException A problem creating the directories.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,mkdirs,org.apache.hadoop.fs.ChecksumFileSystem:mkdirs(org.apache.hadoop.fs.Path),986,989,"/**
 * Creates directory and its parent directories, if they don't exist.
 * @param f the path of the directory to create
 * @return true if the directory was created, false otherwise
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,mkdirs,org.apache.hadoop.fs.FilterFileSystem:mkdirs(org.apache.hadoop.fs.Path),339,342,"/**
 * Creates directory recursively.
 * @param f The path representing the directory to create.
 * @return True if the creation was successful.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/protocolPB/PBHelper.java,convert,org.apache.hadoop.fs.protocolPB.PBHelper:convert(org.apache.hadoop.fs.FSProtos$FileStatusProto),49,106,"/**
* Converts a FileStatusProto to a FileStatus object.
* @param proto The FileStatusProto to convert.
* @return A FileStatus object.
*/
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean)",151,158,"/**
 * Constructs a FileStatus with default attributes.
 * @param length File length, isdir, replication, blocksize, timestamps,
 *                permissions, owner, group, symlink, path.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Set,org.apache.hadoop.fs.BlockLocation[])",142,149,"/**
 * Constructs a LocatedFileStatus with file details and block locations.
 * @param locations Array of BlockLocation objects
 */
","* Constructor.
   *
   * @param length a file's length
   * @param isdir if the path is a directory
   * @param block_replication the file's replication factor
   * @param blocksize a file's block size
   * @param modification_time a file's modification time
   * @param access_time a file's access time
   * @param permission a file's permission
   * @param owner a file's owner
   * @param group a file's group
   * @param symlink symlink if the path is a symbolic link
   * @param path the path's qualified name
   * @param attr Attribute flags (See {@link FileStatus.AttrFlags}).
   * @param locations a file's block locations",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,getPermission,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:getPermission(),59,62,"/**
 * Returns the permission of this file system object.
 * Delegates to the superclass implementation.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,append,"org.apache.hadoop.io.SequenceFile$Writer:append(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",1458,1461,"/**
* Appends a key-value pair to the output stream.
* @param key The key to append.
* @param val The value associated with the key.
*/
","* Append a key/value pair.
     * @param key input Writable key.
     * @param val input Writable val.
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,writeFile,"org.apache.hadoop.io.SequenceFile$Sorter:writeFile(org.apache.hadoop.io.SequenceFile$Sorter$RawKeyValueIterator,org.apache.hadoop.io.SequenceFile$Writer)",3431,3438,"/**
 * Writes raw key-value pairs from iterator to writer.
 * @param records Iterator of raw key-value pairs.
 * @param writer Writer to append data to.
 */
","* Writes records from RawKeyValueIterator into a file represented by the 
     * passed writer.
     * @param records the RawKeyValueIterator
     * @param writer the Writer created earlier 
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,init,org.apache.hadoop.service.AbstractService:init(org.apache.hadoop.conf.Configuration),152,178,"/**
 * Initializes the service. Throws exception if configuration is null.
 */","* {@inheritDoc}
   * This invokes {@link #serviceInit}
   * @param conf the configuration of the service. This must not be null
   * @throws ServiceStateException if the configuration was null,
   * the state change not permitted, or something else went wrong",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,stop,org.apache.hadoop.service.AbstractService:stop(),213,240,"/**
 * Stops the service. Handles state transitions and exceptions.
 */",* {@inheritDoc},,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Metadata:equals(org.apache.hadoop.io.SequenceFile$Metadata),797,820,"/**
 * Checks if this Metadata object equals another.
 * @param other The Metadata object to compare to.
 * @return True if objects are equal, false otherwise.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,handleKind,org.apache.hadoop.security.token.Token$TrivialRenewer:handleKind(org.apache.hadoop.io.Text),528,531,"/**
 * Checks if the given text kind matches the object's kind.
 * @param kind The text kind to compare.
 * @return True if kinds match, false otherwise.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSelector.java,selectToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSelector:selectToken(org.apache.hadoop.io.Text,java.util.Collection)",46,60,"/**
 * Finds a token matching the service and kind.
 * @param service The service to match.
 * @param tokens Tokens to search. Returns matching token or null.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,isPrivateCloneOf,org.apache.hadoop.security.token.Token$PrivateToken:isPrivateCloneOf(org.apache.hadoop.io.Text),279,282,"/**
 * Checks if this service is a private clone of another.
 * @param thePublicService The service to compare to.
 * @return True if it's a clone, false otherwise.
 */
","* Whether this is a private clone of a public token.
     * @param thePublicService the public service name
     * @return true when the public service is the same as specified",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,equals,org.apache.hadoop.security.token.Token:equals(java.lang.Object),386,400,"/**
 * Checks if two Token objects are equal based on their fields.
 * @param right the Token object to compare to
 * @return true if the tokens are equal, false otherwise
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,matchAlias,"org.apache.hadoop.security.token.DtFileOperations:matchAlias(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",73,75,"/**
 * Checks if a token's service matches a given alias.
 * @param token The token to check.
 * @param alias The alias to compare against.
 * @return True if alias is null or service matches.
 */
",Match token service field to alias text.  True if alias is null.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,matchService,"org.apache.hadoop.security.token.DtFileOperations:matchService(org.apache.hadoop.security.token.DtFetcher,org.apache.hadoop.io.Text,java.lang.String)",78,83,"/**
 * Checks if a service matches based on fetcher and URL.
 * @param fetcher Fetches service name.
 * @param service Service text to compare.
 * @param url URL string to check.
 * @return True if service matches, false otherwise.
 */
",Match fetcher's service name to the service text and/or url prefix.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,selectDelegationToken,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text)",1008,1018,"/**
 * Selects a delegation token based on credentials and service.
 * @param creds Credentials object
 * @param service Service identifier
 * @return Delegation token or null if none found
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,handleKind,org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:handleKind(org.apache.hadoop.io.Text),179,182,"/**
 * Checks if the given text kind matches the expected token kind.
 * @param kind The text kind to check.
 * @return True if the kinds match, false otherwise.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDir,org.apache.hadoop.util.DiskChecker:checkDir(java.io.File),76,78,"/**
 * Checks the directory for errors.
 * @param dir The directory to check.
 * @throws DiskErrorException if an error is found.
 */
","* Create the directory if it doesn't exist and check that dir is readable,
   * writable and executable
   *  
   * @param dir dir.
   * @throws DiskErrorException disk problem.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirWithDiskIo,org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(java.io.File),88,92,"/**
 * Checks a directory and performs disk I/O operations on it.
 * @param dir The directory to check.
 * @throws DiskErrorException if an error occurs during disk I/O.
 */
","* Create the directory if it doesn't exist and check that dir is
   * readable, writable and executable. Perform some disk IO to
   * ensure that the disk is usable for writes.
   *
   * @param dir dir.
   * @throws DiskErrorException disk problem.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,flushBuffer,org.apache.hadoop.fs.FSOutputSummer:flushBuffer(),145,147,"/**
 * Flushes the internal buffer.
 * Synchronized to prevent race conditions.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,flush,org.apache.hadoop.fs.FSOutputSummer:flush(),183,185,"/**
 * Flushes the internal buffer to the underlying stream.
 */","* Checksums all complete data chunks and flushes them to the underlying
   * stream. If there is a trailing partial chunk, it is not flushed and is
   * maintained in the buffer.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,doDecodeSingle,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeSingle(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int,int,boolean)",123,222,"/**
 * Decodes data based on erasure locations and input buffers.
 * @param inputs Input buffers for data and parity.
 * @param outputs Output buffers for decoded parity.
 * @param erasedLocationToFix Location of erased data to fix.
 * @param bufSize Buffer size.
 * @param isDirect Whether the buffers are direct.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,doDecodeMultiAndParity,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:doDecodeMultiAndParity(java.nio.ByteBuffer[][],java.nio.ByteBuffer[][],int[],int)",265,351,"/**
 * Decodes data and parity units, handling erased locations.
 * @param inputs Input ByteBuffer arrays.
 * @param outputs Output ByteBuffer arrays.
 * @param erasedLocationToFix Array of erased unit locations.
 * @param bufSize Buffer size for position adjustments.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DecodingValidator.java,validate,"org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",73,128,"/**
 * Validates decoding process using provided inputs and outputs.
 * @param inputs Input buffers for decoding.
 * @param erasedIndexes Indexes of erased inputs.
 * @param outputs Output buffers for validation.
 */
","* Validate outputs decoded from inputs, by decoding an input back from
   * the outputs and comparing it with the original one.
   *
   * For instance, in RS (6, 3), let (d0, d1, d2, d3, d4, d5) be sources
   * and (p0, p1, p2) be parities, and assume
   *  inputs = [d0, null (d1), d2, d3, d4, d5, null (p0), p1, null (p2)];
   *  erasedIndexes = [1, 6];
   *  outputs = [d1, p0].
   * Then
   *  1. Create new inputs, erasedIndexes and outputs for validation so that
   *     the inputs could contain the decoded outputs, and decode them:
   *      newInputs = [d1, d2, d3, d4, d5, p0]
   *      newErasedIndexes = [0]
   *      newOutputs = [d0']
   *  2. Compare d0 and d0'. The comparison will fail with high probability
   *     when the initial outputs are wrong.
   *
   * Note that the input buffers' positions must be the ones where data are
   * read: If the input buffers have been processed by a decoder, the buffers'
   * positions must be reset before being passed into this method.
   *
   * This method does not change outputs and erasedIndexes.
   *
   * @param inputs input buffers used for decoding. The buffers' position
   *               are moved to the end after this method.
   * @param erasedIndexes indexes of erased units used for decoding
   * @param outputs decoded output buffers, which are ready to be read after
   *                the call
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RawErasureDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RawErasureDecoder:decode(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])",168,173,"/**
 * Decodes data chunks, handling erased indexes.
 * @param inputs Input ECChunk array.
 * @param erasedIndexes Indexes of erased chunks.
 * @param outputs Output ECChunk array.
 */
","* Decode with inputs and erasedIndexes, generates outputs. More see above.
   *
   * Note, for both input and output ECChunks, no mixing of on-heap buffers and
   * direct buffers are allowed.
   *
   * @param inputs input buffers to read data from
   * @param erasedIndexes indexes of erased units in the inputs array
   * @param outputs output buffers to put decoded data into according to
   *                erasedIndexes, ready for read after the call
   * @throws IOException if the decoder is closed",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(java.nio.ByteBuffer[],int[],java.nio.ByteBuffer[])",55,68,"/**
 * Decodes input data, creating copies and adjusting order before calling super.decode.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSLegacyRawDecoder.java,decode,"org.apache.hadoop.io.erasurecode.rawcoder.RSLegacyRawDecoder:decode(byte[][],int[],byte[][])",70,83,"/**
 * Decodes input data, creating copies and adjusting order before calling super.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteBufferDecodingState),73,84,"/**
 * Decodes data using Reed-Solomon encoding.
 * Uses provided inputs and outputs buffers.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/RSRawDecoder.java,doDecode,org.apache.hadoop.io.erasurecode.rawcoder.RSRawDecoder:doDecode(org.apache.hadoop.io.erasurecode.rawcoder.ByteArrayDecodingState),86,101,"/**
 * Decodes data using the provided decoding state and RSUtil.
 * @param decodingState State containing input/output buffers.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,initBlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:initBlock(),511,570,"/**
 * Initializes block processing based on read mode and header checks.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,internalReset,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:internalReset(),274,280,"/**
 * Resets the internal state if a reset is needed.
 * Writes header and creates a new CBZip2OutputStream.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,writeRun,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:writeRun(),654,706,"/**
* Writes a run of data to the output block, updating CRC.
* Handles run lengths 1, 2, 3, and default (>= 4).
*/
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,close,org.apache.hadoop.io.MapFile$Merger:close(),1148,1157,"/**
 * Closes all input readers and the output writer, setting them to null.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,close,org.apache.hadoop.io.file.tfile.BCFile$Writer$BlockAppender:close(),256,271,"/**
 * Closes the block writer, finalizing and registering block data.
 */","* Signaling the end of write to the block. The block register will be
       * called for registering the finished block.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,cleanup,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:cleanup(),3887,3892,"/**
 * Closes resources and deletes the segment if preserveInput is false.
 */","* The default cleanup. Subclasses can override this with a custom
       * cleanup.
       * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,close,org.apache.hadoop.io.file.tfile.BCFile$Reader$BlockReader:close(),557,569,"/**
 * Closes the resource, finishing the request block state.
 * Sets the 'closed' flag to prevent further operations.
 */
",* Finishing reading the block. Release all resources.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeWritableOutputStream,org.apache.hadoop.security.Credentials:writeWritableOutputStream(java.io.DataOutputStream),321,326,"/**
 * Writes data to the DataOutputStream, including magic and format.
 * @param os Output stream to write to.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readFields,org.apache.hadoop.security.Credentials:readFields(java.io.DataInput),420,443,"/**
 * Reads fields from input, populating tokenMap and secretKeysMap.
 */","* Loads all the keys.
   * @param in DataInput.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,createTokenIdent,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:createTokenIdent(byte[]),253,260,"/**
 * Creates a TokenIdent object from byte array.
 * @param tokenIdentBytes byte array containing token identifier data
 * @return TokenIdent object created from the byte array
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,processTokenAddOrUpdate,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processTokenAddOrUpdate(byte[]),411,427,"/**
 * Processes token add/update request.
 * @param data Token data as byte array.
 * @return TokenIdent object or null on failure.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,processTokenRemoved,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:processTokenRemoved(org.apache.curator.framework.recipes.cache.ChildData),429,435,"/**
 * Processes a removed token data. Reads token info, removes it.
 * @param data ChildData containing token data to process.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfoFromZK,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(java.lang.String,boolean)",652,679,"/**
 * Retrieves DelegationTokenInformation from ZooKeeper.
 * @param nodePath ZK node path.
 * @param quiet Whether to log errors.
 * @return DelegationTokenInformation or null if not found.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawKey,org.apache.hadoop.io.SequenceFile$Reader:nextRawKey(org.apache.hadoop.io.DataOutputBuffer),2662,2697,"/**
 * Reads the next raw key into the provided buffer.
 * @param key DataOutputBuffer to write the key to.
 * @return Key length or -1 if no more keys are available.
 */
","* Read 'raw' keys.
     * @param key - The buffer into which the key is read
     * @return Returns the key length or -1 for end of file
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getCurrentValue,org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(org.apache.hadoop.io.Writable),2376,2408,"/**
 * Reads the current value into the provided Writable object.
 * @param val The Writable object to populate with the value.
 * @throws IOException if an I/O error occurs.
 */
","* Get the 'value' corresponding to the last read 'key'.
     * @param val : The 'value' to be read.
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getCurrentValue,org.apache.hadoop.io.SequenceFile$Reader:getCurrentValue(java.lang.Object),2415,2448,"/**
 * Retrieves the current value, applying config and deserializing.
 * @param val The value to process.
 * @return The processed value.
 */
","* @return Get the 'value' corresponding to the last read 'key'.
     * @param val : The 'value' to be read.
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRaw,"org.apache.hadoop.io.SequenceFile$Reader:nextRaw(org.apache.hadoop.io.DataOutputBuffer,org.apache.hadoop.io.SequenceFile$ValueBytes)",2603,2654,"/**
 * Reads next raw key-value pair. Returns length or -1 if EOF.
 * @param key DataOutputBuffer for the key.
 * @param val ValueBytes for the value.
 * @return Length of the record, or -1 if end of file.
 */
","* Read 'raw' records.
     * @param key - The buffer into which the key is read
     * @param val - The 'raw' value
     * @return Returns the total record length or -1 for end of file
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawValue,org.apache.hadoop.io.SequenceFile$Reader:nextRawValue(org.apache.hadoop.io.SequenceFile$ValueBytes),2765,2790,"/**
 * Reads the next raw value length.
 * @param val ValueBytes object to reset.
 * @return Length of the raw value.
 * @throws IOException if an I/O error occurs.
 */","* Read 'raw' values.
     * @param val - The 'raw' value
     * @return Returns the value length
     * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,getTokenInfoFromSQL,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getTokenInfoFromSQL(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),238,251,"/**
 * Retrieves DelegationTokenInformation from SQL secret manager.
 * @param ident TokenIdent containing sequence number and bytes.
 * @throws RuntimeException if token retrieval fails.
 */
","* Obtains the DelegationTokenInformation associated with the given
   * TokenIdentifier in the SQL database.
   * @param ident Existing TokenIdentifier in the SQL database.
   * @return DelegationTokenInformation that matches the given TokenIdentifier or
   *         null if it doesn't exist in the database.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/PermissionStatus.java,read,org.apache.hadoop.fs.permission.PermissionStatus:read(java.io.DataInput),114,118,"/**
 * Reads a PermissionStatus object from the input stream.
 * @param in DataInput stream to read from
 * @return Populated PermissionStatus object
 */
","* Create and initialize a {@link PermissionStatus} from {@link DataInput}.
   * @param in data input.
   * @throws IOException raised on errors performing I/O.
   * @return PermissionStatus.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,readEnum,"org.apache.hadoop.io.WritableUtils:readEnum(java.io.DataInput,java.lang.Class)",422,425,"/**
 * Reads an enum value from input.
 * @param in DataInput to read from.
 * @param enumType Enum type to read.
 * @return Enum value or null if invalid.
 */
","* Read an Enum value from DataInput, Enums are read and written 
   * using String values. 
   * @param <T> Enum type
   * @param in DataInput to read from 
   * @param enumType Class type of Enum
   * @return Enum represented by String read from DataInput
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,readFields,org.apache.hadoop.security.authorize.AccessControlList:readFields(java.io.DataInput),326,330,"/**
 * Reads ACL data from input stream and builds the ACL.
 * @param in DataInput stream to read from.
 * @throws IOException if an I/O error occurs.
 */
",* Deserializes the AccessControlList object,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getDelegationKey,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getDelegationKey(int),561,577,"/**
 * Retrieves a DelegationKey by ID, caching it if not found.
 * @param keyId The ID of the DelegationKey to retrieve.
 * @return The DelegationKey object or null if not found.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,write,org.apache.hadoop.io.file.tfile.BCFile$MetaIndex:write(java.io.DataOutput),790,796,"/**
 * Writes the index to the DataOutput.
 * @param out Output stream to write to.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,getFixedByteString,org.apache.hadoop.ipc.ProtobufHelper:getFixedByteString(org.apache.hadoop.io.Text),85,87,"/**
 * Gets a fixed-length byte string from the key.
 * @param key The key to look up.
 * @return The corresponding byte string.
 */
","* Get the ByteString for frequently used fixed and small set strings.
   * @param key string
   * @return the ByteString for frequently used fixed and small set strings.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/internal/ShadedProtobufHelper.java,protoFromToken,org.apache.hadoop.ipc.internal.ShadedProtobufHelper:protoFromToken(org.apache.hadoop.security.token.Token),142,149,"/**
 * Converts a Token object to a TokenProto.
 * @param tok Token object to convert
 * @return TokenProto representation of the Token
 */
","* Create a {@code TokenProto} instance
   * from a hadoop token.
   * This builds and caches the fields
   * (identifier, password, kind, service) but not
   * renewer or any payload.
   * @param tok token
   * @return a marshallable protobuf class.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,copyToken,org.apache.hadoop.security.token.Token:copyToken(),114,116,"/**
 * Creates and returns a copy of the current Token object.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:createToken(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",160,164,"/**
 * Creates a token for delegation.
 * @param ugi UserGroupInformation context.
 * @param renewer Token renewer principal.
 * @return Delegation token.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenIdentifier.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text)",49,53,"/**
 * Constructs a DelegationTokenIdentifier with kind, owner, renewer, realUser.
 */
","* Create a new delegation token identifier
   *
   * @param kind token kind
   * @param owner the effective username of the token owner
   * @param renewer the username of the renewer
   * @param realUser the real username of the token owner",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,<init>,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:<init>(),51,53,"/**
 * Default constructor. Initializes with null values for all parameters.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/InstrumentedReadWriteLock.java,<init>,"org.apache.hadoop.util.InstrumentedReadWriteLock:<init>(boolean,java.lang.String,org.slf4j.Logger,long,long)",40,47,"/**
 * Constructs an InstrumentedReadWriteLock with specified fairness.
 * @param fair if true, locks are granted in FIFO order
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invokeOnce,org.apache.hadoop.io.retry.RetryInvocationHandler$Call:invokeOnce(),89,117,"/**
 * Executes the method once, handling retries and failures.
 * Returns CallReturn object with result or exception.
 */
",Invoke the call once without retrying.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,checkKey,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:checkKey(),1596,1611,"/**
 * Reads and validates the key from the input stream.
 * Reads key length, key buffer, and prepares for value read.
 */","* check whether we have already successfully obtained the key. It also
       * initializes the valueInputStream.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getValue,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:getValue(org.apache.hadoop.io.BytesWritable),1706,1720,"/**
 * Reads data from input stream into BytesWritable.
 * @param value BytesWritable to populate with data.
 * @return Length of data read.
 */
","* Copy the value into BytesWritable. The input BytesWritable will be
         * automatically resized to the actual value size. The implementation
         * directly uses the buffer inside BytesWritable for storing the value.
         * The call does not require the value length to be known.
         * 
         * @param value value.
         * @throws IOException raised on errors performing I/O.
         * @return long value.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,writeValue,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:writeValue(java.io.OutputStream),1747,1763,"/**
 * Writes the value stream to the output stream.
 * @param out the output stream to write to
 * @return the number of bytes written
 */
","* Writing the value to the output stream. This method avoids copying
         * value data from Scanner into user buffer, then writing to the output
         * stream. It does not require the value length to be known.
         * 
         * @param out
         *          The output stream
         * @return the length of the value
         * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,read,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:read(byte[]),146,149,"/**
* Reads data from the input stream into the provided byte array.
* @param b the byte array to read into
* @return the number of bytes read
*/
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Chunk.java,close,org.apache.hadoop.io.file.tfile.Chunk$ChunkDecoder:close(),186,197,"/**
 * Closes the resource, skipping remaining data.
 * Sets the 'closed' flag to true.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:compareTo(byte[]),1932,1934,"/**
 * Compares this byte array with another byte array lexicographically.
 */","* Compare the entry key to another key. Synonymous to compareTo(key, 0,
         * key.length).
         * 
         * @param buf
         *          The key buffer.
         * @return comparison result between the entry key with the input key.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,equals,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:equals(java.lang.Object),1966,1971,"/**
 * Checks if this entry equals another Entry based on key comparison.
 */",* Compare whether this and other points to the same key value.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String,java.lang.String)",188,203,"/**
 * Retrieves a delegation token from a URL.
 * @param url URL to fetch token from.
 * @param token Authentication token.
 * @return Delegation token.
 * @throws IOException, AuthenticationException
 */
","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return abstract delegation token identifier.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)",237,245,"/**
 * Renews a delegation token.
 * @param url URL for renewal, token, dToken, doAsUser.
 * @return New token expiry timestamp.
 */
","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @param doAsUser the user to do as, which will be the token owner.
   * @param dToken abstract delegation token identifier.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token,java.lang.String)",275,286,"/**
 * Cancels a delegation token.
 * @param url URL of the delegation token endpoint.
 * @param token Authentication token.
 * @param dToken Delegation token.
 * @param doAsUser User to execute as.
 */
","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @param dToken abstract delegation token identifier.
   * @param doAsUser the user to do as, which will be the token owner.
   * @throws IOException if an IO error occurred.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,chooseRandom,org.apache.hadoop.net.NetworkTopology:chooseRandom(java.lang.String),468,470,"/**
 * Chooses a random node within the specified scope.
 * @param scope The scope to search for a random node.
 */
","* Randomly choose a node.
   *
   * @param scope range of nodes from which a node will be chosen
   * @return the chosen node
   *
   * @see #chooseRandom(String, Collection)",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,sortByDistance,"org.apache.hadoop.net.NetworkTopologyWithNodeGroup:sortByDistance(org.apache.hadoop.net.Node,org.apache.hadoop.net.Node[],int)",285,300,"/**
 * Sorts nodes by distance, handling cases where the reader is invalid.
 * @param reader The reader node.
 * @param nodes The array of nodes.
 * @param activeLen Active length.
 */
","* Sort nodes array by their distances to <i>reader</i>.
   * <p>
   * This is the same as {@link NetworkTopology#sortByDistance(Node, Node[],
   * int)} except with a four-level network topology which contains the
   * additional network distance of a ""node group"" which is between local and
   * same rack.
   *
   * @param reader    Node where data will be read
   * @param nodes     Available replicas with the requested data
   * @param activeLen Number of active nodes at the front of the array",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getInputStream,"org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket,long)",496,503,"/**
 * Gets an input stream wrapper for the socket with a timeout.
 * @param socket The socket to wrap.
 * @param timeout Timeout in milliseconds.
 * @return SocketInputWrapper object.
 * @throws IOException if an I/O error occurs.
 */
","* Return a {@link SocketInputWrapper} for the socket and set the given
   * timeout. If the socket does not have an associated channel, then its socket
   * timeout will be set to the specified value. Otherwise, a
   * {@link SocketInputStream} will be created which reads with the configured
   * timeout.
   * 
   * Any socket created using socket factories returned by {@link #NetUtils},
   * must use this interface instead of {@link Socket#getInputStream()}.
   * 
   * In general, this should be called only once on each socket: see the note
   * in {@link SocketInputWrapper#setTimeout(long)} for more information.
   *
   * @see Socket#getChannel()
   * 
   * @param socket socket.
   * @param timeout timeout in milliseconds. zero for waiting as
   *                long as necessary.
   * @return SocketInputWrapper for reading from the socket.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getOutputStream,"org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket,long)",550,554,"/**
 * Gets the output stream for the socket, using a timeout if available.
 * @param socket The socket to get the output stream from.
 * @param timeout Timeout in milliseconds.
 * @return Output stream for the socket.
 */
","* Returns OutputStream for the socket. If the socket has an associated
   * SocketChannel then it returns a 
   * {@link SocketOutputStream} with the given timeout. If the socket does not
   * have a channel, {@link Socket#getOutputStream()} is returned. In the later
   * case, the timeout argument is ignored and the write will wait until 
   * data is available.<br><br>
   * 
   * Any socket created using socket factories returned by {@link NetUtils},
   * must use this interface instead of {@link Socket#getOutputStream()}.
   * 
   * @see Socket#getChannel()
   * 
   * @param socket socket.
   * @param timeout timeout in milliseconds. This may not always apply. zero
   *        for waiting as long as necessary.
   * @return OutputStream for writing to the socket.
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,writeMetric,org.apache.hadoop.metrics2.sink.StatsDSink:writeMetric(java.lang.String),150,157,"/**
 * Writes a metric to StatsD. Throws MetricsException on failure.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopologyWithNodeGroup.java,<init>,org.apache.hadoop.net.NetworkTopologyWithNodeGroup:<init>(),38,40,"/**
 * Initializes the NetworkTopologyWithNodeGroup with a root cluster.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,connect,"org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,java.net.SocketAddress,int)",590,636,"/**
 * Connects a socket to a remote endpoint with a specified timeout.
 * @param socket The socket to connect.
 * @param endpoint Remote address to connect to.
 * @param localAddr Local address to bind to (optional).
 * @param timeout Connection timeout in milliseconds.
 */
","* Like {@link NetUtils#connect(Socket, SocketAddress, int)} but
   * also takes a local address and port to bind the socket to. 
   * 
   * @param socket socket.
   * @param endpoint the remote address
   * @param localAddr the local address to bind the socket to
   * @param timeout timeout in milliseconds
   * @throws IOException raised on errors performing I/O.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:<init>(java.lang.String,java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource,java.lang.Iterable,long,org.apache.hadoop.metrics2.impl.MetricsConfig)",90,98,"/**
 * Constructs a MetricsSourceAdapter with default filters and startup flag.
 * @param prefix Metric name prefix.
 * @param name Metric name.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,snapshotMetrics,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:snapshotMetrics(org.apache.hadoop.metrics2.impl.MetricsSourceAdapter,org.apache.hadoop.metrics2.impl.MetricsBufferBuilder)",420,427,"/**
 * Snapshots metrics from a source adapter and adds to buffer.
 * @param sa Source adapter to snapshot.
 * @param bufferBuilder Metrics buffer builder.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,updateJmxCache,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:updateJmxCache(),160,194,"/**
 * Updates the JMX cache, refreshing metrics if TTL expired.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,register,"org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.util.Map,java.lang.Object)",89,114,"/**
 * Registers an MBean with the platform MBean server.
 * @param serviceName Service name, name MBean name, properties, MBean.
 * @return ObjectName of registered MBean, or null on failure.
 */
","* Register the MBean using our standard MBeanName format
   * ""hadoop:service={@literal <serviceName>,name=<nameName>}""
   * Where the {@literal <serviceName> and <nameName>} are the supplied
   * parameters.
   *
   * @param serviceName serviceName.
   * @param nameName nameName.
   * @param properties - Key value pairs to define additional JMX ObjectName
   *                     properties.
   * @param theMbean    - the MBean to register
   * @return the named used to register the MBean",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,unregisterSource,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:unregisterSource(java.lang.String),245,258,"/**
 * Unregisters a data source by name. Removes all related resources.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stopSources,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stopSources(),460,469,"/**
 * Stops all metrics sources and clears the sources map.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newInverseQuantiles,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newInverseQuantiles(java.lang.String,java.lang.String,java.lang.String,java.lang.String,int)",240,251,"/**
 * Creates and registers a new inverse quantiles metric.
 * @param name Metric name
 * @param interval Interval for quantiles
 * @return New MutableInverseQuantiles object
 */
","* Create a mutable inverse metric that estimates inverse quantiles of a stream of values
   * @param name of the metric
   * @param desc metric description
   * @param sampleName of the metric (e.g., ""Ops"")
   * @param valueName of the metric (e.g., ""Rate"")
   * @param interval rollover interval of estimator in seconds
   * @return a new inverse quantile estimator object
   * @throws MetricsException if interval is not a positive integer",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,<init>,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:<init>(),53,71,"/**
 * Initializes the read/write disk validator metrics with quantile gauges.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,<init>,"org.apache.hadoop.ipc.RetryCache:<init>(java.lang.String,double,long)",196,204,"/**
 * Constructs a RetryCache with specified name, percentage, and expiration.
 */","* Constructor
   * @param cacheName name to identify the cache by
   * @param percentage percentage of total java heap space used by this cache
   * @param expirationTime time for an entry to expire in nanoseconds",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,init,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class),71,81,"/**
 * Initializes metric tracking for a protocol class.
 * @param protocol Class containing methods to track.
 */
","* Initialize the registry with all the methods in a protocol
   * so they all show up in the first snapshot.
   * Convenient for JMX implementations.
   * @param protocol the protocol class",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,init,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.String[]),89,93,"/**
 * Initializes metrics by adding them if they don't already exist.
 * @param names Array of metric names to initialize.
 */
","* Initialize the registry with all rate names passed in.
   * This is an alternative to the above init function since this metric
   * can be used more than just for rpc name.
   * @param names the array of all rate names",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,aggregateLocalStatesToGlobalMetrics,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:aggregateLocalStatesToGlobalMetrics(java.util.concurrent.ConcurrentMap),149,157,"/**
 * Aggregates local stats to global metrics.
 * @param localStats Map of local stats to aggregate.
 */
","* Aggregates the thread's local samples into the global metrics. The caller
   * should ensure its thread safety.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String,boolean)",310,312,"/**
* Creates a new rate with extended flag defaulted to true.
* @param name Rate name.
* @param desc Rate description.
* @param extended Extended rate flag.
* @return New MutableRate object.
*/
","* Create a mutable rate metric (for throughput measurement).
   * @param name  of the metric
   * @param desc  description
   * @param extended  produce extended stat (stdev/min/max etc.) if true
   * @return a new mutable rate metric object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,init,org.apache.hadoop.metrics2.lib.MutableRates:init(java.lang.Class),58,69,"/**
 * Initializes rate metrics for methods in the given protocol class.
 * @param protocol Class containing methods to register for metrics.
 */
","* Initialize the registry with all the methods in a protocol
   * so they all show up in the first snapshot.
   * Convenient for JMX implementations.
   * @param protocol the protocol class",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,<init>,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:<init>(java.lang.String),53,58,"/**
 * Initializes detailed RPC scheduler metrics with a namespace.
 * @param ns Namespace for metrics; used in metric names.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,<init>,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:<init>(int),57,62,"/**
 * Initializes RPC detailed metrics for a given port.
 * @param port The RPC port to track metrics for.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,addResponseTime,"org.apache.hadoop.ipc.DecayRpcScheduler:addResponseTime(java.lang.String,org.apache.hadoop.ipc.Schedulable,org.apache.hadoop.ipc.ProcessingDetails)",747,770,"/**
 * Records response time metrics for a schedulable task.
 * @param callName Name of the call.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,updateDeferredMetrics,"org.apache.hadoop.ipc.Server:updateDeferredMetrics(java.lang.String,long)",670,673,"/**
 * Updates deferred RPC metrics with processing time.
 * @param name RPC name, @param processingTime processing time in ms.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,updateMetrics,"org.apache.hadoop.ipc.Server:updateMetrics(org.apache.hadoop.ipc.Server$Call,long,boolean)",617,668,"/**
 * Updates RPC metrics based on call details and timings.
 * @param call The RPC call object.
 * @param processingStartTimeNanos Processing start time in nanoseconds.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,add,"org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:add(java.lang.Object,java.lang.reflect.Method)",162,170,"/**
 * Processes methods annotated with @Metric to create metrics.
 * @param source The object the method belongs to.
 * @param method The method to process.
 */
",Add {@link MutableMetric} for a method annotated with {@link Metric},,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getMetrics,"org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getMetrics(org.apache.hadoop.metrics2.MetricsCollector,boolean)",963,969,"/**
 * Retrieves metrics from the delegate scheduler.
 * @param collector MetricsCollector to populate.
 * @param all Whether to fetch all metrics.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateKeyLength,org.apache.hadoop.security.KDiag:validateKeyLength(),442,450,"/**
 * Validates the maximum AES key length and throws an exception if too short.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateUGI,"org.apache.hadoop.security.KDiag:validateUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",697,706,"/**
 * Validates Kerberos authentication for the given user.
 * @param messagePrefix Prefix for verification messages.
 * @param user UserGroupInformation to validate.
 */
","* Validate the UGI: verify it is kerberized.
   * @param messagePrefix message in exceptions
   * @param user user to validate",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,verifyFileIsValid,"org.apache.hadoop.security.KDiag:verifyFileIsValid(java.io.File,java.lang.String,java.lang.String)",795,805,"/**
 * Verifies a file exists, is a file, is not empty, and is readable.
 * @param file The file to verify.
 * @param category Category for verification context.
 * @param text Verification message prefix.
 */
","* Verify that a file is valid: it is a file, non-empty and readable.
   * @param file file
   * @param category category for exceptions
   * @param text text message
   * @return true if the validation held; false if it did not <i>and</i>
   * {@link #nofail} has disabled raising exceptions.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateShortName,org.apache.hadoop.security.KDiag:validateShortName(),459,476,"/**
 * Validates the short name of the Kerberos principal.
 * Throws exception if validation fails.
 */
","* Verify whether auth_to_local rules transform a principal name
   * <p>
   * Having a local user name ""bar@foo.com"" may be harmless, so it is noted at
   * info. However if what was intended is a transformation to ""bar""
   * it can be difficult to debug, hence this check.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenIdentifier.java,getUser,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier:getUser(),71,87,"/**
* Retrieves UserGroupInformation, potentially using a proxy user.
* Returns null if owner is null or empty.
*/
","* Get the username encoded in the token identifier
   * 
   * @return the username or owner",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,getUgi,org.apache.hadoop.util.ProtoUtil:getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$UserInformationProto),133,150,"/**
 * Creates a UserGroupInformation based on UserInformationProto.
 * @param userInfo UserInformationProto object containing user details
 * @return UserGroupInformation object or null if no effective user.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,<init>,org.apache.hadoop.fs.LocalFileSystem:<init>(org.apache.hadoop.fs.FileSystem),70,72,"/**
 * Constructs a LocalFileSystem using a provided FileSystem.
 * @param rawLocalFileSystem The FileSystem to wrap.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,<init>,org.apache.hadoop.fs.shell.find.Find:<init>(),162,164,"/**
 * Default constructor. Sets the recursive flag to true.
 */
",Default constructor for the Find command.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Ls.java,<init>,org.apache.hadoop.fs.shell.Ls:<init>(),120,120,"/**
 * Private constructor to prevent direct instantiation of Ls.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,<init>,org.apache.hadoop.fs.shell.Count:<init>(),110,110,"/**
* Default constructor for the Count class.
*/
",Constructor,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/RSErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:createEncoder(),38,41,"/**
 * Creates a new ErasureEncoder instance using coder options.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/HHXORErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:createEncoder(),38,41,"/**
 * Creates a new ErasureEncoder instance using coder options.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/DummyErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:createEncoder(),36,39,"/**
 * Creates an ErasureEncoder instance using coder options.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/XORErasureCodec.java,createEncoder,org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:createEncoder(),39,42,"/**
 * Creates a new ErasureEncoder instance using coder options.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/DummyErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.DummyErasureCodec:createDecoder(),41,44,"/**
 * Creates an ErasureDecoder instance.
 * Returns a DummyErasureDecoder with coder options.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/XORErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.XORErasureCodec:createDecoder(),44,47,"/**
 * Creates a new ErasureDecoder instance using coder options.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/RSErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.RSErasureCodec:createDecoder(),43,46,"/**
 * Creates a new ErasureDecoder instance using coder options.
 * @return An ErasureDecoder object.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/codec/HHXORErasureCodec.java,createDecoder,org.apache.hadoop.io.erasurecode.codec.HHXORErasureCodec:createDecoder(),43,46,"/**
 * Creates and returns a new ErasureDecoder instance.
 * Uses coder options to configure the decoder.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,checkAndUpdateMaps,org.apache.hadoop.security.ShellBasedIdMapping:checkAndUpdateMaps(),166,176,"/**
 * Checks if expired and updates maps if so, handling IO errors.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,createConnection,org.apache.hadoop.ha.ActiveStandbyElector:createConnection(),894,909,"/**
 * Creates a ZooKeeper connection, closing existing one if present.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,forceReloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:forceReloginFromKeytab(),1262,1266,"/**
 * Forces a relogin using a keytab file.
 * Delegates to reloginFromKeytab with specific flags.
 */
","* Force re-Login a user in from a keytab file irrespective of the last login
   * time. Loads a user identity from a keytab file and logs them in. They
   * become the currently logged-in user. This method assumes that
   * {@link #loginUserFromKeytab(String, String)} had happened already. The
   * Subject field of this UserGroupInformation object is updated to have the
   * new credentials.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException on a failure",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(boolean),1268,1270,"/**
 * Relogins using a keytab file.
 * @param checkTGT Whether to check the Ticket Granting Ticket.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,forceReloginFromTicketCache,org.apache.hadoop.security.UserGroupInformation:forceReloginFromTicketCache(),1302,1306,"/**
 * Forces a relogin from the ticket cache.
 * Delegates to reloginFromTicketCache(true).
 */
","* Force re-Login a user in from the ticket cache irrespective of the last
   * login time. This method assumes that login had happened already. The
   * Subject field of this UserGroupInformation object is updated to have the
   * new credentials.
   *
   * @throws IOException
   *           raised on errors performing I/O.
   * @throws KerberosAuthException
   *           on a failure",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromTicketCache,org.apache.hadoop.security.UserGroupInformation:reloginFromTicketCache(),1316,1320,"/**
 * Relogins the user from the ticket cache.
 * Overloads reloginFromTicketCache(boolean) with default false.
 */
","* Re-Login a user in from the ticket cache.  This
   * method assumes that login had happened already.
   * The Subject field of this UserGroupInformation object is updated to have
   * the new credentials.
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException on a failure",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,createEncryptor,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:createEncryptor(),58,62,"/**
 * Creates and returns an OpensslCtrCipher encryptor.
 * Uses the cipher suite configured in this class.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslAesCtrCryptoCodec.java,createDecryptor,org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec:createDecryptor(),64,68,"/**
 * Creates a Decryptor instance using the configured cipher suite.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,fillQueueForKey,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$EncryptedQueueRefiller:fillQueueForKey(java.lang.String,java.util.Queue,int)",145,161,"/**
 * Generates and adds encrypted key versions to the provided queue.
 * @param keyName Key name.
 * @param keyQueue Queue to add encrypted key versions to.
 * @param numEKVs Number of encrypted key versions to generate.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,generateEncryptedKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:generateEncryptedKey(java.lang.String),788,799,"/**
 * Generates an encrypted key version.
 * @param encryptionKeyName Key name to retrieve.
 * @return EncryptedKeyVersion object.
 * @throws IOException, GeneralSecurityException
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,drain,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:drain(java.lang.String),311,316,"/**
 * Drains data for the given key name across all providers.
 * @param keyName The key name to drain data for.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,queueCall,org.apache.hadoop.ipc.Server:queueCall(org.apache.hadoop.ipc.Server$Call),3097,3104,"/**
 * Queues a call, wrapping RpcServerException as IOException.
 * @param call The call object to queue.
 * @throws IOException, InterruptedException
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddrForHost,"org.apache.hadoop.net.NetUtils:createSocketAddrForHost(java.lang.String,int)",308,325,"/**
 * Creates an InetSocketAddress for the given host and port.
 * Uses static resolution if available, otherwise uses the host.
 */","* Create a socket address with the given host and port.  The hostname
   * might be replaced with another host that was set via
   * {@link #addStaticResolution(String, String)}.  The value of
   * hadoop.security.token.service.use_ip will determine whether the
   * standard java host resolver is used, or if the fully qualified resolver
   * is used.
   * @param host the hostname or IP use to instantiate the object
   * @param port the port number
   * @return InetSocketAddress",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,canonicalizeHost,org.apache.hadoop.net.NetUtils:canonicalizeHost(java.lang.String),362,377,"/**
 * Returns the canonical host name, caching the result.
 * @param host The host name to canonicalize.
 * @return The canonical host name or the original if unknown.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getLocalInetAddress,org.apache.hadoop.net.NetUtils:getLocalInetAddress(java.lang.String),798,811,"/**
 * Gets the InetAddress for the given host, returning null if not local.
 * @param host hostname to resolve
 * @return InetAddress object or null if not found/local
 */
","* Checks if {@code host} is a local host name and return {@link InetAddress}
   * corresponding to that address.
   * 
   * @param host the specified host
   * @return a valid local {@link InetAddress} or null
   * @throws SocketException if an I/O error occurs",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,sendSaslMessage,"org.apache.hadoop.security.SaslRpcClient:sendSaslMessage(java.io.OutputStream,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto)",457,469,"/**
 * Sends a SASL message to the output stream.
 * @param out Output stream to send the message to.
 * @param message SASL message to send.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,writeConnectionContext,"org.apache.hadoop.ipc.Client$Connection:writeConnectionContext(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.security.SaslRpcServer$AuthMethod)",1007,1028,"/**
 * Writes connection context and header to the stream.
 * Sends connection context and initial RPC request.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,sendRpcRequest,org.apache.hadoop.ipc.Client$Connection:sendRpcRequest(org.apache.hadoop.ipc.Client$Call),1159,1191,"/**
 * Sends an RPC request to the queue, serializing and preparing it.
 * @param call The RPC call to send.
 * @throws InterruptedException, IOException if an error occurs.
 */","Initiates a rpc call by sending the rpc request to the remote server.
     * Note: this is not called from the current thread, but by another
     * thread, so that if the current thread is interrupted that the socket
     * state isn't corrupted with a partially written message.
     * @param call - the rpc request",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getPriorityLevel,org.apache.hadoop.ipc.Server:getPriorityLevel(org.apache.hadoop.security.UserGroupInformation),727,730,"/**
 * Gets the priority level for a user group.
 * @param ugi UserGroupInformation object.
 * @return Priority level as an integer.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,receiveRpcResponse,org.apache.hadoop.ipc.Client$Connection:receiveRpcResponse(),1196,1249,"/**
 * Processes an RPC response, handling success or failure states.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processRpcRequest,"org.apache.hadoop.ipc.Server$Connection:processRpcRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)",2869,2970,"/**
 * Processes an RPC request, including tracing and queueing.
 * @param header RPC request header
 * @param buffer Buffer containing the RPC request data
 */","* Process an RPC Request 
     *   - the connection headers and context must have been already read.
     *   - Based on the rpcKind, decode the rpcRequest.
     *   - A successfully decoded RpcCall will be deposited in RPC-Q and
     *     its response will be sent later when the request is processed.
     * @param header - RPC request header
     * @param buffer - stream to request payload
     * @throws RpcServerException - generally due to fatal rpc layer issues
     *   such as invalid header or deserialization error.  The call queue
     *   may also throw a fatal or non-fatal exception on overflow.
     * @throws IOException - fatal internal error that should/could not
     *   be sent to client.
     * @throws InterruptedException",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,setupResponse,"org.apache.hadoop.ipc.Server:setupResponse(org.apache.hadoop.ipc.Server$RpcCall,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcStatusProto,org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcResponseHeaderProto$RpcErrorCodeProto,org.apache.hadoop.io.Writable,java.lang.String,java.lang.String)",3507,3547,"/**
 * Sets up the RPC response header and data based on status.
 * @param call RPC call context
 * @param status RPC status
 */
","* Setup response for the IPC Call.
   * 
   * @param call {@link Call} to which we are setting up the response
   * @param status of the IPC call
   * @param rv return value for the IPC Call, if the call was successful
   * @param errorClass error class, if the the call failed
   * @param error error message, if the call failed
   * @throws IOException",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,wrapWithSasl,org.apache.hadoop.ipc.Server:wrapWithSasl(org.apache.hadoop.ipc.Server$RpcCall),3641,3661,"/**
 * Wraps RPC call response with SASL authentication if enabled.
 * @param call The RPC call to wrap.
 * @throws IOException if SASL wrapping fails.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,initializeAuthContext,org.apache.hadoop.ipc.Server$Connection:initializeAuthContext(int),2567,2593,"/**
 * Initializes AuthProtocol based on authType, throws IOException if invalid.
 * @param authType Authentication type integer value.
 * @return AuthProtocol instance.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CacheableIPList.java,reset,org.apache.hadoop.util.CacheableIPList:reset(),42,45,"/**
 * Reloads the IP list and updates the cache expiry time.
 */
",* Reloads the ip list,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,main,org.apache.hadoop.util.SysInfoLinux:main(java.lang.String[]),705,734,"/**
 * Prints system information to the console using SysInfoLinux plugin.
 */
","* Test the {@link SysInfoLinux}.
   *
   * @param args - arguments to this calculator test",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,open,"org.apache.hadoop.fs.http.HttpsFileSystem:open(org.apache.hadoop.fs.Path,int)",61,67,"/**
 * Opens an input stream for a path.
 * @param path Path to open.
 * @param bufferSize Buffer size for the stream.
 * @return FSDataInputStream for reading data from the path.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,open,"org.apache.hadoop.fs.http.HttpFileSystem:open(org.apache.hadoop.fs.Path,int)",61,67,"/**
 * Opens an input stream for the file at the given path.
 * @param path Path to the file
 * @param bufferSize Buffer size for the stream
 * @return FSDataInputStream for reading the file
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,remove,org.apache.hadoop.util.LightWeightCache:remove(java.lang.Object),223,232,"/**
 * Removes the specified key's entry and updates the queue.
 * @param key the key of the entry to remove
 * @return the removed value, or null if the key was not present
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LightWeightCache.java,put,org.apache.hadoop.util.LightWeightCache:put(java.lang.Object),201,221,"/**
 * Adds an entry to the cache.
 * @param entry The entry to add.
 * @return The previous entry, or null if none.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,"org.apache.hadoop.fs.ContentSummary:toString(boolean,boolean)",394,396,"/**
 * Calls the overloaded toString method with default values.
 * @param qOption boolean option
 * @param hOption boolean option
 */
","Return the string representation of the object in the output format.
   * For description of the options,
   * @see #toString(boolean, boolean, boolean, boolean, List)
   * 
   * @param qOption a flag indicating if quota needs to be printed or not
   * @param hOption a flag indicating if human readable output if to be used
   * @return the string representation of the object",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Count.java,processPath,org.apache.hadoop.fs.shell.Count:processPath(org.apache.hadoop.fs.shell.PathData),195,217,"/**
 * Processes a PathData object, generating and printing output.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,toString,org.apache.hadoop.fs.QuotaUsage:toString(boolean),307,309,"/**
 * Calls the overloaded toString method with default values.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSInputChecker.java,read,"org.apache.hadoop.fs.FSInputChecker:read(byte[],int,int)",191,209,"/**
 * Reads up to {@code len} bytes from this input stream into the provided byte array.
 */","* Read checksum verified bytes from this byte-input stream into 
   * the specified byte array, starting at the given offset.
   *
   * <p> This method implements the general contract of the corresponding
   * <code>{@link InputStream#read(byte[], int, int) read}</code> method of
   * the <code>{@link InputStream}</code> class.  As an additional
   * convenience, it attempts to read as many bytes as possible by repeatedly
   * invoking the <code>read</code> method of the underlying stream.  This
   * iterated <code>read</code> continues until one of the following
   * conditions becomes true: <ul>
   *
   *   <li> The specified number of bytes have been read,
   *
   *   <li> The <code>read</code> method of the underlying stream returns
   *   <code>-1</code>, indicating end-of-file.
   *
   * </ul> If the first <code>read</code> on the underlying stream returns
   * <code>-1</code> to indicate end-of-file then this method returns
   * <code>-1</code>.  Otherwise this method returns the number of bytes
   * actually read.
   *
   * @param      b     destination buffer.
   * @param      off   offset at which to start storing bytes.
   * @param      len   maximum number of bytes to read.
   * @return     the number of bytes read, or <code>-1</code> if the end of
   *             the stream has been reached.
   * @exception  IOException  if an I/O error occurs.
   *             ChecksumException if any checksum error occurs",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,createExpression,"org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.Class,org.apache.hadoop.conf.Configuration)",127,134,"/**
 * Creates an instance of the given Expression class.
 * @param expressionClass Expression class to instantiate.
 * @param conf Configuration object for the new instance.
 * @return New Expression instance or null if class is null.
 */
","* Creates an instance of the requested {@link Expression} class.
   *
   * @param expressionClass
   *          {@link Expression} class to be instantiated
   * @param conf
   *          the Hadoop configuration
   * @return a new instance of the requested {@link Expression} class",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,getInstance,"org.apache.hadoop.fs.shell.CommandFactory:getInstance(java.lang.String,org.apache.hadoop.conf.Configuration)",118,131,"/**
 * Retrieves a Command instance by name, creating it if needed.
 * @param cmdName Command name.
 * @param conf Configuration object for the command.
 * @return Command instance.
 */
","* Get an instance of the requested command
   * @param cmdName name of the command to lookup
   * @param conf the hadoop configuration
   * @return the {@link Command} or null if the command is unknown",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,newKey,org.apache.hadoop.io.WritableComparator:newKey(),166,168,"/**
 * Creates a new key object using the configured key class.
 */","* Construct a new {@link WritableComparable} instance.
   * @return WritableComparable.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SortedMapWritable.java,readFields,org.apache.hadoop.io.SortedMapWritable:readFields(java.io.DataInput),156,180,"/**
 * Reads fields from DataInput, populating the instance map.
 * Reads key-value pairs from the input stream.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/GenericWritable.java,readFields,org.apache.hadoop.io.GenericWritable:readFields(java.io.DataInput),124,135,"/**
 * Reads fields from the input stream into the Writable instance.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,add,"org.apache.hadoop.io.serializer.SerializationFactory:add(org.apache.hadoop.conf.Configuration,java.lang.String)",69,79,"/**
 * Adds a Serialization class to the list using the configuration.
 * @param conf Configuration object.
 * @param serializationName Name of the Serialization class.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/WritableSerialization.java,deserialize,org.apache.hadoop.io.serializer.WritableSerialization$WritableDeserializer:deserialize(org.apache.hadoop.io.Writable),62,73,"/**
 * Deserializes a Writable object from the input stream.
 * @param w The Writable object to deserialize.
 * @return The deserialized Writable object.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapWritable.java,readFields,org.apache.hadoop.io.MapWritable:readFields(java.io.DataInput),165,191,"/**
 * Reads fields from DataInput, populating the internal map.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableFactories.java,newInstance,"org.apache.hadoop.io.WritableFactories:newInstance(java.lang.Class,org.apache.hadoop.conf.Configuration)",63,74,"/**
 * Creates a new instance of a Writable class.
 * @param c Writable class to instantiate
 * @param conf Configuration object, may be used by the instance
 * @return New Writable instance
 */
","* Create a new instance of a class with a defined factory.
   *
   * @param c input c.
   * @param conf input configuration.
   * @return a new instance of a class with a defined factory.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getSocketFactoryFromProperty,"org.apache.hadoop.net.NetUtils:getSocketFactoryFromProperty(org.apache.hadoop.conf.Configuration,java.lang.String)",142,152,"/**
 * Creates a SocketFactory instance from a class name.
 * @param conf Configuration object.
 * @param propValue Class name of the SocketFactory.
 * @return SocketFactory instance.
 */
","* Get the socket factory corresponding to the given proxy URI. If the
   * given proxy URI corresponds to an absence of configuration parameter,
   * returns null. If the URI is malformed raises an exception.
   *
   * @param conf configuration.
   * @param propValue the property which is the class name of the
   *        SocketFactory to instantiate; assumed non null and non empty.
   * @return a socket factory as defined in the property value.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,decodeIdentifier,org.apache.hadoop.security.token.Token:decodeIdentifier(),164,176,"/**
 * Decodes an identifier into a TokenIdentifier object.
 * @return TokenIdentifier object or null if decoding fails.
 */
","* Get the token identifier object, or null if it could not be constructed
   * (because the class could not be loaded, for example).
   * @return the token identifier, or null if there was no class found for it
   * @throws IOException failure to unmarshall the data
   * @throws RuntimeException if the token class could not be instantiated.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskValidatorFactory.java,getInstance,org.apache.hadoop.util.DiskValidatorFactory:getInstance(java.lang.Class),45,62,"/**
 * Retrieves a DiskValidator instance, using singleton pattern.
 * @param clazz DiskValidator class to instantiate.
 * @return DiskValidator instance.
 */
","* Returns a {@link DiskValidator} instance corresponding to the passed clazz.
   * @param clazz a class extends {@link DiskValidator}
   * @return disk validator.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,createFenceMethod,"org.apache.hadoop.ha.NodeFencer:createFenceMethod(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",167,195,"/**
 * Creates a FenceMethodWithArg instance from a class name and arg.
 * @param conf Configuration object
 * @param clazzName Class name of the fencing method
 * @param arg Argument to pass to the method
 * @return FenceMethodWithArg instance
 * @throws BadFencingConfigurationException if config is invalid
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:<init>(),144,146,"/**
 * Constructs a DynamicWrappedIO with the default wrapped class name.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedStatistics.java,<init>,org.apache.hadoop.io.wrappedio.impl.DynamicWrappedStatistics:<init>(),224,226,"/**
 * Default constructor, uses WRAPPED_STATISTICS_CLASSNAME.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refresh,"org.apache.hadoop.util.HostsFileReader:refresh(java.lang.String,java.lang.String)",190,193,"/**
 * Refreshes data using specified includes and excludes files.
 * @param includesFile Path to the includes file.
 * @param excludesFile Path to the excludes file.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,lazyRefresh,"org.apache.hadoop.util.HostsFileReader:lazyRefresh(java.lang.String,java.lang.String)",195,198,"/**
 * Refreshes data using includes/excludes files.
 * @param includesFile Path to includes file.
 * @param excludesFile Path to excludes file.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,<init>,"org.apache.hadoop.util.bloom.DynamicBloomFilter:<init>(int,int,int,int)",126,134,"/**
 * Initializes a DynamicBloomFilter with specified parameters.
 * @param vectorSize Vector size, nbHash hash functions, hashType hash function type, nr initial rows
 */
","* Constructor.
   * <p>
   * Builds an empty Dynamic Bloom filter.
   * @param vectorSize The number of bits in the vector.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).
   * @param nr The threshold for the maximum number of keys to record in a
   * dynamic Bloom filter row.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,addRow,org.apache.hadoop.util.bloom.DynamicBloomFilter:addRow(),275,285,"/**
 * Adds a new row to the Bloom filter matrix.
 */",* Adds a new row to <i>this</i> dynamic Bloom filter.,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,<init>,"org.apache.hadoop.util.bloom.RetouchedBloomFilter:<init>(int,int,int)",111,116,"/**
 * Constructs a RetouchedBloomFilter with specified size, hashes, and hash type.
 */
","* Constructor
   * @param vectorSize The vector size of <i>this</i> filter.
   * @param nbHash The number of hash function to consider.
   * @param hashType type of the hashing function (see
   * {@link org.apache.hadoop.util.hash.Hash}).",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,readFields,org.apache.hadoop.util.bloom.DynamicBloomFilter:readFields(java.io.DataInput),259,270,"/**
 * Reads fields from DataInput, populating the BloomFilter matrix.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/RetouchedBloomFilter.java,readFields,org.apache.hadoop.util.bloom.RetouchedBloomFilter:readFields(java.io.DataInput),429,454,"/**
 * Reads fields from DataInput, populating fpVector, keyVector, and ratio.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printInstanceHelp,"org.apache.hadoop.fs.FsShell:printInstanceHelp(java.io.PrintStream,org.apache.hadoop.fs.shell.Command)",253,288,"/**
 * Prints help information for a command to the output stream.
 * @param out PrintStream to write help to.
 * @param instance Command instance to display help for.
 */
",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,loadProps,"org.apache.hadoop.conf.Configuration:loadProps(java.util.Properties,int,boolean)",2961,2981,"/**
 * Loads properties from a Properties object.
 * @param props Properties to load, startIdx, fullReload flag.
 */","* Loads the resource at a given index into the properties.
   * @param props the object containing the loaded properties.
   * @param startIdx the index where the new resource has been added.
   * @param fullReload flag whether we do complete reload of the conf instead
   *                   of just loading the new resource.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,becomeActive,org.apache.hadoop.ha.ActiveStandbyElector:becomeActive(),936,956,"/**
 * Attempts to become active. Returns true on success, false on failure.
 */",,,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,quitElection,org.apache.hadoop.ha.ActiveStandbyElector:quitElection(boolean),443,452,"/**
 * Quits the election process.
 * @param needFence Whether a fence is needed.
 * @return void
 */
","* Any service instance can drop out of the election by calling quitElection. 
   * <br>
   * This will lose any leader status, if held, and stop monitoring of the lock
   * node. <br>
   * If the instance wants to participate in election again, then it needs to
   * call joinElection(). <br>
   * This allows service instances to take themselves out of rotation for known
   * impending unavailable states (e.g. long GC pause or software upgrade).
   * 
   * @param needFence true if the underlying daemon may need to be fenced
   * if a failover occurs due to dropping out of the election.",,,True,6
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/PositionedReadable.java,readVectored,"org.apache.hadoop.fs.PositionedReadable:readVectored(java.util.List,java.util.function.IntFunction)",132,135,"/**
 * Reads data using vectored reads based on provided ranges.
 * @param ranges List of file ranges to read.
 * @param allocate Allocates ByteBuffer for reading.
 */
","* Read fully a list of file ranges asynchronously from this file.
   * The default iterates through the ranges to read each synchronously, but
   * the intent is that FSDataInputStream subclasses can make more efficient
   * readers.
   * As a result of the call, each range will have FileRange.setData(CompletableFuture)
   * called with a future that when complete will have a ByteBuffer with the
   * data from the file's range.
   * <p>
   *   The position returned by getPos() after readVectored() is undefined.
   * </p>
   * <p>
   *   If a file is changed while the readVectored() operation is in progress, the output is
   *   undefined. Some ranges may have old data, some may have new and some may have both.
   * </p>
   * <p>
   *   While a readVectored() operation is in progress, normal read api calls may block.
   * </p>
   * @param ranges the byte ranges to read
   * @param allocate the function to allocate ByteBuffer
   * @throws IOException any IOE.
   * @throws IllegalArgumentException if the any of ranges are invalid, or they overlap.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,close,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:close(),228,248,"/**
 * Closes the cache, releasing resources and finalizing operations.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,read,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:read(org.apache.hadoop.fs.impl.prefetch.BufferData),308,317,"/**
 * Reads a block of data from the buffer.
 * @param data BufferData object containing data to read.
 * @throws IOException if an I/O error occurs during reading.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,prefetch,"org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:prefetch(org.apache.hadoop.fs.impl.prefetch.BufferData,java.time.Instant)",319,329,"/**
 * Prefetches data into the buffer.
 * @param data BufferData object to prefetch.
 * @param taskQueuedStartTime Start time of the task.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,"org.apache.hadoop.fs.BlockLocation:<init>(java.lang.String[],java.lang.String[],long,long)",109,112,"/**
 * Constructs a BlockLocation with default replication factor.
 * @param names block names, hosts, offset, and length
 */
","* Constructor with host, name, offset and length.
   * @param names names array.
   * @param hosts host array.
   * @param offset offset.
   * @param length length.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationStatisticSummary.java,<init>,"org.apache.hadoop.fs.statistics.DurationStatisticSummary:<init>(java.lang.String,boolean,long,long,long,org.apache.hadoop.fs.statistics.MeanStatistic)",71,83,"/**
 * Creates a DurationStatisticSummary.
 * @param key Summary key.
 * @param success Success status.
 * @param count Number of samples.
 * @param max Maximum duration.
 * @param min Minimum duration.
 * @param mean Statistic mean (cloned if not null).
 */
","* Constructor.
   * @param key Statistic key.
   * @param success Are these success or failure statistics.
   * @param count Count of operation invocations.
   * @param max Max duration; -1 if unknown.
   * @param min Min duration; -1 if unknown.
   * @param mean Mean duration -may be null. (will be cloned)",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,aggregate,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:aggregate(org.apache.hadoop.fs.statistics.IOStatistics),178,199,"/**
 * Aggregates statistics from a source, merging counters, gauges, etc.
 * @param source The source containing statistics to aggregate.
 * @return True if aggregation was successful.
 */
","* Aggregate the current statistics with the
   * source reference passed in.
   *
   * The operation is synchronized.
   * @param source source; may be null
   * @return true if a merge took place.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSnapshot.java,<init>,org.apache.hadoop.fs.statistics.IOStatisticsSnapshot:<init>(org.apache.hadoop.fs.statistics.IOStatistics),123,129,"/**
 * Initializes the snapshot. Copies data from source or creates maps.
 */","* Construct, taking a snapshot of the source statistics data
   * if the source is non-null.
   * If the source is null, the empty maps are created
   * @param source statistics source. Nullable.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,foreach,"org.apache.hadoop.util.functional.RemoteIterators:foreach(org.apache.hadoop.fs.RemoteIterator,org.apache.hadoop.util.functional.ConsumerRaisingIOE)",273,288,"/**
 * Iterates through a remote iterator and applies a consumer.
 * @param source Iterator to iterate.
 * @param consumer Consumer to apply to each element.
 * @return Number of elements processed.
 */
","* Apply an operation to all values of a RemoteIterator.
   *
   * If the iterator is an IOStatisticsSource returning a non-null
   * set of statistics, <i>and</i> this classes log is set to DEBUG,
   * then the statistics of the operation are evaluated and logged at
   * debug.
   * <p>
   * The number of entries processed is returned, as it is useful to
   * know this, especially during tests or when reporting values
   * to users.
   * </p>
   * This does not close the iterator afterwards.
   * @param source iterator source
   * @param consumer consumer of the values.
   * @return the number of elements processed
   * @param <T> type of source
   * @throws IOException if the source RemoteIterator or the consumer raise one.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackInvocation,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackInvocation(org.apache.hadoop.util.functional.InvocationRaisingIOE,java.lang.String,org.apache.hadoop.metrics2.lib.MutableRate)",1014,1024,"/**
 * Tracks invocation duration, updates a metric, and handles exceptions.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,readFully,"org.apache.hadoop.crypto.CryptoInputStream:readFully(long,byte[])",517,520,"/**
 * Reads data from the channel into the provided byte buffer.
 * @param position Starting position in the channel.
 * @param buffer Byte array to read into.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataInputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,int,byte[],byte[])",28,31,"/**
 * Creates a CryptoFSDataInputStream wrapping an FSDataInputStream.
 * @param in Input stream, codec, buffer size, key, and IV.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:performCoding(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",67,99,"/**
 * Encodes data using the Reed-Solomon encoder.
 * @param inputs Input data units.
 * @param outputs Output parity units.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,writeObject,"org.apache.hadoop.io.ObjectWritable:writeObject(java.io.DataOutput,java.lang.Object,java.lang.Class,org.apache.hadoop.conf.Configuration)",142,146,"/**
 * Writes an object to an output stream.
 * @param out Output stream to write to.
 * @param instance Object to serialize.
 * @param declaredClass Class of the object.
 * @param conf Hadoop configuration.
 */
","* Write a {@link Writable}, {@link String}, primitive type, or an array of
   * the preceding.
   *
   * @param out DataOutput.
   * @param instance instance.
   * @param conf Configuration.
   * @param declaredClass declaredClass.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,write,org.apache.hadoop.ipc.WritableRpcEngine$Invocation:write(java.io.DataOutput),166,179,"/**
 * Writes method data to the DataOutput stream.
 * Writes rpcVersion, method details, client info, and parameters.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,requestPrefetch,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:requestPrefetch(int),256,289,"/**
 * Requests a prefetch for the specified block number.
 * @param blockNumber The block number to prefetch.
 */
","* Requests optional prefetching of the given block.
   * The block is prefetched only if we can acquire a free buffer.
   *
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,getData,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getData(int),631,633,"/**
 * Retrieves data for a given block from the buffer pool.
 * @param blockNumber Block number to retrieve.
 * @return BufferData object or null if acquisition fails.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/BufferPool.java,acquire,org.apache.hadoop.fs.impl.prefetch.BufferPool:acquire(int),125,150,"/**
 * Acquires buffer data for a given block number, retrying with delays.
 * @param blockNumber The block number to acquire.
 * @return BufferData object or throws IllegalStateException on failure.
 */
","* Acquires a {@code ByteBuffer}; blocking if necessary until one becomes available.
   * @param blockNumber the id of the block to acquire.
   * @return the acquired block's {@code BufferData}.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPathArgument,org.apache.hadoop.fs.shell.Command:processPathArgument(org.apache.hadoop.fs.shell.PathData),315,320,"/**
 * Processes a PathData item, starting path processing recursively.
 * @param item The PathData to process.
 * @throws IOException if an I/O error occurs.
 */
","*  This is the last chance to modify an argument before going into the
   *  (possibly) recursive {@link #processPaths(PathData, PathData...)}
   *  {@literal ->} {@link #processPath(PathData)} loop.  Ex.  ls and du use
   *  this to expand out directories.
   *  @param item a {@link PathData} representing a path which exists
   *  @throws IOException if anything goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processPaths,"org.apache.hadoop.fs.shell.Command:processPaths(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.RemoteIterator)",361,380,"/**
 * Processes path data, either individually or in groups.
 * @param parent Parent PathData
 * @param itemsIterator Iterator for PathData items
 * @throws IOException if an I/O error occurs
 */
","* Iterates over the given expanded paths and invokes
   * {@link #processPath(PathData)} on each element. If ""recursive"" is true,
   * will do a post-visit DFS on directories.
   * @param parent if called via a recurse, will be the parent dir, else null
   * @param itemsIterator a iterator of {@link PathData} objects to process
   * @throws IOException if anything goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,resolvePartialGroupNames,"org.apache.hadoop.security.ShellBasedUnixGroupsMapping:resolvePartialGroupNames(java.lang.String,java.lang.String,java.lang.String)",280,319,"/**
 * Resolves partial group names for a user.
 * @param userName User's name.
 * @return Set of resolvable group names.
 * @throws PartialGroupNameException if resolution fails.
 */
","* Attempt to partially resolve group names.
   *
   * @param userName the user's name
   * @param errMessage error message from the shell command
   * @param groupNames the incomplete list of group names
   * @return a set of resolved group names
   * @throws PartialGroupNameException if the resolution fails or times out",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,"org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[],java.io.File)",1218,1220,"/**
 * Constructs a ShellCommandExecutor with a directory.
 * @param execString Command to execute.
 * @param dir Working directory for the command.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execCommand,org.apache.hadoop.util.Shell:execCommand(java.lang.String[]),1358,1360,"/**
 * Executes a command and returns its output as a string.
 * @param cmd Command to execute, as an array of strings.
 * @throws IOException If an I/O error occurs during execution.
 */
","* Static method to execute a shell command.
   * Covers most of the simple cases without requiring the user to implement
   * the <code>Shell</code> interface.
   * @param cmd shell command to execute.
   * @return the output of the executed command.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,execCommand,"org.apache.hadoop.util.Shell:execCommand(java.util.Map,java.lang.String[])",1390,1393,"/**
 * Executes a command with given environment variables.
 * @param env Environment variables for the command.
 * @param cmd Command to execute.
 * @return Command output as a String.
 */
","* Static method to execute a shell command.
   * Covers most of the simple cases without requiring the user to implement
   * the <code>Shell</code> interface.
   * @param env the map of environment key=value
   * @param cmd shell command to execute.
   * @return the output of the executed command.
   * @throws IOException on any problem.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,addAll,org.apache.hadoop.security.Credentials:addAll(org.apache.hadoop.security.Credentials),450,452,"/**
 * Adds all credentials from the provided Credentials object.
 * Delegates to the overloaded method with 'replace' set to true.
 */
","* Copy all of the credentials from one credential object into another.
   * Existing secrets and tokens are overwritten.
   * @param other the credentials to copy",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,mergeAll,org.apache.hadoop.security.Credentials:mergeAll(org.apache.hadoop.security.Credentials),459,461,"/**
 * Merges credentials from another Credentials object.
 * Uses addAll with 'ignoreConflicts' set to false.
 */
","* Copy all of the credentials from one credential object into another.
   * Existing secrets and tokens are not overwritten.
   * @param other the credentials to copy",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DelegationTokenIssuer.java,addDelegationTokens,"org.apache.hadoop.security.token.DelegationTokenIssuer:addDelegationTokens(java.lang.String,org.apache.hadoop.security.Credentials)",79,87,"/**
 * Adds delegation tokens for a renewer using provided credentials.
 * @param renewer renewer's identifier
 * @param credentials credentials to use
 * @return Array of delegation tokens.
 */
","* Given a renewer, add delegation tokens for issuer and it's child issuers
   * to the <code>Credentials</code> object if it is not already present.
   *<p>
   * Note: This method is not intended to be overridden.  Issuers should
   * implement getCanonicalService and getDelegationToken to ensure
   * consistent token acquisition behavior.
   *
   * @param renewer the user allowed to renew the delegation tokens
   * @param credentials cache in which to add new delegation tokens
   * @return list of new delegation tokens
   * @throws IOException thrown if IOException if an IO error occurs.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addToken,org.apache.hadoop.security.UserGroupInformation:addToken(org.apache.hadoop.security.token.Token),1701,1703,"/**
 * Adds a token. Returns true if successful, false otherwise.
 */","* Add a token to this UGI
   * 
   * @param token Token to be added
   * @return true on successful add of new token",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.RawLocalFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),850,854,"/**
 * Sets the working directory to the provided absolute path.
 * @param newDir The new working directory path.
 */
",* Set the working directory to the given directory.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,exists,org.apache.hadoop.fs.RawLocalFileSystem:exists(org.apache.hadoop.fs.Path),768,771,"/**
 * Checks if a file exists.
 * @param f Path to the file.
 * @return True if the file exists, false otherwise.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getStatus,org.apache.hadoop.fs.RawLocalFileSystem:getStatus(org.apache.hadoop.fs.Path),866,874,"/**
 * Gets the status of a path, including total, used, and free space.
 * @param p Path to get status for, defaults to root if null.
 * @return FsStatus object representing the path's status.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setTimes,"org.apache.hadoop.fs.RawLocalFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",1129,1140,"/**
 * Sets modification and access times of a file.
 * @param p Path to the file. mtime, atime are millis.
 * @throws IOException if an I/O error occurs.
 */
","* Sets the {@link Path}'s last modified time and last access time to
   * the given valid times.
   *
   * @param mtime the modification time to set (only if no less than zero).
   * @param atime the access time to set (only if no less than zero).
   * @throws IOException if setting the times fails.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,pathToFile,org.apache.hadoop.fs.LocalFileSystem:pathToFile(org.apache.hadoop.fs.Path),79,81,"/**
 * Converts a Path object to a File object.
 * @param path The Path to convert.
 * @return A File representing the given Path.
 */
","* Convert a path to a File.
   * @param path the path.
   * @return file.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getUriPath,org.apache.hadoop.fs.viewfs.ViewFileSystem:getUriPath(org.apache.hadoop.fs.Path),264,267,"/**
 * Returns the URI path of a given Path object.
 * @param p The Path object to get the URI path from.
 */
","* Make the path Absolute and get the path-part of a pathname.
   * Checks that URI matches this file system
   * and that the path-part is a valid name.
   *
   * @param p path
   * @return path-part of the Path p",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,setWorkingDirectory,org.apache.hadoop.fs.viewfs.NflyFSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),854,859,"/**
* Sets the working directory for all nodes' file systems.
* @param newDir The new directory to set as working directory.
*/
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Stat.java,<init>,"org.apache.hadoop.fs.Stat:<init>(org.apache.hadoop.fs.Path,long,boolean,org.apache.hadoop.fs.FileSystem)",50,68,"/**
 * Initializes a Stat object with file path, block size, and flags.
 * @param path Path to the file
 * @param blockSize Block size
 * @param deref Dereference symbolic links
 * @param fs FileSystem object
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,makeQualified,org.apache.hadoop.fs.Path:makeQualified(org.apache.hadoop.fs.FileSystem),547,550,"/**
 * Makes a qualified Path, using the provided FileSystem.
 * @param fs The FileSystem to use for qualification.
 * @return A qualified Path object.
 */
","* Returns a qualified path object for the {@link FileSystem}'s working
   * directory.
   *  
   * @param fs the target FileSystem
   * @return a qualified path object for the FileSystem's working directory
   * @deprecated use {@link #makeQualified(URI, Path)}",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,makeQualified,org.apache.hadoop.fs.FileContext:makeQualified(org.apache.hadoop.fs.Path),629,631,"/**
 * Makes a path qualified with the default FS URI and working dir.
 * @param path The path to qualify.
 * @return The qualified Path object.
 */
","* Make the path fully qualified if it is isn't. 
   * A Fully-qualified path has scheme and authority specified and an absolute
   * path.
   * Use the default file system and working dir in this FileContext to qualify.
   * @param path the path.
   * @return qualified path",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,makeQualified,org.apache.hadoop.fs.AbstractFileSystem:makeQualified(org.apache.hadoop.fs.Path),438,441,"/**
 * Makes a path qualified with the current URI.
 * @param path The path to qualify.
 * @return Qualified path object.
 */
","* Make the path fully qualified to this file system
   * @param path the path.
   * @return the qualified path",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path),1926,1937,"/**
 * Lists the status of a file or directory.
 * @param f Path to the file or directory.
 * @return Array of FileStatus objects.
 */
","* List the statuses of the files/directories in the given path 
     * if the path is a directory.
     * 
     * @param f is the path
     *
     * @return an array that contains statuses of the files/directories 
     *         in the given path
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If <code>f</code> does not exist
     * @throws UnsupportedFileSystemException If file system for <code>f</code> is
     *           not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,fixRelativePart,org.apache.hadoop.fs.Globber:fixRelativePart(org.apache.hadoop.fs.Path),138,144,"/**
 * Fixes a relative path, using either FileSystem or FileContext.
 * @param path The path to fix.
 * @return The fixed path.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,delete,"org.apache.hadoop.fs.FileContext:delete(org.apache.hadoop.fs.Path,boolean)",842,853,"/**
 * Deletes a file or directory.
 * @param f Path to delete.
 * @param recursive If true, deletes directories recursively.
 * @return True if successful, false otherwise.
 */
","* Delete a file.
   * @param f the path to delete.
   * @param recursive if path is a directory and set to 
   * true, the directory is deleted else throws an exception. In
   * case of a file the recursive can be set to either true or false.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is invalid
   *
   * @return if delete success true, not false.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,open,org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path),873,883,"/**
 * Opens a data input stream for the given path.
 * @param f Path to open; resolves relative paths.
 * @throws Exceptions related to file access/existence.
 */
","* Opens an FSDataInputStream at the indicated Path using
   * default buffersize.
   * @param f the file name to open
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code>
   *         is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * @return input stream.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,open,"org.apache.hadoop.fs.FileContext:open(org.apache.hadoop.fs.Path,int)",904,915,"/**
 * Opens a data input stream for the given path with specified buffer size.
 * @param f Path to open
 * @param bufferSize Size of the buffer
 * @return FSDataInputStream object
 */
","* Opens an FSDataInputStream at the indicated Path.
   * 
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * @return output stream.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,truncate,"org.apache.hadoop.fs.FileContext:truncate(org.apache.hadoop.fs.Path,long)",947,958,"/**
 * Truncates the file at the given path to the specified length.
 * @param f Path to the file.
 * @param newLength New length of the file.
 * @return True if successful, false otherwise.
 */
","* Truncate the file in the indicated path to the indicated size.
   * <ul>
   * <li>Fails if path is a directory.
   * <li>Fails if path does not exist.
   * <li>Fails if path is not closed.
   * <li>Fails if new size is greater than current size.
   * </ul>
   * @param f The path to the file to be truncated
   * @param newLength The size the file is to be truncated to
   *
   * @return <code>true</code> if the file has been truncated to the desired
   * <code>newLength</code> and is immediately available to be reused for
   * write operations such as <code>append</code>, or
   * <code>false</code> if a background process of adjusting the length of
   * the last block has been started, and clients should wait for it to
   * complete before proceeding with further file updates.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   *
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setReplication,"org.apache.hadoop.fs.FileContext:setReplication(org.apache.hadoop.fs.Path,short)",978,989,"/**
 * Sets the replication factor for a file.
 * @param f Path to the file.
 * @param replication New replication factor.
 * @return True if successful.
 */
","* Set replication for an existing file.
   * 
   * @param f file name
   * @param replication new replication
   *
   * @return true if successful
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If file <code>f</code> does not exist
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setPermission,"org.apache.hadoop.fs.FileContext:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1079,1091,"/**
 * Sets the permission of a file or directory.
 * @param f The path to the file/directory.
 * @param permission The desired file system permission.
 */
","* Set permission of a path.
   * @param f the path.
   * @param permission - the new absolute permission (umask is not applied)
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code>
   *         is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setOwner,"org.apache.hadoop.fs.FileContext:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1117,1134,"/**
* Sets the owner (username, groupname) of a file.
* @param f The Path of the file.
* @param username Username of the new owner.
* @param groupname Groupname of the new owner.
*/
","* Set owner of a path (i.e. a file or a directory). The parameters username
   * and groupname cannot both be null.
   * 
   * @param f The path
   * @param username If it is null, the original username remains unchanged.
   * @param groupname If it is null, the original groupname remains unchanged.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws HadoopIllegalArgumentException If <code>username</code> or
   *           <code>groupname</code> is invalid.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setTimes,"org.apache.hadoop.fs.FileContext:setTimes(org.apache.hadoop.fs.Path,long,long)",1158,1170,"/**
 * Sets modification and access times of a file.
 * @param f Path to the file. mtime, atime are timestamps.
 * @throws IOException on I/O error.
 */
","* Set access time of a file.
   * @param f The path
   * @param mtime Set the modification time of this file.
   *        The number of milliseconds since epoch (Jan 1, 1970). 
   *        A value of -1 means that this call should not set modification time.
   * @param atime Set the access time of this file.
   *        The number of milliseconds since Jan 1, 1970. 
   *        A value of -1 means that this call should not set access time.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileChecksum,org.apache.hadoop.fs.FileContext:getFileChecksum(org.apache.hadoop.fs.Path),1191,1202,"/**
 * Retrieves the checksum for a file.
 * @param f Path to the file.
 * @return FileChecksum object.
 * @throws IOException on I/O error.
 */
","* Get the checksum of a file.
   *
   * @param f file path
   *
   * @return The file checksum.  The default return value is null,
   *  which indicates that no checksum algorithm is implemented
   *  in the corresponding FileSystem.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileStatus,org.apache.hadoop.fs.FileContext:getFileStatus(org.apache.hadoop.fs.Path),1248,1258,"/**
 * Gets the file status for the given path.
 * @param f Path to the file; resolves relative paths.
 * @throws IOException on failure.
 */
","* Return a file status object that represents the path.
   * @param f The path we want information from
   *
   * @return a FileStatus object
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,access,"org.apache.hadoop.fs.FileContext:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",1305,1318,"/**
 * Checks file system access for a given path and mode.
 * @param path The path to check.
 * @param mode The access mode to check.
 */
","* Checks if the user can access a path.  The mode specifies which access
   * checks to perform.  If the requested permissions are granted, then the
   * method returns normally.  If access is denied, then the method throws an
   * {@link AccessControlException}.
   * <p>
   * The default implementation of this method calls {@link #getFileStatus(Path)}
   * and checks the returned permissions against the requested permissions.
   * Note that the getFileStatus call will be subject to authorization checks.
   * Typically, this requires search (execute) permissions on each directory in
   * the path's prefix, but this is implementation-defined.  Any file system
   * that provides a richer authorization model (such as ACLs) may override the
   * default implementation so that it checks against that model instead.
   * <p>
   * In general, applications should avoid using this method, due to the risk of
   * time-of-check/time-of-use race conditions.  The permissions on a file may
   * change immediately after the access call returns.  Most applications should
   * prefer running specific file system actions as the desired user represented
   * by a {@link UserGroupInformation}.
   *
   * @param path Path to check
   * @param mode type of access to check
   * @throws AccessControlException if access is denied
   * @throws FileNotFoundException if the path does not exist
   * @throws UnsupportedFileSystemException if file system for <code>path</code>
   *   is not supported
   * @throws IOException see specific implementation
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileLinkStatus,org.apache.hadoop.fs.FileContext:getFileLinkStatus(org.apache.hadoop.fs.Path),1334,1350,"/**
 * Retrieves the FileStatus for a given path, resolving symlinks.
 * @param f Path to retrieve FileStatus for.
 * @return FileStatus object.
 */
","* Return a file status object that represents the path. If the path 
   * refers to a symlink then the FileStatus of the symlink is returned.
   * The behavior is equivalent to #getFileStatus() if the underlying
   * file system does not support symbolic links.
   * @param  f The path we want information from.
   * @return A FileStatus object
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getLinkTarget,org.apache.hadoop.fs.FileContext:getLinkTarget(org.apache.hadoop.fs.Path),1367,1378,"/**
 * Resolves a symbolic link target.
 * @param f Path to the symbolic link
 * @return Path to the link target
 */
","* Returns the target of the given symbolic link as it was specified
   * when the link was created.  Links in the path leading up to the
   * final path component are resolved transparently.
   *
   * @param f the path to return the target of
   * @return The un-interpreted target of the symbolic link.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If path <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If the given path does not refer to a symlink
   *           or an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileBlockLocations,"org.apache.hadoop.fs.FileContext:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",1437,1450,"/**
 * Retrieves block locations for a file within a specified range.
 * @param f file path
 * @param start start offset
 * @param len length of data
 * @return BlockLocation array
 */
","* Return blockLocation of the given file for the given offset and len.
   *  For a nonexistent file or regions, null will be returned.
   *
   * This call is most helpful with DFS, where it returns 
   * hostnames of machines that contain the given file.
   *
   * In HDFS, if file is three-replicated, the returned array contains
   * elements like:
   * <pre>
   * BlockLocation(offset: 0, length: BLOCK_SIZE,
   *   hosts: {""host1:9866"", ""host2:9866, host3:9866""})
   * BlockLocation(offset: BLOCK_SIZE, length: BLOCK_SIZE,
   *   hosts: {""host2:9866"", ""host3:9866, host4:9866""})
   * </pre>
   *
   * And if a file is erasure-coded, the returned BlockLocation are logical
   * block groups.
   *
   * Suppose we have a RS_3_2 coded file (3 data units and 2 parity units).
   * 1. If the file size is less than one stripe size, say 2 * CELL_SIZE, then
   * there will be one BlockLocation returned, with 0 offset, actual file size
   * and 4 hosts (2 data blocks and 2 parity blocks) hosting the actual blocks.
   * 3. If the file size is less than one group size but greater than one
   * stripe size, then there will be one BlockLocation returned, with 0 offset,
   * actual file size with 5 hosts (3 data blocks and 2 parity blocks) hosting
   * the actual blocks.
   * 4. If the file size is greater than one group size, 3 * BLOCK_SIZE + 123
   * for example, then the result will be like:
   * <pre>
   * BlockLocation(offset: 0, length: 3 * BLOCK_SIZE, hosts: {""host1:9866"",
   *   ""host2:9866"",""host3:9866"",""host4:9866"",""host5:9866""})
   * BlockLocation(offset: 3 * BLOCK_SIZE, length: 123, hosts: {""host1:9866"",
   *   ""host4:9866"", ""host5:9866""})
   * </pre>
   *
   * @param f - get blocklocations of this file
   * @param start position (byte offset)
   * @param len (in bytes)
   *
   * @return block locations for given file at specified offset of len
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is invalid",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFsStatus,org.apache.hadoop.fs.FileContext:getFsStatus(org.apache.hadoop.fs.Path),1476,1489,"/**
 * Gets the filesystem status for a given path.
 * @param f Path to get status for; uses default if null.
 * @return FsStatus object.
 */
","* Returns a status object describing the use and capacity of the
   * file system denoted by the Parh argument p.
   * If the file system has multiple partitions, the
   * use and capacity of the partition pointed to by the specified
   * path is reflected.
   * 
   * @param f Path for which status should be obtained. null means the
   * root partition of the default file system. 
   *
   * @return a FsStatus object
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createSymlink,"org.apache.hadoop.fs.FileContext:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1570,1588,"/**
 * Creates a symbolic link from {@code link} to {@code target}.
 * @param target The target Path.
 * @param link The link Path.
 * @param createParent Whether to create parent directories.
 */
","* Creates a symbolic link to an existing file. An exception is thrown if 
   * the symlink exits, the user does not have permission to create symlink,
   * or the underlying file system does not support symlinks.
   * 
   * Symlink permissions are ignored, access to a symlink is determined by
   * the permissions of the symlink target.
   * 
   * Symlinks in paths leading up to the final path component are resolved 
   * transparently. If the final path component refers to a symlink some 
   * functions operate on the symlink itself, these are:
   * - delete(f) and deleteOnExit(f) - Deletes the symlink.
   * - rename(src, dst) - If src refers to a symlink, the symlink is 
   *   renamed. If dst refers to a symlink, the symlink is over-written.
   * - getLinkTarget(f) - Returns the target of the symlink. 
   * - getFileLinkStatus(f) - Returns a FileStatus object describing
   *   the symlink.
   * Some functions, create() and mkdir(), expect the final path component
   * does not exist. If they are given a path that refers to a symlink that 
   * does exist they behave as if the path referred to an existing file or 
   * directory. All other functions fully resolve, ie follow, the symlink. 
   * These are: open, setReplication, setOwner, setTimes, setWorkingDirectory,
   * setPermission, getFileChecksum, setVerifyChecksum, getFileBlockLocations,
   * getFsStatus, getFileStatus, exists, and listStatus.
   * 
   * Symlink targets are stored as given to createSymlink, assuming the 
   * underlying file system is capable of storing a fully qualified URI.
   * Dangling symlinks are permitted. FileContext supports four types of 
   * symlink targets, and resolves them as follows
   * <pre>
   * Given a path referring to a symlink of form:
   * 
   *   {@literal <---}X{@literal --->}
   *   fs://host/A/B/link 
   *   {@literal <-----}Y{@literal ----->}
   * 
   * In this path X is the scheme and authority that identify the file system,
   * and Y is the path leading up to the final path component ""link"". If Y is
   * a symlink  itself then let Y' be the target of Y and X' be the scheme and
   * authority of Y'. Symlink targets may:
   * 
   * 1. Fully qualified URIs
   * 
   * fs://hostX/A/B/file  Resolved according to the target file system.
   * 
   * 2. Partially qualified URIs (eg scheme but no host)
   * 
   * fs:///A/B/file  Resolved according to the target file system. Eg resolving
   *                 a symlink to hdfs:///A results in an exception because
   *                 HDFS URIs must be fully qualified, while a symlink to 
   *                 file:///A will not since Hadoop's local file systems 
   *                 require partially qualified URIs.
   * 
   * 3. Relative paths
   * 
   * path  Resolves to [Y'][path]. Eg if Y resolves to hdfs://host/A and path 
   *       is ""../B/file"" then [Y'][path] is hdfs://host/B/file
   * 
   * 4. Absolute paths
   * 
   * path  Resolves to [X'][path]. Eg if Y resolves hdfs://host/A/B and path
   *       is ""/file"" then [X][path] is hdfs://host/file
   * </pre>
   * 
   * @param target the target of the symbolic link
   * @param link the path to be created that points to target
   * @param createParent if true then missing parent dirs are created if 
   *                     false then parent must exist
   *
   *
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If file <code>link</code> already exists
   * @throws FileNotFoundException If <code>target</code> does not exist
   * @throws ParentNotDirectoryException If parent of <code>link</code> is not a
   *           directory.
   * @throws UnsupportedFileSystemException If file system for 
   *           <code>target</code> or <code>link</code> is not supported
   * @throws IOException If an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,org.apache.hadoop.fs.FileContext:listStatus(org.apache.hadoop.fs.Path),1611,1623,"/**
 * Lists status of files/directories under the given path.
 * @param f path to list; resolves relative paths.
 * @return RemoteIterator of FileStatus objects.
 */
","* List the statuses of the files/directories in the given path if the path is
   * a directory.
   * 
   * @param f is the path
   *
   * @return an iterator that traverses statuses of the files/directories 
   *         in the given path
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listCorruptFileBlocks,org.apache.hadoop.fs.FileContext:listCorruptFileBlocks(org.apache.hadoop.fs.Path),1633,1644,"/**
 * Lists corrupt file blocks under the given path.
 * @param path Path to search for corrupt file blocks.
 * @return RemoteIterator of Path objects.
 */
","* List CorruptFile Blocks.
   *
   * @param path the path.
   * @return an iterator over the corrupt files under the given path
   * (may contain duplicates if a file has more than one corrupt block)
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listLocatedStatus,org.apache.hadoop.fs.FileContext:listLocatedStatus(org.apache.hadoop.fs.Path),1673,1686,"/**
 * Lists located status for a path.
 * @param f Path to list; resolves relative paths.
 * @return RemoteIterator of LocatedFileStatus objects.
 */
","* List the statuses of the files/directories in the given path if the path is
   * a directory. 
   * Return the file's status and block locations If the path is a file.
   * 
   * If a returned status is a file, it contains the file's block locations.
   *
   * @param f is the path
   *
   * @return an iterator that traverses statuses of the files/directories 
   *         in the given path
   * If any IO exception (for example the input directory gets deleted while
   * listing is being executed), next() or hasNext() of the returned iterator
   * may throw a RuntimeException with the io exception as the cause.
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,resolveAbstractFileSystems,org.apache.hadoop.fs.FileContext:resolveAbstractFileSystems(org.apache.hadoop.fs.Path),2372,2386,"/**
 * Resolves AbstractFileSystems associated with a given path.
 * @param f Path to resolve; returns Set of AbstractFileSystems.
 * @throws IOException if an I/O error occurs.
 */
","* Returns the list of AbstractFileSystems accessed in the path. The list may
   * contain more than one AbstractFileSystems objects in case of symlinks.
   * 
   * @param f
   *          Path which needs to be resolved
   * @return List of AbstractFileSystems accessed in the path
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,modifyAclEntries,"org.apache.hadoop.fs.FileContext:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",2456,2467,"/**
 * Modifies ACL entries for a given path using the provided list.
 * @param path Path to modify ACL entries for.
 * @param aclSpec List of ACL entries to apply.
 */
","* Modifies ACL entries of files and directories.  This method can add new ACL
   * entries or modify the permissions on existing ACL entries.  All existing
   * ACL entries that are not specified in this call are retained without
   * changes.  (Modifications are merged into the current ACL.)
   *
   * @param path Path to modify
   * @param aclSpec List{@literal <}AclEntry{@literal >} describing
   * modifications
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeAclEntries,"org.apache.hadoop.fs.FileContext:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",2478,2489,"/**
 * Removes ACL entries from a path.
 * @param path Path to modify.
 * @param aclSpec List of ACL entries to remove.
 * @throws IOException If an I/O error occurs.
 */
","* Removes ACL entries from files and directories.  Other ACL entries are
   * retained.
   *
   * @param path Path to modify
   * @param aclSpec List{@literal <}AclEntry{@literal >} describing entries
   * to remove
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeDefaultAcl,org.apache.hadoop.fs.FileContext:removeDefaultAcl(org.apache.hadoop.fs.Path),2497,2508,"/**
 * Removes the default ACL for the given path.
 * @param path The path for which to remove the default ACL.
 * @throws IOException if an I/O error occurs.
 */
","* Removes all default ACL entries from files and directories.
   *
   * @param path Path to modify
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeAcl,org.apache.hadoop.fs.FileContext:removeAcl(org.apache.hadoop.fs.Path),2518,2528,"/**
 * Removes the ACL for the given path.
 * @param path The path for which to remove the ACL.
 * @throws IOException if an I/O error occurs.
 */
","* Removes all but the base ACL entries of files and directories.  The entries
   * for user, group, and others are retained for compatibility with permission
   * bits.
   *
   * @param path Path to modify
   * @throws IOException if an ACL could not be removed",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setAcl,"org.apache.hadoop.fs.FileContext:setAcl(org.apache.hadoop.fs.Path,java.util.List)",2540,2551,"/**
 * Sets the ACL for the given path using the provided ACL entries.
 * @param path Path to set ACL on.
 * @param aclSpec List of ACL entries to apply.
 * @throws IOException if an I/O error occurs.
 */
","* Fully replaces ACL of files and directories, discarding all existing
   * entries.
   *
   * @param path Path to modify
   * @param aclSpec List{@literal <}AclEntry{@literal >} describing
   * modifications, must include entries for user, group, and others for
   * compatibility with permission bits.
   * @throws IOException if an ACL could not be modified",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getAclStatus,org.apache.hadoop.fs.FileContext:getAclStatus(org.apache.hadoop.fs.Path),2561,2570,"/**
 * Retrieves the ACL status for the given path.
 * @param path Path to retrieve ACL status for.
 * @throws IOException if an I/O error occurs.
 */
","* Gets the ACLs of files and directories.
   *
   * @param path Path to get
   * @return RemoteIterator{@literal <}AclStatus{@literal >} which returns
   *         each AclStatus
   * @throws IOException if an ACL could not be read",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setXAttr,"org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",2603,2614,"/**
 * Sets an extended attribute on a path.
 * @param path Path to set the attribute on.
 * @param name Attribute name.
 * @param value Attribute value.
 * @param flag Attribute flags.
 */
","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @param flag xattr set flag
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getXAttr,"org.apache.hadoop.fs.FileContext:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",2628,2637,"/**
 * Retrieves extended attribute (xattr) by name for a path.
 * @param path Path to the file.
 * @param name Name of the xattr to retrieve.
 * @return Byte array representing the xattr value.
 */
","* Get an xattr for a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attribute
   * @param name xattr name.
   * @return byte[] xattr value.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getXAttrs,org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path),2651,2660,"/**
 * Retrieves extended attributes for a given path.
 * @param path The path to retrieve extended attributes from.
 * @return A map of attribute names to byte array values.
 */
","* Get all of the xattrs for a file or directory.
   * Only those xattrs for which the logged-in user has permissions to view
   * are returned.
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attributes
   * @return Map{@literal <}String, byte[]{@literal >} describing the XAttrs
   * of the file or directory
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getXAttrs,"org.apache.hadoop.fs.FileContext:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",2675,2685,"/**
 * Retrieves extended attributes for a path.
 * @param path Path to retrieve attributes from.
 * @param names List of attribute names to fetch.
 * @return Map of attribute names to byte array values.
 */
","* Get all of the xattrs for a file or directory.
   * Only those xattrs for which the logged-in user has permissions to view
   * are returned.
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attributes
   * @param names XAttr names.
   * @return Map{@literal <}String, byte[]{@literal >} describing the XAttrs
   * of the file or directory
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,removeXAttr,"org.apache.hadoop.fs.FileContext:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",2698,2708,"/**
 * Removes an extended attribute from a path.
 * @param path Path to remove the attribute from.
 * @param name Name of the attribute to remove.
 */
","* Remove an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to remove extended attribute
   * @param name xattr name
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listXAttrs,org.apache.hadoop.fs.FileContext:listXAttrs(org.apache.hadoop.fs.Path),2722,2731,"/**
 * Lists extended attributes for a given path.
 * @param path Path to list extended attributes for.
 * @throws IOException if an I/O error occurs.
 */
","* Get all of the xattr names for a file or directory.
   * Only those xattr names which the logged-in user has permissions to view
   * are returned.
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to get extended attributes
   * @return List{@literal <}String{@literal >} of the XAttr names of the
   * file or directory
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createSnapshot,"org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",2766,2777,"/**
 * Creates a snapshot of the given path with the specified name.
 * @param path The path to snapshot.
 * @param snapshotName Name of the snapshot.
 * @return The path to the created snapshot.
 */
","* Create a snapshot.
   *
   * @param path The directory where snapshots will be taken.
   * @param snapshotName The name of the snapshot
   * @return the snapshot path.
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,renameSnapshot,"org.apache.hadoop.fs.FileContext:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",2794,2805,"/**
 * Renames a snapshot using the provided old and new names.
 * @param path Path to the filesystem.
 * @param snapshotOldName Old snapshot name.
 * @param snapshotNewName New snapshot name.
 */
","* Rename a snapshot.
   *
   * @param path The directory path where the snapshot was taken
   * @param snapshotOldName Old name of the snapshot
   * @param snapshotNewName New name of the snapshot
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,deleteSnapshot,"org.apache.hadoop.fs.FileContext:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",2821,2832,"/**
 * Deletes a snapshot from the filesystem at the given path.
 * @param path Path to the directory containing the snapshot.
 * @param snapshotName Name of the snapshot to delete.
 */
","* Delete a snapshot of a directory.
   *
   * @param path The directory that the to-be-deleted snapshot belongs to
   * @param snapshotName The name of the snapshot
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,satisfyStoragePolicy,org.apache.hadoop.fs.FileContext:satisfyStoragePolicy(org.apache.hadoop.fs.Path),2839,2850,"/**
 * Applies the storage policy to the given path.
 * @param path Path to apply the storage policy to.
 * @throws IOException if an I/O error occurs.
 */
","* Set the source path to satisfy storage policy.
   * @param path The source path referring to either a directory or a file.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setStoragePolicy,"org.apache.hadoop.fs.FileContext:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",2861,2872,"/**
 * Sets the storage policy for a given path.
 * @param path Path to apply the policy to.
 * @param policyName Name of the storage policy.
 * @throws IOException If an I/O error occurs.
 */
","* Set the storage policy for a given file or directory.
   *
   * @param path file or directory path.
   * @param policyName the name of the target storage policy. The list
   *                   of supported Storage policies can be retrieved
   *                   via {@link #getAllStoragePolicies}.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,unsetStoragePolicy,org.apache.hadoop.fs.FileContext:unsetStoragePolicy(org.apache.hadoop.fs.Path),2879,2889,"/**
 * Removes the storage policy from the specified path.
 * @param src Path from which to unset the storage policy.
 * @throws IOException if an I/O error occurs.
 */
","* Unset the storage policy set for a given file or directory.
   * @param src file or directory path.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getStoragePolicy,org.apache.hadoop.fs.FileContext:getStoragePolicy(org.apache.hadoop.fs.Path),2898,2908,"/**
 * Retrieves the BlockStoragePolicySpi for the given path.
 * @param path The path to resolve.
 * @throws IOException if an I/O error occurs.
 */
","* Query the effective storage policy ID for the given file or directory.
   *
   * @param path file or directory path.
   * @return storage policy for give file.
   * @throws IOException If an I/O error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,hasPathCapability,"org.apache.hadoop.fs.FileContext:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",3001,3007,"/**
 * Checks if a path has the specified capability.
 * @param path The path to check.
 * @param capability Capability to check for.
 * @throws IOException if an I/O error occurs.
 */
","* Return the path capabilities of the bonded {@code AbstractFileSystem}.
   * @param path path to query the capability of.
   * @param capability string to query the stream support for.
   * @return true iff the capability is supported under that FS.
   * @throws IOException path resolution or other IO failure
   * @throws IllegalArgumentException invalid arguments",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getServerDefaults,org.apache.hadoop.fs.FileContext:getServerDefaults(org.apache.hadoop.fs.Path),3015,3020,"/**
 * Resolves server defaults for a given path.
 * @param path Path to resolve server defaults for.
 * @return FsServerDefaults object.
 */
","* Return a set of server default configuration values based on path.
   * @param path path to fetch server defaults
   * @return server default configuration values for path
   * @throws IOException an I/O error occurred",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createMultipartUploader,org.apache.hadoop.fs.FileContext:createMultipartUploader(org.apache.hadoop.fs.Path),3029,3035,"/**
 * Creates a MultipartUploaderBuilder for the given base path.
 * @param basePath The base path for the multipart upload.
 * @return A MultipartUploaderBuilder instance.
 */
","* Create a multipart uploader.
   * @param basePath file path under which all files are uploaded
   * @return a MultipartUploaderBuilder object to build the uploader
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getInitialWorkingDirectory,org.apache.hadoop.fs.HarFileSystem:getInitialWorkingDirectory(),278,281,"/**
 * Returns the initial working directory.
 * Delegates to getWorkingDirectory().
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getWorkingDirectory(com.jcraft.jsch.ChannelSftp),652,655,"/**
 * Gets the working directory using the SFTP client.
 * Always returns the home directory.
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,<init>,org.apache.hadoop.fs.RawLocalFileSystem:<init>(),101,103,"/**
* Initializes the RawLocalFileSystem with the initial working directory.
*/
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,refreshStatus,org.apache.hadoop.fs.shell.PathData:refreshStatus(),198,208,"/**
 * Refreshes and sets the FileStatus.
 * @return FileStatus object representing the file status.
 * @throws IOException if an I/O error occurs.
 */
","* Updates the paths's file status
   * @return the updated FileStatus
   * @throws IOException if anything goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getUsed,org.apache.hadoop.fs.HarFileSystem:getUsed(),1271,1274,"/**
 * Returns the number of bytes used for storage.
 * @return Long representing the used space.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getUsed,org.apache.hadoop.fs.FilterFileSystem:getUsed(),415,418,"/**
 * Returns the number of bytes used for storage.
 * @return Long representing the used space.
 */
",Return the total size of all files in the filesystem.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/RegexMountPoint.java,resolve,"org.apache.hadoop.fs.viewfs.RegexMountPoint:resolve(java.lang.String,boolean)",168,205,"/**
 * Resolves a source path using regex mount points.
 * @param srcPath Path to resolve.
 * @param resolveLastComponent Whether to resolve last component.
 * @return ResolveResult object or null if no match found.
 */
","* Get resolved path from regex mount points.
   *  E.g. link: ^/user/(?<username>\\w+) => s3://$user.apache.com/_${user}
   *  srcPath: is /user/hadoop/dir1
   *  resolveLastComponent: true
   *  then return value is s3://hadoop.apache.com/_hadoop
   * @param srcPath - the src path to resolve
   * @param resolveLastComponent - whether resolve the path after last `/`
   * @return mapped path of the mount point.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,checkDest,"org.apache.hadoop.fs.FileUtil:checkDest(java.lang.String,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean)",607,630,"/**
 * Checks destination path, handles overwrites, and recurses if needed.
 * @param srcName Source file name (null for directory).
 * @param dst Destination path.
 * @param overwrite Whether to overwrite existing files.
 * @return Destination path.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,advance,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:advance(),561,569,"/**
 * Advances to the next root directory path.
 * Updates 'next' to the next path if it exists.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,ifExists,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)",615,636,"/**
 * Checks if a file exists at the given path within local directories.
 * @param pathStr File path to check.
 * @param conf Hadoop configuration.
 * @return True if file exists, false otherwise.
 */
","We search through all the configured dirs for the file's existence
     *  and return true when we find one",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,delete,"org.apache.hadoop.io.MapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)",913,921,"/**
 * Deletes a directory and its associated data/index files.
 * @param fs Filesystem object.
 * @param name Directory name to delete.
 */
","* Deletes the named map file.
   * @param fs input fs.
   * @param name input name.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,delete,"org.apache.hadoop.io.BloomMapFile:delete(org.apache.hadoop.fs.FileSystem,java.lang.String)",59,69,"/**
 * Deletes a map file and its associated data files from the file system.
 * @param fs FileSystem object
 * @param name Map file name
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,findCurrentDirectory,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:findCurrentDirectory(java.util.Date),551,558,"/**
 * Calculates the current directory path based on the given date.
 * @param now Date object used to determine the current directory.
 * @return Path object representing the current directory.
 */
","* Use the given time to determine the current directory. The current
   * directory will be based on the {@link #rollIntervalMinutes}.
   *
   * @param now the current time
   * @return the current directory",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,createForWrite,"org.apache.hadoop.io.SecureIOUtils:createForWrite(java.io.File,int)",273,280,"/**
 * Creates a FileOutputStream for writing, respecting security settings.
 * @param f The file to write to.
 * @param permissions File permissions to set.
 * @return A FileOutputStream object.
 * @throws IOException If an I/O error occurs.
 */
","* Open the specified File for write access, ensuring that it does not exist.
   * @param f the file that we want to create
   * @param permissions we want to have on the file (if security is enabled)
   *
   * @throws AlreadyExistsException if the file already exists
   * @throws IOException if any other error occurred
   * @return createForWrite FileOutputStream.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,initFileSystem,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:initFileSystem(java.net.URI),165,171,"/**
 * Initializes the file system path from the provided URI.
 * @param keystoreUri URI representing the keystore file system path.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,extractKMSPath,org.apache.hadoop.crypto.key.kms.KMSClientProvider:extractKMSPath(java.net.URI),443,445,"/**
 * Extracts the KMS path from a URI.
 * @param uri The URI containing the KMS path.
 * @return The unnested Path object.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,checkPathsForReservedRaw,"org.apache.hadoop.fs.shell.CommandWithDestination:checkPathsForReservedRaw(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",385,406,"/**
 * Checks if source and target paths are both in RESERVED_RAW.
 * @param src Source path.
 * @param target Target path.
 * @return True if both paths are in RESERVED_RAW; otherwise false.
 */
","* Check the source and target paths to ensure that they are either both in
   * /.reserved/raw or neither in /.reserved/raw. If neither src nor target are
   * in /.reserved/raw, then return false, indicating not to preserve raw.*
   * xattrs. If both src/target are in /.reserved/raw, then return true,
   * indicating raw.* xattrs should be preserved. If only one of src/target is
   * in /.reserved/raw then throw an exception.
   *
   * @param src The source path to check. This should be a fully-qualified
   *            path, not relative.
   * @param target The target path to check. This should be a fully-qualified
   *               path, not relative.
   * @return true if raw.* xattrs should be preserved.
   * @throws PathOperationException is only one of src/target are in
   * /.reserved/raw.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,createInternal,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",997,1042,"/**
 * Creates a file with specified flags, permissions, and options.
 * @param f the path of the file to create
 * @return FSDataOutputStream for the created file
 * @throws Various IOExceptions
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",1051,1071,"/**
 * Gets block locations for a file, potentially falling back to a linked FS.
 * @param f file path
 * @param start start offset
 * @param len length
 * @return BlockLocation array
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,mkdir,"org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",1267,1299,"/**
 * Creates a directory with specified permissions and parent flags.
 * @param dir Path of the directory to create.
 * @param permission FsPermission object defining permissions.
 * @param createParent Whether to create parent directories.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,create,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1458,1496,"/**
 * Creates a data output stream for a file.
 * @param f the path of the file to create
 * @throws IOException if creation fails
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listStatusForFallbackLink,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatusForFallbackLink(),1638,1657,"/**
 * Lists file statuses for fallback link, adjusting paths.
 * Returns empty array if no fallback link or path exists.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1696,1730,"/**
 * Creates directories recursively with specified permissions.
 * @param dir The directory to create.
 * @param permission FsPermission object defining permissions.
 * @return True if directories were created, false otherwise.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,makeTrashRelativePath,"org.apache.hadoop.fs.TrashPolicyDefault:makeTrashRelativePath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",120,122,"/**
 * Combines the base path and provided file path.
 * @param basePath Base directory path.
 * @param rmFilePath File path to merge.
 * @return Combined path.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,innerPutPart,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerPutPart(org.apache.hadoop.fs.Path,java.io.InputStream,int,org.apache.hadoop.fs.UploadHandle,long)",124,153,"/**
 * Writes a part of a file to the filesystem.
 * @param filePath Path to file
 * @param inputStream Input stream for the part data
 * @param partNumber Part number
 * @param uploadId Upload handle
 * @param lengthInBytes Length of the part in bytes
 * @return PartHandle object
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getParent,org.apache.hadoop.fs.Path:getParent(),429,431,"/**
 * Returns the parent directory of this file.
 */","* Returns the parent of a path or null if at root. Better alternative is
   * {@link #getOptionalParentPath()} to handle nullable value for root path.
   *
   * @return the parent of a path or null if at root",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getOptionalParentPath,org.apache.hadoop.fs.Path:getOptionalParentPath(),440,442,"/**
 * Returns the parent path as an Optional, or empty if null.
 */
","* Returns the parent of a path as {@link Optional} or
   * {@link Optional#empty()} i.e an empty Optional if at root.
   *
   * @return Parent of path wrappen in {@link Optional}.
   * {@link Optional#empty()} i.e an empty Optional if at root.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getDirectoryContentsIterator,org.apache.hadoop.fs.shell.PathData:getDirectoryContentsIterator(),293,299,"/**
 * Returns an iterator over the contents of this directory.
 * Returns RemoteIterator<PathData> objects.
 */
","* Returns a RemoteIterator for PathData objects of the items contained in the
   * given directory.
   * @return remote iterator of PathData objects for its children
   * @throws IOException if anything else goes wrong...",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,schemeFromPath,org.apache.hadoop.fs.Globber:schemeFromPath(org.apache.hadoop.fs.Path),171,182,"/**
 * Extracts the scheme from a Path.
 * @param path The Path to extract the scheme from.
 * @return The scheme string, or null if not found.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,authorityFromPath,org.apache.hadoop.fs.Globber:authorityFromPath(org.apache.hadoop.fs.Path),184,195,"/**
 * Extracts authority from a Path, falling back to file system.
 * @param path The Path to extract authority from.
 * @return Authority string or null if not found.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,<init>,"org.apache.hadoop.fs.FSDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",111,122,"/**
 * Creates a FileSystem data output stream builder.
 * @param fc FileContext for file operations.
 * @param p Path to create the output stream for.
 * @throws IOException if an I/O error occurs.
 */
","* Construct from a {@link FileContext}.
   *
   * @param fc FileContext
   * @param p path.
   * @throws IOException failure",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setVerifyChecksum,"org.apache.hadoop.fs.FileContext:setVerifyChecksum(boolean,org.apache.hadoop.fs.Path)",1223,1228,"/**
 * Sets the checksum verification flag for a file path.
 * @param verifyChecksum Flag to enable/disable checksum verification.
 * @param f The file path to set the flag for.
 */
","* Set the verify checksum flag for the  file system denoted by the path.
   * This is only applicable if the 
   * corresponding FileSystem supports checksum. By default doesn't do anything.
   * @param verifyChecksum verify check sum.
   * @param f set the verifyChecksum for the Filesystem containing this path
   *
   * @throws AccessControlException If access is denied
   * @throws FileNotFoundException If <code>f</code> does not exist
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,readFields,org.apache.hadoop.fs.FileStatus:readFields(java.io.DataInput),496,522,"/**
 * Reads FileStatusProto from DataInput, populating the FileStatus fields.
 */","* Read instance encoded as protobuf from stream.
   * @param in Input stream
   * @see PBHelper#convert(FileStatus)
   * @deprecated Use the {@link PBHelper} and protobuf serialization directly.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",140,149,"/**
 * Constructs a FileStatus with default replication type flags.
 * @param length File size, isdir, replication, etc. See constructor.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean,boolean,org.apache.hadoop.fs.BlockLocation[])",113,123,"/**
 * Constructs a LocatedFileStatus with block locations.
 * @param length File length, isdir, replication, blocksize, times,
 *               permission, owner, group, symlink, path, locations
 */
","* Constructor.
   *
   * @param length a file's length
   * @param isdir if the path is a directory
   * @param block_replication the file's replication factor
   * @param blocksize a file's block size
   * @param modification_time a file's modification time
   * @param access_time a file's access time
   * @param permission a file's permission
   * @param owner a file's owner
   * @param group a file's group
   * @param symlink symlink if the path is a symbolic link
   * @param path the path's qualified name
   * @param hasAcl entity has associated ACLs
   * @param isEncrypted entity is encrypted
   * @param isErasureCoded entity is erasure coded
   * @param locations a file's block locations",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,<init>,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:<init>(org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder,org.apache.hadoop.fs.FileSystem)",87,96,"/**
 * Constructs a FileSystemMultipartUploader with provided builder and file system.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,append,"org.apache.hadoop.io.MapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",399,416,"/**
 * Appends a key-value pair to the data and index, if needed.
 * @param key The key to append.
 * @param val The value to append.
 */
","* Append a key/value pair to the map.  The key must be greater or equal
     * to the previous key added to the map.
     *
     * @param key key.
     * @param val value.
     * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/AbstractService.java,close,org.apache.hadoop.service.AbstractService:close(),246,249,"/**
 * Closes the resource by stopping the underlying process.
 */","* Relay to {@link #stop()}
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,equals,org.apache.hadoop.io.SequenceFile$Metadata:equals(java.lang.Object),785,795,"/**
 * Checks if this metadata object is equal to another.
 * @param other The object to compare to.
 * @return True if objects are equal, false otherwise.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegationTokenRenewer.java,equals,org.apache.hadoop.fs.DelegationTokenRenewer$RenewAction:equals(java.lang.Object),100,108,"/**
 * Checks if two RenewAction objects have the same token.
 * @param that The object to compare to.
 * @return True if tokens are equal, false otherwise.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,equals,org.apache.hadoop.security.token.Token$PrivateToken:equals(java.lang.Object),284,297,"/**
 * Checks if two PrivateToken objects are equal based on publicService.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,selectDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials),998,1006,"/**
 * Selects a delegation token based on credentials, trying two services.
 * @param creds The credentials to use for delegation.
 * @return A delegation token or null if none is found.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/BasicDiskValidator.java,checkStatus,org.apache.hadoop.util.BasicDiskValidator:checkStatus(java.io.File),30,33,"/**
 * Checks the status of a directory.
 * @param dir The directory to check.
 * @throws DiskErrorException if an error occurs.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,write,org.apache.hadoop.fs.FSOutputSummer:write(int),76,82,"/**
 * Writes a byte to the buffer. Flushes buffer when full.
 */
",Write one byte,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,write1,"org.apache.hadoop.fs.FSOutputSummer:write1(byte[],int,int)",120,140,"/**
* Writes data to the buffer, potentially flushing it to the stream.
* @param b the data to write
* @param off offset from where to start writing
* @param len max number of bytes to write
* @return actual number of bytes written
*/
","* Write a portion of an array, flushing to the underlying
   * stream at most once if necessary.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:performCoding(java.nio.ByteBuffer[],java.nio.ByteBuffer[])",79,121,"/**
 * Performs Reed-Solomon decoding using provided inputs and outputs.
 * @param inputs Input byte buffers containing encoded data.
 * @param outputs Output byte buffers for decoded data.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/rawcoder/DecodingValidator.java,validate,"org.apache.hadoop.io.erasurecode.rawcoder.DecodingValidator:validate(org.apache.hadoop.io.erasurecode.ECChunk[],int[],org.apache.hadoop.io.erasurecode.ECChunk[])",138,143,"/**
 * Validates erasure coding data. Converts ECChunks to ByteBuffers.
 * @param inputs Input ECChunk array.
 * @param erasedIndexes Indexes of erased chunks.
 * @param outputs Output ECChunk array.
 */
","*  Validate outputs decoded from inputs, by decoding an input back from
   *  those outputs and comparing it with the original one.
   * @param inputs input buffers used for decoding
   * @param erasedIndexes indexes of erased units used for decoding
   * @param outputs decoded output buffers
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/ErasureDecodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.ErasureDecodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",54,58,"/**
 * Decodes input chunks and stores results in output chunks.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupRandPartA,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartA(),1077,1104,"/**
 * Processes a random part A block, updating state and CRC.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupNoRandPartA,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartA(),1106,1126,"/**
 * Handles the 'no random part A' state transition logic.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,finish,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:finish(),718,732,"/**
 * Completes the compression process, writing remaining data and releasing resources.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,write0,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write0(int),881,900,"/**
 * Writes a byte to the stream, handling run-length encoding.
 * @param b The byte to write.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,finishDataBlock,org.apache.hadoop.io.file.tfile.TFile$Writer:finishDataBlock(boolean),653,671,"/**
 * Finishes the current data block, closing the appender if needed.
 * @param bForceFinish Forces completion, even if size limit not reached.
 * @throws IOException if an I/O error occurs during closing.
 */
","* Close the current data block if necessary.
     * 
     * @param bForceFinish
     *          Force the closure regardless of the block size.
     * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,close,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:close(),3549,3556,"/**
 * Closes the segment stream, releasing resources.
 * Cleans up segment descriptors and resets minSegment.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,parkCursorAtEnd,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:parkCursorAtEnd(),1561,1571,"/**
 * Moves the cursor to the end of the data block.
 * Resets klen and closes the block reader if present.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readTokenStorageStream,org.apache.hadoop.security.Credentials:readTokenStorageStream(java.io.DataInputStream),273,295,"/**
 * Reads token storage data from an input stream.
 * @param in DataInputStream to read from.
 * @throws IOException if an I/O error occurs.
 */
","* Convenience method for reading a token from a DataInputStream.
   *
   * @param in DataInputStream.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,getCandidateTokensForCleanup,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:getCandidateTokensForCleanup(),176,197,"/**
 * Retrieves candidate tokens for cleanup from the SQL secret manager.
 * @return Map of TokenIdent to DelegationTokenInformation.
 */","* Obtain a list of tokens that will be considered for cleanup, based on the last
   * time the token was updated in SQL. This list may include tokens that are not
   * expired and should not be deleted (e.g. if the token was last renewed using a
   * higher renewal interval).
   * The number of results is limited to reduce performance impact. Some level of
   * contention is expected when multiple routers run cleanup simultaneously.
   * @return Map of tokens that have not been updated in SQL after the token renewal
   *         period.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfoFromZK,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)",644,650,"/**
 * Retrieves delegation token information from ZK.
 * @param ident TokenIdent object.
 * @param quiet Flag to suppress errors.
 * @return DelegationTokenInformation object.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawValue,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawValue(org.apache.hadoop.io.SequenceFile$ValueBytes),3866,3869,"/**
 * Reads the next raw value length from the input stream.
 * @param rawValue buffer to store the raw value
 * @return Length of the raw value read.
 */
","* Fills up the passed rawValue with the value corresponding to the key
       * read earlier.
       * @param rawValue input ValueBytes rawValue.
       * @return the length of the value
       * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,removeExpiredStoredToken,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:removeExpiredStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),213,229,"/**
 * Removes an expired stored token. Handles cases where token is renewed or already deleted.
 * @param ident TokenIdent representing the token to remove.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsServerDefaults.java,readFields,org.apache.hadoop.fs.FsServerDefaults:readFields(java.io.DataInput),175,185,"/**
 * Reads fields from a DataInput stream.
 * Populates internal fields from the input.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeProto,org.apache.hadoop.security.Credentials:writeProto(java.io.DataOutput),378,397,"/**
 * Writes credentials data to the output stream in protobuf format.
 */","* Write contents of this instance as CredentialsProto message to DataOutput.
   * @param out
   * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufHelper.java,protoFromToken,org.apache.hadoop.ipc.ProtobufHelper:protoFromToken(org.apache.hadoop.security.token.Token),128,130,"/**
 * Converts a Token object to a TokenProto.
 * @param tok The Token object to convert.
 * @return The corresponding TokenProto.
 */
","* Create a {@code TokenProto} instance
   * from a hadoop token.
   * This builds and caches the fields
   * (identifier, password, kind, service) but not
   * renewer or any payload.
   * @param tok token
   * @return a marshallable protobuf class.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenIdentifier.java,<init>,org.apache.hadoop.security.token.delegation.web.DelegationTokenIdentifier:<init>(org.apache.hadoop.io.Text),37,39,"/**
 * Constructs a DelegationTokenIdentifier with the given kind.
 * @param kind The kind of delegation token.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryInvocationHandler.java,invoke,"org.apache.hadoop.io.retry.RetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",358,374,"/**
 * Invokes a method on the proxy, handling RPC and async calls.
 * @param proxy The proxy object.
 * @param method The method to invoke.
 * @param args Arguments for the method.
 * @return The method's return value or null for async calls.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,entry,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:entry(),1619,1622,"/**
 * Creates a new Entry object for the current key.
 * @throws IOException if an I/O error occurs
 */
","* Get an entry to access the key and value.
       * 
       * @return The Entry object to access the key and value.
       * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,compareCursorKeyTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:compareCursorKeyTo(org.apache.hadoop.io.file.tfile.RawComparable),1642,1646,"/**
 * Compares the cursor key to another key.
 * @param other The key to compare to.
 * @throws IOException if an I/O error occurs.
 */
","* Internal API. Comparing the key at cursor to user-specified key.
       * 
       * @param other
       *          user-specified key.
       * @return negative if key at cursor is smaller than user key; 0 if equal;
       *         and positive if key at cursor greater than user key.
       * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,get,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner$Entry:get(org.apache.hadoop.io.BytesWritable,org.apache.hadoop.io.BytesWritable)",1675,1679,"/**
* Retrieves key and value.
* @param key The key to retrieve.
* @param value The value to retrieve.
*/
","* Copy the key and value in one shot into BytesWritables. This is
         * equivalent to getKey(key); getValue(value);
         * 
         * @param key
         *          BytesWritable to hold key.
         * @param value
         *          BytesWritable to hold value
         * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,inBlockAdvance,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(long),1986,1995,"/**
 * Advances the stream by 'n' records, checking keys and closing stream.
 * @param n number of records to advance
 * @throws IOException if an I/O error occurs
 */
","* Advance cursor by n positions within the block.
       * 
       * @param n
       *          Number of key-value pairs to skip in block.
       * @throws IOException",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,getDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:getDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,java.lang.String)",168,172,"/**
 * Gets a delegation token.
 * @param url URL of the token.
 * @param token Authentication token.
 * @param renewer Renewal identifier.
 * @return Delegation token.
 */
","* Requests a delegation token using the configured <code>Authenticator</code>
   * for authentication.
   *
   * @param url the URL to get the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token being used for the user where the
   * Delegation token will be stored.
   * @param renewer the renewer user.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return abstract delegation token identifier.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,renewDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:renewDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)",217,222,"/**
 * Renews a delegation token.
 * @param url URL for renewal, token to renew, delegation token.
 * @return New expiration timestamp.
 */
","* Renews a delegation token from the server end-point using the
   * configured <code>Authenticator</code> for authentication.
   *
   * @param url the URL to renew the delegation token from. Only HTTP/S URLs are
   * supported.
   * @param token the authentication token with the Delegation Token to renew.
   * @param dToken abstract delegation token identifier.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.
   * @return delegation token long value.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,cancelDelegationToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:cancelDelegationToken(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token,org.apache.hadoop.security.token.Token)",257,262,"/**
 * Cancels a delegation token. Calls overloaded method with null options.
 */","* Cancels a delegation token from the server end-point. It does not require
   * being authenticated by the configured <code>Authenticator</code>.
   *
   * @param url the URL to cancel the delegation token from. Only HTTP/S URLs
   * are supported.
   * @param token the authentication token with the Delegation Token to cancel.
   * @param dToken abstract delegation token identifier.
   * @throws IOException if an IO error occurred.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getInputStream,org.apache.hadoop.net.NetUtils:getInputStream(java.net.Socket),470,473,"/**
 * Gets the input stream wrapper for the given socket.
 * @param socket The socket to get the input stream from.
 * @return SocketInputWrapper for the socket.
 */
","* Same as <code>getInputStream(socket, socket.getSoTimeout()).</code>
   *
   * @param socket socket.
   * @throws IOException raised on errors performing I/O.
   * @return SocketInputWrapper for reading from the socket.
   * @see #getInputStream(Socket, long)",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getOutputStream,org.apache.hadoop.net.NetUtils:getOutputStream(java.net.Socket),526,529,"/**
 * Gets the output stream associated with the given socket.
 * @param socket The socket to get the output stream from.
 * @return The output stream.
 */
","* Same as getOutputStream(socket, 0). Timeout of zero implies write will
   * wait until data is available.<br><br>
   * 
   * From documentation for {@link #getOutputStream(Socket, long)} : <br>
   * Returns OutputStream for the socket. If the socket has an associated
   * SocketChannel then it returns a 
   * {@link SocketOutputStream} with the given timeout. If the socket does not
   * have a channel, {@link Socket#getOutputStream()} is returned. In the later
   * case, the timeout argument is ignored and the write will wait until 
   * data is available.<br><br>
   * 
   * Any socket created using socket factories returned by {@link NetUtils},
   * must use this interface instead of {@link Socket#getOutputStream()}.
   * 
   * @see #getOutputStream(Socket, long)
   * 
   * @param socket socket.
   * @return OutputStream for writing to the socket.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/StatsDSink.java,putMetrics,org.apache.hadoop.metrics2.sink.StatsDSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),97,148,"/**
 * Processes a metrics record, extracts tags, and writes metrics.
 * @param record MetricsRecord containing data to be processed.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,connect,"org.apache.hadoop.net.NetUtils:connect(java.net.Socket,java.net.SocketAddress,int)",574,578,"/**
 * Connects a socket to a remote address with a timeout.
 * @param socket The socket to connect.
 * @param address Remote address to connect to.
 * @param timeout Connection timeout in milliseconds.
 */
","* This is a drop-in replacement for 
   * {@link Socket#connect(SocketAddress, int)}.
   * In the case of normal sockets that don't have associated channels, this 
   * just invokes <code>socket.connect(endpoint, timeout)</code>. If 
   * <code>socket.getChannel()</code> returns a non-null channel,
   * connect is implemented using Hadoop's selectors. This is done mainly
   * to avoid Sun's connect implementation from creating thread-local 
   * selectors, since Hadoop does not have control on when these are closed
   * and could end up taking all the available file descriptors.
   * 
   * @see java.net.Socket#connect(java.net.SocketAddress, int)
   * 
   * @param socket socket.
   * @param address the remote address
   * @param timeout timeout in milliseconds
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,sampleMetrics,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:sampleMetrics(),403,418,"/**
 * Samples metrics from configured sources and returns a buffer.
 * @return MetricsBuffer containing sampled metrics.
 */
","* Sample all the sources for a snapshot of metrics/tags
   * @return  the metrics buffer containing the snapshot",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getAttribute,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttribute(java.lang.String),104,118,"/**
 * Retrieves the value of an attribute.
 * @param attribute Attribute name.
 * @return Attribute value or null if not found.
 * @throws AttributeNotFoundException if attribute not found.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getAttributes,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getAttributes(java.lang.String[]),127,141,"/**
 * Retrieves attributes by keys.
 * @param attributes array of attribute keys
 * @return AttributeList containing the requested attributes
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,getMBeanInfo,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:getMBeanInfo(),154,158,"/**
 * Returns MBean info, updating cache first.
 * @return MBeanInfo object from cache.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/MBeans.java,register,"org.apache.hadoop.metrics2.util.MBeans:register(java.lang.String,java.lang.String,java.lang.Object)",71,74,"/**
 * Registers an MBean with the platform.
 * @param serviceName MBean service name.
 * @param name MBean name.
 * @param theMbean The MBean instance to register.
 * @return ObjectName representing the registered MBean.
 */
","* Register the MBean using our standard MBeanName format
   * ""hadoop:service={@literal <serviceName>,name=<nameName>}""
   * Where the {@literal <serviceName> and <nameName>} are the supplied
   * parameters.
   *
   * @param serviceName serviceName.
   * @param nameName nameName.
   * @param theMbean - the MBean to register
   * @return the named used to register the MBean",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,stop,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:stop(),196,219,"/**
 * Stops the metrics system. Logs warnings if not started properly.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidatorMetrics.java,getMetric,org.apache.hadoop.util.ReadWriteDiskValidatorMetrics:getMetric(java.lang.String),86,103,"/**
 * Retrieves or creates a ReadWriteDiskValidatorMetrics for a directory.
 * @param dirName Directory name to get/create metrics for.
 * @return ReadWriteDiskValidatorMetrics object.
 */
","* Get a metric by given directory name.
   *
   * @param dirName directory name
   * @return the metric",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,init,"org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:init(java.lang.Class,java.lang.String)",190,193,"/**
 * Initializes the object with a protocol and prefix.
 * @param protocol The protocol class.
 * @param prefix The type prefix string.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,init,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:init(int),70,80,"/**
 * Initializes RPC stats with specified priority levels.
 * @param numLevels The number of priority levels to initialize.
 */
","* Initialize the metrics for JMX with priority levels.
   * @param numLevels input numLevels.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,snapshot,"org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:snapshot(org.apache.hadoop.metrics2.MetricsRecordBuilder,boolean)",116,132,"/**
 * Updates global metrics from local states and snapshots them.
 * Iterates through weak references, aggregates data, and snapshots.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRatesWithAggregation.java,collectThreadLocalStates,org.apache.hadoop.metrics2.lib.MutableRatesWithAggregation:collectThreadLocalStates(),137,143,"/**
 * Collects ThreadLocal states and aggregates them to global metrics.
 */","* Collects states maintained in {@link ThreadLocal}, if any.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSinkAdapter.java,<init>,"org.apache.hadoop.metrics2.impl.MetricsSinkAdapter:<init>(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink,java.lang.String,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,org.apache.hadoop.metrics2.MetricsFilter,int,int,int,float,int)",62,94,"/**
 * Constructs a MetricsSinkAdapter with specified parameters.
 * @param name Sink name
 * @param sink Metrics sink object
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String),289,291,"/**
 * Creates a new rate with the given name.
 * @param name The name of the new rate.
 * @return A new MutableRate object.
 */
","* Create a mutable rate metric
   * @param name  of the metric
   * @return a new mutable metric object",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newRate,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newRate(java.lang.String,java.lang.String)",299,301,"/**
 * Creates a new MutableRate with the given name and description.
 */
","* Create a mutable rate metric
   * @param name  of the metric
   * @param description of the metric
   * @return a new mutable rate metric object",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/DecayRpcSchedulerDetailedMetrics.java,create,org.apache.hadoop.ipc.metrics.DecayRpcSchedulerDetailedMetrics:create(java.lang.String),60,64,"/**
 * Creates and registers a DecayRpcSchedulerDetailedMetrics instance.
 * @param ns Scheduler namespace.
 * @return Registered DecayRpcSchedulerDetailedMetrics object.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,create,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:create(int),66,69,"/**
 * Creates and registers an RpcDetailedMetrics instance.
 * @param port The port number for the metrics.
 * @return Registered RpcDetailedMetrics object.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Handler:run(),3151,3230,"/**
 * Processes calls from the queue until the handler is stopped.
 * Retrieves calls, executes them, and updates metrics.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dumpKeytab,org.apache.hadoop.security.KDiag:dumpKeytab(java.io.File),596,620,"/**
 * Dumps keytab file contents to standard output.
 * @param keytabFile The keytab file to examine.
 * @throws IOException if an I/O error occurs.
 */
","* Dump a keytab: list all principals.
   *
   * @param keytabFile the keytab file
   * @throws IOException IO problems",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateJAAS,org.apache.hadoop.security.KDiag:validateJAAS(boolean),755,771,"/**
 * Validates JAAS configuration based on the jaasRequired flag.
 * Checks for and verifies the existence of the JAAS file.
 */
","* Validate any JAAS entry referenced in the {@link #SUN_SECURITY_JAAS_FILE}
   * property.
   * @param jaasRequired is JAAS required",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateNTPConf,org.apache.hadoop.security.KDiag:validateNTPConf(),773,784,"/**
 * Validates and displays NTP configuration file if it exists.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,getTokenRealOwner,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:getTokenRealOwner(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),907,917,"/**
 * Returns the real owner of a token, user or proxy.
 * @param id TokenIdent object containing user info.
 * @return String representing the real owner's username.
 */
","* Return the real owner for a token. If this is a token from a proxy user,
   * the real/effective user will be returned.
   *
   * @param id
   * @return real owner",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ProtoUtil.java,getUgi,org.apache.hadoop.util.ProtoUtil:getUgi(org.apache.hadoop.ipc.protobuf.IpcConnectionContextProtos$IpcConnectionContextProto),124,131,"/**
 * Gets UserGroupInformation from IpcConnectionContextProto.
 * @param context Context object containing user info.
 * @return Ugi object or null if user info is absent.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getUid,org.apache.hadoop.security.ShellBasedIdMapping:getUid(java.lang.String),632,644,"/**
 * Gets the UID for a given username.
 * @param user The username to look up.
 * @return The UID as an integer.
 * @throws IOException if user not found.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getGid,org.apache.hadoop.security.ShellBasedIdMapping:getGid(java.lang.String),646,658,"/**
 * Gets the group ID by name.
 * @param group Group name to look up.
 * @return Group ID as int, throws IOException if not found.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getUserName,"org.apache.hadoop.security.ShellBasedIdMapping:getUserName(int,java.lang.String)",660,676,"/**
 * Retrieves user name by UID, using default if not found.
 * @param uid User ID to retrieve name for.
 * @param unknown Default name if UID is not found.
 * @return User's name or the provided default.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getGroupName,"org.apache.hadoop.security.ShellBasedIdMapping:getGroupName(int,java.lang.String)",678,694,"/**
 * Gets group name by ID.
 * @param gid Group ID.
 * @param unknown Default group name if not found.
 * @return Group name or default if not found.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,ensureParentZNode,org.apache.hadoop.ha.ActiveStandbyElector:ensureParentZNode(),360,396,"/**
 * Creates parent ZNodes in ZK, ensuring existence and ACLs.
 * Throws exceptions on failure; requires a valid ZK connection.
 */
","* Utility function to ensure that the configured base znode exists.
   * This recursively creates the znode as well as all of its parents.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException interrupted exception.
   * @throws KeeperException other zookeeper operation errors.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,getActiveData,org.apache.hadoop.ha.ActiveStandbyElector:getActiveData(),474,491,"/**
 * Retrieves active data from Zookeeper.
 * @return Byte array of active data or throws exception if not found.
 */
","* get data set by the active leader
   * 
   * @return data set by the active instance
   * @throws ActiveNotFoundException
   *           when there is no active leader
   * @throws KeeperException
   *           other zookeeper operation errors
   * @throws InterruptedException
   *           interrupted exception.
   * @throws IOException
   *           when ZooKeeper connection could not be established",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reEstablishSession,org.apache.hadoop.ha.ActiveStandbyElector:reEstablishSession(),872,892,"/**
 * Retries establishing a Zookeeper connection with retries.
 * @return True if connection established, false otherwise.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,checkTGTAndReloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:checkTGTAndReloginFromKeytab(),1197,1199,"/**
 * Attempts to renew the TGT and relogin from the keytab.
 */","* Re-login a user from keytab if TGT is expired or is close to expiry.
   * 
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if it's a kerberos login exception.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,reloginFromKeytab,org.apache.hadoop.security.UserGroupInformation:reloginFromKeytab(),1245,1249,"/**
 * Relogins using a keytab file. Calls the overloaded method with false.
 */","* Re-Login a user in from a keytab file. Loads a user identity from a keytab
   * file and logs them in. They become the currently logged-in user. This
   * method assumes that {@link #loginUserFromKeytab(String, String)} had
   * happened already.
   * The Subject field of this UserGroupInformation object is updated to have
   * the new credentials.
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException on a failure",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String,boolean,boolean)",229,261,"/**
 * Creates an InetSocketAddress from target, defaultPort, configName.
 * @param target Hostname or URI. @param defaultPort Default port.
 * @return InetSocketAddress object.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getConnectAddress,org.apache.hadoop.net.NetUtils:getConnectAddress(java.net.InetSocketAddress),450,460,"/**
 * Resolves or creates a connect address, using localhost if unresolved.
 * @param addr The InetSocketAddress to resolve or create.
 * @return The resolved or created InetSocketAddress.
 */
","* Returns an InetSocketAddress that a client can use to connect to the
   * given listening address.
   * 
   * @param addr of a listener
   * @return socket address that a client can use to connect to the server.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,updateAddress,org.apache.hadoop.ipc.Client$Connection:updateAddress(),588,606,"/**
 * Updates the server address if it has changed.
 * @return True if address updated, false otherwise.
 * @throws IOException if there's an IO error during address lookup
 */","* Update the server address if the address corresponding to the host
     * name has changed.
     *
     * @return true if an addr change was detected.
     * @throws IOException when the hostname cannot be resolved.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getCanonicalUri,"org.apache.hadoop.net.NetUtils:getCanonicalUri(java.net.URI,int)",333,354,"/**
 * Returns a URI with a canonical host and default port if needed.
 */","* Resolve the uri's hostname and add the default port if not in the uri
   * @param uri to resolve
   * @param defaultPort if none is given
   * @return URI",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",1467,1531,"/**
* Sends an RPC request and returns a Writable response or an AsyncGet.
* @param rpcKind RPC kind
* @param rpcRequest RPC request
* @return Writable response or AsyncGet for async mode
* @throws IOException if an I/O error occurs
*/
","* Make a call, passing <code>rpcRequest</code>, to the IPC server defined by
   * <code>remoteId</code>, returning the rpc response.
   *
   * @param rpcKind
   * @param rpcRequest -  contains serialized method and method parameters
   * @param remoteId - the target rpc server
   * @param serviceClass - service class for RPC
   * @param fallbackToSimpleAuth - set to true or false during this method to
   *   indicate if a secure client falls back to simple auth
   * @param alignmentContext - state alignment context
   * @return the rpc response
   * Throws exceptions if there are network problems or if the remote code
   * threw an exception.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,run,org.apache.hadoop.ipc.Client$Connection:run(),1082,1109,"/**
* Starts RPC response processing, waits for work, and closes.
*/",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/CacheableIPList.java,isIn,org.apache.hadoop.util.CacheableIPList:isIn(java.lang.String),62,75,"/**
 * Checks if the given IP address is in the cached IP list.
 * Updates cache if expired.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,waitForCompletion,org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache$CacheEntry),259,300,"/**
 * Waits for a cache entry to complete, adding if it's new.
 * @param newEntry The cache entry to wait for.
 * @return The completed CacheEntry.
 */
","* This method handles the following conditions:
   * <ul>
   * <li>If retry is not to be processed, return null</li>
   * <li>If there is no cache entry, add a new entry {@code newEntry} and return
   * it.</li>
   * <li>If there is an existing entry, wait for its completion. If the
   * completion state is {@link CacheEntry#FAILED}, the expectation is that the
   * thread that waited for completion, retries the request. the
   * {@link CacheEntry} state is set to {@link CacheEntry#INPROGRESS} again.
   * <li>If the completion state is {@link CacheEntry#SUCCESS}, the entry is
   * returned so that the thread that waits for it can can return previous
   * response.</li>
   * <ul>
   * 
   * @return {@link CacheEntry}.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,addCacheEntry,"org.apache.hadoop.ipc.RetryCache:addCacheEntry(byte[],int)",309,319,"/**
 * Adds a new cache entry with client ID and call ID.
 * Updates cache metrics after adding the entry.
 */
","* Add a new cache entry into the retry cache. The cache entry consists of 
   * clientId and callId extracted from editlog.
   *
   * @param clientId input clientId.
   * @param callId input callId.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,addCacheEntryWithPayload,"org.apache.hadoop.ipc.RetryCache:addCacheEntryWithPayload(byte[],int,java.lang.Object)",321,333,"/**
 * Adds a cache entry with a payload to the cache.
 * @param clientId Client identifier.
 * @param callId Call identifier.
 * @param payload The data to be cached.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,org.apache.hadoop.fs.ContentSummary:toString(boolean),381,384,"/**
 * Delegates to the other toString method with default values.
 */
","Return the string representation of the object in the output format.
   * if qOption is false, output directory count, file count, and content size;
   * if qOption is true, output quota and remaining quota as well.
   *
   * @param qOption a flag indicating if quota needs to be printed or not
   * @return the string representation of the object",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/QuotaUsage.java,toString,org.apache.hadoop.fs.QuotaUsage:toString(),302,305,"/**
 * Returns a string representation of the object.
 * Delegates to toString(false) for the actual formatting.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,getExpression,"org.apache.hadoop.fs.shell.find.ExpressionFactory:getExpression(java.lang.String,org.apache.hadoop.conf.Configuration)",108,116,"/**
 * Retrieves an Expression instance by name from the expression map.
 * @param expressionName Name of the expression to retrieve.
 * @param conf Configuration object for expression creation.
 * @return Expression instance or null if not found.
 */
","* Get an instance of the requested expression
   *
   * @param expressionName
   *          name of the command to lookup
   * @param conf
   *          the Hadoop configuration
   * @return the {@link Expression} or null if the expression is unknown",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/ExpressionFactory.java,createExpression,"org.apache.hadoop.fs.shell.find.ExpressionFactory:createExpression(java.lang.String,org.apache.hadoop.conf.Configuration)",145,155,"/**
 * Creates an Expression instance by class name.
 * @param expressionClassname Class name of the Expression to create.
 * @param conf Configuration object.
 * @return Expression instance.
 */
","* Creates an instance of the requested {@link Expression} class.
   *
   * @param expressionClassname
   *          name of the {@link Expression} class to be instantiated
   * @param conf
   *          the Hadoop configuration
   * @return a new instance of the requested {@link Expression} class",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,buildDescription,org.apache.hadoop.fs.shell.find.Find:buildDescription(org.apache.hadoop.fs.shell.find.ExpressionFactory),109,159,"/**
 * Builds a description string of recognised expressions and operators.
 * @param factory Expression factory for creating expressions.
 * @return Description string containing usage and help information.
 */
",Build the description used by the help command.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,getExpression,org.apache.hadoop.fs.shell.find.Find:getExpression(java.lang.Class),438,442,"/**
 * Creates an expression of the specified class.
 * @param expressionClass Class of the expression to create.
 * @return An Expression object.
 */
",Gets an instance of an expression from the factory.,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandFactory.java,getInstance,org.apache.hadoop.fs.shell.CommandFactory:getInstance(java.lang.String),108,110,"/**
 * Gets a Command instance for the given command name.
 * @param cmd The command name.
 * @return A Command instance.
 */
","* Returns an instance of the class implementing the given command.  The
   * class must have been registered via
   * {@link #addClass(Class, String...)}
   * @param cmd name of the command
   * @return instance of the requested command",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,"org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration,boolean)",141,154,"/**
 * Constructs a WritableComparator.
 * @param keyClass Class of WritableComparable keys.
 * @param conf Configuration object.
 * @param createInstances Whether to create internal objects.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,readObject,"org.apache.hadoop.io.ObjectWritable:readObject(java.io.DataInput,org.apache.hadoop.io.ObjectWritable,org.apache.hadoop.conf.Configuration)",261,340,"/**
 * Reads an object from the input stream.
 * @param in Input stream.
 * @param objectWritable Optional writable to store result.
 * @param conf Hadoop configuration.
 * @return Object read from the stream.
 */
","* Read a {@link Writable}, {@link String}, primitive type, or an array of
   * the preceding.
   *
   * @param in DataInput.
   * @param objectWritable objectWritable.
   * @param conf configuration.
   * @return Object.
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableFactories.java,newInstance,org.apache.hadoop.io.WritableFactories:newInstance(java.lang.Class),81,83,"/**
 * Creates a new instance of the specified Writable class.
 * @param c Class of the Writable object to instantiate.
 * @return A new Writable object of the given class.
 */
","* Create a new instance of a class with a defined factory.
   * @param c input c.
   * @return a new instance of a class with a defined factory.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,decodeTokenIdentifier,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:decodeTokenIdentifier(org.apache.hadoop.security.token.Token),870,872,"/**
 * Decodes the token identifier.
 * @param token The token to decode.
 * @return The decoded token identifier.
 */
","* Decode the token identifier. The subclass can customize the way to decode
   * the token identifier.
   * 
   * @param token the token where to extract the identifier
   * @return the delegation token identifier
   * @throws IOException raised on errors performing I/O.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,identifierToString,org.apache.hadoop.security.token.Token:identifierToString(java.lang.StringBuilder),422,435,"/**
 * Appends identifier to buffer, decoding or using binary data.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,printCredentials,"org.apache.hadoop.security.token.DtFileOperations:printCredentials(org.apache.hadoop.security.Credentials,org.apache.hadoop.io.Text,java.io.PrintStream)",137,163,"/**
 * Prints credentials matching the given alias to the output stream.
 * @param creds Credentials object.
 * @param alias Alias to match.
 * @param out PrintStream to write to.
 */
","Print out a Credentials object.
   *  @param creds the Credentials object to be printed out.
   *  @param alias print only tokens matching alias (null matches all).
   *  @param out print to this stream.
   *  @throws IOException failure to unmarshall a token identifier.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskValidatorFactory.java,getInstance,org.apache.hadoop.util.DiskValidatorFactory:getInstance(java.lang.String),72,92,"/**
 * Gets DiskValidator instance based on the provided class name.
 * @param diskValidator Class name of the DiskValidator to retrieve.
 * @throws DiskErrorException if class is not found.
 */
","* Returns {@link DiskValidator} instance corresponding to its name.
   * The diskValidator parameter can be ""basic"" for {@link BasicDiskValidator}
   * or ""read-write"" for {@link ReadWriteDiskValidator}.
   * @param diskValidator canonical class name, for example, ""basic""
   * @throws DiskErrorException if the class cannot be located
   * @return disk validator.",,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,parseMethod,"org.apache.hadoop.ha.NodeFencer:parseMethod(org.apache.hadoop.conf.Configuration,java.lang.String)",151,165,"/**
 * Parses a fencing method line.
 * @param conf Configuration object.
 * @param line Line to parse.
 * @return FenceMethodWithArg object or throws exception.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,<init>,"org.apache.hadoop.util.HostsFileReader:<init>(java.lang.String,java.lang.String)",58,65,"/**
 * Constructs a HostsFileReader with input and exception files.
 * @param inFile input file path
 * @param exFile exception file path
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/HostsFileReader.java,refresh,org.apache.hadoop.util.HostsFileReader:refresh(),118,121,"/**
 * Refreshes the include/exclude files based on current host details.
 */",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/bloom/DynamicBloomFilter.java,add,org.apache.hadoop.util.bloom.DynamicBloomFilter:add(org.apache.hadoop.util.bloom.Key),136,153,"/**
 * Adds a key to the active Bloom filter.
 * Throws NullPointerException if key is null.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResourceObject,org.apache.hadoop.conf.Configuration:addResourceObject(org.apache.hadoop.conf.Configuration$Resource),1034,1038,"/**
* Adds a resource to the list and updates system properties.
* @param resource The resource object to add.
*/
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getProps,org.apache.hadoop.conf.Configuration:getProps(),2946,2952,"/**
 * Retrieves the properties object, loading it if necessary.
 * Returns the Properties object.
 */
",,,,True,7
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BufferedFSInputStream.java,readVectored,"org.apache.hadoop.fs.BufferedFSInputStream:readVectored(java.util.List,java.util.function.IntFunction)",179,183,"/**
 * Reads data using vectored reads from file ranges.
 * @param ranges List of file ranges to read.
 * @param allocate Allocates ByteBuffer for reading.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataInputStream.java,readVectored,"org.apache.hadoop.fs.FSDataInputStream:readVectored(java.util.List,java.util.function.IntFunction)",304,308,"/**
 * Reads data using vectored reads from file ranges.
 * @param ranges List of file ranges to read.
 * @param allocate Allocates ByteBuffer for reading.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,getInternal,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:getInternal(org.apache.hadoop.fs.impl.prefetch.BufferData),177,208,"/**
 * Attempts to internally process buffer data, returning true on success.
 * @param data BufferData object to process.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BlockLocation.java,<init>,org.apache.hadoop.fs.BlockLocation:<init>(),82,84,"/**
 * Default constructor, initializes with empty arrays and zeros.
 */",* Default Constructor.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",883,901,"/**
 * Returns block locations for a file segment.
 * @param file FileStatus object
 * @param start Start offset (long)
 * @param len Length of data (long)
 * @return BlockLocation array
 */
","* Return an array containing hostnames, offset and size of
   * portions of the given file.  For nonexistent
   * file or regions, {@code null} is returned.
   *
   * <pre>
   *   if f == null :
   *     result = null
   *   elif f.getLen() {@literal <=} start:
   *     result = []
   *   else result = [ locations(FS, b) for b in blocks(FS, p, s, s+l)]
   * </pre>
   * This call is most helpful with and distributed filesystem
   * where the hostnames of machines that contain blocks of the given file
   * can be determined.
   *
   * The default implementation returns an array containing one element:
   * <pre>
   * BlockLocation( { ""localhost:9866"" },  { ""localhost"" }, 0, file.getLen())
   * </pre>
   *
   * In HDFS, if file is three-replicated, the returned array contains
   * elements like:
   * <pre>
   * BlockLocation(offset: 0, length: BLOCK_SIZE,
   *   hosts: {""host1:9866"", ""host2:9866, host3:9866""})
   * BlockLocation(offset: BLOCK_SIZE, length: BLOCK_SIZE,
   *   hosts: {""host2:9866"", ""host3:9866, host4:9866""})
   * </pre>
   *
   * And if a file is erasure-coded, the returned BlockLocation are logical
   * block groups.
   *
   * Suppose we have a RS_3_2 coded file (3 data units and 2 parity units).
   * 1. If the file size is less than one stripe size, say 2 * CELL_SIZE, then
   * there will be one BlockLocation returned, with 0 offset, actual file size
   * and 4 hosts (2 data blocks and 2 parity blocks) hosting the actual blocks.
   * 3. If the file size is less than one group size but greater than one
   * stripe size, then there will be one BlockLocation returned, with 0 offset,
   * actual file size with 5 hosts (3 data blocks and 2 parity blocks) hosting
   * the actual blocks.
   * 4. If the file size is greater than one group size, 3 * BLOCK_SIZE + 123
   * for example, then the result will be like:
   * <pre>
   * BlockLocation(offset: 0, length: 3 * BLOCK_SIZE, hosts: {""host1:9866"",
   *   ""host2:9866"",""host3:9866"",""host4:9866"",""host5:9866""})
   * BlockLocation(offset: 3 * BLOCK_SIZE, length: 123, hosts: {""host1:9866"",
   *   ""host4:9866"", ""host5:9866""})
   * </pre>
   *
   * @param file FilesStatus to get data from
   * @param start offset into the given file
   * @param len length for which to get locations for
   * @throws IOException IO failure
   * @return block location array.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationStatisticSummary.java,fetchDurationSummary,"org.apache.hadoop.fs.statistics.DurationStatisticSummary:fetchDurationSummary(org.apache.hadoop.fs.statistics.IOStatistics,java.lang.String,boolean)",129,140,"/**
 * Fetches duration summary for a given key, considering success status.
 * @param source IOStatistics source.
 * @param key Key for the duration summary.
 * @param success Whether the operation was successful.
 * @return DurationStatisticSummary object.
 */
","* Fetch the duration timing summary of success or failure operations
   * from an IO Statistics source.
   * If the duration key is unknown, the summary will be incomplete.
   * @param source source of data
   * @param key duration statistic key
   * @param success fetch success statistics, or if false, failure stats.
   * @return a summary of the statistics.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/impl/IOStatisticsContextImpl.java,snapshot,org.apache.hadoop.fs.statistics.impl.IOStatisticsContextImpl:snapshot(),92,96,"/**
 * Creates and returns an IOStatisticsSnapshot from the current context.
 */","* Returns a snapshot of the current thread's IOStatistics.
   *
   * @return IOStatisticsSnapshot of the context.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/IOStatisticsSupport.java,snapshotIOStatistics,org.apache.hadoop.fs.statistics.IOStatisticsSupport:snapshotIOStatistics(org.apache.hadoop.fs.statistics.IOStatistics),46,50,"/**
 * Creates an IOStatisticsSnapshot from the given statistics.
 * @param statistics The IOStatistics object to snapshot.
 * @return A new IOStatisticsSnapshot object.
 */
","* Take a snapshot of the current statistics state.
   * <p>
   * This is not an atomic option.
   * <p>
   * The instance can be serialized, and its
   * {@code toString()} method lists all the values.
   * @param statistics statistics
   * @return a snapshot of the current values.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_create(java.lang.Object),123,125,"/**
 * Creates an IOStatisticsSnapshot from an IOStatistics object.
 * @param source The IOStatistics object to use.
 * @return A new IOStatisticsSnapshot.
 */
","* Create a new {@link IOStatisticsSnapshot} instance.
   * @param source optional source statistics
   * @return an IOStatisticsSnapshot.
   * @throws ClassCastException if the {@code source} is not null and not an IOStatistics instance",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,toList,org.apache.hadoop.util.functional.RemoteIterators:toList(org.apache.hadoop.fs.RemoteIterator),232,237,"/**
 * Converts a RemoteIterator to a List.
 * @param source Iterator to convert; throws IOException.
 * @return List containing elements from the iterator.
 */
","* Build a list from a RemoteIterator.
   * @param source source iterator
   * @param <T> type
   * @return a list of the values.
   * @throws IOException if the source RemoteIterator raises it.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackStoreToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackStoreToken(org.apache.hadoop.util.functional.InvocationRaisingIOE),1002,1004,"/**
* Tracks a store token invocation, using STORE_TOKEN_STAT.
* @param invocation Invocation object to track.
*/
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackUpdateToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackUpdateToken(org.apache.hadoop.util.functional.InvocationRaisingIOE),1006,1008,"/**
 * Tracks an update token invocation.
 * @param invocation The invocation to track.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,trackRemoveToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager$DelegationTokenSecretManagerMetrics:trackRemoveToken(org.apache.hadoop.util.functional.InvocationRaisingIOE),1010,1012,"/**
 * Tracks a remove token invocation, using REMOVE_TOKEN_STAT.
 * @param invocation The invocation to track.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",59,65,"/**
 * Performs the core coding operation using provided chunks.
 * @param inputChunks Input ECChunk array.
 * @param outputChunks Output ECChunk array.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/EnumSetWritable.java,write,org.apache.hadoop.io.EnumSetWritable:write(java.io.DataOutput),135,154,"/**
 * Writes the data to the DataOutput stream. Handles null and array values.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,write,org.apache.hadoop.io.ObjectWritable:write(java.io.DataOutput),89,92,"/**
 * Writes the object to the DataOutput stream.
 * @param out Output stream to write to.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processArgument,org.apache.hadoop.fs.shell.Command:processArgument(org.apache.hadoop.fs.shell.PathData),299,305,"/**
 * Processes a PathData item, handling existence differently.
 * @param item The PathData item to process.
 * @throws IOException if an I/O error occurs.
 */
","* Processes a {@link PathData} item, calling
   * {@link #processPathArgument(PathData)} or
   * {@link #processNonexistentPath(PathData)} on each item.
   * @param item {@link PathData} item to process
   * @throws IOException if anything goes wrong...",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getUnixGroups,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getUnixGroups(java.lang.String),203,231,"/**
 * Retrieves the Unix groups for a given user.
 * @param user The username to retrieve groups for.
 * @return Set of group names, or empty set on failure.
 */
","* Get the current user's group list from Unix by running the command 'groups'
   * NOTE. For non-existing user it will return EMPTY list.
   *
   * @param user get groups for this user
   * @return the groups list that the <code>user</code> belongs to. The primary
   *         group is returned first.
   * @throws IOException if encounter any error when running the command",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,runResolveCommand,"org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:runResolveCommand(java.util.List,java.lang.String)",222,262,"/**
 * Runs a shell command with provided arguments in batches.
 * @param args Arguments to pass to the command.
 * @param commandScriptName Script name to execute.
 * @return Combined output of all command executions, or null.
 */
","* Build and execute the resolution command. The command is
     * executed in the directory specified by the system property
     * ""user.dir"" if set; otherwise the current working directory is used.
     * @param args a list of arguments
     * @param commandScriptName input commandScriptName.
     * @return null if the number of arguments is out of range,
     * or the output of the command.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,<init>,org.apache.hadoop.util.Shell$ShellCommandExecutor:<init>(java.lang.String[]),1214,1216,"/**
 * Constructs a ShellCommandExecutor with an execution string.
 * @param execString Array of strings representing the command.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,readLink,org.apache.hadoop.fs.FileUtil:readLink(java.io.File),213,230,"/**
 * Reads the target of a symbolic link.
 * @param f The File object representing the symbolic link.
 * @return The target path as a String, or """" on error.
 */
","* Returns the target of the given symlink. Returns the empty string if
   * the given path does not refer to a symlink or there is an error
   * accessing the symlink.
   * @param f File representing the symbolic link.
   * @return The target of the symbolic link, empty string on error or if not
   *         a symlink.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,execCommand,"org.apache.hadoop.fs.FileUtil:execCommand(java.io.File,java.lang.String[])",1531,1537,"/**
 * Executes a command with a file path.
 * @param f File to include in the command.
 * @param cmd Command to execute.
 * @return Command output as a string.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setPermission,"org.apache.hadoop.fs.RawLocalFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",1108,1119,"/**
 * Sets the file permission using native or shell commands.
 * @param p The path to the file.
 * @param permission The desired file permission.
 */
",* Use the command chmod to set permission.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,execShellGetUserForNetgroup,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:execShellGetUserForNetgroup(java.lang.String),134,146,"/**
 * Executes shell command to get users for a netgroup.
 * @param netgroup Netgroup name (without leading '@').
 * @return String result of the shell command.
 */
","* Calls shell to get users for a netgroup by calling getent
   * netgroup, this is a low level function that just returns string
   * that 
   *
   * @param netgroup get users for this netgroup
   * @return string of users for a given netgroup in getent netgroups format
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,relogin,org.apache.hadoop.security.UserGroupInformation$TicketCacheRenewalRunnable:relogin(),1073,1078,"/**
 * Renews the Kerberos ticket and refreshes the login state.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,stashOriginalFilePermissions,org.apache.hadoop.security.alias.LocalKeyStoreProvider:stashOriginalFilePermissions(),92,114,"/**
 * Saves original file permissions for potential keystore rewrite.
 * Uses POSIX permissions or a winutils call on Windows.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,<init>,org.apache.hadoop.security.Credentials:<init>(org.apache.hadoop.security.Credentials),103,105,"/**
 * Creates a new Credentials object, copying data from another.
 * @param credentials The Credentials object to copy from.
 */
","* Create a copy of the given credentials.
   * @param credentials to copy",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,addCredentials,org.apache.hadoop.security.UserGroupInformation:addCredentials(org.apache.hadoop.security.Credentials),1753,1757,"/**
 * Adds credentials to the internal list, thread-safe.
 * @param credentials Credentials to add.
 */
","* Add the given Credentials to this user.
   * @param credentials of tokens and secrets",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.RawLocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",580,590,"/**
 * Creates a non-recursive output stream for a file.
 * @param f path to create, permission, flags, buffer size, etc.
 * @return FSDataOutputStream or throws IOException if exists.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,toFile,org.apache.hadoop.fs.shell.PathData:toFile(),494,499,"/**
 * Converts the path to a File object. Throws exception if not local.
 */
","* Get the path to a local file
   * @return File representing the local path
   * @throws IllegalArgumentException if this.fs is not the LocalFileSystem",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,mkdirsWithExistsAndPermissionCheck,"org.apache.hadoop.util.DiskChecker:mkdirsWithExistsAndPermissionCheck(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",224,235,"/**
 * Creates directory, checks/sets permissions if needed.
 * @param localFS FileSystem object
 * @param dir Path to create
 * @param expected Expected permission
 */
","* Create the directory or check permissions if it already exists.
   *
   * The semantics of mkdirsWithExistsAndPermissionCheck method is different
   * from the mkdirs method provided in the Sun's java.io.File class in the
   * following way:
   * While creating the non-existent parent directories, this method checks for
   * the existence of those directories if the mkdir fails at any point (since
   * that directory might have just been created by some other process).
   * If both mkdir() and the exists() check fails for any seemingly
   * non-existent directory, then we signal an error; Sun's mkdir would signal
   * an error (return false) if a directory it is attempting to create already
   * exists or the mkdir fails.
   *
   * @param localFS local filesystem
   * @param dir directory to be created or checked
   * @param expected expected permission
   * @throws IOException",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setWorkingDirectory,org.apache.hadoop.fs.viewfs.ViewFileSystem:setWorkingDirectory(org.apache.hadoop.fs.Path),441,445,"/**
 * Sets the working directory to the provided path.
 * Validates and converts the path to absolute.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getNativeFileLinkStatus,"org.apache.hadoop.fs.RawLocalFileSystem:getNativeFileLinkStatus(org.apache.hadoop.fs.Path,boolean)",1300,1306,"/**
 * Gets the FileStatus of a path, optionally dereferencing links.
 * @param f Path to check.
 * @param dereference Dereference links?
 * @return FileStatus object.
 */
","* Calls out to platform's native stat(1) implementation to get file metadata
   * (permissions, user, group, atime, mtime, etc). This works around the lack
   * of lstat(2) in Java 6.
   * 
   *  Currently, the {@link Stat} class used to do this only supports Linux
   *  and FreeBSD, so the old {@link #deprecatedGetFileLinkStatusInternal(Path)}
   *  implementation (deprecated) remains further OS support is added.
   *
   * @param f File to stat
   * @param dereference whether to dereference symlinks
   * @return FileStatus of f
   * @throws IOException",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getResolvedQualifiedPath,org.apache.hadoop.fs.viewfs.ChRootedFs:getResolvedQualifiedPath(org.apache.hadoop.fs.Path),167,171,"/**
 * Resolves a path to an absolute, qualified path on the filesystem.
 * @param f The path to resolve.
 * @return The resolved, qualified path.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.AbstractFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1614,1625,"/**
 * Checks if the path has the specified capability.
 * @param path The path to check.
 * @param capability Capability to check for.
 * @return True if the path has the capability, false otherwise.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.AbstractFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),1652,1657,"/**
 * Gets the enclosing root Path for the given Path.
 * @param path The Path to find the enclosing root for.
 * @return The enclosing root Path.
 */
","* Return path of the enclosing root for a given path
   * The enclosing root path is a common ancestor that should be used for temp and staging dirs
   * as well as within encryption zones and other restricted directories.
   *
   * Call makeQualified on the param path to ensure its part of the correct filesystem
   *
   * @param path file path to find the enclosing root path for
   * @return a path to the enclosing root
   * @throws IOException early checks like failure to resolve path cause IO failures",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,makeQualified,org.apache.hadoop.fs.FilterFs:makeQualified(org.apache.hadoop.fs.Path),73,76,"/**
 * Qualifies a given path using the filesystem's qualifier.
 * @param path The path to qualify.
 * @return The qualified path.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,"org.apache.hadoop.fs.FileContext$Util:listStatus(java.util.ArrayList,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",1892,1903,"/**
 * Lists file statuses matching a filter into the results list.
 * @param results List to store matching FileStatus objects.
 * @param f The path to list.
 * @param filter Filter to apply to paths.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,listStatus,org.apache.hadoop.fs.Globber:listStatus(org.apache.hadoop.fs.Path),125,136,"/**
 * Lists status of files/directories under the given path.
 * @param path Path to list status for; returns FileStatus[].
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,processDeleteOnExit,org.apache.hadoop.fs.FileContext:processDeleteOnExit(),296,312,"/**
 * Deletes files registered for deletion on exit. Clears the registry.
 */",* Delete all the paths that were marked as delete-on-exit.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,exists,org.apache.hadoop.fs.FileContext$Util:exists(org.apache.hadoop.fs.Path),1757,1766,"/**
 * Checks if a file exists.
 * @param f Path of the file to check.
 * @return True if file exists, false otherwise.
 */
","* Does the file exist?
     * Note: Avoid using this method if you already have FileStatus in hand.
     * Instead reuse the FileStatus 
     * @param f the  file or dir to be checked
     *
     * @throws AccessControlException If access is denied
     * @throws IOException If an I/O error occurred
     * @throws UnsupportedFileSystemException If file system for <code>f</code> is
     *           not supported
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server
     * @return if f exists true, not false.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,getFileStatus,org.apache.hadoop.fs.Globber:getFileStatus(org.apache.hadoop.fs.Path),112,123,"/**
 * Gets the file status for the given path.
 * @param path Path to the file.
 * @return FileStatus object or null if not found.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setWorkingDirectory,org.apache.hadoop.fs.FileContext:setWorkingDirectory(org.apache.hadoop.fs.Path),542,554,"/**
 * Sets the working directory to the provided absolute path.
 * @param newWDir The new absolute working directory.
 * @throws IOException if the path is a file or other error occurs.
 */
","* Set the working directory for wd-relative names (such a ""foo/bar""). Working
   * directory feature is provided by simply prefixing relative names with the
   * working dir. Note this is different from Unix where the wd is actually set
   * to the inode. Hence setWorkingDir does not follow symlinks etc. This works
   * better in a distributed environment that has multiple independent roots.
   * {@link #getWorkingDirectory()} should return what setWorkingDir() set.
   * 
   * @param newWDir new working directory
   * @throws IOException 
   * <br>
   *           NewWdir can be one of:
   *           <ul>
   *           <li>relative path: ""foo/bar"";</li>
   *           <li>absolute without scheme: ""/foo/bar""</li>
   *           <li>fully qualified with scheme: ""xx://auth/foo/bar""</li>
   *           </ul>
   * <br>
   *           Illegal WDs:
   *           <ul>
   *           <li>relative with scheme: ""xx:foo/bar""</li>
   *           <li>non existent directory</li>
   *           </ul>",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,checkDest,"org.apache.hadoop.fs.FileContext:checkDest(java.lang.String,org.apache.hadoop.fs.Path,boolean)",2261,2278,"/**
 * Checks if the destination path exists and allows overwrite.
 * @param srcName Source name (null for directories).
 * @param dst Destination Path.
 * @param overwrite Whether to overwrite existing files.
 */
","* Check if copying srcName to dst would overwrite an existing 
   * file or directory.
   * @param srcName File or directory to be copied.
   * @param dst Destination to copy srcName to.
   * @param overwrite Whether it's ok to overwrite an existing file. 
   * @throws AccessControlException If access is denied.
   * @throws IOException If dst is an existing directory, or dst is an 
   * existing file and the overwrite option is not passed.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getContentSummary,org.apache.hadoop.fs.FileContext$Util:getContentSummary(org.apache.hadoop.fs.Path),1786,1812,"/**
 * Calculates content summary for a Path.
 * @param f Path to summarize; returns ContentSummary object.
 */","* Return the {@link ContentSummary} of path f.
     * @param f path
     *
     * @return the {@link ContentSummary} of path f.
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If <code>f</code> does not exist
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>f</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getDelegationTokens,"org.apache.hadoop.fs.FileContext:getDelegationTokens(org.apache.hadoop.fs.Path,java.lang.String)",2432,2443,"/**
 * Retrieves delegation tokens from AbstractFileSystems.
 * @param p Path to resolve file systems.
 * @param renewer Renewing identifier for the tokens.
 * @return List of delegation tokens.
 */
","* Get delegation tokens for the file systems accessed for a given
   * path.
   * @param p Path for which delegations tokens are requested.
   * @param renewer the account name that is allowed to renew the token.
   * @return List of delegation tokens.
   * @throws IOException If an I/O error occurred.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,setXAttr,"org.apache.hadoop.fs.FileContext:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[])",2584,2588,"/**
* Sets an extended attribute on a path.
* @param path Path to set the attribute on.
* @param name Attribute name.
* @param value Attribute value.
*/
","* Set an xattr of a file or directory.
   * The name must be prefixed with the namespace followed by ""."". For example,
   * ""user.attr"".
   * <p>
   * Refer to the HDFS extended attributes user documentation for details.
   *
   * @param path Path to modify
   * @param name xattr name.
   * @param value xattr value.
   * @throws IOException If an I/O error occurred.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,createSnapshot,org.apache.hadoop.fs.FileContext:createSnapshot(org.apache.hadoop.fs.Path),2747,2749,"/**
 * Creates a snapshot of the given path.
 * @param path The path to snapshot.
 * @return The snapshot path.
 */
","* Create a snapshot with a default name.
   *
   * @param path The directory where snapshots will be taken.
   * @return the snapshot path.
   *
   * @throws IOException If an I/O error occurred
   *
   * <p>Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",77,87,"/**
 * Constructs a MultipartUploaderBuilderImpl with FileContext and Path.
 * @param fc FileContext for server defaults.
 * @param p Path to upload.
 * @throws IOException if an I/O error occurs.
 */
","* Construct from a {@link FileContext}.
   *
   * @param fc FileContext
   * @param p path.
   * @throws IOException failure",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,<init>,org.apache.hadoop.fs.LocalFileSystem:<init>(),40,42,"/**
 * Constructs a LocalFileSystem using a RawLocalFileSystem.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,hasMoreThanOneSourcePaths,org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:hasMoreThanOneSourcePaths(java.util.LinkedList),105,118,"/**
 * Checks if the input list contains more than one source path.
 * @param args List of PathData objects.
 * @return True if more than one path exists, false otherwise.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,waitForRecovery,org.apache.hadoop.fs.shell.Truncate:waitForRecovery(),102,116,"/**
 * Waits for all items in waitList to reach the expected length.
 */",* Wait for all files in waitList to have length equal to newLength.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,tryResolveInRegexMountpoint,"org.apache.hadoop.fs.viewfs.InodeTree:tryResolveInRegexMountpoint(java.lang.String,boolean)",1022,1032,"/**
 * Tries resolving path in regex mountpoints.
 * @param srcPath Path to resolve.
 * @param resolveLastComponent Resolve last component flag.
 * @return ResolveResult object or null if not found.
 */
","* Walk through all regex mount points to see
   * whether the path match any regex expressions.
   *  E.g. link: ^/user/(?&lt;username&gt;\\w+) =&gt; s3://$user.apache.com/_${user}
   *  srcPath: is /user/hadoop/dir1
   *  resolveLastComponent: true
   *  then return value is s3://hadoop.apache.com/_hadoop
   *
   * @param srcPath srcPath.
   * @param resolveLastComponent resolveLastComponent.
   * @return ResolveResult.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,<init>,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.Path[])",548,554,"/**
 * Constructs a PathIterator with a file system, path string, and root directories.
 * @param fs The file system.
 * @param pathStr Path string to iterate over.
 * @param rootDirs Root directories for the path.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,next,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext$PathIterator:next(),571,583,"/**
 * Returns the next path element. Throws exception if empty.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,ifExists,"org.apache.hadoop.fs.LocalDirAllocator:ifExists(java.lang.String,org.apache.hadoop.conf.Configuration)",248,251,"/**
 * Checks if a path exists.
 * @param pathStr Path to check.
 * @param conf Configuration object.
 * @return True if path exists, false otherwise.
 */
","*  We search through all the configured dirs for the file's existence
   *  and return true when we find.
   *  @param pathStr the requested file (this will be searched)
   *  @param conf the Configuration object
   *  @return true if files exist. false otherwise",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,initFileSystem,org.apache.hadoop.security.alias.LocalKeyStoreProvider:initFileSystem(java.net.URI),116,141,"/**
 * Initializes the file system based on the provided URI.
 * @param uri The URI representing the file system location.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,archivePath,org.apache.hadoop.fs.HarFileSystem:archivePath(org.apache.hadoop.fs.Path),200,211,"/**
 * Finds the .har archive path, traversing up the directory tree.
 * @param p The starting path to search from.
 * @return The path to the .har archive, or null if not found.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getPathInHar,org.apache.hadoop.fs.HarFileSystem:getPathInHar(org.apache.hadoop.fs.Path),356,373,"/**
 * Calculates the relative path within the HAR archive.
 * @param path The input path to resolve.
 * @return The relative path or null if not found.
 */
","* this method returns the path 
   * inside the har filesystem.
   * this is relative path inside 
   * the har filesystem.
   * @param path the fully qualified path in the har filesystem.
   * @return relative path in the filesystem.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,makeRelative,"org.apache.hadoop.fs.HarFileSystem:makeRelative(java.lang.String,org.apache.hadoop.fs.Path)",379,393,"/**
 * Creates a relative path from an initial path and a given path.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,getChecksumFile,org.apache.hadoop.fs.ChecksumFs:getChecksumFile(org.apache.hadoop.fs.Path),88,90,"/**
 * Constructs the path to the checksum file for a given file.
 * @param file The input file path.
 * @return Path to the corresponding checksum file.
 */
","* Return the name of the checksum file associated with a file.
   *
   * @param file the file path.
   * @return the checksum file associated with a file.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,createCollectorPath,org.apache.hadoop.fs.impl.FileSystemMultipartUploader:createCollectorPath(org.apache.hadoop.fs.Path),155,161,"/**
 * Creates a path for a multipart file collector.
 * @param filePath The original file path.
 * @return A new Path object.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,parentExists,org.apache.hadoop.fs.shell.PathData:parentExists(),250,253,"/**
 * Checks if the parent directory exists.
 * Returns true if parent exists, false otherwise.
 */
","* Test if the parent directory exists
   * @return boolean indicating parent exists
   * @throws IOException upon unexpected error",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Mkdir.java,processNonexistentPath,org.apache.hadoop.fs.shell.Mkdir:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),69,90,"/**
 * Creates parent directories and the item path, throwing exceptions on failure.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,primitiveMkdir,"org.apache.hadoop.fs.FileSystem:primitiveMkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",1392,1415,"/**
 * Creates a directory. Requires parent if !createParent.
 * @param f The path of the directory to create.
 */","* This version of the mkdirs method assumes that the permission is absolute.
   * It has been added to support the FileContext that processes the permission
   * with umask before calling this method.
   * This a temporary method added to support the transition from FileSystem
   * to FileContext for user applications.
   *
   * @param f the path.
   * @param absolutePermission permission.
   * @param createParent create parent.
   * @throws IOException IO failure.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,rename,"org.apache.hadoop.fs.FileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",1669,1726,"/**
 * Renames a file or directory from src to dst, optionally overwriting.
 * @param src Source path
 * @param dst Destination path
 * @param options Rename options (e.g., OVERWRITE)
 * @throws IOException if rename fails
 */
","* Renames Path src to Path dst
   * <ul>
   *   <li>Fails if src is a file and dst is a directory.</li>
   *   <li>Fails if src is a directory and dst is a file.</li>
   *   <li>Fails if the parent of dst does not exist or is a file.</li>
   * </ul>
   * <p>
   * If OVERWRITE option is not passed as an argument, rename fails
   * if the dst already exists.
   * </p>
   * <p>
   * If OVERWRITE option is passed as an argument, rename overwrites
   * the dst if it is a file or an empty directory. Rename fails if dst is
   * a non-empty directory.
   * </p>
   * Note that atomicity of rename is dependent on the file system
   * implementation. Please refer to the file system documentation for
   * details. This default implementation is non atomic.
   * <p>
   * This method is deprecated since it is a temporary method added to
   * support the transition from FileSystem to FileContext for user
   * applications.
   * </p>
   *
   * @param src path to be renamed
   * @param dst new path after rename
   * @param options rename options.
   * @throws FileNotFoundException src path does not exist, or the parent
   * path of dst does not exist.
   * @throws FileAlreadyExistsException dest path exists and is a file
   * @throws ParentNotDirectoryException if the parent path of dest is not
   * a directory
   * @throws IOException on failure",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,getNflyTmpPath,org.apache.hadoop.fs.viewfs.NflyFSystem:getNflyTmpPath(org.apache.hadoop.fs.Path),447,449,"/**
 * Constructs a path to the NFLY_TMP directory with the file's name.
 * @param f The original Path object.
 * @return A Path object representing the temporary file path.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getChecksumFile,org.apache.hadoop.fs.ChecksumFileSystem:getChecksumFile(org.apache.hadoop.fs.Path),120,122,"/**
 * Creates a Path for the checksum file based on the input file.
 * @param file The input file Path.
 * @return Path to the checksum file.
 */
","* Return the name of the checksum file associated with a file.
   *
   * @param file the file path.
   * @return name of the checksum file associated with a file.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/BulkDeleteUtils.java,validatePathIsUnderParent,"org.apache.hadoop.fs.BulkDeleteUtils:validatePathIsUnderParent(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",56,64,"/**
 * Checks if a path is under a base path.
 * @param p the path to validate
 * @param basePath the base path
 * @return true if p is under basePath, false otherwise
 */
","* Check if a given path is the base path or under the base path.
   * @param p path to check.
   * @param basePath base path.
   * @return true if the given path is the base path or under the base path.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,isRoot,org.apache.hadoop.fs.Path:isRoot(),408,410,"/**
 * Checks if this node is a root node (has no parent).
 * @return True if this is a root node, false otherwise.
 */
","* Returns true if and only if this path represents the root of a file system.
   *
   * @return true if and only if this path represents the root of a file system",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,suffix,org.apache.hadoop.fs.Path:suffix(java.lang.String),467,474,"/**
 * Appends a suffix to the path's name, creating a new Path.
 * @param suffix The suffix to append.
 * @return A new Path object with the suffix appended.
 */
","* Adds a suffix to the final name in the path.
   *
   * @param suffix the suffix to add
   * @return a new path with the suffix added",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSLinkResolver.java,qualifySymlinkTarget,"org.apache.hadoop.fs.FSLinkResolver:qualifySymlinkTarget(java.net.URI,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",46,55,"/**
 * Qualifies a symlink target path. Returns target if not absolute.
 */","* Return a fully-qualified version of the given symlink target if it
   * has no scheme and authority. Partially and fully-qualified paths
   * are returned unmodified.
   * @param pathURI URI of the filesystem of pathWithLink
   * @param pathWithLink Path that contains the symlink
   * @param target The symlink's absolute target
   * @return Fully qualified version of the target.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,renameInternal,"org.apache.hadoop.fs.AbstractFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",848,897,"/**
 * Renames a file or directory from src to dst, optionally overwriting.
 * @param src source path
 * @param dst destination path
 * @param overwrite whether to overwrite existing destination
 */
","* The specification of this method matches that of
   * {@link FileContext#rename(Path, Path, Options.Rename...)} except that Path
   * f must be for this file system.
   *
   * @param src src.
   * @param dst dst.
   * @param overwrite overwrite flag.
   * @throws AccessControlException access control exception.
   * @throws FileAlreadyExistsException file already exists exception.
   * @throws FileNotFoundException file not found exception.
   * @throws ParentNotDirectoryException parent not directory exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,createPath,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createPath(org.apache.hadoop.fs.Path,java.lang.String,boolean)",362,377,"/**
 * Creates a Path object. Checks write access if specified.
 * @param dir Parent directory.
 * @param path Path to create.
 * @param checkWrite Whether to check write access.
 * @return Path object or null if write check fails.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,createInternal,"org.apache.hadoop.fs.DelegateToFileSystem:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",79,109,"/**
 * Creates a data output stream for writing to a file.
 * @param f Path to create.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,<init>,"org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",716,721,"/**
 * Creates a FCDataOutputStreamBuilder with FileContext and Path.
 * @param fc The FileContext.
 * @param p The Path.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path)",131,138,"/**
 * Constructs a FileStatus with a null name.
 * @param path The path of the file status.
 */
","* Constructor for file systems on which symbolic links are not supported
   *
   * @param length length.
   * @param isdir isdir.
   * @param block_replication block replication.
   * @param blocksize block size.
   * @param modification_time modification time.
   * @param access_time access_time.
   * @param permission permission.
   * @param owner owner.
   * @param group group.
   * @param path the path.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,org.apache.hadoop.fs.FileStatus:<init>(org.apache.hadoop.fs.FileStatus),198,206,"/**
 * Copies another FileStatus object.
 * Uses getters to allow subclasses to override values.
 */
","* Copy constructor.
   *
   * @param other FileStatus to copy
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Stat.java,parseExecResult,org.apache.hadoop.fs.Stat:parseExecResult(java.io.BufferedReader),110,170,"/**
 * Parses the exec result from a BufferedReader to populate FileStatus.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,cloneStatus,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:cloneStatus(),172,182,"/**
 * Creates a copy of the FileStatus object.
 * Returns a new FileStatus with the same attributes.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.BlockLocation[])",49,62,"/**
 * Constructs a LocatedFileStatus from a FileStatus and block locations.
 * Sets symlink if present, handling potential IOExceptions.
 */","* Constructor 
   * @param stat a file status
   * @param locations a file's block locations",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,"org.apache.hadoop.fs.LocatedFileStatus:<init>(long,boolean,int,long,long,long,org.apache.hadoop.fs.permission.FsPermission,java.lang.String,java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.BlockLocation[])",80,92,"/**
* Constructs a LocatedFileStatus with basic permission info.
* @param length File length, isdir, replication, etc.
* @param locations Block locations for the file.
*/","* Constructor
   * 
   * @param length a file's length
   * @param isdir if the path is a directory
   * @param block_replication the file's replication factor
   * @param blocksize a file's block size
   * @param modification_time a file's modification time
   * @param access_time a file's access time
   * @param permission a file's permission
   * @param owner a file's owner
   * @param group a file's group
   * @param symlink symlink if the path is a symbolic link
   * @param path the path's qualified name
   * @param locations a file's block locations",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,build,org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:build(),48,52,"/**
 * Builds and returns a FileSystemMultipartUploader instance.
 * Uses the configured FileSystem.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,append,org.apache.hadoop.io.ArrayFile$Writer:append(org.apache.hadoop.io.Writable),84,87,"/**
 * Appends a value to the writer and increments the count.
 * @param value The value to append.
 * @throws IOException if an I/O error occurs.
 */
","* Append a value to the file.
     * @param value value.
     * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,selectDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:selectDelegationToken(org.apache.hadoop.security.Credentials),147,165,"/**
 * Retrieves a delegation token using credentials, falling back to providers.
 * @param creds Credentials to use for token retrieval.
 * @return Delegation token or null if not found.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSOutputSummer.java,write,"org.apache.hadoop.fs.FSOutputSummer:write(byte[],int,int)",102,114,"/**
 * Writes bytes from a byte array to the output stream.
 * @param b byte array to write
 * @param off starting offset in the array
 * @param len number of bytes to write
 * @throws IOException if an I/O error occurs
 */
","* Writes <code>len</code> bytes from the specified byte array 
   * starting at offset <code>off</code> and generate a checksum for
   * each data chunk.
   *
   * <p> This method stores bytes from the given array into this
   * stream's buffer before it gets checksumed. The buffer gets checksumed 
   * and flushed to the underlying output stream when all data 
   * in a checksum chunk are in the buffer.  If the buffer is empty and
   * requested length is at least as large as the size of next checksum chunk
   * size, this method will checksum and write the chunk directly 
   * to the underlying output stream.  Thus it avoids unnecessary data copy.
   *
   * @param      b     the data.
   * @param      off   the start offset in the data.
   * @param      len   the number of bytes to write.
   * @exception  IOException  if an I/O error occurs.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecodingStep.java,performCoding,"org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecodingStep:performCoding(org.apache.hadoop.io.erasurecode.ECChunk[],org.apache.hadoop.io.erasurecode.ECChunk[])",67,77,"/**
 * Performs coding operation on input chunks and stores result in output chunks.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupRandPartC,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartC(),1156,1167,"/**
 * Advances to RAND_PART_A or updates currentChar/CRC.
 * Updates state based on su_j2 and su_z values.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupBlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupBlock(),1039,1075,"/**
 * Initializes data structures for block processing.
 * Populates cftab, tt arrays and sets up initial state.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupNoRandPartC,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartC(),1183,1195,"/**
 * Processes NoRandPartC state: updates CRC, increments counters.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,finalize,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:finalize(),711,715,"/**
 * Calls finish() and then calls the superclass's finalize method.
 */
",* Overriden to close the stream.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,close,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:close(),734,746,"/**
 * Closes the output stream, ensuring resources are released.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,finish,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:finish(),263,272,"/**
 * Finishes writing data to the stream. Resets if needed.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,write,org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write(int),645,652,"/**
 * Writes a byte to the output stream.
 * @param b The byte to write.
 * @throws IOException if the stream is closed.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2OutputStream.java,write,"org.apache.hadoop.io.compress.bzip2.CBZip2OutputStream:write(byte[],int,int)",859,879,"/**
* Writes bytes from a buffer to the stream.
* @param buf buffer containing bytes to write
* @param offs offset within the buffer
* @param len number of bytes to write
* @throws IOException if an I/O error occurs
*/
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Writer$ValueRegister:close(),492,510,"/**
 * Closes the data writer, releasing resources and updating counts.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekToEnd,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekToEnd(),1447,1449,"/**
 * Moves the cursor to the end of the data stream.
 */","* Seek to the end of the scanner. The entry returned by the previous
       * entry() call will be invalid.
       * 
       * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:close(),1578,1581,"/**
 * Closes the resource by moving the cursor to the end.
 */","* Close the scanner. Release all resources. The behavior of using the
       * scanner after calling close is not defined. The entry returned by the
       * previous entry() call will be invalid.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readTokenStorageFile,"org.apache.hadoop.security.Credentials:readTokenStorageFile(java.io.File,org.apache.hadoop.conf.Configuration)",250,265,"/**
 * Reads credentials from a token storage file.
 * @param filename file to read from
 * @param conf configuration object
 * @return Credentials object read from file
 * @throws IOException if an I/O error occurs
 */
","* Convenience method for reading a token storage file and loading its Tokens.
   * @param filename filename.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return Token.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfoFromZK,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfoFromZK(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),639,642,"/**
 * Retrieves delegation token information from Zookeeper.
 * @param ident TokenIdent object
 * @throws IOException if an I/O error occurs
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,removeStoredToken,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,boolean)",791,831,"/**
 * Removes a stored delegation token from Zookeeper.
 * @param ident TokenIdent object identifying the token.
 * @param checkAgainstZkBeforeDeletion Flag to check ZK before deletion.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeProtobufOutputStream,org.apache.hadoop.security.Credentials:writeProtobufOutputStream(java.io.DataOutputStream),328,333,"/**
 * Writes data to an output stream in Protobuf format.
 * Writes magic number, format type, then serialized data.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,decodeToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:decodeToken(org.apache.hadoop.security.token.Token,org.apache.hadoop.io.Text)",223,232,"/**
 * Decodes a DelegationTokenIdentifier from a token.
 * @param token The token to decode.
 * @param tokenKind Token kind.
 * @return Decoded DelegationTokenIdentifier.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createIdentifier,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:createIdentifier(),80,83,"/**
 * Creates a new DelegationTokenIdentifier with the token kind.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,createIdentifier,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:createIdentifier(),103,106,"/**
 * Creates a new DelegationTokenIdentifier with the token kind.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSDelegationToken.java,<init>,org.apache.hadoop.crypto.key.kms.KMSDelegationToken$KMSDelegationTokenIdentifier:<init>(),43,45,"/**
 * Constructs a KMSDelegationTokenIdentifier with the TOKEN_KIND.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/LossyRetryInvocationHandler.java,invoke,"org.apache.hadoop.io.retry.LossyRetryInvocationHandler:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",41,46,"/**
 * Invokes the method on the proxy, resetting the retry count.
 * @param proxy The proxy object.
 * @param method The invoked method.
 * @param args Method arguments.
 * @return Result of the invoked method.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,inBlockAdvance,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:inBlockAdvance(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",2008,2028,"/**
 * Advances to the next key in the current block.
 * @param key The key to compare against.
 * @param greater True for greater than, false for equal or less.
 * @throws IOException If an I/O error occurs.
 */
","* Advance cursor in block until we find a key that is greater than or
       * equal to the input key.
       * 
       * @param key
       *          Key to compare.
       * @param greater
       *          advance until we find a key greater than the input key.
       * @return true if we find a equal key.
       * @throws IOException",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client$IpcStreams:<init>(java.net.Socket,int)",1897,1903,"/**
 * Initializes IpcStreams with a socket and max response length.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,onTimerEvent,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:onTimerEvent(),382,387,"/**
 * Advances the logical time and publishes metrics if sinks exist.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,publishMetricsNow,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:publishMetricsNow(),392,397,"/**
 * Publishes metrics immediately to all configured sinks.
 * Publishes sampled metrics if sinks are present.
 */
",* Requests an immediate publish of all metrics from sources to sinks.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,initSystemMBean,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:initSystemMBean(),583,588,"/**
 * Initializes the system MBean, registering it if not already done.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,startMBeans,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:startMBeans(),216,224,"/**
 * Registers the MBean for the source, or warns if already registered.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,<init>,"org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:<init>(java.lang.String,int,org.apache.hadoop.ipc.DecayRpcScheduler)",859,867,"/**
 * Constructs a MetricsProxy with given namespace, levels, and scheduler.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:<init>(java.lang.String),403,408,"/**
 * Initializes the MetricsProxy with a namespace and registers it.
 * @param namespace The namespace for the metrics.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,shutdown,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:shutdown(),590,614,"/**
 * Shuts down the metrics system. Returns true if successful.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReadWriteDiskValidator.java,checkStatus,org.apache.hadoop.util.ReadWriteDiskValidator:checkStatus(java.io.File),42,94,"/**
 * Checks disk status by writing and reading a temporary file.
 * @param dir Directory to check; throws DiskErrorException on failure.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcDetailedMetrics.java,init,org.apache.hadoop.ipc.metrics.RpcDetailedMetrics:init(java.lang.Class),75,79,"/**
 * Initializes rate trackers with the given protocol.
 * @param protocol The protocol to use for initialization.
 */
","* Initialize the metrics for JMX with protocol methods
   * @param protocol the protocol class",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,run,org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller:run(),223,240,"/**
 * Runs the rates roller process, updating metrics and averages.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,collectThreadLocalStates,org.apache.hadoop.metrics2.lib.MutableRollingAverages:collectThreadLocalStates(),202,204,"/**
 * Collects thread local states from the inner metrics component.
 */","* Collects states maintained in {@link ThreadLocal}, if any.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,newSink,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:newSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink,org.apache.hadoop.metrics2.impl.MetricsConfig)",521,532,"/**
 * Creates a new MetricsSinkAdapter with configuration parameters.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,add,"org.apache.hadoop.metrics2.lib.MetricsRegistry:add(java.lang.String,long)",358,373,"/**
 * Adds a value to the metric with the given name.
 * Creates a default rate metric if one doesn't exist.
 */
","* Add sample to a stat metric by name.
   * @param name  of the metric
   * @param value of the snapshot to add",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,addTokenForOwnerStats,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addTokenForOwnerStats(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),924,928,"/**
 * Increments token owner stats for a given token ID.
 * @param id The token identifier.
 */
","* Add token stats to the owner to token count mapping.
   *
   * @param id token id.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeTokenForOwnerStats,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeTokenForOwnerStats(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),935,945,"/**
 * Decrements token count for owner stats or removes if zero.
 * @param id TokenIdent object representing the token.
 */
","* Remove token stats to the owner to token count mapping.
   *
   * @param id",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getUidAllowingUnknown,org.apache.hadoop.security.ShellBasedIdMapping:getUidAllowingUnknown(java.lang.String),697,707,"/**
 * Gets user UID, falling back to hashcode if mapping fails.
 * @param user User identifier string.
 * @return User UID (integer).
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,getGidAllowingUnknown,org.apache.hadoop.security.ShellBasedIdMapping:getGidAllowingUnknown(java.lang.String),710,720,"/**
 * Gets the GID for a group, using hashcode if mapping fails.
 * @param group The group name.
 * @return The GID, or hashcode if mapping fails.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,getCurrentActive,org.apache.hadoop.ha.ZKFailoverController:getCurrentActive(),784,802,"/**
 * Retrieves the current active HAServiceTarget.
 * Returns null if active data is not found.
 */
","* @return an {@link HAServiceTarget} for the current active node
   * in the cluster, or null if no node is active.
   * @throws IOException if a ZK-related issue occurs
   * @throws InterruptedException if thread is interrupted",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,<init>,"org.apache.hadoop.ha.ActiveStandbyElector:<init>(java.lang.String,int,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback,int,boolean,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",273,299,"/**
 * Constructs an ActiveStandbyElector with provided configuration.
 * @param app Callback for elector events.
 */
","* Create a new ActiveStandbyElector object <br>
   * The elector is created by providing to it the Zookeeper configuration, the
   * parent znode under which to create the znode and a reference to the
   * callback interface. <br>
   * The parent znode name must be the same for all service instances and
   * different across services. <br>
   * After the leader has been lost, a new leader will be elected after the
   * session timeout expires. Hence, the app must set this parameter based on
   * its needs for failure response time. The session timeout must be greater
   * than the Zookeeper disconnect timeout and is recommended to be 3X that
   * value to enable Zookeeper to retry transient disconnections. Setting a very
   * short session timeout may result in frequent transitions between active and
   * standby states during issues like network outages/GS pauses.
   * 
   * @param zookeeperHostPorts
   *          ZooKeeper hostPort for all ZooKeeper servers
   * @param zookeeperSessionTimeout
   *          ZooKeeper session timeout
   * @param parentZnodeName
   *          znode under which to create the lock
   * @param acl
   *          ZooKeeper ACL's
   * @param authInfo a list of authentication credentials to add to the
   *                 ZK connection
   * @param app
   *          reference to callback interface object
   * @param failFast
   *          whether need to add the retry when establishing ZK connection.
   * @param maxRetryNum max Retry Num
   * @param truststoreKeystore truststore keystore, that we will use for ZK if SSL/TLS is enabled
   * @throws IOException
   *          raised on errors performing I/O.
   * @throws HadoopIllegalArgumentException
   *          if valid data is not supplied.
   * @throws KeeperException
   *          other zookeeper operation errors.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,joinElectionInternal,org.apache.hadoop.ha.ActiveStandbyElector:joinElectionInternal(),783,796,"/**
 * Attempts to join the election, ensuring app data and ZK connection.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,relogin,org.apache.hadoop.security.UserGroupInformation$KeytabRenewalRunnable:relogin(),1094,1097,"/**
 * Relogins using the keytab authentication method.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddrUnresolved,org.apache.hadoop.net.NetUtils:createSocketAddrUnresolved(java.lang.String),166,168,"/**
 * Creates an InetSocketAddress from a target string, without resolving the host.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String,boolean)",222,227,"/**
 * Creates an InetSocketAddress.
 * @param target target host.
 * @param defaultPort default port.
 * @param configName config name.
 * @param useCacheIfPresent use cache if present.
 */
","* Create an InetSocketAddress from the given target string and
   * default port. If the string cannot be parsed correctly, the
   * <code>configName</code> parameter is used as part of the
   * exception message, allowing the user to better diagnose
   * the misconfiguration.
   *
   * @param target a string of either ""host"" or ""host:port""
   * @param defaultPort the default port if <code>target</code> does not
   *                    include a port number
   * @param configName the name of the configuration from which
   *                   <code>target</code> was loaded. This is used in the
   *                   exception message in the case that parsing fails.
   * @param useCacheIfPresent Whether use cache when create URI
   * @return  socket addr",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getConnectAddress,org.apache.hadoop.net.NetUtils:getConnectAddress(org.apache.hadoop.ipc.Server),439,441,"/**
 * Gets the connect address for a server.
 * @param server The server object.
 * @return The InetSocketAddress for connection.
 */
","* Returns InetSocketAddress that a client can use to 
   * connect to the server. Server.getListenerAddress() is not correct when
   * the server binds to ""0.0.0.0"". This returns ""hostname:port"" of the server,
   * or ""127.0.0.1:port"" when the getListenerAddress() returns ""0.0.0.0:port"".
   * 
   * @param server server.
   * @return socket address that a client can use to connect to the server.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setupConnection,org.apache.hadoop.ipc.Client$Connection:setupConnection(org.apache.hadoop.security.UserGroupInformation),608,695,"/**
 * Establishes a connection to the server, handling timeouts & address updates.
 * @param ticket UserGroupInformation object for Kerberos credentials.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean)",1415,1420,"/**
 * Calls the method, delegating to a more detailed implementation.
 * @param rpcKind RPC kind, request, remote ID, fallback flag
 * @return Writable result of the RPC call
 * @throws IOException if an I/O error occurs
 */
","* Make a call, passing <code>rpcRequest</code>, to the IPC server defined by
   * <code>remoteId</code>, returning the rpc respond.
   *
   * @param rpcKind - input rpcKind.
   * @param rpcRequest -  contains serialized method and method parameters
   * @param remoteId - the target rpc server
   * @param fallbackToSimpleAuth - set to true or false during this method to
   *   indicate if a secure client falls back to simple auth
   * @return the rpc response
   * Throws exceptions if there are network problems or if the remote code
   * threw an exception.
   * @throws IOException raised on errors performing I/O.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",1422,1428,"/**
 * Calls the RPC method, delegating to a default RPC service class.
 * @param rpcKind RPC kind, request, remote ID, auth flag, context.
 * @return Writable result of the RPC call.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,call,"org.apache.hadoop.ipc.Client:call(org.apache.hadoop.ipc.RPC$RpcKind,org.apache.hadoop.io.Writable,org.apache.hadoop.ipc.Client$ConnectionId,int,java.util.concurrent.atomic.AtomicBoolean)",1444,1450,"/**
 * Calls an RPC operation. Delegates to overloaded call method.
 * @param rpcKind RPC kind, request, remote ID, service class, auth flag
 * @return Writable result or throws IOException
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,waitForCompletion,"org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,byte[],int)",354,362,"/**
 * Waits for cache completion. Returns CacheEntry or null if skipped.
 */
","* Static method that provides null check for retryCache.
   * @param cache input Cache.
   * @param clientId client id of this request
   * @param callId client call id of this request
   * @return CacheEntry.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RetryCache.java,waitForCompletion,"org.apache.hadoop.ipc.RetryCache:waitForCompletion(org.apache.hadoop.ipc.RetryCache,java.lang.Object,byte[],int)",372,380,"/**
 * Waits for cache completion, returns CacheEntryWithPayload or null.
 * @param cache RetryCache instance
 * @param payload Payload data
 */
","* Static method that provides null check for retryCache.
   * @param cache input cache.
   * @param payload input payload.
   * @param clientId client id of this request
   * @param callId client call id of this request
   * @return CacheEntryWithPayload.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ContentSummary.java,toString,org.apache.hadoop.fs.ContentSummary:toString(),369,372,"/**
 * Returns a string representation of the object.
 * Delegates to toString(true) for detailed output.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,getExpression,org.apache.hadoop.fs.shell.find.Find:getExpression(java.lang.String),432,435,"/**
* Retrieves an expression by name using the ExpressionFactory.
* @param expressionName Name of the expression to retrieve.
*/
",Gets a named expression from the factory.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printInfo,"org.apache.hadoop.fs.FsShell:printInfo(java.io.PrintStream,java.lang.String,boolean)",212,247,"/**
 * Prints command help/usage to the output stream.
 * @param out Output stream.
 * @param cmd Command name (optional).
 * @param showHelp Show detailed help (true/false).
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,displayError,"org.apache.hadoop.fs.FsShell:displayError(java.lang.String,java.lang.String)",353,365,"/**
 * Displays an error message, suggesting a dash if missing.
 * @param cmd The command being executed.
 * @param message The error message to display.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,get,"org.apache.hadoop.io.WritableComparator:get(java.lang.Class,org.apache.hadoop.conf.Configuration)",65,81,"/**
 * Retrieves a WritableComparator for the given class.
 * @param c WritableComparable class.
 * @param conf Hadoop configuration.
 * @return WritableComparator instance.
 */
","* Get a comparator for a {@link WritableComparable} implementation.
   * @param c class.
   * @param conf configuration.
   * @return WritableComparator.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class),132,134,"/**
* Constructs a WritableComparator for the given key class.
* @param keyClass The Class of the WritableComparable key.
*/
","* Construct for a {@link WritableComparable} implementation.
   * @param keyClass WritableComparable Class.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,"org.apache.hadoop.io.WritableComparator:<init>(java.lang.Class,boolean)",136,139,"/**
 * Constructor for WritableComparator.
 * @param keyClass Class of WritableComparable to compare.
 * @param createInstances Whether to create instances for comparison.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ObjectWritable.java,readFields,org.apache.hadoop.io.ObjectWritable:readFields(java.io.DataInput),84,87,"/**
 * Reads fields from the input stream using the configuration.
 * @param in DataInput stream to read from.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,readFields,org.apache.hadoop.ipc.WritableRpcEngine$Invocation:readFields(java.io.DataInput),148,164,"/**
 * Reads fields from a DataInput, populating method parameters.
 * @param in DataInput stream to read from
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayWritable.java,readFields,org.apache.hadoop.io.ArrayWritable:readFields(java.io.DataInput),93,101,"/**
 * Reads fields from DataInput, populating the 'values' array.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,verifyToken,org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:verifyToken(org.apache.hadoop.security.token.Token),208,215,"/**
 * Verifies a delegation token and returns the associated user.
 * @param token Delegation token to verify.
 * @return UserGroupInformation object.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/Token.java,toString,org.apache.hadoop.security.token.Token:toString(),437,447,"/**
 * Returns a string representation of the object.
 * Includes kind, service, and identifier details.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,<init>,org.apache.hadoop.fs.LocalDirAllocator:<init>(java.lang.String),85,93,"/**
 * Creates a LocalDirAllocator with the given context configuration name.
 * @param contextCfgItemName Name of the context configuration item.
 */
","* Create an allocator object.
   * @param contextCfgItemName contextCfgItemName.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,parseMethods,"org.apache.hadoop.ha.NodeFencer:parseMethods(org.apache.hadoop.conf.Configuration,java.lang.String)",134,149,"/**
 * Parses fencing methods from a configuration string.
 * @param conf Configuration object.
 * @param spec Fencing method specification string.
 * @return List of FenceMethodWithArg objects.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,append,"org.apache.hadoop.io.BloomMapFile$Writer:append(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",182,190,"/**
 * Appends a key-value pair and adds its key to the bloom filter.
 */",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.lang.String,boolean)",927,929,"/**
* Adds a new resource with the given name and restricted flag.
*/
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.net.URL,boolean)",945,947,"/**
* Adds a resource to the system.
* @param url Resource URL.
* @param restrictedParser Parser restriction flag.
*/
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.fs.Path,boolean)",963,965,"/**
 * Adds a resource to the system.
 * @param file Path to the resource file.
 * @param restrictedParser Flag to restrict parser usage.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,boolean)",984,986,"/**
 * Adds a resource to the parser.
 * @param in InputStream for the resource data.
 * @param restrictedParser If parser should be restricted.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,java.lang.String,boolean)",1002,1005,"/**
 * Adds a resource to the system.
 * @param in InputStream for resource data.
 * @param name Resource name.
 * @param restrictedParser Parser restriction flag.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setDeprecatedProperties,org.apache.hadoop.conf.Configuration:setDeprecatedProperties(),688,706,"/**
 * Updates properties based on deprecation context and overlay.
 * Uses new keys from DeprecatedKeyInfo to update props.
 */
","* Sets all deprecated properties that are not currently set but have a
   * corresponding new property that is set. Useful for iterating the
   * properties when all deprecated properties for currently set properties
   * need to be present.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,updatePropertiesWithDeprecatedKeys,"org.apache.hadoop.conf.Configuration:updatePropertiesWithDeprecatedKeys(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String[])",764,775,"/**
 * Updates properties with new names, migrating values from deprecated keys.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration:<init>(org.apache.hadoop.conf.Configuration),843,875,"/**
 * Creates a new configuration by cloning another configuration.
 * Copies resources, properties, and other configuration data.
 */","* A new configuration with the same settings cloned from another.
   * 
   * @param other the configuration from which to clone settings.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.conf.Configuration),1015,1017,"/**
 * Adds a resource to the configuration.
 * @param conf The configuration object to add the resource to.
 */
","* Add a configuration resource.
   *
   * The properties of this resource will override properties of previously
   * added resources, unless they were marked <a href=""#Final"">final</a>.
   *
   * @param conf Configuration object from which to load properties",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getAlternativeNames,org.apache.hadoop.conf.Configuration:getAlternativeNames(java.lang.String),1372,1393,"/**
 * Retrieves alternative names for a given name, if deprecated.
 * @param name The name to check for deprecation.
 * @return Array of alternative names or null if not deprecated.
 */
","* Returns alternative names (non-deprecated keys or previously-set deprecated keys)
   * for a given non-deprecated key.
   * If the given key is deprecated, return null.
   *
   * @param name property name.
   * @return alternative names.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPropertySources,org.apache.hadoop.conf.Configuration:getPropertySources(java.lang.String),2109,2129,"/**
 * Gets property sources by name. Returns null if not found.
 */","* Gets information about why a property was set.  Typically this is the 
   * path to the resource objects (file, URL, etc.) the property came from, but
   * it can also indicate that it was set programmatically, or because of the
   * command line.
   *
   * @param name - The property name to get the source of.
   * @return null - If the property or its source wasn't found. Otherwise, 
   * returns a list of the sources of the resource.  The older sources are
   * the first ones in the list.  So for example if a configuration is set from
   * the command line, and then written out to a file that is read back in the
   * first entry would indicate that it was set from the command line, while
   * the second one would indicate the file that the new configuration was read
   * in from.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,size,org.apache.hadoop.conf.Configuration:size(),2988,2990,"/**
 * Returns the number of properties.
 * @return The size of the properties collection.
 */
","* Return the number of keys in the configuration.
   *
   * @return number of keys in the configuration.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,clear,org.apache.hadoop.conf.Configuration:clear(),2995,2998,"/**
 * Clears the properties and overlay associated with this object.
 */",* Clears all keys from the configuration.,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,iterator,org.apache.hadoop.conf.Configuration:iterator(),3006,3022,"/**
* Returns an iterator over string-string property pairs.
* Filters non-string entries from the properties.
*/","* Get an {@link Iterator} to go through the list of <code>String</code> 
   * key-value pairs in the configuration.
   * 
   * @return an iterator over the entries.",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,write,org.apache.hadoop.conf.Configuration:write(java.io.DataOutput),3965,3975,"/**
 * Writes properties to the output stream.
 * Writes size, key-value pairs, and optional updating resources.
 */
",,,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getValByRegex,org.apache.hadoop.conf.Configuration:getValByRegex(java.lang.String),3982,4001,"/**
* Extracts values from props matching regex.
* @param regex Regex pattern to match prop keys.
* @return Map of matching keys and their values.
*/
","* get keys matching the the regex.
   * @param regex the regex to match against.
   * @return {@literal Map<String,String>} with matching keys",,,True,8
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,readVectored,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:readVectored(java.util.List,java.util.function.IntFunction)",438,482,"/**
 * Reads data and checksums vectored based on provided ranges.
 * @param ranges List of file ranges to read.
 * @param allocate Allocates ByteBuffer for reading.
 */
","* Vectored read.
     * If the file has no checksums: delegate to the underlying stream.
     * If the file is checksummed: calculate the checksum ranges as
     * well as the data ranges, read both, and validate the checksums
     * as well as returning the data.
     * @param ranges the byte ranges to read
     * @param allocate the function to allocate ByteBuffer
     * @throws IOException",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/CachingBlockManager.java,get,org.apache.hadoop.fs.impl.prefetch.CachingBlockManager:get(int),144,175,"/**
 * Retrieves a BufferData object for the given block number.
 * @param blockNumber Block number to retrieve.
 * @throws IOException if stream is closed.
 */
","* Gets the block having the given {@code blockNumber}.
   *
   * @throws IllegalArgumentException if blockNumber is negative.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,waitForReplication,org.apache.hadoop.fs.shell.SetReplication:waitForReplication(),108,141,"/**
 * Waits for all items in the waitList to reach the desired replication count.
 */",* Wait for all files in waitList to have replication number equal to rep.,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.FileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",924,931,"/**
 * Gets block locations for a file, given a start offset and length.
 * @param p Path to the file
 * @param start Start offset in the file
 * @param len Length of the data to retrieve
 * @throws IOException if an I/O error occurs
 */
","* Return an array containing hostnames, offset and size of
   * portions of the given file.  For a nonexistent
   * file or regions, {@code null} is returned.
   *
   * This call is most helpful with location-aware distributed
   * filesystems, where it returns hostnames of machines that
   * contain the given file.
   *
   * A FileSystem will normally return the equivalent result
   * of passing the {@code FileStatus} of the path to
   * {@link #getFileBlockLocations(FileStatus, long, long)}
   *
   * @param p path is used to identify an FS since an FS could have
   *          another FS that it could be delegating the call to
   * @param start offset into the given file
   * @param len length for which to get locations for
   * @throws FileNotFoundException when the path does not exist
   * @throws IOException IO failure
   * @return block location array.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.FilterFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",151,155,"/**
 * Gets block locations for a file within a specified range.
 * @param file FileStatus object representing the file.
 * @param start Start offset in bytes.
 * @param len Length of data to retrieve.
 * @return Array of BlockLocation objects.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/statistics/DurationStatisticSummary.java,fetchSuccessSummary,"org.apache.hadoop.fs.statistics.DurationStatisticSummary:fetchSuccessSummary(org.apache.hadoop.fs.statistics.IOStatistics,java.lang.String)",149,153,"/**
 * Fetches success duration summary for a key from the source.
 * @param source IOStatistics object
 * @param key Identifier for the summary
 * @return DurationStatisticSummary object
 */
","* Fetch the duration timing summary from an IOStatistics source.
   * If the duration key is unknown, the summary will be incomplete.
   * @param source source of data
   * @param key duration statistic key
   * @return a summary of the statistics.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_create,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_create(),113,115,"/**
 * Creates an IostatisticsSnapshot. Calls the overloaded method with null.
 */","* Create a new {@link IOStatisticsSnapshot} instance.
   * @return an empty IOStatisticsSnapshot.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedStatistics.java,iostatisticsSnapshot_retrieve,org.apache.hadoop.io.wrappedio.WrappedStatistics:iostatisticsSnapshot_retrieve(java.lang.Object),146,152,"/**
 * Retrieves an IO statistics snapshot from the given source.
 * @param source The source object to retrieve statistics from.
 * @return An IOStatisticsSnapshot or null if stats are unavailable.
 */
","* Extract the IOStatistics from an object in a serializable form.
   * @param source source object, may be null/not a statistics source/instance
   * @return {@link IOStatisticsSnapshot} or null if the object is null/doesn't have statistics",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/RemoteIterators.java,toArray,"org.apache.hadoop.util.functional.RemoteIterators:toArray(org.apache.hadoop.fs.RemoteIterator,java.lang.Object[])",248,252,"/**
 * Converts a RemoteIterator to an array.
 * @param source Iterator to convert.
 * @param a Array to populate, can be reused.
 * @return Array containing elements from the iterator.
 */
","* Build an array from a RemoteIterator.
   * @param source source iterator
   * @param a destination array; if too small a new array
   * of the same type is created
   * @param <T> type
   * @return an array of the values.
   * @throws IOException if the source RemoteIterator raises it.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,createPassword,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:createPassword(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),493,515,"/**
 * Creates a password for a token identifier, stores it, and returns.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,renewToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)",592,643,"/**
 * Renews a token with the given renewer, validating its integrity.
 * @param token Token to renew.
 * @param renewer Renewing user.
 * @return New expiration time for the token.
 */
","* Renew a delegation token.
   * @param token the token to renew
   * @param renewer the full principal name of the user doing the renewal
   * @return the new expiration time
   * @throws InvalidToken if the token is invalid
   * @throws AccessControlException if the user can't renew token",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",654,685,"/**
 * Cancels a token and returns the identifier.
 * @param token The token to cancel.
 * @param canceller User canceling the token.
 * @return TokenIdent object.
 */
","* Cancel a token by removing it from cache.
   *
   * @param token token.
   * @param canceller canceller.
   * @return Identifier of the canceled token
   * @throws InvalidToken for invalid token
   * @throws AccessControlException if the user isn't allowed to cancel",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processArguments,org.apache.hadoop.fs.shell.Command:processArguments(java.util.LinkedList),281,290,"/**
 * Processes a list of PathData objects, handling potential IO errors.
 */","*  Processes the command's list of expanded arguments.
   *  {@link #processArgument(PathData)} will be invoked with each item
   *  in the list.  The loop catches IOExceptions, increments the error
   *  count, and displays the exception.
   *  @param args a list of {@link PathData} to process
   *  @throws IOException if anything goes wrong...",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroups,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroups(java.lang.String),98,101,"/**
 * Retrieves a list of Unix groups for a given username.
 * @param userName The username to retrieve groups for.
 * @return A list of group names.
 */
","* Returns list of groups for a user
   *
   * @param userName get groups for this user
   * @return list of groups for a given user",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:getGroupsSet(java.lang.String),121,124,"/**
 * Retrieves a set of Unix groups for a given username.
 * @param userName The username to look up.
 * @return A set of group names.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,resolve,org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:resolve(java.util.List),174,211,"/**
 * Resolves a list of names using a script, returns list of strings.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HardLink.java,getLinkCount,org.apache.hadoop.fs.HardLink:getLinkCount(java.io.File),214,255,"/**
 * Gets the number of hard links to a file.
 * @param fileName The file to check.
 * @throws IOException if file is invalid or an error occurs.
 */
","* Retrieves the number of links to the specified file.
    *
    * @param fileName file name.
    * @throws IOException raised on errors performing I/O.
    * @return link count.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTarUsingTar,"org.apache.hadoop.fs.FileUtil:unTarUsingTar(java.io.File,java.io.File,boolean)",1053,1084,"/**
 * Untars a file to a directory, optionally handling gzip compression.
 * @param inFile File to untar.
 * @param untarDir Directory to extract to.
 * @param gzipped True if the tar file is gzipped.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,symLink,"org.apache.hadoop.fs.FileUtil:symLink(java.lang.String,java.lang.String)",1226,1279,"/**
 * Creates a symbolic link.
 * @param target The target path.
 * @param linkname The link name.
 * @return Exit code of the symlink command.
 */","* Create a soft link between a src and destination
   * only on a local disk. HDFS does not support this.
   * On Windows, when symlink creation fails due to security
   * setting, we will log a warning. The return code in this
   * case is 2.
   *
   * @param target the target for symlink
   * @param linkname the symlink
   * @return 0 on success
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,chmod,"org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String,boolean)",1303,1319,"/**
 * Changes the file permissions using a shell command.
 * @param filename File to modify.
 * @param perm Permission string (e.g., ""u+rwx"").
 * @param recursive If true, applies changes recursively.
 * @return Exit code of the shell command.
 */
","* Change the permissions on a file / directory, recursively, if
   * needed.
   * @param filename name of the file whose permissions are to change
   * @param perm permission string
   * @param recursive true, if permissions should be changed recursively
   * @return the exit code from the command.
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoLinux.java,getConf,org.apache.hadoop.util.SysInfoLinux:getConf(java.lang.String),158,170,"/**
 * Gets a system configuration value using getconf.
 * @param attr Configuration attribute name. Returns -1 on error.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getSystemInfoInfoFromShell,org.apache.hadoop.util.SysInfoWindows:getSystemInfoInfoFromShell(),81,92,"/**
 * Executes systeminfo command and returns the output as a string.
 * Returns null if an IOException occurs.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,checkIsBashSupported,org.apache.hadoop.util.Shell:checkIsBashSupported(),810,834,"/**
 * Checks if bash is supported on the system.
 * @return True if bash is supported, false otherwise.
 * @throws InterruptedIOException if execution is interrupted.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Shell.java,isSetsidSupported,org.apache.hadoop.util.Shell:isSetsidSupported(),845,879,"/**
 * Checks if setsid is supported on the system.
 * Returns true if supported, false otherwise.
 */
","* Look for <code>setsid</code>.
   * @return true if <code>setsid</code> was present",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,loadPermissionInfoByNonNativeIO,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfoByNonNativeIO(),998,1045,"/**
 * Loads file permissions, owner, and group using non-native IO.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setOwner,"org.apache.hadoop.fs.FileUtil:setOwner(java.io.File,java.lang.String,java.lang.String)",1329,1338,"/**
 * Sets the owner of a file.
 * @param file The file to modify.
 * @param username The new owner username.
 * @param groupname The new group name.
 */
","* Set the ownership on a file / directory. User name and group name
   * cannot both be null.
   * @param file the file to change
   * @param username the new user owner name
   * @param groupname the new group owner name
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,execSetPermission,"org.apache.hadoop.fs.FileUtil:execSetPermission(java.io.File,org.apache.hadoop.fs.permission.FsPermission)",1520,1529,"/**
 * Sets file permissions using native methods or shell command.
 * @param f The file to set permissions on.
 * @param permission The desired file permissions.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,getUsersForNetgroup,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getUsersForNetgroup(java.lang.String),97,123,"/**
* Retrieves a list of usernames belonging to a given netgroup.
* @param netgroup The netgroup name to query.
* @return List of usernames in the netgroup.
*/
","* Gets users for a netgroup
   *
   * @param netgroup return users for this netgroup
   * @return list of users for a given netgroup
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getCredentials,org.apache.hadoop.security.UserGroupInformation:getCredentials(),1736,1747,"/**
 * Retrieves credentials, removing private tokens.
 * @return Credentials object with private tokens removed.
 */
","* Obtain the tokens in credentials form associated with this user.
   * 
   * @return Credentials of tokens associated with this user",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,flush,org.apache.hadoop.security.alias.UserProvider:flush(),94,97,"/**
 * Adds credentials to the user.
 * @param credentials Credentials to be added.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,flush,org.apache.hadoop.crypto.key.UserProvider:flush(),138,141,"/**
 * Adds credentials to the user.
 * @param user The user to add credentials to.
 * @param credentials Credentials to be added.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirInternal,"org.apache.hadoop.util.DiskChecker:checkDirInternal(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",138,143,"/**
 * Creates directory and checks access.
 * @param localFS Filesystem.
 * @param dir Directory path.
 * @param expected Expected permissions.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,hasPathCapability,"org.apache.hadoop.fs.FilterFs:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",451,455,"/**
 * Checks if a path has a specific capability.
 * @param path The path to check.
 * @param capability Capability to check for.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getEnclosingRoot,org.apache.hadoop.fs.FilterFs:getEnclosingRoot(org.apache.hadoop.fs.Path),463,466,"/**
 * Gets the enclosing root path for a given path.
 * @param path The path to check.
 * @return The enclosing root path.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,"org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",1850,1856,"/**
 * Lists status of files/directories under a path, filtered by PathFilter.
 * @param f the path
 * @param filter filter for paths
 * @return FileStatus array
 */
","* Filter files/directories in the given path using the user-supplied path
     * filter.
     * 
     * @param f is the path name
     * @param filter is the user-supplied path filter
     *
     * @return an array of FileStatus objects for the files under the given path
     *         after applying the filter
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If <code>f</code> does not exist
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>pathPattern</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,"org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.PathFilter)",1879,1886,"/**
 * Lists file statuses for given paths, filtered by a PathFilter.
 * @param files Paths to check.
 * @param filter Filter to apply.
 * @return Array of FileStatus objects.
 */
","* Filter files/directories in the given list of paths using user-supplied
     * path filter.
     * 
     * @param files is a list of paths
     * @param filter is the filter
     *
     * @return a list of statuses for the files under the given paths after
     *         applying the filter
     *
     * @throws AccessControlException If access is denied
     * @throws FileNotFoundException If a file in <code>files</code> does not 
     *           exist
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,run,org.apache.hadoop.fs.FileContext$FileContextFinalizer:run(),2318,2321,"/**
 * Executes the 'processDeleteOnExit' method in a synchronized manner.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,isMultiThreadNecessary,org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:isMultiThreadNecessary(java.util.LinkedList),98,102,"/**
 * Checks if multithreading is needed based on thread count & paths.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Truncate.java,processArguments,org.apache.hadoop.fs.shell.Truncate:processArguments(java.util.LinkedList),68,73,"/**
 * Processes arguments, waits for recovery if specified.
 * @param args List of PathData objects to process.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,resolve,"org.apache.hadoop.fs.viewfs.InodeTree:resolve(java.lang.String,boolean)",893,988,"/**
 * Resolves a path to a file system.
 * @param p the path to resolve
 * @param resolveLastComponent whether to resolve last component
 * @return ResolveResult object containing resolution details
 */","* Resolve the pathname p relative to root InodeDir.
   * @param p - input path
   * @param resolveLastComponent resolveLastComponent.
   * @return ResolveResult which allows further resolution of the remaining path
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getFileHarStatus,org.apache.hadoop.fs.HarFileSystem:getFileHarStatus(org.apache.hadoop.fs.Path),646,659,"/**
 * Retrieves the HarStatus for a file.
 * @param f Path to the file.
 * @throws IOException if file is invalid or not found.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,int)",153,178,"/**
 * Initializes the checker with a file system, path, and buffer size.
 * @param fs file system
 * @param file path to checksum
 * @param bufferSize buffer size for reading
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,setReplication,"org.apache.hadoop.fs.ChecksumFs:setReplication(org.apache.hadoop.fs.Path,short)",463,475,"/**
 * Sets the replication factor for a path and its checksum file.
 * @param src Path to set replication for.
 * @param replication New replication factor.
 * @return True if successful, false otherwise.
 */
","* Set replication for an existing file.
   * Implement the abstract <tt>setReplication</tt> of <tt>FileSystem</tt>
   * @param src file name
   * @param replication new replication
   * @throws IOException if an I/O error occurs.
   * @return true if successful;
   *         false if file does not exist or is a directory",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,delete,"org.apache.hadoop.fs.ChecksumFs:delete(org.apache.hadoop.fs.Path,boolean)",529,550,"/**
 * Deletes a file or directory recursively.
 * @param f Path to delete
 * @param recursive Whether to delete recursively
 * @return True if successful, false otherwise.
 */
","* Implement the delete(Path, boolean) in checksum
   * file system.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSOutputSummer:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",363,389,"/**
 * Constructs a ChecksumFSOutputSummer for summing checksums.
 * @param fs FileSystem, file, flags, permissions, buffer size, etc.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processArguments,org.apache.hadoop.fs.shell.CommandWithDestination:processArguments(java.util.LinkedList),223,245,"/**
 * Validates arguments, ensuring destination path exists/is directory.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,mkdir,"org.apache.hadoop.fs.DelegateToFileSystem:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",185,192,"/**
 * Creates a directory.
 * @param dir The directory to create.
 * @param permission Permissions for the directory.
 * @param createParent Creates parent directories if they don't exist.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,rename,"org.apache.hadoop.fs.FileUtil:rename(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",2068,2082,"/**
 * Renames a path using the FileSystem's protected rename API.
 * @param srcFs FileSystem object
 * @param src Source path
 * @param dst Destination path
 * @param options Rename options
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,renameInternal,"org.apache.hadoop.fs.DelegateToFileSystem:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",206,212,"/**
 * Renames a file or directory from src to dst.
 * @param src Source path.
 * @param dst Destination path.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,rename,"org.apache.hadoop.fs.FilterFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",254,258,"/**
 * Renames a file or directory from src to dst using provided options.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/InternalOperations.java,rename,"org.apache.hadoop.fs.InternalOperations:rename(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",35,39,"/**
 * Renames a file or directory.
 * @param fs Filesystem to operate on.
 * @param src Source path.
 * @param dst Destination path.
 * @param options Rename options.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,run,org.apache.hadoop.fs.ChecksumFileSystem$FsOperation:run(org.apache.hadoop.fs.Path),771,780,"/**
 * Runs the operation on a path, and checksum file if successful.
 * @param p the path to operate on
 * @return true if the operation was successful, false otherwise
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,int)",187,214,"/**
 * Constructs a ChecksumFSInputChecker.
 * @param fs ChecksumFileSystem instance
 * @param file Path to the file
 * @param bufferSize Buffer size for I/O operations
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,rename,"org.apache.hadoop.fs.ChecksumFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",890,915,"/**
 * Renames a file or directory from src to dst.
 * @param src Source path
 * @param dst Destination path
 * @return True if rename was successful, false otherwise.
 */
",* Rename files/dirs,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,delete,"org.apache.hadoop.fs.ChecksumFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",921,941,"/**
 * Deletes a file or directory.
 * @param f Path to delete.
 * @param recursive If true, deletes directory contents recursively.
 * @return True if deleted, false otherwise.
 */
","* Implement the delete(Path, boolean) in checksum
   * file system.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.permission.FsPermission)",621,642,"/**
 * Creates a ChecksumFSOutputSummer for summing checksums of a file.
 * @param fs ChecksumFileSystem instance
 * @param file Path to the file
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,isAncestor,"org.apache.hadoop.fs.shell.find.Find:isAncestor(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",335,343,"/**
 * Checks if target is an ancestor of source in the path hierarchy.
 */",Returns true if the target is an ancestor of the source.,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,fullPath,org.apache.hadoop.fs.viewfs.ChRootedFs:fullPath(org.apache.hadoop.fs.Path),91,95,"/**
 * Constructs a full path by combining root path and given path.
 * @param path Path to append to the root path.
 * @return The full path as a Path object.
 */
","* 
   * @param path
   * @return return full path including the chroot",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,stripOutRoot,org.apache.hadoop.fs.viewfs.ChRootedFs:stripOutRoot(org.apache.hadoop.fs.Path),137,148,"/**
 * Strips the root path from a given Path object.
 * @param p The Path object to strip the root from.
 * @return The path string without the root, or """" if it matches.
 */
","*  
   * Strip out the root from the path.
   * 
   * @param p - fully qualified path p
   * @return -  the remaining path  without the beginning /",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,stripOutRoot,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:stripOutRoot(org.apache.hadoop.fs.Path),153,163,"/**
 * Strips the root path from a given path.
 * @param p The path to strip.
 * @return The path without the root, or """" if it's the root.
 */
","* Strip out the root from the path.
   * @param p - fully qualified path p
   * @return -  the remaining path  without the beginning /
   * @throws IOException if the p is not prefixed with root",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,createCheckpoint,"org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(org.apache.hadoop.fs.Path,java.util.Date)",335,359,"/**
 * Creates a trash checkpoint directory, retrying on conflicts.
 * @param trashRoot Root directory for trash.
 * @param date Checkpoint date.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSLinkResolver.java,resolve,"org.apache.hadoop.fs.FSLinkResolver:resolve(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",79,112,"/**
 * Resolves a path, following symlinks up to a limit.
 * @param fc FileContext to use for resolution.
 * @param path Path to resolve.
 * @return Resolved object of type T.
 * @throws IOException if resolution fails.
 */
","* Performs the operation specified by the next function, calling it
   * repeatedly until all symlinks in the given path are resolved.
   * @param fc FileContext used to access file systems.
   * @param path The path to resolve symlinks on.
   * @return Generic type determined by the implementation of next.
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,rename,"org.apache.hadoop.fs.AbstractFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",795,808,"/**
 * Renames a file or directory from src to dst.
 * @param src Source path
 * @param dst Destination path
 * @param options Rename options (e.g., overwrite)
 */
","* The specification of this method matches that of
   * {@link FileContext#rename(Path, Path, Options.Rename...)} except that Path
   * f must be for this file system.
   *
   * @param src src.
   * @param dst dst.
   * @param options options.
   * @throws AccessControlException access control exception.
   * @throws FileAlreadyExistsException file already exists exception.
   * @throws FileNotFoundException file not found exception.
   * @throws ParentNotDirectoryException parent not directory exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,renameInternal,"org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",253,259,"/**
 * Renames a file or directory internally.
 * @param src Source path. @param dst Destination path.
 * @param overwrite Overwrites destination if it exists.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,toFileStatus,org.apache.hadoop.fs.HarFileSystem:toFileStatus(org.apache.hadoop.fs.HarFileSystem$HarStatus),530,553,"/**
 * Creates a FileStatus from a HarStatus, handling metadata version.
 */","* Combine the status stored in the index and the underlying status. 
   * @param h status stored in the index
   * @return the combined file status
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,<init>,"org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:<init>(java.io.File,long,org.apache.hadoop.fs.FileSystem)",942,949,"/**
 * Constructs a RawLocalFileStatus using the provided file and details.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,org.apache.hadoop.fs.FileStatus:<init>(),107,107,"/**
 * Default constructor for FileStatus, initializes with default values.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileStatus.java,<init>,"org.apache.hadoop.fs.FileStatus:<init>(long,boolean,int,long,long,org.apache.hadoop.fs.Path)",110,115,"/**
 * Constructs a FileStatus with default access control and owner.
 * @param length File length, isDir, replication, blocksize, mod time, path.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.commons.net.ftp.FTPFile,org.apache.hadoop.fs.Path)",557,572,"/**
 * Creates a FileStatus object from an FTPFile.
 * @param ftpFile The FTPFile to create status from.
 * @param parentPath Parent path of the file.
 * @return A FileStatus object representing the FTP file.
 */
","* Convert the file information in FTPFile to a {@link FileStatus} object. *
   * 
   * @param ftpFile
   * @param parentPath
   * @return FileStatus",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupRandPartB,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupRandPartB(),1128,1154,"/**
 * Transitions to RAND_PART_C or RAND_PART_A based on state and count.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,changeStateToProcessABlock,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:changeStateToProcessABlock(),355,362,"/**
 * Transitions state to PROCESSING or EOF based on skipResult.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,init,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:init(),492,509,"/**
 * Initializes the BZip2 decompressor by validating stream format.
 * Reads magic number and block size, then sets up the block.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,setupNoRandPartB,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:setupNoRandPartB(),1169,1181,"/**
 * Handles part B of a no-random sequence setup.
 * Updates state based on su_ch2, su_chPrev, and su_count.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,close,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:close(),302,308,"/**
 * Closes this stream and closes the underlying output stream.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,write,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:write(int),288,293,"/**
 * Writes a byte to the output stream. Resets if needed.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,write,"org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionOutputStream:write(byte[],int,int)",295,300,"/**
 * Writes a portion of a byte array to the output stream.
 * @param b The byte array.
 * @param off Offset within the array.
 * @param len Number of bytes to write.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,verify,"org.apache.hadoop.security.KDiag:verify(java.io.File,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",990,1003,"/**
 * Verifies token file integrity. Fails/errors if read fails.
 * @param tokenFile File to verify.
 * @param conf Hadoop configuration.
 * @param category Error category.
 * @param message Error message.
 * @return True if verification succeeds, false otherwise.
 */
","* Verify that tokenFile contains valid Credentials.
   *
   * If not, an exception is raised, or, if {@link #nofail} is set,
   * an error will be logged and the method return false.
   *",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,printTokenFile,"org.apache.hadoop.security.token.DtFileOperations:printTokenFile(java.io.File,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration,java.io.PrintStream)",123,129,"/**
 * Prints token file details to the output stream.
 * @param tokenFile File containing token storage.
 * @param alias Text alias for the token.
 */
","Print out a Credentials file from the local filesystem.
   *  @param tokenFile a local File object.
   *  @param alias print only tokens matching alias (null matches all).
   *  @param conf Configuration object passed along.
   *  @param out print to this stream.
   *  @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,getTokenInfo,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:getTokenInfo(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),600,617,"/**
 * Retrieves DelegationTokenInformation, caching if needed.
 * @param ident TokenIdent to look up; returns token info or null.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,syncLocalCacheWithZk,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:syncLocalCacheWithZk(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),625,637,"/**
 * Syncs local cache with ZK for a given token identifier.
 * @param ident TokenIdent object representing the token.
 */
","* This method synchronizes the state of a delegation token information in
   * local cache with its actual value in Zookeeper.
   *
   * @param ident Identifier of the token",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,removeStoredToken,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:removeStoredToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier),785,789,"/**
* Removes a stored token. Calls overloaded method with 'false'.
* @param ident TokenIdent to remove.
* @throws IOException if an I/O error occurs.
*/
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageToStream,"org.apache.hadoop.security.Credentials:writeTokenStorageToStream(java.io.DataOutputStream,org.apache.hadoop.security.Credentials$SerializedFormat)",306,319,"/**
 * Writes token storage to a DataOutputStream based on the format.
 * @param os Output stream to write to.
 * @param format Serialized format (WRITABLE or PROTOBUF).
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,<init>,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<init>(java.lang.String),118,134,"/**
 * Constructs a MetricsSystemImpl with an optional prefix.
 * @param prefix A prefix for metrics names (can be null).
 */
","* Construct the metrics system
   * @param prefix  for the system",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,startMetricsMBeans,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:startMetricsMBeans(),334,339,"/**
 * Starts the metrics MBeans for each registered source adapter.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSourceAdapter.java,start,org.apache.hadoop.metrics2.impl.MetricsSourceAdapter:start(),100,102,"/**
 * Starts the application, optionally starting MBeans.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,getInstance,"org.apache.hadoop.ipc.DecayRpcScheduler$MetricsProxy:getInstance(java.lang.String,int,org.apache.hadoop.ipc.DecayRpcScheduler)",869,881,"/**
 * Gets the MetricsProxy instance for the given namespace.
 * @param namespace Namespace for metrics.
 * @param numLevels Number of levels.
 * @param drs DecayRpcScheduler delegate.
 * @return MetricsProxy instance.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,getInstance,org.apache.hadoop.ipc.FairCallQueue$MetricsProxy:getInstance(java.lang.String),410,418,"/**
 * Retrieves the MetricsProxy instance for the given namespace.
 * @param namespace Namespace for the metrics proxy.
 * @return MetricsProxy instance.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,call,"org.apache.hadoop.ipc.WritableRpcEngine$Server$WritableRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)",546,642,"/**
 * Handles an RPC call, version verification, and method invocation.
 * @param server RPC server instance
 * @param protocolName Protocol name
 * @param rpcRequest Writable RPC request
 * @return Writable object representing the result
 * @throws IOException on RPC errors
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,processCall,"org.apache.hadoop.ipc.ProtobufRpcEngine$Server:processCall(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)",463,504,"/**
 * Processes an RPC call, executing the specified method.
 * @param server RPC server, protocol name, request, method name, protocol impl
 * @return RpcWritable wrapper of the result, or null if deferred
 */
","* This implementation is same as
     * ProtobufRpcEngine2.Server.ProtobufInvoker#call(..)
     * except this implementation uses non-shaded protobuf classes from legacy
     * protobuf version (default 2.5.0).",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,call,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,java.lang.String,org.apache.hadoop.ipc.RPC$Server$ProtoClassProtoImpl)",599,641,"/**
 * Calls a blocking method on a server, handling exceptions and metrics.
 * @param server RPC server, request, method name, protocol impl.
 * @return RpcWritable result or null if deferred.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,<init>,org.apache.hadoop.metrics2.lib.MutableRollingAverages$RatesRoller:<init>(org.apache.hadoop.metrics2.lib.MutableRollingAverages),219,221,"/**
 * Constructs a RatesRoller with a parent RollingAverages object.
 * @param parent The parent RollingAverages instance.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,registerSink,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)",296,306,"/**
 * Registers a metrics sink with a given name and description.
 * @param name Sink name.
 * @param desc Sink description.
 * @param sink The MetricsSink instance.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,newSink,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:newSink(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.impl.MetricsConfig)",534,537,"/**
 * Creates a new MetricsSinkAdapter using the provided name, description, and config.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRates.java,add,"org.apache.hadoop.metrics2.lib.MutableRates:add(java.lang.String,long)",76,78,"/**
 * Adds a timed event to the registry.
 * @param name Event name.
 * @param elapsed Elapsed time in milliseconds.
 */
","* Add a rate sample for a rate metric
   * @param name of the rate metric
   * @param elapsed time",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,addPersistedDelegationToken,"org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:addPersistedDelegationToken(org.apache.hadoop.security.token.delegation.AbstractDelegationTokenIdentifier,long)",404,433,"/**
 * Adds a persisted delegation token, or expires it if no key exists.
 * @param identifier Token identifier.
 * @param renewDate Token renewal date.
 * @throws IOException If the SecretManager is running or token is duplicated.
 */
","* This method is intended to be used for recovering persisted delegation
   * tokens. Tokens that have an unknown <code>DelegationKey</code> are
   * marked as expired and automatically cleaned up.
   * This method must be called before this secret manager is activated (before
   * startThreads() is called)
   * @param identifier identifier read from persistent storage
   * @param renewDate token renew time
   * @throws IOException raised on errors performing I/O.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,syncTokenOwnerStats,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:syncTokenOwnerStats(),952,957,"/**
 * Clears and updates token owner statistics based on current tokens.
 */","* This method syncs token information from currentTokens to tokenOwnerStats.
   * It is used when the currentTokens is initialized or refreshed. This is
   * called from a single thread thus no synchronization is needed.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/AbstractDelegationTokenSecretManager.java,removeExpiredToken,org.apache.hadoop.security.token.delegation.AbstractDelegationTokenSecretManager:removeExpiredToken(),762,780,"/**
 * Removes expired delegation tokens from the token store.
 * Logs expired tokens after releasing the lock.
 */",Remove expired delegation tokens from cache,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,<init>,"org.apache.hadoop.ha.ActiveStandbyElector:<init>(java.lang.String,int,java.lang.String,java.util.List,java.util.List,org.apache.hadoop.ha.ActiveStandbyElector$ActiveStandbyElectorCallback,int,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore)",226,233,"/**
 * Creates an ActiveStandbyElector with default election enabled.
 * @param zookeeperHostPorts Zookeeper host ports
 * @param app Elector callback
 */
","* Create a new ActiveStandbyElector object <br>
   * The elector is created by providing to it the Zookeeper configuration, the
   * parent znode under which to create the znode and a reference to the
   * callback interface. <br>
   * The parent znode name must be the same for all service instances and
   * different across services. <br>
   * After the leader has been lost, a new leader will be elected after the
   * session timeout expires. Hence, the app must set this parameter based on
   * its needs for failure response time. The session timeout must be greater
   * than the Zookeeper disconnect timeout and is recommended to be 3X that
   * value to enable Zookeeper to retry transient disconnections. Setting a very
   * short session timeout may result in frequent transitions between active and
   * standby states during issues like network outages/GS pauses.
   * 
   * @param zookeeperHostPorts
   *          ZooKeeper hostPort for all ZooKeeper servers
   * @param zookeeperSessionTimeout
   *          ZooKeeper session timeout
   * @param parentZnodeName
   *          znode under which to create the lock
   * @param acl
   *          ZooKeeper ACL's
   * @param authInfo a list of authentication credentials to add to the
   *                 ZK connection
   * @param app
   *          reference to callback interface object
   * @param maxRetryNum maxRetryNum.
   * @param truststoreKeystore truststore keystore, that we will use for ZK if SSL/TLS is enabled
   * @throws IOException raised on errors performing I/O.
   * @throws HadoopIllegalArgumentException
   *         if valid data is not supplied.
   * @throws KeeperException
   *         other zookeeper operation errors.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,joinElection,org.apache.hadoop.ha.ActiveStandbyElector:joinElection(byte[]),315,334,"/**
 * Joins an election using provided data.
 * @param data Election data; must not be null.
 * @throws HadoopIllegalArgumentException if data is null.
 */
","* To participate in election, the app will call joinElection. The result will
   * be notified by a callback on either the becomeActive or becomeStandby app
   * interfaces. <br>
   * After this the elector will automatically monitor the leader status and
   * perform re-election if necessary<br>
   * The app could potentially start off in standby mode and ignore the
   * becomeStandby call.
   * 
   * @param data
   *          to be set by the app. non-null data must be set.
   * @throws HadoopIllegalArgumentException
   *           if valid data is not supplied",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reJoinElection,org.apache.hadoop.ha.ActiveStandbyElector:reJoinElection(int),798,823,"/**
 * Re-establishes ZK session after termination, waits, and rejoins election.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int,java.lang.String)",200,204,"/**
 * Creates an InetSocketAddress.
 * @param target Hostname or IP address.
 * @param defaultPort Default port number.
 * @param configName Configuration name.
 * @return InetSocketAddress object.
 */
","* Create an InetSocketAddress from the given target string and
   * default port. If the string cannot be parsed correctly, the
   * <code>configName</code> parameter is used as part of the
   * exception message, allowing the user to better diagnose
   * the misconfiguration.
   *
   * @param target a string of either ""host"" or ""host:port""
   * @param defaultPort the default port if <code>target</code> does not
   *                    include a port number
   * @param configName the name of the configuration from which
   *                   <code>target</code> was loaded. This is used in the
   *                   exception message in the case that parsing fails.
   * @return socket addr.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,invoke,"org.apache.hadoop.ipc.WritableRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",232,261,"/**
 * Invokes a remote method on the client.
 * @param proxy The proxy object.
 * @param method The method to invoke.
 * @param args Arguments for the method.
 * @return The result of the remote method call.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,invoke,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",212,294,"/**
 * Invokes a remote method on a service.
 * @param proxy Proxy object.
 * @param method Method to invoke.
 * @param args Arguments to the method.
 * @return Message object or null in async mode.
 * @throws ServiceException if an error occurs.
 */","* This is the client side invoker of RPC method. It only throws
     * ServiceException, since the invocation proxy expects only
     * ServiceException to be thrown by the method in case protobuf service.
     * 
     * ServiceException has the following causes:
     * <ol>
     * <li>Exceptions encountered on the client side in this method are 
     * set as cause in ServiceException as is.</li>
     * <li>Exceptions from the server are wrapped in RemoteException and are
     * set as cause in ServiceException</li>
     * </ol>
     * 
     * Note that the client calling protobuf RPC methods, must handle
     * ServiceException by getting the cause from the ServiceException. If the
     * cause is RemoteException, then unwrap it to get the exception thrown by
     * the server.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,invoke,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:invoke(java.lang.Object,java.lang.reflect.Method,java.lang.Object[])",220,304,"/**
 * Invokes a method on a remote service.
 * @param proxy The proxy object.
 * @param method The method to invoke.
 * @param args Method arguments.
 * @return Message object or null in async mode.
 * @throws ServiceException if an error occurs.
 */","* This is the client side invoker of RPC method. It only throws
     * ServiceException, since the invocation proxy expects only
     * ServiceException to be thrown by the method in case protobuf service.
     *
     * ServiceException has the following causes:
     * <ol>
     * <li>Exceptions encountered on the client side in this method are
     * set as cause in ServiceException as is.</li>
     * <li>Exceptions from the server are wrapped in RemoteException and are
     * set as cause in ServiceException</li>
     * </ol>
     *
     * Note that the client calling protobuf RPC methods, must handle
     * ServiceException by getting the cause from the ServiceException. If the
     * cause is RemoteException, then unwrap it to get the exception thrown by
     * the server.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,parseExpression,org.apache.hadoop.fs.shell.find.Find:parseExpression(java.util.Deque),272,332,"/**
 * Parses an expression from a deque of string arguments.
 * @param args deque of string arguments
 * @return Expression object representing parsed expression
 */
","* Parse a list of arguments to to extract the {@link Expression} elements.
   * The input Deque will be modified to remove the used elements.
   * 
   * @param args arguments to be parsed
   * @return list of {@link Expression} elements applicable to this command
   * @throws IOException if list can not be parsed",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printUsage,org.apache.hadoop.fs.FsShell:printUsage(java.io.PrintStream),193,195,"/**
 * Prints usage information to the specified output stream.
 * @param out PrintStream to write usage details to.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printUsage,"org.apache.hadoop.fs.FsShell:printUsage(java.io.PrintStream,java.lang.String)",198,200,"/**
 * Prints usage information to the given output stream.
 * @param out Output stream to print to.
 * @param cmd Command used.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printHelp,org.apache.hadoop.fs.FsShell:printHelp(java.io.PrintStream),203,205,"/**
 * Prints help information to the specified output stream.
 * @param out PrintStream to write help information to.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,printHelp,"org.apache.hadoop.fs.FsShell:printHelp(java.io.PrintStream,java.lang.String)",208,210,"/**
 * Prints help information to the specified output stream.
 * @param out PrintStream to write help text to
 * @param cmd Command name to include in help message
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,get,org.apache.hadoop.io.WritableComparator:get(java.lang.Class),55,57,"/**
 * Returns the comparator for the given WritableComparable class.
 * @param c Class of WritableComparable to get comparator for.
 */
","* For backwards compatibility.
   *
   * @param c WritableComparable Type.
   * @return WritableComparator.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ByteWritable.java,<init>,org.apache.hadoop.io.ByteWritable$Comparator:<init>(),88,90,"/**
 * Default constructor. Calls super with ByteWritable.class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IntWritable.java,<init>,org.apache.hadoop.io.IntWritable$Comparator:<init>(),90,92,"/**
 * Constructs a Comparator, using IntWritable for comparison.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableComparator.java,<init>,org.apache.hadoop.io.WritableComparator:<init>(),124,126,"/**
 * Default constructor. Calls the single-argument constructor.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/Text.java,<init>,org.apache.hadoop.io.Text$Comparator:<init>(),429,431,"/**
 * Constructs a Comparator using the Text class for comparison.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/NullWritable.java,<init>,org.apache.hadoop.io.NullWritable$Comparator:<init>(),62,64,"/**
 * Default constructor. Calls superclass constructor with NullWritable.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/LongWritable.java,<init>,org.apache.hadoop.io.LongWritable$Comparator:<init>(),90,92,"/**
 * Constructs a Comparator, using LongWritable for comparison.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DoubleWritable.java,<init>,org.apache.hadoop.io.DoubleWritable$Comparator:<init>(),88,90,"/**
 * Default constructor. Calls super with DoubleWritable class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MD5Hash.java,<init>,org.apache.hadoop.io.MD5Hash$Comparator:<init>(),249,251,"/**
 * Constructs a Comparator, initializing it with MD5Hash class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ShortWritable.java,<init>,org.apache.hadoop.io.ShortWritable$Comparator:<init>(),98,100,"/**
 * Constructs a Comparator using ShortWritable for comparison.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/FloatWritable.java,<init>,org.apache.hadoop.io.FloatWritable$Comparator:<init>(),85,87,"/**
* Default constructor. Calls super with FloatWritable.class.
*/",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BytesWritable.java,<init>,org.apache.hadoop.io.BytesWritable$Comparator:<init>(),224,226,"/**
 * Default constructor. Calls super with BytesWritable.class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BooleanWritable.java,<init>,org.apache.hadoop.io.BooleanWritable$Comparator:<init>(),111,113,"/**
 * Default constructor. Calls super with BooleanWritable class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/UTF8.java,<init>,org.apache.hadoop.io.UTF8$Comparator:<init>(),212,214,"/**
 * Constructs a Comparator, initializing it with the UTF8 class.
 */",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,authenticate,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",380,410,"/**
 * Authenticates a request, using delegation token or fallback handler.
 * @param request HTTP request
 * @param response HTTP response
 * @return AuthenticationToken or null if authentication fails
 */
","* Authenticates a request looking for the <code>delegation</code>
   * query-string parameter and verifying it is a valid token. If there is not
   * <code>delegation</code> query-string parameter, it delegates the
   * authentication to the {@link KerberosAuthenticationHandler} unless it is
   * disabled.
   *
   * @param request the HTTP client request.
   * @param response the HTTP client response.
   * @return the authentication token for the authenticated request.
   * @throws IOException thrown if an IO error occurred.
   * @throws AuthenticationException thrown if the authentication failed.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,<init>,"org.apache.hadoop.ha.NodeFencer:<init>(org.apache.hadoop.conf.Configuration,java.lang.String)",78,81,"/**
 * Parses fencing methods from configuration and specification.
 * @param conf Configuration object.
 * @param spec Fencing specification string.
 * @throws BadFencingConfigurationException on parsing failure.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleDeprecation,"org.apache.hadoop.conf.Configuration:handleDeprecation(org.apache.hadoop.conf.Configuration$DeprecationContext,java.lang.String)",724,762,"/**
 * Handles deprecated keys, logs warnings, and updates property names.
 * @param deprecations Deprecation context containing key info.
 * @param name Name to handle; trimmed if not null.
 * @return Array of property names, potentially updated.
 */
","* Checks for the presence of the property <code>name</code> in the
   * deprecation map. Returns the first of the list of new keys if present
   * in the deprecation map or the <code>name</code> itself. If the property
   * is not presently set but the property map contains an entry for the
   * deprecated key, the value of the deprecated key is set as the value for
   * the provided property name.
   *
   * Also updates properties and overlays with deprecated keys, if the new
   * key does not already exist.
   *
   * @param deprecations deprecation context
   * @param name the property name
   * @return the first property in the list of properties mapping
   *         the <code>name</code> or the <code>name</code> itself.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,setConfAsEnvVars,org.apache.hadoop.ha.ShellCommandFencer:setConfAsEnvVars(java.util.Map),207,211,"/**
 * Sets configuration key-value pairs into the provided environment map.
 */","* Set the environment of the subprocess to be the Configuration,
   * with '.'s replaced by '_'s.",,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,setHeaders,org.apache.hadoop.http.HttpServer2:setHeaders(org.apache.hadoop.conf.Configuration),1958,1970,"/**
 * Creates a map of HTTP headers from configuration.
 * @param conf Configuration object containing header settings.
 * @return Map of HTTP header key-value pairs.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HttpCrossOriginFilterInitializer.java,getFilterParameters,"org.apache.hadoop.security.HttpCrossOriginFilterInitializer:getFilterParameters(org.apache.hadoop.conf.Configuration,java.lang.String)",54,65,"/**
 * Extracts filter parameters from configuration with a prefix.
 * @param conf Configuration object.
 * @param prefix Parameter prefix to extract.
 * @return Map of filter parameter names and values.
 */
",,,,True,9
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/SetReplication.java,processArguments,org.apache.hadoop.fs.shell.SetReplication:processArguments(java.util.LinkedList),74,79,"/**
 * Processes arguments, optionally waiting for replication.
 * @param args List of PathData objects to process.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.DelegateToFileSystem:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",117,122,"/**
 * Gets block locations for a file within a given range.
 * @param f Path to the file.
 * @param start Start offset in bytes.
 * @param len Length of the block in bytes.
 * @return BlockLocation array.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",1512,1535,"/**
 * Gets block locations for a file, potentially falling back to a linked FS.
 * @param fs FileStatus object
 * @param start Starting offset
 * @param len Length of data
 * @return BlockLocation array
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,renewToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:renewToken(org.apache.hadoop.security.token.Token,java.lang.String)",190,196,"/**
 * Renews a token with the secret manager.
 * @param token The token to renew.
 * @param renewer The renewer string.
 * @return Renewed token's expiry timestamp.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",198,206,"/**
 * Cancels a token using the provided canceler or short username.
 * @param token The token to cancel.
 * @param canceler The canceler identifier, or null to use username.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",154,164,"/**
 * Cancels a token.
 * @param token Token to cancel.
 * @param canceller Cancelling entity.
 * @return TokenIdent of the cancelled token.
 */
","* Cancels a token by removing it from the SQL database. This will
   * call the corresponding method in {@link AbstractDelegationTokenSecretManager}
   * to perform validation and remove the token from the cache.
   * @return Identifier of the canceled token",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,getGroups,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:getGroups(java.lang.String),52,58,"/**
 * Retrieves a list of groups for a user, incorporating netgroup data.
 * @param user The username to retrieve groups for.
 * @return A list of group names.
 */
","* Get unix groups (parent) and netgroups for given user
   *
   * @param user get groups and netgroups for this user
   * @return groups and netgroups for user",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,unTar,"org.apache.hadoop.fs.FileUtil:unTar(java.io.File,java.io.File)",1015,1033,"/**
 * Extracts a tar archive to the specified directory.
 * @param inFile Tar file to extract.
 * @param untarDir Directory to extract to.
 */
","* Given a Tar File as input it will untar the file in a the untar directory
   * passed as the second parameter
   *
   * This utility will untar "".tar"" files and "".tar.gz"",""tgz"" files.
   *
   * @param inFile The tar file as input.
   * @param untarDir The untar directory where to untar the tar file.
   * @throws IOException an exception occurred.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,chmod,"org.apache.hadoop.fs.FileUtil:chmod(java.lang.String,java.lang.String)",1289,1292,"/**
 * Changes the file mode bits.
 * @param filename file to modify
 * @param perm permission string (e.g., ""u+rwx"")
 * @return Exit code of chmod command.
 */
","* Change the permissions on a filename.
   * @param filename the name of the file to change
   * @param perm the permission string
   * @return the exit code from the command
   * @throws IOException raised on errors performing I/O.
   * @throws InterruptedException command interrupted.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setReadable,"org.apache.hadoop.fs.FileUtil:setReadable(java.io.File,boolean)",1347,1359,"/**
 * Sets the readability of a file. Returns true on success, false otherwise.
 */
","* Platform independent implementation for {@link File#setReadable(boolean)}
   * File#setReadable does not work as expected on Windows.
   * @param f input file
   * @param readable readable.
   * @return true on success, false otherwise",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setWritable,"org.apache.hadoop.fs.FileUtil:setWritable(java.io.File,boolean)",1368,1380,"/**
 * Sets the writable flag of a file. Returns true on success,
 * false on failure (Windows only uses FileUtil.chmod).
 */
","* Platform independent implementation for {@link File#setWritable(boolean)}
   * File#setWritable does not work as expected on Windows.
   * @param f input file
   * @param writable writable.
   * @return true on success, false otherwise",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setExecutable,"org.apache.hadoop.fs.FileUtil:setExecutable(java.io.File,boolean)",1392,1404,"/**
 * Sets the executable flag on a file. Windows uses FileUtil.chmod.
 * @param f The file to modify.
 * @param executable True to set executable, false to remove it.
 * @return True on success, false on failure.
 */
","* Platform independent implementation for {@link File#setExecutable(boolean)}
   * File#setExecutable does not work as expected on Windows.
   * Note: revoking execute permission on folders does not have the same
   * behavior on Windows as on Unix platforms. Creating, deleting or renaming
   * a file within that folder will still succeed on Windows.
   * @param f input file
   * @param executable executable.
   * @return true on success, false otherwise",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,refreshIfNeeded,org.apache.hadoop.util.SysInfoWindows:refreshIfNeeded(),94,141,"/**
 * Refreshes system information if the refresh interval has passed.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,loadPermissionInfo,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:loadPermissionInfo(),983,995,"/**
 * Loads permission information, first via native IO if available.
 */","* Load file permission information (UNIX symbol rwxrwxrwx, sticky bit info).
     *
     * To improve peformance, give priority to native stat() call. First try get
     * permission information by using native JNI call then fall back to use non
     * native (ProcessBuilder) call in case native lib is not loaded or native
     * call is not successful",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,setOwner,"org.apache.hadoop.fs.RawLocalFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1099,1103,"/**
* Sets the owner of a file.
* @param p Path to the file. username, groupname: new owner.
* @throws IOException if an I/O error occurs.
*/
",* Use the command chown to set owner.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,setPermission,"org.apache.hadoop.fs.FileUtil:setPermission(java.io.File,org.apache.hadoop.fs.permission.FsPermission)",1470,1508,"/**
 * Sets the permission of a file using native or fallback methods.
 * @param f the file to set permissions on
 * @param permission the FsPermission object defining permissions
 */
","* Set permissions to the required value. Uses the java primitives instead
   * of forking if group == other.
   * @param f the file to change
   * @param permission the new permissions
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,cacheGroupsAdd,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsAdd(java.util.List),75,88,"/**
 * Adds netgroups to the cache, caching only those starting with '@'.
 * @param groups List of group names to add to the cache.
 */
","* Add a group to cache, only netgroups are cached
   *
   * @param groups list of group names to add to cache",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dumpTokens,org.apache.hadoop.security.KDiag:dumpTokens(org.apache.hadoop.security.UserGroupInformation),811,819,"/**
 * Dumps tokens associated with the given UserGroupInformation.
 * @param ugi UserGroupInformation to retrieve tokens from.
 */
","* Dump all tokens of a UGI.
   * @param ugi UGI to examine",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logUserInfo,"org.apache.hadoop.security.UserGroupInformation:logUserInfo(org.slf4j.Logger,java.lang.String,org.apache.hadoop.security.UserGroupInformation)",1980,1991,"/**
 * Logs user info with credentials for debugging purposes.
 * @param log Logger instance
 * @param caption Descriptive caption for the log message
 * @param ugi UserGroupInformation object
 */
","* Log current UGI and token information into specified log.
   * @param ugi - UGI
   * @param log log.
   * @param caption caption.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,containsKmsDt,org.apache.hadoop.crypto.key.kms.KMSClientProvider:containsKmsDt(org.apache.hadoop.security.UserGroupInformation),1155,1165,"/**
 * Checks if UGI contains a KMS delegation token.
 * @param ugi UserGroupInformation to check.
 * @return True if a token exists, false otherwise.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDir,"org.apache.hadoop.util.DiskChecker:checkDir(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",113,117,"/**
 * Checks directory permissions using internal helper.
 * @param localFS File system.
 * @param dir Path to check.
 * @param expected Expected permissions.
 */
","* Create the local directory if necessary, check permissions and also ensure
   * it can be read from and written into.
   *
   * @param localFS local filesystem
   * @param dir directory
   * @param expected permission
   * @throws DiskErrorException disk problem.
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/DiskChecker.java,checkDirWithDiskIo,"org.apache.hadoop.util.DiskChecker:checkDirWithDiskIo(org.apache.hadoop.fs.LocalFileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",131,136,"/**
 * Checks directory permissions and performs disk I/O.
 * @param localFS File system. @param dir Directory to check.
 * @param expected Expected permissions.
 */
","* Create the local directory if necessary, also ensure permissions
   * allow it to be read from and written into. Perform some diskIO
   * to ensure that the disk is usable for writes. 
   *
   * @param localFS local filesystem
   * @param dir directory
   * @param expected permission
   * @throws DiskErrorException disk problem.
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,listStatus,org.apache.hadoop.fs.FileContext$Util:listStatus(org.apache.hadoop.fs.Path[]),1823,1826,"/**
 * Lists the status of files in the given paths.
 * @param files array of paths to check
 * @return FileStatus array for each path
 */
","* See {@link #listStatus(Path[], PathFilter)}
     *
     * @param files files.
     * @throws AccessControlException If access is denied.
     * @throws FileNotFoundException If <code>files</code> does not exist.
     * @throws IOException If an I/O error occurred.
     * @return file status array.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getEnclosingRoot,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getEnclosingRoot(org.apache.hadoop.fs.Path),1481,1496,"/**
 * Finds the enclosing root path of a given path.
 * @param path The path to resolve.
 * @return The enclosing root Path.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,append,"org.apache.hadoop.fs.viewfs.ViewFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",447,453,"/**
 * Appends data to a file.
 * @param f Path to the file.
 * @param bufferSize Buffer size for appending.
 * @param progress Progressable object for monitoring.
 * @return FSDataOutputStream for appending.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.viewfs.ViewFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",455,468,"/**
 * Creates a non-recursive FSDataOutputStream.
 * @param f Path to create, permission, flags, buffersize, etc.
 * @return FSDataOutputStream or throws IOException
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,create,"org.apache.hadoop.fs.viewfs.ViewFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",470,483,"/**
 * Creates a data output stream at the specified path.
 * @param f Path to create stream on
 * @return FSDataOutputStream
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,"org.apache.hadoop.fs.viewfs.ViewFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",486,496,"/**
 * Deletes a file or directory.
 * @param f The path to delete.
 * @param recursive If true, deletes recursively.
 * @return True if successful, false otherwise.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),514,521,"/**
 * Gets the checksum for a file.
 * @param f Path to the file.
 * @return FileChecksum object.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileChecksum,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",523,530,"/**
 * Gets the file checksum for a given path and length.
 * @param f Path to file, @param length file length
 * @return FileChecksum object
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listLocatedStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",630,655,"/**
 * Lists located file statuses for a path, filtered by the given filter.
 * @param f path to list
 * @param filter PathFilter to apply
 * @return RemoteIterator of LocatedFileStatus objects
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,mkdirs,org.apache.hadoop.fs.viewfs.ViewFileSystem:mkdirs(org.apache.hadoop.fs.Path),670,675,"/**
 * Creates directories recursively.
 * @param dir Path representing the directory to create.
 * @return True if directories were created, false otherwise.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.ViewFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",677,683,"/**
 * Creates directories recursively with specified permissions.
 * @param dir The directory to create.
 * @param permission FsPermission object for directory permissions.
 * @return True if directories were created, false otherwise.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,open,"org.apache.hadoop.fs.viewfs.ViewFileSystem:open(org.apache.hadoop.fs.Path,int)",685,691,"/**
 * Opens a file for reading.
 * @param f Path to the file.
 * @param bufferSize I/O buffer size.
 * @return FSDataInputStream for reading the file.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,truncate,"org.apache.hadoop.fs.viewfs.ViewFileSystem:truncate(org.apache.hadoop.fs.Path,long)",800,806,"/**
 * Truncates a file to the specified length.
 * @param f The file to truncate.
 * @param newLength The new length of the file.
 * @return True if successful, false otherwise.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setOwner,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",808,815,"/**
 * Sets the owner of a file system object.
 * @param f Path to the object, username, groupname.
 * @throws AccessControlException, FileNotFoundException, IOException
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setPermission,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",817,823,"/**
 * Sets the permission for a file or directory.
 * @param f Path of the file/directory.
 * @param permission FsPermission to set.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setReplication,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",825,831,"/**
 * Sets the replication factor for a file.
 * @param f Path to the file. @param replication New replication factor.
 * @return True on success.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setTimes,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",833,839,"/**
 * Sets modification and access times for a file.
 * @param f Path to the file, mtime modification time, atime access time.
 * @throws AccessControlException, FileNotFoundException, IOException
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",841,847,"/**
* Modifies ACL entries for a given path.
* @param path The path to modify.
* @param aclSpec List of ACL entries to apply.
*/
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ViewFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",849,855,"/**
 * Removes ACL entries from a path in the filesystem.
 * @param path The path to remove ACL entries from.
 * @param aclSpec List of ACL entries to remove.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path),857,863,"/**
 * Removes the default ACL for a given path in the file system.
 * @param path The path for which to remove the default ACL.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeAcl,org.apache.hadoop.fs.viewfs.ViewFileSystem:removeAcl(org.apache.hadoop.fs.Path),865,871,"/**
 * Removes ACL entries for a given path.
 * @param path The path for which to remove ACLs.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setAcl,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",873,878,"/**
 * Sets the ACL for a given path using the filesystem's setAcl method.
 * @param path The path for which to set the ACL.
 * @param aclSpec The ACL entries to apply.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getAclStatus(org.apache.hadoop.fs.Path),880,885,"/**
 * Gets the ACL status for a given path.
 * @param path The path to check.
 * @return AclStatus object.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",887,893,"/**
 * Sets an extended attribute on a file.
 * @param path Path to the file.
 * @param name Attribute name.
 * @param value Attribute value.
 * @param flag Attribute set flags.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",895,900,"/**
 * Gets an extended attribute by name from a Path.
 * @param path The Path to check.
 * @param name Attribute name.
 * @return Byte array of the attribute value.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttrs(org.apache.hadoop.fs.Path),902,907,"/**
 * Retrieves extended attributes for a given path.
 * @param path Path to retrieve xattrs for.
 * @return Map of xattr names and values.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",909,915,"/**
 * Retrieves extended attributes for a path.
 * @param path The path to retrieve attributes from.
 * @param names Attribute names to fetch.
 * @return Map of attribute names to byte array values.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listXAttrs,org.apache.hadoop.fs.viewfs.ViewFileSystem:listXAttrs(org.apache.hadoop.fs.Path),917,922,"/**
 * Lists extended attributes for a given path.
 * @param path Path to list XAttrs for.
 * @return List of extended attribute names.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ViewFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",924,929,"/**
 * Removes an extended attribute from a file.
 * @param path Path to the file.
 * @param name Name of the attribute to remove.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),990,1002,"/**
 * Gets the default replication for a file path.
 * @param f the file path to check
 * @throws NotInMountpointException if file is not in a mountpoint
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getContentSummary,org.apache.hadoop.fs.viewfs.ViewFileSystem:getContentSummary(org.apache.hadoop.fs.Path),1015,1020,"/**
 * Gets content summary for a file path.
 * @param f Path to the file.
 * @return ContentSummary object.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getQuotaUsage,org.apache.hadoop.fs.viewfs.ViewFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path),1022,1027,"/**
* Gets quota usage for a file.
* @param f Path object representing the file.
* @return QuotaUsage object.
*/
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1072,1078,"/**
 * Creates a snapshot of a file system at a given path.
 * @param path Path to snapshot.
 * @param snapshotName Snapshot name.
 * @return Path to the created snapshot.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",1080,1087,"/**
 * Renames a snapshot within a file system.
 * @param path Path to snapshot.
 * @param snapshotOldName Old snapshot name.
 * @param snapshotNewName New snapshot name.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ViewFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",1089,1095,"/**
 * Deletes a snapshot from the filesystem at the given path.
 * @param path Path to the snapshot.
 * @param snapshotName Name of the snapshot to delete.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path),1097,1102,"/**
 * Applies storage policy to a path within a file system.
 * @param src Path to apply the storage policy to.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ViewFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",1104,1109,"/**
 * Sets the storage policy for a path in the filesystem.
 * @param src source path
 * @param policyName name of the storage policy
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path),1111,1116,"/**
 * Removes the storage policy from a path.
 * @param src Path for which to unset the storage policy.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ViewFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path),1118,1123,"/**
 * Gets the storage policy for a given path.
 * @param src Path to retrieve the storage policy for.
 * @return Storage policy associated with the path.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getStatus(org.apache.hadoop.fs.Path),1304,1312,"/**
 * Gets the status of a path.
 * @param p The path to get the status of.
 * @return The status of the path.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getUsed,org.apache.hadoop.fs.viewfs.ViewFileSystem:getUsed(),1321,1330,"/**
 * Gets the used space of the file system at the given URI.
 * @return Used space in bytes, throws exception if not a mount.
 */
","* Return the total size of all files under ""/"", if {@link
   * Constants#CONFIG_VIEWFS_LINK_MERGE_SLASH} is supported and is a valid
   * mount point. Else, throw NotInMountpointException.
   *
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ViewFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),1332,1341,"/**
 * Resolves the target path of a link.
 * @param path The path to resolve.
 * @return The resolved Path object.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.HarFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",468,481,"/**
 * Retrieves block locations for a file within an archive.
 * @param file FileStatus object
 * @param start Start offset
 * @param len Length of the requested block
 * @return BlockLocation[] array
 */
","* Get block locations from the underlying fs and fix their
   * offsets and lengths.
   * @param file the input file status to get block locations
   * @param start the start of the desired range in the contained file
   * @param len the length of the desired range
   * @return block locations for this segment of file
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,open,"org.apache.hadoop.fs.HarFileSystem:open(org.apache.hadoop.fs.Path,int)",679,690,"/**
 * Opens an FSDataInputStream for a file.
 * @param f Path to the file
 * @param bufferSize I/O buffer size
 * @throws IOException if an I/O error occurs
 */
","* Returns a har input stream which fakes end of 
   * file. It reads the index files to get the part 
   * file name and the size and start of the file.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,<init>,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFs,org.apache.hadoop.fs.Path)",148,151,"/**
 * Constructs a ChecksumFSInputChecker with default buffer size.
 * @param fs ChecksumFs instance
 * @param file Path to the file
 * @throws IOException, UnresolvedLinkException
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,open,"org.apache.hadoop.fs.ChecksumFs:open(org.apache.hadoop.fs.Path,int)",334,339,"/**
 * Opens an input stream for the given file.
 * @param f Path to the file. @param bufferSize Buffer size.
 * @return FSDataInputStream for reading data.
 */
","* Opens an FSDataInputStream at the indicated Path.
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,createInternal,"org.apache.hadoop.fs.ChecksumFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",418,428,"/**
 * Creates a data output stream for writing to a file.
 * @param f Path to the file
 * @return FSDataOutputStream
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,processArguments,org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:processArguments(java.util.LinkedList),81,94,"/**
 * Processes argument paths, potentially using a thread pool.
 * Initializes/waits for executor if multi-threading is needed.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,open,"org.apache.hadoop.fs.ChecksumFileSystem:open(org.apache.hadoop.fs.Path,int)",566,578,"/**
 * Opens an input stream for a given path, with optional checksum verification.
 * @param f Path to open
 * @param bufferSize Buffer size for the stream
 * @return FSDataInputStream object
 */
","* Opens an FSDataInputStream at the indicated Path.
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.
   * @throws IOException if an I/O error occurs.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,create,"org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable)",704,734,"/**
 * Creates a data output stream for the given file path.
 * @param f path to create, permission, overwrite, etc.
 * @return FSDataOutputStream or null if creation fails.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,isValidName,org.apache.hadoop.fs.viewfs.ChRootedFs:isValidName(java.lang.String),97,100,"/**
 * Checks if a file name is valid.
 * @param src The file name string to validate.
 * @return True if the name is valid, false otherwise.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,createInternal,"org.apache.hadoop.fs.viewfs.ChRootedFs:createInternal(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.permission.FsPermission,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt,boolean)",173,182,"/**
 * Creates an internal data output stream.
 * @param f Path to create stream on, flags, permissions, etc.
 * @return FSDataOutputStream
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,delete,"org.apache.hadoop.fs.viewfs.ChRootedFs:delete(org.apache.hadoop.fs.Path,boolean)",184,188,"/**
 * Deletes a file or directory.
 * @param f Path to delete.
 * @param recursive If true, deletes recursively.
 * @return True if successful, false otherwise.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ChRootedFs:getFileBlockLocations(org.apache.hadoop.fs.Path,long,long)",190,194,"/**
 * Gets block locations for a file.
 * @param f Path to file. @param start Start offset. @param len Length.
 * @return BlockLocation array.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ChRootedFs:getFileChecksum(org.apache.hadoop.fs.Path),196,200,"/**
 * Gets the file checksum for a given file path.
 * @param f Path to the file.
 * @return FileChecksum object.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:getFileStatus(org.apache.hadoop.fs.Path),202,206,"/**
 * Gets the file status for the given path.
 * @param f Path to get the file status for.
 * @return FileStatus object.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getFileLinkStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:getFileLinkStatus(org.apache.hadoop.fs.Path),213,217,"/**
 * Gets the status of a file link.
 * @param f Path to the file link.
 * @return FileStatus object or throws exception.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ChRootedFs:getServerDefaults(org.apache.hadoop.fs.Path),230,233,"/**
 * Retrieves server defaults for a given path.
 * @param f Path to retrieve defaults for.
 * @return FsServerDefaults object.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:listStatus(org.apache.hadoop.fs.Path),240,244,"/**
 * Lists the status of files and directories under the given path.
 * @param f the path to list
 * @return FileStatus array or null if empty
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listStatusIterator,org.apache.hadoop.fs.viewfs.ChRootedFs:listStatusIterator(org.apache.hadoop.fs.Path),246,250,"/**
 * Returns an iterator for file statuses under the given path.
 * @param f The path to list file statuses for.
 * @return A RemoteIterator of FileStatus objects.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:listLocatedStatus(org.apache.hadoop.fs.Path),252,256,"/**
 * Lists located status for a path.
 * @param f Path to list.
 * @return RemoteIterator of LocatedFileStatus.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,mkdir,"org.apache.hadoop.fs.viewfs.ChRootedFs:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",258,263,"/**
 * Creates a directory with specified permissions and parent creation.
 * @param dir The directory to create.
 * @param permission FsPermission for the new directory.
 * @param createParent Whether to create parent directories if needed.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,open,"org.apache.hadoop.fs.viewfs.ChRootedFs:open(org.apache.hadoop.fs.Path,int)",265,269,"/**
 * Opens a file for input.
 * @param f Path to the file.
 * @param bufferSize I/O buffer size.
 * @return FSDataInputStream for reading.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,truncate,"org.apache.hadoop.fs.viewfs.ChRootedFs:truncate(org.apache.hadoop.fs.Path,long)",271,275,"/**
 * Truncates a file to the specified length.
 * @param f Path to the file. @param newLength New file length.
 * @return True if successful, false otherwise.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",277,283,"/**
 * Renames a file or directory from src to dst within the filesystem.
 * @param src Source path
 * @param dst Destination path
 * @throws IOException, UnresolvedLinkException
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ChRootedFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",285,292,"/**
 * Renames a file or directory from src to dst, overwriting if true.
 * @param src Source path
 * @param dst Destination path
 * @param overwrite Overwrite existing file/directory
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setOwner,"org.apache.hadoop.fs.viewfs.ChRootedFs:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",294,300,"/**
 * Sets the owner of a file.
 * @param f file path, username, and groupname.
 * @throws IOException, UnresolvedLinkException
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setPermission,"org.apache.hadoop.fs.viewfs.ChRootedFs:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",302,306,"/**
* Sets the file system permission for a given path.
* @param f Path to set permission on.
* @param permission FsPermission to apply.
*/
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setReplication,"org.apache.hadoop.fs.viewfs.ChRootedFs:setReplication(org.apache.hadoop.fs.Path,short)",308,312,"/**
 * Sets the replication factor for a file.
 * @param f Path to the file. @param replication New replication count.
 * @return True if successful, false otherwise.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setTimes,"org.apache.hadoop.fs.viewfs.ChRootedFs:setTimes(org.apache.hadoop.fs.Path,long,long)",314,318,"/**
 * Sets modification and access times for a file.
 * @param f Path to the file. mtime, atime are timestamps.
 * @throws IOException, UnresolvedLinkException
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFs:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",320,324,"/**
 * Modifies ACL entries for a given path.
 * @param path The path to modify.
 * @param aclSpec List of ACL entries to apply.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFs:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",326,330,"/**
 * Removes ACL entries from a path.
 * @param path The path to remove ACL entries from.
 * @param aclSpec List of ACL entries to remove.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ChRootedFs:removeDefaultAcl(org.apache.hadoop.fs.Path),332,335,"/**
 * Removes the default ACL for the given path.
 * @param path The path for which to remove the default ACL.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeAcl,org.apache.hadoop.fs.viewfs.ChRootedFs:removeAcl(org.apache.hadoop.fs.Path),337,340,"/**
 * Removes ACL entries for the given path.
 * @param path The path for which to remove ACLs.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setAcl,"org.apache.hadoop.fs.viewfs.ChRootedFs:setAcl(org.apache.hadoop.fs.Path,java.util.List)",342,345,"/**
 * Sets the ACL for a given path.
 * @param path The path to set the ACL for.
 * @param aclSpec ACL entries to apply.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getAclStatus,org.apache.hadoop.fs.viewfs.ChRootedFs:getAclStatus(org.apache.hadoop.fs.Path),347,350,"/**
 * Gets the ACL status for a given path.
 * @param path the path to check
 * @return AclStatus object representing the ACL status.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFs:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",352,356,"/**
 * Sets an extended attribute on a path.
 * @param path Path to set the attribute on.
 * @param name Attribute name.
 * @param value Attribute value.
 * @param flag Attribute set flags.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",358,361,"/**
 * Retrieves extended attribute for a given path and name.
 * @param path Path object
 * @param name Attribute name
 * @return Byte array representing the attribute value
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttrs(org.apache.hadoop.fs.Path),363,366,"/**
 * Retrieves extended attributes for a given path.
 * @param path The path to retrieve attributes for.
 * @return Map of attribute names and values.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ChRootedFs:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",368,372,"/**
 * Retrieves extended attributes for a path.
 * @param path The path to retrieve attributes from.
 * @param names Attribute names to fetch.
 * @return Map of attribute names and values.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,listXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFs:listXAttrs(org.apache.hadoop.fs.Path),374,377,"/**
 * Lists extended attributes for a given path.
 * @param path Path to list attributes for.
 * @return List of extended attribute names.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFs:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",379,382,"/**
 * Removes an extended attribute from a path.
 * @param path The path to remove the attribute from.
 * @param name The name of the attribute to remove.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFs:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",384,387,"/**
 * Creates a snapshot of the given path.
 * @param path The path to snapshot.
 * @param name Snapshot name.
 * @return Path to the created snapshot.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFs:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",389,393,"/**
 * Renames a snapshot.
 * @param path Snapshot path.
 * @param snapshotOldName Old snapshot name.
 * @param snapshotNewName New snapshot name.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFs:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",395,399,"/**
 * Deletes a snapshot using the provided directory and name.
 * @param snapshotDir Snapshot directory path
 * @param snapshotName Snapshot name to delete
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ChRootedFs:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",406,410,"/**
 * Sets the storage policy for a given path.
 * @param path The path to set the policy on.
 * @param policyName The name of the storage policy.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFs:unsetStoragePolicy(org.apache.hadoop.fs.Path),412,416,"/**
 * Removes the storage policy from the given path.
 * @param src Path from which to remove the policy.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,createSymlink,"org.apache.hadoop.fs.viewfs.ChRootedFs:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",441,451,"/**
 * Creates a symbolic link from {@code link} to {@code target}.
 * @param target The target Path.
 * @param link The link Path.
 * @param createParent Whether to create parent directories.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ChRootedFs:getLinkTarget(org.apache.hadoop.fs.Path),453,456,"/**
 * Gets the link target of a file.
 * @param f Path object representing the file.
 * @return Path object representing the link target.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",568,636,"/**
* Renames a file or directory from src to dst, handling internal dirs.
* @param src Source Path
* @param dst Destination Path
* @param overwrite Overwrites destination if it exists
*/
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,next,org.apache.hadoop.fs.viewfs.ViewFs$WrappingRemoteIterator:next(),945,952,"/**
 * Returns the next file status in the iteration.
 * Returns a FileStatus object, throws IOException on failure.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getChrootedPath,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getChrootedPath(org.apache.hadoop.fs.viewfs.InodeTree$ResolveResult,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",657,668,"/**
 * Gets the path, stripping the root based on the file system type.
 * @param res ResolveResult containing resolved path and file system.
 * @param status FileStatus object.
 * @param f Path object.
 * @return Qualified path after root stripping.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,renameInternal,"org.apache.hadoop.fs.ChecksumFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",480,497,"/**
 * Renames a file or directory from src to dst.
 * @param src Source path
 * @param dst Destination path
 */
",* Rename files/dirs.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,renameInternal,"org.apache.hadoop.fs.ChecksumFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",499,523,"/**
 * Renames a file or directory. Overwrites if specified.
 * @param src Source path.
 * @param dst Destination path.
 * @param overwrite Whether to overwrite existing files.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,rename,"org.apache.hadoop.fs.FileContext:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Options$Rename[])",1031,1060,"/**
 * Renames a file or directory from src to dst, supporting options.
 * @param src Source path to rename.
 * @param dst Destination path.
 * @param options Rename options.
 */
","* Renames Path src to Path dst
   * <ul>
   * <li>Fails if src is a file and dst is a directory.
   * <li>Fails if src is a directory and dst is a file.
   * <li>Fails if the parent of dst does not exist or is a file.
   * </ul>
   * <p>
   * If OVERWRITE option is not passed as an argument, rename fails if the dst
   * already exists.
   * <p>
   * If OVERWRITE option is passed as an argument, rename overwrites the dst if
   * it is a file or an empty directory. Rename fails if dst is a non-empty
   * directory.
   * <p>
   * Note that atomicity of rename is dependent on the file system
   * implementation. Please refer to the file system documentation for details
   * <p>
   * 
   * @param src path to be renamed
   * @param dst new path after rename
   * @param options rename options.
   * 
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If <code>dst</code> already exists and
   *           <code>options</code> has {@link Options.Rename#OVERWRITE}
   *           option false.
   * @throws FileNotFoundException If <code>src</code> does not exist
   * @throws ParentNotDirectoryException If parent of <code>dst</code> is not a
   *           directory
   * @throws UnsupportedFileSystemException If file system for <code>src</code>
   *           and <code>dst</code> is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,renameInternal,"org.apache.hadoop.fs.FilterFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",245,251,"/**
 * Renames a file or directory from src to dst.
 * @param src Source path
 * @param dst Destination path
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,fileStatusesInIndex,"org.apache.hadoop.fs.HarFileSystem:fileStatusesInIndex(org.apache.hadoop.fs.HarFileSystem$HarStatus,java.util.List)",511,522,"/**
 * Adds file statuses for children of a HarStatus to the list.
 * @param parent The parent HarStatus.
 * @param statuses List to add file statuses to.
 */
","* Get filestatuses of all the children of a given directory. This just reads
   * through index file and reads line by line to get all statuses for children
   * of a directory. Its a brute force way of getting all such filestatuses
   * 
   * @param parent
   *          the parent path directory
   * @param statuses
   *          the list to add the children filestatuses to",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getFileStatus,org.apache.hadoop.fs.HarFileSystem:getFileStatus(org.apache.hadoop.fs.Path),640,644,"/**
 * Gets the file status for the given path.
 * @param f the path to get the file status for
 * @return FileStatus object representing the file status
 */
","* return the filestatus of files in har archive.
   * The permission returned are that of the archive
   * index files. The permissions are not persisted 
   * while creating a hadoop archive.
   * @param f the path in har filesystem
   * @return filestatus.
   * @throws IOException raised on errors performing I/O.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,deprecatedGetFileStatus,org.apache.hadoop.fs.RawLocalFileSystem:deprecatedGetFileStatus(org.apache.hadoop.fs.Path),910,919,"/**
 * Gets the file status for a path. Deprecated.
 * @param f Path to get file status for.
 * @throws IOException if file doesn't exist or error occurs.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocatedFileStatus.java,<init>,org.apache.hadoop.fs.LocatedFileStatus:<init>(),40,42,"/**
 * Default constructor for LocatedFileStatus.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyStatus:<init>(org.apache.hadoop.fs.viewfs.ChRootedFileSystem,org.apache.hadoop.fs.FileStatus)",464,468,"/**
 * Initializes NflyStatus with a FileStatus and ChRootedFileSystem.
 * @param realFs The ChRootedFileSystem.
 * @param realStatus The FileStatus.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsFileStatus.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFsFileStatus:<init>(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",35,38,"/**
 * Initializes a ViewFsFileStatus with a FileStatus and new path.
 * @param fs The FileStatus to wrap.
 * @param newPath The new path for the ViewFs.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",205,250,"/**
 * Gets the FileStatus for a file on the SFTP server.
 * @param client SFTP client connection
 * @param file Path to the file
 * @return FileStatus object or throws FileNotFoundException
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getFileStatus,org.apache.hadoop.fs.http.AbstractHttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path),113,116,"/**
 * Gets the status of a file.
 * @param path Path to the file.
 * @return FileStatus object representing the file.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,doGlob,org.apache.hadoop.fs.Globber:doGlob(),208,396,"/**
 * Lists files matching a glob pattern.
 * Returns FileStatus[] or null if no match.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,notFoundStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:notFoundStatus(org.apache.hadoop.fs.Path),623,625,"/**
 * Creates a FileStatus indicating a file was not found.
 * @param f The path of the not-found file.
 * @return A FileStatus object representing the not-found file.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",516,548,"/**
 * Retrieves FileStatus for a file on the FTP server.
 * @param client FTPClient object
 * @param file Path to the file
 * @return FileStatus object or throws FileNotFoundException
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,<init>,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE,boolean)",299,321,"/**
 * Initializes the CBZip2InputStream with given input stream and mode.
 * @param in Input stream to read from.
 * @param readMode Read mode (CONTINUOUS or BYBLOCK).
 * @param skipDecompression Whether to skip decompression.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,read0,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read0(),450,490,"/**
 * Reads a character based on the current state.
 * Returns character or special code (-1, -2) based on state.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Print:execute(),193,198,"/**
 * Executes the process, printing each token file.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,cancelToken,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:cancelToken(org.apache.hadoop.security.token.Token,java.lang.String)",833,843,"/**
 * Cancels a token, synchronizes cache, and calls super.
 * @param token Token to cancel.
 * @param canceller Canceller identifier.
 * @return TokenIdent object.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageToStream,org.apache.hadoop.security.Credentials:writeTokenStorageToStream(java.io.DataOutputStream),300,304,"/**
 * Writes the token storage to the given DataOutputStream.
 * Delegates to the default format for compatibility.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,<init>,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:<init>(),139,141,"/**
 * Default constructor. Initializes the MetricsSystemImpl.
 */",* Construct the system but not initializing (read config etc.) it.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,registerSource,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSource(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSource)",260,270,"/**
 * Registers a metrics source with the given name and description.
 * @param name Source name.
 * @param desc Source description.
 * @param source The metrics source to register.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,call,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.ipc.RpcWritable$Buffer,long,java.lang.String,java.lang.String,long)",577,597,"/**
 * Executes an RPC call, handling both shaded and legacy protobuf implementations.
 * @param server RPC server context
 * @return Writable result of the RPC call
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,<init>,org.apache.hadoop.metrics2.lib.MutableRollingAverages:<init>(java.lang.String),144,155,"/**
 * Initializes a MutableRollingAverages object.
 * @param metricValueName Name of the metric being averaged.
 */
","* Constructor for {@link MutableRollingAverages}.
   * @param metricValueName input metricValueName.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableRollingAverages.java,replaceScheduledTask,"org.apache.hadoop.metrics2.lib.MutableRollingAverages:replaceScheduledTask(int,long,java.util.concurrent.TimeUnit)",160,167,"/**
 * Replaces the scheduled task with new window count and interval.
 * @param windows Number of windows.
 * @param interval Interval for task execution.
 * @param timeUnit Time unit of the interval.
 */
",* This method is for testing only to replace the scheduledTask.,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSink)",272,294,"/**
 * Registers a metrics sink. Returns the sink. Creates if needed.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configureSinks,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSinks(),492,519,"/**
 * Configures and starts metrics sinks based on configuration.
 * Calculates the common period for sinks and sets the overall period.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,recheckElectability,org.apache.hadoop.ha.ZKFailoverController:recheckElectability(),808,860,"/**
 * Rechecks elector state and joins/quits election based on health.
 */","* Check the current state of the service, and join the election
   * if it should be in the election.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,reJoinElectionAfterFailureToBecomeActive,org.apache.hadoop.ha.ActiveStandbyElector:reJoinElectionAfterFailureToBecomeActive(),627,629,"/**
 * Re-joins the election process after failing to become active.
 * Re-joins with a specified sleep duration.
 */
","* We failed to become active. Re-join the election, but
   * sleep for a few seconds after terminating our existing
   * session, so that other nodes have a chance to become active.
   * The failure to become active is already logged inside
   * becomeActive().",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,processWatchEvent,"org.apache.hadoop.ha.ActiveStandbyElector:processWatchEvent(org.apache.zookeeper.ZooKeeper,org.apache.zookeeper.WatchedEvent)",635,713,"/**
 * Processes a ZooKeeper watch event, handling connection state changes
 * and node changes.  Takes a ZK instance and WatchedEvent.
 */
","* interface implementation of Zookeeper watch events (connection and node),
   * proxied by {@link WatcherWithClientRef}.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,"org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String,int)",180,183,"/**
 * Creates an InetSocketAddress from target and default port.
 * @param target Hostname or IP address.
 * @param defaultPort Default port number.
 * @return InetSocketAddress object.
 */
","* Util method to build socket addr from either.
   *   {@literal <host>}
   *   {@literal <host>:<port>}
   *   {@literal <fs>://<host>:<port>/<path>}
   *
   * @param target target.
   * @param defaultPort default port.
   * @return socket addr.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,processOptions,org.apache.hadoop.fs.shell.find.Find:processOptions(java.util.LinkedList),166,212,"/**
 * Processes command-line options and parses expressions from arguments.
 */",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/MultiSchemeDelegationTokenAuthenticationHandler.java,authenticate,"org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:authenticate(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",154,182,"/**
 * Authenticates a request, handling delegation auth schemes.
 * @param request HTTP request
 * @param response HTTP response
 * @return AuthenticationToken or null if unauthorized
 */
","* This method is overridden to restrict HTTP authentication schemes
   * available for delegation token management functionality. The
   * authentication schemes to be used for delegation token management are
   * configured using {@link DELEGATION_TOKEN_SCHEMES_PROPERTY}
   *
   * The basic logic here is to check if the current request is for delegation
   * token management. If yes then check if the request contains an
   * ""Authorization"" header. If it is missing, then return the HTTP 401
   * response with WWW-Authenticate header for each scheme configured for
   * delegation token management.
   *
   * It is also possible for a client to preemptively send Authorization header
   * for a scheme not configured for delegation token management. We detect
   * this case and return the HTTP 401 response with WWW-Authenticate header
   * for each scheme configured for delegation token management.
   *
   * If a client has sent a request with ""Authorization"" header for a scheme
   * configured for delegation token management, then it is forwarded to
   * underlying {@link MultiSchemeAuthenticationHandler} for actual
   * authentication.
   *
   * Finally all other requests (excluding delegation token management) are
   * forwarded to underlying {@link MultiSchemeAuthenticationHandler} for
   * actual authentication.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,handleDeprecation,org.apache.hadoop.conf.Configuration:handleDeprecation(),777,786,"/**
 * Handles deprecation for all properties in the configuration.
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,onlyKeyExists,org.apache.hadoop.conf.Configuration:onlyKeyExists(java.lang.String),1295,1305,"/**
 * Checks if any property equals DEFAULT_STRING_CHECK for given name.
 * @param name The name to check for property existence.
 * @return True if a matching property exists, false otherwise.
 */
","* Return existence of the <code>name</code> property, but only for
   * names which have no valid value, usually non-existent or commented
   * out in XML.
   *
   * @param name the property name
   * @return true if the property <code>name</code> exists without value",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getRaw,org.apache.hadoop.conf.Configuration:getRaw(java.lang.String),1355,1362,"/**
 * Retrieves raw property value by name, handling deprecations.
 * @param name Property name to retrieve.
 * @return Property value or null if not found.
 */
","* Get the value of the <code>name</code> property, without doing
   * <a href=""#VariableExpansion"">variable expansion</a>.If the key is 
   * deprecated, it returns the value of the first key which replaces 
   * the deprecated key and is not null.
   * 
   * @param name the property name.
   * @return the value of the <code>name</code> property or 
   *         its replacing property and null if no such property exists.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,set,"org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String,java.lang.String)",1420,1458,"/**
 * Sets a property value, handling deprecation and alternative names.
 * @param name Property name.
 * @param value Property value.
 * @param source Source of the property.
 */
","* Set the <code>value</code> of the <code>name</code> property. If 
   * <code>name</code> is deprecated, it also sets the <code>value</code> to
   * the keys that replace the deprecated key. Name will be trimmed before put
   * into configuration.
   *
   * @param name property name.
   * @param value property value.
   * @param source the place that this configuration value came from 
   * (For debugging).
   * @throws IllegalArgumentException when the value or name is null.",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,unset,org.apache.hadoop.conf.Configuration:unset(java.lang.String),1476,1492,"/**
 * Removes a property and its aliases from overlays and props.
 * @param name The name of the property to unset.
 */
","* Unset a previously set property.
   * @param name the property name",,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ShellCommandFencer.java,tryFence,"org.apache.hadoop.ha.ShellCommandFencer:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",81,135,"/**
 * Executes a fencing command based on target and arguments.
 * @param target HA service target object
 * @param args Command-line arguments for the fencing command
 * @return True if the command executed successfully (rc == 0)
 */
",,,,True,10
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,grantPermissions,org.apache.hadoop.fs.FileUtil:grantPermissions(java.io.File),235,239,"/**
 * Grants read, write, and execute permissions on the given file.
 * @param f the file to grant permissions on
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getVirtualMemorySize,org.apache.hadoop.util.SysInfoWindows:getVirtualMemorySize(),144,148,"/**
 * Returns the virtual memory size. Refreshes if needed.
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getPhysicalMemorySize,org.apache.hadoop.util.SysInfoWindows:getPhysicalMemorySize(),151,155,"/**
 * Returns the physical memory size in bytes.
 * Refreshes the size if necessary.
 */
",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getAvailableVirtualMemorySize,org.apache.hadoop.util.SysInfoWindows:getAvailableVirtualMemorySize(),158,162,"/**
 * Returns the size of available virtual memory.
 * Refreshes data if needed; returns a long value.
 */
",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getAvailablePhysicalMemorySize,org.apache.hadoop.util.SysInfoWindows:getAvailablePhysicalMemorySize(),165,169,"/**
 * Returns the available physical memory size in bytes.
 * Refreshes memory info if needed.
 */
",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNumProcessors,org.apache.hadoop.util.SysInfoWindows:getNumProcessors(),172,176,"/**
 * Returns the number of processors available.
 * Refreshes the value if necessary.
 */
",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getCpuFrequency,org.apache.hadoop.util.SysInfoWindows:getCpuFrequency(),185,189,"/**
 * Gets the CPU frequency in kHz. Refreshes if needed.
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getCumulativeCpuTime,org.apache.hadoop.util.SysInfoWindows:getCumulativeCpuTime(),192,196,"/**
 * Returns the cumulative CPU time in milliseconds.
 * Refreshes data if needed.
 */
",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getCpuUsagePercentage,org.apache.hadoop.util.SysInfoWindows:getCpuUsagePercentage(),199,207,"/**
 * Gets CPU usage percentage, normalized by the number of processors.
 */",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNumVCoresUsed,org.apache.hadoop.util.SysInfoWindows:getNumVCoresUsed(),210,218,"/**
 * Returns the number of vCores used, scaled to a fraction.
 */
",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNetworkBytesRead,org.apache.hadoop.util.SysInfoWindows:getNetworkBytesRead(),221,225,"/**
* Returns the number of network bytes read.
* Refreshes the value if necessary.
*/
",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNetworkBytesWritten,org.apache.hadoop.util.SysInfoWindows:getNetworkBytesWritten(),228,232,"/**
 * Returns the number of bytes written on the network.
 * @return Number of bytes written, updated via refreshIfNeeded().
 */
",{@inheritDoc},,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getStorageBytesRead,org.apache.hadoop.util.SysInfoWindows:getStorageBytesRead(),234,238,"/**
 * Returns the total storage bytes read.
 * Refreshes internal value if needed.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getStorageBytesWritten,org.apache.hadoop.util.SysInfoWindows:getStorageBytesWritten(),240,244,"/**
 * Returns the total storage bytes written.
 * Refreshes data if needed. Returns a long value.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getPermission,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getPermission(),951,957,"/**
 * Retrieves the permission. Loads if not already loaded.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getOwner,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getOwner(),959,965,"/**
 * Gets the owner of the resource. Loads permission info if needed.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getGroup,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:getGroup(),967,973,"/**
 * Gets the group associated with this permission. Loads info if needed.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,write,org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus:write(java.io.DataOutput),1087,1093,"/**
 * Writes data to the output stream, loading permissions if needed.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,flush,org.apache.hadoop.security.alias.LocalKeyStoreProvider:flush(),143,160,"/**
 * Flushes the buffer, resetting file permissions based on the platform.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsNetgroupMapping.java,cacheGroupsRefresh,org.apache.hadoop.security.ShellBasedUnixGroupsNetgroupMapping:cacheGroupsRefresh(),63,68,"/**
 * Refreshes cached netgroups: fetches names, clears cache, and adds them.
 */",* Refresh the netgroup cache,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,delete,org.apache.hadoop.fs.viewfs.ViewFileSystem:delete(org.apache.hadoop.fs.Path),498,503,"/**
 * Deletes the file specified by the path.
 * @param f the path to the file to delete
 * @return true if successfully deleted, false otherwise
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getStatus(),1299,1302,"/**
 * Gets the status of the file system.
 * Delegates to getStatus(null).
 * @return FsStatus object representing the status.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.java,updateMountPointFsStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:updateMountPointFsStatus(org.apache.hadoop.fs.viewfs.ViewFileSystem,java.util.Map,org.apache.hadoop.fs.viewfs.ViewFileSystem$MountPoint,org.apache.hadoop.fs.Path)",169,175,"/**
 * Updates mount point status in the map with fs status from path.
 * @param viewFileSystem ViewFileSystem instance
 * @param mountPointMap Map to store mount point and status
 * @param mountPoint The mount point to update
 * @param path The path to get the fs status from
 */
","* Update FsStatus for the given the mount point.
   *
   * @param viewFileSystem
   * @param mountPointMap
   * @param mountPoint
   * @param path",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFs.java,read,"org.apache.hadoop.fs.ChecksumFs$ChecksumFSInputChecker:read(long,byte[],int,int)",194,210,"/**
 * Reads up to {@code len} bytes from the file at {@code position}.
 * @param position Starting position for reading.
 * @param b Buffer to read into.
 * @param off Offset into buffer.
 * @param len Number of bytes to read.
 * @return Number of bytes read.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processArguments,org.apache.hadoop.fs.shell.CopyCommands$Put:processArguments(java.util.LinkedList),306,315,"/**
 * Processes arguments; handles stdin copy when ""-"" is provided.
 * @param args List of PathData objects representing arguments.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,create,"org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",696,702,"/**
 * Creates a data output stream at the specified path.
 * @param f Path to create stream at.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",736,742,"/**
 * Creates a non-recursive data output stream for a path.
 * @param f path to create stream for
 * @return FSDataOutputStream
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.ChecksumFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",757,768,"/**
 * Creates a non-recursive data output stream at the specified path.
 * @param f Path to create stream at
 * @param permission File permissions
 * @param flags Creation flags
 * @return FSDataOutputStream
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,renameInternal,"org.apache.hadoop.fs.viewfs.ViewFs:renameInternal(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",638,644,"/**
 * Renames a file or directory internally, without overwriting.
 * @param src Source path.
 * @param dst Destination path.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,listStatus,org.apache.hadoop.fs.HarFileSystem:listStatus(org.apache.hadoop.fs.Path),784,804,"/**
 * Lists file statuses for a given path, returning an array.
 * @param f the path to list
 * @throws IOException if an I/O error occurs
 */
","* liststatus returns the children of a directory 
   * after looking up the index files.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getFileLinkStatusInternal,"org.apache.hadoop.fs.RawLocalFileSystem:getFileLinkStatusInternal(org.apache.hadoop.fs.Path,boolean)",1233,1242,"/**
 * Gets file link status, using deprecated path if useDeprecatedFileStatus is true.
 * @param f Path to the file.
 * @param dereference Dereference symbolic links.
 * @return FileStatus object.
 */
","* Public {@link FileStatus} methods delegate to this function, which in turn
   * either call the new {@link Stat} based implementation or the deprecated
   * methods based on platform support.
   * 
   * @param f Path to stat
   * @param dereference whether to dereference the final path component if a
   *          symlink
   * @return FileStatus of f
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFsLocatedFileStatus.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFsLocatedFileStatus:<init>(org.apache.hadoop.fs.LocatedFileStatus,org.apache.hadoop.fs.Path)",32,35,"/**
 * Initializes LocatedFileStatus and modified Path.
 * @param locatedFileStatus The LocatedFileStatus object.
 * @param path The modified Path object.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",505,512,"/**
 * Gets block locations for a file.
 * @param fs FileStatus of the file.
 * @param start Starting offset.
 * @param len Length of the data.
 * @return BlockLocation array.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFs:getFileStatus(org.apache.hadoop.fs.Path),407,426,"/**
 * Gets the file status for a given path.
 * @param f Path to get status for.
 * @return FileStatus object.
 */
","* {@inheritDoc}
   *
   * If the given path is a symlink(mount link), the path will be resolved to a
   * target path and it will get the resolved path's FileStatus object. It will
   * not be represented as a symlink and isDirectory API returns true if the
   * resolved path is a directory, false otherwise.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFs:listStatus(org.apache.hadoop.fs.Path),518,538,"/**
 * Lists file statuses for a given path.
 * @param f path to list statuses for
 * @return Array of FileStatus objects
 */
","* {@inheritDoc}
   *
   * Note: listStatus considers listing from fallbackLink if available. If the
   * same directory path is present in configured mount path as well as in
   * fallback fs, then only the fallback path will be listed in the returned
   * result except for link.
   *
   * If any of the the immediate children of the given path f is a symlink(mount
   * link), the returned FileStatus object of that children would be represented
   * as a symlink. It will not be resolved to the target path and will not get
   * the target path FileStatus object. The target path will be available via
   * getSymlink on that children's FileStatus object. Since it represents as
   * symlink, isDirectory on that children's FileStatus will return false.
   * This behavior can be changed by setting an advanced configuration
   * fs.viewfs.mount.links.as.symlinks to false. In this case, mount points will
   * be represented as non-symlinks and all the file/directory attributes like
   * permissions, isDirectory etc will be assigned from it's resolved target
   * directory/file.
   *
   * If you want to get the FileStatus of target path for that children, you may
   * want to use GetFileStatus API with that children's symlink path. Please see
   * {@link ViewFs#getFileStatus(Path f)}
   *
   * Note: In ViewFs, by default the mount links are represented as symlinks.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,exists,"org.apache.hadoop.fs.sftp.SFTPFileSystem:exists(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",189,198,"/**
 * Checks if a file exists on the SFTP channel.
 * @param channel SFTP channel to check.
 * @param file Path to the file.
 * @return True if file exists, false otherwise.
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getFileStatus,"org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(com.jcraft.jsch.ChannelSftp,com.jcraft.jsch.ChannelSftp$LsEntry,org.apache.hadoop.fs.Path)",260,297,"/**
 * Gets the FileStatus for an SFTP file or link.
 * @param channel SFTP channel
 * @param sftpFile SFTP file entry
 * @param parentPath Parent path of the file
 * @return FileStatus object
 */
","* Convert the file information in LsEntry to a {@link FileStatus} object. *
   *
   * @param sftpFile
   * @param parentPath
   * @return file status
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,isFile,"org.apache.hadoop.fs.sftp.SFTPFileSystem:isFile(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",355,363,"/**
 * Checks if a file exists and is not a directory.
 * @param channel SFTP channel
 * @param file Path to check
 * @throws IOException if an I/O error occurs
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * @throws IOException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getFileStatus,org.apache.hadoop.fs.http.HttpsFileSystem:getFileStatus(org.apache.hadoop.fs.Path),113,116,"/**
 * Returns a dummy FileStatus for the given path.
 * @param path The path for which to create a FileStatus.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,getFileStatus,org.apache.hadoop.fs.http.HttpFileSystem:getFileStatus(org.apache.hadoop.fs.Path),113,116,"/**
 * Returns a file status for the given path.
 * @param path The path to get the status for.
 * @return A FileStatus object.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Globber.java,glob,org.apache.hadoop.fs.Globber:glob(),197,206,"/**
 * Retrieves FileStatus[] matching the configured path pattern.
 * @throws IOException if an I/O error occurs during globbing.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,exists,"org.apache.hadoop.fs.ftp.FTPFileSystem:exists(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",391,398,"/**
 * Checks if a file exists on the FTP server.
 * @param client FTPClient object
 * @param file Path of the file to check
 * @return True if file exists, false otherwise.
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * @throws IOException on IO problems other than FileNotFoundException",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,listStatus,"org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",484,498,"/**
 * Lists file statuses within a directory.
 * @param client FTPClient object
 * @param file Path to directory, returns FileStatus array
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,isFile,"org.apache.hadoop.fs.ftp.FTPFileSystem:isFile(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path)",617,625,"/**
 * Checks if a file exists on the FTP server.
 * @param client FTP client, Path file path
 * @return True if file exists, false otherwise.
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,<init>,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",294,297,"/**
 * Creates a CBZip2InputStream with default settings.
 * @param in Input stream to decompress.
 * @param readMode Read mode for decompression.
 */
","* Constructs a new CBZip2InputStream which decompresses bytes read from the
  * specified stream.
  *
  * <p>
  * Although BZip2 headers are marked with the magic <tt>""Bz""</tt> this
  * constructor expects the next byte in the stream to be the first one after
  * the magic. Thus callers have to skip the first two bytes. Otherwise this
  * constructor will throw an exception.
  * </p>
  * @param in in.
  * @param readMode READ_MODE.
  * @throws IOException
  *             if the stream content is malformed or an I/O error occurs.
  * @throws NullPointerException
  *             if <tt>in == null</tt>",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,numberOfBytesTillNextMarker,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:numberOfBytesTillNextMarker(java.io.InputStream),346,349,"/**
 * Gets the number of processed bytes until the next CBZ2 marker.
 * @param in Input stream to read from.
 * @return Number of bytes processed.
 */
","* Returns the number of bytes between the current stream position
   * and the immediate next BZip2 block marker.
   *
   * @param in
   *             The InputStream
   *
   * @return long Number of bytes between current stream position and the
   * next BZip2 block start marker.
 * @throws IOException raised on errors performing I/O.
   *",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,read,"org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read(byte[],int,int)",400,448,"/**
 * Reads bytes from the stream into the provided byte array.
 * @param dest Destination byte array.
 * @param offs Offset into the destination array.
 * @param len Number of bytes to read.
 * @return Number of bytes read.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,call,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server$ProtoBufRpcInvoker:call(org.apache.hadoop.ipc.RPC$Server,java.lang.String,org.apache.hadoop.io.Writable,long)",529,575,"/**
 * Executes RPC call using server, protocol, request, and version.
 * @param server RPC server instance
 * @return Writable response object
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsRegistry.java,newMutableRollingAverages,"org.apache.hadoop.metrics2.lib.MetricsRegistry:newMutableRollingAverages(java.lang.String,java.lang.String)",339,346,"/**
 * Creates and registers a MutableRollingAverages object.
 * @param name Metric name.
 * @param valueName Value name for the averages.
 * @return The newly created MutableRollingAverages object.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,verifyChangedServiceState,org.apache.hadoop.ha.ZKFailoverController:verifyChangedServiceState(org.apache.hadoop.ha.HAServiceProtocol$HAServiceState),884,922,"/**
 * Handles service state changes, quitting election if necessary.
 * @param changedState The new service state received.
 */
",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,processResult,"org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,java.lang.String)",496,551,"/**
 * Processes the result of a ZNode creation attempt. Handles success, failure, and retries.
 */",* interface implementation of Zookeeper callback for create,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,processResult,"org.apache.hadoop.ha.ActiveStandbyElector:processResult(int,java.lang.String,java.lang.Object,org.apache.zookeeper.data.Stat)",556,613,"/**
 * Processes the result from Zookeeper for a StatNode.
 * Handles success, errors, and retries based on the result code.
 */
",* interface implementation of Zookeeper callback for monitor (exists),,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ActiveStandbyElector.java,process,org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef:process(org.apache.zookeeper.WatchedEvent),1233,1247,"/**
 * Processes a ZooKeeper watch event. Awaits session setup, then delegates.
 */",,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,createSocketAddr,org.apache.hadoop.net.NetUtils:createSocketAddr(java.lang.String),162,164,"/**
 * Creates an InetSocketAddress from the given target string.
 * @param target Host:port string, e.g., ""example.com:8080""
 * @return InetSocketAddress object
 */
","* Util method to build socket addr from either.
   *   {@literal <host>:<port>}
   *   {@literal <fs>://<host>:<port>/<path>}
   *
   * @param target target.
   * @return socket addr.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/util/Servers.java,parse,"org.apache.hadoop.metrics2.util.Servers:parse(java.lang.String,int)",50,62,"/**
 * Parses host/port specifications into a list of InetSocketAddresses.
 * @param specs Comma/space-separated host/port strings.
 * @param defaultPort Default port to use if not specified.
 * @return List of InetSocketAddress objects.
 */
","* Parses a space and/or comma separated sequence of server specifications
   * of the form <i>hostname</i> or <i>hostname:port</i>.  If
   * the specs string is null, defaults to localhost:defaultPort.
   *
   * @param specs   server specs (see description)
   * @param defaultPort the default port if not specified
   * @return a list of InetSocketAddress objects.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,buildDTServiceName,"org.apache.hadoop.security.SecurityUtil:buildDTServiceName(java.net.URI,int)",338,345,"/**
 * Builds a data transfer service name from a URI and default port.
 * @param uri The URI to parse.
 * @param defPort Default port number.
 * @return Service name string.
 */
","* create the service name for a Delegation token
   * @param uri of the service
   * @param defPort is used if the uri lacks a port
   * @return the token service, or null if no authority
   * @see #buildTokenService(InetSocketAddress)",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,asXmlDocument,"org.apache.hadoop.conf.Configuration:asXmlDocument(java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",3650,3685,"/**
 * Creates an XML document representing configuration properties.
 * @param propertyName Property name to include, or null for all.
 * @param redactor Redactor for property values.
 * @return XML Document object.
 */
",* Return the XML DOM corresponding to this Configuration.,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,substituteVars,org.apache.hadoop.conf.Configuration:substituteVars(java.lang.String),1150,1218,"/**
 * Substitutes variables in an expression using system properties.
 * @param expr Expression string to substitute variables in.
 * @return String with variables substituted, or original if unbound.
 */","* Attempts to repeatedly expand the value {@code expr} by replacing the
   * left-most substring of the form ""${var}"" in the following precedence order
   * <ol>
   *   <li>by the value of the environment variable ""var"" if defined</li>
   *   <li>by the value of the Java system property ""var"" if defined</li>
   *   <li>by the value of the configuration key ""var"" if defined</li>
   * </ol>
   *
   * If var is unbounded the current state of expansion ""prefix${var}suffix"" is
   * returned.
   * <p>
   * This function also detects self-referential substitutions, i.e.
   * <pre>
   *   {@code
   *   foo.bar = ${foo.bar}
   *   }
   * </pre>
   * If a cycle is detected then the original expr is returned. Loops
   * involving multiple substitutions are not detected.
   *
   * In order not to introduce breaking changes (as Oozie for example contains a method with the
   * same name and same signature) do not make this method public, use substituteCommonVariables
   * in this case.
   *
   * @param expr the literal value of a config key
   * @return null if expr is null, otherwise the value resulting from expanding
   * expr using the algorithm above.
   * @throws IllegalArgumentException when more than
   * {@link Configuration#MAX_SUBST} replacements are required",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,applyChanges,"org.apache.hadoop.conf.ReconfigurationServlet:applyChanges(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable,javax.servlet.http.HttpServletRequest)",140,197,"/**
 * Applies configuration changes from request to reconfigurable object.
 * @param out PrintWriter for outputting change messages
 * @param reconf Reconfigurable object to update
 * @param req HttpServletRequest containing parameters
 */
",* Apply configuratio changes after admin has approved them.,,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,set,"org.apache.hadoop.conf.Configuration:set(java.lang.String,java.lang.String)",1404,1406,"/**
 * Sets the value for a given name, using the default scope.
 * @param name The name of the property to set.
 * @param value The value to set for the property.
 */
","* Set the <code>value</code> of the <code>name</code> property. If 
   * <code>name</code> is deprecated or there is a deprecated name associated to it,
   * it sets the value to both names. Name will be trimmed before put into
   * configuration.
   * 
   * @param name property name.
   * @param value property value.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,set,"org.apache.hadoop.conf.ConfigurationWithLogging:set(java.lang.String,java.lang.String,java.lang.String)",107,112,"/**
 * Sets the attribute value, logs the action, and calls super.set.
 * @param name Attribute name
 * @param value Attribute value
 * @param source Source of the attribute
 */
","* See {@link Configuration#set(String, String, String)}.",,,True,11
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDelete,"org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File,boolean)",187,203,"/**
 * Deletes a directory and its contents recursively.
 * @param dir The directory to delete.
 * @param tryGrantPermissions Attempts to grant permissions.
 * @return True if deletion was successful, false otherwise.
 */
","* Delete a directory and all its contents.  If
   * we return false, the directory may be partially-deleted.
   * (1) If dir is symlink to a file, the symlink is deleted. The file pointed
   *     to by the symlink is not deleted.
   * (2) If dir is symlink to a directory, symlink is deleted. The directory
   *     pointed to by symlink is not deleted.
   * (3) If dir is a normal file, it is deleted.
   * (4) If dir is a normal directory, then dir and all its contents recursively
   *     are deleted.
   * @param dir the file or directory to be deleted
   * @param tryGrantPermissions true if permissions should be modified to delete a file.
   * @return true on success false on failure.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/SysInfoWindows.java,getNumCores,org.apache.hadoop.util.SysInfoWindows:getNumCores(),179,182,"/**
 * Returns the number of processor cores.
 * Delegates to getNumProcessors().
 */
",{@inheritDoc},,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemUtil.java,getStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystemUtil:getStatus(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",106,159,"/**
 * Retrieves the status of mount points for a given path.
 * @param fileSystem The FileSystem to check.
 * @param path The path to check.
 * @return Map of MountPoint to FsStatus.
 */
","* Get FsStatus for all ViewFsMountPoints matching path for the given
   * ViewFileSystem.
   *
   * Say ViewFileSystem has following mount points configured
   *  (1) hdfs://NN0_host:port/sales mounted on /dept/sales
   *  (2) hdfs://NN1_host:port/marketing mounted on /dept/marketing
   *  (3) hdfs://NN2_host:port/eng_usa mounted on /dept/eng/usa
   *  (4) hdfs://NN3_host:port/eng_asia mounted on /dept/eng/asia
   *
   * For the above config, here is a sample list of paths and their matching
   * mount points while getting FsStatus
   *
   *  Path                  Description                      Matching MountPoint
   *
   *  ""/""                   Root ViewFileSystem lists all    (1), (2), (3), (4)
   *                         mount points.
   *
   *  ""/dept""               Not a mount point, but a valid   (1), (2), (3), (4)
   *                         internal dir in the mount tree
   *                         and resolved down to ""/"" path.
   *
   *  ""/dept/sales""         Matches a mount point            (1)
   *
   *  ""/dept/sales/india""   Path is over a valid mount point (1)
   *                         and resolved down to
   *                         ""/dept/sales""
   *
   *  ""/dept/eng""           Not a mount point, but a valid   (1), (2), (3), (4)
   *                         internal dir in the mount tree
   *                         and resolved down to ""/"" path.
   *
   *  ""/erp""                Doesn't match or leads to or
   *                         over any valid mount points     None
   *
   *
   * @param fileSystem - ViewFileSystem on which mount point exists
   * @param path - URI for which FsStatus is requested
   * @return Map of ViewFsMountPoint and FsStatus
   * @throws IOException raised on errors performing I/O.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,create,"org.apache.hadoop.fs.ChecksumFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.Options$ChecksumOpt)",744,755,"/**
 * Creates a data output stream for writing to a file.
 * @param f Path to create stream for
 * @param permission File permissions
 * @param flags Creation flags
 * @return FSDataOutputStream
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getFileStatus,org.apache.hadoop.fs.RawLocalFileSystem:getFileStatus(org.apache.hadoop.fs.Path),905,908,"/**
 * Gets the file status for the given path.
 * @param f the path to get the status of
 * @return FileStatus object
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getFileLinkStatus,org.apache.hadoop.fs.RawLocalFileSystem:getFileLinkStatus(org.apache.hadoop.fs.Path),1209,1220,"/**
 * Gets the FileStatus, resolving symlinks to qualified paths.
 * @param f the path to check
 * @return FileStatus object
 */
","* Return a FileStatus representing the given path. If the path refers
   * to a symlink return a FileStatus representing the link rather than
   * the object the link refers to.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,getLinkTarget,org.apache.hadoop.fs.RawLocalFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),1308,1313,"/**
 * Gets the target path of a symbolic link.
 * @param f Path of the symbolic link
 * @return The target path of the symbolic link
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,wrapLocalFileStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystem:wrapLocalFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",552,557,"/**
 * Wraps a FileStatus, handling LocatedFileStatus correctly.
 * @param orig Original FileStatus.
 * @param qualified Qualified Path.
 * @return Wrapped FileStatus.
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,rename,"org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",464,491,"/**
 * Renames a file on the SFTP server.
 * @param channel SFTP channel
 * @param src source path
 * @param dst destination path
 * @return True if rename successful, false otherwise.
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   *
   * @param channel
   * @param src
   * @param dst
   * @return rename successful?
   * @throws IOException",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,listStatus,"org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path)",421,451,"/**
 * Lists file statuses within a directory on the SFTP server.
 * @param client SFTP client connection
 * @param file Path to the directory
 * @return Array of FileStatus objects
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",314,347,"/**
 * Creates directories recursively on SFTP server.
 * @param client SFTP client.
 * @param file Path of the directory to create.
 * @param permission FsPermission for the directory.
 * @return True if created, false otherwise.
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,globStatus,org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path),2122,2126,"/**
 * Retrieves FileStatus objects matching the given path pattern.
 * @param pathPattern Path pattern to match
 * @return Array of FileStatus objects
 */
","* <p>Return all the files that match filePattern and are not checksum
     * files. Results are sorted by their names.
     * 
     * <p>
     * A filename pattern is composed of <i>regular</i> characters and
     * <i>special pattern matching</i> characters, which are:
     *
     * <dl>
     *  <dd>
     *   <dl>
     *    <dt> <tt> ? </tt>
     *    <dd> Matches any single character.
     *
     *    <dt> <tt> * </tt>
     *    <dd> Matches zero or more characters.
     *
     *    <dt> <tt> [<i>abc</i>] </tt>
     *    <dd> Matches a single character from character set
     *     <tt>{<i>a,b,c</i>}</tt>.
     *
     *    <dt> <tt> [<i>a</i>-<i>b</i>] </tt>
     *    <dd> Matches a single character from the character range
     *     <tt>{<i>a...b</i>}</tt>. Note: character <tt><i>a</i></tt> must be
     *     lexicographically less than or equal to character <tt><i>b</i></tt>.
     *
     *    <dt> <tt> [^<i>a</i>] </tt>
     *    <dd> Matches a single char that is not from character set or range
     *     <tt>{<i>a</i>}</tt>.  Note that the <tt>^</tt> character must occur
     *     immediately to the right of the opening bracket.
     *
     *    <dt> <tt> \<i>c</i> </tt>
     *    <dd> Removes (escapes) any special meaning of character <i>c</i>.
     *
     *    <dt> <tt> {ab,cd} </tt>
     *    <dd> Matches a string from the string set <tt>{<i>ab, cd</i>} </tt>
     *
     *    <dt> <tt> {ab,c{de,fh}} </tt>
     *    <dd> Matches a string from string set <tt>{<i>ab, cde, cfh</i>}</tt>
     *
     *   </dl>
     *  </dd>
     * </dl>
     *
     * @param pathPattern a glob specifying a path pattern
     *
     * @return an array of paths that match the path pattern
     *
     * @throws AccessControlException If access is denied
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>pathPattern</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,globStatus,"org.apache.hadoop.fs.FileContext$Util:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2151,2155,"/**
 * Retrieves FileStatus objects matching a path pattern.
 * @param pathPattern Path pattern to match.
 * @param filter Filter to apply to matching paths.
 * @return Array of FileStatus objects.
 */
","* Return an array of FileStatus objects whose path names match pathPattern
     * and is accepted by the user-supplied path filter. Results are sorted by
     * their path names.
     * Return null if pathPattern has no glob and the path does not exist.
     * Return an empty array if pathPattern has a glob and no path matches it. 
     * 
     * @param pathPattern glob specifying the path pattern
     * @param filter user-supplied path filter
     *
     * @return an array of FileStatus objects
     *
     * @throws AccessControlException If access is denied
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>pathPattern</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,globStatus,org.apache.hadoop.fs.FileSystem:globStatus(org.apache.hadoop.fs.Path),2219,2226,"/**
 * Returns an array of FileStatus objects matching the path pattern.
 * @param pathPattern Path pattern to match.
 * @return FileStatus array of matching paths.
 */
","* <p>Return all the files that match filePattern and are not checksum
   * files. Results are sorted by their names.
   *
   * <p>
   * A filename pattern is composed of <i>regular</i> characters and
   * <i>special pattern matching</i> characters, which are:
   *
   * <dl>
   *  <dd>
   *   <dl>
   *    <dt> <tt> ? </tt>
   *    <dd> Matches any single character.
   *
   *    <dt> <tt> * </tt>
   *    <dd> Matches zero or more characters.
   *
   *    <dt> <tt> [<i>abc</i>] </tt>
   *    <dd> Matches a single character from character set
   *     <tt>{<i>a,b,c</i>}</tt>.
   *
   *    <dt> <tt> [<i>a</i>-<i>b</i>] </tt>
   *    <dd> Matches a single character from the character range
   *     <tt>{<i>a...b</i>}</tt>.  Note that character <tt><i>a</i></tt> must be
   *     lexicographically less than or equal to character <tt><i>b</i></tt>.
   *
   *    <dt> <tt> [^<i>a</i>] </tt>
   *    <dd> Matches a single character that is not from character set or range
   *     <tt>{<i>a</i>}</tt>.  Note that the <tt>^</tt> character must occur
   *     immediately to the right of the opening bracket.
   *
   *    <dt> <tt> \<i>c</i> </tt>
   *    <dd> Removes (escapes) any special meaning of character <i>c</i>.
   *
   *    <dt> <tt> {ab,cd} </tt>
   *    <dd> Matches a string from the string set <tt>{<i>ab, cd</i>} </tt>
   *
   *    <dt> <tt> {ab,c{de,fh}} </tt>
   *    <dd> Matches a string from the string set <tt>{<i>ab, cde, cfh</i>}</tt>
   *
   *   </dl>
   *  </dd>
   * </dl>
   *
   * @param pathPattern a glob specifying a path pattern

   * @return an array of paths that match the path pattern
   * @throws IOException IO failure",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,globStatus,"org.apache.hadoop.fs.FileSystem:globStatus(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.PathFilter)",2241,2244,"/**
 * Retrieves FileStatus objects matching a path pattern.
 * @param pathPattern Path pattern to match.
 * @param filter Filter to apply to matching paths.
 * @return Array of FileStatus objects.
 */
","* Return an array of {@link FileStatus} objects whose path names match
   * {@code pathPattern} and is accepted by the user-supplied path filter.
   * Results are sorted by their path names.
   *
   * @param pathPattern a glob specifying the path pattern
   * @param filter a user-supplied path filter
   * @return null if {@code pathPattern} has no glob and the path does not exist
   *         an empty array if {@code pathPattern} has a glob and no path
   *         matches it else an array of {@link FileStatus} objects matching the
   *         pattern
   * @throws IOException if any I/O error occurs when fetching file status",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,rename,"org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",670,705,"/**
 * Renames a file or directory on the FTP server.
 * @param client FTP client
 * @param src source path
 * @param dst destination path
 * @return True if rename was successful, false otherwise.
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.
   * 
   * @param client
   * @param src
   * @param dst
   * @return
   * @throws IOException",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,delete,"org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,boolean)",416,438,"/**
 * Deletes a file or directory recursively using FTPClient.
 * @param client FTP client, Path file to delete, recursive flag
 * @throws IOException if deletion fails
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",590,610,"/**
 * Creates directories recursively on FTP server.
 * @param client FTP client, path to create, permission
 * @return True if directories were created, false otherwise.
 */","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,<init>,"org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:<init>(java.io.InputStream,long,long,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",358,407,"/**
 * Creates a BZip2CompressionInputStream with specified input stream.
 * @param in Input stream.
 * @param start Start position.
 * @param end End position.
 * @param readMode Read mode.
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,internalReset,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:internalReset(),530,537,"/**
 * Resets the internal state for decompression.
 * Sets needsReset to false and reinitializes input stream.
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,<init>,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:<init>(java.io.InputStream),351,353,"/**
 * Creates a CBZip2InputStream with continuous read mode.
 * @param in The input stream to read from.
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/CBZip2InputStream.java,read,org.apache.hadoop.io.compress.bzip2.CBZip2InputStream:read(),365,376,"/**
 * Reads a single byte from the input stream.
 * Returns the byte value or -1 if stream is closed.
 */",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MutableMetricsFactory.java,newForField,"org.apache.hadoop.metrics2.lib.MutableMetricsFactory:newForField(java.lang.reflect.Field,org.apache.hadoop.metrics2.annotation.Metric,org.apache.hadoop.metrics2.lib.MetricsRegistry)",40,92,"/**
 * Creates and registers a mutable metric for the given field.
 * @param field Field to create metric for.
 * @param annotation Metric annotation.
 * @param registry Metrics registry.
 * @return MutableMetric instance.
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,reportServiceStatus,org.apache.hadoop.ha.ZKFailoverController$ServiceStateCallBacks:reportServiceStatus(org.apache.hadoop.ha.HAServiceStatus),999,1002,"/**
 * Reports service status, verifying state changes.
 * @param status The HAServiceStatus object to report.
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,normalizeIP2HostName,org.apache.hadoop.net.NetUtils:normalizeIP2HostName(java.lang.String),731,738,"/**
 * Normalizes an IP:Port string to a host:port string.
 * @param ipPort IP:Port string to normalize.
 * @return Normalized host:port string.
 */
","* Attempt to normalize the given string to ""host:port""
   * if it like ""ip:port"".
   *
   * @param ipPort maybe lik ip:port or host:port.
   * @return host:port",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getTokenServiceAddr,org.apache.hadoop.security.SecurityUtil:getTokenServiceAddr(org.apache.hadoop.security.token.Token),447,449,"/**
 * Creates an InetSocketAddress from the token's service string.
 */","* Decode the given token's service field into an InetAddress
   * @param token from which to obtain the service
   * @return InetAddress for the service",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,buildTokenService,org.apache.hadoop.security.SecurityUtil:buildTokenService(java.net.URI),495,497,"/**
 * Builds a TokenService from a URI.
 * @param uri The URI to build the TokenService from.
 * @return A TokenService instance.
 */
","* Construct the service key for a token
   * @param uri of remote connection with a token
   * @return ""ip:port"" or ""host:port"" depending on the value of
   *          hadoop.security.token.service.use_ip",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/AbstractGangliaSink.java,init,org.apache.hadoop.metrics2.sink.ganglia.AbstractGangliaSink:init(org.apache.commons.configuration2.SubsetConfiguration),120,172,"/**
 * Initializes the GangliaSink with configuration from the provided object.
 * @param conf Configuration object containing Ganglia sink properties.
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getCanonicalServiceName,org.apache.hadoop.fs.FileSystem:getCanonicalServiceName(),453,460,"/**
 * Returns the canonical service name, or null if child FS exists.
 */
","* Get a canonical service name for this FileSystem.
   * The token cache is the only user of the canonical service name,
   * and uses it to lookup this FileSystem's service tokens.
   * If the file system provides a token of its own then it must have a
   * canonical name, otherwise the canonical name can be null.
   *
   * Default implementation: If the FileSystem has child file systems
   * (such as an embedded file system) then it is assumed that the FS has no
   * tokens of its own and hence returns a null name; otherwise a service
   * name is built using Uri and port.
   *
   * @return a service string that uniquely identifies this file system, null
   *         if the filesystem does not implement tokens
   * @see SecurityUtil#buildDTServiceName(URI, int)",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getCanonicalServiceName,org.apache.hadoop.fs.AbstractFileSystem:getCanonicalServiceName(),1243,1245,"/**
 * Returns the canonical service name based on URI and port.
 */","* Get a canonical name for this file system.
   * @return a URI string that uniquely identifies this file system",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,substituteCommonVariables,org.apache.hadoop.conf.Configuration:substituteCommonVariables(java.lang.String),1115,1117,"/**
 * Substitutes common variables in the expression.
 * @param expr The expression string to substitute variables in.
 * @return The expression with variables substituted.
 */
","* Provides a public wrapper over substituteVars in order to avoid compatibility issues.
   * See HADOOP-18021 for further details.
   *
   * @param expr the literal value of a config key
   * @return null if expr is null, otherwise the value resulting from expanding
   * expr using the algorithm above.
   * @throws IllegalArgumentException when more than
   * {@link Configuration#MAX_SUBST} replacements are required",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,get,org.apache.hadoop.conf.Configuration:get(java.lang.String),1263,1270,"/**
 * Retrieves a property value by name, handling deprecation.
 * @param name Property name to retrieve.
 * @return Property value or null if not found.
 */
","* Get the value of the <code>name</code> property, <code>null</code> if
   * no such property exists. If the key is deprecated, it returns the value of
   * the first key which replaces the deprecated key and is not null.
   * 
   * Values are processed for <a href=""#VariableExpansion"">variable expansion</a> 
   * before being returned.
   *
   * As a side effect get loads the properties from the sources if called for
   * the first time as a lazy init.
   * 
   * @param name the property name, will be trimmed before get value.
   * @return the value of the <code>name</code> or its replacing property, 
   *         or null if no such property exists.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,get,"org.apache.hadoop.conf.Configuration:get(java.lang.String,java.lang.String)",1524,1531,"/**
 * Retrieves property value by name, substituting variables.
 * @param name Property name.
 * @param defaultValue Default value if not found.
 * @return Property value or default if not found.
 */
","* Get the value of the <code>name</code>. If the key is deprecated,
   * it returns the value of the first key which replaces the deprecated key
   * and is not null.
   * If no such property exists,
   * then <code>defaultValue</code> is returned.
   * 
   * @param name property name, will be trimmed before get value.
   * @param defaultValue default value.
   * @return property value, or <code>defaultValue</code> if the property 
   *         doesn't exist.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,doPost,"org.apache.hadoop.conf.ReconfigurationServlet:doPost(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",214,236,"/**
 * Handles POST requests to apply reconfigurable changes.
 * Applies changes from request, handles errors, and sends response.
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,java.lang.String)",169,175,"/**
 * Adds an optional key-value pair.
 * @param key The key to add.
 * @param value The value associated with the key.
 * @return This builder instance.
 */
",* Set optional Builder parameter.,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,java.lang.String)",256,261,"/**
 * Adds a mandatory key-value pair to the builder.
 * @param key The key to make mandatory.
 * @param value The value associated with the key.
 * @return This builder for chaining.
 */
","* Set mandatory option to the Builder.
   *
   * If the option is not supported or unavailable on the {@link FileSystem},
   * the client should expect {@link #build()} throws IllegalArgumentException.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setDefaultUri,"org.apache.hadoop.fs.FileSystem:setDefaultUri(org.apache.hadoop.conf.Configuration,java.net.URI)",311,313,"/**
 * Sets the default filesystem URI in the configuration.
 * @param conf Configuration object to update.
 * @param uri URI of the default filesystem.
 */
","* Set the default FileSystem URI in a configuration.
   * @param conf the configuration to alter
   * @param uri the new default filesystem uri",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLink,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.net.URI)",55,59,"/**
* Adds a link configuration for a mount table.
* @param conf Configuration object.
* @param mountTableName Mount table name.
* @param src Source path.
* @param target Link target URI.
*/
","* Add a link to the config for the specified mount table
   * @param conf - add the link to this conf
   * @param mountTableName mountTable.
   * @param src - the src path name
   * @param target - the target URI link",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMergeSlash,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",79,83,"/**
 * Sets the link merge slash configuration for a mount table.
 * @param conf Configuration object.
 * @param mountTableName Mount table name.
 * @param target Target URI.
 */
","* Add a LinkMergeSlash to the config for the specified mount table.
   *
   * @param conf configuration.
   * @param mountTableName mountTable.
   * @param target target.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkFallback,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",102,106,"/**
 * Sets link fallback URI for a mount table in configuration.
 * @param conf Configuration object.
 * @param mountTableName Mount table name.
 * @param target Target URI for link fallback.
 */
","* Add a LinkFallback to the config for the specified mount table.
   *
   * @param conf configuration.
   * @param mountTableName mountTable.
   * @param target targets.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMerge,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])",125,129,"/**
 * Sets the link merge targets for a configuration.
 * @param conf Configuration object.
 * @param mountTableName Mount table name.
 * @param targets Array of target URIs.
 */
","* Add a LinkMerge to the config for the specified mount table.
   *
   * @param conf configuration.
   * @param mountTableName mountTable.
   * @param targets targets.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkNfly,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",150,156,"/**
 * Sets a configuration value for a link using NFly configuration.
 * @param conf Configuration object.
 * @param mountTableName Mount table name.
 */
","* Add nfly link to configuration for the given mount table.
   *
   * @param conf configuration.
   * @param mountTableName mount table.
   * @param src src.
   * @param settings settings.
   * @param targets targets.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkRegex,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkRegex(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",191,202,"/**
 * Adds a link regex configuration to the given configuration.
 * @param conf Configuration object.
 * @param mountTableName Mount table name.
 * @param srcRegex Source regex.
 * @param targetStr Target string.
 */
","* Add a LinkRegex to the config for the specified mount table.
   * @param conf - get mountable config from this conf
   * @param mountTableName - the mountable name of the regex config item
   * @param srcRegex - the src path regex expression that applies to this config
   * @param targetStr - the string of target path
   * @param interceptorSettings - the serialized interceptor string to be
   *                            applied while resolving the mapping",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,setHomeDirConf,"org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",220,228,"/**
 * Sets the home directory configuration for a mount table.
 * @param conf Configuration object.
 * @param mountTableName Mount table name.
 * @param homedir Home directory path.
 */
","* Add config variable for homedir the specified mount table
   * @param conf - add to this conf
   * @param homedir - the home dir path starting with slash
   * @param mountTableName - the mount table.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,setUMask,"org.apache.hadoop.fs.permission.FsPermission:setUMask(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.permission.FsPermission)",400,402,"/**
 * Sets the umask for the configuration.
 * @param conf Configuration object to set umask on.
 * @param umask FsPermission representing the umask value.
 */
","* Set the user file creation mask (umask)
   * @param conf configuration.
   * @param umask umask.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,setCodecClasses,"org.apache.hadoop.io.compress.CompressionCodecFactory:setCodecClasses(org.apache.hadoop.conf.Configuration,java.util.List)",156,169,"/**
 * Sets the codec classes for the given configuration.
 * @param conf Configuration object.
 * @param classes List of codec class objects.
 */
","* Sets a list of codec classes in the configuration. In addition to any
   * classes specified using this method, {@link CompressionCodec} classes on
   * the classpath are discovered using a Java ServiceLoader.
   * @param conf the configuration to modify
   * @param classes the list of classes to set",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,setDefaultCompressionType,"org.apache.hadoop.io.SequenceFile:setDefaultCompressionType(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$CompressionType)",262,265,"/**
 * Sets the default compression type for a Hadoop job configuration.
 * @param job The Hadoop job configuration.
 * @param val The CompressionType to set.
 */
","* Set the default compression type for sequence files.
   * @param job the configuration to modify
   * @param val the new compression type (none, block, record)",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,getProxyuserConfiguration,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig),107,119,"/**
 * Retrieves proxyuser configuration from filter init parameters.
 * @param filterConfig Filter configuration object
 * @return Configuration object with proxyuser settings
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,getProxyuserConfiguration,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:getProxyuserConfiguration(javax.servlet.FilterConfig),158,174,"/**
 * Extracts proxyuser configuration from filter config.
 * @param filterConfig Servlet filter configuration.
 * @return Configuration object with proxyuser settings.
 */
","* Returns the proxyuser configuration. All returned properties must start
   * with <code>proxyuser.</code>'
   * <p>
   * Subclasses may override this method if the proxyuser configuration is 
   * read from other place than the filter init parameters.
   *
   * @param filterConfig filter configuration object
   * @return the proxyuser configuration properties.
   * @throws ServletException thrown if the configuration could not be created.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,prepareConf,org.apache.hadoop.security.CompositeGroupsMapping:prepareConf(java.lang.String),175,191,"/**
 * Prepares a configuration for a specific provider.
 * @param providerName Provider name to configure.
 * @return Configuration object with provider-specific settings.
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,init,org.apache.hadoop.security.alias.CredentialShell:init(java.lang.String[]),78,126,"/**
 * Parses command-line arguments and sets the sub-command.
 * @param args Command-line arguments passed to the tool.
 * @return 0 on success, 1 on failure.
 */
","* Parse the command line arguments and initialize the data.
   * <pre>
   * % hadoop credential create alias [-provider providerPath]
   * % hadoop credential list [-provider providerPath]
   * % hadoop credential check alias [-provider providerPath]
   * % hadoop credential delete alias [-provider providerPath] [-f]
   * </pre>
   * @param args args.
   * @return 0 if the argument(s) were recognized, 1 otherwise
   * @throws IOException raised on errors performing I/O.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setAuthenticationMethod,"org.apache.hadoop.security.SecurityUtil:setAuthenticationMethod(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod,org.apache.hadoop.conf.Configuration)",741,748,"/**
 * Sets the authentication method in the configuration.
 * @param authenticationMethod Authentication method to set.
 * @param conf Configuration object to update.
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setInt,"org.apache.hadoop.conf.Configuration:setInt(java.lang.String,int)",1582,1584,"/**
 * Sets the value of a named property to an integer.
 * @param name Property name.
 * @param value Integer value to set.
 */
","* Set the value of the <code>name</code> property to an <code>int</code>.
   * 
   * @param name property name.
   * @param value <code>int</code> value of the property.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setLong,"org.apache.hadoop.conf.Configuration:setLong(java.lang.String,long)",1655,1657,"/**
* Sets the specified property to the given long value.
* @param name property name
* @param value long value to set
*/
","* Set the value of the <code>name</code> property to a <code>long</code>.
   * 
   * @param name property name.
   * @param value <code>long</code> value of the property.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setFloat,"org.apache.hadoop.conf.Configuration:setFloat(java.lang.String,float)",1684,1686,"/**
* Sets the value of a property to the given float value.
* @param name Property name.
* @param value Float value to set.
*/
","* Set the value of the <code>name</code> property to a <code>float</code>.
   * 
   * @param name property name.
   * @param value property value.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setDouble,"org.apache.hadoop.conf.Configuration:setDouble(java.lang.String,double)",1713,1715,"/**
* Sets the value of a property to a double, converting it to a string.
* @param name Property name.
* @param value Double value to set.
*/
","* Set the value of the <code>name</code> property to a <code>double</code>.
   * 
   * @param name property name.
   * @param value property value.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setBoolean,"org.apache.hadoop.conf.Configuration:setBoolean(java.lang.String,boolean)",1750,1752,"/**
* Sets the value of a property to the given boolean.
* @param name Property name.
* @param value Boolean value to set.
*/
","* Set the value of the <code>name</code> property to a <code>boolean</code>.
   * 
   * @param name property name.
   * @param value <code>boolean</code> value of the property.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setTimeDuration,"org.apache.hadoop.conf.Configuration:setTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)",1868,1870,"/**
* Sets the time duration with a unit suffix.
* @param name duration name, value in specified unit
*/
","* Set the value of <code>name</code> to the given time duration. This
   * is equivalent to <code>set(&lt;name&gt;, value + &lt;time suffix&gt;)</code>.
   * @param name Property name
   * @param value Time duration
   * @param unit Unit of time",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setStorageSize,"org.apache.hadoop.conf.Configuration:setStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)",2039,2041,"/**
 * Sets the storage size with a unit identifier.
 * @param name storage name
 * @param value size value
 * @param unit storage unit
 */
","* Sets Storage Size for the specified key.
   *
   * @param name - Key to set.
   * @param value - The numeric value to set.
   * @param unit - Storage Unit to be used.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setPattern,"org.apache.hadoop.conf.Configuration:setPattern(java.lang.String,java.util.regex.Pattern)",2089,2092,"/**
 * Sets a pattern associated with the given name.
 * @param name Pattern name
 * @param pattern Pattern object to associate
 */
","* Set the given property to <code>Pattern</code>.
   * If the pattern is passed as null, sets the empty pattern which results in
   * further calls to getPattern(...) returning the default value.
   *
   * @param name property name
   * @param pattern new value",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setStrings,"org.apache.hadoop.conf.Configuration:setStrings(java.lang.String,java.lang.String[])",2405,2407,"/**
* Sets a string value associated with a name from a string array.
* @param name key for the value
* @param values string array to be concatenated
*/
","* Set the array of string values for the <code>name</code> property as 
   * as comma delimited values.  
   * 
   * @param name property name.
   * @param values The values",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setSocketAddr,"org.apache.hadoop.conf.Configuration:setSocketAddr(java.lang.String,java.net.InetSocketAddress)",2578,2580,"/**
 * Sets the socket address for the given name.
 * @param name Identifier for the address.
 * @param addr The InetSocketAddress to set.
 */
","* Set the socket address for the <code>name</code> property as
   * a <code>host:port</code>.
   * @param name property name.
   * @param addr inetSocketAddress addr.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setClass,"org.apache.hadoop.conf.Configuration:setClass(java.lang.String,java.lang.Class,java.lang.Class)",2811,2815,"/**
* Sets a class name and validates its assignability.
* @param name class name
* @param theClass the class object
* @param xface interface the class must implement
*/
","* Set the value of the <code>name</code> property to the name of a 
   * <code>theClass</code> implementing the given interface <code>xface</code>.
   * 
   * An exception is thrown if <code>theClass</code> does not implement the 
   * interface <code>xface</code>. 
   * 
   * @param name property name.
   * @param theClass property value.
   * @param xface the interface implemented by the named class.",,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,readFields,org.apache.hadoop.conf.Configuration:readFields(java.io.DataInput),3949,3962,"/**
 * Reads fields from DataInput, populating the object's data.
 * @param in DataInput stream to read from.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,12
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDelete,org.apache.hadoop.fs.FileUtil:fullyDelete(java.io.File),169,171,"/**
* Deletes a directory and all its contents recursively.
*/
","* Delete a directory and all its contents.  If
   * we return false, the directory may be partially-deleted.
   * (1) If dir is symlink to a file, the symlink is deleted. The file pointed
   *     to by the symlink is not deleted.
   * (2) If dir is symlink to a directory, symlink is deleted. The directory
   *     pointed to by symlink is not deleted.
   * (3) If dir is a normal file, it is deleted.
   * (4) If dir is a normal directory, then dir and all its contents recursively
   *     are deleted.
   * @param dir dir.
   * @return fully delete status.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDeleteContents,"org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File,boolean)",282,316,"/**
 * Deletes all contents of a directory.
 * @param dir Directory to clear.
 * @param tryGrantPermissions Whether to grant permissions.
 * @return True if deletion succeeded, false otherwise.
 */","* Delete the contents of a directory, not the directory itself.  If
   * we return false, the directory may be partially-deleted.
   * If dir is a symlink to a directory, all the contents of the actual
   * directory pointed to by dir will be deleted.
   *
   * @param dir dir.
   * @param tryGrantPermissions if 'true', try grant +rwx permissions to this
   * and all the underlying directories before trying to delete their contents.
   * @return fully delete contents status.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsUsage.java,processPath,org.apache.hadoop.fs.shell.FsUsage$Df:processPath(org.apache.hadoop.fs.shell.PathData),129,157,"/**
 * Processes a path data item, handling ViewFileSystem status.
 * @param item PathData object containing file system info.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,open,"org.apache.hadoop.fs.RawLocalFileSystem:open(org.apache.hadoop.fs.Path,int)",392,397,"/**
 * Opens a file for input.
 * @param f Path to the file
 * @param bufferSize I/O buffer size
 * @return FSDataInputStream for reading the file
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,open,"org.apache.hadoop.fs.RawLocalFileSystem:open(org.apache.hadoop.fs.PathHandle,int)",399,409,"/**
 * Opens an input stream for the file identified by the path handle.
 * @param fd PathHandle representing the file to open.
 * @param bufferSize Buffer size for the input stream.
 * @return FSDataInputStream for reading the file.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,append,"org.apache.hadoop.fs.RawLocalFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",535,545,"/**
 * Appends data to a file.
 * @param f Path to file.
 * @param bufferSize Output stream buffer size.
 * @param progress Progressable object for tracking progress.
 * @return FSDataOutputStream for appending data.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,truncate,"org.apache.hadoop.fs.RawLocalFileSystem:truncate(org.apache.hadoop.fs.Path,long)",675,698,"/**
 * Truncates a file to the specified length.
 * @param f Path to the file.
 * @param newLength New length of the file.
 * @return True on success.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,listStatus,org.apache.hadoop.fs.RawLocalFileSystem:listStatus(org.apache.hadoop.fs.Path),729,766,"/**
 * Lists status of files/directories under the given path.
 * @param f the path to list
 * @return Array of FileStatus objects, or empty array if none.
 */
","* {@inheritDoc}
   *
   * (<b>Note</b>: Returned list is not sorted in any given order,
   * due to reliance on Java's {@link File#list()} API.)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,deprecatedGetFileLinkStatusInternal,org.apache.hadoop.fs.RawLocalFileSystem:deprecatedGetFileLinkStatusInternal(org.apache.hadoop.fs.Path),1248,1285,"/**
 * Gets file status, handling symlinks and potential errors.
 * @param f Path to the file/symlink.
 * @return FileStatus object or throws IOException.
 */
","* Deprecated. Remains for legacy support. Should be removed when {@link Stat}
   * gains support for Windows and other operating systems.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,fixFileStatus,"org.apache.hadoop.fs.viewfs.ViewFileSystem:fixFileStatus(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.Path)",532,550,"/**
* Adjusts a FileStatus to use a qualified path, wrapping local statuses.
* @param orig The original FileStatus to adjust.
* @param qualified The qualified path to set.
* @return Adjusted FileStatus.
*/
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,delete,"org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(com.jcraft.jsch.ChannelSftp,org.apache.hadoop.fs.Path,boolean)",370,414,"/**
 * Deletes a file or directory on the SFTP channel.
 * @param channel SFTP channel to use
 * @param file Path to delete
 * @param recursive Whether to delete recursively
 * @return True if deletion was successful, false otherwise.
 */
","* Convenience method, so that we don't open a new connection when using this
   * method from within another method. Otherwise every API invocation incurs
   * the overhead of opening/closing a TCP connection.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,<init>,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:<init>(java.io.InputStream),354,356,"/**
 * Creates a BZip2CompressionInputStream using the provided input stream.
 * @param in The input stream to be decompressed.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createInputStream,"org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,long,long,org.apache.hadoop.io.compress.SplittableCompressionCodec$READ_MODE)",200,211,"/**
 * Creates a SplitCompressionInputStream for decompression.
 * @param seekableIn Input stream to decompress.
 * @param decompressor Decompressor object.
 * @param start Start offset.
 * @param end End offset.
 * @param readMode Read mode.
 * @return SplitCompressionInputStream instance.
 */
","* Creates CompressionInputStream to be used to read off uncompressed data
   * in one of the two reading modes. i.e. Continuous or Blocked reading modes
   *
   * @param seekableIn The InputStream
   * @param start The start offset into the compressed stream
   * @param end The end offset into the compressed stream
   * @param readMode Controls whether progress is reported continuously or
   *                 only at block boundaries.
   *
   * @return CompressionInputStream for BZip2 aligned at block boundaries",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,read,"org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read(byte[],int,int)",483,522,"/**
 * Reads up to {@code len} bytes from the stream into the buffer.
 * @param b buffer to read into
 * @param off offset in the buffer
 * @param len number of bytes to read
 * @return number of bytes read
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,add,"org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:add(java.lang.Object,java.lang.reflect.Field)",133,159,"/**
 * Adds a MutableMetric to the specified field, if not already set.
 * @param source Object containing the field to be annotated.
 * @param field Field to be annotated with a Metric.
 */
","* Change the declared field {@code field} in {@code source} Object to
   * {@link MutableMetric}",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/ganglia/GangliaSink30.java,init,org.apache.hadoop.metrics2.sink.ganglia.GangliaSink30:init(org.apache.commons.configuration2.SubsetConfiguration),57,84,"/**
 * Initializes the configuration, processing tags for specified contexts.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getCanonicalServiceName,org.apache.hadoop.fs.DelegateToFileSystem:getCanonicalServiceName(),262,265,"/**
 * Returns the canonical service name of the file system.
 * Delegates to the underlying file system implementation.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getCanonicalServiceName,org.apache.hadoop.fs.FilterFs:getCanonicalServiceName(),312,315,"/**
 * Returns the canonical service name of the underlying file system.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/StorageType.java,getConf,"org.apache.hadoop.fs.StorageType:getConf(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.StorageType,java.lang.String)",120,123,"/**
 * Retrieves configuration value by key.
 * @param conf Configuration object.
 * @param t StorageType enum.
 * @param name Configuration key name.
 */
","* Get the configured values for different StorageType.
   * @param conf - absolute or fully qualified path
   * @param t - the StorageType
   * @param name - the sub-name of key
   * @return the file system of the path",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getTransferMode,org.apache.hadoop.fs.ftp.FTPFileSystem:getTransferMode(org.apache.hadoop.conf.Configuration),188,209,"/**
 * Determines the FTP transfer mode from the configuration.
 * @param conf Configuration object to read transfer mode from.
 * @return FTP transfer mode constant.
 */
","* Set FTP's transfer mode based on configuration. Valid values are
   * STREAM_TRANSFER_MODE, BLOCK_TRANSFER_MODE and COMPRESSED_TRANSFER_MODE.
   * <p>
   * Defaults to BLOCK_TRANSFER_MODE.
   *
   * @param conf
   * @return",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,setDataConnectionMode,"org.apache.hadoop.fs.ftp.FTPFileSystem:setDataConnectionMode(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.conf.Configuration)",222,240,"/**
 * Sets the data connection mode for the FTP client based on config.
 * @param client FTP client to configure
 * @param conf Configuration object containing mode setting
 * @throws IOException if an I/O error occurs
 */
","* Set the FTPClient's data connection mode based on configuration. Valid
   * values are ACTIVE_LOCAL_DATA_CONNECTION_MODE,
   * PASSIVE_LOCAL_DATA_CONNECTION_MODE and PASSIVE_REMOTE_DATA_CONNECTION_MODE.
   * <p>
   * Defaults to ACTIVE_LOCAL_DATA_CONNECTION_MODE.
   *
   * @param client
   * @param conf
   * @throws IOException",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getHomeDirValue,"org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration,java.lang.String)",245,249,"/**
 * Gets the home directory value from the configuration.
 * @param conf Configuration object.
 * @param mountTableName Mount table name prefix.
 * @return Home directory value as a string.
 */
","* Get the value of the home dir conf value for specified mount table
   * @param conf - from this conf
   * @param mountTableName - the mount table
   * @return home dir value, null if variable is not in conf",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/permission/FsPermission.java,getUMask,org.apache.hadoop.fs.permission.FsPermission:getUMask(org.apache.hadoop.conf.Configuration),325,349,"/**
 * Gets the umask from the configuration.
 * @param conf Hadoop configuration object.
 * @return FsPermission object representing the umask.
 */
","* Get the user file creation mask (umask)
   * 
   * {@code UMASK_LABEL} config param has umask value that is either symbolic 
   * or octal.
   * 
   * Symbolic umask is applied relative to file mode creation mask; 
   * the permission op characters '+' clears the corresponding bit in the mask, 
   * '-' sets bits in the mask.
   * 
   * Octal umask, the specified bits are set in the file mode creation mask.
   *
   * @param conf configuration.
   * @return FsPermission UMask.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,<init>,"org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",791,796,"/**
 * Constructs a DiskBlockFactory with a buffer directory.
 * @param keyToBufferDir key for buffer directory in config
 * @param conf Hadoop configuration object
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,getCodecClasses,org.apache.hadoop.io.compress.CompressionCodecFactory:getCodecClasses(org.apache.hadoop.conf.Configuration),111,147,"/**
 * Retrieves a list of CompressionCodec classes from configuration.
 * @param conf Hadoop configuration object.
 * @return List of CompressionCodec classes.
 */
","* Get the list of codecs discovered via a Java ServiceLoader, or
   * listed in the configuration. Codecs specified in configuration come
   * later in the returned list, and are considered to override those
   * from the ServiceLoader.
   * @param conf the configuration to look in
   * @return a list of the {@link CompressionCodec} classes",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getDefaultCompressionType,org.apache.hadoop.io.SequenceFile:getDefaultCompressionType(org.apache.hadoop.conf.Configuration),251,255,"/**
 * Gets the default compression type from the configuration.
 * @param job The Hadoop job configuration.
 * @return CompressionType, RECORD if not specified.
 */
","* Get the compression type for the reduce outputs
   * @param job the job config to look in
   * @return the kind of compression to use",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/SocksSocketFactory.java,setConf,org.apache.hadoop.net.SocksSocketFactory:setConf(org.apache.hadoop.conf.Configuration),135,142,"/**
 * Sets the configuration and configures the proxy if specified.
 * @param conf The Hadoop configuration to set.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/AbstractDNSToSwitchMapping.java,isSingleSwitchByScriptPolicy,org.apache.hadoop.net.AbstractDNSToSwitchMapping:isSingleSwitchByScriptPolicy(),135,138,"/**
 * Checks if single switch is configured via script policy.
 * Returns true if script file name is not set in config.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,createWebAppContext,"org.apache.hadoop.http.HttpServer2:createWebAppContext(org.apache.hadoop.http.HttpServer2$Builder,org.apache.hadoop.security.authorize.AccessControlList,java.lang.String)",834,860,"/**
 * Creates a WebAppContext with specified configuration.
 * @param b Builder object for context configuration
 * @param adminsAcl Access control list for administrators
 * @param appDir Application directory
 * @return WebAppContext instance
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,stringifySecurityProperty,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:stringifySecurityProperty(java.lang.String),314,333,"/**
 * Creates a string representation of a security property.
 * @param property The name of the property to stringify.
 * @return String representation of the property value.
 */
","* Turn a security property into a nicely formatted set of <i>name=value</i>
   * strings, allowing for either the property or the configuration not to be
   * set.
   *
   * @param property the property to stringify
   * @return the stringified property",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateHadoopTokenFiles,org.apache.hadoop.security.KDiag:validateHadoopTokenFiles(org.apache.hadoop.conf.Configuration),521,553,"/**
 * Validates Hadoop token files based on system properties & config.
 */","* Validate that hadoop.token.files (if specified) exist and are valid.
   * @throws ClassNotFoundException
   * @throws SecurityException
   * @throws NoSuchMethodException
   * @throws KerberosDiagsFailure",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,locateKeystore,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:locateKeystore(),314,339,"/**
 * Locates and loads the keystore, using environment variables or defaults.
 */","* Open up and initialize the keyStore.
   *
   * @throws IOException If there is a problem reading the password file
   * or a problem reading the keystore.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,needsPassword,org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:needsPassword(),341,346,"/**
 * Checks if a password is needed based on environment/file config.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getLocalHostName,org.apache.hadoop.security.SecurityUtil:getLocalHostName(org.apache.hadoop.conf.Configuration),255,272,"/**
 * Gets the local hostname, using config or falling back to default.
 * @param conf Hadoop configuration; null for default behavior.
 * @return Hostname string.
 * @throws UnknownHostException if hostname resolution fails.
 */
","* Retrieve the name of the current host. Multihomed hosts may restrict the
   * hostname lookup to a specific interface and nameserver with {@link
   * org.apache.hadoop.fs.CommonConfigurationKeysPublic#HADOOP_SECURITY_DNS_INTERFACE_KEY}
   * and {@link org.apache.hadoop.fs.CommonConfigurationKeysPublic#HADOOP_SECURITY_DNS_NAMESERVER_KEY}
   *
   * @param conf Configuration object. May be null.
   * @return
   * @throws UnknownHostException",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getClientPrincipal,"org.apache.hadoop.security.SecurityUtil:getClientPrincipal(java.lang.Class,org.apache.hadoop.conf.Configuration)",404,413,"/**
 * Gets the client principal from Kerberos info, or config value.
 * @param protocol Protocol class.
 * @param conf Hadoop configuration.
 * @return Client principal string or null if not found.
 */
","* Look up the client principal for a given protocol. It searches all known
   * SecurityInfo providers.
   * @param protocol the protocol class to get the information for
   * @param conf configuration object
   * @return client principal or null if it has no client principal defined.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,needsPassword,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:needsPassword(),308,313,"/**
 * Checks if a password is needed based on environment/file.
 * @return True if password is needed, false otherwise.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,getMetricsTimeUnit,org.apache.hadoop.ipc.metrics.RpcMetrics:getMetricsTimeUnit(org.apache.hadoop.conf.Configuration),189,204,"/**
 * Gets the time unit for RPC metrics from configuration.
 * @param conf Configuration object to retrieve the time unit from.
 * @return The TimeUnit for RPC metrics.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,validateSslConfiguration,org.apache.hadoop.util.curator.ZKCuratorManager:validateSslConfiguration(org.apache.hadoop.conf.Configuration),196,221,"/**
 * Validates ZooKeeper SSL configuration parameters in the given config.
 * @param config Configuration object containing SSL parameters.
 * @throws IOException if any required SSL parameter is missing.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmed,org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String),1320,1328,"/**
 * Returns the trimmed version of the given string.
 * @param name The string to trim.
 * @return The trimmed string, or null if input is null.
 */
","* Get the value of the <code>name</code> property as a trimmed <code>String</code>, 
   * <code>null</code> if no such property exists. 
   * If the key is deprecated, it returns the value of
   * the first key which replaces the deprecated key and is not null
   * 
   * Values are processed for <a href=""#VariableExpansion"">variable expansion</a> 
   * before being returned. 
   * 
   * @param name the property name.
   * @return the value of the <code>name</code> or its replacing property, 
   *         or null if no such property exists.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setIfUnset,"org.apache.hadoop.conf.Configuration:setIfUnset(java.lang.String,java.lang.String)",1499,1503,"/**
 * Sets the value for the given name only if it's currently unset.
 */","* Sets a property if it is currently unset.
   * @param name the property name
   * @param value the new value",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",1906,1914,"/**
 * Gets time duration for given name, using default if not found.
 * @param name Duration name, @param defaultValue Default value.
 */
","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d). If no unit is
   * provided, the default unit is applied.
   *
   * @param name Property name
   * @param defaultValue Value returned if no mapping exists.
   * @param defaultUnit Default time unit if no valid suffix is provided.
   * @param returnUnit The unit used for the returned value.
   * @throws NumberFormatException If the property stripped of its unit is not
   *         a number
   * @return time duration in given time unit",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit,java.util.concurrent.TimeUnit)",1916,1924,"/**
 * Gets time duration, using value or default if not found.
 * @param name Property name.
 * @param defaultValue Default value if property is missing.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStorageSize,"org.apache.hadoop.conf.Configuration:getStorageSize(java.lang.String,java.lang.String,org.apache.hadoop.conf.StorageUnit)",1988,2005,"/**
 * Gets storage size by name, using default if missing, and converts unit.
 * @param name Key for storage size.
 * @param defaultValue Default storage size string.
 * @param targetUnit Target storage unit.
 * @return Storage size in the target unit.
 */
","* Gets the Storage Size from the config, or returns the defaultValue. The
   * unit of return value is specified in target unit.
   *
   * @param name - Key Name
   * @param defaultValue - Default Value -- e.g. 100MB
   * @param targetUnit - The units that we want result to be in.
   * @return double -- formatted in target Units",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStorageSize,"org.apache.hadoop.conf.Configuration:getStorageSize(java.lang.String,double,org.apache.hadoop.conf.StorageUnit)",2017,2030,"/**
 * Gets storage size by name, using default if not found.
 * @param name Storage size name.
 * @param defaultValue Default value if not found.
 * @param targetUnit Unit to convert to.
 * @return Storage size in target unit.
 */
","* Gets storage size from a config file.
   *
   * @param name - Key to read.
   * @param defaultValue - The default value to return in case the key is
   * not present.
   * @param targetUnit - The Storage unit that should be used
   * for the return value.
   * @return - double value in the Storage Unit specified.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPattern,"org.apache.hadoop.conf.Configuration:getPattern(java.lang.String,java.util.regex.Pattern)",2067,2079,"/**
 * Retrieves a Pattern by name, using defaultValue if invalid.
 * @param name Pattern name.
 * @param defaultValue Default Pattern to use if invalid.
 * @return Pattern object.
 */
","* Get the value of the <code>name</code> property as a <code>Pattern</code>.
   * If no such property is specified, or if the specified value is not a valid
   * <code>Pattern</code>, then <code>DefaultValue</code> is returned.
   * Note that the returned value is NOT trimmed by this method.
   *
   * @param name property name
   * @param defaultValue default value
   * @return property value as a compiled Pattern, or defaultValue",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStringCollection,org.apache.hadoop.conf.Configuration:getStringCollection(java.lang.String),2310,2313,"/**
 * Parses a string into a collection of strings.
 * @param name key to retrieve value string from store
 * @return Collection of strings parsed from the value.
 */
","* Get the comma delimited values of the <code>name</code> property as 
   * a collection of <code>String</code>s.  
   * If no such property is specified then empty collection is returned.
   * <p>
   * This is an optimized version of {@link #getStrings(String)}
   * 
   * @param name property name.
   * @return property value as a collection of <code>String</code>s.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStrings,org.apache.hadoop.conf.Configuration:getStrings(java.lang.String),2324,2327,"/**
 * Parses a string value into an array of strings.
 * @param name Key to retrieve the string value for.
 * @return String array parsed from the value.
 */
","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s.  
   * If no such property is specified then <code>null</code> is returned.
   * 
   * @param name property name.
   * @return property value as an array of <code>String</code>s, 
   *         or <code>null</code>.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getStrings,"org.apache.hadoop.conf.Configuration:getStrings(java.lang.String,java.lang.String[])",2339,2346,"/**
 * Gets strings by name, returns default if not found.
 * @param name property name
 * @param defaultValue default string array
 * @return String array or default if name not found
 */
","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s.  
   * If no such property is specified then default value is returned.
   * 
   * @param name property name.
   * @param defaultValue The default value
   * @return property value as an array of <code>String</code>s, 
   *         or default value.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmedStringCollection,org.apache.hadoop.conf.Configuration:getTrimmedStringCollection(java.lang.String),2356,2363,"/**
 * Gets a trimmed string collection from a named value.
 * @param name Key to retrieve the value from.
 * @return Collection of trimmed strings, or empty if null.
 */
","* Get the comma delimited values of the <code>name</code> property as 
   * a collection of <code>String</code>s, trimmed of the leading and trailing whitespace.  
   * If no such property is specified then empty <code>Collection</code> is returned.
   *
   * @param name property name.
   * @return property value as a collection of <code>String</code>s, or empty <code>Collection</code>",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmedStrings,org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String),2374,2377,"/**
 * Gets trimmed string array from value associated with name.
 * @param name Key to retrieve value from underlying store.
 * @return Array of trimmed strings, or empty array if null.
 */
","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s, trimmed of the leading and trailing whitespace.
   * If no such property is specified then an empty array is returned.
   * 
   * @param name property name.
   * @return property value as an array of trimmed <code>String</code>s, 
   *         or empty array.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmedStrings,"org.apache.hadoop.conf.Configuration:getTrimmedStrings(java.lang.String,java.lang.String[])",2389,2396,"/**
 * Gets trimmed strings from 'name', or uses default if null.
 * @param name Property name.
 * @param defaultValue Default string array.
 * @return Trimmed string array.
 */
","* Get the comma delimited values of the <code>name</code> property as 
   * an array of <code>String</code>s, trimmed of the leading and trailing whitespace.
   * If no such property is specified then default value is returned.
   * 
   * @param name property name.
   * @param defaultValue The default value
   * @return property value as an array of trimmed <code>String</code>s, 
   *         or default value.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPropsWithPrefix,org.apache.hadoop.conf.Configuration:getPropsWithPrefix(java.lang.String),3032,3043,"/**
 * Returns props with a specified prefix.
 * @param confPrefix Prefix to filter properties by.
 * @return Map of properties starting with the prefix.
 */
","* Constructs a mapping of configuration and includes all properties that
   * start with the specified configuration prefix.  Property names in the
   * mapping are trimmed to remove the configuration prefix.
   *
   * @param confPrefix configuration prefix
   * @return mapping of configuration properties with prefix stripped",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,appendJSONProperty,"org.apache.hadoop.conf.Configuration:appendJSONProperty(com.fasterxml.jackson.core.JsonGenerator,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.conf.ConfigRedactor)",3861,3881,"/**
 * Appends a JSON property to the generator with name, value, & resource.
 * @param jsonGen JsonGenerator instance
 * @param config Configuration object
 */
","* Write property and its attributes as json format to given
   * {@link JsonGenerator}.
   *
   * @param jsonGen json writer
   * @param config configuration
   * @param name property name
   * @throws IOException raised on errors performing I/O.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationUtil.java,getChangedProperties,"org.apache.hadoop.conf.ReconfigurationUtil:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",39,65,"/**
 * Returns a collection of PropertyChange objects for changed properties.
 * @param newConf The new configuration.
 * @param oldConf The old configuration.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,reconfigureProperty,"org.apache.hadoop.conf.ReconfigurableBase:reconfigureProperty(java.lang.String,java.lang.String)",223,241,"/**
* Reconfigures a property with a new value.
* @param property Property to reconfigure.
* @param newVal New value for the property.
* @throws ReconfigurationException if property is not reconfigurable.
*/
","* {@inheritDoc}
   *
   * This method makes the change to this objects {@link Configuration}
   * and calls reconfigurePropertyImpl to update internal data structures.
   * This method cannot be overridden, subclasses should instead override
   * reconfigurePropertyImpl.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,get,org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String),46,51,"/**
 * Retrieves a value by name, logs the redaction, and returns it.
 */",* See {@link Configuration#get(String)}.,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/NodeFencer.java,create,"org.apache.hadoop.ha.NodeFencer:create(org.apache.hadoop.conf.Configuration,java.lang.String)",83,90,"/**
 * Creates a NodeFencer instance from configuration.
 * @param conf Configuration object.
 * @param confKey Key to retrieve fencer string from config.
 * @return NodeFencer instance or null if key is not found.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getDefaultMountTableName,org.apache.hadoop.fs.viewfs.ConfigUtil:getDefaultMountTableName(org.apache.hadoop.conf.Configuration),260,263,"/**
 * Gets the default ViewFS mount table name from config.
 * @param conf Configuration object to retrieve the value from.
 * @return Default mount table name.
 */
","* Get the name of the default mount table to use. If
   * {@link Constants#CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE_NAME_KEY} is specified,
   * it's value is returned. Otherwise,
   * {@link Constants#CONFIG_VIEWFS_DEFAULT_MOUNT_TABLE} is returned.
   *
   * @param conf Configuration to use.
   * @return the name of the default mount table to use.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,getCodecClassName,"org.apache.hadoop.io.erasurecode.CodecUtil:getCodecClassName(org.apache.hadoop.conf.Configuration,java.lang.String)",256,285,"/**
 * Gets the class name for a given codec based on configuration.
 * @param conf Configuration object.
 * @param codec Codec identifier.
 * @return Class name string.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,isNativeBzip2Loaded,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:isNativeBzip2Loaded(org.apache.hadoop.conf.Configuration),47,70,"/**
 * Checks if the native Bzip2 library is loaded.
 * @param conf Hadoop configuration object.
 * @return True if native Bzip2 is loaded, false otherwise.
 */
","* Check if native-bzip2 code is loaded &amp; initialized correctly and
   * can be loaded for this job.
   * 
   * @param conf configuration
   * @return <code>true</code> if native-bzip2 is loaded &amp; initialized
   *         and can be loaded for this job, else <code>false</code>",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getDefaultSocketFactory,org.apache.hadoop.net.NetUtils:getDefaultSocketFactory(org.apache.hadoop.conf.Configuration),121,130,"/**
 * Gets the default socket factory from configuration.
 * @param conf Hadoop configuration object
 * @return SocketFactory instance
 */
","* Get the default socket factory as specified by the configuration
   * parameter <tt>hadoop.rpc.socket.factory.default</tt>
   * 
   * @param conf the configuration
   * @return the default socket factory as specified in the configuration or
   *         the JVM default socket factory if the configuration does not
   *         contain a default socket factory property.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,load,org.apache.hadoop.net.TableMapping$RawTableMapping:load(),93,125,"/**
 * Loads table mappings from a file. Returns a map or null on failure.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/lib/StaticUserWebFilter.java,getUsernameFromConf,org.apache.hadoop.http.lib.StaticUserWebFilter:getUsernameFromConf(org.apache.hadoop.conf.Configuration),133,146,"/**
 * Gets the username from the configuration.
 * Uses DEPRECATED_UGI_KEY or HADOOP_HTTP_STATIC_USER.
 */
",* Retrieve the static username from the configuration.,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,setEnabledProtocols,org.apache.hadoop.http.HttpServer2$Builder:setEnabledProtocols(org.eclipse.jetty.util.ssl.SslContextFactory),665,696,"/**
 * Resets excluded SSL protocols based on configuration.
 * @param sslContextFactory SslContextFactory to modify.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,parseStaticMapping,org.apache.hadoop.security.Groups:parseStaticMapping(org.apache.hadoop.conf.Configuration),164,191,"/**
 * Parses static user-to-group mappings from configuration.
 * Reads and processes static user group overrides.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,printConfOpt,org.apache.hadoop.security.KDiag:printConfOpt(java.lang.String),907,909,"/**
 * Prints configuration option with its value.
 * @param option configuration option name
 */
","* Print a configuration option, or {@link #UNSET} if unset.
   *
   * @param option option to print",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,<init>,org.apache.hadoop.security.SecurityUtil$TruststoreKeystore:<init>(org.apache.hadoop.conf.Configuration),868,873,"/**
 * Initializes TruststoreKeystore with configuration parameters.
 * @param conf Configuration object containing SSL settings.
 */
","* Configuration for the ZooKeeper connection when SSL/TLS is enabled.
     * When a value is not configured, ensure that empty string is set instead of null.
     *
     * @param conf ZooKeeper Client configuration",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getDirContext,org.apache.hadoop.security.LdapGroupsMapping:getDirContext(),653,706,"/**
 * Retrieves a DirContext object for LDAP connectivity.
 * @return DirContext object or existing instance if available.
 * @throws NamingException if an error occurs during LDAP connection.
 */",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,spawnAutoRenewalThreadForUserCreds,org.apache.hadoop.security.UserGroupInformation:spawnAutoRenewalThreadForUserCreds(boolean),882,899,"/**
 * Spawns a thread to renew user credentials, optionally forced.
 */","* Spawn a thread to do periodic renewals of kerberos credentials. NEVER
   * directly call this method. This method should only be used for ticket cache
   * based kerberos credentials.
   *
   * @param force - used by tests to forcibly spawn thread",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,getHostnameVerifier,org.apache.hadoop.security.ssl.SSLFactory:getHostnameVerifier(org.apache.hadoop.conf.Configuration),206,210,"/**
 * Retrieves the hostname verifier based on configuration.
 * @param conf Configuration object; determines the verifier.
 * @throws GeneralSecurityException, IOException on failure.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getAuthenticationMethod,org.apache.hadoop.security.SecurityUtil:getAuthenticationMethod(org.apache.hadoop.conf.Configuration),730,739,"/**
 * Gets the AuthenticationMethod from the configuration.
 * @param conf Configuration object; retrieves authentication method.
 * @return AuthenticationMethod enum value.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoCodec.java,getCodecClasses,"org.apache.hadoop.crypto.CryptoCodec:getCodecClasses(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)",105,140,"/**
 * Retrieves a list of CryptoCodec classes based on the cipher suite.
 * @param conf Configuration object.
 * @param cipherSuite Cipher suite to use.
 * @return List of CryptoCodec classes or null if not configured.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/JceCtrCryptoCodec.java,setConf,org.apache.hadoop.crypto.JceCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration),83,102,"/**
* Sets the configuration and initializes secure random number generator.
* @param conf Hadoop configuration object.
*/
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/random/OsSecureRandom.java,setConf,org.apache.hadoop.crypto.random.OsSecureRandom:setConf(org.apache.hadoop.conf.Configuration),83,90,"/**
 * Sets the configuration and reinitializes with it.
 * @param conf The new Configuration object.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,<init>,org.apache.hadoop.crypto.key.KeyProvider:<init>(org.apache.hadoop.conf.Configuration),403,417,"/**
 * Initializes the KeyProvider with a Configuration. Sets JCEKS filter & adds BC provider if needed.
 */","* Constructor.
   * 
   * @param conf configuration for the provider",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallerContext.java,<init>,"org.apache.hadoop.ipc.CallerContext$Builder:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",143,146,"/**
 * Constructs a Builder with a context and separator.
 * @param context The context string.
 * @param conf Configuration object.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getZKAcls,org.apache.hadoop.util.curator.ZKCuratorManager:getZKAcls(org.apache.hadoop.conf.Configuration),97,109,"/**
 * Retrieves ZooKeeper ACLs from configuration.
 * @param conf Hadoop configuration object.
 * @throws IOException if ACLs cannot be read.
 */
","* Utility method to fetch the ZK ACLs from the configuration.
   *
   * @param conf configuration.
   * @throws java.io.IOException if the Zookeeper ACLs configuration file
   * cannot be read
   * @return acl list.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/Hash.java,getHashType,org.apache.hadoop.util.hash.Hash:getHashType(org.apache.hadoop.conf.Configuration),64,68,"/**
 * Gets the hash type from the configuration.
 * @param conf Configuration object; retrieves hash type key.
 * @return Integer representing the hash type.
 */
","* This utility method converts the name of the configured
   * hash type to a symbolic constant.
   * @param conf configuration
   * @return one of the predefined constants",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getEnumSet,"org.apache.hadoop.conf.Configuration:getEnumSet(java.lang.String,java.lang.Class,boolean)",1803,1809,"/**
 * Parses an EnumSet from a configuration string.
 * @param key config key, enumClass, ignoreUnknown
 * @return EnumSet of the specified enum type.
 */
","* Build an enumset from a comma separated list of values.
   * Case independent.
   * Special handling of ""*"" meaning: all values.
   * @param key key to look for
   * @param enumClass class of enum
   * @param ignoreUnknown should unknown values raise an exception?
   * @return a mutable set of the identified enum values declared in the configuration
   * @param <E> enumeration type
   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,
   *           or there are two entries in the enum which differ only by case.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getRange,"org.apache.hadoop.conf.Configuration:getRange(java.lang.String,java.lang.String)",2296,2298,"/**
 * Creates an IntegerRanges object from the given name and default.
 * @param name key for retrieving the range
 * @param defaultValue default value if not found
 * @return IntegerRanges object
 */
","* Parse the given attribute as a set of integer ranges.
   * @param name the attribute name
   * @param defaultValue the default value if it is not set
   * @return a new set of ranges from the configured value",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigRedactor.java,<init>,org.apache.hadoop.conf.ConfigRedactor:<init>(org.apache.hadoop.conf.Configuration),44,55,"/**
 * Initializes the ConfigRedactor with a configuration.
 * @param conf Configuration object containing sensitive keys.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,get,"org.apache.hadoop.conf.ConfigurationWithLogging:get(java.lang.String,java.lang.String)",56,62,"/**
 * Retrieves a value by name, using a default if not found.
 * Logs the retrieved (redacted) value and default.
 */
","* See {@link Configuration#get(String, String)}.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,getParentZnode,org.apache.hadoop.ha.ZKFailoverController:getParentZnode(),384,391,"/**
 * Retrieves the parent ZNode path.
 * @return Parent ZNode path with scope appended.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,boolean)",182,185,"/**
 * Overloads opt to accept a boolean value.
 * @param key The key to search for.
 * @param value The boolean value to convert to a string.
 * @return The result of opt(key, String.valueOf(value)).
 */
","* Set optional boolean parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,optLong,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:optLong(java.lang.String,long)",202,205,"/**
 * Sets a long value for the given key.
 * @param key key to set
 * @param value long value to set
 * @return this builder
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,optDouble,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:optDouble(java.lang.String,double)",232,235,"/**
 * Sets a double value for the given key.
 * @param key key to set value for
 * @param value double value to set
 * @return This builder instance
 */
","* Set optional double parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,boolean)",268,271,"/**
* Calls must with key and value converted to String.
* @param key The key to set.
* @param value The boolean value to convert to String.
* @return This object, for chaining.
*/
","* Set mandatory boolean option.
   *
   * @see #must(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,mustLong,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:mustLong(java.lang.String,long)",273,276,"/**
 * Sets the value of the key to the string representation of the long value.
 * @param key The key to set.
 * @param value The long value to set.
 * @return This builder.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,mustDouble,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:mustDouble(java.lang.String,double)",283,286,"/**
 * Doubles the value associated with a key.
 * @param key The key to update.
 * @param value The double value to double.
 * @return The updated object.
 */
","* Set optional double parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,setDefaultUri,"org.apache.hadoop.fs.FileSystem:setDefaultUri(org.apache.hadoop.conf.Configuration,java.lang.String)",319,321,"/**
* Sets the default URI for the given configuration.
* @param conf Configuration object to update.
* @param uri The URI string to set as default.
*/
","Set the default FileSystem URI in a configuration.
   * @param conf the configuration to alter
   * @param uri the new default filesystem uri",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkNfly,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String,java.net.URI[])",167,175,"/**
 * Adds a link to a table, using default settings if none provided.
 * @param conf Configuration object
 * @param mountTableName Table name to link to
 * @param src Source string
 * @param settings Link settings (optional)
 * @param targets Target URIs
 */
","* Add nfly link to configuration for the given mount table.
   *
   * @param conf configuration.
   * @param mountTableName mount table.
   * @param src src.
   * @param settings settings.
   * @param targets targets.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,addMappingProvider,"org.apache.hadoop.security.CompositeGroupsMapping:addMappingProvider(java.lang.String,java.lang.Class)",162,168,"/**
 * Adds a mapping provider to the list.
 * @param providerName Provider name.
 * @param providerClass Class of the mapping provider.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,setBlockSize,"org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setBlockSize(org.apache.hadoop.conf.Configuration,int)",126,128,"/**
 * Sets the Bzip2 compression block size in the configuration.
 * @param conf Configuration object to update.
 * @param blockSize The desired block size.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,setWorkFactor,"org.apache.hadoop.io.compress.bzip2.Bzip2Factory:setWorkFactor(org.apache.hadoop.conf.Configuration,int)",135,137,"/**
 * Sets the Bzip2 compression work factor in the configuration.
 * @param conf Configuration object.
 * @param workFactor The work factor value to set.
 */
",,,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,setIndexInterval,"org.apache.hadoop.io.MapFile$Writer:setIndexInterval(org.apache.hadoop.conf.Configuration,int)",380,382,"/**
 * Sets the index interval in the configuration.
 * @param conf Configuration object to update.
 * @param interval The interval value to set.
 */
","* Sets the index interval and stores it in conf.
     * @see #getIndexInterval()
     *
     * @param conf configuration.
     * @param interval interval.",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setPingInterval,"org.apache.hadoop.ipc.Client:setPingInterval(org.apache.hadoop.conf.Configuration,int)",175,178,"/**
 * Sets the IPC ping interval in the configuration.
 * @param conf Configuration object to update.
 * @param pingInterval Ping interval in milliseconds.
 */
","* set the ping interval value in configuration
   * 
   * @param conf Configuration
   * @param pingInterval the ping interval",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setConnectTimeout,"org.apache.hadoop.ipc.Client:setConnectTimeout(org.apache.hadoop.conf.Configuration,int)",233,235,"/**
 * Sets the client connect timeout for the configuration.
 * @param conf Configuration object to update.
 * @param timeout Timeout value in milliseconds.
 */
","* set the connection timeout value in configuration
   * 
   * @param conf Configuration
   * @param timeout the socket connect timeout value",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,setIsNestedMountPointSupported,"org.apache.hadoop.fs.viewfs.ConfigUtil:setIsNestedMountPointSupported(org.apache.hadoop.conf.Configuration,boolean)",279,281,"/**
 * Sets whether nested mount points are supported in the config.
 * @param conf Configuration object to update.
 * @param isNestedMountPointSupported boolean value.
 */
","* Set the bool value isNestedMountPointSupported in config.
   * @param conf - from this conf
   * @param isNestedMountPointSupported - whether nested mount point is supported",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,java.lang.String[])",242,248,"/**
 * Adds a key with optional values.
 * @param key Key to add.
 * @param values Optional values associated with the key.
 * @return This builder instance.
 */
","* Set an array of string values as optional parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,java.lang.String[])",318,324,"/**
 * Marks a key as mandatory and sets associated values.
 * @param key Key to mark as mandatory.
 * @param values Values associated with the key.
 * @return This builder instance.
 */
","* Set a string array as mandatory option.
   *
   * @see #must(String, String)",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,updateConnectAddr,"org.apache.hadoop.conf.Configuration:updateConnectAddr(java.lang.String,java.net.InetSocketAddress)",2624,2629,"/**
 * Updates the connect address for a given name.
 * @param name identifier for the address
 * @param addr address to update to
 * @return Updated InetSocketAddress
 */
","* Set the socket address a client can use to connect for the
   * <code>name</code> property as a <code>host:port</code>.  The wildcard
   * address is replaced with the local host's address.
   * @param name property name.
   * @param addr InetSocketAddress of a listener to store in the given property
   * @return InetSocketAddress for clients to connect",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,setProtocolEngine,"org.apache.hadoop.ipc.RPC:setProtocolEngine(org.apache.hadoop.conf.Configuration,java.lang.Class,java.lang.Class)",211,217,"/**
 * Sets the RPC engine class for a protocol in the configuration.
 * @param conf Configuration object. protocol, engine class types.
 */
","* Set a protocol to use a non-default RpcEngine if one
   * is not specified in the configuration.
   * @param conf configuration to use
   * @param protocol the protocol interface
   * @param engine the RpcEngine impl",,,True,13
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,delete,"org.apache.hadoop.fs.RawLocalFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",707,721,"/**
 * Deletes a file or directory.
 * @param p Path to delete.
 * @param recursive If true, deletes recursively.
 * @throws IOException if directory is not empty and not recursive
 */
","* Delete the given path to a file or directory.
   * @param p the path to delete
   * @param recursive to delete sub-directories
   * @return true if the file or directory and all its contents were deleted
   * @throws IOException if p is non-empty and recursive is false",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,fullyDeleteContents,org.apache.hadoop.fs.FileUtil:fullyDeleteContents(java.io.File),267,269,"/**
 * Deletes all files and subdirectories within a directory.
 * @param dir The directory to be fully deleted.
 */
","* Delete the contents of a directory, not the directory itself.  If
   * we return false, the directory may be partially-deleted.
   * If dir is a symlink to a directory, all the contents of the actual
   * directory pointed to by dir will be deleted.
   *
   * @param dir dir.
   * @return fullyDeleteContents Status.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:getFileStatus(org.apache.hadoop.fs.Path),567,574,"/**
 * Gets the file status for the given path.
 * @param f Path to the file.
 * @return FileStatus object.
 */
","* {@inheritDoc}
   *
   * If the given path is a symlink(mount link), the path will be resolved to a
   * target path and it will get the resolved path's FileStatus object. It will
   * not be represented as a symlink and isDirectory API returns true if the
   * resolved path is a directory, false otherwise.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem:listStatus(org.apache.hadoop.fs.Path),611,628,"/**
 * Lists status of files/directories under the given path.
 * @param f the path to list
 * @return FileStatus array or empty array if no status found
 */","* {@inheritDoc}
   *
   * Note: listStatus considers listing from fallbackLink if available. If the
   * same directory path is present in configured mount path as well as in
   * fallback fs, then only the fallback path will be listed in the returned
   * result except for link.
   *
   * If any of the the immediate children of the given path f is a symlink(mount
   * link), the returned FileStatus object of that children would be represented
   * as a symlink. It will not be resolved to the target path and will not get
   * the target path FileStatus object. The target path will be available via
   * getSymlink on that children's FileStatus object. Since it represents as
   * symlink, isDirectory on that children's FileStatus will return false.
   * This behavior can be changed by setting an advanced configuration
   * fs.viewfs.mount.links.as.symlinks to false. In this case, mount points will
   * be represented as non-symlinks and all the file/directory attributes like
   * permissions, isDirectory etc will be assigned from it's resolved target
   * directory/file.
   *
   * If you want to get the FileStatus of target path for that children, you may
   * want to use GetFileStatus API with that children's symlink path. Please see
   * {@link ViewFileSystem#getFileStatus(Path f)}
   *
   * Note: In ViewFileSystem, by default the mount links are represented as
   * symlinks.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,read,org.apache.hadoop.io.compress.BZip2Codec$BZip2CompressionInputStream:read(),524,528,"/**
 * Reads a single byte from the input stream.
 * Returns the byte value or -1 if EOF.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsSourceBuilder.java,<init>,"org.apache.hadoop.metrics2.lib.MetricsSourceBuilder:<init>(java.lang.Object,org.apache.hadoop.metrics2.lib.MutableMetricsFactory)",62,74,"/**
 * Creates a MetricsSourceBuilder for the given source object.
 * @param source The object to be instrumented.
 * @param factory Factory for creating mutable metrics.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkOneDirWithMode,"org.apache.hadoop.fs.RawLocalFileSystem:mkOneDirWithMode(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.permission.FsPermission)",777,802,"/**
 * Creates a single directory with specified permissions.
 * @param p Path to create directory.
 * @param p2f File object representing the directory.
 * @param permission File permissions to apply.
 * @return True if directory creation was successful.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",1226,1236,"/**
 * Creates a data output stream for writing to a file.
 * @param f path to the file, overwrite, buffer size, replication, block size, progress.
 * @throws IOException if an I/O error occurs
 */
","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * @param f the file name to open
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize the size of the buffer to be used.
   * @param progress to report progress.
   * @throws IOException IO failure
   * @return output stream.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getUMask,org.apache.hadoop.fs.FileContext:getUMask(),585,587,"/**
 * Returns the Umask. Uses cached value or gets from configuration.
 */","* 
   * @return the umask of this FileContext",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,createFactory,"org.apache.hadoop.fs.store.DataBlocks:createFactory(java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String)",129,144,"/**
 * Creates a BlockFactory based on the provided name.
 * @param keyToBufferDir Buffer directory key.
 * @param configuration Configuration object.
 * @param name Factory type name.
 * @return BlockFactory instance.
 */
","* Create a factory.
   *
   * @param keyToBufferDir Key to buffer directory config for a FS.
   * @param configuration  factory configurations.
   * @param name           factory name -the option from {@link CommonConfigurationKeys}.
   * @return the factory, ready to be initialized.
   * @throws IllegalArgumentException if the name is unknown.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,<init>,org.apache.hadoop.io.compress.CompressionCodecFactory:<init>(org.apache.hadoop.conf.Configuration),177,191,"/**
 * Initializes the CompressionCodecFactory with codecs from configuration.
 * @param conf Hadoop configuration object
 */
","* Find the codecs specified in the config value io.compression.codecs 
   * and register them. Defaults to gzip and deflate.
   *
   * @param conf configuration.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/AbstractJavaKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.AbstractJavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",81,90,"/**
 * Initializes the keystore provider with URI and configuration.
 * @param uri keystore URI
 * @param conf configuration object
 * @throws IOException if an I/O error occurs during initialization
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,replacePattern,"org.apache.hadoop.security.SecurityUtil:replacePattern(java.lang.String[],java.lang.String)",235,243,"/**
 * Constructs a string by combining components and hostname.
 * @param components String array of components
 * @param hostname Hostname to include in the string
 * @return Combined string
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,registerProtocolAndImpl,"org.apache.hadoop.ipc.RPC$Server:registerProtocolAndImpl(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)",1101,1135,"/**
 * Registers an RPC protocol and implementation with the system.
 * @param rpcKind RPC kind, protocol class, and implementation.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,getKeyProviderUri,"org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration,java.lang.String)",71,79,"/**
 * Retrieves the provider URI from the configuration.
 * @param conf Configuration object.
 * @param configKeyName Key to retrieve the URI from.
 * @return URI object or null if not found.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTrimmed,"org.apache.hadoop.conf.Configuration:getTrimmed(java.lang.String,java.lang.String)",1340,1343,"/**
 * Returns trimmed name or defaultValue if name is null/empty.
 */
","* Get the value of the <code>name</code> property as a trimmed <code>String</code>, 
   * <code>defaultValue</code> if no such property exists. 
   * See @{Configuration#getTrimmed} for more details.
   * 
   * @param name          the property name.
   * @param defaultValue  the property default value.
   * @return              the value of the <code>name</code> or defaultValue
   *                      if it is not set.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getInt,"org.apache.hadoop.conf.Configuration:getInt(java.lang.String,int)",1546,1555,"/**
 * Parses an integer from a string, using default if parsing fails.
 * @param name String to parse, @param defaultValue int default value
 */
","* Get the value of the <code>name</code> property as an <code>int</code>.
   *   
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>int</code>,
   * then an error is thrown.
   * 
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as an <code>int</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getLong,"org.apache.hadoop.conf.Configuration:getLong(java.lang.String,long)",1599,1608,"/**
 * Parses a long value from a string, or returns defaultValue.
 * @param name String to parse.
 * @param defaultValue Default value if parsing fails.
 * @return Parsed long value.
 */
","* Get the value of the <code>name</code> property as a <code>long</code>.  
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>long</code>,
   * then an error is thrown.
   * 
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>long</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getLongBytes,"org.apache.hadoop.conf.Configuration:getLongBytes(java.lang.String,long)",1624,1629,"/**
 * Converts a string to a long, using default if null/invalid.
 * @param name String to convert; defaults to defaultValue if null.
 * @return Long value of the string, or defaultValue.
 */
","* Get the value of the <code>name</code> property as a <code>long</code> or
   * human readable format. If no such property exists, the provided default
   * value is returned, or if the specified value is not a valid
   * <code>long</code> or human readable format, then an error is thrown. You
   * can use the following suffix (case insensitive): k(kilo), m(mega), g(giga),
   * t(tera), p(peta), e(exa)
   *
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>long</code>,
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getFloat,"org.apache.hadoop.conf.Configuration:getFloat(java.lang.String,float)",1671,1676,"/**
 * Gets a float value by name, returns defaultValue if not found.
 */","* Get the value of the <code>name</code> property as a <code>float</code>.  
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>float</code>,
   * then an error is thrown.
   *
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>float</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getDouble,"org.apache.hadoop.conf.Configuration:getDouble(java.lang.String,double)",1700,1705,"/**
 * Parses a double value from a string, or returns defaultValue.
 * @param name String to parse.
 * @param defaultValue Default value if parsing fails.
 */
","* Get the value of the <code>name</code> property as a <code>double</code>.  
   * If no such property exists, the provided default value is returned,
   * or if the specified value is not a valid <code>double</code>,
   * then an error is thrown.
   *
   * @param name property name.
   * @param defaultValue default value.
   * @throws NumberFormatException when the value is invalid
   * @return property value as a <code>double</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getBoolean,"org.apache.hadoop.conf.Configuration:getBoolean(java.lang.String,boolean)",1727,1742,"/**
 * Gets a boolean value by name, returns defaultValue if invalid.
 * @param name property name
 * @param defaultValue default boolean value
 * @return boolean value or defaultValue if invalid
 */
","* Get the value of the <code>name</code> property as a <code>boolean</code>.  
   * If no such property is specified, or if the specified value is not a valid
   * <code>boolean</code>, then <code>defaultValue</code> is returned.
   * 
   * @param name property name.
   * @param defaultValue default value.
   * @return property value as a <code>boolean</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClass,"org.apache.hadoop.conf.Configuration:getClass(java.lang.String,java.lang.Class)",2730,2739,"/**
 * Retrieves a Class object by name, returning defaultValue if not found.
 * @param name Class name to lookup.
 * @param defaultValue Default Class if not found.
 */
","* Get the value of the <code>name</code> property as a <code>Class</code>.  
   * If no such property is specified, then <code>defaultValue</code> is 
   * returned.
   * 
   * @param name the conf key name.
   * @param defaultValue default value.
   * @return property value as a <code>Class</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,setBooleanIfUnset,"org.apache.hadoop.conf.Configuration:setBooleanIfUnset(java.lang.String,boolean)",1759,1761,"/**
 * Sets the value to boolean string if the property is not set.
 */","* Set the given property, if it is currently unset.
   * @param name property name
   * @param value new value",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,long,java.util.concurrent.TimeUnit)",1884,1886,"/**
 * Gets time duration by name, using default value and unit.
 */","* Return time duration in the given time unit. Valid units are encoded in
   * properties as suffixes: nanoseconds (ns), microseconds (us), milliseconds
   * (ms), seconds (s), minutes (m), hours (h), and days (d).
   *
   * @param name Property name
   * @param defaultValue Value returned if no mapping exists.
   * @param unit Unit to convert the stored property, if it exists.
   * @throws NumberFormatException If the property stripped of its unit is not
   *         a number
   * @return time duration in given time unit",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDuration,"org.apache.hadoop.conf.Configuration:getTimeDuration(java.lang.String,java.lang.String,java.util.concurrent.TimeUnit)",1888,1890,"/**
 * Gets time duration, defaulting to the provided unit.
 * @param name duration name, @param defaultValue default unit
 * @param unit time unit
 * @return Time duration in the specified unit.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialProviderFactory.java,getProviders,org.apache.hadoop.security.alias.CredentialProviderFactory:getProviders(org.apache.hadoop.conf.Configuration),73,112,"/**
 * Retrieves a list of CredentialProvider instances from configured paths.
 * @param conf Configuration object containing provider paths.
 * @return List of CredentialProvider objects.
 * @throws IOException if providers cannot be loaded or paths are invalid.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderFactory.java,getProviders,org.apache.hadoop.crypto.key.KeyProviderFactory:getProviders(org.apache.hadoop.conf.Configuration),62,81,"/**
 * Retrieves a list of KeyProvider instances from the configuration.
 * @param conf Configuration object containing provider paths.
 * @return List of KeyProvider objects.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseServiceUserNames,"org.apache.hadoop.ipc.DecayRpcScheduler:parseServiceUserNames(java.lang.String,org.apache.hadoop.conf.Configuration)",409,413,"/**
 * Parses service user names from configuration.
 * @param ns namespace key, @param conf configuration object
 * @return Set of service user names.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java,getPackages,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:getPackages(),65,73,"/**
 * Retrieves Avro reflection packages from configuration.
 * Populates the 'packages' set with trimmed package names.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,getRawCoderNames,"org.apache.hadoop.io.erasurecode.CodecUtil:getRawCoderNames(org.apache.hadoop.conf.Configuration,java.lang.String)",168,174,"/**
 * Gets raw coder names from configuration.
 * @param conf Configuration object.
 * @param codecName Codec name.
 * @return Array of raw coder names.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getSaslProperties,"org.apache.hadoop.security.SaslPropertiesResolver:getSaslProperties(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.SaslRpcServer$QualityOfProtection)",136,150,"/**
 * Creates a map of SASL properties based on configuration.
 * @param conf Configuration object.
 * @param configKey Key for QOP values.
 * @param defaultQOP Default Quality of Protection.
 * @return Map of SASL properties.
 */
","* A util function to retrieve specific additional sasl property from config.
   * Used by subclasses to read sasl properties used by themselves.
   * @param conf the configuration
   * @param configKey the config key to look for
   * @param defaultQOP the default QOP if the key is missing
   * @return sasl property associated with the given key",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,getKeyFiles,org.apache.hadoop.ha.SshFenceByTcpPort:getKeyFiles(),220,222,"/**
 * Retrieves a collection of key files from the configuration.
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyServers.java,refresh,org.apache.hadoop.security.authorize.ProxyServers:refresh(org.apache.hadoop.conf.Configuration),35,45,"/**
 * Refreshes the list of proxy servers from the configuration.
 * @param conf Hadoop configuration object.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getInts,org.apache.hadoop.conf.Configuration:getInts(java.lang.String),1567,1574,"/**
 * Converts string array to integer array.
 * @param name string array name
 * @return integer array representation of the strings
 */
","* Get the value of the <code>name</code> property as a set of comma-delimited
   * <code>int</code> values.
   * 
   * If no such property exists, an empty array is returned.
   * 
   * @param name property name
   * @return property value interpreted as an array of comma-delimited
   *         <code>int</code> values",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getTimeDurations,"org.apache.hadoop.conf.Configuration:getTimeDurations(java.lang.String,java.util.concurrent.TimeUnit)",1971,1978,"/**
 * Gets time durations for given name and time unit.
 * @param name Name to get durations for.
 * @param unit Time unit for durations.
 * @return Array of time durations in specified unit.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClasses,"org.apache.hadoop.conf.Configuration:getClasses(java.lang.String,java.lang.Class[])",2703,2718,"/**
 * Resolves classes by name. Returns default if not found.
 * @param name property name
 * @param defaultValue default class array
 * @return Class<?> array
 */
","* Get the value of the <code>name</code> property
   * as an array of <code>Class</code>.
   * The value of the property specifies a list of comma separated class names.  
   * If no such property is specified, then <code>defaultValue</code> is 
   * returned.
   * 
   * @param name the property name.
   * @param defaultValue default value.
   * @return property value as a <code>Class[]</code>, 
   *         or <code>defaultValue</code>.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getFile,"org.apache.hadoop.conf.Configuration:getFile(java.lang.String,java.lang.String)",2861,2874,"/**
 * Retrieves a file from a set of local directories.
 * @param dirsProp Comma-separated string of directory paths.
 * @param path File path to retrieve.
 * @return File object or throws IOException if not found.
 */
","* Get a local file name under a directory named in <i>dirsProp</i> with
   * the given <i>path</i>.  If <i>dirsProp</i> contains multiple directories,
   * then one is chosen based on <i>path</i>'s hash code.  If the selected
   * directory does not exist, an attempt is made to create it.
   *
   * @param dirsProp directory in which to locate the file.
   * @param path file-path.
   * @return local file under the directory with the given path.
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/SerializationFactory.java,<init>,org.apache.hadoop.io.serializer.SerializationFactory:<init>(org.apache.hadoop.conf.Configuration),58,67,"/**
 * Creates a SerializationFactory with configurations and serializers.
 * @param conf Hadoop configuration object.
 */
","* <p>
   * Serializations are found by reading the <code>io.serializations</code>
   * property from <code>conf</code>, which is a comma-delimited list of
   * classnames.
   * </p>
   *
   * @param conf configuration.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,setConf,org.apache.hadoop.security.SaslPropertiesResolver:setConf(org.apache.hadoop.conf.Configuration),60,73,"/**
 * Sets the configuration and initializes SASL properties.
 * @param conf Hadoop configuration object.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java,getFilterParams,"org.apache.hadoop.security.http.RestCsrfPreventionFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)",229,232,"/**
 * Retrieves configuration parameters with a specified prefix.
 * @param conf Configuration object.
 * @param confPrefix Prefix for the configuration parameters.
 * @return Map of configuration parameters.
 */
","* Constructs a mapping of configuration properties to be used for filter
   * initialization.  The mapping includes all properties that start with the
   * specified configuration prefix.  Property names in the mapping are trimmed
   * to remove the configuration prefix.
   *
   * @param conf configuration to read
   * @param confPrefix configuration prefix
   * @return mapping of configuration properties to be used for filter
   *     initialization",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/http/XFrameOptionsFilter.java,getFilterParams,"org.apache.hadoop.security.http.XFrameOptionsFilter:getFilterParams(org.apache.hadoop.conf.Configuration,java.lang.String)",80,83,"/**
 * Retrieves configuration parameters with a given prefix.
 * @param conf Configuration object
 * @param confPrefix Prefix to filter configuration parameters
 * @return Map of configuration parameters with the prefix
 */
","* Constructs a mapping of configuration properties to be used for filter
   * initialization.  The mapping includes all properties that start with the
   * specified configuration prefix.  Property names in the mapping are trimmed
   * to remove the configuration prefix.
   *
   * @param conf configuration to read
   * @param confPrefix configuration prefix
   * @return mapping of configuration properties to be used for filter
   *     initialization",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,propagateOptions,"org.apache.hadoop.util.functional.FutureIO:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",356,374,"/**
 * Propagates configuration options to the schema builder.
 * @param builder Schema builder to add options to.
 * @param conf Configuration object.
 * @param prefix Prefix for option keys.
 * @param mandatory Whether options are mandatory.
 */
","* Propagate options to any builder, converting everything with the
   * prefix to an option where, if there were 2+ dot-separated elements,
   * it is converted to a schema.
   * <pre>
   *   fs.example.s3a.option becomes ""s3a.option""
   *   fs.example.fs.io.policy becomes ""fs.io.policy""
   *   fs.example.something becomes ""something""
   * </pre>
   * @param builder builder to modify
   * @param conf configuration to read
   * @param prefix prefix to scan/strip
   * @param mandatory are the options to be mandatory or optional?",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationUtil.java,parseChangedProperties,"org.apache.hadoop.conf.ReconfigurationUtil:parseChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",67,70,"/**
 * Parses and returns PropertyChange objects for config changes.
 * @param newConf The new configuration.
 * @param oldConf The old configuration.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,printConf,"org.apache.hadoop.conf.ReconfigurationServlet:printConf(java.io.PrintWriter,org.apache.hadoop.conf.Reconfigurable)",88,130,"/**
 * Prints a form displaying configuration changes to the output stream.
 * @param out PrintWriter to write the form to.
 * @param reconf Reconfigurable object containing configuration.
 */
",* Print configuration options that can be changed.,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLink,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLink(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI)",67,70,"/**
 * Adds a link to the default mount table of the configuration.
 * @param conf Configuration object.
 * @param src Source string for the link.
 * @param target URI representing the link target.
 */
","* Add a link to the config for the default mount table
   * @param conf - add the link to this conf
   * @param src - the src path name
   * @param target - the target URI link",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMergeSlash,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMergeSlash(org.apache.hadoop.conf.Configuration,java.net.URI)",91,93,"/**
 * Adds a link merge slash entry for the given URI.
 * @param conf Hadoop configuration object.
 * @param target The URI to add.
 */
","* Add a LinkMergeSlash to the config for the default mount table.
   *
   * @param conf configuration.
   * @param target targets.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkFallback,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkFallback(org.apache.hadoop.conf.Configuration,java.net.URI)",114,116,"/**
 * Adds a link fallback for a target URI using default table name.
 * @param conf Hadoop configuration object
 * @param target The URI to add as a fallback link.
 */
","* Add a LinkFallback to the config for the default mount table.
   *
   * @param conf configuration.
   * @param target targets.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkMerge,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkMerge(org.apache.hadoop.conf.Configuration,java.net.URI[])",137,139,"/**
 * Adds link merge targets to the configuration's default table.
 * @param conf Hadoop configuration object
 * @param targets URIs representing the targets to merge
 */
","* Add a LinkMerge to the config for the default mount table.
   *
   * @param conf configuration.
   * @param targets targets array.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,setHomeDirConf,"org.apache.hadoop.fs.viewfs.ConfigUtil:setHomeDirConf(org.apache.hadoop.conf.Configuration,java.lang.String)",209,212,"/**
 * Sets the home directory configuration for a given Hadoop configuration.
 * @param conf Hadoop configuration object
 * @param homedir Home directory path
 */
","* Add config variable for homedir for default mount table
   * @param conf - add to this conf
   * @param homedir - the home dir path starting with slash",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,getHomeDirValue,org.apache.hadoop.fs.viewfs.ConfigUtil:getHomeDirValue(org.apache.hadoop.conf.Configuration),235,237,"/**
 * Gets the home directory value from the configuration.
 * @param conf Configuration object to retrieve the value from.
 */
","* Get the value of the home dir conf value for default mount table
   * @param conf - from this conf
   * @return home dir value, null if variable is not in conf",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createEncoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createEncoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",94,104,"/**
 * Creates an ErasureEncoder using the given configuration and options.
 */","* Create encoder corresponding to given codec.
   * @param options Erasure codec options
   * @param conf configuration.
   * @return erasure encoder",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createDecoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createDecoder(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.erasurecode.ErasureCodecOptions)",112,122,"/**
 * Creates an ErasureDecoder using the given configuration and options.
 * @param conf Hadoop configuration
 * @param options ErasureCodecOptions object
 * @return ErasureDecoder instance
 */
","* Create decoder corresponding to given codec.
   * @param options Erasure codec options
   * @param conf configuration.
   * @return erasure decoder",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getLibraryName,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getLibraryName(org.apache.hadoop.conf.Configuration),72,78,"/**
 * Returns the Bzip2 library name based on native library loading.
 * @param conf Configuration object.
 * @return Library name string.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2CompressorType,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2CompressorType(org.apache.hadoop.conf.Configuration),86,90,"/**
 * Returns the Bzip2 compressor class based on native library loading.
 * @param conf Hadoop configuration object.
 * @return Class extending Compressor.
 */
","* Return the appropriate type of the bzip2 compressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the bzip2 compressor.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2DecompressorType,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2DecompressorType(org.apache.hadoop.conf.Configuration),109,113,"/**
 * Returns the Bzip2Decompressor class based on native library loading.
 */
","* Return the appropriate type of the bzip2 decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate type of the bzip2 decompressor.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2Decompressor,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Decompressor(org.apache.hadoop.conf.Configuration),121,124,"/**
 * Returns a Bzip2Decompressor, native or dummy, based on config.
 */
","* Return the appropriate implementation of the bzip2 decompressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the bzip2 decompressor.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetUtils.java,getSocketFactory,"org.apache.hadoop.net.NetUtils:getSocketFactory(org.apache.hadoop.conf.Configuration,java.lang.Class)",96,110,"/**
 * Gets a SocketFactory based on configuration or defaults to a standard one.
 * @param conf Hadoop configuration.
 * @param clazz Class used for property key.
 * @return SocketFactory instance.
 */
","* Get the socket factory for the given class according to its
   * configuration parameter
   * <tt>hadoop.rpc.socket.factory.class.&lt;ClassName&gt;</tt>. When no
   * such parameter exists then fall back on the default socket factory as
   * configured by <tt>hadoop.rpc.socket.factory.class.default</tt>. If
   * this default socket factory is not configured, then fall back on the JVM
   * default socket factory.
   * 
   * @param conf the configuration
   * @param clazz the class (usually a {@link VersionedProtocol})
   * @return a socket factory",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,resolve,org.apache.hadoop.net.TableMapping$RawTableMapping:resolve(java.util.List),127,147,"/**
 * Resolves rack names from a list, using a map or default rack.
 * @param names List of names to resolve.
 * @return List of resolved rack names.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,reloadCachedMappings,org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings(),149,160,"/**
 * Reloads topology mappings from source, updates cache.
 * Logs error if reload fails; cache not cleared in that case.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/lib/StaticUserWebFilter.java,initFilter,"org.apache.hadoop.http.lib.StaticUserWebFilter:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",122,128,"/**
 * Adds a static user filter to the filter container using config.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,<init>,"org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String,java.lang.String,java.lang.String)",520,524,"/**
 * Constructs a HadoopZookeeperFactory with default truststore.
 * @param zkPrincipal Zookeeper principal.
 * @param kerberosPrincipal Kerberos principal.
 * @param kerberosKeytab Kerberos keytab file.
 */
","* Constructor for the helper class to configure the ZooKeeper client connection.
     * @param zkPrincipal Optional.
     * @param kerberosPrincipal Optional. Use along with kerberosKeytab.
     * @param kerberosKeytab Optional. Use along with kerberosPrincipal.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,goUpGroupHierarchy,"org.apache.hadoop.security.LdapGroupsMapping:goUpGroupHierarchy(java.util.Set,int,java.util.Set)",592,618,"/**
 * Recursively searches LDAP for parent groups.
 * @param groupDNs initial group DNs to search from
 * @param goUpHierarchy how many levels up to search
 * @param groups Accumulates found group DNs.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,init,org.apache.hadoop.security.ssl.SSLFactory:init(),194,204,"/**
 * Initializes SSL context with keystores, managers, and protocols.
 */","* Initializes the factory.
   *
   * @throws  GeneralSecurityException thrown if an SSL initialization error
   * happened.
   * @throws IOException thrown if an IO error happened while reading the SSL
   * configuration.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,isSimpleAuthentication,org.apache.hadoop.security.KDiag:isSimpleAuthentication(org.apache.hadoop.conf.Configuration),427,430,"/**
 * Checks if authentication method is SIMPLE.
 * @param conf Configuration object containing authentication method.
 * @return True if SIMPLE authentication, false otherwise.
 */
","* Is the authentication method of this configuration ""simple""?
   * @param conf configuration to check
   * @return true if auth is simple (i.e. not kerberos)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HadoopKerberosName.java,setConfiguration,org.apache.hadoop.security.HadoopKerberosName:setConfiguration(org.apache.hadoop.conf.Configuration),63,85,"/**
 * Sets the authentication rule based on the provided configuration.
 * @param conf Hadoop configuration object.
 */
","* Set the static configuration to get and evaluate the rules.
   * <p>
   * IMPORTANT: This method does a NOP if the rules have been set already.
   * If there is a need to reset the rules, the {@link KerberosName#setRules(String)}
   * method should be invoked directly.
   * 
   * @param conf the new configuration
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getAuthMethods,"org.apache.hadoop.ipc.Server:getAuthMethods(org.apache.hadoop.security.token.SecretManager,org.apache.hadoop.conf.Configuration)",3472,3491,"/**
 * Retrieves a list of authentication methods based on config & secret manager.
 */",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoCodec.java,getInstance,"org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.crypto.CipherSuite)",59,88,"/**
 * Retrieves a CryptoCodec instance for the given cipher suite.
 * @param conf Configuration object.
 * @param cipherSuite Cipher suite to match.
 * @return CryptoCodec instance or null if not found.
 */
","* Get crypto codec for specified algorithm/mode/padding.
   * 
   * @param conf
   *          the configuration
   * @param cipherSuite
   *          algorithm/mode/padding
   * @return CryptoCodec the codec object. Null value will be returned if no
   *         crypto codec classes with cipher suite configured.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,<init>,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(org.apache.hadoop.crypto.key.JavaKeyStoreProvider),115,127,"/**
 * Copies another JavaKeyStoreProvider, creating a new instance.
 * @param other The JavaKeyStoreProvider to copy.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/hash/Hash.java,getInstance,org.apache.hadoop.util.hash.Hash:getInstance(org.apache.hadoop.conf.Configuration),92,95,"/**
 * Gets a Hash instance based on the provided configuration.
 * @param conf Configuration object specifying the hash type.
 * @return A Hash instance.
 */
","* Get a singleton instance of hash function of a type
   * defined in the configuration.
   * @param conf current configuration
   * @return defined hash type, or null if type is invalid",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FlagSet.java,buildFlagSet,"org.apache.hadoop.fs.impl.FlagSet:buildFlagSet(java.lang.Class,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",318,325,"/**
 * Builds a FlagSet from an EnumSet retrieved from configuration.
 * @param enumClass Enum class for flags.
 * @param conf Configuration object.
 * @param key Key to fetch EnumSet from config.
 * @param ignoreUnknown Whether to ignore unknown enum values.
 * @return FlagSet containing the flags.
 */
","* Build a FlagSet from a comma separated list of values.
   * Case independent.
   * Special handling of ""*"" meaning: all values.
   * @param enumClass class of enum
   * @param conf configuration
   * @param key key to look for
   * @param ignoreUnknown should unknown values raise an exception?
   * @param <E> enumeration type
   * @return a mutable FlagSet
   * @throws IllegalArgumentException if one of the entries was unknown and ignoreUnknown is false,
   * or there are two entries in the enum which differ only by case.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,bind,"org.apache.hadoop.ipc.Server:bind(java.net.ServerSocket,java.net.InetSocketAddress,int,org.apache.hadoop.conf.Configuration,java.lang.String)",690,720,"/**
 * Binds a ServerSocket to an address, optionally trying multiple ports.
 * @param socket ServerSocket to bind
 * @param address Address to bind to
 * @param backlog Backlog size
 * @param conf Configuration object
 * @param rangeConf Range configuration string
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,"org.apache.hadoop.conf.Configuration:writeXml(java.lang.String,java.io.Writer,org.apache.hadoop.conf.Configuration)",3622,3640,"/**
 * Writes configuration as XML to a Writer.
 * @param propertyName Property name (optional).
 * @param out Writer to write XML to.
 * @param config Configuration object.
 * @throws IOException, IllegalArgumentException
 */
","* Write out the non-default properties in this configuration to the
   * given {@link Writer}.
   * <ul>
   * <li>
   * When property name is not empty and the property exists in the
   * configuration, this method writes the property and its attributes
   * to the {@link Writer}.
   * </li>
   *
   * <li>
   * When property name is null or empty, this method writes all the
   * configuration properties and their attributes to the {@link Writer}.
   * </li>
   *
   * <li>
   * When property name is not empty but the property doesn't exist in
   * the configuration, this method throws an {@link IllegalArgumentException}.
   * </li>
   * </ul>
   * @param propertyName xml property name.
   * @param out the writer to write to.
   * @param config configuration.
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,dumpConfiguration,"org.apache.hadoop.conf.Configuration:dumpConfiguration(org.apache.hadoop.conf.Configuration,java.io.Writer)",3832,3850,"/**
 * Dumps the configuration to a JSON writer.
 * @param config Configuration object to dump.
 * @param out Writer to write the JSON to.
 */
","*  Writes out all properties and their attributes (final and resource) to
   *  the given {@link Writer}, the format of the output would be,
   *
   *  <pre>
   *  { ""properties"" :
   *      [ { key : ""key1"",
   *          value : ""value1"",
   *          isFinal : ""key1.isFinal"",
   *          resource : ""key1.resource"" },
   *        { key : ""key2"",
   *          value : ""value2"",
   *          isFinal : ""ke2.isFinal"",
   *          resource : ""key2.resource"" }
   *       ]
   *   }
   *  </pre>
   *
   *  It does not output the properties of the configuration object which
   *  is loaded from an input stream.
   *  <p>
   *
   * @param config the configuration
   * @param out the Writer to write to
   * @throws IOException raised on errors performing I/O.",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,<init>,org.apache.hadoop.conf.ConfigurationWithLogging:<init>(org.apache.hadoop.conf.Configuration),37,41,"/**
 * Constructs a ConfigurationWithLogging object from a Configuration.
 * @param conf The base configuration object.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,confirmFormat,org.apache.hadoop.ha.ZKFailoverController:confirmFormat(),301,317,"/**
* Prompts user to confirm format operation on parent Znode.
* Returns true if confirmed, false otherwise, handles IO errors.
*/
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,int)",192,195,"/**
 * Returns the value associated with the key, or default value.
 * @param key Key to look up.
 * @param value Default value if key is not found.
 * @return Value associated with the key.
 */
","* Set optional int parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,long)",197,200,"/**
 * Sets the value associated with the given key to the given long value.
 * @param key key for the value
 * @param value long value to set
 * @return This object to allow chaining.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,float)",212,215,"/**
 * Returns the value associated with the key, cast to a long.
 * @param key key to search for
 * @param value float value to cast to long
 * @return This builder instance.
 */
","* Set optional float parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,opt,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:opt(java.lang.String,double)",222,225,"/**
 * Sets or gets the value associated with the key as a long.
 * @param key The key for the value.
 * @param value The long value to set.
 * @return This builder.
 */
","* Set optional double parameter for the Builder.
   *
   * @see #opt(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,int)",293,296,"/**
 * Sets a key-value pair, using a long value.
 * @param key The key to set.
 * @param value The long value to associate with the key.
 * @return The builder instance.
 */
","* Set mandatory int option.
   *
   * @see #must(String, String)",,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,long)",298,301,"/**
 * Adds a long value to the builder with the given key.
 * @param key The key to associate with the value.
 * @param value The long value to set.
 * @return This builder.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,float)",303,306,"/**
 * Sets a float value for the given key.
 * @param key the key to set
 * @param value the float value to set
 * @return The current builder object.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/AbstractFSBuilderImpl.java,must,"org.apache.hadoop.fs.impl.AbstractFSBuilderImpl:must(java.lang.String,double)",308,311,"/**
 * Sets a double value for the given key, using the mustLong method.
 * @param key The key to set.
 * @param value The double value to set.
 * @return The object with the set value.
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,addLinkNfly,"org.apache.hadoop.fs.viewfs.ConfigUtil:addLinkNfly(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI[])",177,180,"/**
 * Adds links to a table, delegating to the default mount table.
 * @param conf Hadoop configuration; src source URI; targets link URIs
 */
",,,,True,14
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,handleEmptyDstDirectoryOnWindows,"org.apache.hadoop.fs.RawLocalFileSystem:handleEmptyDstDirectoryOnWindows(org.apache.hadoop.fs.Path,java.io.File,org.apache.hadoop.fs.Path,java.io.File)",646,673,"/**
 * Handles empty destination directory on Windows to ensure rename works.
 * @param src Source Path
 * @param dst Destination Path
 * @return True if rename was successful, false otherwise.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsAnnotations.java,makeSource,org.apache.hadoop.metrics2.lib.MetricsAnnotations:makeSource(java.lang.Object),36,39,"/**
 * Creates a MetricsSource from a given object.
 * @param source The object to create a MetricsSource from.
 * @return A MetricsSource instance.
 */
","* Make an metrics source from an annotated object.
   * @param source  the annotated object.
   * @return a metrics source",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/lib/MetricsAnnotations.java,newSourceBuilder,org.apache.hadoop.metrics2.lib.MetricsAnnotations:newSourceBuilder(java.lang.Object),41,44,"/**
 * Creates a new MetricsSourceBuilder for the given source object.
 * @param source The object to create a metrics source for.
 * @return A MetricsSourceBuilder instance.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkOneDir,org.apache.hadoop.fs.RawLocalFileSystem:mkOneDir(java.io.File),773,775,"/**
 * Creates a single directory.
 * @param p2f File object representing the directory to create.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkdirsWithOptionalPermission,"org.apache.hadoop.fs.RawLocalFileSystem:mkdirsWithOptionalPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",818,839,"/**
 * Creates directories recursively with optional permissions.
 * @param f The path to create.
 * @param permission File system permission to apply.
 * @return True if successful, false otherwise.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long)",1205,1211,"/**
* Creates a data output stream for writing to a file.
* @param f Path to the file. Overwrite, bufferSize, etc.
* @return FSDataOutputStream
*/
","* Create an FSDataOutputStream at the indicated Path.
   * @param f the file name to open
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param replication required block replication for the file.
   * @param blockSize the size of the buffer to be used.
   * @throws IOException IO failure
   * @return output stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,create,"org.apache.hadoop.fs.FileContext:create(org.apache.hadoop.fs.Path,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",681,706,"/**
 * Creates a data output stream for the given path.
 * @param f Path to create.
 * @param createFlag Create flags.
 * @param opts Creation options.
 * @return FSDataOutputStream object.
 */
","* Create or overwrite file on indicated path and returns an output stream for
   * writing into the file.
   * 
   * @param f the file name to open
   * @param createFlag gives the semantics of create; see {@link CreateFlag}
   * @param opts file creation options; see {@link Options.CreateOpts}.
   *          <ul>
   *          <li>Progress - to report progress on the operation - default null
   *          <li>Permission - umask is applied against permission: default is
   *          FsPermissions:getDefault()
   * 
   *          <li>CreateParent - create missing parent path; default is to not
   *          to create parents
   *          <li>The defaults for the following are SS defaults of the file
   *          server implementing the target path. Not all parameters make sense
   *          for all kinds of file system - eg. localFS ignores Blocksize,
   *          replication, checksum
   *          <ul>
   *          <li>BufferSize - buffersize used in FSDataOutputStream
   *          <li>Blocksize - block size for file blocks
   *          <li>ReplicationFactor - replication for blocks
   *          <li>ChecksumParam - Checksum parameters. server default is used
   *          if not specified.
   *          </ul>
   *          </ul>
   * 
   * @return {@link FSDataOutputStream} for created file
   * 
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If file <code>f</code> already exists
   * @throws FileNotFoundException If parent of <code>f</code> does not exist
   *           and <code>createParent</code> is false
   * @throws ParentNotDirectoryException If parent of <code>f</code> is not a
   *           directory.
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>f</code> is not valid",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,mkdir,"org.apache.hadoop.fs.FileContext:mkdir(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean)",799,816,"/**
 * Creates a directory with specified permissions and parent creation.
 * @param dir The path of the directory to create.
 * @param permission Permissions for the directory.
 * @param createParent Whether to create parent directories.
 */
","* Make(create) a directory and all the non-existent parents.
   * 
   * @param dir - the dir to make
   * @param permission - permissions is set permission{@literal &~}umask
   * @param createParent - if true then missing parent dirs are created if false
   *          then parent must exist
   * 
   * @throws AccessControlException If access is denied
   * @throws FileAlreadyExistsException If directory <code>dir</code> already
   *           exists
   * @throws FileNotFoundException If parent of <code>dir</code> does not exist
   *           and <code>createParent</code> is false
   * @throws ParentNotDirectoryException If parent of <code>dir</code> is not a
   *           directory
   * @throws UnsupportedFileSystemException If file system for <code>dir</code>
   *         is not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server
   * 
   * RuntimeExceptions:
   * @throws InvalidPathException If path <code>dir</code> is not valid",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/CompressionCodecFactory.java,main,org.apache.hadoop.io.compress.CompressionCodecFactory:main(java.lang.String[]),301,358,"/**
 * Processes a file, encoding or decoding based on command-line args.
 */","* A little test program.
   * @param args arguments.
   * @throws Exception exception.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.KeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",47,50,"/**
 * Initializes a KeyStoreProvider with a URI and configuration.
 * @param uri The URI of the keystore.
 * @param conf The configuration object.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.LocalKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",54,57,"/**
 * Initializes a LocalKeyStoreProvider with URI and configuration.
 * @param uri The URI of the keystore.
 * @param conf The configuration object.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getServerPrincipal,"org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.lang.String)",185,196,"/**
 * Extracts server principal from config, replacing hostname.
 * @param principalConfig Principal config string.
 * @param hostname Hostname to substitute in the config.
 * @return Server principal string.
 */
","* Convert Kerberos principal name pattern to valid Kerberos principal
   * names. It replaces hostname pattern with hostname, which should be
   * fully-qualified domain name. If hostname is null or ""0.0.0.0"", it uses
   * dynamically looked-up fqdn of the current host instead.
   * 
   * @param principalConfig
   *          the Kerberos principal name conf value to convert
   * @param hostname
   *          the fully-qualified domain name used for substitution
   * @return converted Kerberos principal name
   * @throws IOException if the client address cannot be determined",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getServerPrincipal,"org.apache.hadoop.security.SecurityUtil:getServerPrincipal(java.lang.String,java.net.InetAddress)",212,227,"/**
 * Extracts server principal from config, replacing hostname pattern.
 * @param principalConfig Principal configuration string.
 * @param addr Client InetAddress, used to resolve hostname.
 * @return Server principal string.
 */
","* Convert Kerberos principal name pattern to valid Kerberos principal names.
   * This method is similar to {@link #getServerPrincipal(String, String)},
   * except 1) the reverse DNS lookup from addr to hostname is done only when
   * necessary, 2) param addr can't be null (no default behavior of using local
   * hostname when addr is null).
   * 
   * @param principalConfig
   *          Kerberos principal name pattern to convert
   * @param addr
   *          InetAddress of the host used for substitution
   * @return converted Kerberos principal name
   * @throws IOException if the client address cannot be determined",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,addProtocol,"org.apache.hadoop.ipc.RPC$Server:addProtocol(org.apache.hadoop.ipc.RPC$RpcKind,java.lang.Class,java.lang.Object)",1218,1222,"/**
 * Adds a protocol and implementation to the server.
 * @param rpcKind Protocol type.
 * @param protocolClass Protocol class.
 * @param protocolImpl Protocol implementation.
 * @return The server instance.
 */
","* Add a protocol to the existing server.
     * @param rpcKind - input rpcKind
     * @param protocolClass - the protocol class
     * @param protocolImpl - the impl of the protocol that will be called
     * @return the server (for convenience)",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,createKeyProvider,"org.apache.hadoop.util.KMSUtil:createKeyProvider(org.apache.hadoop.conf.Configuration,java.lang.String)",59,64,"/**
 * Creates a KeyProvider based on configuration and key name.
 * @param conf Configuration object.
 * @param configKeyName Key name in the configuration.
 * @return KeyProvider or null if URI is null.
 */
","* Creates a new KeyProvider from the given Configuration
   * and configuration key name.
   *
   * @param conf Configuration
   * @param configKeyName The configuration key name
   * @return new KeyProvider, or null if no provider was found.
   * @throws IOException if the KeyProvider is improperly specified in
   *                             the Configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/KMSUtil.java,getKeyProviderUri,org.apache.hadoop.util.KMSUtil:getKeyProviderUri(org.apache.hadoop.conf.Configuration),66,69,"/**
 * Gets the URI for the key provider from the configuration.
 * @param conf Configuration object to retrieve URI from.
 * @return URI of the key provider.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultUri,org.apache.hadoop.fs.FileSystem:getDefaultUri(org.apache.hadoop.conf.Configuration),297,304,"/**
 * Creates a URI from the default filesystem name in configuration.
 * @param conf Hadoop configuration object.
 * @return URI representing the default filesystem.
 */
","* Get the default FileSystem URI from a configuration.
   * @param conf the configuration to use
   * @return the uri of the default filesystem",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/PassthroughCodec.java,setConf,org.apache.hadoop.io.compress.PassthroughCodec:setConf(org.apache.hadoop.conf.Configuration),95,102,"/**
 * Sets the configuration object. Updates the default extension.
 * @param conf The configuration object to set.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateKinitExecutable,org.apache.hadoop.security.KDiag:validateKinitExecutable(),715,727,"/**
 * Validates the Kerberos kinit executable path.
 * Checks if the path is absolute and verifies file validity.
 */","* A cursory look at the {@code kinit} executable.
   *
   * If it is an absolute path: it must exist with a size > 0.
   * If it is just a command, it has to be on the path. There's no check
   * for that -but the PATH is printed out.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getSocketAddr,"org.apache.hadoop.conf.Configuration:getSocketAddr(java.lang.String,java.lang.String,int)",2566,2570,"/**
 * Creates an InetSocketAddress from a name, default address, and port.
 */
","* Get the socket address for <code>name</code> property as a
   * <code>InetSocketAddress</code>.
   * @param name property name.
   * @param defaultAddress the default value
   * @param defaultPort the default port
   * @return InetSocketAddress",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,updateConnectAddr,"org.apache.hadoop.conf.Configuration:updateConnectAddr(java.lang.String,java.lang.String,java.lang.String,java.net.InetSocketAddress)",2596,2614,"/**
 * Updates connection address based on properties, or falls back.
 * @param hostProperty Host property name.
 * @param addressProperty Address property name.
 * @param defaultAddressValue Default address value.
 * @param addr Original InetSocketAddress.
 * @return Updated InetSocketAddress.
 */
","* Set the socket address a client can use to connect for the
   * <code>name</code> property as a <code>host:port</code>.  The wildcard
   * address is replaced with the local host's address. If the host and address
   * properties are configured the host component of the address will be combined
   * with the port component of the addr to generate the address.  This is to allow
   * optional control over which host name is used in multi-home bind-host
   * cases where a host can have multiple names
   * @param hostProperty the bind-host configuration name
   * @param addressProperty the service address configuration name
   * @param defaultAddressValue the service default address configuration value
   * @param addr InetSocketAddress of the service listener
   * @return InetSocketAddress for clients to connect",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,initializeMetadataCache,org.apache.hadoop.fs.HarFileSystem:initializeMetadataCache(org.apache.hadoop.conf.Configuration),108,113,"/**
 * Initializes the metadata cache with a size from configuration.
 * @param conf Configuration object to get cache size from.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,build,org.apache.hadoop.fs.FileContext$FSDataInputStreamBuilder:build(),2971,2990,"/**
 * Opens a file using provided parameters and options.
 * @return CompletableFuture containing FSDataInputStream.
 * @throws IOException if an I/O error occurs.
 */
","* Perform the open operation.
     *
     * @return a future to the input stream.
     * @throws IOException early failure to open
     * @throws UnsupportedOperationException if the specific operation
     * is not supported.
     * @throws IllegalArgumentException if the parameters are not valid.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,build,org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:build(),4941,4958,"/**
* Builds and opens an FSDataInputStream with specified options.
* Returns a CompletableFuture wrapping the stream.
*/
","* Perform the open operation.
     * Returns a future which, when get() or a chained completion
     * operation is invoked, will supply the input stream of the file
     * referenced by the path/path handle.
     * @return a future to the input stream.
     * @throws IOException early failure to open
     * @throws UnsupportedOperationException if the specific operation
     * is not supported.
     * @throws IllegalArgumentException if the parameters are not valid.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,setConfigurationFromURI,"org.apache.hadoop.fs.sftp.SFTPFileSystem:setConfigurationFromURI(java.net.URI,org.apache.hadoop.conf.Configuration)",97,135,"/**
 * Configures the provided Configuration object from the URI info.
 * @param uriInfo URI containing SFTP configuration details.
 * @param conf Configuration object to be updated.
 * @throws IOException if host is null.
 */
","* Set configuration from UI.
   *
   * @param uri
   * @param conf
   * @throws IOException",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,connect,org.apache.hadoop.fs.sftp.SFTPFileSystem:connect(),143,157,"/**
 * Connects to an SFTP server using configuration.
 * @return ChannelSftp object representing the connection.
 * @throws IOException if connection fails.
 */
","* Connecting by using configuration parameters.
   *
   * @return An FTPClient instance
   * @throws IOException",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,<init>,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:<init>(org.apache.hadoop.fs.ChecksumFileSystem,org.apache.hadoop.fs.Path)",180,185,"/**
 * Constructs a ChecksumFSInputChecker with default buffer size.
 * @param fs ChecksumFileSystem instance
 * @param file Path to the file being checked
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,initFromFS,org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:initFromFS(),113,116,"/**
 * Initializes the buffer size from the file system configuration.
 */",* Initialize from a filesystem.,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,create,"org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:create(org.apache.hadoop.fs.shell.PathData,boolean)",515,544,"/**
 * Creates a FSDataOutputStream. Uses lazy persist if specified.
 * @param item PathData object containing the path.
 * @param lazyPersist Flag to enable lazy persist.
 * @return FSDataOutputStream object.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,open,org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.Path),996,999,"/**
 * Opens an FSDataInputStream for the given path.
 * @param f Path to open; must not be null.
 * @throws IOException if an I/O error occurs.
 */
","* Opens an FSDataInputStream at the indicated Path.
   * @param f the file to open
   * @throws IOException IO failure
   * @return input stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,open,org.apache.hadoop.fs.FileSystem:open(org.apache.hadoop.fs.PathHandle),1014,1017,"/**
 * Opens an FSDataInputStream for the given file handle.
 * @param fd PathHandle representing the file to open.
 * @throws IOException if an I/O error occurs.
 */
","* Open an FSDataInputStream matching the PathHandle instance. The
   * implementation may encode metadata in PathHandle to address the
   * resource directly and verify that the resource referenced
   * satisfies constraints specified at its construciton.
   * @param fd PathHandle object returned by the FS authority.
   * @throws InvalidPathHandleException If {@link PathHandle} constraints are
   *                                    not satisfied
   * @throws IOException IO failure
   * @throws UnsupportedOperationException If {@link #open(PathHandle, int)}
   *                                       not overridden by subclass
   * @return input stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,append,org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path),1516,1519,"/**
 * Appends data to a file.
 * @param f the path to the file
 * @return FSDataOutputStream for appending
 */
","* Append to an existing file (optional operation).
   * Same as
   * {@code append(f, getConf().getInt(IO_FILE_BUFFER_SIZE_KEY,
   *     IO_FILE_BUFFER_SIZE_DEFAULT), null)}
   * @param f the existing file to be appended.
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported
   *         (default).
   * @return output stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,append,"org.apache.hadoop.fs.FileSystem:append(org.apache.hadoop.fs.Path,boolean)",1558,1561,"/**
 * Appends data to a file.
 * @param f the path to the file
 * @param appendToNewBlock if true, appends to a new block
 * @return FSDataOutputStream for appending
 */
","* Append to an existing file (optional operation).
   * @param f the existing file to be appended.
   * @param appendToNewBlock whether to append data to a new block
   * instead of the end of the last partial block
   * @throws IOException IO failure
   * @throws UnsupportedOperationException if the operation is unsupported
   *         (default).
   * @return output stream.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,setConf,org.apache.hadoop.fs.ChecksumFileSystem:setConf(org.apache.hadoop.conf.Configuration),83,93,"/**
 * Sets the configuration.
 * @param conf The configuration object.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,getSumBufferSize,"org.apache.hadoop.fs.ChecksumFileSystem:getSumBufferSize(int,int)",156,163,"/**
 * Calculates the sum buffer size based on provided values.
 * @param bytesPerSum Bytes per sum.
 * @param bufferSize Overall buffer size.
 * @return Calculated sum buffer size.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,org.apache.hadoop.fs.FileSystem$Cache:<init>(org.apache.hadoop.conf.Configuration),3657,3663,"/**
 * Initializes the Cache with a given configuration and permits.
 * @param conf Configuration object for cache parameters.
 */
","* Instantiate. The configuration is used to read the
     * count of permits issued for concurrent creation
     * of filesystem instances.
     * @param conf configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Metadata)",2948,2974,"/**
 * Creates a Sorter with provided configuration and metadata.
 * Uses deprecated keys if present in the configuration.
 */
","* Sort and merge using an arbitrary {@link RawComparator}.
     * @param fs input FileSystem.
     * @param comparator input RawComparator.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param conf input Configuration.
     * @param metadata input metadata.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBlockSize,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBlockSize(org.apache.hadoop.conf.Configuration),130,133,"/**
 * Gets the Bzip2 compression block size from configuration.
 * @param conf Configuration object; retrieves the block size.
 * @return int block size value.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getWorkFactor,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getWorkFactor(org.apache.hadoop.conf.Configuration),139,142,"/**
 * Gets the Bzip2 compression work factor from configuration.
 * @param conf Configuration object to retrieve the value from.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createOutputStream,"org.apache.hadoop.io.compress.DefaultCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",60,67,"/**
 * Creates a compression output stream.
 * @param out OutputStream to wrap.
 * @param compressor Compressor to use.
 * @return CompressionOutputStream instance.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/DefaultCodec.java,createInputStream,"org.apache.hadoop.io.compress.DefaultCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",86,93,"/**
 * Creates a decompression input stream.
 * @param in Input stream to decompress.
 * @param decompressor Decompression algorithm.
 * @return DecompressionInputStream instance.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createOutputStream,"org.apache.hadoop.io.compress.Lz4Codec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",82,94,"/**
 * Creates a compression output stream using the given compressor.
 * @param out Output stream to compress.
 * @param compressor Compressor to use.
 * @return CompressionOutputStream instance.
 * @throws IOException if an I/O error occurs.
 */
","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createInputStream,"org.apache.hadoop.io.compress.Lz4Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",146,153,"/**
 * Creates a compression input stream using LZ4 codec.
 * @param in Input stream to decompress.
 * @param decompressor Decompressor object.
 * @return CompressionInputStream instance.
 */
","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createDecompressor,org.apache.hadoop.io.compress.Lz4Codec:createDecompressor(),170,176,"/**
 * Creates and returns an Lz4Decompressor with configured buffer size.
 */","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createOutputStream,"org.apache.hadoop.io.compress.BZip2Codec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",122,130,"/**
 * Creates a compression output stream based on native Bzip2.
 * @param out OutputStream to wrap
 * @param compressor Compressor to use
 * @return CompressionOutputStream instance
 */
","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to, to have it 
   *         compressed
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createInputStream,"org.apache.hadoop.io.compress.BZip2Codec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",177,186,"/**
 * Creates a compression input stream based on native Bzip2.
 * @param in Input stream to decompress.
 * @param decompressor Decompressor object.
 * @return CompressionInputStream instance.
 */
","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}, and return a 
   * stream for uncompressed data.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getCompressionLevel,org.apache.hadoop.io.compress.ZStandardCodec:getCompressionLevel(org.apache.hadoop.conf.Configuration),88,92,"/**
 * Gets the Zstd compression level from the configuration.
 * @param conf Hadoop configuration object.
 * @return Zstd compression level (int).
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getBufferSize,org.apache.hadoop.io.compress.ZStandardCodec:getBufferSize(org.apache.hadoop.conf.Configuration),108,111,"/**
 * Gets the Zstd buffer size from the configuration.
 * @param conf Hadoop configuration object.
 * @return Zstd buffer size (int).
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createOutputStream,"org.apache.hadoop.io.compress.GzipCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",51,60,"/**
 * Creates a compression output stream.
 * @param out OutputStream to wrap.
 * @param compressor Compressor to use, null for no compression.
 * @return CompressionOutputStream or standard OutputStream.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/GzipCodec.java,createInputStream,"org.apache.hadoop.io.compress.GzipCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",83,93,"/**
 * Creates a compression input stream using the given decompressor.
 * @param in InputStream to decompress
 * @param decompressor Decompressor to use
 * @return CompressionInputStream
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createOutputStream,"org.apache.hadoop.io.compress.SnappyCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",82,94,"/**
 * Creates a compression output stream using Snappy codec.
 * @param out OutputStream to write to.
 * @param compressor Compressor to use.
 * @return BlockCompressorStream instance.
 */
","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to have it compressed
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createCompressor,org.apache.hadoop.io.compress.SnappyCodec:createCompressor(),111,117,"/**
 * Creates a SnappyCompressor with buffer size from configuration.
 */
","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createInputStream,"org.apache.hadoop.io.compress.SnappyCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",143,150,"/**
 * Creates a compression input stream using Snappy codec.
 * @param in Input stream to decompress.
 * @param decompressor Decompressor instance.
 * @return CompressionInputStream object.
 */
","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/SnappyCodec.java,createDecompressor,org.apache.hadoop.io.compress.SnappyCodec:createDecompressor(),167,173,"/**
 * Creates a SnappyDecompressor with buffer size from configuration.
 */","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration)",114,118,"/**
 * Copies bytes from an input stream to an output stream.
 * Uses a buffer size from the configuration.
 */
","* Copies from one stream to another. <strong>closes the input and output streams 
   * at the end</strong>.
   *
   * @param in InputStrem to read from
   * @param out OutputStream to write to
   * @param conf the Configuration object.
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/IOUtils.java,copyBytes,"org.apache.hadoop.io.IOUtils:copyBytes(java.io.InputStream,java.io.OutputStream,org.apache.hadoop.conf.Configuration,boolean)",130,134,"/**
* Copies bytes from an InputStream to an OutputStream.
* @param in Input stream. @param out Output stream.
* @param close Whether to close streams after copying.
*/
","* Copies from one stream to another.
   *
   * @param in InputStream to read from
   * @param out OutputStream to write to
   * @param conf the Configuration object
   * @param close whether or not close the InputStream and 
   * OutputStream at the end. The streams are closed in the finally clause.
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getChunkBufferSize,org.apache.hadoop.io.file.tfile.TFile:getChunkBufferSize(org.apache.hadoop.conf.Configuration),142,145,"/**
 * Gets the chunk buffer size from configuration, defaults to 1MB.
 * @param conf Configuration object to retrieve the value from.
 * @return Chunk buffer size in bytes.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getFSInputBufferSize,org.apache.hadoop.io.file.tfile.TFile:getFSInputBufferSize(org.apache.hadoop.conf.Configuration),147,149,"/**
 * Gets the FS input buffer size from the configuration.
 * @param conf Configuration object; provides default if not set.
 * @return int representing the buffer size.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getFSOutputBufferSize,org.apache.hadoop.io.file.tfile.TFile:getFSOutputBufferSize(org.apache.hadoop.conf.Configuration),151,153,"/**
 * Gets the FS output buffer size from the configuration.
 * @param conf Configuration object; provides default if not set.
 * @return int representing the buffer size.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,getBufferSize,org.apache.hadoop.io.SequenceFile:getBufferSize(org.apache.hadoop.conf.Configuration),1738,1740,"/**
 * Gets the buffer size from the configuration.
 * @param conf Configuration object to retrieve the value from.
 * @return The buffer size as an integer.
 */
",Get the configured buffer size,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,setConf,org.apache.hadoop.net.ScriptBasedMapping$RawScriptBasedMapping:setConf(org.apache.hadoop.conf.Configuration),156,166,"/**
 * Sets the configuration.
 * @param conf Configuration object; sets scriptName & maxArgs.
 */
","* Set the configuration and extract the configuration parameters of interest
     * @param conf the new configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,createHttpChannelConnector,"org.apache.hadoop.http.HttpServer2$Builder:createHttpChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration)",566,581,"/**
 * Creates and configures a ServerConnector for HTTP connections.
 * @param server The server to connect to.
 * @param httpConfig HTTP configuration.
 * @return A configured ServerConnector.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,doOp,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:doOp(org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider$ProviderCallable,int,boolean)",167,234,"/**
* Executes a provider operation with failover and retry logic.
* @param op Callable operation to perform on a KMS provider.
* @return Result of the operation.
* @throws IOException if operation fails after retries.
*/
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,<init>,org.apache.hadoop.crypto.key.KeyProvider$Options:<init>(org.apache.hadoop.conf.Configuration),341,344,"/**
* Constructs an Options object using the provided configuration.
* @param conf Configuration object containing cipher and bit length.
*/
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoStreamUtils.java,getBufferSize,org.apache.hadoop.crypto.CryptoStreamUtils:getBufferSize(org.apache.hadoop.conf.Configuration),65,68,"/**
 * Gets the buffer size for Hadoop security crypto.
 * @param conf Configuration object to retrieve the value from.
 * @return Integer buffer size.
 */
","* Read crypto buffer size.
   *
   * @param conf configuration.
   * @return hadoop.security.crypto.buffer.size.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,parseNumLevels,"org.apache.hadoop.ipc.CallQueueManager:parseNumLevels(java.lang.String,org.apache.hadoop.conf.Configuration)",391,411,"/**
 * Parses the number of priority levels from the configuration.
 * Uses FCQ levels if set, otherwise falls back to scheduler levels.
 */","* Read the number of levels from the configuration.
   * This will affect the FairCallQueue's overall capacity.
   * @throws IllegalArgumentException on invalid queue count",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getRpcTimeout,org.apache.hadoop.ipc.RPC:getRpcTimeout(org.apache.hadoop.conf.Configuration),829,832,"/**
 * Gets the RPC timeout from the configuration.
 * @param conf Hadoop configuration object.
 * @return RPC timeout in milliseconds.
 */
","* Get the RPC time from configuration;
   * If not set in the configuration, return the default value.
   *
   * @param conf Configuration
   * @return the RPC timeout (ms)",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getPingInterval,org.apache.hadoop.ipc.Client:getPingInterval(org.apache.hadoop.conf.Configuration),187,190,"/**
 * Gets the IPC ping interval from the configuration.
 * @param conf Configuration object.
 * @return Ping interval in milliseconds.
 */
","* Get the ping interval from configuration;
   * If not set in the configuration, return the default value.
   * 
   * @param conf Configuration
   * @return the ping interval",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getRpcTimeout,org.apache.hadoop.ipc.Client:getRpcTimeout(org.apache.hadoop.conf.Configuration),221,226,"/**
 * Gets the RPC timeout from configuration, defaulting if needed.
 * @param conf Hadoop configuration object.
 * @return RPC timeout in milliseconds.
 */
","* The time after which a RPC will timeout.
   *
   * @param conf Configuration
   * @return the timeout period in milliseconds.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedTimeCostProvider.java,init,"org.apache.hadoop.ipc.WeightedTimeCostProvider:init(java.lang.String,org.apache.hadoop.conf.Configuration)",65,90,"/**
 * Initializes timing weights from configuration.
 * @param namespace Configuration namespace.
 * @param conf Hadoop configuration object.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,<init>,"org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration)",94,96,"/**
 * Constructs a LineReader with default buffer size.
 * @param in Input stream to read from.
 * @param conf Configuration object.
 */
","* Create a line reader that reads from the given stream using the
   * <code>io.file.buffer.size</code> specified in the given
   * <code>Configuration</code>.
   * @param in input stream
   * @param conf configuration
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/LineReader.java,<init>,"org.apache.hadoop.util.LineReader:<init>(java.io.InputStream,org.apache.hadoop.conf.Configuration,byte[])",138,144,"/**
 * Constructs a LineReader with an input stream, config, and delimiter.
 * @param in Input stream to read from.
 * @param conf Configuration object.
 * @param recordDelimiterBytes Record delimiter bytes.
 */
","* Create a line reader that reads from the given stream using the
   * <code>io.file.buffer.size</code> specified in the given
   * <code>Configuration</code>, and using a custom delimiter of array of
   * bytes.
   * @param in input stream
   * @param conf configuration
   * @param recordDelimiterBytes The delimiter
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getInt,"org.apache.hadoop.conf.ConfigurationWithLogging:getInt(java.lang.String,int)",87,92,"/**
 * Gets an integer property. Logs the retrieved value.
 * @param name Property name.
 * @param defaultValue Default value if not found.
 * @return Integer property value.
 */
","* See {@link Configuration#getInt(String, int)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,setConf,org.apache.hadoop.ha.HAAdmin:setConf(org.apache.hadoop.conf.Configuration),337,345,"/**
 * Sets the configuration.
 * @param conf The Hadoop configuration object.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,getSshConnectTimeout,org.apache.hadoop.ha.SshFenceByTcpPort:getSshConnectTimeout(),215,218,"/**
 * Gets the SSH connection timeout in milliseconds.
 * Uses config or default if not found.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,getGracefulFenceTimeout,org.apache.hadoop.ha.FailoverController:getGracefulFenceTimeout(org.apache.hadoop.conf.Configuration),82,86,"/**
 * Gets the graceful fence timeout from the configuration.
 * @param conf Hadoop configuration object.
 * @return Graceful fence timeout in milliseconds.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,getRpcTimeoutToNewActive,org.apache.hadoop.ha.FailoverController:getRpcTimeoutToNewActive(org.apache.hadoop.conf.Configuration),88,92,"/**
 * Gets the timeout for new active RPC calls.
 * @param conf Hadoop configuration object.
 * @return Timeout value in milliseconds.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,setTimeout,"org.apache.hadoop.fs.ftp.FTPFileSystem:setTimeout(org.apache.commons.net.ftp.FTPClient,org.apache.hadoop.conf.Configuration)",173,177,"/**
 * Sets the control keep-alive timeout for the FTP client.
 * @param client FTP client to configure.
 * @param conf Configuration containing timeout value.
 */
","* Set the FTPClient's timeout based on configuration.
   * FS_FTP_TIMEOUT is set as timeout (defaults to DEFAULT_TIMEOUT).",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FSBuilderSupport.java,getLong,"org.apache.hadoop.fs.impl.FSBuilderSupport:getLong(java.lang.String,long)",77,93,"/**
 * Gets a long value for the given key, using defVal if invalid.
 * @param key option key
 * @param defVal default long value
 * @return long value or default if parsing fails.
 */
","* Get a long value with resilience to unparseable values.
   * @param key key to log
   * @param defVal default value
   * @return long value",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,canBeSafelyDeleted,org.apache.hadoop.fs.shell.Delete$Rm:canBeSafelyDeleted(org.apache.hadoop.fs.shell.PathData),129,149,"/**
 * Checks if an item can be safely deleted based on configuration.
 * @param item PathData object containing file system details.
 * @return True if deletion is safe, false otherwise.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FileSystem:getDefaultBlockSize(),2753,2757,"/**
 * Gets the default block size from configuration, defaults to 32MB.
 */","* Return the number of bytes that large input files should be optimally
   * be split into to minimize I/O time.
   * @deprecated use {@link #getDefaultBlockSize(Path)} instead
   * @return default block size.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DF.java,<init>,"org.apache.hadoop.fs.DF:<init>(java.io.File,org.apache.hadoop.conf.Configuration)",49,52,"/**
 * Constructs a DiskFailure with a file path and interval.
 * @param path File object representing the disk path.
 * @param interval Interval for disk failure checks.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,getInterval,org.apache.hadoop.fs.GetSpaceUsed$Builder:getInterval(),58,67,"/**
 * Gets the interval for dual access. Returns default if not configured.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,getJitter,org.apache.hadoop.fs.GetSpaceUsed$Builder:getJitter(),118,129,"/**
 * Returns the jitter value, using default if not configured.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,ensureInitialized,org.apache.hadoop.io.nativeio.NativeIO:ensureInitialized(),1039,1048,"/**
 * Initializes the UID to User mapping cache if not already done.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,"org.apache.hadoop.security.ShellBasedIdMapping:<init>(org.apache.hadoop.conf.Configuration,boolean)",106,128,"/**
 * Initializes the ShellBasedIdMapping with config and build flag.
 * @param conf Configuration object.
 * @param constructFullMapAtInit Flag to build full map at init.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$DelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",71,78,"/**
 * Constructs a DelegationTokenSecretManager with configuration and token kind.
 * @param conf Configuration object.
 * @param tokenKind Token identifier.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseDecayPeriodMillis,"org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayPeriodMillis(java.lang.String,org.apache.hadoop.conf.Configuration)",359,377,"/**
 * Parses decay period in milliseconds from configuration.
 * @param ns namespace
 * @param conf Configuration object
 * @return Decay period in milliseconds
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JvmPauseMonitor.java,serviceInit,org.apache.hadoop.util.JvmPauseMonitor:serviceInit(org.apache.hadoop.conf.Configuration),74,79,"/**
 * Initializes service configuration from provided Configuration.
 * @param conf Configuration object containing service parameters.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getLong,"org.apache.hadoop.conf.ConfigurationWithLogging:getLong(java.lang.String,long)",97,102,"/**
 * Gets a long value by name, using default if not found.
 * Logs the retrieved value.
 */
","* See {@link Configuration#getLong(String, long)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,<init>,"org.apache.hadoop.ha.HealthMonitor:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceTarget)",115,135,"/**
 * Initializes HealthMonitor with configuration and target.
 * @param conf Configuration object.
 * @param target Target service to monitor.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,initBloomFilter,org.apache.hadoop.io.BloomMapFile$Writer:initBloomFilter(org.apache.hadoop.conf.Configuration),166,180,"/**
 * Initializes the Bloom filter with configuration parameters.
 * @param conf Configuration object containing filter settings.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getFloat,"org.apache.hadoop.conf.ConfigurationWithLogging:getFloat(java.lang.String,float)",77,82,"/**
 * Gets a float value by name, using defaultValue if not found.
 * Logs the retrieved value.
 */
","* See {@link Configuration#getFloat(String, float)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseDecayFactor,"org.apache.hadoop.ipc.DecayRpcScheduler:parseDecayFactor(java.lang.String,org.apache.hadoop.conf.Configuration)",339,357,"/**
 * Parses decay factor from configuration, validates, and returns it.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,initialize,"org.apache.hadoop.fs.TrashPolicyDefault:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",87,99,"/**
 * Initializes the file system with configuration, file system, and home path.
 */","* @deprecated Use {@link #initialize(Configuration, FileSystem)} instead.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,initialize,"org.apache.hadoop.fs.TrashPolicyDefault:initialize(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",101,118,"/**
 * Initializes the trash operation with configuration and file system.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/FsCommand.java,processRawArguments,org.apache.hadoop.fs.shell.FsCommand:processRawArguments(java.util.LinkedList),102,122,"/**
 * Processes raw arguments, expands them, and checks fs.defaultFS.
 * @param args LinkedList of String arguments to process.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,closeChildFileSystems,org.apache.hadoop.fs.viewfs.ViewFileSystem:closeChildFileSystems(org.apache.hadoop.fs.FileSystem),1966,1984,"/**
 * Closes child file systems, disabling cache if configured.
 * @param fs The parent FileSystem containing child file systems.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ConfigUtil.java,isNestedMountPointSupported,org.apache.hadoop.fs.viewfs.ConfigUtil:isNestedMountPointSupported(org.apache.hadoop.conf.Configuration),270,272,"/**
 * Checks if nested mount point support is enabled in config.
 * @param conf Configuration object; retrieves boolean value.
 * @return True if supported, false otherwise.
 */
","* Check the bool config whether nested mount point is supported. Default: true
   * @param conf - from this conf
   * @return whether nested mount point is supported",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:<init>(org.apache.hadoop.fs.viewfs.InodeTree$INodeDir,long,org.apache.hadoop.security.UserGroupInformation,java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.viewfs.InodeTree)",1410,1426,"/**
 * Constructs a view filesystem directory.
 * @param dir Directory inode, cTime, ugi, URI, config, fsState
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/Lz4Codec.java,createCompressor,org.apache.hadoop.io.compress.Lz4Codec:createCompressor(),111,120,"/**
 * Creates an Lz4Compressor with config-defined buffer size and LZ4HC flag.
 */","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,handleChecksumException,org.apache.hadoop.io.SequenceFile$Reader:handleChecksumException(org.apache.hadoop.fs.ChecksumException),2792,2801,"/**
 * Handles checksum errors: skips or re-throws the exception.
 * Skips if IO_SKIP_CHECKSUM_ERRORS_KEY is true in config.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,getMultipleLinearRandomRetry,"org.apache.hadoop.io.retry.RetryUtils:getMultipleLinearRandomRetry(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String)",179,201,"/**
 * Parses retry policy from configuration, returns null if disabled.
 * @param conf Configuration object, policy spec keys, default values.
 */
","* Return the MultipleLinearRandomRetry policy specified in the conf,
   * or null if the feature is disabled.
   * If the policy is specified in the conf but the policy cannot be parsed,
   * the default policy is returned.
   * 
   * Retry policy spec:
   *   N pairs of sleep-time and number-of-retries ""s1,n1,s2,n2,...""
   * 
   * @param conf configuration.
   * @param retryPolicyEnabledKey     conf property key for enabling retry
   * @param defaultRetryPolicyEnabled default retryPolicyEnabledKey conf value 
   * @param retryPolicySpecKey        conf property key for retry policy spec
   * @param defaultRetryPolicySpec    default retryPolicySpecKey conf value
   * @return the MultipleLinearRandomRetry policy specified in the conf,
   *         or null if the feature is disabled.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,isSupported,org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:isSupported(),283,283,"/**
 * Checks if this feature is supported. Returns true if supported.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addPrometheusServlet,org.apache.hadoop.http.HttpServer2:addPrometheusServlet(org.apache.hadoop.conf.Configuration),818,828,"/**
 * Adds a Prometheus servlet if enabled in configuration.
 * @param conf Configuration object containing Prometheus settings.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addDefaultApps,"org.apache.hadoop.http.HttpServer2:addDefaultApps(org.eclipse.jetty.server.handler.ContextHandlerCollection,java.lang.String,org.apache.hadoop.conf.Configuration)",926,973,"/**
 * Adds default application contexts for logs and static content.
 * @param parent Parent ContextHandlerCollection.
 * @param appDir Application directory.
 * @param conf Configuration object.
 */
","* Add default apps.
   *
   * @param parent contexthandlercollection.
   * @param appDir The application directory
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,addDefaultServlets,org.apache.hadoop.http.HttpServer2:addDefaultServlets(org.apache.hadoop.conf.Configuration),985,995,"/**
 * Adds default servlets to the given configuration.
 */","* Add default servlets.
   * @param configuration the hadoop configuration",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,create,"org.apache.hadoop.metrics2.source.JvmMetrics:create(java.lang.String,java.lang.String,org.apache.hadoop.metrics2.MetricsSystem)",112,123,"/**
 * Creates and registers a JvmMetrics instance with the MetricsSystem.
 * @param processName Process name.
 * @param sessionId Session ID.
 * @param ms MetricsSystem to register with.
 * @return Registered JvmMetrics instance.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HttpCrossOriginFilterInitializer.java,initFilter,"org.apache.hadoop.security.HttpCrossOriginFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",39,52,"/**
 * Initializes the CORS filter based on configuration.
 * @param container Filter container to add the filter to.
 * @param conf Configuration object to check filter enablement.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,validate,org.apache.hadoop.crypto.key.KeyShell$ListCommand:validate(),242,250,"/**
 * Validates the configuration.
 * @return True if valid, false otherwise.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,getServerFailOverEnable,"org.apache.hadoop.ipc.CallQueueManager:getServerFailOverEnable(java.lang.String,org.apache.hadoop.conf.Configuration)",119,141,"/**
 * Checks if server failover is enabled for the given namespace.
 * @param namespace Namespace to check, @param conf Configuration object.
 * @return True if failover is enabled, false otherwise.
 */
","* Return boolean value configured by property 'ipc.<port>.callqueue.overflow.trigger.failover'
   * if it is present. If the config is not present, default config
   * (without port) is used to derive class i.e 'ipc.callqueue.overflow.trigger.failover',
   * and derived value is returned if configured. Otherwise, default value
   * {@link CommonConfigurationKeys#IPC_CALLQUEUE_SERVER_FAILOVER_ENABLE_DEFAULT} is returned.
   *
   * @param namespace Namespace ""ipc"" + ""."" + Server's listener port.
   * @param conf Configuration properties.
   * @return Value returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseBackOffByResponseTimeEnabled,"org.apache.hadoop.ipc.DecayRpcScheduler:parseBackOffByResponseTimeEnabled(java.lang.String,org.apache.hadoop.conf.Configuration)",467,472,"/**
 * Checks if backoff by response time is enabled in config.
 * @param ns Namespace for the configuration.
 * @param conf Configuration object.
 * @return True if enabled, false otherwise.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",1325,1342,"/**
 * Constructs a new Client with provided value class, config, and socket factory.
 */","* Construct an IPC client whose values are of the given {@link Writable}
   * class.
   *
   * @param valueClass input valueClass.
   * @param conf input configuration.
   * @param factory input factory.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getClientBackoffEnable,"org.apache.hadoop.ipc.Server:getClientBackoffEnable(java.lang.String,org.apache.hadoop.conf.Configuration)",920,927,"/**
 * Checks if client backoff is enabled in the configuration.
 * @param prefix Configuration prefix.
 * @param conf Configuration object.
 * @return True if enabled, false otherwise.
 */
",* Get from config if client backoff is enabled on that port.,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getClientBackoffEnable,"org.apache.hadoop.ipc.Server:getClientBackoffEnable(java.lang.String,int,org.apache.hadoop.conf.Configuration)",941,953,"/**
 * Checks if client backoff is enabled for given namespace & port.
 * @param namespace Namespace string.
 * @param port Port integer.
 * @param conf Configuration object.
 * @return True if backoff is enabled, false otherwise.
 */
","* Return boolean value configured by property 'ipc.<port>.backoff.enable'
   * if it is present. If the config is not present, default config
   * (without port) is used to derive class i.e 'ipc.backoff.enable',
   * and derived value is returned if configured. Otherwise, default value
   * {@link CommonConfigurationKeys#IPC_BACKOFF_ENABLE_DEFAULT} is returned.
   *
   * @param namespace Namespace ""ipc"".
   * @param port Server's listener port.
   * @param conf Configuration properties.
   * @return Value returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPasswordFromConfig,org.apache.hadoop.conf.Configuration:getPasswordFromConfig(java.lang.String),2513,2524,"/**
 * Retrieves password from configuration by name.
 * @param name configuration key name
 * @return char array of password or null if not found
 */
","* Fallback to clear text passwords in configuration.
   * @param name the property name.
   * @return clear text password or null",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfigurationWithLogging.java,getBoolean,"org.apache.hadoop.conf.ConfigurationWithLogging:getBoolean(java.lang.String,boolean)",67,72,"/**
 * Gets a boolean value by name, using defaultValue if absent.
 * Logs the retrieved value.
 */
","* See {@link Configuration#getBoolean(String, boolean)}.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFileSystemClass,"org.apache.hadoop.fs.FileSystem:getFileSystemClass(java.lang.String,org.apache.hadoop.conf.Configuration)",3559,3594,"/**
 * Gets the FileSystem class for a given scheme, using config or service files.
 * @param scheme Filesystem scheme (e.g., ""hdfs"")
 * @param conf Hadoop configuration object
 * @return FileSystem class or throws UnsupportedFileSystemException
 */
","* Get the FileSystem implementation class of a filesystem.
   * This triggers a scan and load of all FileSystem implementations listed as
   * services and discovered via the {@link ServiceLoader}
   * @param scheme URL scheme of FS
   * @param conf configuration: can be null, in which case the check for
   * a filesystem binding declaration in the configuration is skipped.
   * @return the filesystem
   * @throws UnsupportedFileSystemException if there was no known implementation
   *         for the scheme.
   * @throws IOException if the filesystem could not be loaded",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,createFileSystem,"org.apache.hadoop.fs.AbstractFileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",169,181,"/**
 * Creates an AbstractFileSystem instance for the given URI.
 * @param uri filesystem URI
 * @param conf configuration object
 * @throws UnsupportedFileSystemException if implementation is missing
 */
","* Create a file system instance for the specified uri using the conf. The
   * conf is used to find the class name that implements the file system. The
   * conf is also passed to the file system for its configuration.
   *
   * @param uri URI of the file system
   * @param conf Configuration for the file system
   * 
   * @return Returns the file system for the given URI
   *
   * @throws UnsupportedFileSystemException file system for <code>uri</code> is
   *           not found",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,loadMappingProviders,org.apache.hadoop.security.CompositeGroupsMapping:loadMappingProviders(),147,160,"/**
 * Loads mapping providers based on configuration.
 * Uses provider names from config to load and register them.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolEngine,"org.apache.hadoop.ipc.RPC:getProtocolEngine(java.lang.Class,org.apache.hadoop.conf.Configuration)",220,230,"/**
 * Retrieves the RpcEngine for a given protocol class and config.
 * @param protocol Protocol class to get the engine for.
 * @param conf Configuration object.
 * @return RpcEngine instance.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getQueueClass,"org.apache.hadoop.ipc.Server:getQueueClass(java.lang.String,org.apache.hadoop.conf.Configuration)",796,802,"/**
 * Gets the CallQueue class.
 * @param prefix Prefix for the queue class name.
 * @param conf Configuration object.
 * @return Queue class for IPC calls.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getQueueClass,"org.apache.hadoop.ipc.Server:getQueueClass(java.lang.String,int,org.apache.hadoop.conf.Configuration)",816,827,"/**
 * Retrieves the queue class for IPC calls.
 * @param namespace Namespace for queue.
 * @param port Port number.
 * @param conf Configuration object.
 * @return Queue class for IPC calls.
 */
","* Return class configured by property 'ipc.<port>.callqueue.impl' if it is
   * present. If the config is not present, default config (without port) is
   * used to derive class i.e 'ipc.callqueue.impl', and derived class is
   * returned if class value is present and valid. If default config is also
   * not present, default class {@link LinkedBlockingQueue} is returned.
   *
   * @param namespace Namespace ""ipc"".
   * @param port Server's listener port.
   * @param conf Configuration properties.
   * @return Class returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getSchedulerClass,"org.apache.hadoop.ipc.Server:getSchedulerClass(java.lang.String,org.apache.hadoop.conf.Configuration)",829,853,"/**
 * Retrieves the RPC scheduler class from the configuration.
 * @param prefix Configuration prefix.
 * @param conf Hadoop configuration object.
 * @return Scheduler class, defaults if not found.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,getSchedulerClass,"org.apache.hadoop.ipc.Server:getSchedulerClass(java.lang.String,int,org.apache.hadoop.conf.Configuration)",869,898,"/**
 * Gets the RpcScheduler class from configuration, falling back as needed.
 * @param namespace Namespace for scheduler config.
 * @param port Port for scheduler config.
 * @param conf Hadoop configuration object.
 * @return RpcScheduler class.
 */
","* Return class configured by property 'ipc.<port>.scheduler.impl' if it is
   * present. If the config is not present, and if property
   * 'ipc.<port>.callqueue.impl' represents FairCallQueue class,
   * return DecayRpcScheduler. If config 'ipc.<port>.callqueue.impl'
   * does not have value FairCallQueue, default config (without port) is used
   * to derive class i.e 'ipc.scheduler.impl'. If default config is also not
   * present, default class {@link DefaultRpcScheduler} is returned.
   *
   * @param namespace Namespace ""ipc"".
   * @param port Server's listener port.
   * @param conf Configuration properties.
   * @return Class returned based on configuration.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getClass,"org.apache.hadoop.conf.Configuration:getClass(java.lang.String,java.lang.Class,java.lang.Class)",2758,2772,"/**
 * Retrieves a class by name, ensuring it extends a given interface.
 * @param name class name
 * @param defaultValue default class
 * @param xface interface to extend
 * @return Class extending xface, or null if not found.
 */
","* Get the value of the <code>name</code> property as a <code>Class</code>
   * implementing the interface specified by <code>xface</code>.
   *   
   * If no such property is specified, then <code>defaultValue</code> is 
   * returned.
   * 
   * An exception is thrown if the returned class does not implement the named
   * interface. 
   * 
   * @param name the conf key name.
   * @param defaultValue default value.
   * @param xface the interface implemented by the named class.
   * @param <U> Interface class type.
   * @return property value as a <code>Class</code>, 
   *         or <code>defaultValue</code>.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getInternal,"org.apache.hadoop.fs.FileSystem$Cache:getInternal(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem$Cache$Key)",3689,3765,"/**
 * Retrieves a FileSystem for the given URI, configuration, and key.
 * @param uri URI of the filesystem
 * @param conf Configuration object
 * @param key Key used to identify the filesystem
 * @return FileSystem object
 */
","* Get the FS instance if the key maps to an instance, creating and
     * initializing the FS if it is not found.
     * If this is the first entry in the map and the JVM is not shutting down,
     * this registers a shutdown hook to close filesystems, and adds this
     * FS to the {@code toAutoClose} set if {@code ""fs.automatic.close""}
     * is set in the configuration (default: true).
     * @param uri filesystem URI
     * @param conf configuration
     * @param key key to store/retrieve this FileSystem in the cache
     * @return a cached or newly instantiated FileSystem.
     * @throws IOException If an I/O error occurred.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/SQLDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.SQLDelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration),81,102,"/**
 * Constructs a SQLDelegationTokenSecretManager with configuration.
 * @param conf Hadoop configuration object
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedUnixGroupsMapping.java,setConf,org.apache.hadoop.security.ShellBasedUnixGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),61,72,"/**
 * Sets the configuration.
 * @param conf The Hadoop configuration object.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,getShutdownTimeout,org.apache.hadoop.util.ShutdownHookManager:getShutdownTimeout(org.apache.hadoop.conf.Configuration),180,191,"/**
 * Gets the service shutdown timeout from configuration.
 * @param conf Configuration object; returns timeout duration.
 */
","* Get the shutdown timeout in seconds, from the supplied
   * configuration.
   * @param conf configuration to use.
   * @return a timeout, always greater than or equal to {@link #TIMEOUT_MINIMUM}",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,getCredentialProvider,org.apache.hadoop.security.alias.CredentialShell$Command:getCredentialProvider(),144,166,"/**
* Retrieves a CredentialProvider, prioritizing user-supplied ones.
* Returns the provider or null if none are valid.
*/
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPasswordFromCredentialProviders,org.apache.hadoop.conf.Configuration:getPasswordFromCredentialProviders(java.lang.String),2478,2506,"/**
 * Retrieves password from credential providers for given name.
 * @param name credential name to search for
 * @return char[] password or null if not found
 * @throws IOException if there's an IO error during retrieval
 */
","* Try and resolve the provided element name as a credential provider
   * alias.
   * @param name alias of the provisioned credential
   * @return password or null if not found
   * @throws IOException when error in fetching password",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,getKeyProvider,org.apache.hadoop.crypto.key.KeyShell$Command:getKeyProvider(),191,213,"/**
 * Retrieves a KeyProvider, prioritizing user-supplied if available.
 * Returns the provider or null if no valid providers are found.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/serializer/avro/AvroReflectSerialization.java,accept,org.apache.hadoop.io.serializer.avro.AvroReflectSerialization:accept(java.lang.Class),55,63,"/**
 * Checks if a class is serializable via Avro reflection.
 * @param c Class to check; returns true if serializable.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawEncoderWithFallback,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",176,202,"/**
 * Creates a RawErasureEncoder, falling back on alternatives if needed.
 * @param conf Configuration object.
 * @param codecName Codec name.
 * @param coderOptions Erasure coder options.
 * @return RawErasureEncoder instance.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawDecoderWithFallback,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoderWithFallback(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",204,230,"/**
 * Creates a RawErasureDecoder, falling back on alternatives if needed.
 * @param conf Configuration object.
 * @param codecName Codec name.
 * @param coderOptions Erasure coder options.
 * @return RawErasureDecoder object.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,createSession,"org.apache.hadoop.ha.SshFenceByTcpPort:createSession(java.lang.String,org.apache.hadoop.ha.SshFenceByTcpPort$Args)",118,128,"/**
 * Creates an SSH session with the given host and user.
 * @param host The SSH host.
 * @param args SSH connection arguments.
 * @return SSH session object.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyServers.java,refresh,org.apache.hadoop.security.authorize.ProxyServers:refresh(),31,33,"/**
 * Refreshes the configuration using default settings.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,parseCapacityWeights,"org.apache.hadoop.ipc.CallQueueManager:parseCapacityWeights(int,java.lang.String,org.apache.hadoop.conf.Configuration)",417,440,"/**
 * Parses capacity weights from configuration, or uses defaults.
 * @param priorityLevels Number of priority levels.
 * @param ns Namespace for configuration.
 * @param conf Hadoop configuration object.
 * @return Array of capacity weights.
 */
","* Read the weights of capacity in callqueue and pass the value to
   * callqueue constructions.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,<init>,"org.apache.hadoop.ipc.metrics.RpcMetrics:<init>(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)",58,111,"/**
 * Initializes RPC metrics with server, config, and quantile intervals.
 * @param server The RPC server instance.
 * @param conf Configuration object for metrics settings.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseThresholds,"org.apache.hadoop.ipc.DecayRpcScheduler:parseThresholds(java.lang.String,org.apache.hadoop.conf.Configuration,int)",379,407,"/**
 * Parses threshold percentages from configuration, or uses defaults.
 * @param ns Configuration namespace.
 * @param conf Hadoop configuration.
 * @param numLevels Number of levels.
 * @return Array of threshold decimals.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WeightedRoundRobinMultiplexer.java,<init>,"org.apache.hadoop.ipc.WeightedRoundRobinMultiplexer:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)",56,79,"/**
 * Creates a WeightedRoundRobinMultiplexer with specified queues and weights.
 * @param aNumQueues Number of queues.
 * @param ns Namespace for configuration.
 * @param conf Hadoop configuration object.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseBackOffResponseTimeThreshold,"org.apache.hadoop.ipc.DecayRpcScheduler:parseBackOffResponseTimeThreshold(java.lang.String,org.apache.hadoop.conf.Configuration,int)",433,456,"/**
 * Parses backoff response time thresholds from configuration.
 * @param ns namespace, conf configuration, numLevels priority levels
 * @return Array of response time thresholds.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getFilterInitializers,org.apache.hadoop.http.HttpServer2:getFilterInitializers(org.apache.hadoop.conf.Configuration),894,916,"/**
 * Retrieves FilterInitializer instances from Configuration.
 * @param conf Configuration object; null if no initializers.
 * @return Array of FilterInitializer instances.
 */
",Get an array of FilterConfiguration specified in the conf,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getInstances,"org.apache.hadoop.conf.Configuration:getInstances(java.lang.String,java.lang.Class)",2787,2798,"/**
 * Retrieves instances of classes implementing the given interface.
 * @param name class name to search
 * @param xface interface to implement
 * @return List of instances implementing the interface
 */
","* Get the value of the <code>name</code> property as a <code>List</code>
   * of objects implementing the interface specified by <code>xface</code>.
   * 
   * An exception is thrown if any of the classes does not exist, or if it does
   * not implement the named interface.
   * 
   * @param name the property name.
   * @param xface the interface implemented by the classes named by
   *        <code>name</code>.
   * @param <U> Interface class type.
   * @return a <code>List</code> of objects implementing <code>xface</code>.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,<init>,"org.apache.hadoop.io.DefaultStringifier:<init>(org.apache.hadoop.conf.Configuration,java.lang.Class)",60,73,"/**
 * Initializes a DefaultStringifier with a configuration and class.
 * @param conf Configuration object.
 * @param c Class to serialize/deserialize.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,init,"org.apache.hadoop.io.SequenceFile$Writer:init(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,boolean,java.lang.Class,java.lang.Class,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,int)",1292,1354,"/**
 * Initializes the object with provided configuration and serializers.
 * @param config Configuration object.
 */",Initialize.,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,getFactory,org.apache.hadoop.util.ReflectionUtils:getFactory(org.apache.hadoop.conf.Configuration),330,335,"/**
 * Returns the SerializationFactory, creating it if necessary.
 * @param conf Configuration object for factory initialization.
 * @return SerializationFactory instance.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/IngressPortBasedResolver.java,setConf,org.apache.hadoop.security.IngressPortBasedResolver:setConf(org.apache.hadoop.conf.Configuration),65,79,"/**
 * Sets the configuration, populating port-to-QOP mapping.
 * @param conf The Hadoop Configuration object.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/WhitelistBasedResolver.java,setConf,org.apache.hadoop.security.WhitelistBasedResolver:setConf(org.apache.hadoop.conf.Configuration),91,109,"/**
 * Sets the configuration, initializes the IP whitelist, and SASL props.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,propagateOptions,"org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",159,166,"/**
 * Delegates option propagation to FutureIO.
 * @param builder FSBuilder instance.
 * @param conf Configuration object.
 * @param prefix Option prefix.
 * @param mandatory If options are mandatory.
 */
","* Propagate options to any builder.
   * {@link FutureIO#propagateOptions(FSBuilder, Configuration, String, boolean)}
   * @param builder builder to modify
   * @param conf configuration to read
   * @param prefix prefix to scan/strip
   * @param mandatory are the options to be mandatory or optional?",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/functional/FutureIO.java,propagateOptions,"org.apache.hadoop.util.functional.FutureIO:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",330,340,"/**
 * Propagates configuration options to the builder, split by prefix.
 * @param builder The builder to update.
 * @param conf Configuration to propagate.
 */
","* Propagate options to any builder, converting everything with the
   * prefix to an option where, if there were 2+ dot-separated elements,
   * it is converted to a schema.
   * See {@link #propagateOptions(FSBuilder, Configuration, String, boolean)}.
   * @param builder builder to modify
   * @param conf configuration to read
   * @param optionalPrefix prefix for optional settings
   * @param mandatoryPrefix prefix for mandatory settings
   * @param <T> type of result
   * @param <U> type of builder
   * @return the builder passed in.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,getChangedProperties,"org.apache.hadoop.conf.ReconfigurableBase:getChangedProperties(org.apache.hadoop.conf.Configuration,org.apache.hadoop.conf.Configuration)",99,103,"/**
 * Gets changed properties between two configurations.
 * @param newConf The new configuration.
 * @param oldConf The old configuration.
 * @return Collection of PropertyChange objects.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurationServlet.java,doGet,"org.apache.hadoop.conf.ReconfigurationServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",199,212,"/**
 * Handles GET requests, prints header, configuration, and footer.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/NativeLibraryChecker.java,main,org.apache.hadoop.util.NativeLibraryChecker:main(java.lang.String[]),45,156,"/**
 * Checks native Hadoop libraries and exits with code 1 if failed.
 * Accepts -a to check all libraries, -h for help.
 */
","* A tool to test native library availability.
   * @param args args.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,getCompressorType,org.apache.hadoop.io.compress.BZip2Codec:getCompressorType(),137,140,"/**
 * Returns the Bzip2 compressor type based on the configuration.
 */","* Get the type of {@link Compressor} needed by this {@link CompressionCodec}.
   *
   * @return the type of compressor needed by this codec.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,getDecompressorType,org.apache.hadoop.io.compress.BZip2Codec:getDecompressorType(),218,221,"/**
 * Returns the Bzip2 decompressor type based on the configuration.
 */","* Get the type of {@link Decompressor} needed by this {@link CompressionCodec}.
   *
   * @return the type of decompressor needed by this codec.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createDecompressor,org.apache.hadoop.io.compress.BZip2Codec:createDecompressor(),228,231,"/**
 * Creates a Bzip2 decompressor using the provided configuration.
 */","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,reloadCachedMappings,org.apache.hadoop.net.TableMapping:reloadCachedMappings(),82,86,"/**
 * Reloads cached mappings, including the raw mapping.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/TableMapping.java,reloadCachedMappings,org.apache.hadoop.net.TableMapping$RawTableMapping:reloadCachedMappings(java.util.List),162,167,"/**
 * Reloads all cached mappings.
 * Reloads all cached mappings, not individual ones.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,<init>,org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:<init>(java.lang.String),510,512,"/**
 * Constructs a HadoopZookeeperFactory with a Zookeeper principal.
 * @param zkPrincipal Zookeeper principal string.
 */
","* Constructor for the helper class to configure the ZooKeeper client connection.
     * @param zkPrincipal Optional.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,lookupGroup,"org.apache.hadoop.security.LdapGroupsMapping:lookupGroup(javax.naming.directory.SearchResult,javax.naming.directory.DirContext,int)",438,475,"/**
* Looks up group names for a user.
* @param result SearchResult object.
* @param c DirContext object.
* @param goUpHierarchy Hierarchy level to traverse.
* @return Set of group names.
*/
","* Perform the second query to get the groups of the user.
   *
   * If posixGroups is enabled, use use posix gid/uid to find.
   * Otherwise, use the general group member attribute to find it.
   *
   * @param result the result object returned from the prior user lookup.
   * @param c the context object of the LDAP connection.
   * @return a list of strings representing group names of the user.
   * @throws NamingException if unable to find group names",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/HadoopKerberosName.java,main,org.apache.hadoop.security.HadoopKerberosName:main(java.lang.String[]),87,93,"/**
 * Processes command-line arguments, converting to short names & printing.
 */",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoCodec.java,getInstance,org.apache.hadoop.crypto.CryptoCodec:getInstance(org.apache.hadoop.conf.Configuration),99,103,"/**
 * Gets a CryptoCodec instance based on configuration.
 * @param conf Configuration object; determines cipher suite.
 * @return CryptoCodec instance.
 */
","* Get crypto codec for algorithm/mode/padding in config value
   * hadoop.security.crypto.cipher.suite
   * 
   * @param conf
   *          the configuration
   * @return CryptoCodec the codec object Null value will be returned if no
   *         crypto codec classes with cipher suite configured.",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,bind,"org.apache.hadoop.ipc.Server:bind(java.net.ServerSocket,java.net.InetSocketAddress,int)",685,688,"/**
 * Binds a ServerSocket to a specific address with a given backlog.
 * @param socket ServerSocket to bind
 * @param address InetSocketAddress to bind to
 * @param backlog Maximum queued connections
 */
","* A convenience method to bind to a given address and report 
   * better exceptions if the address is not a valid host.
   * @param socket the socket to bind
   * @param address the address to bind to
   * @param backlog the number of connections allowed in the queue
   * @throws BindException if the address can't be bound
   * @throws UnknownHostException if the address isn't a valid host name
   * @throws IOException other random errors from bind",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,"org.apache.hadoop.conf.Configuration:writeXml(java.lang.String,java.io.Writer)",3642,3645,"/**
 * Writes XML to a Writer. Delegates to overloaded method.
 * @param propertyName Property name (can be null).
 * @param out Writer to write XML to.
 * @throws IOException, IllegalArgumentException
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,dumpConfiguration,"org.apache.hadoop.conf.Configuration:dumpConfiguration(org.apache.hadoop.conf.Configuration,java.lang.String,java.io.Writer)",3787,3804,"/**
 * Dumps a configuration property to a writer.
 * @param config Configuration object.
 * @param propertyName Property name to dump.
 * @param out Writer to write the JSON to.
 */
","*  Writes properties and their attributes (final and resource)
   *  to the given {@link Writer}.
   *  <ul>
   *  <li>
   *  When propertyName is not empty, and the property exists
   *  in the configuration, the format of the output would be,
   *  <pre>
   *  {
   *    ""property"": {
   *      ""key"" : ""key1"",
   *      ""value"" : ""value1"",
   *      ""isFinal"" : ""key1.isFinal"",
   *      ""resource"" : ""key1.resource""
   *    }
   *  }
   *  </pre>
   *  </li>
   *
   *  <li>
   *  When propertyName is null or empty, it behaves same as
   *  {@link #dumpConfiguration(Configuration, Writer)}, the
   *  output would be,
   *  <pre>
   *  { ""properties"" :
   *      [ { key : ""key1"",
   *          value : ""value1"",
   *          isFinal : ""key1.isFinal"",
   *          resource : ""key1.resource"" },
   *        { key : ""key2"",
   *          value : ""value2"",
   *          isFinal : ""ke2.isFinal"",
   *          resource : ""key2.resource"" }
   *       ]
   *   }
   *  </pre>
   *  </li>
   *
   *  <li>
   *  When propertyName is not empty, and the property is not
   *  found in the configuration, this method will throw an
   *  {@link IllegalArgumentException}.
   *  </li>
   *  </ul>
   *  <p>
   * @param config the configuration
   * @param propertyName property name
   * @param out the Writer to write to
   * @throws IOException raised on errors performing I/O.
   * @throws IllegalArgumentException when property name is not
   *   empty and the property is not found in configuration
   *",,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,formatZK,"org.apache.hadoop.ha.ZKFailoverController:formatZK(boolean,boolean)",282,299,"/**
 * Formats Zookeeper. Requires confirmation unless force is true.
 * @param force Forces format, bypassing confirmation.
 * @param interactive Enables interactive confirmation.
 * @return 0 on success, error code on failure.
 */
",,,,True,15
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,registerSystemSource,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:registerSystemSource(),561,567,"/**
 * Registers and starts the system metrics source adapter.
 * Uses sourceConfigs or default config if not found.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,register,"org.apache.hadoop.metrics2.impl.MetricsSystemImpl:register(java.lang.String,java.lang.String,java.lang.Object)",221,243,"/**
 * Registers a metrics source with a name and description.
 * @param name Source name, uses description if null.
 * @param desc Source description, used if name is null.
 * @param source The metrics source object.
 * @return The provided source object.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkdirs,org.apache.hadoop.fs.RawLocalFileSystem:mkdirs(org.apache.hadoop.fs.Path),808,811,"/**
 * Creates directory and its parent directories, if they do not exist.
 * @param f the path to create
 * @return true if creation was successful
 */
","* Creates the specified directory hierarchy. Does not
   * treat existence as an error.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,mkdirs,"org.apache.hadoop.fs.RawLocalFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",813,816,"/**
 * Creates directory, including parent directories, with permissions.
 * @param f The path of the directory to create.
 * @param permission FsPermission object for setting permissions.
 * @return True if directories were created.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,build,org.apache.hadoop.fs.FileContext$FCDataOutputStreamBuilder:build(),728,748,"/**
 * Builds and creates an FSDataOutputStream with specified options.
 * @return FSDataOutputStream object
 * @throws IOException if an I/O error occurs
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/BouncyCastleFipsKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.BouncyCastleFipsKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",41,44,"/**
 * Initializes the provider with URI and configuration.
 * @param uri URI of the keystore.
 * @param conf Configuration object.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/JavaKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.JavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",40,43,"/**
 * Initializes a JavaKeyStoreProvider with a URI and configuration.
 * @param uri The URI of the keystore.
 * @param conf The configuration for the keystore.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalJavaKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.LocalJavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",40,43,"/**
 * Initializes a LocalJavaKeyStoreProvider with URI and Configuration.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/LocalBouncyCastleFipsKeyStoreProvider.java,<init>,"org.apache.hadoop.security.alias.LocalBouncyCastleFipsKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",41,44,"/**
 * Initializes the provider with a URI and configuration.
 * @param uri The keystore URI.
 * @param conf The configuration object.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,initSpnego,"org.apache.hadoop.http.HttpServer2:initSpnego(org.apache.hadoop.conf.Configuration,java.lang.String,java.util.Properties,java.lang.String,java.lang.String)",1356,1375,"/**
 * Initializes SPNEGO authentication filter with configuration parameters.
 * @param conf Configuration object.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java,getFilterConfigMap,"org.apache.hadoop.security.AuthenticationFilterInitializer:getFilterConfigMap(org.apache.hadoop.conf.Configuration,java.lang.String)",66,91,"/**
 * Retrieves filter configuration from the given configuration.
 * @param conf Configuration object.
 * @param prefix Prefix for configuration properties.
 * @return Map containing filter configuration properties.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,createCuratorClient,"org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:createCuratorClient(org.apache.hadoop.conf.Configuration,java.lang.String)",185,241,"/**
 * Creates a CuratorFramework client with configuration from the given config.
 * @param conf Configuration object.
 * @param namespace ZK namespace.
 * @return CuratorFramework client.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,setJaasConfiguration,org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:setJaasConfiguration(org.apache.zookeeper.client.ZKClientConfig),585,597,"/**
 * Sets JAAS configuration using provided principal and keytab.
 * @param zkClientConfig ZKClientConfig to set the login context.
 * @throws IOException if an error occurs during configuration.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,getServerPrincipal,org.apache.hadoop.security.SaslRpcClient:getServerPrincipal(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),303,356,"/**
 * Gets the server principal, validating against configuration.
 * @param authType Authentication type, used for protocol/server ID.
 * @return Server principal string.
 */","* Get the remote server's principal.  The value will be obtained from
   * the config and cross-checked against the server's advertised principal.
   * 
   * @param authType of the SASL client
   * @return String of the server's principal
   * @throws IOException - error determining configured principal",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,initProtocolMetaInfo,org.apache.hadoop.ipc.RPC$Server:initProtocolMetaInfo(org.apache.hadoop.conf.Configuration),1200,1209,"/**
 * Initializes protocol meta info and registers the protocol service.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKeyProvider,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:createKeyProvider(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",231,242,"/**
 * Creates a KeyProvider based on token service or configuration.
 * @param token Token containing service URI.
 * @param conf Configuration object.
 * @return KeyProvider or null if URI is null.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(java.net.URI,org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)",100,140,"/**
 * Initializes the load-balancing KMS client provider with given parameters.
 * @param uri Token service URI; @param providers KMS client providers
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,decodeHarURI,"org.apache.hadoop.fs.HarFileSystem:decodeHarURI(java.net.URI,org.apache.hadoop.conf.Configuration)",218,254,"/**
 * Decodes a Har URI, converting it to a standard URI format.
 * @param rawURI The Har URI to decode.
 * @param conf Hadoop configuration.
 * @return Decoded URI.
 */
","* decode the raw URI to get the underlying URI
   * @param rawURI raw Har URI
   * @return filtered URI of the underlying fileSystem",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,org.apache.hadoop.fs.FileSystem:get(org.apache.hadoop.conf.Configuration),288,290,"/**
 * Gets a FileSystem instance.
 * @param conf Configuration object for file system settings.
 * @throws IOException if an I/O error occurs.
 */
","* Returns the configured FileSystem implementation.
   * @param conf the configuration to use
   * @return FileSystem.
   * @throws IOException If an I/O error occurred.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,initialize,"org.apache.hadoop.fs.FileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",339,350,"/**
 * Initializes the client with a URI and configuration.
 * @param name URI to connect to; scheme used if empty.
 * @param conf Hadoop configuration object.
 */
","* Initialize a FileSystem.
   *
   * Called after the new FileSystem instance is constructed, and before it
   * is ready for use.
   *
   * FileSystem implementations overriding this method MUST forward it to
   * their superclass, though the order in which it is done, and whether
   * to alter the configuration before the invocation are options of the
   * subclass.
   * @param name a URI whose authority section names the host, port, etc.
   *   for this FileSystem
   * @param conf the configuration
   * @throws IOException on any failure to initialize this instance.
   * @throws IllegalArgumentException if the URI is considered invalid.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstance,org.apache.hadoop.fs.FileSystem:newInstance(org.apache.hadoop.conf.Configuration),621,623,"/**
 * Creates a new FileSystem instance.
 * @param conf Configuration object for the filesystem.
 * @throws IOException if an I/O error occurs.
 */
","* Returns a unique configured FileSystem implementation for the default
   * filesystem of the supplied configuration.
   * This always returns a new FileSystem object.
   * @param conf the configuration to use
   * @return the new FS instance
   * @throws IOException FS creation or initialization failure.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,checkPath,org.apache.hadoop.fs.FileSystem:checkPath(org.apache.hadoop.fs.Path),792,825,"/**
 * Checks if the given path's URI is compatible with this FS.
 * @param path The path to check.
 * @throws IllegalArgumentException if the path is incompatible.
 */
","* Check that a Path belongs to this FileSystem.
   *
   * The base implementation performs case insensitive equality checks
   * of the URIs' schemes and authorities. Subclasses may implement slightly
   * different checks.
   * @param path to check
   * @throws IllegalArgumentException if the path is not considered to be
   * part of this FileSystem.
   *",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getSocketAddr,"org.apache.hadoop.conf.Configuration:getSocketAddr(java.lang.String,java.lang.String,java.lang.String,int)",2539,2556,"/**
 * Creates an InetSocketAddress, using host from property or fallback.
 * @param hostProperty Property for host, null uses fallback.
 * @param addressProperty Unused parameter.
 * @param defaultAddressValue Fallback host value.
 * @param defaultPort Default port value.
 * @return InetSocketAddress object.
 */
","* Get the socket address for <code>hostProperty</code> as a
   * <code>InetSocketAddress</code>. If <code>hostProperty</code> is
   * <code>null</code>, <code>addressProperty</code> will be used. This
   * is useful for cases where we want to differentiate between host
   * bind address and address clients should use to establish connection.
   *
   * @param hostProperty bind host property name.
   * @param addressProperty address property name.
   * @param defaultAddressValue the default value
   * @param defaultPort the default port
   * @return InetSocketAddress",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FutureDataInputStreamBuilder.java,build,org.apache.hadoop.fs.FutureDataInputStreamBuilder:build(),47,50,"/**
 * Builds an FSDataInputStream CompletableFuture.
 * @throws IllegalArgumentException, UnsupportedOperationException, IOException
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,open,"org.apache.hadoop.fs.sftp.SFTPFileSystem:open(org.apache.hadoop.fs.Path,int)",507,539,"/**
 * Opens a file for input stream.
 * @param f Path to the file.
 * @param bufferSize Buffer size for input stream.
 * @throws IOException on error.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,create,"org.apache.hadoop.fs.sftp.SFTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",545,589,"/**
 * Creates a data output stream for writing to a file.
 * @param f path to create, permission, overwrite, etc.
 * @return FSDataOutputStream for writing to the file
 */
","* A stream obtained via this call must be closed before using other APIs of
   * this class or else the invocation will block.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,rename,"org.apache.hadoop.fs.sftp.SFTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",603,612,"/**
 * Renames a file or directory on the remote server.
 * @param src Source path
 * @param dst Destination path
 * @return True on success, false otherwise.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,delete,"org.apache.hadoop.fs.sftp.SFTPFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",614,623,"/**
 * Deletes a file or directory via SFTP.
 * @param f Path to delete.
 * @param recursive If true, delete directories recursively.
 * @return True on success, false otherwise.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,listStatus,org.apache.hadoop.fs.sftp.SFTPFileSystem:listStatus(org.apache.hadoop.fs.Path),625,634,"/**
 * Lists status of files/directories under the given path.
 * @param f Path to list status for
 * @return FileStatus array or null if empty
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getHomeDirectory(),657,673,"/**
 * Gets the home directory path via SFTP connection.
 * Returns Path object or null if connection fails.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.sftp.SFTPFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",688,697,"/**
 * Creates directory tree recursively.
 * @param f Path of the directory to create.
 * @param permission Permissions for the directory.
 * @return True if successful, false otherwise.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getFileStatus,org.apache.hadoop.fs.sftp.SFTPFileSystem:getFileStatus(org.apache.hadoop.fs.Path),699,708,"/**
 * Gets the file status for a given path.
 * @param f the path to get the status of
 * @return FileStatus object or null on error
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,read,"org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSInputChecker:read(long,byte[],int,int)",230,246,"/**
 * Reads up to {@code len} bytes from the file at the given position.
 * @param position starting position
 * @param b buffer to read into
 * @param off offset into buffer
 * @param len number of bytes to read
 * @return number of bytes read
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",91,96,"/**
 * Constructs a FutureDataInputStreamBuilderImpl.
 * @param fileSystem The file system to use.
 * @param path The path to the data stream.
 */
","* Constructor.
   * @param fileSystem owner FS.
   * @param path path",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureDataInputStreamBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.FutureDataInputStreamBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",103,108,"/**
 * Constructs a FutureDataInputStreamBuilderImpl.
 * @param fileSystem The file system.
 * @param pathHandle The path handle.
 */
","* Constructor with PathHandle.
   * @param fileSystem owner FS.
   * @param pathHandle path handle",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,openFileOnInstance,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:openFileOnInstance(org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String)",472,498,"/**
 * Opens a file using either fileSystem_openFile or fs.open().
 * @param instance DynamicWrappedIO instance
 * @param fs FileSystem object
 * @param status FileStatus object
 * @param readPolicies Read policies string
 * @return FSDataInputStream object
 */
","* Open a file.
   * <p>
   * If the WrappedIO class is found, uses
   * {@link #fileSystem_openFile(FileSystem, Path, String, FileStatus, Long, Map)} with
   * {@link #PARQUET_READ_POLICIES} as the list of read policies and passing down
   * the file status.
   * <p>
   * If not, falls back to the classic {@code fs.open(Path)} call.
   * @param instance dynamic wrapped IO instance.
   * @param fs filesystem
   * @param status file status
   * @param readPolicies read policy to use
   * @return the input stream
   * @throws IOException any IO failure.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,getInputStreamForFile,org.apache.hadoop.security.alias.KeyStoreProvider:getInputStreamForFile(),63,66,"/**
 * Opens an input stream for the file.
 * @return InputStream for the file, or null on error.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,loadFromPath,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadFromPath(org.apache.hadoop.fs.Path,char[])",291,298,"/**
 * Loads a FileSystem permission from a path.
 * @param p Path to load from; @param password keystore password
 * @return FsPermission object
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,checkAppend,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:checkAppend(org.apache.hadoop.fs.FileSystem),483,495,"/**
 * Checks if the FileSystem supports appending to the base path.
 * @param fs The FileSystem to check.
 * @return True if appending is supported, false otherwise.
 */
","* Test whether the file system supports append and return the answer.
   *
   * @param fs the target file system",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.io.RawComparator,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)",2934,2937,"/**
 * Constructs a Sorter with default Metadata.
 * @param fs FileSystem, comparator, key/val classes, config.
 */
","* Sort and merge using an arbitrary {@link RawComparator}.
     * @param fs input FileSystem.
     * @param comparator input RawComparator.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param conf input Configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,<init>,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:<init>(org.apache.hadoop.conf.Configuration),72,76,"/**
 * Constructs a Bzip2Compressor with default direct buffer size.
 * @param conf Configuration object for compression parameters.
 */
","* Creates a new compressor, taking settings from the configuration.
   * @param conf configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Compressor.java,reinit,org.apache.hadoop.io.compress.bzip2.Bzip2Compressor:reinit(org.apache.hadoop.conf.Configuration),108,122,"/**
 * Reinitializes the compressor with a new configuration.
 * @param conf Configuration object; null resets to defaults.
 */
","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration. It will reset the compressor's block size and
   * and work factor.
   * 
   * @param conf Configuration storing new settings",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createCompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",279,281,"/**
 * Creates a compression stream wrapping the given output stream.
 * @param downStream Output stream to compress.
 * @param compressor Compressor to use.
 * @param downStreamBufferSize Buffer size for the downstream stream.
 * @return Compression stream.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,init,org.apache.hadoop.io.SequenceFile$Reader:init(boolean),2022,2161,"/**
 * Initializes the SequenceFileReader.
 * @param tempReader if true, skips initialization.
 * @throws IOException if an I/O error occurs.
 */
","* Initialize the {@link Reader}
     * @param tmpReader <code>true</code> if we are constructing a temporary
     *                  reader {@link SequenceFile.Sorter.cloneFileAttributes}, 
     *                  and hence do not initialize every component; 
     *                  <code>false</code> otherwise.
     * @throws IOException",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createDecompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$2:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",275,277,"/**
 * Creates a decompression stream from an input stream.
 * @param downStream Input stream to decompress.
 * @param decompressor Decompressor implementation.
 * @param downStreamBufferSize Buffer size for the stream.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/zstd/ZStandardCompressor.java,reinit,org.apache.hadoop.io.compress.zstd.ZStandardCompressor:reinit(org.apache.hadoop.conf.Configuration),112,120,"/**
 * Reinitializes the compressor with a new configuration.
 * @param conf Configuration object; null if no reinit needed.
 */
","* Prepare the compressor to be used in a new stream with settings defined in
   * the given Configuration. It will reset the compressor's compression level
   * and compression strategy.
   *
   * @param conf Configuration storing new settings",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getCompressionBufferSize,org.apache.hadoop.io.compress.ZStandardCodec:getCompressionBufferSize(org.apache.hadoop.conf.Configuration),94,99,"/**
 * Gets the compression buffer size. Uses conf if set, else uses ZStandard's recommended size.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,getDecompressionBufferSize,org.apache.hadoop.io.compress.ZStandardCodec:getDecompressionBufferSize(org.apache.hadoop.conf.Configuration),101,106,"/**
 * Gets the decompression buffer size. Uses conf if set, else uses default.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,writeStreamToFile,"org.apache.hadoop.fs.shell.CommandWithDestination$TargetFileSystem:writeStreamToFile(java.io.InputStream,org.apache.hadoop.fs.shell.PathData,boolean,boolean)",499,512,"/**
 * Writes an input stream to a file, optionally deleting on exit.
 * @param in Input stream to write.
 * @param target PathData of the target file.
 * @param lazyPersist Whether to use lazy persistence.
 * @param direct Whether to delete the file directly.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,printToStdout,org.apache.hadoop.fs.shell.Display$Cat:printToStdout(java.io.InputStream),98,104,"/**
* Copies bytes from input stream to stdout, closing the input stream.
*/
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareAppendValue,org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendValue(int),554,575,"/**
 * Prepares a DataOutputStream for appending a value.
 * @param length Value length, or -1 for unknown length.
 * @return DataOutputStream to append the value.
 */","* Obtain an output stream for writing a value into TFile. This may only be
     * called right after a key appending operation (the key append stream must
     * be closed).
     * 
     * @param length
     *          The expected length of the value. If length of the value is not
     *          known, set length = -1. Otherwise, the application must write
     *          exactly as many bytes as specified here before calling close on
     *          the returned output stream. Advertising the value size up-front
     *          guarantees that the value is encoded in one chunk, and avoids
     *          intermediate chunk buffering.
     * @throws IOException raised on errors performing I/O.
     * @return DataOutputStream.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Reader$RBlockState:<init>(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion,org.apache.hadoop.conf.Configuration)",496,513,"/**
 * Constructs a RBlockState with given compression algo, stream, region, and config.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Writer$WBlockState:<init>(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.io.BytesWritable,org.apache.hadoop.conf.Configuration)",119,139,"/**
 * Initializes a WBlockState with compression, output stream, and buffer.
 * @param compressionAlgo Compression algorithm.
 */","* @param compressionAlgo
       *          The compression algorithm to be used to for compression.
       * @throws IOException",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,setConf,org.apache.hadoop.net.ScriptBasedMapping:setConf(org.apache.hadoop.conf.Configuration),135,139,"/**
 * Sets the configuration for this object and its raw mapping.
 * @param conf The Configuration object to set.
 */
","* {@inheritDoc}.
   * <p>
   * This will get called in the superclass constructor, so a check is needed
   * to ensure that the raw mapping is defined before trying to relaying a null
   * configuration.
   * </p>
   * @param conf input Configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,setConf,org.apache.hadoop.net.ScriptBasedMappingWithDependency$RawScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration),130,138,"/**
 * Sets the configuration, retrieving dependency script filename.
 * @param conf The configuration object.
 */
","* Set the configuration and extract the configuration parameters of interest
     * @param conf the new configuration",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,createHttpsChannelConnector,"org.apache.hadoop.http.HttpServer2$Builder:createHttpsChannelConnector(org.eclipse.jetty.server.Server,org.eclipse.jetty.server.HttpConfiguration)",583,632,"/**
 * Creates an HTTPS channel connector with SSL context configuration.
 * @param server The server instance.
 * @param httpConfig The HTTP configuration.
 * @return The configured ServerConnector.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getDelegationToken(java.lang.String),251,264,"/**
 * Gets a delegation token for the specified renewer.
 * @param renewer The renewer for the delegation token.
 * @return Delegation token or null if an error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,renewDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token),266,274,"/**
 * Renews a delegation token.
 * @param token The token to renew.
 * @return The renewed token's value.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,cancelDelegationToken,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token),276,285,"/**
 * Cancels a delegation token using the KMSClientProvider.
 * @param token The token to cancel.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,generateEncryptedKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:generateEncryptedKey(java.lang.String),326,344,"/**
 * Generates an encrypted key version using KMS.
 * @param encryptionKeyName Name of the encryption key
 * @return EncryptedKeyVersion object
 * @throws IOException, GeneralSecurityException
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,decryptEncryptedKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),346,364,"/**
 * Decrypts an encrypted key version using KMS.
 * @param encryptedKeyVersion Encrypted key version to decrypt.
 * @throws IOException, GeneralSecurityException on failure.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,reencryptEncryptedKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),366,384,"/**
 * Re-encrypts an encrypted key version using the KMS provider.
 * @param ekv The EncryptedKeyVersion to re-encrypt.
 * @return The re-encrypted EncryptedKeyVersion.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,reencryptEncryptedKeys,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:reencryptEncryptedKeys(java.util.List),386,404,"/**
 * Re-encrypts a list of encrypted key versions using KMS.
 * @param ekvs List of EncryptedKeyVersion objects to re-encrypt.
 * @throws IOException, GeneralSecurityException if re-encryption fails.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeyVersion(java.lang.String),406,414,"/**
 * Retrieves a KeyVersion by versionName.
 * @param versionName Version name of the KeyVersion.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeys,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeys(),416,424,"/**
 * Retrieves a list of keys.
 * @return List of keys, may be empty.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeysMetadata,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeysMetadata(java.lang.String[]),426,434,"/**
 * Retrieves metadata for specified keys.
 * @param names Array of key names.
 * @return Metadata array or null on error.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getKeyVersions(java.lang.String),436,445,"/**
 * Retrieves key versions for a given key name.
 * @param name The name of the key to fetch versions for.
 * @return List of KeyVersion objects.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getCurrentKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getCurrentKey(java.lang.String),447,455,"/**
 * Gets the current key version by name.
 * @param name Key name.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,getMetadata,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:getMetadata(java.lang.String),457,465,"/**
 * Retrieves metadata for the given name.
 * @param name The name of the metadata to retrieve.
 * @return Metadata object or throws IOException.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",467,476,"/**
 * Creates a new key with the given name, material, and options.
 * @param name Key name.
 * @param material Key material.
 * @param options Key creation options.
 * @return KeyVersion object.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",478,495,"/**
 * Creates a new key with the given name and options.
 * @param name Key name.
 * @param options Key creation options.
 * @return The created KeyVersion object.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProvider.java,options,org.apache.hadoop.crypto.key.KeyProvider:options(org.apache.hadoop.conf.Configuration),433,435,"/**
 * Creates and returns an Options object initialized with the given configuration.
 */","* A helper function to create an options object.
   * @param conf the configuration to use
   * @return a new options object",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoInputStream.java,<init>,"org.apache.hadoop.crypto.CryptoInputStream:<init>(java.io.InputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",142,145,"/**
 * Constructs a CryptoInputStream with default buffer size.
 * @param in Input stream.
 * @param codec CryptoCodec for encryption/decryption.
 * @param key Encryption key.
 * @param iv Initialization vector.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[],long,boolean)",130,135,"/**
 * Creates a CryptoOutputStream with a default buffer size.
 * @param out Output stream.
 * @param codec CryptoCodec.
 * @param key Key.
 * @param iv IV.
 * @param streamOffset Stream offset.
 * @param closeOutputStream Whether to close the output stream.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client$ConnectionId:<init>(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)",1679,1709,"/**
 * Constructs a ConnectionId with provided address, protocol, ticket, etc.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getTimeout,org.apache.hadoop.ipc.Client:getTimeout(org.apache.hadoop.conf.Configuration),202,213,"/**
 * Gets the RPC timeout from configuration.
 * Returns -1 if no timeout is configured.
 */","* The time after which a RPC will timeout.
   * If ping is not enabled (via ipc.client.ping), then the timeout value is the 
   * same as the pingInterval.
   * If ping is enabled, then there is no timeout value.
   * 
   * @param conf Configuration
   * @return the timeout period in milliseconds. -1 if no timeout value is set
   * @deprecated use {@link #getRpcTimeout(Configuration)} instead",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,parseMetaData,org.apache.hadoop.fs.HarFileSystem$HarMetaData:parseMetaData(),1166,1236,"/**
 * Parses metadata from master and archive index files.
 * Reads and processes index entries for subsequent use.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,<init>,"org.apache.hadoop.ha.FailoverController:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.ha.HAServiceProtocol$RequestSource)",61,80,"/**
 * Constructs a FailoverController with configuration and request source.
 * @param conf Configuration object.
 * @param source Request source.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,connect,org.apache.hadoop.fs.ftp.FTPFileSystem:connect(),141,167,"/**
 * Connects to an FTP server using configuration settings.
 * @return FTPClient object, or null on failure.
 * @throws IOException if connection or login fails.
 */
","* Connect to the FTP server using configuration parameters *
   * 
   * @return An FTPClient instance
   * @throws IOException",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FSBuilderSupport.java,getPositiveLong,"org.apache.hadoop.fs.impl.FSBuilderSupport:getPositiveLong(java.lang.String,long)",61,69,"/**
 * Returns a positive long value for the given key, using defVal if negative.
 * @param key key to look up
 * @param defVal default value if key is negative
 * @return Positive long value.
 */
","* Get a long value with resilience to unparseable values.
   * Negative values are replaced with the default.
   * @param key key to log
   * @param defVal default value
   * @return long value",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize(),1282,1286,"/**
 * Returns the default block size of the file system.
 * Delegates to the underlying file system's method.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getServerDefaults,org.apache.hadoop.fs.FileSystem:getServerDefaults(),939,954,"/**
 * Creates and returns an FsServerDefaults object with default values.
 */","* Return a set of server default configuration values.
   * @return server default configuration values
   * @throws IOException IO failure
   * @deprecated use {@link #getServerDefaults(Path)} instead",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),2766,2768,"/**
 * Returns the default block size for the file system.
 */
","* Return the number of bytes that large input files should be optimally
   * be split into to minimize I/O time.  The given path will be used to
   * locate the actual filesystem.  The full path does not have to exist.
   * @param f path of file
   * @return the default block size for the path's filesystem",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize(),426,429,"/**
 * Returns the default block size of the file system.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,reportChecksumFailure,"org.apache.hadoop.fs.LocalFileSystem:reportChecksumFailure(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.fs.FSDataInputStream,long)",99,149,"/**
 * Moves a corrupted file and its checksum to a ""bad_files"" directory.
 * @param p file path, in, sums, sumsPos are streams/positions.
 * @return false always
 */
","* Moves files to a bad file directory on the same device, so that their
   * storage will not be reused.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,<init>,org.apache.hadoop.fs.DU:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),43,48,"/**
 * Constructs a DU object using provided builder parameters.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/CachingGetSpaceUsed.java,<init>,org.apache.hadoop.fs.CachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),60,66,"/**
 * Constructs CachingGetSpaceUsed with provided builder values.
 * @param builder Builder object containing configuration details.
 */
","* This is the constructor used by the builder.
   * All overriding classes should implement this.
   *
   * @param builder builder.
   * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/WindowsGetSpaceUsed.java,<init>,org.apache.hadoop.fs.WindowsGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),34,40,"/**
 * Constructs a WindowsGetSpaceUsed object.
 * @param builder Builder object containing configuration.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/nativeio/NativeIO.java,getOwner,org.apache.hadoop.io.nativeio.NativeIO:getOwner(java.io.FileDescriptor),934,954,"/**
 * Gets the owner of a file descriptor. Returns username or null.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ShellBasedIdMapping.java,<init>,org.apache.hadoop.security.ShellBasedIdMapping:<init>(org.apache.hadoop.conf.Configuration),135,137,"/**
 * Constructs a ShellBasedIdMapping with default flag.
 * @param conf Configuration object for mapping details.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,initHM,org.apache.hadoop.ha.ZKFailoverController:initHM(),323,328,"/**
 * Initializes the health monitor with configurations and callbacks.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,<init>,"org.apache.hadoop.fs.TrashPolicyDefault:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)",79,82,"/**
 * Constructs a TrashPolicyDefault with a FileSystem and Configuration.
 * @param fs The FileSystem to operate on.
 * @param conf The configuration to use.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,close,org.apache.hadoop.fs.viewfs.ViewFileSystem:close(),1986,2007,"/**
 * Closes the file system, releasing resources and closing caches.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.DataOutputBuffer),2565,2584,"/**
 * Reads the next record length and key length.
 * @param buffer DataOutputBuffer to write data to.
 * @return Key length or -1 if end of file.
 */","@deprecated Call {@link #nextRaw(DataOutputBuffer,SequenceFile.ValueBytes)}.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/retry/RetryUtils.java,getDefaultRetryPolicy,"org.apache.hadoop.io.retry.RetryUtils:getDefaultRetryPolicy(org.apache.hadoop.conf.Configuration,java.lang.String,boolean,java.lang.String,java.lang.String,java.lang.String)",59,85,"/**
 * Creates a default RetryPolicy based on configuration.
 * @param conf Configuration object.
 * @return RetryPolicy object.
 */
","* Return the default retry policy set in conf.
   * 
   * If the value retryPolicyEnabledKey is set to false in conf,
   * use TRY_ONCE_THEN_FAIL.
   * 
   * Otherwise, get the MultipleLinearRandomRetry policy specified in the conf
   * and then
   * (1) use multipleLinearRandomRetry for
   *     - remoteExceptionToRetry, or
   *     - IOException other than RemoteException, or
   *     - ServiceException; and
   * (2) use TRY_ONCE_THEN_FAIL for
   *     - non-remoteExceptionToRetry RemoteException, or
   *     - non-IOException.
   *     
   *
   * @param conf configuration.
   * @param retryPolicyEnabledKey     conf property key for enabling retry
   * @param defaultRetryPolicyEnabled default retryPolicyEnabledKey conf value 
   * @param retryPolicySpecKey        conf property key for retry policy spec
   * @param defaultRetryPolicySpec    default retryPolicySpecKey conf value
   * @param remoteExceptionToRetry    The particular RemoteException to retry
   * @return the default retry policy.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,getCodec,org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:getCodec(),273,273,"/**
* Gets the compression codec.
* @throws IOException if an I/O error occurs
*/
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createDecompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createDecompressionStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor,int)",275,277,"/**
 * Creates a decompression stream from an input stream.
 * @param downStream Input stream to decompress.
 * @param decompressor Decompressor to use.
 * @param downStreamBufferSize Buffer size for the stream.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/Compression.java,createCompressionStream,"org.apache.hadoop.io.file.tfile.Compression$Algorithm$1:createCompressionStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor,int)",279,281,"/**
 * Creates a compression stream using the given compressor.
 * @param downStream Output stream to compress to.
 * @param compressor Compressor to use.
 * @param downStreamBufferSize Buffer size for the downstream stream.
 * @throws IOException If an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,init,"org.apache.hadoop.metrics2.source.JvmMetrics$Singleton:init(java.lang.String,java.lang.String)",59,64,"/**
 * Initializes the JvmMetrics instance.
 * Creates if not already initialized.
 * @param processName Process name.
 * @param sessionId Session identifier.
 * @return JvmMetrics instance.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,<init>,"org.apache.hadoop.ipc.Client:<init>(java.lang.Class,org.apache.hadoop.conf.Configuration)",1349,1351,"/**
 * Constructs a Client with a Writable class and Configuration.
 * @param valueClass Class of the Writable object.
 * @param conf Hadoop configuration.
 */
","* Construct an IPC client with the default SocketFactory.
   * @param valueClass input valueClass.
   * @param conf input Configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,getClient,"org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,java.lang.Class)",50,68,"/**
 * Retrieves a cached Client. Creates one if not found.
 * @param conf Configuration object for timeout.
 * @param factory SocketFactory for client creation.
 * @param valueClass Writable class for client data.
 * @return Client instance.
 */
","* Construct &amp; cache an IPC client with the user-provided SocketFactory
   * if no cached client exists.
   * 
   * @param conf Configuration
   * @param factory SocketFactory for client socket
   * @param valueClass Class of the expected response
   * @return an IPC client",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java,<init>,org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>(org.apache.hadoop.conf.Configuration),73,85,"/**
 * Constructs a FsUrlStreamHandlerFactory with a Configuration.
 * Initializes FileSystem and sets up protocols.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java,createURLStreamHandler,org.apache.hadoop.fs.FsUrlStreamHandlerFactory:createURLStreamHandler(java.lang.String),87,111,"/**
 * Creates a URLStreamHandler for the given protocol.
 * @param protocol protocol string; returns handler or null.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ProviderUtils.java,excludeIncompatibleCredentialProviders,"org.apache.hadoop.security.ProviderUtils:excludeIncompatibleCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.Class)",141,199,"/**
 * Excludes incompatible credential providers from the configuration.
 * @param config Configuration object to modify.
 * @param fileSystemClass Class to check for compatibility.
 * @return Modified Configuration object.
 */
","* There are certain integrations of the credential provider API in
   * which a recursive dependency between the provider and the hadoop
   * filesystem abstraction causes a problem. These integration points
   * need to leverage this utility method to remove problematic provider
   * types from the existing provider path within the configuration.
   *
   * @param config the existing configuration with provider path
   * @param fileSystemClass the class which providers must be compatible
   * @return Configuration clone with new provider path
   * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,get,"org.apache.hadoop.fs.AbstractFileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration)",263,266,"/**
 * Retrieves a FileSystem for the given URI and configuration.
 * @param uri The URI of the filesystem.
 * @param conf The configuration for the filesystem.
 * @return An AbstractFileSystem instance.
 */
","* The main factory method for creating a file system. Get a file system for
   * the URI's scheme and authority. The scheme of the <code>uri</code>
   * determines a configuration property name,
   * <tt>fs.AbstractFileSystem.<i>scheme</i>.impl</tt> whose value names the
   * AbstractFileSystem class.
   * 
   * The entire URI and conf is passed to the AbstractFileSystem factory method.
   * 
   * @param uri for the file system to be created.
   * @param conf which is passed to the file system impl.
   * 
   * @return file system for the given URI.
   * 
   * @throws UnsupportedFileSystemException if the file system for
   *           <code>uri</code> is not supported.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/CompositeGroupsMapping.java,setConf,org.apache.hadoop.security.CompositeGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),138,145,"/**
 * Sets the configuration and loads mapping providers.
 * @param conf The Configuration object to set.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,getProtocolMetaInfoProxy,"org.apache.hadoop.ipc.RpcClientUtil:getProtocolMetaInfoProxy(java.lang.Object,org.apache.hadoop.conf.Configuration)",179,187,"/**
 * Gets the protocol meta info proxy from the given proxy.
 * @param proxy The proxy object.
 * @param conf RPC configuration.
 * @return ProtocolMetaInfoPB proxy.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,build,org.apache.hadoop.ipc.RPC$Builder:build(),975,991,"/**
 * Builds and returns a Server instance using configured parameters.
 * @throws HadoopIllegalArgumentException if required configs are missing.
 */","* @return Build the RPC Server.
     * @throws IOException on error
     * @throws HadoopIllegalArgumentException when mandatory fields are not set",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicy.java,getInstance,"org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",139,146,"/**
 * Gets the TrashPolicy instance.
 * @param conf Configuration object.
 * @param fs FileSystem object.
 * @param home Home directory Path.
 * @return TrashPolicy instance.
 */
","* Get an instance of the configured TrashPolicy based on the value
   * of the configuration parameter fs.trash.classname.
   *
   * @param conf the configuration to be used
   * @param fs the file system to be used
   * @param home the home directory
   * @return an instance of TrashPolicy
   * @deprecated Use {@link #getInstance(Configuration, FileSystem)} instead.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicy.java,getInstance,"org.apache.hadoop.fs.TrashPolicy:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",156,162,"/**
 * Retrieves the TrashPolicy instance.
 * @param conf Configuration object.
 * @param fs FileSystem object.
 * @return TrashPolicy instance.
 */
","* Get an instance of the configured TrashPolicy based on the value
   * of the configuration parameter fs.trash.classname.
   *
   * @param conf the configuration to be used
   * @param fs the file system to be used
   * @return an instance of TrashPolicy",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getMountTableConfigLoader,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getMountTableConfigLoader(org.apache.hadoop.conf.Configuration),181,203,"/**
 * Gets a MountTableConfigLoader instance using the provided configuration.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,getKlass,org.apache.hadoop.fs.GetSpaceUsed$Builder:getKlass(),74,89,"/**
 * Returns the Class for getting space used, using config if available.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getInstance,"org.apache.hadoop.net.NetworkTopology:getInstance(org.apache.hadoop.conf.Configuration,org.apache.hadoop.net.InnerNode$Factory)",77,83,"/**
 * Gets the NetworkTopology instance.
 * @param conf Configuration object.
 * @param factory InnerNode factory.
 * @return NetworkTopology instance.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DomainNameResolverFactory.java,newInstance,"org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.lang.String)",68,75,"/**
 * Creates and returns a new DomainNameResolver instance.
 * @param conf Configuration object.
 * @param configKey Key to fetch resolver class from config.
 */
","* This function gets the instance based on the config.
   *
   * @param conf Configuration
   * @param configKey config key name.
   * @return Domain name resolver.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslPropertiesResolver.java,getInstance,org.apache.hadoop.security.SaslPropertiesResolver:getInstance(org.apache.hadoop.conf.Configuration),52,58,"/**
 * Gets the SaslPropertiesResolver instance from the configuration.
 * @param conf Hadoop configuration object
 * @return SaslPropertiesResolver instance
 */
","* Returns an instance of SaslPropertiesResolver.
   * Looks up the configuration to see if there is custom class specified.
   * Constructs the instance by passing the configuration directly to the
   * constructor to achieve thread safety using final fields.
   * @param conf configuration.
   * @return SaslPropertiesResolver",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,<init>,"org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Timer)",104,153,"/**
 * Constructs a Groups object, initializing configuration and caches.
 * @param conf Hadoop configuration object.
 * @param timer Timer for cache refresh ticker.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,validateSasl,org.apache.hadoop.security.KDiag:validateSasl(java.lang.String),733,748,"/**
 * Loads and validates a SaslPropertiesResolver class.
 * @param saslPropsResolverKey Key for the resolver class.
 */
","* Try to load the SASL resolver.
   * @param saslPropsResolverKey key for the SASL resolver",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,getInstance,org.apache.hadoop.security.authorize.ProxyUsers:getInstance(org.apache.hadoop.conf.Configuration),47,53,"/**
 * Gets the impersonation provider class.
 * @param conf Hadoop configuration object.
 * @return ImpersonationProvider instance.
 */
","* Returns an instance of ImpersonationProvider.
   * Looks up the configuration to see if there is custom class specified.
   * @param conf
   * @return ImpersonationProvider",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslCtrCryptoCodec.java,setConf,org.apache.hadoop.crypto.OpensslCtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration),84,100,"/**
* Sets the configuration and initializes the random number generator.
* @param conf Hadoop configuration object.
*/
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,<init>,"org.apache.hadoop.util.ShutdownHookManager$HookEntry:<init>(java.lang.Runnable,int)",205,209,"/**
 * Creates a HookEntry with default shutdown timeout.
 * @param hook The runnable to execute.
 * @param priority Hook priority.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,shutdownExecutor,org.apache.hadoop.util.ShutdownHookManager:shutdownExecutor(org.apache.hadoop.conf.Configuration),142,162,"/**
 * Shuts down the executor, waiting for termination or forcefully.
 * @param conf Configuration object for timeout settings.
 */
","* Shutdown the executor thread itself.
   * @param conf the configuration containing the shutdown timeout setting.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getPasswordFromCredentialProviders,"org.apache.hadoop.security.LdapGroupsMapping:getPasswordFromCredentialProviders(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",893,906,"/**
 * Retrieves password from credential providers, using default if unavailable.
 * @param config Configuration object. @param alias Credential alias.
 * @param defaultPass Default password to use if not found.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getPassword,org.apache.hadoop.conf.Configuration:getPassword(java.lang.String),2418,2428,"/**
 * Retrieves the password for a given name from various providers.
 * @param name User's name, used to fetch the password.
 * @return char[] Password or null if not found.
 */
","* Get the value for a known password configuration element.
   * In order to enable the elimination of clear text passwords in config,
   * this method attempts to resolve the property name as an alias through
   * the CredentialProvider API and conditionally fallsback to config.
   * @param name property name
   * @return password
   * @throws IOException when error in fetching password",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawEncoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawEncoder(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",131,137,"/**
 * Creates a RawErasureEncoder with given config, codec, and options.
 */","* Create RS raw encoder according to configuration.
   * @param conf configuration
   * @param coderOptions coder options that's used to create the coder
   * @param codec the codec to use. If null, will use the default codec
   * @return raw encoder",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/CodecUtil.java,createRawDecoder,"org.apache.hadoop.io.erasurecode.CodecUtil:createRawDecoder(org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.io.erasurecode.ErasureCoderOptions)",146,152,"/**
 * Creates a RawErasureDecoder with given config, codec, and options.
 */","* Create RS raw decoder according to configuration.
   * @param conf configuration
   * @param coderOptions coder options that's used to create the coder
   * @param codec the codec to use. If null, will use the default codec
   * @return raw decoder",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/SshFenceByTcpPort.java,tryFence,"org.apache.hadoop.ha.SshFenceByTcpPort:tryFence(org.apache.hadoop.ha.HAServiceTarget,java.lang.String)",80,115,"/**
 * Attempts to fence a target host via SSH.
 * @param target The HAServiceTarget to fence.
 * @param argsStr Fencing arguments string.
 * @return True if fencing was successful, false otherwise.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyServers.java,isProxyServer,org.apache.hadoop.security.authorize.ProxyServers:isProxyServer(java.lang.String),47,52,"/**
 * Checks if the given address is a proxy server.
 * @param remoteAddr The address to check.
 * @return True if the address is a proxy, false otherwise.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,<init>,"org.apache.hadoop.ipc.CallQueueManager:<init>(java.lang.Class,java.lang.Class,boolean,int,java.lang.String,org.apache.hadoop.conf.Configuration)",78,96,"/**
 * Creates a CallQueueManager with specified configurations.
 * @param backingClass Queue implementation class.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/CallQueueManager.java,swapQueue,"org.apache.hadoop.ipc.CallQueueManager:swapQueue(java.lang.Class,java.lang.Class,int,java.lang.String,org.apache.hadoop.conf.Configuration)",464,495,"/**
 * Swaps the queue and scheduler with new implementations.
 * @param schedulerClass Scheduler class to use.
 * @param queueClassToUse Queue class to use.
 * @param maxSize Max size of the queue.
 */
","* Replaces active queue with the newly requested one and transfers
   * all calls to the newQ before returning.
   *
   * @param schedulerClass input schedulerClass.
   * @param queueClassToUse input queueClassToUse.
   * @param maxSize input maxSize.
   * @param ns input ns.
   * @param conf input configuration.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/metrics/RpcMetrics.java,create,"org.apache.hadoop.ipc.metrics.RpcMetrics:create(org.apache.hadoop.ipc.Server,org.apache.hadoop.conf.Configuration)",115,118,"/**
 * Creates and registers an RpcMetrics instance.
 * @param server The server for metrics.
 * @param conf Configuration for metrics.
 * @return Registered RpcMetrics instance.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,"org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,int[],boolean,org.apache.hadoop.conf.Configuration)",119,154,"/**
 * Creates a FairCallQueue with specified priority levels, capacity,
 * and configuration.
 */","* Create a FairCallQueue.
   * @param priorityLevels the total size of all multi-level queue
   *                       priority policies
   * @param capacity the total size of all sub-queues
   * @param ns the prefix to use for configuration
   * @param capacityWeights the weights array for capacity allocation
   *                        among subqueues
   * @param serverFailOverEnabled whether or not to enable callqueue overflow trigger failover
   *                              for stateless servers when RPC call queue is filled
   * @param conf the configuration to read from
   * Notes: Each sub-queue has a capacity of `capacity / numSubqueues`.
   * The first or the highest priority sub-queue has an excess capacity
   * of `capacity % numSubqueues`",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,initializeWebServer,"org.apache.hadoop.http.HttpServer2:initializeWebServer(java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration,java.lang.String[])",726,795,"/**
 * Initializes the web server with configurations and handlers.
 * @param name server name, host, conf, pathspecs
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseCostProvider,"org.apache.hadoop.ipc.DecayRpcScheduler:parseCostProvider(java.lang.String,org.apache.hadoop.conf.Configuration)",281,309,"/**
 * Retrieves a CostProvider instance from configuration.
 * Returns DefaultCostProvider if none are found.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,parseIdentityProvider,"org.apache.hadoop.ipc.DecayRpcScheduler:parseIdentityProvider(java.lang.String,org.apache.hadoop.conf.Configuration)",312,337,"/**
 * Parses IdentityProvider instances from configuration.
 * Returns the first found provider or UserIdentityProvider.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,store,"org.apache.hadoop.io.DefaultStringifier:store(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.String)",110,117,"/**
 * Stores an item in the configuration using a stringifier.
 * @param conf Configuration object.
 * @param item Item to store.
 * @param keyName Key name for the item.
 */
","* Stores the item in the configuration with the given keyName.
   * 
   * @param <K>  the class of the item
   * @param conf the configuration to store
   * @param item the object to be stored
   * @param keyName the name of the key to use
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,load,"org.apache.hadoop.io.DefaultStringifier:load(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)",130,140,"/**
 * Loads an item from configuration by key.
 * @param conf Configuration object.
 * @param keyName Key to load.
 * @param itemClass Class of the item to load.
 * @return Loaded item or null if not found.
 */
","* Restores the object from the configuration.
   * 
   * @param <K> the class of the item
   * @param conf the configuration to use
   * @param keyName the name of the key to use
   * @param itemClass the class of the item
   * @return restored object
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,storeArray,"org.apache.hadoop.io.DefaultStringifier:storeArray(org.apache.hadoop.conf.Configuration,java.lang.Object[],java.lang.String)",153,171,"/**
 * Stores an array of items in the configuration under the given key.
 * @param conf Hadoop configuration object
 * @param items Array of items to store
 * @param keyName Key to store the array under
 */
","* Stores the array of items in the configuration with the given keyName.
   * 
   * @param <K> the class of the item
   * @param conf the configuration to use 
   * @param items the objects to be stored
   * @param keyName the name of the key to use
   * @throws IndexOutOfBoundsException if the items array is empty
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/DefaultStringifier.java,loadArray,"org.apache.hadoop.io.DefaultStringifier:loadArray(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.Class)",184,203,"/**
 * Loads an array from configuration.
 * @param conf Configuration object.
 * @param keyName Key name in configuration.
 * @param itemClass Class of array elements.
 * @return Array of type K or null if not found.
 */
","* Restores the array of objects from the configuration.
   * 
   * @param <K> the class of the item
   * @param conf the configuration to use
   * @param keyName the name of the key to use
   * @param itemClass the class of the item
   * @return restored object
   * @throws IOException : forwards Exceptions from the underlying 
   * {@link Serialization} classes.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",1257,1266,"/**
 * Creates a Writer with specified configuration and parameters.
 * @param fs FileSystem, Configuration, Path, class types, etc.
 */
","* Create the named file with write-progress reporter.
     * @deprecated Use 
     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} 
     *   instead.
     * @param fs input filesystem.
     * @param conf input configuration.
     * @param name input name.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param bufferSize input bufferSize.
     * @param replication input replication.
     * @param blockSize input blockSize.
     * @param progress input progress.
     * @param metadata input metadata.
     * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ReflectionUtils.java,copy,"org.apache.hadoop.util.ReflectionUtils:copy(org.apache.hadoop.conf.Configuration,java.lang.Object,java.lang.Object)",346,361,"/**
 * Copies data from src object to dst object using serialization.
 * @param conf Configuration object.
 * @param src Source object to copy from.
 * @param dst Destination object to copy to.
 * @return The deserialized dst object.
 */
","* Make a copy of the writable object using serialization to a buffer.
   * @param src the object to copy from
   * @param dst the object to copy into, which is destroyed
   * @param <T> Generics Type.
   * @param conf configuration.
   * @return dst param (the copy)
   * @throws IOException raised on errors performing I/O.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FutureIOSupport.java,propagateOptions,"org.apache.hadoop.fs.impl.FutureIOSupport:propagateOptions(org.apache.hadoop.fs.FSBuilder,org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",140,149,"/**
 * Propagates configuration options to the builder.
 * @param builder FSBuilder instance
 * @param conf Configuration object
 */
","* Propagate options to any builder.
   * {@link FutureIO#propagateOptions(FSBuilder, Configuration, String, String)}
   * @param builder builder to modify
   * @param conf configuration to read
   * @param optionalPrefix prefix for optional settings
   * @param mandatoryPrefix prefix for mandatory settings
   * @param <T> type of result
   * @param <U> type of builder
   * @return the builder passed in.",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ReconfigurableBase.java,run,org.apache.hadoop.conf.ReconfigurableBase$ReconfigurationThread:run(),116,162,"/**
 * Executes the reconfiguration task, applying changes and logging results.
 */",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,doGetGroups,"org.apache.hadoop.security.LdapGroupsMapping:doGetGroups(java.lang.String,int)",509,557,"/**
 * Retrieves group names for a user, optionally traversing hierarchy.
 * @param user User's DN.
 * @param goUpHierarchy Hierarchy traversal level.
 * @return Set of group names.
 */
","* Perform LDAP queries to get group names of a user.
   *
   * Perform the first LDAP query to get the user object using the user's name.
   * If one-query is enabled, retrieve the group names from the user object.
   * If one-query is disabled, or if it failed, perform the second query to
   * get the groups.
   *
   * @param user user name
   * @return a list of group names for the user. If the user can not be found,
   * return an empty string array.
   * @throws NamingException if unable to get group names",,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,generateEncryptedKey,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:generateEncryptedKey(java.lang.String),285,305,"/**
 * Generates an encrypted key version for a given encryption key name.
 * @param encryptionKeyName Name of the encryption key.
 * @return EncryptedKeyVersion object.
 * @throws IOException, GeneralSecurityException on failure.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,reencryptEncryptedKeys,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:reencryptEncryptedKeys(java.util.List),356,408,"/**
 * Re-encrypts a list of encrypted key versions.
 * @param ekvs List of EncryptedKeyVersion objects to re-encrypt.
 * @throws IOException, GeneralSecurityException if an error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,decryptEncryptedKey,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),433,457,"/**
 * Decrypts an encrypted key version using the provided key.
 * @param encryptedKeyVersion Encrypted key version to decrypt
 * @return Decrypted KeyVersion object
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,org.apache.hadoop.conf.Configuration:writeXml(java.io.Writer),3593,3595,"/**
 * Writes XML data to the provided Writer.
 * @param out Writer to write XML data to.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfServlet.java,writeResponse,"org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String,java.lang.String)",95,105,"/**
 * Writes configuration to a writer in specified format.
 * @param conf Configuration object.
 * @param out Writer to write to.
 * @param format Format (JSON or XML).
 * @param propertyName Property to write.
 */
",* Guts of the servlet - extracted for easy testing.,,,True,16
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configureSources,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configureSources(),539,543,"/**
 * Configures data sources based on configuration settings.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,create,"org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,boolean,int,short,long,org.apache.hadoop.util.Progressable,org.apache.hadoop.fs.permission.FsPermission)",555,568,"/**
 * Creates a new data output stream for the given file path.
 * @param f file path, overwrite, createParent, buffer size, etc.
 * @throws IOException if file exists and overwrite is false.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createSymlink,"org.apache.hadoop.fs.RawLocalFileSystem:createSymlink(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1179,1202,"/**
 * Creates a symbolic link from the target path to the link path.
 * @param target The target path.
 * @param link The link path.
 * @param createParent Whether to create parent directories.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getFilterProperties,"org.apache.hadoop.http.HttpServer2:getFilterProperties(org.apache.hadoop.conf.Configuration,java.util.List)",872,886,"/**
 * Retrieves filter properties from configuration with given prefixes.
 * @param conf Configuration object.
 * @param prefixes List of prefixes to search for.
 * @return Properties object containing filter configurations.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilterInitializer.java,createFilterConfig,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:createFilterConfig(org.apache.hadoop.conf.Configuration),42,51,"/**
 * Creates a filter configuration map from the given configuration.
 * @param conf Configuration object to read filter properties from.
 * @return Map containing filter configuration properties.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/AuthenticationFilterInitializer.java,initFilter,"org.apache.hadoop.security.AuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",57,64,"/**
 * Initializes an authentication filter within the filter container.
 * @param container Filter container to add the filter to.
 * @param conf Configuration object containing filter settings.
 */
","* Initializes hadoop-auth AuthenticationFilter.
   * <p>
   * Propagates to hadoop-auth AuthenticationFilter configuration all Hadoop
   * configuration properties prefixed with ""hadoop.http.authentication.""
   *
   * @param container The filter container
   * @param conf Configuration for run-time parameters",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/ZKDelegationTokenSecretManager.java,<init>,org.apache.hadoop.security.token.delegation.ZKDelegationTokenSecretManager:<init>(org.apache.hadoop.conf.Configuration),159,183,"/**
 * Constructs a ZKDelegationTokenSecretManager with configuration.
 * @param conf Hadoop configuration object.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,newZooKeeper,"org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean,org.apache.zookeeper.client.ZKClientConfig)",555,576,"/**
 * Creates a new ZooKeeper instance with specified configuration.
 * @param connectString Zookeeper connect string.
 * @return ZooKeeper instance.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,createSaslClient,org.apache.hadoop.security.SaslRpcClient:createSaslClient(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto$SaslAuth),211,270,"/**
 * Creates a SaslClient based on the provided authentication type.
 * @param authType Authentication type to use for client creation.
 * @return SaslClient object or null if creation fails.
 * @throws SaslException, IOException on failure.
 */
","* Try to create a SaslClient for an authentication type.  May return
   * null if the type isn't supported or the client lacks the required
   * credentials.
   * 
   * @param authType - the requested authentication method
   * @return SaslClient for the authType or null
   * @throws SaslException - error instantiating client
   * @throws IOException - misc errors",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,renew,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:renew(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",189,208,"/**
 * Renews a delegation token using the provided key provider.
 * @param token The token to renew.
 * @param conf Configuration parameters.
 * @return New expiration timestamp.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,cancel,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$KMSTokenRenewer:cancel(org.apache.hadoop.security.token.Token,org.apache.hadoop.conf.Configuration)",210,229,"/**
 * Cancels a delegation token using the provided key provider.
 * @param token Token to cancel
 * @param conf Configuration object
 * @throws IOException if cancellation fails
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(java.net.URI,org.apache.hadoop.crypto.key.kms.KMSClientProvider[],org.apache.hadoop.conf.Configuration)",89,92,"/**
 * Constructs a LoadBalancingKMSClientProvider with a monotonic clock.
 * @param providerUri URI of the provider.
 * @param providers KMSClientProvider array.
 * @param conf Configuration object.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:<init>(org.apache.hadoop.crypto.key.kms.KMSClientProvider[],long,org.apache.hadoop.conf.Configuration)",94,98,"/**
 * Creates a LoadBalancingKMSClientProvider for testing.
 * @param providers KMSClientProvider array
 * @param seed Random seed
 * @param conf Configuration object
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getFS,org.apache.hadoop.fs.FsShell:getFS(),79,84,"/**
 * Gets the FileSystem instance, initializing if necessary.
 * @return The FileSystem instance.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,initialize,"org.apache.hadoop.fs.sftp.SFTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",493,500,"/**
 * Initializes the component with URI info and configuration.
 * @param uriInfo URI information. @param conf Configuration object.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,<init>,"org.apache.hadoop.fs.DelegateToFileSystem:<init>(java.net.URI,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,java.lang.String,boolean)",49,57,"/**
 * Constructs a DelegateToFileSystem with URI, FileSystem impl,
 * config, scheme, and authority requirement.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,initialize,"org.apache.hadoop.fs.http.AbstractHttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",48,52,"/**
 * Initializes the object with a URI and configuration.
 * @param name The URI of the object.
 * @param conf The configuration object.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,initialize,"org.apache.hadoop.fs.ftp.FTPFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",102,133,"/**
 * Initializes the FTP file system with URI and configuration.
 * Sets host, port, user, and password from URI or configuration.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,initialize,"org.apache.hadoop.fs.RawLocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",130,135,"/**
 * Initializes the input split, setting configuration and block size.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createFileSystem,"org.apache.hadoop.fs.FileSystem:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",3604,3629,"/**
 * Creates a FileSystem instance for the given URI and configuration.
 * @param uri the URI of the filesystem
 * @param conf Hadoop configuration
 * @return FileSystem instance
 * @throws IOException if filesystem creation fails
 */
","* Create and initialize a new instance of a FileSystem.
   * @param uri URI containing the FS schema and FS details
   * @param conf configuration to use to look for the FS instance declaration
   * and to pass to the {@link FileSystem#initialize(URI, Configuration)}.
   * @return the initialized filesystem.
   * @throws IOException problems loading or initializing the FileSystem",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,initialize,"org.apache.hadoop.fs.viewfs.ViewFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",310,384,"/**
 * Initializes the ViewFileSystem with URI and configuration.
 * @param theUri The URI for the ViewFileSystem.
 * @param conf Hadoop configuration object.
 * @throws IOException if initialization fails.
 */
","* Called after a new FileSystem instance is constructed.
   * @param theUri a uri whose authority section names the host, port, etc. for
   *        this FileSystem
   * @param conf the configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,createFileSystem,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:createFileSystem(java.net.URI,org.apache.hadoop.conf.Configuration)",277,291,"/**
 * Creates a FileSystem instance based on URI and configuration.
 * @param uri The URI of the file system.
 * @param conf Hadoop configuration object.
 * @return A FileSystem instance.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,initialize,"org.apache.hadoop.fs.FilterFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",92,104,"/**
 * Initializes the filesystem, handling potential missing initialization.
 * @param name URI representing the filesystem name.
 * @param conf Hadoop configuration.
 */
","Called after a new FileSystem instance is constructed.
   * @param name a uri whose authority section names the host, port, etc.
   *   for this FileSystem
   * @param conf the configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,initialize,"org.apache.hadoop.fs.LocalFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",44,53,"/**
 * Initializes the FileSystem, potentially swapping the scheme.
 * @param name URI representing the filesystem.
 * @param conf Hadoop configuration.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,checkPath,org.apache.hadoop.fs.HarFileSystem:checkPath(org.apache.hadoop.fs.Path),338,341,"/**
 * Checks the given path using the file system's checkPath method.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,makeQualified,org.apache.hadoop.fs.FileSystem:makeQualified(org.apache.hadoop.fs.Path),682,685,"/**
 * Makes a path qualified with the URI and working directory.
 * @param path The path to qualify.
 * @return The qualified path.
 */
","* Qualify a path to one which uses this FileSystem and, if relative,
   * made absolute.
   * @param path to qualify.
   * @return this path if it contains a scheme and authority and is absolute, or
   * a new path that includes a path and authority and is fully qualified
   * @see Path#makeQualified(URI, Path)
   * @throws IllegalArgumentException if the path has a schema/URI different
   * from this FileSystem.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,resolvePath,org.apache.hadoop.fs.FileSystem:resolvePath(org.apache.hadoop.fs.Path),975,978,"/**
 * Resolves a path, checking its validity and returning its Path.
 * @param p Path to resolve
 * @return Resolved Path object
 */
","* Return the fully-qualified path of path, resolving the path
   * through any symlinks or mount point.
   * @param p path to be resolved
   * @return fully qualified path
   * @throws FileNotFoundException if the path is not present
   * @throws IOException for any other error",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,checkPath,org.apache.hadoop.fs.FilterFileSystem:checkPath(org.apache.hadoop.fs.Path),146,149,"/**
 * Delegates path checking to the underlying file system.
 */
",Check that a Path belongs to this FileSystem.,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AvroFSInput.java,<init>,"org.apache.hadoop.fs.AvroFSInput:<init>(org.apache.hadoop.fs.FileContext,org.apache.hadoop.fs.Path)",55,63,"/**
 * Initializes AvroFSInput with a FileContext and Path.
 * @param fc FileContext for file operations.
 * @param p Path to the Avro file.
 */
","Construct given a {@link FileContext} and a {@link Path}.
   * @param fc filecontext.
   * @param p the path.
   * @throws IOException If an I/O error occurred.
   *",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,copy,"org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean,boolean)",2208,2248,"/**
 * Copies a file or directory from src to dst.
 * @param src Source path.
 * @param dst Destination path.
 * @param deleteSource Whether to delete src after copy.
 * @param overwrite Whether to overwrite existing dst.
 * @return True if copy was successful.
 */
","* Copy from src to dst, optionally deleting src and overwriting dst.
     * @param src src.
     * @param dst dst.
     * @param deleteSource - delete src if true
     * @param overwrite  overwrite dst if true; throw IOException if dst exists
     *         and overwrite is false.
     *
     * @return true if copy is successful
     *
     * @throws AccessControlException If access is denied
     * @throws FileAlreadyExistsException If <code>dst</code> already exists
     * @throws FileNotFoundException If <code>src</code> does not exist
     * @throws ParentNotDirectoryException If parent of <code>dst</code> is not
     *           a directory
     * @throws UnsupportedFileSystemException If file system for 
     *         <code>src</code> or <code>dst</code> is not supported
     * @throws IOException If an I/O error occurred
     * 
     * Exceptions applicable to file systems accessed over RPC:
     * @throws RpcClientException If an exception occurred in the RPC client
     * @throws RpcServerException If an exception occurred in the RPC server
     * @throws UnexpectedServerException If server implementation throws 
     *           undeclared exception to RPC server
     * 
     * RuntimeExceptions:
     * @throws InvalidPathException If path <code>dst</code> is invalid",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/sftp/SFTPFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.sftp.SFTPFileSystem:getWorkingDirectory(),641,645,"/**
 * Returns the working directory, always the home directory.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",4913,4917,"/**
 * Constructs an FSDataInputStreamBuilder.
 * @param fileSystem The FileSystem to use.
 * @param path The path to build the stream for.
 */
","* Path Constructor.
     * @param fileSystem owner
     * @param path path to open.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$FSDataInputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",4924,4928,"/**
 * Constructs an FSDataInputStreamBuilder.
 * @param fileSystem The file system.
 * @param pathHandle The path handle.
 */
","* Construct from a path handle.
     * @param fileSystem owner
     * @param pathHandle path handle of file to open.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/impl/DynamicWrappedIO.java,openFile,"org.apache.hadoop.io.wrappedio.impl.DynamicWrappedIO:openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.lang.String)",449,454,"/**
 * Opens a file using the provided file system and status.
 * @param fs FileSystem object.
 * @param status FileStatus object.
 * @param readPolicies Read policies string.
 * @return FSDataInputStream object.
 */
","* Open a file.
   * <p>
   * If the WrappedIO class is found, use it.
   * <p>
   * If not, falls back to the classic {@code fs.open(Path)} call.
   * @param fs filesystem
   * @param status file status
   * @param readPolicies read policy to use
   * @return the input stream
   * @throws IOException any IO failure.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,tryLoadFromPath,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadFromPath(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",188,216,"/**
 * Loads FsPermission from a path, falling back to backup if corrupted.
 * @param path Path to the permission file.
 * @param backupPath Path to the backup permission file.
 * @return FsPermission object.
 */
","* Try loading from the user specified path, else load from the backup
   * path in case Exception is not due to bad/wrong password.
   * @param path Actual path to load from
   * @param backupPath Backup path (_OLD)
   * @return The permissions of the loaded file
   * @throws NoSuchAlgorithmException
   * @throws CertificateException
   * @throws IOException",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,loadAndReturnPerm,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:loadAndReturnPerm(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",252,272,"/**
 * Loads FsPermission from a path, renames, and deletes a path.
 * @param pathToLoad Path to load permission from.
 * @param pathToDelete Path to delete after loading.
 * @return FsPermission object.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,resetKeyStoreState,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:resetKeyStoreState(org.apache.hadoop.fs.Path),586,598,"/**
 * Resets the Keystore state to a previous version from the given path.
 * @param path Path to the previous Keystore state.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Sorter:<init>(org.apache.hadoop.fs.FileSystem,java.lang.Class,java.lang.Class,org.apache.hadoop.conf.Configuration)",2921,2924,"/**
 * Constructs a Sorter with a default WritableComparator.
 * @param fs FileSystem object
 * @param keyClass Class of the key to sort
 * @param valClass Class of the value
 * @param conf Configuration object
 */
","* Sort and merge files containing the named classes.
     * @param fs input FileSystem.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param conf input Configuration.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/bzip2/Bzip2Factory.java,getBzip2Compressor,org.apache.hadoop.io.compress.bzip2.Bzip2Factory:getBzip2Compressor(org.apache.hadoop.conf.Configuration),98,101,"/**
 * Returns a Bzip2Compressor, or a dummy if native library is unavailable.
 * @param conf Hadoop configuration object.
 * @return Bzip2Compressor instance.
 */
","* Return the appropriate implementation of the bzip2 compressor. 
   * 
   * @param conf configuration
   * @return the appropriate implementation of the bzip2 compressor.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,initialize,"org.apache.hadoop.io.SequenceFile$Reader:initialize(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FSDataInputStream,long,long,org.apache.hadoop.conf.Configuration,boolean)",1965,1989,"/**
 * Initializes the reader with input stream, start position, and length.
 * @param filename File name, or null for unknown.
 */",Common work of the constructors.,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createOutputStream,"org.apache.hadoop.io.compress.ZStandardCodec:createOutputStream(java.io.OutputStream,org.apache.hadoop.io.compress.Compressor)",137,144,"/**
 * Creates a compression output stream using the given compressor.
 * @param out OutputStream to write to.
 * @param compressor Compressor to use for compression.
 * @return CompressionOutputStream - compressed output stream.
 */
","* Create a {@link CompressionOutputStream} that will write to the given
   * {@link OutputStream} with the given {@link Compressor}.
   *
   * @param out        the location for the final output stream
   * @param compressor compressor to use
   * @return a stream the user can write uncompressed data to have compressed
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createCompressor,org.apache.hadoop.io.compress.ZStandardCodec:createCompressor(),162,167,"/**
 * Creates a ZStandardCompressor instance with configured parameters.
 */","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createInputStream,"org.apache.hadoop.io.compress.ZStandardCodec:createInputStream(java.io.InputStream,org.apache.hadoop.io.compress.Decompressor)",194,201,"/**
 * Creates a compression input stream using the provided stream and decompressor.
 */","* Create a {@link CompressionInputStream} that will read from the given
   * {@link InputStream} with the given {@link Decompressor}.
   *
   * @param in           the stream to read compressed bytes from
   * @param decompressor decompressor to use
   * @return a stream to read uncompressed bytes from
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createDecompressor,org.apache.hadoop.io.compress.ZStandardCodec:createDecompressor(),220,224,"/**
 * Creates a ZStandardDecompressor with configured buffer size.
 * @return A new ZStandardDecompressor instance.
 */
","* Create a new {@link Decompressor} for use by this {@link CompressionCodec}.
   *
   * @return a new decompressor for use by this codec",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/ZStandardCodec.java,createDirectDecompressor,org.apache.hadoop.io.compress.ZStandardCodec:createDirectDecompressor(),236,241,"/**
 * Creates a DirectDecompressor using ZStandard decompression.
 * @return A ZStandardDirectDecompressor instance.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,createReader,"org.apache.hadoop.io.file.tfile.BCFile$Reader:createReader(org.apache.hadoop.io.file.tfile.Compression$Algorithm,org.apache.hadoop.io.file.tfile.BCFile$BlockRegion)",730,734,"/**
 * Creates a BlockReader instance using provided algorithm and region.
 * @param compressAlgo Compression algorithm
 * @param region Block region to read
 * @return BlockReader instance
 * @throws IOException if an I/O error occurs
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareMetaBlock,"org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String,org.apache.hadoop.io.file.tfile.Compression$Algorithm)",345,363,"/**
 * Creates and prepares a MetaBlockAppender.
 * @param name MetaBlock name.
 * @param compressAlgo Compression algorithm.
 * @return BlockAppender instance.
 * @throws IOException, MetaBlockAlreadyExists
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareDataBlock,org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareDataBlock(),417,436,"/**
 * Creates a DataBlockAppender. Throws exception if block is in progress.
 * @return BlockAppender object for writing data blocks.
 */
","* Create a Data Block and obtain an output stream for adding data into the
     * block. There can only be one BlockAppender stream active at any time.
     * Data Blocks may not be created after the first Meta Blocks. The caller
     * must call BlockAppender.close() to conclude the block creation.
     * 
     * @return The BlockAppender stream
     * @throws IOException",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMapping.java,<init>,org.apache.hadoop.net.ScriptBasedMapping:<init>(org.apache.hadoop.conf.Configuration),103,106,"/**
 * Constructs a ScriptBasedMapping with the given configuration.
 * @param conf The configuration object to use.
 */
","* Create an instance from the given configuration
   * @param conf configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/ScriptBasedMappingWithDependency.java,setConf,org.apache.hadoop.net.ScriptBasedMappingWithDependency:setConf(org.apache.hadoop.conf.Configuration),85,89,"/**
 * Sets the configuration for this object and its raw mapping.
 * @param conf The Configuration object to set.
 */
","* {@inheritDoc}.
   * <p>
   * This will get called in the superclass constructor, so a check is needed
   * to ensure that the raw mapping is defined before trying to relaying a null
   * configuration.
   * </p>
   * @param conf input Configuration.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,init,org.apache.hadoop.crypto.key.KeyShell:init(java.lang.String[]),80,168,"/**
 * Parses command line arguments and sets up the command execution.
 * @param args Command-line arguments passed to the tool.
 * @return 0 on success, 1 on error.
 */
","* Parse the command line arguments and initialize the data.
   * <pre>
   * % hadoop key create keyName [-size size] [-cipher algorithm]
   *    [-provider providerPath]
   * % hadoop key roll keyName [-provider providerPath]
   * % hadoop key list [-provider providerPath]
   * % hadoop key delete keyName [-provider providerPath] [-i]
   * % hadoop key invalidateCache keyName [-provider providerPath]
   * </pre>
   * @param args Command line arguments.
   * @return 0 on success, 1 on failure.
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataInputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataInputStream:<init>(org.apache.hadoop.fs.FSDataInputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",33,36,"/**
 * Creates a CryptoFSDataInputStream wrapping an FSDataInputStream.
 * @param in Input stream, codec, key, and IV for encryption.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[],long)",125,128,"/**
 * Creates a CryptoOutputStream.
 * @param out Output stream.
 * @param codec CryptoCodec.
 * @param key Encryption key.
 * @param iv Initialization vector.
 * @param streamOffset Stream offset.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,getConnectionId,"org.apache.hadoop.ipc.Client$ConnectionId:getConnectionId(java.net.InetSocketAddress,java.lang.Class,org.apache.hadoop.security.UserGroupInformation,int,org.apache.hadoop.io.retry.RetryPolicy,org.apache.hadoop.conf.Configuration)",1798,1817,"/**
 * Creates a ConnectionId for RPC connection.
 * @param addr Address of the remote server.
 * @return ConnectionId object.
 */
","* Returns a ConnectionId object. 
     * @param addr Remote address for the connection.
     * @param protocol Protocol for RPC.
     * @param ticket UGI
     * @param rpcTimeout timeout
     * @param conf Configuration object
     * @return A ConnectionId instance
     * @throws IOException",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,open,"org.apache.hadoop.fs.ftp.FTPFileSystem:open(org.apache.hadoop.fs.Path,int)",276,306,"/**
 * Opens a file for reading.
 * @param file Path to the file.
 * @param bufferSize Buffer size for I/O.
 * @return FSDataInputStream for reading the file.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,create,"org.apache.hadoop.fs.ftp.FTPFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",312,375,"/**
 * Creates a new data output stream for writing to a file.
 * @param file Path to the file.
 * @param permission File permissions.
 * @return FSDataOutputStream for the new file.
 */
","* A stream obtained via this call must be closed before using other APIs of
   * this class or else the invocation will block.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,delete,"org.apache.hadoop.fs.ftp.FTPFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",400,409,"/**
 * Deletes a file or directory from the FTP server.
 * @param file Path to the file/directory to delete.
 * @param recursive If true, deletes recursively.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,listStatus,org.apache.hadoop.fs.ftp.FTPFileSystem:listStatus(org.apache.hadoop.fs.Path),468,477,"/**
 * Lists status of files/directories under the given path.
 * @param file Path to list status for.
 * @return Array of FileStatus objects.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getFileStatus,org.apache.hadoop.fs.ftp.FTPFileSystem:getFileStatus(org.apache.hadoop.fs.Path),500,509,"/**
 * Gets the file status for the given path.
 * @param file The path to the file.
 * @return FileStatus object.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,mkdirs,"org.apache.hadoop.fs.ftp.FTPFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",574,583,"/**
 * Creates directories recursively.
 * @param file The path to create.
 * @param permission FsPermission for the directories.
 * @return True if successful, false otherwise.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,rename,"org.apache.hadoop.fs.ftp.FTPFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",631,640,"/**
 * Renames a file/directory on the FTP server.
 * @param src Source path.
 * @param dst Destination path.
 * @return True if rename was successful, false otherwise.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.ftp.FTPFileSystem:getHomeDirectory(),713,729,"/**
 * Gets the FTP server's home directory as a Path object.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getServerDefaults,org.apache.hadoop.fs.HarFileSystem:getServerDefaults(),1260,1264,"/**
* Returns the server defaults from the underlying file system.
*/",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getServerDefaults,org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults(),158,162,"/**
 * Returns the server defaults from the underlying file system implementation.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getServerDefaults,org.apache.hadoop.fs.FileSystem:getServerDefaults(org.apache.hadoop.fs.Path),963,965,"/**
 * Returns the server defaults for the given path.
 * Returns the default settings for the file system.
 */
","* Return a set of server default configuration values.
   * @param p path is used to identify an FS since an FS could have
   *          another FS that it could be delegating the call to
   * @return server default configuration values
   * @throws IOException IO failure",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getServerDefaults,org.apache.hadoop.fs.FilterFileSystem:getServerDefaults(),436,439,"/**
 * Returns the server defaults from the underlying file system.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.HarFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),1288,1292,"/**
 * Gets the default block size for a given file path.
 * @param f The path to the file.
 * @return The default block size as a long.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean)",1089,1096,"/**
 * Creates a data output stream for writing to a file.
 * @param f Path to the file
 * @param overwrite Whether to overwrite if file exists
 * @return FSDataOutputStream
 */
","* Create an FSDataOutputStream at the indicated Path.
   * @param f the file to create
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an exception will be thrown.
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)",1107,1114,"/**
 * Creates a data output stream at the specified path.
 * @param f Path to create stream at.
 * @param progress Progressable object for tracking progress.
 * @return FSDataOutputStream object.
 */
","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * Files are overwritten by default.
   * @param f the file to create
   * @param progress to report progress
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,short)",1124,1131,"/**
 * Creates a data output stream for writing to a file.
 * @param f Path to the file
 * @param replication replication factor
 * @return FSDataOutputStream
 */
","* Create an FSDataOutputStream at the indicated Path.
   * Files are overwritten by default.
   * @param f the file to create
   * @param replication the replication factor
   * @throws IOException IO failure
   * @return output stream1",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,short,org.apache.hadoop.util.Progressable)",1143,1149,"/**
 * Creates a data output stream for writing to a file.
 * @param f Path to the file.
 * @param replication Replication factor.
 * @param progress Progressable object.
 * @return FSDataOutputStream
 */
","* Create an FSDataOutputStream at the indicated Path with write-progress
   * reporting.
   * Files are overwritten by default.
   * @param f the file to create
   * @param replication the replication factor
   * @param progress to report progress
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int)",1161,1168,"/**
 * Creates a data output stream for writing to a file.
 * @param f Path to the file. Overwrite & buffer size are params.
 * @throws IOException if an I/O error occurs.
 */
","* Create an FSDataOutputStream at the indicated Path.
   * @param f the file to create
   * @param overwrite if a path with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path,boolean,int,org.apache.hadoop.util.Progressable)",1183,1191,"/**
 * Creates a data output stream for writing to a file.
 * @param f Path to the file. Overwrite, bufferSize, progress.
 */
","* Create an {@link FSDataOutputStream} at the indicated Path
   * with write-progress reporting.
   *
   * The frequency of callbacks is implementation-specific; it may be ""none"".
   * @param f the path of the file to open
   * @param overwrite if a file with this name already exists, then if true,
   *   the file will be overwritten, and if false an error will be thrown.
   * @param bufferSize the size of the buffer to be used.
   * @param progress to report progress.
   * @throws IOException IO failure
   * @return output stream.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ViewFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),976,988,"/**
 * Gets the default block size of a file.
 * @param f the Path object representing the file
 * @throws NotInMountpointException if file is not in a mountpoint
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.FilterFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),442,445,"/**
* Returns the default block size for a file.
* @param f the Path of the file
* @return the default block size as a long
*/
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FSDataOutputStreamBuilder.java,<init>,"org.apache.hadoop.fs.FSDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",130,139,"/**
 * Constructs an FSDataOutputStreamBuilder with a FileSystem and Path.
 * @param fileSystem The FileSystem to use.
 * @param p The Path to write to.
 */
","* Constructor.
   *
   * @param fileSystem file system.
   * @param p the path.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DFCachingGetSpaceUsed.java,<init>,org.apache.hadoop.fs.DFCachingGetSpaceUsed:<init>(org.apache.hadoop.fs.GetSpaceUsed$Builder),39,42,"/**
* Initializes a DFCachingGetSpaceUsed object.
* @param builder Builder object containing path & interval.
* @throws IOException if an I/O error occurs.
*/
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable),2462,2506,"/**
 * Reads the next key from the input stream.
 * @param key Writable key to populate with data.
 * @return True if a key was read, false otherwise.
 */","* @return Read the next key in the file into <code>key</code>, skipping its
     * value.True if another entry exists, and false at end of file.
     *
     * @param key key.
     * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Reader:next(java.lang.Object),2707,2752,"/**
 * Retrieves the next key-value pair.
 * @param key Key to validate, must match expected class.
 * @return The deserialized key object, or null if end reached.
 */
","* Read the next key in the file, skipping its
     * value.
     *
     * @param key input Object key.
     * @throws IOException raised on errors performing I/O.
     * @return Return null at end of file.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/source/JvmMetrics.java,initSingleton,"org.apache.hadoop.metrics2.source.JvmMetrics:initSingleton(java.lang.String,java.lang.String)",129,131,"/**
 * Initializes the JvmMetrics singleton with process and session ID.
 * @param processName Name of the process.
 * @param sessionId Session identifier.
 * @return JvmMetrics instance.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>(java.lang.Class,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",162,170,"/**
 * Constructs an Invoker with provided protocol, connection, config, etc.
 */","* This constructor takes a connectionId, instead of creating a new one.
     * @param protocol input protocol.
     * @param connId input connId.
     * @param conf input Configuration.
     * @param factory input factory.
     * @param alignmentContext Alignment context",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,getClient,org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration),77,79,"/**
 * Gets a client using the provided configuration.
 * @param conf Configuration object for client setup.
 * @return Client instance.
 */
","* Construct &amp; cache an IPC client with the default SocketFactory
   * and default valueClass if no cached client exists. 
   * 
   * @param conf Configuration
   * @return an IPC client",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ClientCache.java,getClient,"org.apache.hadoop.ipc.ClientCache:getClient(org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",89,91,"/**
 * Gets a client using the provided configuration and socket factory.
 * @param conf Configuration object
 * @param factory SocketFactory to use
 * @return Client object
 */
","* Construct &amp; cache an IPC client with the user-provided SocketFactory
   * if no cached client exists. Default response type is ObjectWritable.
   * 
   * @param conf Configuration
   * @param factory SocketFactory for client socket
   * @return an IPC client",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getClient,org.apache.hadoop.ipc.ProtobufRpcEngine2:getClient(org.apache.hadoop.conf.Configuration),370,376,"/**
 * Retrieves a client instance using the provided configuration.
 * @param conf Configuration object for client setup.
 * @return Client instance.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:<init>(java.lang.Class,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",170,178,"/**
 * Constructs an Invoker with given protocol, connection ID, config, etc.
 */","* This constructor takes a connectionId, instead of creating a new one.
     *
     * @param protocol input protocol.
     * @param connId input connId.
     * @param conf input Configuration.
     * @param factory input factory.
     * @param alignmentContext Alignment context",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getClient,org.apache.hadoop.ipc.ProtobufRpcEngine:getClient(org.apache.hadoop.conf.Configuration),360,366,"/**
 * Retrieves a client instance using the provided configuration.
 * @param conf Configuration object for client setup.
 * @return Client instance.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlStreamHandlerFactory.java,<init>,org.apache.hadoop.fs.FsUrlStreamHandlerFactory:<init>(),69,71,"/**
 * Default constructor, uses default Configuration.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RpcClientUtil.java,isMethodSupported,"org.apache.hadoop.ipc.RpcClientUtil:isMethodSupported(java.lang.Object,java.lang.Class,org.apache.hadoop.ipc.RPC$RpcKind,long,java.lang.String)",108,147,"/**
 * Checks if a method is supported by the RPC protocol version.
 * @param rpcProxy RPC proxy object.
 * @param protocol Protocol class.
 * @param rpcKind RPC kind.
 * @param version Version number.
 * @param methodName Method name to check.
 * @return True if method is supported, false otherwise.
 */
","* Returns whether the given method is supported or not.
   * The protocol signatures are fetched and cached. The connection id for the
   * proxy provided is re-used.
   * @param rpcProxy Proxy which provides an existing connection id.
   * @param protocol Protocol for which the method check is required.
   * @param rpcKind The RpcKind for which the method check is required.
   * @param version The version at the client.
   * @param methodName Name of the method.
   * @return true if the method is supported, false otherwise.
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFCRpcServer.java,<init>,"org.apache.hadoop.ha.ZKFCRpcServer:<init>(org.apache.hadoop.conf.Configuration,java.net.InetSocketAddress,org.apache.hadoop.ha.ZKFailoverController,org.apache.hadoop.security.authorize.PolicyProvider)",47,76,"/**
 * Constructs a ZKFCRpcServer with given config, address, failover controller, and policy.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,<init>,"org.apache.hadoop.fs.Trash:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration)",60,63,"/**
 * Constructs a Trash object with a FileSystem and Configuration.
 * @param fs The FileSystem to operate on.
 * @param conf The configuration to use.
 */
","* Construct a trash can accessor for the FileSystem provided.
   * @param fs the FileSystem
   * @param conf a Configuration
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/GetSpaceUsed.java,build,org.apache.hadoop.fs.GetSpaceUsed$Builder:build(),144,176,"/**
 * Builds and returns a GetSpaceUsed instance, falling back to a default.
 * @return GetSpaceUsed object, or a default implementation if creation fails.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/NetworkTopology.java,getInstance,org.apache.hadoop.net.NetworkTopology:getInstance(org.apache.hadoop.conf.Configuration),73,75,"/**
* Returns the NetworkTopology instance for the given configuration.
* @param conf Configuration object used to initialize the topology.
* @return The NetworkTopology instance.
*/
","* Get an instance of NetworkTopology based on the value of the configuration
   * parameter net.topology.impl.
   * 
   * @param conf the configuration to be used
   * @return an instance of NetworkTopology",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DomainNameResolverFactory.java,newInstance,"org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",55,59,"/**
 * Creates a DomainNameResolver with a combined configuration key.
 * @param conf Configuration object.
 * @param host Hostname.
 * @param configKey Base configuration key.
 * @return New DomainNameResolver instance.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setConfigurationInternal,org.apache.hadoop.security.SecurityUtil:setConfigurationInternal(org.apache.hadoop.conf.Configuration),105,125,"/**
 * Configures internal settings based on the provided configuration.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,<init>,"org.apache.hadoop.security.SaslRpcClient:<init>(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",118,125,"/**
 * Constructs a SaslRpcClient with provided user info, protocol, address, and config.
 */","* Create a SaslRpcClient that can be used by a RPC client to negotiate
   * SASL authentication with a RPC server
   * @param ugi - connecting user
   * @param protocol - RPC protocol
   * @param serverAddr - InetSocketAddress of remote server
   * @param conf - Configuration",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,<init>,org.apache.hadoop.security.Groups:<init>(org.apache.hadoop.conf.Configuration),100,102,"/**
 * Constructs a Groups object with a default Timer.
 * @param conf Configuration object for group management.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,refreshSuperUserGroupsConfiguration,"org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration,java.lang.String)",70,80,"/**
 * Refreshes superuser group configuration using provided prefix.
 * @param conf Configuration object.
 * @param proxyUserPrefix Prefix for proxy users.
 */
","* Refreshes configuration using the specified Proxy user prefix for
   * properties.
   *
   * @param conf configuration
   * @param proxyUserPrefix proxy user configuration prefix",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/OpensslSm4CtrCryptoCodec.java,setConf,org.apache.hadoop.crypto.OpensslSm4CtrCryptoCodec:setConf(org.apache.hadoop.conf.Configuration),55,59,"/**
 * Sets the configuration and engine ID from it.
 * @param conf The Hadoop configuration object.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ShutdownHookManager.java,addShutdownHook,"org.apache.hadoop.util.ShutdownHookManager:addShutdownHook(java.lang.Runnable,int)",294,305,"/**
 * Adds a shutdown hook with a specified priority.
 * @param shutdownHook Runnable to execute during shutdown.
 * @param priority Hook execution priority.
 */
","* Adds a shutdownHook with a priority, the higher the priority
   * the earlier will run. ShutdownHooks with same priority run
   * in a non-deterministic order.
   *
   * @param shutdownHook shutdownHook <code>Runnable</code>
   * @param priority priority of the shutdownHook.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,getPasswordString,"org.apache.hadoop.http.HttpServer2$Builder:getPasswordString(org.apache.hadoop.conf.Configuration,java.lang.String)",443,450,"/**
 * Retrieves the password string from the configuration.
 * @param conf Configuration object.
 * @param name Password name.
 * @return Password string or null if not found.
 */
","* A wrapper of {@link Configuration#getPassword(String)}. It returns
     * <code>String</code> instead of <code>char[]</code>.
     *
     * @param conf the configuration
     * @param name the property name
     * @return the password string or null",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getPassword,"org.apache.hadoop.security.LdapGroupsMapping:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",914,927,"/**
 * Gets password from configuration, using default if unavailable.
 * @param conf Configuration object.
 * @param alias Password alias.
 * @param defaultPass Default password to use.
 * @return Password string.
 */
","* Passwords should not be stored in configuration. Use
   * {@link #getPasswordFromCredentialProviders(
   *            Configuration, String, String)}
   * to avoid reading passwords from a configuration file.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,getPassword,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:getPassword(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",302,315,"/**
 * Retrieves password for given alias from configuration,
 * or returns default password if not found/error.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,getZKAuthInfos,"org.apache.hadoop.security.SecurityUtil:getZKAuthInfos(org.apache.hadoop.conf.Configuration,java.lang.String)",775,791,"/**
 * Retrieves ZK authentication information from configuration.
 * @param conf Configuration object.
 * @param configKey Key for the authentication configuration.
 * @return List of ZKAuthInfo objects or empty list if not found.
 */
","* Utility method to fetch ZK auth info from the configuration.
   *
   * @param conf configuration.
   * @param configKey config key.
   * @throws java.io.IOException if the Zookeeper ACLs configuration file
   * cannot be read
   * @throws ZKUtil.BadAuthFormatException if the auth format is invalid
   * @return ZKAuthInfo List.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,checkCreateRSRawEncoder,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:checkCreateRSRawEncoder(),52,59,"/**
 * Gets the RawErasureEncoder, creating it if it's null.
 * Uses CodecUtil to create the encoder based on configuration.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,checkCreateRSRawEncoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:checkCreateRSRawEncoder(),61,67,"/**
 * Gets the RawErasureEncoder, creating it if it's null.
 * @return RawErasureEncoder instance.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,checkCreateXorRawEncoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:checkCreateXorRawEncoder(),69,76,"/**
 * Gets the RawErasureEncoder, creating it if it's null.
 * @return RawErasureEncoder instance.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,checkCreateXorRawEncoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:checkCreateXorRawEncoder(),75,81,"/**
 * Lazily initializes and returns the RawErasureEncoder.
 * Returns the encoder or creates it if it's null.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.XORErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),40,50,"/**
 * Prepares an ErasureCodingStep using the provided block group.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/XORErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.XORErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),40,51,"/**
 * Prepares a decoding step for erasure coding.
 * @param blockGroup ECBlockGroup for decoding.
 * @return ErasureDecodingStep object.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,checkCreateRSRawDecoder,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:checkCreateRSRawDecoder(),52,58,"/**
 * Gets the RawErasureDecoder instance, creating it if null.
 * @return The RawErasureDecoder instance.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,checkCreateRSRawDecoder,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:checkCreateRSRawDecoder(),67,73,"/**
 * Gets the RawErasureDecoder instance, creating it if null.
 * @return RawErasureDecoder instance.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,refreshCallQueue,org.apache.hadoop.ipc.Server:refreshCallQueue(org.apache.hadoop.conf.Configuration),903,915,"/**
 * Refreshes the call queue with new size and settings from config.
 * @param conf Configuration object containing queue settings.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,"org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,org.apache.hadoop.conf.Configuration)",88,94,"/**
 * Constructs a FairCallQueue with default capacity weights.
 * @param priorityLevels Number of priority levels.
 * @param capacity Queue capacity.
 * @param ns Namespace.
 * @param conf Configuration object.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/FairCallQueue.java,<init>,"org.apache.hadoop.ipc.FairCallQueue:<init>(int,int,java.lang.String,boolean,org.apache.hadoop.conf.Configuration)",96,102,"/**
 * Constructs a FairCallQueue with default capacity weights.
 * @param priorityLevels Number of priority levels.
 * @param capacity Queue capacity.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/DecayRpcScheduler.java,<init>,"org.apache.hadoop.ipc.DecayRpcScheduler:<init>(int,java.lang.String,org.apache.hadoop.conf.Configuration)",236,279,"/**
 * Constructs a DecayRpcScheduler with specified levels, namespace, and config.
 */","* Create a decay scheduler.
   * @param numLevels number of priority levels
   * @param ns config prefix, so that we can configure multiple schedulers
   *           in a single instance.
   * @param conf configuration to use.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/WritableUtils.java,clone,"org.apache.hadoop.io.WritableUtils:clone(org.apache.hadoop.io.Writable,org.apache.hadoop.conf.Configuration)",218,227,"/**
 * Creates a clone of an object.
 * @param orig The object to clone.
 * @param conf Configuration for the clone.
 * @return A clone of the original object.
 */
","* Make a copy of a writable object using serialization to a buffer.
   *
   * @param <T> Generics Type T.
   * @param orig The object to copy
   * @param conf input Configuration.
   * @return The copied object",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.LdapGroupsMapping:getGroupsSet(java.lang.String),726,760,"/**
 * Retrieves a set of groups for a user, retrying with failover.
 * @param user The username to fetch groups for.
 * @return A set of group names, or an empty set on failure.
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyProviderCryptoExtension.java,reencryptEncryptedKey,org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$DefaultCryptoExtension:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),327,354,"/**
 * Re-encrypts an encrypted key version to a newer key version.
 */",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,writeXml,org.apache.hadoop.conf.Configuration:writeXml(java.io.OutputStream),3589,3591,"/**
* Writes XML data to an OutputStream, using UTF-8 encoding.
* @param out the output stream to write to
* @throws IOException if an I/O error occurs
*/
","* Write out the non-default properties in this configuration to the given
   * {@link OutputStream} using UTF-8 encoding.
   *
   * @param out the output stream to write to.
   * @throws IOException raised on errors performing I/O.",,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfServlet.java,writeResponse,"org.apache.hadoop.conf.ConfServlet:writeResponse(org.apache.hadoop.conf.Configuration,java.io.Writer,java.lang.String)",107,110,"/**
 * Writes a formatted response to the Writer.
 * @param conf Configuration object
 * @param out Writer to write to
 * @param format Response format
 * @throws IOException, BadFormatException
 */
",,,,True,17
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,configure,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:configure(java.lang.String),481,486,"/**
 * Configures metrics system with a given prefix.
 * @param prefix Prefix for metrics configuration.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,create,"org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,boolean,int,short,long,org.apache.hadoop.util.Progressable)",547,553,"/**
 * Creates a data output stream. Overwrites if true.
 * @param f Path to create stream on.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,create,"org.apache.hadoop.fs.RawLocalFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",592,600,"/**
 * Creates a data output stream for writing to a file.
 * @param f path to the file; permission, overwrite, etc.
 * @return FSDataOutputStream object
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.RawLocalFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",602,610,"/**
 * Creates a non-recursive data output stream for a given path.
 * @param f Path to create stream for, permission, overwrite flag.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,constructSecretProvider,"org.apache.hadoop.http.HttpServer2:constructSecretProvider(org.apache.hadoop.http.HttpServer2$Builder,javax.servlet.ServletContext)",862,870,"/**
 * Creates a SignerSecretProvider based on builder config.
 * @param b Builder object containing configuration.
 * @param ctx ServletContext for secret provider construction.
 * @return SignerSecretProvider instance.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilterInitializer.java,initFilter,"org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilterInitializer:initFilter(org.apache.hadoop.http.FilterContainer,org.apache.hadoop.conf.Configuration)",53,58,"/**
 * Initializes the ProxyUserAuthenticationFilter with config.
 * @param container Filter container to add the filter to.
 * @param conf Configuration object for filter settings.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager$ZKSecretManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",98,101,"/**
 * Constructs a ZKSecretManager with a configuration and token kind.
 * @param conf Configuration object.
 * @param tokenKind Token type (e.g., ""password"").
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,newZooKeeper,"org.apache.hadoop.util.curator.ZKCuratorManager$HadoopZookeeperFactory:newZooKeeper(java.lang.String,int,org.apache.zookeeper.Watcher,boolean)",547,553,"/**
 * Creates a ZooKeeper instance. Delegates to overloaded method.
 * @param connectString ZooKeeper connection string.
 * @param sessionTimeout Session timeout in milliseconds.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,selectSaslClient,org.apache.hadoop.security.SaslRpcClient:selectSaslClient(java.util.List),154,187,"/**
 * Selects a suitable Sasl authentication type from a list.
 * @param authTypes List of SaslAuth objects to consider.
 * @return Selected SaslAuth type, or null if none are suitable.
 */
","* Instantiate a sasl client for the first supported auth type in the
   * given list.  The auth type must be defined, enabled, and the user
   * must possess the required credentials, else the next auth is tried.
   * 
   * @param authTypes to attempt in the given order
   * @return SaslAuth of instantiated client
   * @throws AccessControlException - client doesn't support any of the auths
   * @throws IOException - misc errors",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FtpFs.java,<init>,"org.apache.hadoop.fs.ftp.FtpFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",50,53,"/**
 * Constructs an FTPFileSystem for accessing FTP resources.
 * @param theUri The URI of the FTP resource.
 * @param conf Configuration settings.
 */
","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   * 
   * @param theUri which must be that of localFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFs.java,<init>,"org.apache.hadoop.fs.HarFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",28,31,"/**
 * Constructs a HarFileSystem for reading/writing data from URI.
 * @param theUri The URI to access.
 * @param conf Hadoop configuration.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,<init>,"org.apache.hadoop.fs.local.RawLocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",55,59,"/**
 * Constructs a RawLocalFs instance for accessing local filesystems.
 * @param theUri The URI of the filesystem.
 * @param conf Hadoop configuration.
 */
","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   * 
   * @param theUri which must be that of localFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,initialize,"org.apache.hadoop.fs.http.HttpsFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",48,52,"/**
 * Initializes the object with a URI and configuration.
 * @param name The URI of the object.
 * @param conf The configuration object.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,initialize,"org.apache.hadoop.fs.http.HttpFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",48,52,"/**
 * Initializes the object with a URI and configuration.
 * @param name URI representing the object's location.
 * @param conf Hadoop configuration object.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,initialize,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",150,179,"/**
 * Initializes the ViewFileSystemOverloadScheme with URI and config.
 * Sets default boolean configs and loads mount table config.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,initialize,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",135,140,"/**
 * Initializes the component with a URI and configuration.
 * @param name URI identifying the component.
 * @param conf Hadoop configuration object.
 */
","* Called after a new FileSystem instance is constructed.
   * @param name a uri whose authority section names the host, port, etc.
   *   for this FileSystem
   * @param conf the configuration",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,checkDependencies,"org.apache.hadoop.fs.FileUtil:checkDependencies(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",336,353,"/**
 * Checks if copying a path would result in copying to itself.
 * @param srcFS Source file system.
 * @param src Source path.
 * @param dstFS Destination file system.
 * @param dst Destination path.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/MultipartUploaderBuilderImpl.java,<init>,"org.apache.hadoop.fs.impl.MultipartUploaderBuilderImpl:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",95,104,"/**
 * Constructs a MultipartUploaderBuilderImpl.
 * @param fileSystem The file system to use.
 * @param p The path to upload.
 */
","* Constructor.
   *
   * @param fileSystem fileSystem.
   * @param p path.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.fs.FileStatus)",156,166,"/**
 * Initializes PathData with file system, path string, and status.
 * @param fs FileSystem object
 * @param pathString Path string
 * @param stat FileStatus object
 * @throws IOException if an I/O error occurs
 */
","* Creates an object to wrap the given parameters as fields.  The string
   * used to create the path will be recorded since the Path object does not
   * return exactly the same string used to initialize it.
   * @param fs the FileSystem
   * @param pathString a String of the path
   * @param stat the FileStatus (may be null if the path doesn't exist)",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,hasPathCapability,"org.apache.hadoop.fs.FileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",3487,3501,"/**
 * Checks if the path has the specified capability.
 * @param path The path to check.
 * @param capability The capability to check for.
 * @return True if the path has the capability, false otherwise.
 */
","* The base FileSystem implementation generally has no knowledge
   * of the capabilities of actual implementations.
   * Unless it has a way to explicitly determine the capabilities,
   * this method returns false.
   * {@inheritDoc}",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.FileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),4973,4978,"/**
 * Gets the enclosing root Path.
 * @param path The Path to resolve.
 * @return The enclosing root Path.
 */
","* Return path of the enclosing root for a given path.
   * The enclosing root path is a common ancestor that should be used for temp and staging dirs
   * as well as within encryption zones and other restricted directories.
   *
   * Call makeQualified on the param path to ensure its part of the correct filesystem.
   *
   * @param path file path to find the enclosing root path for
   * @return a path to the enclosing root
   * @throws IOException early checks like failure to resolve path cause IO failures",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,makeQualified,org.apache.hadoop.fs.FilterFileSystem:makeQualified(org.apache.hadoop.fs.Path),124,139,"/**
 * Makes a path qualified, potentially swapping the scheme.
 * @param path The path to qualify.
 * @return The qualified Path object.
 */
",Make sure that a path specifies a FileSystem.,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,resolvePath,org.apache.hadoop.fs.HarFileSystem:resolvePath(org.apache.hadoop.fs.Path),343,346,"/**
 * Resolves a path using the filesystem.
 * @param p The path to resolve.
 * @return The resolved path.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/BaseExpression.java,getFileStatus,"org.apache.hadoop.fs.shell.find.BaseExpression:getFileStatus(org.apache.hadoop.fs.shell.PathData,int)",279,290,"/**
 * Gets the FileStatus, following symlinks if configured.
 * @param item PathData containing the file status.
 * @param depth Symlink following depth.
 * @return FileStatus object.
 */
","* Returns the {@link FileStatus} from the {@link PathData} item. If the
   * current options require links to be followed then the returned file status
   * is that of the linked file.
   *
   * @param item
   *          PathData
   * @param depth
   *          current depth in the process directories
   * @return FileStatus
   * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,resolvePath,org.apache.hadoop.fs.viewfs.ViewFileSystem:resolvePath(org.apache.hadoop.fs.Path),412,420,"/**
 * Resolves a path using the file system state.
 * @param f The path to resolve.
 * @return Resolved path or null if not found.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,resolvePath,org.apache.hadoop.fs.FilterFileSystem:resolvePath(org.apache.hadoop.fs.Path),157,160,"/**
 * Resolves a path using the filesystem.
 * @param p The path to resolve.
 * @return The resolved path.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,fullPath,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:fullPath(org.apache.hadoop.fs.Path),91,97,"/**
 * Returns the full path by combining root and given path.
 * @param path The path to resolve.
 * @return The full path as a Path object.
 */
","* @param path
   * @return  full path including the chroot",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,copy,"org.apache.hadoop.fs.FileContext$Util:copy(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2173,2178,"/**
 * Copies a file from the source path to the destination path.
 * @param src Source file path
 * @param dst Destination file path
 * @return True if copy successful, false otherwise.
 */
","* Copy file from src to dest. See
     * {@link #copy(Path, Path, boolean, boolean)}
     *
     * @param src src.
     * @param dst dst.
     * @throws AccessControlException If access is denied.
     * @throws FileAlreadyExistsException If file <code>src</code> already exists.
     * @throws FileNotFoundException if next file does not exist any more.
     * @throws ParentNotDirectoryException If parent of <code>src</code> is not a
     * directory.
     * @throws UnsupportedFileSystemException If file system for
     * <code>src/dst</code> is not supported.
     * @throws IOException If an I/O error occurred.
     * @return if success copy true, not false.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createDataInputStreamBuilder,"org.apache.hadoop.fs.FileSystem:createDataInputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",4877,4883,"/**
 * Creates an FSDataInputStreamBuilder for the given file system and path.
 * @param fileSystem The file system.
 * @param path The path.
 */
","* Create instance of the standard {@link FSDataInputStreamBuilder} for the
   * given filesystem and path.
   * @param fileSystem owner
   * @param path path to read
   * @return a builder.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createDataInputStreamBuilder,"org.apache.hadoop.fs.FileSystem:createDataInputStreamBuilder(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.PathHandle)",4892,4898,"/**
 * Creates an FSDataInputStreamBuilder for a given filesystem and path.
 * @param fileSystem The filesystem.
 * @param pathHandle The path to the file.
 */
","* Create instance of the standard {@link FSDataInputStreamBuilder} for the
   * given filesystem and path handle.
   * @param fileSystem owner
   * @param pathHandle path handle of file to open.
   * @return a builder.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,tryLoadIncompleteFlush,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:tryLoadIncompleteFlush(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",229,250,"/**
 * Loads FsPermission from either the new or old path.
 * Creates a new keystore if not found. Returns FsPermission.
 */","* The KeyStore might have gone down during a flush, In which case either the
   * _NEW or _OLD files might exists. This method tries to load the KeyStore
   * from one of these intermediate files.
   * @param oldPath the _OLD file created during flush
   * @param newPath the _NEW file created during flush
   * @return The permissions of the loaded file
   * @throws IOException
   * @throws NoSuchAlgorithmException
   * @throws CertificateException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/compress/BZip2Codec.java,createCompressor,org.apache.hadoop.io.compress.BZip2Codec:createCompressor(),147,150,"/**
 * Creates a Bzip2 compressor using the provided configuration.
 */","* Create a new {@link Compressor} for use by this {@link CompressionCodec}.
   *
   * @return a new compressor for use by this codec",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getMetaBlock,org.apache.hadoop.io.file.tfile.BCFile$Reader:getMetaBlock(java.lang.String),701,710,"/**
 * Retrieves a BlockReader for a meta block by name.
 * @param name Meta block name.
 * @return BlockReader for the specified meta block.
 * @throws MetaBlockDoesNotExist if the block doesn't exist.
 */
","* Stream access to a Meta Block.
     * 
     * @param name
     *          meta block name
     * @return BlockReader input stream for reading the meta block.
     * @throws IOException
     * @throws MetaBlockDoesNotExist
     *           The Meta Block with the given name does not exist.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,getDataBlock,org.apache.hadoop.io.file.tfile.BCFile$Reader:getDataBlock(int),720,728,"/**
 * Retrieves a BlockReader for the specified block index.
 * @param blockIndex Index of the block to retrieve.
 * @throws IndexOutOfBoundsException if index is out of bounds.
 */
","* Stream access to a Data Block.
     * 
     * @param blockIndex
     *          0-based data block index.
     * @return BlockReader input stream for reading the data block.
     * @throws IOException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,close,org.apache.hadoop.io.file.tfile.BCFile$Writer:close(),303,339,"/**
 * Closes the data index, finalizing metadata and closing appenders.
 */","* Close the BCFile Writer. Attempting to use the Writer after calling
     * <code>close</code> is not allowed and may lead to undetermined results.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareMetaBlock,"org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)",381,385,"/**
 * Prepares a meta block with the given name and compression.
 * @param name Meta block name.
 * @param compressionName Compression algorithm name.
 * @return The prepared MetaBlock.
 * @throws IOException, MetaBlockAlreadyExists
 */
","* Create a Meta Block and obtain an output stream for adding data into the
     * block. There can only be one BlockAppender stream active at any time.
     * Regular Blocks may not be created after the first Meta Blocks. The caller
     * must call BlockAppender.close() to conclude the block creation.
     * 
     * @param name
     *          The name of the Meta Block. The name must not conflict with
     *          existing Meta Blocks.
     * @param compressionName
     *          The name of the compression algorithm to be used.
     * @return The BlockAppender stream
     * @throws IOException
     * @throws MetaBlockAlreadyExists
     *           If the meta block with the name already exists.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,prepareMetaBlock,org.apache.hadoop.io.file.tfile.BCFile$Writer:prepareMetaBlock(java.lang.String),403,406,"/**
 * Prepares a meta block with default compression.
 * @param name Meta block name.
 * @return The prepared MetaBlockAppender.
 * @throws IOException, MetaBlockAlreadyExists
 */
","* Create a Meta Block and obtain an output stream for adding data into the
     * block. The Meta Block will be compressed with the same compression
     * algorithm as data blocks. There can only be one BlockAppender stream
     * active at any time. Regular Blocks may not be created after the first
     * Meta Blocks. The caller must call BlockAppender.close() to conclude the
     * block creation.
     * 
     * @param name
     *          The name of the Meta Block. The name must not conflict with
     *          existing Meta Blocks.
     * @return The BlockAppender stream
     * @throws MetaBlockAlreadyExists
     *           If the meta block with the name already exists.
     * @throws IOException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,initDataBlock,org.apache.hadoop.io.file.tfile.TFile$Writer:initDataBlock(),639,644,"/**
 * Initializes the data block appender for processing.
 */","* Check if we need to start a new data block.
     * 
     * @throws IOException",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/crypto/CryptoFSDataOutputStream.java,<init>,"org.apache.hadoop.fs.crypto.CryptoFSDataOutputStream:<init>(org.apache.hadoop.fs.FSDataOutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",42,47,"/**
 * Creates a CryptoFSDataOutputStream.
 * @param out FSDataOutputStream, codec, key, iv for encryption.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/CryptoOutputStream.java,<init>,"org.apache.hadoop.crypto.CryptoOutputStream:<init>(java.io.OutputStream,org.apache.hadoop.crypto.CryptoCodec,byte[],byte[])",120,123,"/**
 * Creates a CryptoOutputStream with default update mode.
 * @param out Output stream.
 * @param codec Crypto codec.
 * @param key Encryption key.
 * @param iv Initialization vector.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ftp/FTPFileSystem.java,getWorkingDirectory,org.apache.hadoop.fs.ftp.FTPFileSystem:getWorkingDirectory(),707,711,"/**
 * Returns the working directory, always the home directory.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,getServerDefaults,org.apache.hadoop.fs.HarFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),1266,1269,"/**
 * Retrieves server defaults for the given path.
 * @param f Path to retrieve defaults for.
 * @return FsServerDefaults object.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getServerDefaults,org.apache.hadoop.fs.DelegateToFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),164,167,"/**
 * Gets server defaults for a given path.
 * @param f Path to retrieve defaults for.
 * @return FsServerDefaults object.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ViewFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),1004,1013,"/**
 * Retrieves server defaults for a path.
 * @param f the path to resolve
 * @throws IOException if an error occurs during resolution
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getServerDefaults,org.apache.hadoop.fs.FilterFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),452,455,"/**
 * Retrieves server defaults for a given path.
 * @param f Path to retrieve defaults for.
 * @return FsServerDefaults object.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processArguments,org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:processArguments(java.util.LinkedList),392,424,"/**
 * Processes input paths, copying data to the destination.
 * @param args LinkedList of PathData objects representing inputs
 * @throws IOException if an I/O error occurs
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.Path),1077,1079,"/**
 * Creates a data output stream for the given path.
 * @param f the path for the new stream
 * @return FSDataOutputStream for writing data
 */
","* Create an FSDataOutputStream at the indicated Path.
   * Files are overwritten by default.
   * @param f the file to create
   * @throws IOException IO failure
   * @return output stream.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,close,org.apache.hadoop.io.BloomMapFile$Writer:close(),192,204,"/**
 * Closes the resource, writing the bloom filter to a file.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,createLogFile,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:createLogFile(org.apache.hadoop.fs.Path),688,716,"/**
 * Creates a log file, retrying with a suffix if the file exists.
 * @param initial Initial path for the log file.
 * @throws IOException if file creation fails.
 */
","* Create a new log file and return the {@link FSDataOutputStream}. If a
   * file with the specified path already exists, add a suffix, starting with 1
   * and try again. Keep incrementing the suffix until a nonexistent target
   * path is found.
   *
   * Once the file is open, update {@link #currentFSOutStream},
   * {@link #currentOutStream}, and {@#link #currentFilePath} are set
   * appropriately.
   *
   * @param initial the target path
   * @throws IOException thrown if the call to see if the exists fails",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,createOrAppendLogFile,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:createOrAppendLogFile(org.apache.hadoop.fs.Path),789,820,"/**
 * Creates or appends to a log file.
 * @param targetFile Path to the log file.
 * @throws IOException if file creation/appending fails.
 */
","* Create a new log file and return the {@link FSDataOutputStream}. If a
   * file with the specified path already exists, open the file for append
   * instead.
   *
   * Once the file is open, update {@link #currentFSOutStream},
   * {@link #currentOutStream}, and {@#link #currentFilePath}.
   *
   * @param initial the target path
   * @throws IOException thrown if the call to see the append operation fails.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,save,"org.apache.hadoop.util.JsonSerialization:save(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Object,boolean)",293,297,"/**
 * Saves an instance as JSON to a file system path.
 * @param fs FileSystem, Path, instance, overwrite flag
 */
","* Save to a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @param overwrite should any existing file be overwritten
   * @param instance instance
   * @throws IOException IO exception.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",1231,1238,"/**
 * Creates a Writer with specified configuration and metadata.
 * @param fs FileSystem, Configuration, Path, classes, metadata
 */
","* Create the named file with write-progress reporter.
     * @deprecated Use 
     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} 
     *   instead.
     * @param fs input filesystem.
     * @param conf input configuration.
     * @param name input name.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @param progress input progress.
     * @param metadata input metadata.
     * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,createNewFile,org.apache.hadoop.fs.FileSystem:createNewFile(org.apache.hadoop.fs.Path),1495,1503,"/**
 * Creates a new file at the given path. Returns true on success.
 * @param f Path to create.
 * @throws IOException if an I/O error occurs.
 */
","* Creates the given Path as a brand-new zero-length file.  If
   * create fails, or if it already existed, return false.
   * <i>Important: the default implementation is not atomic</i>
   * @param f path to use for create
   * @throws IOException IO failure
   * @return if create new file success true,not false.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$FileSystemDataOutputStreamBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",4690,4692,"/**
 * Constructs a FileSystemDataOutputStreamBuilder.
 * @param fileSystem The file system.
 * @param p The path.
 */
","* Constructor.
     * @param fileSystem owner
     * @param p path to create",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,"org.apache.hadoop.io.SequenceFile$Reader:next(org.apache.hadoop.io.Writable,org.apache.hadoop.io.Writable)",2518,2530,"/**
 * Advances to the next key-value pair.
 * @param key Writable key
 * @param val Writable value
 * @return True if a next pair exists, false otherwise.
 */
","* Read the next key/value pair in the file into <code>key</code> and
     * <code>val</code>.
     * @return Returns true if such a pair exists and false when at
     * end of file.
     *
     * @param key input key.
     * @param val input val.
     * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,read,org.apache.hadoop.fs.shell.Display$TextRecordInputStream:read(),236,257,"/**
 * Reads a value from the input buffer, handling refills.
 * @return Integer value read, or -1 if end of data.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",143,152,"/**
 * Constructs an Invoker with SimpleAuth fallback configuration.
 * @param fallbackToSimpleAuth Whether to fallback to simple auth.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",80,88,"/**
 * Creates a dynamic proxy for the given protocol.
 * @param protocol Protocol class to proxy.
 * @return ProtocolProxy instance.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProtocolMetaInfoProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",121,130,"/**
 * Creates and returns a ProtocolProxy for ProtocolMetaInfoPB.
 * @param connId Connection ID
 * @param conf Configuration
 * @param factory SocketFactory
 * @return ProtocolProxy instance
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getClient,org.apache.hadoop.ipc.WritableRpcEngine:getClient(org.apache.hadoop.conf.Configuration),279,283,"/**
 * Retrieves a client instance using the provided configuration.
 * @param conf The configuration to use for client retrieval.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",219,230,"/**
 * Constructs an Invoker with given parameters for RPC calls.
 * @param protocol RPC protocol class
 * @param address Socket address
 * @param ticket UserGroupInformation ticket
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",103,111,"/**
 * Creates a dynamic proxy for the given protocol.
 * @param protocol Protocol class to proxy.
 * @return ProtocolProxy instance.
 * @throws IOException if socket creation fails.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProtocolMetaInfoProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProtocolMetaInfoProxy(org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",128,137,"/**
 * Creates and returns a ProtocolProxy for ProtocolMetaInfoPB.
 * @param connId Connection ID
 * @param conf Configuration
 * @param factory Socket factory
 * @return ProtocolProxy instance
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker:<init>(java.lang.Class,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",150,159,"/**
 * Constructs an Invoker with additional parameters for auth fallback.
 * @param fallbackToSimpleAuth Whether to use simple authentication.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshAuthorizationPolicyProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.security.protocolPB.RefreshAuthorizationPolicyProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),60,67,"/**
 * Checks if the given method is supported by the RPC proxy.
 * @param methodName The name of the method to check.
 * @return True if the method is supported, false otherwise.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/protocolPB/RefreshUserMappingsProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.security.protocolPB.RefreshUserMappingsProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),71,78,"/**
 * Checks if the specified method is supported by the RPC service.
 * @param methodName The name of the method to check.
 * @return True if the method is supported, false otherwise.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/GenericRefreshProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.ipc.protocolPB.GenericRefreshProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),106,113,"/**
 * Checks if the specified method is supported by the RPC proxy.
 * @param methodName The name of the method to check.
 * @return True if the method is supported, false otherwise.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/protocolPB/RefreshCallQueueProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.ipc.protocolPB.RefreshCallQueueProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),60,67,"/**
 * Checks if the specified method is supported by the RPC proxy.
 * @param methodName The name of the method to check.
 * @return True if the method is supported, false otherwise.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/protocolPB/GetUserMappingsProtocolClientSideTranslatorPB.java,isMethodSupported,org.apache.hadoop.tools.protocolPB.GetUserMappingsProtocolClientSideTranslatorPB:isMethodSupported(java.lang.String),61,66,"/**
 * Checks if a method is supported by the RPC protocol.
 * @param methodName The name of the method to check.
 * @return True if the method is supported, false otherwise.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,initRPC,org.apache.hadoop.ha.ZKFailoverController:initRPC(),330,334,"/**
 * Initializes the RPC server with configured address and policy.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,<init>,org.apache.hadoop.fs.Trash:<init>(org.apache.hadoop.conf.Configuration),50,52,"/**
 * Constructs a Trash object using the provided Configuration.
 * @param conf Hadoop configuration object.
 * @throws IOException if an I/O error occurs.
 */
","* Construct a trash can accessor.
   * @param conf a Configuration
   * @throws IOException raised on errors performing I/O.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DU.java,main,org.apache.hadoop.fs.DU:main(java.lang.String[]),91,102,"/**
 * Calculates and prints disk space usage of a directory.
 * Uses command-line argument for path, defaults to current dir.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/net/DomainNameResolverFactory.java,newInstance,"org.apache.hadoop.net.DomainNameResolverFactory:newInstance(org.apache.hadoop.conf.Configuration,java.net.URI,java.lang.String)",50,53,"/**
 * Creates a DomainNameResolver with host from URI.
 * @param conf Configuration object.
 * @param uri URI object.
 * @param configKey Configuration key.
 * @return A new DomainNameResolver instance.
 */
","* Create a domain name resolver to convert the domain name in the config to
   * the actual IP addresses of the Namenode/Router/RM.
   *
   * @param conf Configuration to get the resolver from.
   * @param uri the url that the resolver will be used against
   * @param configKey The config key name suffixed with
   *                  the nameservice/yarnservice.
   * @return Domain name resolver.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,setConfiguration,org.apache.hadoop.security.SecurityUtil:setConfiguration(org.apache.hadoop.conf.Configuration),98,103,"/**
 * Sets the configuration for the system.
 * @param conf The Configuration object to set.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,<init>,org.apache.hadoop.security.UserGroupInformation$TestingGroups:<init>(org.apache.hadoop.security.Groups),1575,1578,"/**
 * Constructs a TestingGroups with a given underlying implementation.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getUserToGroupsMappingService,org.apache.hadoop.security.Groups:getUserToGroupsMappingService(org.apache.hadoop.conf.Configuration),471,481,"/**
 * Returns the Groups object, creating it if it doesn't exist.
 * @param conf Configuration object for initialization.
 * @return The Groups object.
 */
","* Get the groups being used to map user-to-groups.
   * @param conf configuration.
   * @return the groups being used to map user-to-groups.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getUserToGroupsMappingServiceWithLoadedConfiguration,org.apache.hadoop.security.Groups:getUserToGroupsMappingServiceWithLoadedConfiguration(org.apache.hadoop.conf.Configuration),488,495,"/**
 * Returns a synchronized Groups service with the provided configuration.
 */","* Create new groups used to map user-to-groups with loaded configuration.
   * @param conf configuration.
   * @return the groups being used to map user-to-groups.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,init,org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:init(javax.servlet.FilterConfig),53,58,"/**
 * Initializes the filter, refreshing proxy user group configurations.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,init,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:init(javax.servlet.FilterConfig),177,202,"/**
 * Initializes the filter, setting authentication methods and refreshing proxyuser config.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,refreshSuperUserGroupsConfiguration,org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(org.apache.hadoop.conf.Configuration),86,88,"/**
 * Refreshes superuser groups configuration using default proxy user.
 * @param conf Configuration object to update.
 */
","* Refreshes configuration using the default Proxy user prefix for properties.
   * @param conf configuration",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,deleteOnExit,org.apache.hadoop.fs.FileContext:deleteOnExit(org.apache.hadoop.fs.Path),1706,1724,"/**
* Registers a file to be deleted on JVM exit.
* @param f the file path to delete
* @return true if registration was successful
*/
","* Mark a path to be deleted on JVM shutdown.
   * 
   * @param f the existing path to delete.
   *
   * @return  true if deleteOnExit is successful, otherwise false.
   *
   * @throws AccessControlException If access is denied
   * @throws UnsupportedFileSystemException If file system for <code>f</code> is
   *           not supported
   * @throws IOException If an I/O error occurred
   * 
   * Exceptions applicable to file systems accessed over RPC:
   * @throws RpcClientException If an exception occurred in the RPC client
   * @throws RpcServerException If an exception occurred in the RPC server
   * @throws UnexpectedServerException If server implementation throws 
   *           undeclared exception to RPC server",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceShutdownHook.java,register,org.apache.hadoop.service.launcher.ServiceShutdownHook:register(int),61,64,"/**
* Registers this object as a shutdown hook with a given priority.
* @param priority Hook execution priority (lower value is higher priority)
*/
","* Register the service for shutdown with Hadoop's
   * {@link ShutdownHookManager}.
   * @param priority shutdown hook priority",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/StringUtils.java,startupShutdownMessage,"org.apache.hadoop.util.StringUtils:startupShutdownMessage(java.lang.Class,java.lang.String[],org.slf4j.Logger)",805,828,"/**
 * Logs startup message and registers signal loggers, adds shutdown hook.
 * @param clazz Class of the application.
 * @param args Command line arguments.
 * @param log SLF4J logger instance.
 */
","* Print a log message for starting up and shutting down
   * @param clazz the class of the server
   * @param args arguments
   * @param log the target log object",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,loadSSLConfiguration,org.apache.hadoop.http.HttpServer2$Builder:loadSSLConfiguration(),455,483,"/**
 * Loads SSL configuration from the provided configuration.
 * Throws IOException if required properties are missing.
 */
",* Load SSL properties from the SSL configuration.,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,loadSslConf,org.apache.hadoop.security.LdapGroupsMapping:loadSslConf(org.apache.hadoop.conf.Configuration),874,891,"/**
 * Loads SSL configuration from the provided configuration object.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getPasswordForBindUser,org.apache.hadoop.security.LdapGroupsMapping:getPasswordForBindUser(java.lang.String),977,991,"/**
 * Retrieves the password for a bound user, trying multiple sources.
 * @param keyPrefix Prefix for configuration keys.
 * @return Password string.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,createTrustManagersFromConfiguration,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createTrustManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,java.lang.String,long)",105,150,"/**
 * Creates TrustManagers from configuration, reloading as needed.
 * @param mode SSL mode
 * @param truststoreType Truststore type
 * @param truststoreLocation Truststore location
 * @param storesReloadInterval Reload interval in millis
 * @throws IOException, GeneralSecurityException
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,createKeyManagersFromConfiguration,"org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:createKeyManagersFromConfiguration(org.apache.hadoop.security.ssl.SSLFactory$Mode,java.lang.String,long)",160,205,"/**
 * Creates KeyManagers from configuration, reloading on file change.
 * @param mode SSL mode, keystore type, reload interval.
 * @throws GeneralSecurityException, IOException on config error.
 */
","* Implements logic of initializing the KeyManagers with the options
   * to reload keystores.
   * @param mode client or server
   * @param keystoreType The keystore type.
   * @param storesReloadInterval The interval to check if the keystore certificates
   *                             file has changed.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,getZKAuths,org.apache.hadoop.util.curator.ZKCuratorManager:getZKAuths(org.apache.hadoop.conf.Configuration),120,123,"/**
 * Retrieves ZooKeeper authentication information from configuration.
 * @param conf Configuration object to fetch auth info from.
 * @return List of ZKAuthInfo objects.
 */
","* Utility method to fetch ZK auth info from the configuration.
   *
   * @param conf configuration.
   * @throws java.io.IOException if the Zookeeper ACLs configuration file
   * cannot be read
   * @throws ZKUtil.BadAuthFormatException if the auth format is invalid
   * @return ZKAuthInfo List.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,initZK,org.apache.hadoop.ha.ZKFailoverController:initZK(),341,382,"/**
 * Initializes ZooKeeper connection and ActiveStandbyElector.
 * Configures quorum, timeout, ACLs, auth info, and retry count.
 */",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.RSErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),41,50,"/**
 * Prepares an ErasureCodingStep for encoding a block group.
 * @param blockGroup ECBlockGroup to be encoded.
 * @return An ErasureEncodingStep object.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureEncoder.java,prepareEncodingStep,org.apache.hadoop.io.erasurecode.coder.HHXORErasureEncoder:prepareEncodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),48,59,"/**
 * Prepares an erasure coding step using RS and XOR encoders.
 * @param blockGroup The block group for encoding.
 * @return An HHXORErasureEncodingStep object.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/RSErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.RSErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),41,50,"/**
 * Prepares a decoding step for erasure coding.
 * @param blockGroup ECBlockGroup for decoding.
 * @return ErasureDecodingStep object.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/erasurecode/coder/HHXORErasureDecoder.java,prepareDecodingStep,org.apache.hadoop.io.erasurecode.coder.HHXORErasureDecoder:prepareDecodingStep(org.apache.hadoop.io.erasurecode.ECBlockGroup),49,65,"/**
 * Creates and returns an ErasureCodingStep for decoding.
 * @param blockGroup ECBlockGroup containing blocks to decode.
 * @return HHXORErasureDecodingStep object.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/RuleBasedLdapGroupsMapping.java,getGroupsSet,org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroupsSet(java.lang.String),92,105,"/**
 * Returns a set of groups for a user, applying case conversion based on rule.
 * @param user User identifier.
 * @return Set of group names.
 */
",,,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,getGroups,org.apache.hadoop.security.LdapGroupsMapping:getGroups(java.lang.String),357,360,"/**
 * Retrieves a list of groups for the given user.
 * @param user The username to fetch groups for.
 * @return A list of group names.
 */
","* Returns list of groups for a user.
   * 
   * The LdapCtx which underlies the DirContext object is not thread-safe, so
   * we need to block around this whole method. The caching infrastructure will
   * ensure that performance stays in an acceptable range.
   *
   * @param user get groups for this user
   * @return list of groups for a given user",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,main,org.apache.hadoop.conf.Configuration:main(java.lang.String[]),3945,3947,"/**
 * Writes the Hibernate configuration to XML on standard output.
 */
","For debugging.  List non-default properties to the terminal and exit.
   * @param args the argument to be parsed.
   * @throws Exception exception.",,,True,18
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,start,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:start(),178,194,"/**
 * Starts the metrics system, ensuring it hasn't already started.
 * Calls preStart/postStart callbacks and configures the system.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,<init>,org.apache.hadoop.http.HttpServer2:<init>(org.apache.hadoop.http.HttpServer2$Builder),699,724,"/**
 * Constructs a new HttpServer2 instance using the provided Builder.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenManager.java,<init>,"org.apache.hadoop.security.token.delegation.web.DelegationTokenManager:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.Text)",118,125,"/**
 * Creates a DelegationTokenManager using ZK or file-based secret management.
 * @param conf Configuration object.
 * @param tokenKind Token kind identifier.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcClient.java,saslConnect,org.apache.hadoop.security.SaslRpcClient:saslConnect(org.apache.hadoop.ipc.Client$IpcStreams),365,455,"/**
 * Establishes a SASL connection using the provided IPC streams.
 * @param ipcStreams streams for sending/receiving SASL messages
 * @return Authentication method used for the connection.
 */","* Do client side SASL authentication with server via the given IpcStreams.
   *
   * @param ipcStreams ipcStreams.
   * @return AuthMethod used to negotiate the connection
   * @throws IOException raised on errors performing I/O.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/RawLocalFs.java,<init>,org.apache.hadoop.fs.local.RawLocalFs:<init>(org.apache.hadoop.conf.Configuration),42,44,"/**
 * Constructs a RawLocalFs instance using the default local FS URI.
 * @param conf Hadoop configuration object.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploaderBuilder.java,<init>,"org.apache.hadoop.fs.impl.FileSystemMultipartUploaderBuilder:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",37,41,"/**
 * Creates a FileSystemMultipartUploaderBuilder.
 * @param fileSystem The file system to use.
 * @param path The upload path.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String)",111,113,"/**
 * Constructs a PathData with a file system and path string.
 * @param fs The file system.
 * @param pathString The path as a string.
 * @throws IOException If an I/O error occurs.
 */
","* Looks up the file status for a path.  If the path
   * doesn't exist, then the status will be null
   * @param fs the FileSystem for the path
   * @param pathString a string for a path 
   * @throws IOException if anything goes wrong",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getDirectoryContents,org.apache.hadoop.fs.shell.PathData:getDirectoryContents(),274,285,"/**
 * Lists directory contents.
 * @return Array of PathData objects, sorted alphabetically.
 * @throws IOException if an I/O error occurs.
 */
","* Returns a list of PathData objects of the items contained in the given
   * directory.
   * @return list of PathData objects for its children
   * @throws IOException if anything else goes wrong...",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,maybeIgnoreMissingDirectory,"org.apache.hadoop.fs.FileUtil:maybeIgnoreMissingDirectory(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.FileNotFoundException)",2094,2110,"/**
 * Rethrows FileNotFoundException if directory listing is inconsistent.
 * Otherwise, logs and ignores missing directory.
 */","* Method to call after a FNFE has been raised on a treewalk, so as to
   * decide whether to throw the exception (default), or, if the FS
   * supports inconsistent directory listings, to log and ignore it.
   * If this returns then the caller should ignore the failure and continue.
   * @param fs filesystem
   * @param path path
   * @param e exception caught
   * @throws FileNotFoundException the exception passed in, if rethrown.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.DelegateToFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",287,292,"/**
 * Checks if the path has the specified capability.
 * @param path The path to check.
 * @param capability The capability to check for.
 * @return True if the path has the capability.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.http.AbstractHttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",122,131,"/**
 * Checks if the path has the specified capability.
 * Returns true for FS_READ_ONLY_CONNECTOR, else delegates.
 */
","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.RawLocalFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1315,1332,"/**
 * Checks if the path has the specified capability.
 * @param path The path to check.
 * @param capability The capability to check for.
 * @return True if the path has the capability, false otherwise.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.viewfs.ViewFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1350,1371,"/**
 * Checks if a path has the specified capability.
 * @param path The path to check.
 * @param capability The capability to check for.
 * @return True if the path has the capability, false otherwise.
 */
","* Reject the concat operation; forward the rest to the viewed FS.
   * @param path path to query the capability of.
   * @param capability string to query the stream support for.
   * @return the capability
   * @throws IOException if there is no resolved FS, or it raises an IOE.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.viewfs.ViewFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),1373,1389,"/**
 * Resolves the enclosing root path for a given path.
 * @param path Path to resolve; throws IOException if not found.
 * @return Enclosing root Path object.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getEnclosingRoot(org.apache.hadoop.fs.Path),1941,1958,"/**
 * Finds the enclosing root path of a given path.
 * @param path Path to resolve
 * @return Enclosing root Path
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getEnclosingRoot,org.apache.hadoop.fs.FilterFileSystem:getEnclosingRoot(org.apache.hadoop.fs.Path),735,738,"/**
 * Gets the enclosing root path for a given path.
 * @param path The path to check.
 * @return The enclosing root path.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/wrappedio/WrappedIO.java,fileSystem_getEnclosingRoot,"org.apache.hadoop.io.wrappedio.WrappedIO:fileSystem_getEnclosingRoot(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",202,204,"/**
 * Gets the enclosing root of a path within a file system.
 * @param fs the file system
 * @param path the path to check
 * @return the enclosing root path
 */
","* Return path of the enclosing root for a given path.
   * The enclosing root path is a common ancestor that should be used for temp and staging dirs
   * as well as within encryption zones and other restricted directories.
   * @param fs filesystem
   * @param path file path to find the enclosing root path for
   * @return a path to the enclosing root
   * @throws IOException early checks like failure to resolve path cause IO failures",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.FilterFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",740,753,"/**
 * Checks if the path has the specified capability.
 * Returns false for unsupported capabilities, otherwise delegates.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,rename,"org.apache.hadoop.fs.viewfs.ViewFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",694,760,"/**
* Renames a file or directory from src to dst.
* @param src source Path
* @param dst destination Path
* @return true if rename succeeds, false otherwise
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,create,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:create(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,boolean,int,short,long,org.apache.hadoop.util.Progressable)",193,199,"/**
 * Creates a data output stream for writing to a file.
 * @param f Path to the file, permission, overwrite, etc.
 * @return FSDataOutputStream
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,createNonRecursive,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createNonRecursive(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission,java.util.EnumSet,int,short,long,org.apache.hadoop.util.Progressable)",201,208,"/**
 * Creates a non-recursive data output stream.
 * @param f Path to create stream on
 * @return FSDataOutputStream or null on failure
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,delete,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete(org.apache.hadoop.fs.Path,boolean)",210,214,"/**
 * Deletes a file or directory.
 * @param f Path to delete.
 * @param recursive If true, delete recursively.
 * @return True if successful, false otherwise.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileBlockLocations,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileBlockLocations(org.apache.hadoop.fs.FileStatus,long,long)",223,228,"/**
 * Gets block locations for a file. Delegates to super, using ViewFs.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileChecksum,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path),230,234,"/**
 * Gets the checksum of a file.
 * @param f Path to the file.
 * @return FileChecksum object.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileChecksum,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileChecksum(org.apache.hadoop.fs.Path,long)",236,240,"/**
 * Retrieves file checksum using parent class implementation.
 * @param f Path to the file. @param length File length.
 * @return FileChecksum object.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getFileStatus(org.apache.hadoop.fs.Path),242,246,"/**
 * Gets the file status for the given path.
 * @param f Path to get the file status for.
 * @return FileStatus object.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getLinkTarget(org.apache.hadoop.fs.Path),248,251,"/**
 * Gets the target of a symbolic link.
 * @param f Path to the symbolic link
 * @return Path to the link target
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStatus(org.apache.hadoop.fs.Path),259,262,"/**
 * Gets the status of a path.
 * @param p The path to get the status of.
 * @return FsStatus object representing the path's status.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,listStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listStatus(org.apache.hadoop.fs.Path),264,268,"/**
 * Lists status of files/directories under the given path.
 * @param f Path to list status for; wraps in fullPath.
 * @return FileStatus array.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,listLocatedStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listLocatedStatus(org.apache.hadoop.fs.Path),270,274,"/**
 * Lists located status for a path, using full path.
 * @param f the path to list
 * @return RemoteIterator of LocatedFileStatus objects
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",276,280,"/**
 * Creates directories recursively.
 * @param f The path to create.
 * @param permission Permissions for the created directories.
 * @return True if successful, false otherwise.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,mkdirs,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:mkdirs(org.apache.hadoop.fs.Path),282,285,"/**
* Creates directories recursively. Delegates to super with full path.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,open,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:open(org.apache.hadoop.fs.Path,int)",287,291,"/**
 * Opens a file using the superclass implementation.
 * @param f Path to the file.
 * @param bufferSize Buffer size for the stream.
 * @return FSDataInputStream
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,append,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:append(org.apache.hadoop.fs.Path,int,org.apache.hadoop.util.Progressable)",293,297,"/**
 * Appends data to a file. Delegates to superclass implementation.
 * @param f Path to append to. @param bufferSize Buffer size.
 * @param progress Progressable object. @return FSDataOutputStream
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,rename,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",299,304,"/**
 * Renames a file or directory from src to dst.
 * @param src Source path
 * @param dst Destination path
 * @return True if rename succeeds, false otherwise.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setOwner,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setOwner(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",306,311,"/**
 * Sets the owner of the file represented by the Path.
 * @param f The Path representing the file.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setPermission,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setPermission(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",313,317,"/**
* Sets the permission for a file or directory.
* @param f Path of the file/directory.
* @param permission FsPermission to set.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setReplication,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setReplication(org.apache.hadoop.fs.Path,short)",319,323,"/**
 * Sets the replication factor for a file.
 * @param f Path to the file. @param replication replication factor.
 * @return True if successful, false otherwise.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setTimes,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setTimes(org.apache.hadoop.fs.Path,long,long)",325,329,"/**
* Sets modification and access times of a file.
* @param f Path to the file. mtime, atime are timestamps.
* @throws IOException if an I/O error occurs.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,modifyAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:modifyAclEntries(org.apache.hadoop.fs.Path,java.util.List)",331,335,"/**
* Modifies ACL entries for a given path, delegating to super.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeAclEntries,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAclEntries(org.apache.hadoop.fs.Path,java.util.List)",337,341,"/**
 * Removes ACL entries from a path. Delegates to superclass implementation.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeDefaultAcl,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeDefaultAcl(org.apache.hadoop.fs.Path),343,346,"/**
* Removes the default ACL for a given path.
* @param path The path for which to remove the ACL.
* @throws IOException if an I/O error occurs.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeAcl,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeAcl(org.apache.hadoop.fs.Path),348,351,"/**
 * Removes the ACL for the given path.
 * @param path The path for which to remove ACL.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setAcl,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setAcl(org.apache.hadoop.fs.Path,java.util.List)",353,356,"/**
 * Sets the ACL for a given path, delegating to the parent class.
 * @param path The path to set the ACL for.
 * @param aclSpec The ACL entries to apply.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getAclStatus,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getAclStatus(org.apache.hadoop.fs.Path),358,361,"/**
 * Gets the ACL status for a given path.
 * @param path The path to check.
 * @return ACL status, delegates to superclass.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setXAttr(org.apache.hadoop.fs.Path,java.lang.String,byte[],java.util.EnumSet)",363,367,"/**
 * Sets an extended attribute on a path, delegating to the parent.
 * @param path The path to set the attribute on.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttr(org.apache.hadoop.fs.Path,java.lang.String)",369,372,"/**
 * Retrieves extended attribute by name for a given path.
 * @param path The path to retrieve the attribute from.
 * @param name The name of the attribute.
 * @return The attribute value as a byte array.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path),374,377,"/**
 * Retrieves extended attributes for a given path.
 * @param path The path to retrieve attributes for.
 * @return Map of attribute names and values.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getXAttrs,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getXAttrs(org.apache.hadoop.fs.Path,java.util.List)",379,383,"/**
 * Retrieves extended attributes for a path.
 * @param path The path to retrieve attributes from.
 * @param names Attribute names to fetch.
 * @return Map of attribute names to byte array values.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,truncate,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:truncate(org.apache.hadoop.fs.Path,long)",385,388,"/**
 * Truncates the file at the given path to the specified length.
 * @param path The path to the file.
 * @param newLength The new length of the file.
 * @return True if successful, false otherwise.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,listXAttrs,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:listXAttrs(org.apache.hadoop.fs.Path),390,393,"/**
 * Lists extended attributes for a given path.
 * @param path The path to list attributes for.
 * @return List of extended attribute names.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,removeXAttr,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:removeXAttr(org.apache.hadoop.fs.Path,java.lang.String)",395,398,"/**
 * Removes an extended attribute from a path.
 * @param path The path to remove the attribute from.
 * @param name The name of the attribute to remove.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,createSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",400,403,"/**
 * Creates a snapshot of the given path with the specified name.
 * @param path The path to snapshot.
 * @param name The name of the snapshot.
 * @return The path to the created snapshot.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,renameSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:renameSnapshot(org.apache.hadoop.fs.Path,java.lang.String,java.lang.String)",405,409,"/**
 * Renames a snapshot using the provided old and new names.
 * @param path Path of the snapshot.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,deleteSnapshot,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:deleteSnapshot(org.apache.hadoop.fs.Path,java.lang.String)",411,415,"/**
 * Deletes a snapshot using the provided directory and name.
 * @param snapshotDir Snapshot directory path.
 * @param snapshotName Snapshot name.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,resolvePath,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:resolvePath(org.apache.hadoop.fs.Path),417,420,"/**
 * Resolves a path, using the full path for resolution.
 * @param p The path to resolve.
 * @return The resolved path.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getContentSummary,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getContentSummary(org.apache.hadoop.fs.Path),422,425,"/**
 * Gets content summary for a file.
 * @param f Path to the file.
 * @return ContentSummary object.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getQuotaUsage,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getQuotaUsage(org.apache.hadoop.fs.Path),427,430,"/**
 * Gets the quota usage for a given path.
 * @param f Path to check quota usage for
 * @return QuotaUsage object
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize(org.apache.hadoop.fs.Path),439,442,"/**
* Gets the default block size for a file.
* @param f Path to the file.
* @return Default block size as a long.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication(org.apache.hadoop.fs.Path),449,452,"/**
* Gets the default replication factor for a path.
* @param f Path object
* @return Short replication factor.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getStoragePolicy(org.apache.hadoop.fs.Path),464,467,"/**
 * Gets the storage policy for a given path.
 * @param src Path to check.
 * @return Storage policy SPI.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,satisfyStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:satisfyStoragePolicy(org.apache.hadoop.fs.Path),469,472,"/**
 * Applies storage policy to the given source path.
 * Delegates to superclass with full path.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,setStoragePolicy,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:setStoragePolicy(org.apache.hadoop.fs.Path,java.lang.String)",474,477,"/**
* Sets the storage policy for a source path.
* @param src The source path.
* @param policyName The name of the storage policy.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,unsetStoragePolicy,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:unsetStoragePolicy(org.apache.hadoop.fs.Path),479,482,"/**
* Removes storage policy from a path.
* @param src The path to unset the policy from.
* @throws IOException if an I/O error occurs.
*/
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,createFile,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:createFile(org.apache.hadoop.fs.Path),484,487,"/**
 * Creates a file at the given path.
 * @param path Path to create the file at.
 * @return FSDataOutputStreamBuilder for the created file.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFile,org.apache.hadoop.fs.FileSystem:openFile(org.apache.hadoop.fs.Path),4761,4765,"/**
 * Opens a file and returns a FutureDataInputStreamBuilder.
 * @param path Path to the file to open.
 * @return FutureDataInputStreamBuilder instance.
 */
","* Open a file for reading through a builder API.
   * Ultimately calls {@link #open(Path, int)} unless a subclass
   * executes the open command differently.
   *
   * The semantics of this call are therefore the same as that of
   * {@link #open(Path, int)} with one special point: it is in
   * {@code FSDataInputStreamBuilder.build()} in which the open operation
   * takes place -it is there where all preconditions to the operation
   * are checked.
   * @param path file path
   * @return a FSDataInputStreamBuilder object to build the input stream
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,openFile,org.apache.hadoop.fs.FileSystem:openFile(org.apache.hadoop.fs.PathHandle),4780,4785,"/**
 * Opens a file and returns a FutureDataInputStreamBuilder.
 * @param pathHandle Path handle for the file to open.
 * @return FutureDataInputStreamBuilder for the file.
 */
","* Open a file for reading through a builder API.
   * Ultimately calls {@link #open(PathHandle, int)} unless a subclass
   * executes the open command differently.
   *
   * If PathHandles are unsupported, this may fail in the
   * {@code FSDataInputStreamBuilder.build()}  command,
   * rather than in this {@code openFile()} operation.
   * @param pathHandle path handle.
   * @return a FSDataInputStreamBuilder object to build the input stream
   * @throws IOException if some early checks cause IO failures.
   * @throws UnsupportedOperationException if support is checked early.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,locateKeystore,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:locateKeystore(),145,176,"/**
 * Locates and loads the keystore, handling password and path inconsistencies.
 */","* Open up and initialize the keyStore.
   * @throws IOException If there is a problem reading the password file
   * or a problem reading the keystore.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,checkTFileDataIndex,org.apache.hadoop.io.file.tfile.TFile$Reader:checkTFileDataIndex(),882,893,"/**
 * Initializes the tfileIndex if it's null, reading from meta block.
 */","* Lazily loading the TFile index.
     * 
     * @throws IOException",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getMetaBlock,org.apache.hadoop.io.file.tfile.TFile$Reader:getMetaBlock(java.lang.String),967,970,"/**
 * Retrieves a meta block by name.
 * @param name Name of the meta block to retrieve.
 * @return DataInputStream representing the meta block.
 */
","* Stream access to a meta block.``
     * 
     * @param name
     *          The name of the meta block.
     * @return The input stream.
     * @throws IOException
     *           on I/O error.
     * @throws MetaBlockDoesNotExist
     *           If the meta block with the name does not exist.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/BCFile.java,<init>,"org.apache.hadoop.io.file.tfile.BCFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.conf.Configuration)",617,645,"/**
 * Initializes a Reader with input stream, file length, and configuration.
 */","* Constructor
     * 
     * @param fin
     *          FS input stream.
     * @param fileLength
     *          Length of the corresponding file
     * @throws IOException",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getBlockReader,org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockReader(int),2035,2037,"/**
 * Retrieves a BlockReader for the specified block index.
 * @param blockIndex Index of the block to retrieve.
 * @return BlockReader instance.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareMetaBlock,"org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String,java.lang.String)",595,606,"/**
 * Prepares a meta block with given name and compressed name.
 * @param name Meta block name.
 * @param compressName Compressed name.
 * @return DataOutputStream for the meta block.
 */
","* Obtain an output stream for creating a meta block. This function may not
     * be called when there is a key append stream or value append stream
     * active. No more key-value insertion is allowed after a meta data block
     * has been added to TFile.
     * 
     * @param name
     *          Name of the meta block.
     * @param compressName
     *          Name of the compression algorithm to be used. Must be one of the
     *          strings returned by
     *          {@link TFile#getSupportedCompressionAlgorithms()}.
     * @return A DataOutputStream that can be used to write Meta Block data.
     *         Closing the stream would signal the ending of the block.
     * @throws IOException raised on errors performing I/O.
     * @throws MetaBlockAlreadyExists
     *           the Meta Block with the same name already exists.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,close,org.apache.hadoop.io.file.tfile.TFile$Writer:close(),300,343,"/**
 * Closes the TFile, ensuring data and index blocks are written.
 */","* Close the Writer. Resources will be released regardless of the exceptions
     * being thrown. Future close calls will have no effect.
     * 
     * The underlying FSDataOutputStream is not closed.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareMetaBlock,org.apache.hadoop.io.file.tfile.TFile$Writer:prepareMetaBlock(java.lang.String),623,632,"/**
 * Prepares a meta block with the given name.
 * @param name Meta block name.
 * @throws IOException on write error.
 */
","* Obtain an output stream for creating a meta block. This function may not
     * be called when there is a key append stream or value append stream
     * active. No more key-value insertion is allowed after a meta data block
     * has been added to TFile. Data will be compressed using the default
     * compressor as defined in Writer's constructor.
     * 
     * @param name
     *          Name of the meta block.
     * @return A DataOutputStream that can be used to write Meta Block data.
     *         Closing the stream would signal the ending of the block.
     * @throws IOException raised on errors performing I/O.
     * @throws MetaBlockAlreadyExists
     *           the Meta Block with the same name already exists.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,prepareAppendKey,org.apache.hadoop.io.file.tfile.TFile$Writer:prepareAppendKey(int),527,537,"/**
 * Creates a DataOutputStream for appending a key of given length.
 * @param length Length of the key to be appended.
 * @return DataOutputStream for key appending.
 */
","* Obtain an output stream for writing a key into TFile. This may only be
     * called when there is no active Key appending stream or value appending
     * stream.
     * 
     * @param length
     *          The expected length of the key. If length of the key is not
     *          known, set length = -1. Otherwise, the application must write
     *          exactly as many bytes as specified here before calling close on
     *          the returned output stream.
     * @return The key appending output stream.
     * @throws IOException raised on errors performing I/O.
     *",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults(org.apache.hadoop.fs.Path),459,462,"/**
 * Retrieves server defaults for a given path.
 * @param f Path to retrieve defaults for.
 * @return FsServerDefaults object.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(java.io.File,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)",517,557,"/**
 * Copies a file or directory to a destination file system.
 * @param src Source file/directory.
 * @param dstFS Destination file system.
 * @param dst Destination path.
 * @param deleteSource Delete source after copy?
 * @param conf Hadoop configuration.
 * @return True if copy successful.
 */
","* Copy local files to a FileSystem.
   *
   * @param src src.
   * @param dstFS dstFs.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return true if the operation succeeded.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/FileSystemMultipartUploader.java,innerComplete,"org.apache.hadoop.fs.impl.FileSystemMultipartUploader:innerComplete(org.apache.hadoop.fs.UploadHandle,org.apache.hadoop.fs.Path,java.util.Map)",195,243,"/**
 * Completes a multipart upload, merging parts and cleaning up.
 * @param multipartUploadId Upload handle to complete.
 * @param filePath Desired final file path.
 * @param handleMap Map of part handles to byte arrays.
 * @return PathHandle to the completed file.
 */
","* The upload complete operation.
   * @param multipartUploadId the ID of the upload
   * @param filePath path
   * @param handleMap map of handles
   * @return the path handle
   * @throws IOException failure",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,touch,org.apache.hadoop.fs.shell.TouchCommands$Touch:touch(org.apache.hadoop.fs.shell.PathData),162,175,"/**
 * Touches a file/directory. Creates if missing, updates timestamp if present.
 * @param item PathData object containing file system and path.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,touchz,org.apache.hadoop.fs.shell.TouchCommands$Touchz:touchz(org.apache.hadoop.fs.shell.PathData),88,90,"/**
 * Creates an empty file at the specified path.
 * @param item PathData object containing file system and path.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,create,"org.apache.hadoop.fs.FileSystem:create(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",742,749,"/**
 * Creates a data output stream for the given file with specified permission.
 * @param fs FileSystem object
 * @param file Path of the file
 * @param permission FsPermission object
 * @return FSDataOutputStream
 */
","* Create a file with the provided permission.
   *
   * The permission of the file is set to be the provided permission as in
   * setPermission, not permission{@literal &~}umask
   *
   * The HDFS implementation is implemented using two RPCs.
   * It is understood that it is inefficient,
   * but the implementation is thread-safe. The other option is to change the
   * value of umask in configuration to be 0, but it is not thread-safe.
   *
   * @param fs FileSystem
   * @param file the name of the file to be created
   * @param permission the permission of the file
   * @return an output stream
   * @throws IOException IO failure",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)",1209,1215,"/**
 * Creates a Writer with specified FileSystem, Configuration, and types.
 * @param fs FileSystem, conf Configuration, name Path, key/val Classes
 */
","* Create the named file.
     * @deprecated Use 
     *   {@link SequenceFile#createWriter(Configuration, Writer.Option...)} 
     *   instead.
     * @param fs input filesystem.
     * @param conf input configuration.
     * @param name input name.
     * @param keyClass input keyClass.
     * @param valClass input valClass.
     * @throws IOException raised on errors performing I/O.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,rollLogDir,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:rollLogDir(),661,673,"/**
 * Creates or appends a log file with a unique filename.
 * Creates the directory if it doesn't exist.
 */
","* Create a new directory based on the current interval and a new log file in
   * that directory.
   *
   * @throws IOException thrown if an error occurs while creating the
   * new directory or new log file",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,readIndex,org.apache.hadoop.io.MapFile$Reader:readIndex(),577,631,"/**
 * Reads the index entirely into memory. Populates keys and positions.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,next,"org.apache.hadoop.io.MapFile$Reader:next(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",811,814,"/**
* Delegates to the underlying data structure's next method.
*/
","* Read the next key/value pair in the map into <code>key</code> and
     * <code>val</code>.  Returns true if such a pair exists and false when at
     * the end of the map.
     *
     * @param key WritableComparable.
     * @param val Writable.
     * @return if such a pair exists true,not false.
     * @throws IOException raised on errors performing I/O.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",106,119,"/**
 * Creates a proxy for the given protocol.
 * @param protocol Protocol interface.
 * @return ProtocolProxy instance.
 * @throws IOException on failure.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getProxy,"org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",348,367,"/**
 * Creates a proxy for the given protocol.
 * @param protocol Protocol class.
 * @return ProtocolProxy instance.
 * @throws IOException if an error occurs.
 */
","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   * @param <T> Generics Type.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param ticket input ticket.
   * @param conf input configuration.
   * @param factory input factory.
   * @param rpcTimeout input rpcTimeout.
   * @param connectionRetryPolicy input connectionRetryPolicy.
   * @param fallbackToSimpleAuth input fallbackToSimpleAuth.
   * @param alignmentContext input alignmentContext.
   * @return ProtocolProxy.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",113,126,"/**
 * Creates a proxy for the given protocol.
 * @param protocol Protocol interface.
 * @return ProtocolProxy instance.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processArguments,org.apache.hadoop.fs.shell.Delete$Expunge:processArguments(java.util.LinkedList),244,273,"/**
 * Processes arguments, setting FS and expunging trash.
 * @param args List of PathData objects (unused).
 * @throws IOException if an I/O error occurs.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getTrash,org.apache.hadoop.fs.FsShell:getTrash(),86,91,"/**
 * Returns the Trash object, creating it if it doesn't exist.
 * @return Trash object, initialized with configuration.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Groups.java,getUserToGroupsMappingService,org.apache.hadoop.security.Groups:getUserToGroupsMappingService(),462,464,"/**
 * Returns the user-to-groups mapping service using default configuration.
 */","* Get the groups being used to map user-to-groups.
   * @return the groups being used to map user-to-groups.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,initialize,"org.apache.hadoop.security.UserGroupInformation:initialize(org.apache.hadoop.conf.Configuration,boolean)",309,354,"/**
 * Initializes Kerberos authentication settings from the configuration.
 * @param conf Hadoop configuration object
 * @param overrideNameRules Whether to override name rules.
 */
","* Initialize UGI and related classes.
   * @param conf the configuration to use",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,<init>,org.apache.hadoop.security.authorize.AccessControlList:<init>(),73,74,"/**
 * Default constructor for the AccessControlList class.
 */",* This constructor exists primarily for AccessControlList to be Writable.,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,<init>,org.apache.hadoop.security.authorize.AccessControlList:<init>(java.lang.String),85,87,"/**
 * Constructs an ACL from a string representation.
 * @param aclString String containing ACL rules.
 */
","* Construct a new ACL from a String representation of the same.
   * 
   * The String is a a comma separated list of users and groups.
   * The user list comes first and is separated by a space followed 
   * by the group list. For e.g. ""user1,user2 group1,group2""
   * 
   * @param aclString String representation of the ACL",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,<init>,"org.apache.hadoop.security.authorize.AccessControlList:<init>(java.lang.String,java.lang.String)",97,99,"/**
 * Constructs an ACL with users and groups.
 * @param users Comma-separated list of users.
 * @param groups Comma-separated list of groups.
 */
","* Construct a new ACL from String representation of users and groups
   * 
   * The arguments are comma separated lists
   * 
   * @param users comma separated list of users
   * @param groups comma separated list of groups",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,refreshSuperUserGroupsConfiguration,org.apache.hadoop.security.authorize.ProxyUsers:refreshSuperUserGroupsConfiguration(),58,61,"/**
 * Refreshes superuser group configuration using default settings.
 */
",* refresh Impersonation rules,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,coreServiceLaunch,"org.apache.hadoop.service.launcher.ServiceLauncher:coreServiceLaunch(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean)",571,647,"/**
 * Launches and potentially executes a service.
 * @param conf Configuration object.
 * @param instance Existing service instance, or null to create.
 * @param processedArgs CLI arguments.
 * @param addShutdownHook Whether to register a shutdown hook.
 * @param execute Whether to execute the service.
 * @return Exit code of the service.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,build,org.apache.hadoop.http.HttpServer2$Builder:build(),485,564,"/**
 * Builds and configures an HttpServer2 instance.
 * @return Configured HttpServer2 instance.
 * @throws IOException if an I/O error occurs during setup.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,initializeBindUsers,org.apache.hadoop.security.LdapGroupsMapping:initializeBindUsers(),950,975,"/**
 * Initializes bind users from configuration, cycles through them.
 */",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/FileBasedKeyStoresFactory.java,init,org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory:init(org.apache.hadoop.security.ssl.SSLFactory$Mode),252,300,"/**
 * Initializes SSL context with key and trust managers based on config.
 * @param mode SSLFactory mode (client/server)
 * @throws IOException, GeneralSecurityException on failure
 */
","* Initializes the keystores of the factory.
   *
   * @param mode if the keystores are to be used in client or server mode.
   * @throws IOException thrown if the keystores could not be initialized due
   * to an IO error.
   * @throws GeneralSecurityException thrown if the keystores could not be
   * initialized due to a security error.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,start,"org.apache.hadoop.util.curator.ZKCuratorManager:start(java.util.List,boolean)",149,193,"/**
 * Starts the CuratorFramework client with provided auth info and SSL.
 * @param authInfos List of authentication information.
 * @param sslEnabled Whether SSL is enabled.
 */
","* Start the connection to the ZooKeeper ensemble.
   *
   * @param authInfos  List of authentication keys.
   * @param sslEnabled If the connection should be SSL/TLS encrypted.
   * @throws IOException            If the connection cannot be started.",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doRun,org.apache.hadoop.ha.ZKFailoverController:doRun(java.lang.String[]),202,270,"/**
 * Main entry point for the failover controller.
 * Handles arguments, initialization, and main loop.
 * @return 0 on success, error code on failure.
 */
",,,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/RuleBasedLdapGroupsMapping.java,getGroups,org.apache.hadoop.security.RuleBasedLdapGroupsMapping:getGroups(java.lang.String),76,90,"/**
 * Gets user groups, applying case conversion based on the rule.
 * @param user The user whose groups are to be retrieved.
 * @return A list of group names, potentially converted to upper/lower case.
 */
","* Returns list of groups for a user.
     * This calls {@link LdapGroupsMapping}'s getGroups and applies the
     * configured rules on group names before returning.
     *
     * @param user get groups for this user
     * @return list of groups for a given user",,,True,19
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/impl/MetricsSystemImpl.java,init,org.apache.hadoop.metrics2.impl.MetricsSystemImpl:init(java.lang.String),148,176,"/**
 * Initializes the MetricsSystem with a given prefix.
 * @param prefix Prefix for metrics names.
 * @return The MetricsSystem instance.
 */
","* Initialized the metrics system with a prefix.
   * @param prefix  the system will look for configs with the prefix
   * @return the metrics system object itself",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,initTokenManager,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:initTokenManager(java.util.Properties),148,163,"/**
 * Initializes the DelegationTokenManager with properties.
 * @param config Properties to configure the token manager.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setupSaslConnection,org.apache.hadoop.ipc.Client$Connection:setupSaslConnection(org.apache.hadoop.ipc.Client$IpcStreams),571,579,"/**
 * Establishes a SASL connection using provided streams.
 * @param streams IpcStreams for the connection
 * @return SASL connection object
 * @throws IOException if connection fails
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/LocalFs.java,<init>,org.apache.hadoop.fs.local.LocalFs:<init>(org.apache.hadoop.conf.Configuration),36,38,"/**
 * Constructs a LocalFs object using the provided Configuration.
 * @param conf Hadoop configuration object
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,suffix,org.apache.hadoop.fs.shell.PathData:suffix(java.lang.String),241,243,"/**
 * Appends an extension to the path and returns a new PathData.
 * @param extension The extension to append.
 * @return A new PathData object with the appended extension.
 */
","* Returns a new PathData with the given extension.
   * @param extension for the suffix
   * @return PathData
   * @throws IOException shouldn't happen",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,getPathDataForChild,org.apache.hadoop.fs.shell.PathData:getPathDataForChild(org.apache.hadoop.fs.shell.PathData),307,310,"/**
 * Gets path data for a child path.
 * @param child The child PathData.
 * @return PathData object for the child path.
 */
","* Creates a new object for a child entry in this directory
   * @param child the basename will be appended to this object's path
   * @return PathData for the child
   * @throws IOException if this object does not exist or is not a directory",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,recursePath,org.apache.hadoop.fs.shell.Command:recursePath(org.apache.hadoop.fs.shell.PathData),449,466,"/**
 * Recursively processes path data, handling directory contents.
 * Uses iterative or recursive listing based on sorting status.
 */","*  Gets the directory listing for a path and invokes
   *  {@link #processPaths(PathData, PathData...)}
   *  @param item {@link PathData} for directory to recurse into
   *  @throws IOException if anything goes wrong...",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.http.HttpsFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",122,131,"/**
 * Checks if the path has the specified capability.
 * @param path The path to check.
 * @param capability The capability to check for.
 */
","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/http/AbstractHttpFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.http.HttpFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",122,131,"/**
 * Checks if the path has the specified capability.
 * Returns true for FS_READ_ONLY_CONNECTOR, else delegates.
 */
","* Declare that this filesystem connector is always read only.
   * {@inheritDoc}",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",495,499,"/**
 * Checks if a path has the specified capability.
 * @param path The path to check.
 * @param capability The capability to check for.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,hasPathCapability,"org.apache.hadoop.fs.ChecksumFileSystem:hasPathCapability(org.apache.hadoop.fs.Path,java.lang.String)",1128,1140,"/**
 * Checks if path has capability, short-circuits for append/concat.
 * @param path The path to check.
 * @param capability The capability to check for.
 * @return True if path has capability, false otherwise.
 */
","* Disable those operations which the checksummed FS blocks.
   * {@inheritDoc}",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,delete,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:delete(org.apache.hadoop.fs.Path),217,221,"/**
 * Deletes a file or directory.
 * @param f Path to the file/directory to delete.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,updateFileStatus,org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode:updateFileStatus(org.apache.hadoop.fs.Path),131,136,"/**
 * Updates the file status based on the provided path.
 * @param f Path to the file whose status needs updating.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,listStatus,org.apache.hadoop.fs.viewfs.NflyFSystem:listStatus(org.apache.hadoop.fs.Path),804,845,"/**
 * Lists file statuses for a given path. Handles exceptions and returns statuses.
 */","* Returns the closest non-failing destination's result.
   *
   * @param f given path
   * @return array of file statuses according to nfly modes
   * @throws FileNotFoundException
   * @throws IOException",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,mkdirs,"org.apache.hadoop.fs.viewfs.NflyFSystem:mkdirs(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsPermission)",866,873,"/**
 * Creates directories recursively.
 * @param f The path to create.
 * @param permission Permissions for the directories.
 * @return True if all directories were created successfully.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,rename,"org.apache.hadoop.fs.viewfs.NflyFSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",739,765,"/**
 * Renames a file or directory from src to dst across all nodes.
 * @param src Source path
 * @param dst Destination path
 * @return True if rename succeeds on all nodes, false otherwise.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultBlockSize,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultBlockSize(),434,437,"/**
 * Returns the default block size for the root path.
 * Delegates to the overloaded method with the full path.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getDefaultReplication,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getDefaultReplication(),444,447,"/**
 * Returns the default replication factor.
 * Delegates to the overloaded method.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",462,504,"/**
 * Copies a file or directory from one FileSystem to another.
 * @param deleteSource Whether to delete the source file/dir after copy.
 */
","* Copy a file/directory tree within/between filesystems.
   * <p>
   * returns true if the operation succeeded. When deleteSource is true,
   * this means ""after the copy, delete(source) returned true""
   * If the destination is a directory, and mkdirs (dest) fails,
   * the operation will return false rather than raise any exception.
   * </p>
   * The overwrite flag is about overwriting files; it has no effect about
   * handing an attempt to copy a file atop a directory (expect an IOException),
   * or a directory over a path which contains a file (mkdir will fail, so
   * ""false"").
   * <p>
   * The operation is recursive, and the deleteSource operation takes place
   * as each subdirectory is copied. Therefore, if an operation fails partway
   * through, the source tree may be partially deleted.
   * </p>
   * @param srcFS source filesystem
   * @param srcStatus status of source
   * @param dstFS destination filesystem
   * @param dst path of source
   * @param deleteSource delete the source?
   * @param overwrite overwrite files at destination?
   * @param conf configuration to use when opening files
   * @return true if the operation succeeded.
   * @throws IOException failure",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.FileStatus,java.io.File,boolean,org.apache.hadoop.conf.Configuration)",578,605,"/**
 * Copies a file or directory from srcFS to dst.
 * @param deleteSource Whether to delete source after copy.
 * @return True if copy was successful.
 */",Copy FileSystem files to local files.,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,openFile,org.apache.hadoop.fs.shell.PathData:openFile(java.lang.String),632,639,"/**
 * Opens a file with specified policy and file length hint.
 * @param policy Policy string for file opening.
 * @return FSDataInputStream for the opened file.
 */
","* Open a file.
   * @param policy fadvise policy.
   * @return an input stream
   * @throws IOException failure",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFile,org.apache.hadoop.fs.FilterFileSystem:openFile(org.apache.hadoop.fs.Path),709,713,"/**
 * Opens a file and returns a FutureDataInputStreamBuilder.
 * @param path Path to the file to open.
 * @return Builder for FutureDataInputStream.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,openFile,"org.apache.hadoop.io.SequenceFile$Reader:openFile(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,int,long)",2002,2012,"/**
 * Opens a file with specified options.
 * @param fs FileSystem object
 * @param file Path to the file
 * @param bufferSize Buffer size for the stream
 * @param length File length, or -1 if unknown
 * @return FSDataInputStream object
 * @throws IOException if an I/O error occurs
 */
","* Override this method to specialize the type of
     * {@link FSDataInputStream} returned.
     * @param fs The file system used to open the file.
     * @param file The file being read.
     * @param bufferSize The buffer size used to read the file.
     * @param length The length being read if it is {@literal >=} 0.
     *               Otherwise, the length is not available.
     * @return The opened stream.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,load,"org.apache.hadoop.util.JsonSerialization:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileStatus)",264,283,"/**
 * Loads data from a file system path.
 * @param fs FileSystem, Path to read, Status (optional)
 * @return Data object or throws IOException
 */
","* Load from a Hadoop filesystem.
   * If a file status is supplied, it's passed in to the openFile()
   * call so that FS implementations can optimize their opening.
   * @param fs filesystem
   * @param path path
   * @param status status of the file to open.
   * @return a loaded object
   * @throws PathIOException JSON parse problem
   * @throws EOFException file status references an empty file
   * @throws IOException IO problems",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,openFile,org.apache.hadoop.fs.FilterFileSystem:openFile(org.apache.hadoop.fs.PathHandle),715,719,"/**
* Opens a file using the file system and returns a builder.
* @param pathHandle Handle to the file to open.
* @return FutureDataInputStreamBuilder for the opened file.
*/
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getFirstKey,org.apache.hadoop.io.file.tfile.TFile$Reader:getFirstKey(),901,904,"/**
* Gets the first key from the TFileIndex.
* Throws IOException if an error occurs.
*/
","* Get the first key in the TFile.
     * 
     * @return The first key in the TFile.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLastKey,org.apache.hadoop.io.file.tfile.TFile$Reader:getLastKey(),912,915,"/**
* Gets the last key from the TFileIndex.
* @throws IOException if an I/O error occurs.
*/
","* Get the last key in the TFile.
     * 
     * @return The last key in the TFile.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getBlockContainsKey,"org.apache.hadoop.io.file.tfile.TFile$Reader:getBlockContainsKey(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",985,995,"/**
 * Finds the block containing the given key.
 * @param key The key to search for.
 * @param greater True for upperBound, false for lowerBound.
 * @return Location of the block, or end if not found.
 */
","* if greater is true then returns the beginning location of the block
     * containing the key strictly greater than input key. if greater is false
     * then returns the beginning location of the block greater than equal to
     * the input key
     * 
     * @param key
     *          the input key
     * @param greater
     *          boolean flag
     * @return
     * @throws IOException",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getLocationByRecordNum,org.apache.hadoop.io.file.tfile.TFile$Reader:getLocationByRecordNum(long),997,1000,"/**
 * Retrieves a location by its record number.
 * @param recNum Record number of the location to retrieve.
 * @return Location object or null if not found.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNumByLocation,org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumByLocation(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),1002,1005,"/**
 * Gets the number of records for a given location.
 * @param location The location to query.
 * @return Record count for the location.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getKeyNear,org.apache.hadoop.io.file.tfile.TFile$Reader:getKeyNear(long),1063,1068,"/**
 * Gets the key near the given offset.
 * @param offset Offset to search near.
 * @return Key near the offset or null if not found.
 */
","* Get a sample key that is within a block whose starting offset is greater
     * than or equal to the specified offset.
     * 
     * @param offset
     *          The file offset.
     * @return the key that fits the requirement; or null if no such key exists
     *         (which could happen if the offset is close to the end of the
     *         TFile).
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,long,org.apache.hadoop.conf.Configuration)",802,818,"/**
 * Initializes a Reader with an FSDataInputStream, file length, and config.
 * @param fsdis Input stream, fileLength file size, conf Hadoop configuration
 */","* Constructor
     * 
     * @param fsdis
     *          FS input stream of the TFile.
     * @param fileLength
     *          The length of TFile. This is required because we have no easy
     *          way of knowing the actual size of the input file through the
     *          File input stream.
     * @param conf configuration.
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,initBlock,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:initBlock(int),1548,1559,"/**
 * Initializes a block reader for a given block index.
 * @param blockIndex Index of the block to be initialized.
 */
","* Load a compressed block for reading. Expecting blockIndex is valid.
       * 
       * @throws IOException",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,append,"org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],int,int,byte[],int,int)",380,413,"/**
 * Appends key-value data to the output.
 * @param key key data, offset, length
 * @param value value data, offset, length
 * @throws IOException if an I/O error occurs
 */
","* Adding a new key-value pair to TFile.
     * 
     * @param key
     *          buffer for key.
     * @param koff
     *          offset in key buffer.
     * @param klen
     *          length of key.
     * @param value
     *          buffer for value.
     * @param voff
     *          offset in value buffer.
     * @param vlen
     *          length of value.
     * @throws IOException
     *           Upon IO errors.
     *           <p>
     *           If an exception is thrown, the TFile will be in an inconsistent
     *           state. The only legitimate call after that would be close",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,getServerDefaults,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:getServerDefaults(),454,457,"/**
 * Returns the server defaults for the filesystem.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processPath,org.apache.hadoop.fs.shell.TouchCommands$Touch:processPath(org.apache.hadoop.fs.shell.PathData),148,151,"/**
* Processes a PathData item by touching it.
* @param item The PathData item to process.
*/
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processNonexistentPath,org.apache.hadoop.fs.shell.TouchCommands$Touch:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),153,160,"/**
 * Throws PathNotFoundException if parent doesn't exist, otherwise touches item.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processPath,org.apache.hadoop.fs.shell.TouchCommands$Touchz:processPath(org.apache.hadoop.fs.shell.PathData),67,77,"/**
 * Processes a path data item, throwing exceptions for directories
 * and non-zero length files, then touches the file.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/TouchCommands.java,processNonexistentPath,org.apache.hadoop.fs.shell.TouchCommands$Touchz:processNonexistentPath(org.apache.hadoop.fs.shell.PathData),79,86,"/**
 * Throws PathNotFoundException if parent doesn't exist; otherwise, touchz.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,getOutputStreamForKeystore,org.apache.hadoop.security.alias.KeyStoreProvider:getOutputStreamForKeystore(),52,56,"/**
 * Creates and returns an OutputStream for the keystore file.
 */",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,writeToNew,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:writeToNew(org.apache.hadoop.fs.Path),607,620,"/**
 * Writes the keystore to a new file at the specified path.
 * @param newPath Path to write the keystore to.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,midKey,org.apache.hadoop.io.MapFile$Reader:midKey(),649,657,"/**
 * Returns the middle key. Returns null if the count is zero.
 */
","* Get the key at approximately the middle of the file. Or null if the
     *  file is empty.
     *
     * @throws IOException raised on errors performing I/O.
     * @return WritableComparable.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,finalKey,org.apache.hadoop.io.MapFile$Reader:finalKey(org.apache.hadoop.io.WritableComparable),665,681,"/**
 * Skips to the last indexed entry and scans to the end of data.
 * @param key The key to use for scanning.
 * @throws IOException if an I/O error occurs.
 */
","* Reads the final key from the file.
     *
     * @param key key to read into
     * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,seekInternal,"org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable,boolean)",721,780,"/**
 * Seeks to a position based on the provided key.
 * @param key The key to seek for.
 * @param before True to seek before the key.
 * @return Comparison result.
 */","* Positions the reader at the named key, or if none such exists, at the
     * key that falls just before or just after dependent on how the
     * <code>before</code> parameter is set.
     * 
     * @param before - IF true, and <code>key</code> does not exist, position
     * file at entry that falls just before <code>key</code>.  Otherwise,
     * position file at record that sorts just after.
     * @return  0   - exact match found
     *          < 0 - positioned at next record
     *          1   - no more records in file",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,mergePass,org.apache.hadoop.io.MapFile$Merger:mergePass(),1101,1146,"/**
 * Merges input streams based on key comparison, writing to output.
 */","* Merge all input files to output map file.<br>
     * 1. Read first key/value from all input files to keys/values array. <br>
     * 2. Select the least key and corresponding value. <br>
     * 3. Write the selected key and value to output file. <br>
     * 4. Replace the already written key/value in keys/values arrays with the
     * next key/value from the selected input <br>
     * 5. Repeat step 2-4 till all keys are read. <br>",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",97,104,"/**
 * Gets a protocol proxy. Delegates to overloaded method.
 * @param protocol Protocol class.
 * @return ProtocolProxy instance.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getProxy,"org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",299,307,"/**
 * Gets a proxy for the given protocol.
 * @param protocol Protocol class.
 * @return ProtocolProxy object.
 * @throws IOException if an I/O error occurs.
 */
","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   * @param <T> Generics Type T
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param ticket input ticket.
   * @param conf input configuration.
   * @param factory input factory.
   * @param rpcTimeout input rpcTimeout.
   * @param connectionRetryPolicy input connectionRetryPolicy.
   * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getProxy,"org.apache.hadoop.ipc.WritableRpcEngine:getProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",322,330,"/**
 * Gets a protocol proxy. Delegates to overloaded method.
 * @param protocol Protocol class.
 * @param clientVersion Client version.
 * @return ProtocolProxy object.
 */
","* Construct a client-side proxy object with a ConnectionId.
   *
   * @param <T> Generics Type T.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param connId input ConnectionId.
   * @param conf input Configuration.
   * @param factory input factory.
   * @param alignmentContext Alignment context
   * @throws IOException raised on errors performing I/O.
   * @return ProtocolProxy.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",93,101,"/**
 * Gets a protocol proxy. Delegates to overloaded method.
 * @param protocol Protocol class.
 * @return ProtocolProxy object.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getCurrentTrashDir,org.apache.hadoop.fs.FsShell:getCurrentTrashDir(),125,127,"/**
 * Returns the current trash directory.
 * Delegates to TrashManager.getCurrentTrashDir().
 */","* Returns the Trash object associated with this shell.
   * @return Path to the trash
   * @throws IOException upon error",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,getCurrentTrashDir,org.apache.hadoop.fs.FsShell:getCurrentTrashDir(org.apache.hadoop.fs.Path),135,137,"/**
 * Gets the current trash directory for a given path.
 * @param path The path to get the trash directory for.
 * @return The current trash directory Path.
 */
","* Returns the current trash location for the path specified
   * @param path to be deleted
   * @return path to the trash
   * @throws IOException raised on errors performing I/O.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,ensureInitialized,org.apache.hadoop.security.UserGroupInformation:ensureInitialized(),295,303,"/**
 * Initializes UserGroupInformation if not already initialized.
 * Uses double-checked locking for thread safety.
 */
","* A method to initialize the fields that depend on a configuration.
   * Must be called before useKerberos or groups is used.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,setConfiguration,org.apache.hadoop.security.UserGroupInformation:setConfiguration(org.apache.hadoop.conf.Configuration),362,366,"/**
 * Sets the configuration for the class.
 * @param conf The configuration object to set.
 */
","* Set the static configuration for UGI.
   * In particular, set the security authentication mechanism and the
   * group look up service.
   * @param conf the configuration to use",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ServiceAuthorizationManager.java,refreshWithLoadedConfiguration,"org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refreshWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",152,200,"/**
 * Refreshes authorization data from the provided configuration.
 * @param conf Configuration object containing authorization settings.
 * @param provider PolicyProvider to fetch services from.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,init,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:init(java.lang.String),68,101,"/**
 * Initializes the access control list and machine list.
 * @param configurationPrefix Configuration prefix to use.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,getSip,org.apache.hadoop.security.authorize.ProxyUsers:getSip(),116,124,"/**
 * Returns the ImpersonationProvider instance, refreshing config if needed.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,launchService,"org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,org.apache.hadoop.service.Service,java.util.List,boolean,boolean)",485,539,"/**
 * Launches the service with given configuration and arguments.
 * @param conf Service configuration
 * @return ExitException indicating launch status
 */
","* Launch a service catching all exceptions and downgrading them to exit codes
   * after logging.
   *
   * Sets {@link #serviceException} to this value.
   * @param conf configuration to use
   * @param instance optional instance of the service.
   * @param processedArgs command line after the launcher-specific arguments
   * have been stripped out.
   * @param addShutdownHook should a shutdown hook be added to terminate
   * this service on shutdown. Tests should set this to false.
   * @param execute execute/wait for the service to stop.
   * @return an exit exception, which will have a status code of 0 if it worked",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/LdapGroupsMapping.java,setConf,org.apache.hadoop.security.LdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),767,864,"/**
 * Configures LDAP settings from the provided configuration.
 * @param conf Configuration object containing LDAP settings.
 */
",,,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,start,org.apache.hadoop.util.curator.ZKCuratorManager:start(java.util.List),138,140,"/**
 * Starts the process using provided authentication information.
 * @param authInfos List of authentication details to use.
 */
","* Start the connection to the ZooKeeper ensemble.
   * @param authInfos List of authentication keys.
   * @throws IOException If the connection cannot be started.",,,True,20
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,init,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:init(java.util.Properties),127,132,"/**
 * Initializes the servlet, configuring authentication, tokens, and JSON factory.
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/local/LocalFs.java,<init>,"org.apache.hadoop.fs.local.LocalFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",49,52,"/**
 * Constructs a LocalFs object using the provided configuration.
 * @param theUri The URI for the local filesystem.
 * @param conf The configuration object.
 */
","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   * 
   * @param theUri which must be that of localFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,copyStreamToTarget,"org.apache.hadoop.fs.shell.CommandWithDestination:copyStreamToTarget(java.io.InputStream,org.apache.hadoop.fs.shell.PathData)",418,434,"/**
 * Copies an input stream to a target PathData, handling overwrites.
 * @param in Input stream to copy.
 * @param target Target PathData to write to.
 * @throws IOException if an I/O error occurs.
 */
","* If direct write is disabled ,copies the stream contents to a temporary
   * file ""target._COPYING_"". If the copy is successful, the temporary file
   * will be renamed to the real path, else the temporary file will be deleted.
   * if direct write is enabled , then creation temporary file is skipped.
   *
   * @param in     the input stream for the copy
   * @param target where to store the contents of the stream
   * @throws IOException if copy fails",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,getTargetPath,org.apache.hadoop.fs.shell.CommandWithDestination:getTargetPath(org.apache.hadoop.fs.shell.PathData),330,342,"/**
 * Determines the target PathData based on the source PathData.
 * Returns the target PathData object.
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,cleanupAllTmpFiles,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:cleanupAllTmpFiles(),399,407,"/**
 * Deletes all temporary files associated with the output streams.
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,commit,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:commit(),409,444,"/**
 * Commits changes to nodes, handling potential IOExceptions.
 * Throws MultipleIOException if commit fails for some nodes.
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,delete,"org.apache.hadoop.fs.viewfs.NflyFSystem:delete(org.apache.hadoop.fs.Path,boolean)",768,793,"/**
 * Deletes a file/directory, recursively if specified.
 * @param f Path to delete
 * @param recursive Whether to delete recursively
 * @return True if all deletes succeeded, false otherwise.
 */
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",426,433,"/**
 * Copies a file or directory from srcFS to dstFS.
 * @param src Source path
 * @param dst Destination path
 * @return True if copy successful, false otherwise.
 */
","* Copy files between FileSystems.
   *
   * @param srcFS srcFs.
   * @param src src.
   * @param dstFS dstFs.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param overwrite overwrite.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return true if the operation succeeded.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,repairAndOpen,"org.apache.hadoop.fs.viewfs.NflyFSystem:repairAndOpen(org.apache.hadoop.fs.viewfs.NflyFSystem$MRNflyNode[],org.apache.hadoop.fs.Path,int)",636,713,"/**
* Repairs and opens a file, attempting repair if necessary.
* @param mrNodes Array of MRNflyNodes to consider.
* @param f Path of the file to open.
* @param bufferSize Buffer size for opening the file.
* @return FSDataInputStream or null if open fails.
*/
","* Iterate all available nodes in the proximity order to attempt repair of all
   * FileNotFound nodes.
   *
   * @param mrNodes work set copy of nodes
   * @param f path to repair and open
   * @param bufferSize buffer size for read RPC
   * @return the closest/most recent replica stream AFTER repair",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.io.File,boolean,org.apache.hadoop.conf.Configuration)",570,575,"/**
 * Copies a file from a FileSystem to a local file.
 * @param srcFS Source FileSystem, dst Destination file,
 * @param deleteSource Delete source file after copy?
 * @param conf Hadoop configuration.
 */
","* Copy FileSystem files to local files.
   *
   * @param srcFS srcFs.
   * @param src src.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return true if the operation succeeded.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,openForSequentialIO,org.apache.hadoop.fs.shell.PathData:openForSequentialIO(),621,624,"/**
 * Opens a file for sequential read operations.
 * @return FSDataInputStream for sequential IO.
 * @throws IOException if an I/O error occurs.
 */
","* Open a file for sequential IO.
   * <p>
   * This uses FileSystem.openFile() to request sequential IO;
   * the file status is also passed in.
   * Filesystems may use to optimize their IO.
   * </p>
   * @return an input stream
   * @throws IOException failure",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,dumpToOffset,org.apache.hadoop.fs.shell.Head:dumpToOffset(org.apache.hadoop.fs.shell.PathData),72,77,"/**
 * Copies data from a PathData file to System.out.
 * @param item PathData object containing the file to copy.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,dumpFromOffset,"org.apache.hadoop.fs.shell.Tail:dumpFromOffset(org.apache.hadoop.fs.shell.PathData,long)",105,121,"/**
* Dumps file data from a given offset.
* @param item PathData object, @param offset starting offset
* @return New offset after dumping data.
*/
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,openFile,org.apache.hadoop.fs.viewfs.ChRootedFileSystem:openFile(org.apache.hadoop.fs.Path),489,493,"/**
 * Opens a file using the parent class, using the provided path.
 * @param path The path to the file to open.
 * @return FutureDataInputStreamBuilder for the opened file.
 */
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/JsonSerialization.java,load,"org.apache.hadoop.util.JsonSerialization:load(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",248,250,"/**
 * Loads a resource.
 * @param fs FileSystem to use.
 * @param path Path to the resource.
 * @return The loaded resource.
 */
","* Load from a Hadoop filesystem.
   * @param fs filesystem
   * @param path path
   * @return a loaded object
   * @throws PathIOException JSON parse problem
   * @throws IOException IO problems",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNumNear,org.apache.hadoop.io.file.tfile.TFile$Reader:getRecordNumNear(long),1048,1050,"/**
 * Gets the record number near the given offset.
 * @param offset The offset to find a nearby record.
 * @return The record number near the offset.
 */
","* Get the RecordNum for the first key-value pair in a compressed block
     * whose byte offset in the TFile is greater than or equal to the specified
     * offset.
     * 
     * @param offset
     *          the user supplied offset.
     * @return the RecordNum to the corresponding entry. If no such entry
     *         exists, it returns the total entry count.
     * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,getRecordNum,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:getRecordNum(),1629,1631,"/**
 * Gets the record number at the current location.
 * @return Record number at the current location.
 */
","* Get the RecordNum corresponding to the entry pointed by the cursor.
       * @return The RecordNum corresponding to the entry pointed by the cursor.
       * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,org.apache.hadoop.io.file.tfile.TFile$Reader$Location,org.apache.hadoop.io.file.tfile.TFile$Reader$Location)",1281,1303,"/**
 * Initializes a Scanner with a Reader and location boundaries.
 * @param reader Source Reader; @param begin, @param end Location bounds.
 * @throws IOException if an I/O error occurs.
 */","* Constructor
       * 
       * @param reader
       *          The TFile reader object.
       * @param begin
       *          Begin location of the scan.
       * @param end
       *          End location of the scan.
       * @throws IOException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.TFile$Reader$Location),1396,1429,"/**
 * Seeks to the specified location within the data stream.
 * @param l The target Location to seek to.
 * @throws IllegalArgumentException if location is out of bounds.
 */
","* Move the cursor to the new location. The entry returned by the previous
       * entry() call will be invalid.
       * 
       * @param l
       *          new cursor location. It must fall between the begin and end
       *          location of the scanner.
       * @throws IOException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,advance,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:advance(),1521,1541,"/**
 * Advances the cursor to the next record. Returns false if at end.
 */","* Move the cursor to the next key-value pair. The entry returned by the
       * previous entry() call will be invalid.
       * 
       * @return true if the cursor successfully moves. False when cursor is
       *         already at the end location and cannot be advanced.
       * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,append,"org.apache.hadoop.io.file.tfile.TFile$Writer:append(byte[],byte[])",355,357,"/**
 * Appends a key-value pair to the internal buffer.
 * @param key The key as a byte array.
 * @param value The value as a byte array.
 */
","* Adding a new key-value pair to the TFile. This is synonymous to
     * append(key, 0, key.length, value, 0, value.length)
     * 
     * @param key
     *          Buffer for key.
     * @param value
     *          Buffer for value.
     * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,flush,org.apache.hadoop.crypto.key.JavaKeyStoreProvider:flush(),529,584,"/**
 * Flushes changes to the keystore, backing up and renaming files.
 * Renames old/new paths, writes to keystore, and updates state.
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,seekInternal,org.apache.hadoop.io.MapFile$Reader:seekInternal(org.apache.hadoop.io.WritableComparable),704,707,"/**
* Seeks to a given key.
* @param key The key to seek to.
* @throws IOException if an I/O error occurs.
*/
","* Positions the reader at the named key, or if none such exists, at the
     * first entry after the named key.
     *
     * @return  0   - exact match found
     *          < 0 - positioned at next record
     *          1   - no more records in file",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getClosest,"org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable,boolean)",859,875,"/**
 * Finds the closest key to the given key, before or after.
 * @param key The key to search near.
 * @param val Writable value object.
 * @param before True for before, false for after.
 * @return The closest WritableComparable key.
 */
","* Finds the record that is the closest match to the specified key.
     * 
     * @param key       - key that we're trying to find
     * @param val       - data value if key is found
     * @param before    - IF true, and <code>key</code> does not exist, return
     * the first entry that falls just before the <code>key</code>.  Otherwise,
     * return the record that sorts just after.
     * @return          - the key that was the closest match or null if eof.
     * @throws IOException raised on errors performing I/O.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",90,95,"/**
 * Gets a protocol proxy.
 * @param protocol Protocol class.
 * @param version Client version.
 * @return ProtocolProxy object.
 */
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getProxy,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",86,91,"/**
 * Gets a proxy for the given protocol.
 * @param protocol Protocol class.
 * @param clientVersion Client version.
 * @return ProtocolProxy object.
 */
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isAuthenticationMethodEnabled,org.apache.hadoop.security.UserGroupInformation:isAuthenticationMethodEnabled(org.apache.hadoop.security.UserGroupInformation$AuthenticationMethod),391,396,"/**
 * Checks if the specified authentication method is enabled.
 * @param method The authentication method to check.
 * @return True if enabled, false otherwise.
 */
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isKerberosKeyTabLoginRenewalEnabled,org.apache.hadoop.security.UserGroupInformation:isKerberosKeyTabLoginRenewalEnabled(),398,404,"/**
 * Checks if Kerberos keytab login renewal is enabled.
 */
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getKerberosLoginRenewalExecutor,org.apache.hadoop.security.UserGroupInformation:getKerberosLoginRenewalExecutor(),406,412,"/**
 * Returns the ExecutorService for Kerberos login renewal.
 * Returns Optional.empty() if not initialized.
 */
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createUserForTesting,"org.apache.hadoop.security.UserGroupInformation:createUserForTesting(java.lang.String,java.lang.String[])",1607,1620,"/**
 * Creates a UserGroupInformation object for testing purposes.
 * @param user User's short username.
 * @param userGroups Array of user group names.
 * @return UserGroupInformation object.
 */
","* Create a UGI for testing HDFS and MapReduce
   * @param user the full user principal name
   * @param userGroups the names of the groups that the user belongs to
   * @return a fake user for running unit tests",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createProxyUserForTesting,"org.apache.hadoop.security.UserGroupInformation:createProxyUserForTesting(java.lang.String,org.apache.hadoop.security.UserGroupInformation,java.lang.String[])",1634,1645,"/**
 * Creates a proxy UserGroupInformation for testing, adding user groups.
 * @param user Proxy user name.
 * @param realUser The real UserGroupInformation.
 * @param userGroups User groups to assign.
 * @return The created proxy UserGroupInformation.
 */
","* Create a proxy user UGI for testing HDFS and MapReduce
   * 
   * @param user
   *          the full user principal name for effective user
   * @param realUser
   *          UGI of the real user
   * @param userGroups
   *          the names of the groups that the user belongs to
   * @return a fake user for running unit tests",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroups,org.apache.hadoop.security.UserGroupInformation:getGroups(),1790,1799,"/**
 * Retrieves the user's groups. Returns empty list on failure.
 */
","* Get the group names for this user. {@link #getGroupsSet()} is less
   * expensive alternative when checking for a contained element.
   * @return the list of users with the primary group first. If the command
   *    fails, it returns an empty list.
   * @deprecated Use {@link #getGroupsSet()} instead.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroupsSet,org.apache.hadoop.security.UserGroupInformation:getGroupsSet(),1806,1814,"/**
 * Returns a set of group names for the current user.
 * Returns an empty set on failure.
 */
","* Get the groups names for the user as a Set.
   * @return the set of users with the primary group first. If the command
   *     fails, it returns an empty set.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,doSubjectLogin,"org.apache.hadoop.security.UserGroupInformation:doSubjectLogin(javax.security.auth.Subject,org.apache.hadoop.security.UserGroupInformation$LoginParams)",2042,2073,"/**
 * Performs a login using provided parameters or defaults.
 * @param subject Subject to login, or null.
 * @param params Login parameters, or null for defaults.
 * @return UserGroupInformation object representing the user.
 */
","* Login a subject with the given parameters.  If the subject is null,
   * the login context used to create the subject will be attached.
   * @param subject to login, null for new subject.
   * @param params for login, null for externally managed ugi.
   * @return UserGroupInformation for subject
   * @throws IOException",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,init,org.apache.hadoop.fs.FsShell:init(),100,109,"/**
 * Initializes the system by setting configuration and registering commands.
 */
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,refreshServiceAclWithLoadedConfiguration,"org.apache.hadoop.ipc.Server:refreshServiceAclWithLoadedConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",778,782,"/**
 * Refreshes the service ACL with a configuration and policy provider.
 */","* Refresh the service authorization ACL for the service handled by this server
   * using the specified Configuration.
   *
   * @param conf input Configuration.
   * @param provider input provider.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,getTestProvider,org.apache.hadoop.security.authorize.DefaultImpersonationProvider:getTestProvider(),52,59,"/**
 * Returns the test impersonation provider, creating it if null.
 */
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,authorize,"org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String)",99,102,"/**
 * Authorizes a user based on user group information and address.
 * @param user UserGroupInformation object
 * @param remoteAddress Remote address string
 * @throws AuthorizationException if authorization fails
 */
","* Authorize the superuser which is doing doAs.
   * {@link #authorize(UserGroupInformation, InetAddress)} should be preferred
   * to avoid possibly re-resolving the ip address.
   *
   * @param user ugi of the effective or proxy user which contains a real user
   * @param remoteAddress the ip address of client
   * @throws AuthorizationException Authorization Exception.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,authorize,"org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)",111,114,"/**
 * Authorizes a user based on their credentials and remote address.
 * @param user UserGroupInformation object
 * @param remoteAddress Remote address of the user
 * @throws AuthorizationException if authorization fails
 */
","* Authorize the superuser which is doing doAs.
   *
   * @param user ugi of the effective or proxy user which contains a real user
   * @param remoteAddress the inet address of client
   * @throws AuthorizationException Authorization Exception.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,getDefaultImpersonationProvider,org.apache.hadoop.security.authorize.ProxyUsers:getDefaultImpersonationProvider(),140,143,"/**
 * Returns the default impersonation provider.
 * Retrieves the provider from the Sip context.
 */",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,launchService,"org.apache.hadoop.service.launcher.ServiceLauncher:launchService(org.apache.hadoop.conf.Configuration,java.util.List,boolean,boolean)",464,469,"/**
 * Launches the service with default error handling.
 * @param conf Configuration object.
 * @param processedArgs List of processed arguments.
 * @param addShutdownHook Whether to add a shutdown hook.
 * @param execute Whether to execute the service.
 */
","* Launch a service catching all exceptions and downgrading them to exit codes
   * after logging.
   *
   * Sets {@link #serviceException} to this value.
   * @param conf configuration to use
   * @param processedArgs command line after the launcher-specific arguments
   * have been stripped out.
   * @param addShutdownHook should a shutdown hook be added to terminate
   * this service on shutdown. Tests should set this to false.
   * @param execute execute/wait for the service to stop.
   * @return an exit exception, which will have a status code of 0 if it worked",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/RuleBasedLdapGroupsMapping.java,setConf,org.apache.hadoop.security.RuleBasedLdapGroupsMapping:setConf(org.apache.hadoop.conf.Configuration),56,66,"/**
 * Sets the conversion rule from the given configuration.
 * @param conf Configuration object containing the rule value.
 */
",,,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/curator/ZKCuratorManager.java,start,org.apache.hadoop.util.curator.ZKCuratorManager:start(),129,131,"/**
 * Starts the process, using a default empty list.
 */","* Start the connection to the ZooKeeper ensemble.
   * @throws IOException If the connection cannot be started.",,,True,21
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/MultiSchemeDelegationTokenAuthenticationHandler.java,init,org.apache.hadoop.security.token.delegation.web.MultiSchemeDelegationTokenAuthenticationHandler:init(java.util.Properties),99,126,"/**
 * Initializes the servlet, configuring HTTP authentication schemes.
 * @param config Servlet configuration properties.
 * @throws ServletException if initialization fails.
 */
",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,copyFileToTarget,"org.apache.hadoop.fs.shell.CommandWithDestination:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",350,367,"/**
 * Copies a file from source to target PathData, handling attributes.
 * @param src Source PathData
 * @param target Target PathData
 * @throws IOException if an I/O error occurs
 */
","* Copies the source file to the target.
   * @param src item to copy
   * @param target where to copy the item
   * @throws IOException if copy fails",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processPathArgument,org.apache.hadoop.fs.shell.CommandWithDestination:processPathArgument(org.apache.hadoop.fs.shell.PathData),247,274,"/**
 * Processes a path argument, handling directory checks and errors.
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,recursePath,org.apache.hadoop.fs.shell.CommandWithDestination:recursePath(org.apache.hadoop.fs.shell.PathData),299,328,"/**
 * Recursively processes a path, updating destination and handling attributes.
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,close,org.apache.hadoop.fs.viewfs.NflyFSystem$NflyOutputStream:close(),378,397,"/**
 * Closes output streams and commits changes, throwing an IOException if replication fails.
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,org.apache.hadoop.conf.Configuration)",366,371,"/**
 * Copies a file or directory from src to dst.
 * @param deleteSource Whether to delete src after copy.
 * @param conf Hadoop configuration.
 * @return True if copy succeeded, false otherwise.
 */
","* Copy files between FileSystems.
   * @param srcFS src fs.
   * @param src src.
   * @param dstFS dst fs.
   * @param dst dst.
   * @param deleteSource delete source.
   * @param conf configuration.
   * @return if copy success true, not false.
   * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,copy,"org.apache.hadoop.fs.FileUtil:copy(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,boolean,boolean,org.apache.hadoop.conf.Configuration)",373,411,"/**
 * Copies multiple files/directories from srcFS to dstFS.
 * @param srcs Source paths to copy.
 * @param dst Destination path (must be a directory).
 * @return True if all copies were successful.
 */
",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,open,"org.apache.hadoop.fs.viewfs.NflyFSystem:open(org.apache.hadoop.fs.Path,int)",579,621,"/**
 * Opens a data input stream for the given path.
 * @param f Path to open.
 * @param bufferSize I/O buffer size.
 * @throws IOException if an I/O error occurs.
 */
","* Category: READ.
   *
   * @param f the file name to open
   * @param bufferSize the size of the buffer to be used.
   * @return input stream according to nfly flags (closest, most recent)
   * @throws IOException
   * @throws FileNotFoundException iff all destinations generate this exception",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processArguments,org.apache.hadoop.fs.shell.CopyCommands$Merge:processArguments(java.util.LinkedList),91,114,"/**
 * Processes path data, copies file contents, and writes delimiters.
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,getInputStream,org.apache.hadoop.fs.shell.Display$Cat:getInputStream(org.apache.hadoop.fs.shell.PathData),106,109,"/**
* Opens an input stream for sequential reading of PathData.
* @param item The PathData object to open.
* @throws IOException if an I/O error occurs.
*/
",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,processPath,org.apache.hadoop.fs.shell.Head:processPath(org.apache.hadoop.fs.shell.PathData),63,70,"/**
 * Processes a PathData item. Throws exception if directory.
 * Dumps data to offset.
 */
",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,processPath,org.apache.hadoop.fs.shell.Tail:processPath(org.apache.hadoop.fs.shell.PathData),88,103,"/**
 * Processes a PathData item, throwing exception if directory.
 * Dumps data from offset, repeatedly with delay if follow.
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScanner,org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(),1077,1079,"/**
 * Creates a Scanner for parsing the input stream.
 * @return A Scanner instance configured with begin and end.
 */
","* Get a scanner than can scan the whole TFile.
     * 
     * @return The scanner object. A valid Scanner is always returned even if
     *         the TFile is empty.
     * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByRecordNum,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByRecordNum(long,long)",1194,1202,"/**
 * Creates a Scanner for records between beginRecNum and endRecNum.
 * @param beginRecNum Start record number (inclusive).
 * @param endRecNum End record number (inclusive).
 */
","* Create a scanner that covers a range of records.
     * 
     * @param beginRecNum
     *          The RecordNum for the first record (inclusive).
     * @param endRecNum
     *          The RecordNum for the last record (exclusive). To scan the whole
     *          file, either specify endRecNum==-1 or endRecNum==getEntryCount().
     * @return The TFile scanner that covers the specified range of records.
     * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,long,long)",1264,1268,"/**
 * Constructs a Scanner with specified begin/end offsets.
 * @param reader The Reader to read from.
 * @param offBegin Start offset.
 * @param offEnd End offset.
 */
","* Constructor
       * 
       * @param reader
       *          The TFile reader object.
       * @param offBegin
       *          Begin byte-offset of the scan.
       * @param offEnd
       *          End byte-offset of the scan.
       * @throws IOException
       * 
       *           The offsets will be rounded to the beginning of a compressed
       *           block whose offset is greater than or equal to the specified
       *           offset.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(org.apache.hadoop.io.file.tfile.RawComparable,boolean)",1366,1385,"/**
* Seeks to a location based on the key, potentially beyond the end.
* @param key The key to seek to.
* @param beyond Whether to seek beyond the key.
* @return True if seek was successful, false otherwise.
*/
",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,rewind,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:rewind(),1437,1439,"/**
 * Rewinds the stream to the beginning location.
 */","* Rewind to the first entry in the scanner. The entry returned by the
       * previous entry() call will be invalid.
       * 
       * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,seek,org.apache.hadoop.io.MapFile$Reader:seek(org.apache.hadoop.io.WritableComparable),692,694,"/**
 * Seeks to a specific key in the input stream.
 * @param key The key to seek to.
 * @return True if seek was successful, false otherwise.
 */
","* Positions the reader at the named key, or if none such exists, at the
     * first entry after the named key.  Returns true iff the named key exists
     * in this map.
     *
     * @param key key.
     * @throws IOException raised on errors performing I/O.
     * @return if the named key exists in this map true, not false.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,getClosest,"org.apache.hadoop.io.MapFile$Reader:getClosest(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",842,846,"/**
 * Gets the closest WritableComparable to the given key.
 * @param key The key to find the closest comparable to.
 * @param val Unused parameter.
 * @return The closest WritableComparable.
 */
","* Finds the record that is the closest match to the specified key.
     * Returns <code>key</code> or if it does not exist, at the first entry
     * after the named key.
     * 
     * @param key key that we're trying to find.
     * @param val data value if key is found.
     * @return the key that was the closest match or null if eof.
     * @throws IOException raised on errors performing I/O.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isSecurityEnabled,org.apache.hadoop.security.UserGroupInformation:isSecurityEnabled(),387,389,"/**
 * Checks if security is enabled.
 * Returns true if simple authentication is disabled.
 */
","* Determine if UserGroupInformation is using Kerberos to determine
   * user identities or is relying on simple authentication
   * 
   * @return true if UGI is working in a secure environment",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logoutUserFromKeytab,org.apache.hadoop.security.UserGroupInformation:logoutUserFromKeytab(),1158,1189,"/**
 * Logs out the user from the keytab, shutting down renewal tasks.
 * Throws KerberosAuthException on failure.
 */
","* Log the current user out who previously logged in using keytab.
   * This method assumes that the user logged in by calling
   * {@link #loginUserFromKeytab(String, String)}.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if a failure occurred in logout,
   * or if the user did not log in by invoking loginUserFromKeyTab() before.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,checkStat,"org.apache.hadoop.io.SecureIOUtils:checkStat(java.io.File,java.lang.String,java.lang.String,java.lang.String,java.lang.String)",282,303,"/**
 * Checks file owner against expected owner, throws IOException if mismatch.
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getPrimaryGroupName,org.apache.hadoop.security.UserGroupInformation:getPrimaryGroupName(),1655,1661,"/**
 * Returns the primary group name.
 * @throws IOException if no primary group exists.
 */
",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getGroupNames,org.apache.hadoop.security.UserGroupInformation:getGroupNames(),1778,1781,"/**
 * Returns an array of group names from the internal set.
 */","* Get the group names for this user. {@link #getGroupsSet()} is less
   * expensive alternative when checking for a contained element.
   * @return the list of users with the primary group first. If the command
   *    fails, it returns an empty list.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,isUserInList,org.apache.hadoop.security.authorize.AccessControlList:isUserInList(org.apache.hadoop.security.UserGroupInformation),235,249,"/**
 * Checks if a user is in the list, considering users, groups, or real user ACLs.
 * @param ugi UserGroupInformation object to check.
 * @return True if user is in the list, false otherwise.
 */
","* Checks if a user represented by the provided {@link UserGroupInformation}
   * is a member of the Access Control List. If user was proxied and
   * USE_REAL_ACLS + the real user name is in the control list, then treat this
   * case as if user were in the ACL list.
   * @param ugi UserGroupInformation to check if contained in the ACL
   * @return true if ugi is member of the list or if USE_REAL_ACLS + real user
   * is in the list",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getUGIFromSubject,org.apache.hadoop.security.UserGroupInformation:getUGIFromSubject(javax.security.auth.Subject),651,664,"/**
 * Obtains UserGroupInformation from a Subject.
 * @param subject The Subject to extract UGI from.
 * @throws IOException if Subject is invalid or missing principal.
 */
","* Create a UserGroupInformation from a Subject with Kerberos principal.
   *
   * @param subject             The KerberosPrincipal to use in UGI.
   *                            The creator of subject is responsible for
   *                            renewing credentials.
   *
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if the kerberos login fails
   * @return UserGroupInformation",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,createLoginUser,org.apache.hadoop.security.UserGroupInformation:createLoginUser(javax.security.auth.Subject),731,803,"/**
 * Creates a login UserGroupInformation, potentially as a proxy user.
 * @param subject The Subject to login with.
 * @return UserGroupInformation object.
 */
",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authentication/server/ProxyUserAuthenticationFilter.java,doFilter,"org.apache.hadoop.security.authentication.server.ProxyUserAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",60,105,"/**
 * Filters a request, potentially impersonating a user via 'doAs'.
 * @param filterChain FilterChain to pass to next filter
 * @param request HTTP request
 * @param response HTTP response
 */
",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationFilter.java,doFilter,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationFilter:doFilter(javax.servlet.FilterChain,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",243,308,"/**
 * Filters an HTTP request, authenticating the user and setting context.
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticationHandler.java,managementOperation,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticationHandler:managementOperation(org.apache.hadoop.security.authentication.server.AuthenticationToken,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",220,355,"/**
 * Processes management operation based on request and token.
 * @param token Authentication token, may be null.
 * @param request HTTP request object.
 * @param response HTTP response object.
 * @return True if request should continue, false otherwise.
 */",,,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ProxyUsers.java,authorize,"org.apache.hadoop.security.authorize.ProxyUsers:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,org.apache.hadoop.conf.Configuration)",134,138,"/**
 * Authorizes a user, DEPRECATED. Calls the non-config version.
 * @param user UserGroupInformation to authorize
 * @param remoteAddress Remote address of the request
 * @throws AuthorizationException if authorization fails
 */
","* This function is kept to provide backward compatibility.
   * @param user user.
   * @param remoteAddress remote address.
   * @param conf configuration.
   * @throws AuthorizationException Authorization Exception.
   * @deprecated use {@link #authorize(UserGroupInformation, String)} instead.",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,authorizeConnection,org.apache.hadoop.ipc.Server$Connection:authorizeConnection(),3018,3039,"/**
 * Authorizes a connection, potentially using ProxyUsers.
 * Throws FatalRpcServerException on authorization failure.
 */","* Authorize proxy users to access this server
     * @throws RpcServerException - user is not allowed to proxy",,,True,22
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommandWithMultiThread.java,copyFileToTarget,"org.apache.hadoop.fs.shell.CopyCommandWithMultiThread:copyFileToTarget(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",140,154,"/**
 * Copies a file to the target path, optionally using an executor.
 * @param src Source PathData
 * @param target Target PathData
 */
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processPath,"org.apache.hadoop.fs.shell.CommandWithDestination:processPath(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.shell.PathData)",287,297,"/**
* Processes a path, copying files or throwing exceptions.
* Handles symlinks, files, and directories based on flags.
*/
","* Called with a source and target destination pair
   * @param src for the operation
   * @param dst for the operation
   * @throws IOException if anything goes wrong",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.HarFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",845,849,"/**
 * Copies a file to the local file system.
 * @param delSrc Whether to delete the source file after copy.
 * @param src Source file path.
 * @param dst Destination file path.
 */
",* copies the file in the har filesystem to a local file.,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,rename,"org.apache.hadoop.fs.RawLocalFileSystem:rename(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",624,644,"/**
* Renames a file or directory from src to dst, handling Windows rename.
* @param src Source path
* @param dst Destination path
* @return True if rename succeeds, false otherwise.
*/
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.ChecksumFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",991,996,"/**
 * Copies a local file to the destination path.
 * @param delSrc Whether to delete the source file after copy.
 * @param src Source file path.
 * @param dst Destination file path.
 */
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",1002,1007,"/**
* Copies a file from source to local destination, deleting source if specified.
* @param delSrc Delete source file after copy?
* @param src Source file path
* @param dst Destination file path
*/
","* The src file is under FS, and the dst is on the local disk.
   * Copy it from FS control to the local dst name.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.LocalFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",83,87,"/**
 * Copies a file from a local source to a destination, optionally deleting source.
 * @param delSrc Whether to delete the source file after copying.
 * @param src Source file path.
 * @param dst Destination file path.
 */
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.LocalFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",89,93,"/**
 * Copies a file to a local destination.
 * @param delSrc Delete source file after copy.
 * @param src Source file path.
 * @param dst Destination file path.
 */
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,processPath,org.apache.hadoop.fs.shell.Display$Cat:processPath(org.apache.hadoop.fs.shell.PathData),88,96,"/**
 * Processes a PathData item, throwing exception if directory.
 * Sets checksum verification and prints input stream.
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByByteRange,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByByteRange(long,long)",1094,1096,"/**
 * Creates a Scanner for a byte range.
 * @param offset Start offset in bytes.
 * @param length Number of bytes to read.
 * @return Scanner instance for the specified range.
 */
","* Get a scanner that covers a portion of TFile based on byte offsets.
     * 
     * @param offset
     *          The beginning byte offset in the TFile.
     * @param length
     *          The length of the region.
     * @return The actual coverage of the returned scanner tries to match the
     *         specified byte-region but always round up to the compression
     *         block boundaries. It is possible that the returned scanner
     *         contains zero key-value pairs even if length is positive.
     * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,<init>,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:<init>(org.apache.hadoop.io.file.tfile.TFile$Reader,org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1318,1331,"/**
 * Constructs a Scanner with custom block keys.
 * @param reader Reader to read from
 * @param beginKey Block start key
 * @param endKey Block end key
 * @throws IOException if I/O error occurs
 */
","* Constructor
       * 
       * @param reader
       *          The TFile reader object.
       * @param beginKey
       *          Begin key of the scan. If null, scan from the first
       *          &lt;K, V&gt; entry of the TFile.
       * @param endKey
       *          End key of the scan. If null, scan up to the last &lt;K, V&gt;
       *          entry of the TFile.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[],int,int)",1361,1364,"/**
 * Seeks to the specified key position.
 * @param key Key to seek to.
 * @param keyOffset Offset in the key.
 * @param keyLen Length of the key.
 * @return True if seek was successful.
 */
","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. The entry returned by the previous entry() call will
       * be invalid.
       * 
       * @param key
       *          The input key
       * @param keyOffset
       *          offset in the key buffer.
       * @param keyLen
       *          key buffer length.
       * @return true if we find an equal key; false otherwise.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,lowerBound,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[],int,int)",1477,1480,"/**
* Seeks to a position based on the provided byte array.
* @param key byte array used for seeking.
*/
","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. The entry returned by the previous entry() call will
       * be invalid.
       * 
       * @param key
       *          The input key
       * @param keyOffset
       *          offset in the key buffer.
       * @param keyLen
       *          key buffer length.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,upperBound,"org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[],int,int)",1508,1511,"/**
 * Seeks to a position in the stream using a byte array as a marker.
 */","* Move the cursor to the first entry whose key is strictly greater than
       * the input key. The entry returned by the previous entry() call will be
       * invalid.
       * 
       * @param key
       *          The input key
       * @param keyOffset
       *          offset in the key buffer.
       * @param keyLen
       *          key buffer length.
       * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,seek,org.apache.hadoop.io.SetFile$Reader:seek(org.apache.hadoop.io.WritableComparable),130,134,"/**
* Seeks to a given key using the superclass implementation.
* @param key The key to seek to.
* @return True if seek was successful.
*/
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,get,"org.apache.hadoop.io.MapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",823,830,"/**
 * Gets the value associated with the given key.
 * @param key The key to search for.
 * @param val The Writable to store the value in.
 * @return The value associated with the key, or null if not found.
 */
","* Return the value for the named key, or null if none exists.
     * @param key key.
     * @param val val.
     * @return Writable if such a pair exists true,not false.
     * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,commit,org.apache.hadoop.security.UserGroupInformation$HadoopLoginModule:commit(),189,235,"/**
 * Commits the login process, attempting to find and add a user.
 * @return True if commit was successful, false otherwise.
 * @throws LoginException if user cannot be found.
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,org.apache.hadoop.ipc.AlignmentContext)",579,587,"/**
 * Gets a ProtocolProxy for the given protocol, version, and context.
 * @param protocol Protocol class
 * @return ProtocolProxy instance
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T
   * @param protocol protocol class
   * @param clientVersion client's version
   * @param connId client connection identifier
   * @param conf configuration
   * @param factory socket factory
   * @param alignmentContext StateID alignment context
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean)",661,677,"/**
 * Gets a ProtocolProxy for the given protocol and configuration.
 * @param protocol Protocol class.
 * @return ProtocolProxy instance.
 * @throws IOException if an I/O error occurs.
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @param connectionRetryPolicy retry policy
   * @param fallbackToSimpleAuth set to true or false during calls to indicate if
   *   a secure client falls back to simple auth
   * @return the proxy
   * @throws IOException if any error occurs",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy,java.util.concurrent.atomic.AtomicBoolean,org.apache.hadoop.ipc.AlignmentContext)",698,715,"/**
 * Gets a ProtocolProxy for the given protocol.
 * @param protocol Protocol class.
 * @return ProtocolProxy instance.
 * @throws IOException if an I/O error occurs.
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @param connectionRetryPolicy retry policy
   * @param fallbackToSimpleAuth set to true or false during calls to indicate
   *   if a secure client falls back to simple auth
   * @param alignmentContext state alignment context
   * @param <T> Generics Type T.
   * @return the proxy
   * @throws IOException if any error occurs",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setFallBackToSimpleAuth,org.apache.hadoop.ipc.Client$Connection:setFallBackToSimpleAuth(java.util.concurrent.atomic.AtomicBoolean),858,890,"/**
 * Sets fallback to SIMPLE authentication based on configuration.
 * @param fallbackToSimpleAuth AtomicBoolean to control fallback.
 * @throws AccessControlException if secure connections are required.
 */
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,forceSecureOpenForRandomRead,"org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRandomRead(java.io.File,java.lang.String,java.lang.String,java.lang.String)",126,143,"/**
 * Opens a file for random read access, verifying owner/group.
 * @param f file to open
 * @param mode access mode (e.g., ""r"")
 * @param expectedOwner expected owner
 * @param expectedGroup expected group
 * @return RandomAccessFile object
 * @throws IOException if an I/O error occurs
 */
","* @return Same as openForRandomRead except that it will run even if security is off.
   * This is used by unit tests.
   *
   * @param f input f.
   * @param mode input mode.
   * @param expectedOwner input expectedOwner.
   * @param expectedGroup input expectedGroup.
   * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,forceSecureOpenFSDataInputStream,"org.apache.hadoop.io.SecureIOUtils:forceSecureOpenFSDataInputStream(java.io.File,java.lang.String,java.lang.String)",174,192,"/**
 * Opens a FSDataInputStream with security checks.
 * @param file The file to open.
 * @param expectedOwner Expected owner.
 * @param expectedGroup Expected group.
 * @return FSDataInputStream or throws IOException.
 */
","* Same as openFSDataInputStream except that it will run even if security is
   * off. This is used by unit tests.
   *
   * @param file input file.
   * @param expectedOwner input expectedOwner.
   * @param expectedGroup input expectedGroup.
   * @throws IOException raised on errors performing I/O.
   * @return FSDataInputStream.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,forceSecureOpenForRead,"org.apache.hadoop.io.SecureIOUtils:forceSecureOpenForRead(java.io.File,java.lang.String,java.lang.String)",224,241,"/**
 * Opens a FileInputStream, verifying owner/group permissions.
 * @param f file to open
 * @param expectedOwner expected owner
 * @param expectedGroup expected group
 * @return FileInputStream
 */
","* @return Same as openForRead() except that it will run even if security is off.
   * This is used by unit tests.
   * @param f input f.
   * @param expectedOwner input expectedOwner.
   * @param expectedGroup input expectedGroup.
   * @throws IOException raised on errors performing I/O.",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileStatus(org.apache.hadoop.fs.Path),1080,1087,"/**
 * Gets the file status for a given path.
 * @param f The path to get the status for.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getFileLinkStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getFileLinkStatus(org.apache.hadoop.fs.Path),1089,1128,"/**
 * Gets the FileStatus for a given Path, handling links and errors.
 * @param f the Path to get the FileStatus for
 * @return FileStatus object
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path),1406,1413,"/**
 * Gets the ACL status for the given path.
 * @param path The path to check.
 * @return AclStatus object representing the ACL status.
 */
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getFileStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getFileStatus(org.apache.hadoop.fs.Path),1544,1551,"/**
 * Gets the file status for the given path.
 * @param f The path to get the status for.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path),1554,1620,"/**
 * Lists file statuses for a given path, handling links and fallbacks.
 * @param f the path to list statuses for
 * @return array of FileStatus objects
 */",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getAclStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getAclStatus(org.apache.hadoop.fs.Path),1832,1839,"/**
 * Gets the ACL status for a given path.
 * @param path The path to check.
 * @return AclStatus object representing the ACL status.
 */
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,dumpUGI,"org.apache.hadoop.security.KDiag:dumpUGI(java.lang.String,org.apache.hadoop.security.UserGroupInformation)",666,690,"/**
 * Dumps UserGroupInformation details including credentials & groups.
 * @param title Section title for the dump
 * @param ugi The UserGroupInformation to dump
 */
","* Dump a UGI.
   *
   * @param title title of this section
   * @param ugi UGI to dump
   * @throws IOException",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,print,org.apache.hadoop.security.UserGroupInformation:print(),2022,2032,"/**
 * Prints user details, including name and group names.
 */
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/AccessControlList.java,isUserAllowed,org.apache.hadoop.security.authorize.AccessControlList:isUserAllowed(org.apache.hadoop.security.UserGroupInformation),251,253,"/**
 * Checks if a user is allowed based on group membership.
 * @param ugi UserGroupInformation object to check
 * @return True if user is allowed, false otherwise.
 */
",,,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getLoginUser,org.apache.hadoop.security.UserGroupInformation:getLoginUser(),673,698,"/**
 * Retrieves the login user. Lazily initializes if not already set.
 */","* Get the currently logged in user.  If no explicit login has occurred,
   * the user will automatically be logged in with either kerberos credentials
   * if available, or as the local OS user, based on security settings.
   * @return the logged in user
   * @throws IOException if login fails",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,loginUserFromSubject,org.apache.hadoop.security.UserGroupInformation:loginUserFromSubject(javax.security.auth.Subject),725,729,"/**
 * Logs in a user based on the provided Subject.
 * @param subject The Subject representing the user to log in.
 */
","* Log in a user using the given subject
   * @param subject the subject to use when logging in a user, or null to
   * create a new subject.
   *
   * If subject is not null, the creator of subject is responsible for renewing
   * credentials.
   *
   * @throws IOException if login fails",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processConnectionContext,org.apache.hadoop.ipc.Server$Connection:processConnectionContext(org.apache.hadoop.ipc.RpcWritable$Buffer),2678,2724,"/**
 * Processes the connection context from the buffer.
 * Extracts protocol details and authenticates the user.
 */","Reads the connection context following the connection header
     * @throws RpcServerException - if the header cannot be
     *         deserialized, or the user is not authorized",,,True,23
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,processPath,org.apache.hadoop.fs.shell.CommandWithDestination:processPath(org.apache.hadoop.fs.shell.PathData),276,279,"/**
 * Processes a path, delegating to a helper with target path.
 * @param src The source PathData to process.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/RawLocalFileSystem.java,moveFromLocalFile,"org.apache.hadoop.fs.RawLocalFileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",877,880,"/**
* Moves a file from the source path to the destination path.
* @param src Source path of the file.
* @param dst Destination path for the file.
*/
",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByKey,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1174,1181,"/**
 * Creates a Scanner with specified key range.
 * @param beginKey Start key for the scanner.
 * @param endKey End key for the scanner.
 */
","* Get a scanner that covers a specific key range.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,seekTo,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:seekTo(byte[]),1343,1345,"/**
 * Seeks to a specific position based on the provided key.
 * @param key The key to seek to.
 * @throws IOException If an I/O error occurs.
 */
","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. Synonymous to seekTo(key, 0, key.length). The entry
       * returned by the previous entry() call will be invalid.
       * 
       * @param key
       *          The input key
       * @return true if we find an equal key.
       * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,lowerBound,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:lowerBound(byte[]),1460,1462,"/**
 * Finds the lower bound of key in a byte array.
 * @param key The byte array to search within.
 */","* Move the cursor to the first entry whose key is greater than or equal
       * to the input key. Synonymous to lowerBound(key, 0, key.length). The
       * entry returned by the previous entry() call will be invalid.
       * 
       * @param key
       *          The input key
       * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,upperBound,org.apache.hadoop.io.file.tfile.TFile$Reader$Scanner:upperBound(byte[]),1491,1493,"/**
* Calls upperBound with start and end indices.
* @param key The byte array to search within.
*/
","* Move the cursor to the first entry whose key is strictly greater than
       * the input key. Synonymous to upperBound(key, 0, key.length). The entry
       * returned by the previous entry() call will be invalid.
       * 
       * @param key
       *          The input key
       * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,get,org.apache.hadoop.io.SetFile$Reader:get(org.apache.hadoop.io.WritableComparable),156,163,"/**
* Retrieves the next record with the given key.
* @param key The key to seek for the next record.
* @return The key of the next record or null if not found.
*/
","* Read the matching key from a set into <code>key</code>.
     *
     * @param key input key.
     * @return Returns <code>key</code>, or null if no match exists.
     * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,get,"org.apache.hadoop.io.BloomMapFile$Reader:get(org.apache.hadoop.io.WritableComparable,org.apache.hadoop.io.Writable)",281,288,"/**
 * Gets the value associated with the given key.
 * Returns null if the key is not likely to exist.
 */","* Fast version of the
     * {@link MapFile.Reader#get(WritableComparable, Writable)} method. First
     * it checks the Bloom filter for the existence of the key, and only if
     * present it performs the real get operation. This yields significant
     * performance improvements for get operations on sparsely populated files.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,org.apache.hadoop.ipc.Client$ConnectionId,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",558,563,"/**
 * Gets a ProtocolProxy instance.
 * @param protocol Protocol class
 * @param version Client version
 * @return ProtocolProxy instance
 * @throws IOException on error
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T
   * @param protocol protocol class
   * @param clientVersion client's version
   * @param connId client connection identifier
   * @param conf configuration
   * @param factory socket factory
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int,org.apache.hadoop.io.retry.RetryPolicy)",631,641,"/**
 * Gets a ProtocolProxy for the given protocol.
 * @param protocol Protocol class.
 * @return ProtocolProxy instance.
 * @throws IOException if an I/O error occurs.
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @param connectionRetryPolicy retry policy
   * @return the proxy
   * @throws IOException if any error occurs",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,setupIOstreams,org.apache.hadoop.ipc.Client$Connection:setupIOstreams(java.util.concurrent.atomic.AtomicBoolean),764,856,"/**
 * Sets up IO streams for IPC communication, handling authentication.
 */","Connect to the server and set up the I/O streams. It then sends
     * a header to the server and starts
     * the connection thread that waits for responses.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,openForRandomRead,"org.apache.hadoop.io.SecureIOUtils:openForRandomRead(java.io.File,java.lang.String,java.lang.String,java.lang.String)",107,114,"/**
 * Opens a file for random read access, securely if enabled.
 * @param f file to open
 * @param mode access mode
 * @param expectedOwner owner
 * @param expectedGroup group
 * @return RandomAccessFile object
 * @throws IOException if an I/O error occurs
 */
","* @return Open the given File for random read access, verifying the expected user/
   * group constraints if security is enabled.
   * 
   * Note that this function provides no additional security checks if hadoop
   * security is disabled, since doing the checks would be too expensive when
   * native libraries are not available.
   * 
   * @param f file that we are trying to open
   * @param mode mode in which we want to open the random access file
   * @param expectedOwner the expected user owner for the file
   * @param expectedGroup the expected group owner for the file
   * @throws IOException if an IO error occurred or if the user/group does
   * not match when security is enabled.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,openFSDataInputStream,"org.apache.hadoop.io.SecureIOUtils:openFSDataInputStream(java.io.File,java.lang.String,java.lang.String)",156,162,"/**
 * Opens an FSDataInputStream, using secure mode if enabled.
 * @param file The file to open.
 * @param expectedOwner Expected owner of the file.
 * @param expectedGroup Expected group of the file.
 */
","* Opens the {@link FSDataInputStream} on the requested file on local file
   * system, verifying the expected user/group constraints if security is
   * enabled.
   * @param file absolute path of the file
   * @param expectedOwner the expected user owner for the file
   * @param expectedGroup the expected group owner for the file
   * @throws IOException if an IO Error occurred or the user/group does not
   * match if security is enabled
   * @return FSDataInputStream.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SecureIOUtils.java,openForRead,"org.apache.hadoop.io.SecureIOUtils:openForRead(java.io.File,java.lang.String,java.lang.String)",208,214,"/**
 * Opens a FileInputStream for reading, securely if enabled.
 * @param f file to open
 * @param expectedOwner owner
 * @param expectedGroup group
 * @throws IOException if an I/O error occurs
 */
","* Open the given File for read access, verifying the expected user/group
   * constraints if security is enabled.
   *
   * @return Note that this function provides no additional checks if Hadoop
   * security is disabled, since doing the checks would be too expensive
   * when native libraries are not available.
   *
   * @param f the file that we are trying to open
   * @param expectedOwner the expected user owner for the file
   * @param expectedGroup the expected group owner for the file
   * @throws IOException if an IO Error occurred, or security is enabled and
   * the user/group does not match",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,getLinkTarget,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:getLinkTarget(org.apache.hadoop.fs.Path),1334,1338,"/**
 * Retrieves the target path of a symbolic link.
 * @param f Path to the symbolic link
 * @return Path object representing the link target
 */
",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getContentSummary,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getContentSummary(org.apache.hadoop.fs.Path),1659,1678,"/**
 * Calculates content summary (length, file/dir count) for a Path.
 * @param f the Path to summarize
 * @throws IOException if an I/O error occurs
 */
",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getStatus,org.apache.hadoop.fs.viewfs.ViewFileSystem$InternalDirOfViewFs:getStatus(org.apache.hadoop.fs.Path),1680,1694,"/**
 * Calculates the total status of a path by summing child statuses.
 * @param p the path to calculate status for
 * @return FsStatus object representing the total status
 */
",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,userHasAdministratorAccess,"org.apache.hadoop.http.HttpServer2:userHasAdministratorAccess(javax.servlet.ServletContext,java.lang.String)",1727,1734,"/**
 * Checks if a user has administrator access based on ACL.
 * @param servletContext Servlet context holding the ACL.
 * @param remoteUser User's remote username.
 * @return True if the user is an administrator, false otherwise.
 */
","* Get the admin ACLs from the given ServletContext and check if the given
   * user is in the ACL.
   *
   * @param servletContext the context containing the admin ACL.
   * @param remoteUser the remote user to check for.
   * @return true if the user is present in the ACL, false if no ACL is set or
   *         the user is not present",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ServiceAuthorizationManager.java,authorize,"org.apache.hadoop.security.authorize.ServiceAuthorizationManager:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.Class,org.apache.hadoop.conf.Configuration,java.net.InetAddress)",88,138,"/**
 * Authorizes a user for a given protocol, configuration, and address.
 * @param user UserGroupInformation object
 * @param protocol Protocol class
 * @param conf Configuration object
 * @param addr InetAddress of the connection
 */
","* Authorize the user to access the protocol being used.
   * 
   * @param user user accessing the service 
   * @param protocol service being accessed
   * @param conf configuration to use
   * @param addr InetAddress of the client
   * @throws AuthorizationException on authorization failure",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/DefaultImpersonationProvider.java,authorize,"org.apache.hadoop.security.authorize.DefaultImpersonationProvider:authorize(org.apache.hadoop.security.UserGroupInformation,java.net.InetAddress)",108,135,"/**
 * Authorizes a user to act on behalf of another user.
 * @param user UserGroupInformation attempting authorization.
 * @param remoteAddress Remote address of the connection.
 */
",,,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getCurrentUser,org.apache.hadoop.security.UserGroupInformation:getCurrentUser(),583,594,"/**
 * Gets the current user's UserGroupInformation.
 * Returns login user if no principals are found.
 */
","* Return the current user, including any doAs in the current stack.
   * @return the current user
   * @throws IOException if login fails",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isLoginKeytabBased,org.apache.hadoop.security.UserGroupInformation:isLoginKeytabBased(),1417,1421,"/**
 * Checks if login is based on a keytab.
 * @return True if login is keytab-based, false otherwise.
 */
","* Did the login happen via keytab.
   * @return true or false
   * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,isLoginTicketBased,org.apache.hadoop.security.UserGroupInformation:isLoginTicketBased(),1428,1430,"/**
 * Checks if the current login is based on a ticket.
 * @return True if login is ticket-based, false otherwise.
 */
","* Did the login happen via ticket cache.
   * @return true or false
   * @throws IOException raised on errors performing I/O.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsLoginUserOrFatal,org.apache.hadoop.security.SecurityUtil:doAsLoginUserOrFatal(java.security.PrivilegedAction),508,522,"/**
 * Executes action as the login user or runs it directly.
 * @param action PrivilegedAction to execute.
 * @return Result of the action.
 */
","* Perform the given action as the daemon's login user. If the login
   * user cannot be determined, this will log a FATAL error and exit
   * the whole JVM.
   *
   * @param action action.
   * @param <T> generic type T.
   * @return generic type T.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsLoginUser,org.apache.hadoop.security.SecurityUtil:doAsLoginUser(java.security.PrivilegedExceptionAction),533,536,"/**
 * Executes an action as the logged-in user.
 * @param action Action to execute; returns result of type T.
 * @return Result of the action.
 */
","* Perform the given action as the daemon's login user. If an
   * InterruptedException is thrown, it is converted to an IOException.
   *
   * @param action the action to perform
   * @param <T> Generics Type T.
   * @return the result of the action
   * @throws IOException in the event of error",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,cedeActive,org.apache.hadoop.ha.ZKFailoverController:cedeActive(int),575,588,"/**
 * Cedes active status for a specified duration.
 * @param millisToCede milliseconds to cede active status
 * @throws AccessControlException, ServiceFailedException, IOException
 */
","* Request from graceful failover to cede active role. Causes
   * this ZKFC to transition its local node to standby, then quit
   * the election for the specified period of time, after which it
   * will rejoin iff it is healthy.",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,gracefulFailoverToYou,org.apache.hadoop.ha.ZKFailoverController:gracefulFailoverToYou(),630,643,"/**
 * Performs a graceful failover operation using UserGroupInformation.
 * Throws ServiceFailedException or IOException on failure.
 */
","* Coordinate a graceful failover to this node.
   * @throws ServiceFailedException if the node fails to become active
   * @throws IOException some other error occurs",,,True,24
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScannerByKey,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScannerByKey(byte[],byte[])",1132,1137,"/**
 * Creates a scanner with specified key ranges.
 * @param beginKey Start key for the scanner.
 * @param endKey End key for the scanner.
 * @return Scanner object.
 */
","* Get a scanner that covers a portion of TFile based on keys.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScanner,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(org.apache.hadoop.io.file.tfile.RawComparable,org.apache.hadoop.io.file.tfile.RawComparable)",1155,1159,"/**
* Creates a scanner with specified key range.
* @param beginKey Start key for the scanner.
* @param endKey End key for the scanner.
*/
","* Get a scanner that covers a specific key range.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.
     * 
     * @deprecated Use {@link #createScannerByKey(RawComparable, RawComparable)}
     *             instead.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",535,543,"/**
 * Gets a ProtocolProxy for the specified protocol.
 * @param protocol Protocol class.
 * @param clientVersion Client version.
 * @return ProtocolProxy instance.
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param ticket user group information
   * @param conf configuration to use
   * @param factory socket factory
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",604,613,"/**
 * Creates a proxy for the given protocol.
 * @param protocol Protocol class.
 * @return Proxy object.
 * @throws IOException on failure.
 */
","* Construct a client-side proxy that implements the named protocol,
   * talking to a server at the named address.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol
   * @param clientVersion client's version
   * @param addr server address
   * @param ticket security ticket
   * @param conf configuration
   * @param factory socket factory
   * @param rpcTimeout max time for each rpc; 0 means no timeout
   * @return the proxy
   * @throws IOException if any error occurs",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,hasAdministratorAccess,"org.apache.hadoop.http.HttpServer2:hasAdministratorAccess(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",1686,1716,"/**
 * Checks if the user has administrator access.
 * Returns true if authorized, otherwise returns false.
 */
","* Does the user sending the HttpServletRequest has the administrator ACLs? If
   * it isn't the case, response will be modified to send an error to the user.
   *
   * @param servletContext servletContext.
   * @param request request.
   * @param response used to send the error response if user does not have admin access.
   * @return true if admin-authorized, false otherwise
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,authorize,"org.apache.hadoop.ipc.Server:authorize(org.apache.hadoop.security.UserGroupInformation,java.lang.String,java.net.InetAddress)",3806,3821,"/**
 * Authorizes a user for a protocol.
 * @param user UserGroupInformation object.
 * @param protocolName Protocol name to authorize.
 * @param addr InetAddress of the connection.
 */
","* Authorize the incoming client connection.
   * 
   * @param user client user
   * @param protocolName - the protocol
   * @param addr InetAddress of incoming connection
   * @throws AuthorizationException when the client isn't authorized to talk the protocol",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getHomeDirectory,org.apache.hadoop.fs.FileSystem:getHomeDirectory(),2445,2456,"/**
 * Gets the user's home directory as a Path object.
 * Uses current user or system property if unavailable.
 */
","Return the current user's home directory in this FileSystem.
   * The default implementation returns {@code ""/user/$USER/""}.
   * @return the path.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,checkAccessPermissions,"org.apache.hadoop.fs.FileSystem:checkAccessPermissions(org.apache.hadoop.fs.FileStatus,org.apache.hadoop.fs.permission.FsAction)",2855,2877,"/**
 * Checks if the current user has the required file access permissions.
 * @param stat FileStatus object.
 * @param mode FsAction to check against.
 * @throws AccessControlException if access is denied.
 */
","* This method provides the default implementation of
   * {@link #access(Path, FsAction)}.
   *
   * @param stat FileStatus to check
   * @param mode type of access to check
   * @throws AccessControlException if access is denied
   * @throws IOException for any error",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(),280,283,"/**
 * Constructs a ViewFileSystem with current user and timestamp.
 */","* This is the  constructor with the signature needed by
   * {@link FileSystem#createFileSystem(URI, Configuration)}
   *
   * After this constructor is called initialize() is called.
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFs:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",227,290,"/**
 * Initializes a ViewFileSystem with the given URI and configuration.
 * @param theUri The URI for the ViewFileSystem.
 * @param conf Hadoop configuration object.
 */
","* This constructor has the signature needed by
   * {@link AbstractFileSystem#createFileSystem(URI, Configuration)}.
   *
   * @param theUri which must be that of ViewFs
   * @param conf
   * @throws IOException
   * @throws URISyntaxException",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/InodeTree.java,<init>,"org.apache.hadoop.fs.viewfs.InodeTree:<init>(org.apache.hadoop.conf.Configuration,java.lang.String,java.net.URI,boolean)",617,770,"/**
 * Initializes the InodeTree with configurations and link entries.
 * @param config Hadoop configuration object
 * @throws Various IOExceptions during initialization
 */","* Create Inode Tree from the specified mount-table specified in Config.
   *
   * @param config the mount table keys are prefixed with
   *               FsConstants.CONFIG_VIEWFS_PREFIX.
   * @param viewName the name of the mount table
   *                 if null use defaultMT name.
   * @param theUri heUri.
   * @param initingUriAsFallbackOnNoMounts initingUriAsFallbackOnNoMounts.
   * @throws UnsupportedFileSystemException file system for <code>uri</code> is
   *                                        not found.
   * @throws URISyntaxException if the URI does not have an authority
   *                            it is badly formed.
   * @throws FileAlreadyExistsException there is a file at the path specified
   *                                    or is discovered on one of its ancestors.
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration,long)",3881,3889,"/**
 * Initializes a Key object with URI, configuration, and unique ID.
 * @param uri The URI to parse.
 * @param conf Configuration object.
 * @param unique Unique identifier.
 */
",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.AbstractFileSystem:getHomeDirectory(),461,472,"/**
 * Gets the home directory path for the current user.
 * Returns a Path object, falling back to system property if needed.
 */
","* Return the current user's home directory in this file system.
   * The default implementation returns ""/user/$USER/"".
   * 
   * @return current user's home directory.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,openConnection,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token,java.lang.String)",283,335,"/**
 * Opens an HttpURLConnection with optional token and proxy user.
 * @param url The URL to connect to.
 * @param token Authentication token.
 * @param doAs User to execute as (proxy user).
 * @return HttpURLConnection object.
 */","* Returns an authenticated {@link HttpURLConnection}. If the Delegation
   * Token is present, it will be used taking precedence over the configured
   * <code>Authenticator</code>. If the <code>doAs</code> parameter is not NULL,
   * the request will be done on behalf of the specified <code>doAs</code> user.
   *
   * @param url the URL to connect to. Only HTTP/S URLs are supported.
   * @param token the authentication token being used for the user.
   * @param doAs user to do the the request on behalf of, if NULL the request is
   * as self.
   * @return an authenticated {@link HttpURLConnection}.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticator.java,authenticate,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticator:authenticate(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",134,153,"/**
 * Authenticates a URL, using delegation token if available, otherwise renews TGT.
 * @param url URL to authenticate
 * @param token Authentication token
 * @throws IOException, AuthenticationException on authentication failure
 */
",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getBestUGI,"org.apache.hadoop.security.UserGroupInformation:getBestUGI(java.lang.String,java.lang.String)",606,615,"/**
 * Gets the best UserGroupInformation based on ticket cache or user.
 * @param ticketCachePath Path to ticket cache, user is username.
 * @return UserGroupInformation object.
 */
","* Find the most appropriate UserGroupInformation to use
   *
   * @param ticketCachePath    The Kerberos ticket cache path, or NULL
   *                           if none is specfied
   * @param user               The user name, or NULL if none is specified.
   *
   * @return                   The most appropriate UserGroupInformation
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,loginUserFromKeytabAndReturnUGI,"org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytabAndReturnUGI(java.lang.String,java.lang.String)",1388,1399,"/**
 * Logs in a user from a keytab file.
 * @param user User principal name.
 * @param path Keytab file path.
 * @return UserGroupInformation object.
 */
","* Log a user in from a keytab file. Loads a user identity from a keytab
   * file and login them in. This new user does not affect the currently
   * logged-in user.
   * @param user the principal name to load from the keytab
   * @param path the path to the keytab file
   * @throws IOException if the keytab file can't be read
   * @return UserGroupInformation.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logAllUserInfo,"org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.slf4j.Logger,org.apache.hadoop.security.UserGroupInformation)",1999,2010,"/**
 * Logs user information (current, real, login) if debug is enabled.
 * @param log Logger to use for logging.
 * @param ugi UserGroupInformation object.
 */
","* Log all (current, real, login) UGI and token info into specified log.
   * @param ugi - UGI
   * @param log - log.
   * @throws IOException raised on errors performing I/O.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/UserProvider.java,<init>,org.apache.hadoop.security.alias.UserProvider:<init>(),44,47,"/**
 * Initializes the UserProvider with current user and credentials.
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,doAsCurrentUser,org.apache.hadoop.security.SecurityUtil:doAsCurrentUser(java.security.PrivilegedExceptionAction),547,550,"/**
 * Executes an action as the current user, throwing exceptions.
 * @param action The action to execute.
 * @return The result of the action.
 */
","* Perform the given action as the daemon's current user. If an
   * InterruptedException is thrown, it is converted to an IOException.
   *
   * @param action the action to perform
   * @param <T> generic type T.
   * @return the result of the action
   * @throws IOException in the event of error",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,<init>,org.apache.hadoop.security.SaslRpcServer:<init>(org.apache.hadoop.security.SaslRpcServer$AuthMethod),89,120,"/**
 * Initializes a SaslRpcServer with the provided authentication method.
 * @param authMethod The authentication method to use.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SaslRpcServer.java,create,"org.apache.hadoop.security.SaslRpcServer:create(org.apache.hadoop.ipc.Server$Connection,java.util.Map,org.apache.hadoop.security.token.SecretManager)",122,173,"/**
 * Creates a SaslServer based on the authentication method.
 * @param connection The connection for SASL negotiation.
 * @param saslProperties SASL properties.
 * @param secretManager Secret manager for token authentication.
 * @return A SaslServer instance.
 * @throws IOException, InterruptedException
 */
",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/UserProvider.java,<init>,org.apache.hadoop.crypto.key.UserProvider:<init>(org.apache.hadoop.conf.Configuration),47,51,"/**
 * Initializes UserProvider with configuration and current user credentials.
 * @param conf Configuration object for provider setup.
 * @throws IOException if an I/O error occurs during initialization.
 */
",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getDoAsUser,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDoAsUser(),1129,1134,"/**
 * Returns short username if using proxy authentication, else null.
 */","* Get the doAs user name.
   *
   * 'actualUGI' is the UGI of the user creating the client
   * It is possible that the creator of the KMSClientProvier
   * calls this method on behalf of a proxyUser (the doAsUser).
   * In which case this call has to be made as the proxy user.
   *
   * @return the doAs user name.
   * @throws IOException",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProtocolProxy,"org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,org.apache.hadoop.io.retry.RetryPolicy,long)",411,453,"/**
 * Waits for a ProtocolProxy to become available, retrying on failure.
 * @param protocol Protocol class to connect to.
 * @return ProtocolProxy object when available.
 * @throws IOException if connection fails or timeout occurs.
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param rpcTimeout timeout for each RPC
   * @param connectionRetryPolicy input connectionRetryPolicy.
   * @param timeout time in milliseconds before giving up
   * @return the proxy
   * @throws IOException if the far end through a RemoteException.",,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,shouldAuthenticateOverKrb,org.apache.hadoop.ipc.Client$Connection:shouldAuthenticateOverKrb(),556,569,"/**
 * Checks if authentication should occur over Kerberos.
 * Returns true if the login user has Kerberos credentials.
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getRestrictParserDefault,org.apache.hadoop.conf.Configuration$Resource:getRestrictParserDefault(java.lang.Object),288,299,"/**
 * Checks if parser restriction is disabled based on resource.
 * Returns false if resource is String or UGI is not initialized.
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Client.java,run,org.apache.hadoop.ipc.Client$Connection$1:run(),1082,1109,"/**
 * Runs the connection processing loop, receiving RPC responses.
 * Handles errors and closes connections upon completion.
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doKerberosRelogin,org.apache.hadoop.ipc.Server:doKerberosRelogin(),3407,3425,"/**
 * Forces a Kerberos re-login for the current user.
 * Attempts force or regular re-login based on login status.
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,run,org.apache.hadoop.ha.ZKFailoverController:run(java.lang.String[]),173,199,"/**
 * Runs the failover controller.
 * Checks auto-failover, executes doRun, handles exceptions.
 */",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFCRpcServer.java,cedeActive,org.apache.hadoop.ha.ZKFCRpcServer:cedeActive(int),91,96,"/**
 * Cedes active controller status to another controller.
 * @param millisToCede Time in milliseconds to cede.
 * @throws IOException, AccessControlException on failure.
 */
",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFCRpcServer.java,gracefulFailover,org.apache.hadoop.ha.ZKFCRpcServer:gracefulFailover(),98,102,"/**
 * Performs a graceful failover operation using the ZooKeeper Failover Client.
 * @throws IOException, AccessControlException if failover fails.
 */
",,,,True,25
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,createScanner,"org.apache.hadoop.io.file.tfile.TFile$Reader:createScanner(byte[],byte[])",1113,1117,"/**
 * Creates a scanner with specified begin and end keys.
 * @param beginKey Start key for scanning.
 * @param endKey End key for scanning.
 * @return Scanner object.
 */
","* Get a scanner that covers a portion of TFile based on keys.
     * 
     * @param beginKey
     *          Begin key of the scan (inclusive). If null, scan from the first
     *          key-value entry of the TFile.
     * @param endKey
     *          End key of the scan (exclusive). If null, scan up to the last
     *          key-value entry of the TFile.
     * @return The actual coverage of the returned scanner will cover all keys
     *         greater than or equal to the beginKey and less than the endKey.
     * @throws IOException raised on errors performing I/O.
     * 
     * @deprecated Use {@link #createScannerByKey(byte[], byte[])} instead.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",488,494,"/**
 * Gets a ProtocolProxy instance.
 * @param protocol Protocol class.
 * @param clientVersion Client version.
 * @param addr Socket address.
 * @param conf Configuration object.
 * @param factory Socket factory.
 * @return ProtocolProxy instance.
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param factory socket factory
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.security.UserGroupInformation,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",511,519,"/**
 * Creates a proxy object for the given protocol.
 * @param protocol Protocol class.
 * @return Proxy object or null if creation fails.
 */
","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   *
   * @param <T> Generics Type T.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param ticket input tocket.
   * @param conf input conf.
   * @param factory input factory.
   * @return the protocol proxy.
   * @throws IOException raised on errors performing I/O.
   *",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/ZKFCProtocolClientSideTranslatorPB.java,<init>,"org.apache.hadoop.ha.protocolPB.ZKFCProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",46,54,"/**
 * Creates a ZKFC protocol client-side translator using given parameters.
 * @param addr Socket address.
 * @param conf Configuration object.
 * @param socketFactory Socket factory.
 * @param timeout Socket timeout.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,<init>,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory,int)",74,82,"/**
 * Creates a HAServiceProtocolClientSideTranslatorPB proxy.
 * @param addr Socket address.
 * @param conf Hadoop configuration.
 * @param socketFactory Socket factory.
 * @param timeout Socket timeout.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,doGet,"org.apache.hadoop.log.LogLevel$Servlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",325,360,"/**
 * Handles GET requests, authorizes admin access, and displays log info.
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/AdminAuthorizedServlet.java,doGet,"org.apache.hadoop.http.AdminAuthorizedServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",36,45,"/**
 * Handles GET requests, authorizing admin access before forwarding.
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,isInstrumentationAccessAllowed,"org.apache.hadoop.http.HttpServer2:isInstrumentationAccessAllowed(javax.servlet.ServletContext,javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",1660,1674,"/**
 * Checks if instrumentation access is allowed based on configuration.
 * @param servletContext Servlet context.
 * @param request HTTP request.
 * @param response HTTP response.
 * @return True if access is allowed, false otherwise.
 */
","* Checks the user has privileges to access to instrumentation servlets.
   * <p>
   * If <code>hadoop.security.instrumentation.requires.admin</code> is set to FALSE
   * (default value) it always returns TRUE.
   * <p>
   * If <code>hadoop.security.instrumentation.requires.admin</code> is set to TRUE
   * it will check that if the current user is in the admin ACLS. If the user is
   * in the admin ACLs it returns TRUE, otherwise it returns FALSE.
   *
   * @param servletContext the servlet context.
   * @param request the servlet request.
   * @param response the servlet response.
   * @return TRUE/FALSE based on the logic decribed above.
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/DelegateToFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.DelegateToFileSystem:getHomeDirectory(),169,172,"/**
 * Returns the home directory path.
 * Delegates to the underlying file system implementation.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getTrashRoot,org.apache.hadoop.fs.FileSystem:getTrashRoot(org.apache.hadoop.fs.Path),3439,3442,"/**
 * Returns the path to the trash directory for the given path.
 * @param path The path to determine the trash root for.
 * @return Path object representing the trash root.
 */
","* Get the root directory of Trash for current user when the path specified
   * is deleted.
   *
   * @param path the trash root of the path to be determined.
   * @return the default implementation returns {@code /user/$USER/.Trash}",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getTrashRoots,org.apache.hadoop.fs.FileSystem:getTrashRoots(boolean),3452,3478,"/**
 * Gets trash roots, optionally for all users.
 * @param allUsers true to get all user trash roots
 * @return Collection of FileStatus objects representing trash roots
 */
","* Get all the trash roots for current user or all users.
   *
   * @param allUsers return trash roots for all users if true.
   * @return all the trash root directories.
   *         Default FileSystem returns .Trash under users' home directories if
   *         {@code /user/$USER/.Trash} exists.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getHomeDirectory,org.apache.hadoop.fs.FilterFileSystem:getHomeDirectory(),297,300,"/**
 * Gets the home directory.
 * @return Path object representing the home directory.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,access,"org.apache.hadoop.fs.FileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",2840,2844,"/**
 * Checks access permissions for a given path and mode.
 * @param path Path to check.
 * @param mode FsAction representing the requested access.
 */
","* Checks if the user can access a path.  The mode specifies which access
   * checks to perform.  If the requested permissions are granted, then the
   * method returns normally.  If access is denied, then the method throws an
   * {@link AccessControlException}.
   * <p>
   * The default implementation calls {@link #getFileStatus(Path)}
   * and checks the returned permissions against the requested permissions.
   *
   * Note that the {@link #getFileStatus(Path)} call will be subject to
   * authorization checks.
   * Typically, this requires search (execute) permissions on each directory in
   * the path's prefix, but this is implementation-defined.  Any file system
   * that provides a richer authorization model (such as ACLs) may override the
   * default implementation so that it checks against that model instead.
   * <p>
   * In general, applications should avoid using this method, due to the risk of
   * time-of-check/time-of-use race conditions.  The permissions on a file may
   * change immediately after the access call returns.  Most applications should
   * prefer running specific file system actions as the desired user represented
   * by a {@link UserGroupInformation}.
   *
   * @param path Path to check
   * @param mode type of access to check
   * @throws AccessControlException if access is denied
   * @throws FileNotFoundException if the path does not exist
   * @throws IOException see specific implementation",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/AbstractFileSystem.java,access,"org.apache.hadoop.fs.AbstractFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",1046,1050,"/**
 * Checks if access to the path is allowed based on the mode.
 * @param path Path to check.
 * @param mode FsAction specifying the access mode.
 */
","* The specification of this method matches that of
   * {@link FileContext#access(Path, FsAction)}
   * except that an UnresolvedLinkException may be thrown if a symlink is
   * encountered in the path.
   *
   * @param path the path.
   * @param mode fsaction mode.
   * @throws AccessControlException access control exception.
   * @throws FileNotFoundException file not found exception.
   * @throws UnresolvedLinkException unresolved link exception.
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",392,396,"/**
 * Constructs a ViewFileSystem with a URI and configuration.
 * @param theUri The URI for the filesystem.
 * @param conf Configuration settings for the filesystem.
 */
","* Convenience Constructor for apps to call directly.
   * @param theUri which must be that of ViewFileSystem
   * @param conf conf configuration.
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,<init>,org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:<init>(),123,125,"/**
 * Constructs a ViewFileSystemOverloadScheme.
 * Initializes the scheme by calling the superclass constructor.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,<init>,org.apache.hadoop.fs.viewfs.ViewFs:<init>(org.apache.hadoop.conf.Configuration),213,216,"/**
 * Constructs a ViewFs object using a default URI and configuration.
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,<init>,"org.apache.hadoop.fs.FileSystem$Cache$Key:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",3877,3879,"/**
 * Constructs a Key object with a default retry count.
 * @param uri The URI for the key.
 * @param conf Configuration object.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getUnique,"org.apache.hadoop.fs.FileSystem$Cache:getUnique(java.net.URI,org.apache.hadoop.conf.Configuration)",3671,3674,"/**
 * Retrieves a FileSystem with a unique key.
 * @param uri The URI of the file system.
 * @param conf Hadoop configuration.
 * @return A FileSystem instance.
 */
",The objects inserted into the cache using this method are all unique.,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,getHomeDirectory,org.apache.hadoop.fs.viewfs.ChRootedFs:getHomeDirectory(),151,154,"/**
 * Gets the home directory.
 * @return Path object representing the home directory.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,<init>,"org.apache.hadoop.fs.FileContext:<init>(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.conf.Configuration)",243,271,"/**
 * Constructs a FileContext with a default filesystem and configuration.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getHomeDirectory,org.apache.hadoop.fs.FileContext:getHomeDirectory(),577,579,"/**
 * Returns the home directory path.
 * Delegates to the default file system.
 */
","* Return the current user's home directory in this file system.
   * The default implementation returns ""/user/$USER/"".
   * @return the home directory",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,getHomeDirectory,org.apache.hadoop.fs.FilterFs:getHomeDirectory(),83,86,"/**
 * Gets the home directory.
 * @return Path object representing the home directory.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,openConnection,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL$Token)",230,235,"/**
 * Opens a connection to the given URL, using the provided token.
 * Delegates to superclass if token is not a Token instance.
 */
","* Returns an authenticated {@link HttpURLConnection}, it uses a Delegation
   * Token only if the given auth token is an instance of {@link Token} and
   * it contains a Delegation Token, otherwise use the configured
   * {@link DelegationTokenAuthenticator} to authenticate the connection.
   *
   * @param url the URL to connect to. Only HTTP/S URLs are supported.
   * @param token the authentication token being used for the user.
   * @return an authenticated {@link HttpURLConnection}.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,"org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)",268,280,"/**
 * Gets a FileSystem object for the given URI, config, and user.
 * @param uri The URI of the file system.
 * @param conf Hadoop configuration.
 * @param user User name for authentication.
 * @return A FileSystem object.
 * @throws IOException, InterruptedException
 */
","* Get a FileSystem instance based on the uri, the passed in
   * configuration and the user.
   * @param uri of the filesystem
   * @param conf the configuration to use
   * @param user to perform the get as
   * @return the filesystem instance
   * @throws IOException failure to load
   * @throws InterruptedException If the {@code UGI.doAs()} call was
   * somehow interrupted.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstance,"org.apache.hadoop.fs.FileSystem:newInstance(java.net.URI,org.apache.hadoop.conf.Configuration,java.lang.String)",571,583,"/**
 * Creates a FileSystem instance with specified URI, config, and user.
 * @param uri The URI of the file system.
 * @param conf Hadoop configuration.
 * @param user User name for FileSystem access.
 * @return A FileSystem instance.
 * @throws IOException, InterruptedException
 */
","* Returns the FileSystem for this URI's scheme and authority and the
   * given user. Internally invokes {@link #newInstance(URI, Configuration)}
   * @param uri uri of the filesystem.
   * @param conf the configuration to use
   * @param user to perform the get as
   * @return filesystem instance
   * @throws IOException if the FileSystem cannot be instantiated.
   * @throws InterruptedException If the {@code UGI.doAs()} call was
   *         somehow interrupted.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,getUGIFromTicketCache,"org.apache.hadoop.security.UserGroupInformation:getUGIFromTicketCache(java.lang.String,java.lang.String)",627,638,"/**
 * Retrieves UserGroupInformation from a Kerberos ticket cache.
 * @param ticketCache Path to the Kerberos ticket cache file.
 * @param user User principal name.
 * @return UserGroupInformation object.
 */
","* Create a UserGroupInformation from a Kerberos ticket cache.
   * 
   * @param user                The principal name to load from the ticket
   *                            cache
   * @param ticketCache     the path to the ticket cache file
   *
   * @throws IOException        if the kerberos login fails
   * @return UserGroupInformation.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,loginFromKeytab,org.apache.hadoop.security.KDiag:loginFromKeytab(),628,657,"/**
 * Logs in using a keytab file or as the current user if no keytab.
 * @throws IOException if login fails
 */
","* Log in from a keytab, dump the UGI, validate it, then try and log in again.
   *
   * That second-time login catches JVM/Hadoop compatibility problems.
   * @throws IOException Keytab loading problems",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,loginUserFromKeytab,"org.apache.hadoop.security.UserGroupInformation:loginUserFromKeytab(java.lang.String,java.lang.String)",1127,1147,"/**
 * Logs in a user from a keytab file.
 * @param user User's principal name.
 * @param path Path to the keytab file.
 */
","* Log a user in from a keytab file. Loads a user identity from a keytab
   * file and logs them in. They become the currently logged-in user.
   * @param user the principal name to load from the keytab
   * @param path the path to the keytab file
   * @throws IOException raised on errors performing I/O.
   * @throws KerberosAuthException if it's a kerberos login exception.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,logAllUserInfo,org.apache.hadoop.security.UserGroupInformation:logAllUserInfo(org.apache.hadoop.security.UserGroupInformation),2017,2020,"/**
 * Logs all user information using the default logger.
 * @param ugi UserGroupInformation object to log.
 */","* Log all (current, real, login) UGI and token info into UGI debug log.
   * @param ugi - UGI
   * @throws IOException raised on errors performing I/O.",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getActualUgi,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getActualUgi(),1167,1190,"/**
 * Returns the actual UserGroupInformation, considering proxy users & security.
 * @return UserGroupInformation object representing the effective user.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,buildNegotiateResponse,org.apache.hadoop.ipc.Server:buildNegotiateResponse(java.util.List),3445,3467,"/**
 * Builds a negotiate response based on available authentication methods.
 * @param authMethods List of supported authentication methods.
 * @return RpcSaslProto object representing the negotiate response.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,createSaslServer,org.apache.hadoop.ipc.Server$Connection:createSaslServer(org.apache.hadoop.security.SaslRpcServer$AuthMethod),2621,2626,"/**
 * Creates a SaslServer using provided auth method and properties.
 */",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProtocolProxy,"org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)",366,372,"/**
 * Waits for a ProtocolProxy. Delegates to overloaded method.
 * @param protocol Protocol class to wait for.
 * @param clientVersion Client version.
 * @param addr Socket address.
 * @param conf Configuration object.
 * @param connTimeout Connection timeout in milliseconds.
 * @return ProtocolProxy object.
 * @throws IOException if an I/O error occurs.
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param connTimeout time in milliseconds before giving up
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProxy,"org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,int,long)",387,394,"/**
 * Waits for a proxy of the given protocol.
 * @param protocol Protocol class.
 * @return Proxy object.
 * @throws IOException on connection error.
 */
","* Get a proxy connection to a remote server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param rpcTimeout timeout for each RPC
   * @param timeout time in milliseconds before giving up
   * @return the proxy
   * @throws IOException if the far end through a RemoteException",,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,"org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object,java.lang.String)",261,263,"/**
 * Constructs a Resource with a default restrict parser.
 * @param resource The resource object.
 * @param name The name of the resource.
 */
",,,,True,26
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,javax.net.SocketFactory)",467,473,"/**
 * Creates and returns a proxy object for the given protocol.
 * @param protocol Protocol class, client version, address, config, factory.
 * @return Proxy object or null if creation fails.
 */
","* Construct a client-side proxy object that implements the named protocol,
   * talking to a server at the named address. 
   * @param <T> Generics Type T.
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param conf input Configuration.
   * @param factory input factory.
   * @throws IOException raised on errors performing I/O.
   * @return proxy.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProtocolProxy,"org.apache.hadoop.ipc.RPC:getProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",773,780,"/**
 * Gets a ProtocolProxy instance.
 * @param protocol Protocol class.
 * @param version Client version.
 * @param addr Socket address.
 * @param conf Configuration object.
 * @return ProtocolProxy instance.
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server
   * 
   * @param protocol input protocol.
   * @param clientVersion input clientVersion.
   * @param addr input addr.
   * @param conf input configuration.
   * @param <T> Generics Type T.
   * @return a protocol proxy
   * @throws IOException if the thread is interrupted.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,getUgmProtocol,org.apache.hadoop.tools.GetGroupsBase:getUgmProtocol(),97,105,"/**
 * Obtains a proxy for the GetUserMappingsProtocol.
 * @return GetUserMappingsProtocol proxy object.
 * @throws IOException if an I/O error occurs.
 */
","* Get a client of the {@link GetUserMappingsProtocol}.
   * @return A {@link GetUserMappingsProtocol} client proxy.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getZKFCProxy,"org.apache.hadoop.ha.HAServiceTarget:getZKFCProxy(org.apache.hadoop.conf.Configuration,int)",164,173,"/**
 * Creates a ZKFC proxy using provided configuration and timeout.
 * @param conf Configuration object.
 * @param timeoutMs Timeout in milliseconds.
 * @return ZKFCProtocol proxy.
 */
","* @return a proxy to the ZKFC which is associated with this HA service.
   * @param conf configuration.
   * @param timeoutMs timeout in milliseconds.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getProxyForAddress,"org.apache.hadoop.ha.HAServiceTarget:getProxyForAddress(org.apache.hadoop.conf.Configuration,int,int,java.net.InetSocketAddress)",146,156,"/**
 * Creates a proxy for the given address with specified timeout & retries.
 * @param addr InetSocketAddress to connect to
 * @param timeoutMs Connection timeout in milliseconds
 * @param retries Number of connection retries
 * @return Proxy object for the address
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,isInstrumentationAccessAllowed,"org.apache.hadoop.jmx.JMXJsonServlet:isInstrumentationAccessAllowed(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",152,156,"/**
 * Checks if instrumentation access is allowed for the request.
 * @param request HTTP request
 * @param response HTTP response
 * @return True if access is allowed, false otherwise.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/HttpServer2.java,doGet,"org.apache.hadoop.http.HttpServer2$StackServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",1745,1758,"/**
 * Handles GET requests, prints thread info, and logs activity.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileOutputServlet.java,doGet,"org.apache.hadoop.http.ProfileOutputServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",48,77,"/**
 * Handles GET requests, checks authorization, and serves profiling data.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/http/ProfileServlet.java,doGet,"org.apache.hadoop.http.ProfileServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",187,327,"/**
 * Handles HTTP GET requests to start profiling.
 * Parses request parameters and executes profiling.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/ConfServlet.java,doGet,"org.apache.hadoop.conf.ConfServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",57,83,"/**
 * Handles GET requests, writing response based on format and name.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,moveToTrash,org.apache.hadoop.fs.TrashPolicyDefault:moveToTrash(org.apache.hadoop.fs.Path),129,204,"/**
 * Moves a file or directory to the trash.
 * @param path Path to move to trash.
 * @return True if successful, false otherwise.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,getCurrentTrashDir,org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir(),241,244,"/**
 * Returns the path to the current trash directory.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,getCurrentTrashDir,org.apache.hadoop.fs.TrashPolicyDefault:getCurrentTrashDir(org.apache.hadoop.fs.Path),246,249,"/**
 * Returns the path to the current trash directory for the given path.
 * @param path The original path for which to find the trash directory.
 * @return Path object representing the current trash directory.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getTrashRoot,org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoot(org.apache.hadoop.fs.Path),1180,1220,"/**
 * Resolves the trash root path based on configuration and paths.
 * @param path The path to resolve the trash root for.
 * @return Trash root path.
 */
","* Get the trash root directory for current user when the path
   * specified is deleted.
   *
   * If FORCE_INSIDE_MOUNT_POINT flag is not set, return the default trash root
   * from targetFS.
   *
   * When FORCE_INSIDE_MOUNT_POINT is set to true,
   * <ol>
   *   <li>
   *     If the trash root for path p is in the same mount point as path p,
   *       and one of:
   *       <ol>
   *         <li>The mount point isn't at the top of the target fs.</li>
   *         <li>The resolved path of path is root (in fallback FS).</li>
   *         <li>The trash isn't in user's target fs home directory
   *            get the corresponding viewFS path for the trash root and return
   *            it.
   *         </li>
   *       </ol>
   *   </li>
   *   <li>
   *     else, return the trash root under the root of the mount point
   *     (/{mntpoint}/.Trash/{user}).
   *   </li>
   * </ol>
   *
   * These conditions handle several different important cases:
   * <ul>
   *   <li>File systems may need to have more local trash roots, such as
   *         encryption zones or snapshot roots.</li>
   *   <li>The fallback mount should use the user's home directory.</li>
   *   <li>Cloud storage systems should not use trash in an implicity defined
   *        home directory, per a container, unless it is the fallback fs.</li>
   * </ul>
   *
   * @param path the trash root of the path to be determined.
   * @return the trash root path.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getTrashRoot,org.apache.hadoop.fs.FilterFileSystem:getTrashRoot(org.apache.hadoop.fs.Path),689,692,"/**
 * Gets the trash root directory for a given path.
 * @param path The path to determine the trash root for.
 * @return The trash root Path object.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,run,org.apache.hadoop.fs.TrashPolicyDefault$Emptier:run(),278,320,"/**
 * Empties the trash periodically, deleting checkpoints.
 * Exits on interrupt or exception. Closes FileSystem on exit.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,createCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(java.util.Date),212,220,"/**
 * Creates checkpoints for each trash root.
 * @param date The date for the checkpoint.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(boolean),232,239,"/**
 * Deletes checkpoint files from trash roots, optionally immediately.
 * @param deleteImmediately Whether to delete immediately or not.
 * @throws IOException if an I/O error occurs during deletion.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,getTrashRoots,org.apache.hadoop.fs.viewfs.ViewFileSystem:getTrashRoots(boolean),1231,1297,"/**
 * Gets trash roots, optionally for all users, considering mount points.
 * @param allUsers True to get trash roots for all users.
 * @return Collection of FileStatus objects representing trash roots.
 */
","* Get all the trash roots for current user or all users.
   *
   * When FORCE_INSIDE_MOUNT_POINT is set to true, we also return trash roots
   * under the root of each mount point, with their viewFS paths.
   *
   * @param allUsers return trash roots for all users if true.
   * @return all Trash root directories.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,getTrashRoots,org.apache.hadoop.fs.FilterFileSystem:getTrashRoots(boolean),694,697,"/**
 * Gets trash roots.
 * @param allUsers If true, returns all trash roots; otherwise, user's.
 * @return Collection of FileStatus objects representing trash roots.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,testAccess,"org.apache.hadoop.fs.shell.Test:testAccess(org.apache.hadoop.fs.shell.PathData,org.apache.hadoop.fs.permission.FsAction)",110,118,"/**
 * Tests file system access.
 * @param item PathData object. @param action FsAction to test.
 * @return True if access allowed, false otherwise.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,access,"org.apache.hadoop.fs.viewfs.ViewFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",576,582,"/**
 * Accesses a path with the specified mode in the target file system.
 * @param path The path to access.
 * @param mode The access control mode.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,access,"org.apache.hadoop.fs.FilterFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",470,474,"/**
 * Delegates access check to the underlying filesystem.
 * @param path The path to check.
 * @param mode The access mode to check.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFs.java,access,"org.apache.hadoop.fs.viewfs.ChRootedFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",208,211,"/**
 * Checks file accessibility using the provided mode.
 * @param path Path object.
 * @param mode FsAction to check.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,access,"org.apache.hadoop.fs.viewfs.ViewFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",428,434,"/**
 * Accesses a path with the given mode in the target filesystem.
 * @param path Path to access.
 * @param mode FsAction specifying the access type.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFs.java,access,"org.apache.hadoop.fs.FilterFs:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",132,137,"/**
 * Checks path and delegates access check to the underlying filesystem.
 * @param path The path to check.
 * @param mode The access mode to check.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,<init>,org.apache.hadoop.fs.viewfs.ViewFileSystem:<init>(org.apache.hadoop.conf.Configuration),403,405,"/**
 * Constructs a ViewFileSystem using a default URI and configuration.
 * @param conf Hadoop configuration object.
 * @throws IOException if an I/O error occurs.
 */
","* Convenience Constructor for apps to call directly.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,addFileSystemForTesting,"org.apache.hadoop.fs.FileSystem:addFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",240,244,"/**
 * Adds a FileSystem to the cache for testing purposes.
 * @param uri URI of the file system.
 * @param conf Configuration object.
 * @param fs The FileSystem instance to add.
 */
","* This method adds a FileSystem instance to the cache so that it can
   * be retrieved later. It is only for testing.
   * @param uri the uri to store it under
   * @param conf the configuration to store it under
   * @param fs the FileSystem to store
   * @throws IOException if the current user cannot be determined.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,removeFileSystemForTesting,"org.apache.hadoop.fs.FileSystem:removeFileSystemForTesting(java.net.URI,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem)",246,250,"/**
 * Removes a FileSystem from the cache for testing purposes.
 * @param uri The URI of the FileSystem.
 * @param conf Configuration object.
 * @param fs The FileSystem to remove.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,"org.apache.hadoop.fs.FileSystem$Cache:get(java.net.URI,org.apache.hadoop.conf.Configuration)",3665,3668,"/**
 * Retrieves a FileSystem object for the given URI and configuration.
 * @param uri The URI of the file system.
 * @param conf Hadoop configuration.
 * @return FileSystem object.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstance,"org.apache.hadoop.fs.FileSystem:newInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",594,611,"/**
 * Creates a new FileSystem instance based on URI and config.
 * Returns default FS if URI is incomplete.
 */
","* Returns the FileSystem for this URI's scheme and authority.
   * The entire URI is passed to the FileSystem instance's initialize method.
   * This always returns a new FileSystem object.
   * @param uri FS URI
   * @param config configuration to use
   * @return the new FS instance
   * @throws IOException FS creation or initialization failure.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,"org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem,org.apache.hadoop.conf.Configuration)",373,376,"/**
 * Creates a FileContext using the provided file system and configuration.
 */
","* Create a FileContext with specified FS as default using the specified
   * config.
   * 
   * @param defFS default fs.
   * @param aConf configutration.
   * @return new FileContext with specified FS as default.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/delegation/web/DelegationTokenAuthenticatedURL.java,openConnection,"org.apache.hadoop.security.token.delegation.web.DelegationTokenAuthenticatedURL:openConnection(java.net.URL,org.apache.hadoop.security.authentication.client.AuthenticatedURL$Token)",230,235,"/**
 * Opens a connection to the URL, using the provided token if applicable.
 */","* Returns an authenticated {@link HttpURLConnection}, it uses a Delegation
   * Token only if the given auth token is an instance of {@link Token} and
   * it contains a Delegation Token, otherwise use the configured
   * {@link DelegationTokenAuthenticator} to authenticate the connection.
   *
   * @param url the URL to connect to. Only HTTP/S URLs are supported.
   * @param token the authentication token being used for the user.
   * @return an authenticated {@link HttpURLConnection}.
   * @throws IOException if an IO error occurred.
   * @throws AuthenticationException if an authentication exception occurred.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,execute,org.apache.hadoop.security.KDiag:execute(),282,420,"/**
 * Executes Kerberos diagnostics, checks configuration, and attempts login.
 */","* Execute diagnostics.
   * <p>
   * Things it would be nice if UGI made accessible
   * <ol>
   *   <li>A way to enable JAAS debug programatically</li>
   *   <li>Access to the TGT</li>
   * </ol>
   * @return true if security was enabled and all probes were successful
   * @throws KerberosDiagsFailure explicitly raised failure
   * @throws Exception other security problems",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,maybeDoLoginFromKeytabAndPrincipal,org.apache.hadoop.security.token.DtUtilShell:maybeDoLoginFromKeytabAndPrincipal(java.lang.String[]),82,106,"/**
 * Parses args for -principal and -keytab, attempts Kerberos login if both are provided.
 * @param args command-line arguments
 * @return filtered args array
 */
","* Parse arguments looking for Kerberos keytab/principal.
   * If both are found: remove both from the argument list and attempt login.
   * If only one of the two is found: remove it from argument list, log warning
   * and do not attempt login.
   * If neither is found: return original args array, doing nothing.
   * Return the pruned args array if either flag is present.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/UserGroupInformation.java,main,org.apache.hadoop.security.UserGroupInformation:main(java.lang.String[]),2300,2318,"/**
 * Main method: gets/prints UGI, or logs in from keytab.
 * @param args keytab path and principal (optional)
 */
","* A test method to print out the current user's UGI.
   * @param args if there are two arguments, read the user from the keytab
   * and print it out.
   * @throws Exception Exception.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,login,"org.apache.hadoop.security.SecurityUtil:login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String,java.lang.String)",309,329,"/**
 * Logs in using a keytab file and username for secure Hadoop access.
 * @param conf Configuration object.
 * @param keytabFileKey Keytab file configuration key.
 * @param userNameKey Username configuration key.
 * @param hostname Hostname to use for principal.
 */
","* Login as a principal specified in config. Substitute $host in user's Kerberos principal 
   * name with hostname. If non-secure mode - return. If no keytab available -
   * bail out with an exception
   * 
   * @param conf
   *          conf to use
   * @param keytabFileKey
   *          the key to look for keytab file in conf
   * @param userNameKey
   *          the key to look for user's Kerberos principal name in conf
   * @param hostname
   *          hostname to use for substitution
   * @throws IOException if the config doesn't specify a keytab",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createConnection,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createConnection(java.net.URL,java.lang.String)",502,537,"/**
 * Creates and configures an HttpURLConnection with delegation token auth.
 * @param url URL to connect to
 * @param method HTTP method (e.g., GET, POST)
 * @return Configured HttpURLConnection
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getDelegationToken(java.lang.String),1025,1059,"/**
 * Obtains a delegation token for a renewer.
 * @param renewer The renewer for the delegation token.
 * @return Delegation token, or null if an error occurs.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,renewDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:renewDelegationToken(org.apache.hadoop.security.token.Token),1061,1087,"/**
 * Renews a delegation token.
 * @param dToken The delegation token to renew.
 * @return The renewed token's expiration time.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,cancelDelegationToken,org.apache.hadoop.crypto.key.kms.KMSClientProvider:cancelDelegationToken(org.apache.hadoop.security.token.Token),1089,1116,"/**
 * Cancels a delegation token using doAs.
 * @param dToken The delegation token to cancel.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)",3307,3405,"/**
 * Constructs a Server instance with specified configurations.
 * @param bindAddress Server bind address.
 * @param port Server port.
 */
","* Constructs a server listening on the named port and address.  Parameters passed must
   * be of the named class.  The <code>handlerCount</code> determines
   * the number of handler threads that will be used to process calls.
   * If queueSizePerHandler or numReaders are not -1 they will be used instead of parameters
   * from configuration. Otherwise the configuration will be picked up.
   * 
   * If rpcRequestClass is null then the rpcRequestClass must have been 
   * registered via {@link #registerProtocolEngine(RPC.RpcKind,
   *  Class, RPC.RpcInvoker)}
   * This parameter has been retained for compatibility with existing tests
   * and usage.
   *
   * @param bindAddress input bindAddress.
   * @param port input port.
   * @param rpcRequestClass input rpcRequestClass.
   * @param handlerCount input handlerCount.
   * @param numReaders input numReaders.
   * @param queueSizePerHandler input queueSizePerHandler.
   * @param conf input Configuration.
   * @param serverName input serverName.
   * @param secretManager input secretManager.
   * @param portRangeConfig input portRangeConfig.
   * @throws IOException raised on errors performing I/O.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,buildSaslNegotiateResponse,org.apache.hadoop.ipc.Server$Connection:buildSaslNegotiateResponse(),2603,2619,"/**
 * Builds the SASL negotiate response, including initial challenge if TOKEN is enabled.
 * @return RpcSaslProto - The built SASL negotiate response.
 * @throws InterruptedException, SaslException, IOException
 */
","* Process the Sasl's Negotiate request, including the optimization of 
     * accelerating token negotiation.
     * @return the response to Negotiate request - the list of enabled 
     *         authMethods and challenge if the TOKENS are supported. 
     * @throws SaslException - if attempt to generate challenge fails.
     * @throws IOException - if it fails to create the SASL server for Tokens",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProtocolProxy,"org.apache.hadoop.ipc.RPC:waitForProtocolProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",326,332,"/**
 * Waits for a ProtocolProxy to become available.
 * @param protocol Protocol class to wait for.
 * @param clientVersion Client version.
 * @param addr Socket address.
 * @param conf Configuration.
 * @throws IOException If an I/O error occurs.
 */
","* Get a protocol proxy that contains a proxy connection to a remote server
   * and a set of methods that are supported by the server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @return the protocol proxy
   * @throws IOException if the far end through a RemoteException",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProxy,"org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration,long)",346,351,"/**
 * Retrieves a proxy for the given protocol.
 * @param protocol Protocol class.
 * @return Proxy object.
 * @throws IOException if connection fails.
 */
","* Get a proxy connection to a remote server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @param connTimeout time in milliseconds before giving up
   * @return the proxy
   * @throws IOException if the far end through a RemoteException",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,"org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream,java.lang.String)",998,1000,"/**
 * Adds a resource with an input stream and name.
 * @param in The input stream for the resource.
 * @param name The name of the resource.
 */
","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param in InputStream to deserialize the object from.
   * @param name the name of the resource because InputStream.toString is not
   * very descriptive some times.",,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,<init>,org.apache.hadoop.conf.Configuration$Resource:<init>(java.lang.Object),253,255,"/**
 * Constructs a Resource with the given object and its string representation.
 */",,,,True,27
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,getProxy,"org.apache.hadoop.ipc.RPC:getProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",728,734,"/**
 * Creates and returns a proxy object for the given protocol.
 * @param protocol Protocol class.
 * @param clientVersion Client version.
 * @param addr Socket address.
 * @param conf Configuration object.
 * @return Proxy object.
 */
","* Construct a client-side proxy object with the default SocketFactory.
    *
    * @param <T> Generics Type T.
    * @param protocol input protocol.
    * @param clientVersion input clientVersion.
    * @param addr input addr.
    * @param conf input Configuration.
    * @return a proxy instance
    * @throws IOException  if the thread is interrupted.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/tools/GetGroupsBase.java,run,org.apache.hadoop.tools.GetGroupsBase:run(java.lang.String[]),62,79,"/**
 * Executes the Hadoop group listing command.
 * @param args Usernames to list groups for, or current user if empty.
 * @return 0 on success.
 */
","* Get the groups for the users given and print formatted output to the
   * {@link PrintStream} configured earlier.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,gracefulFailoverThroughZKFCs,org.apache.hadoop.ha.HAAdmin:gracefulFailoverThroughZKFCs(org.apache.hadoop.ha.HAServiceTarget),276,290,"/**
 * Attempts graceful failover to the specified node via ZKFC.
 * @param toNode Target HAServiceTarget for failover.
 * @return 0 on success, -1 on failure.
 */
","* Initiate a graceful failover by talking to the target node's ZKFC.
   * This sends an RPC to the ZKFC, which coordinates the failover.
   *
   * @param toNode the node to fail to
   * @return status code (0 for success)
   * @throws IOException if failover does not succeed",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,cedeRemoteActive,"org.apache.hadoop.ha.ZKFailoverController:cedeRemoteActive(org.apache.hadoop.ha.HAServiceTarget,int)",749,756,"/**
 * Requests remote to cede its active state.
 * @param remote The remote HAServiceTarget.
 * @param timeout Timeout in milliseconds.
 * @return The ZKFCProtocol object.
 */
","* Ask the remote zkfc to cede its active status and wait for the specified
   * timeout before attempting to claim leader status.
   * @param remote node to ask
   * @param timeout amount of time to cede
   * @return the {@link ZKFCProtocol} used to talk to the ndoe
   * @throws IOException",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getHealthMonitorProxy,"org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int,int)",131,138,"/**
 * Gets a HealthMonitorProxy.
 * @param conf Configuration object.
 * @param timeoutMs Timeout in milliseconds.
 * @param retries Number of retries.
 * @return HealthMonitorProxy object.
 */
",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getProxyForAddress,"org.apache.hadoop.ha.HAServiceTarget:getProxyForAddress(org.apache.hadoop.conf.Configuration,int,java.net.InetSocketAddress)",140,144,"/**
 * Gets a proxy for the given address with a reduced timeout.
 * @param conf Configuration object. @param timeoutMs Timeout in ms.
 * @param addr Address to connect to. @return Proxy object.
 */
",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/jmx/JMXJsonServlet.java,doGet,"org.apache.hadoop.jmx.JMXJsonServlet:doGet(javax.servlet.http.HttpServletRequest,javax.servlet.http.HttpServletResponse)",175,232,"/**
 * Handles HTTP GET requests to retrieve JMX bean data in JSON.
 */","* Process a GET request for the specified resource.
   * 
   * @param request
   *          The servlet request we are processing
   * @param response
   *          The servlet response we are creating",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,createCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:createCheckpoint(),206,210,"/**
 * Creates a checkpoint with a current timestamp.
 * Overloads the method with a default timestamp.
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpoint,org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpoint(),222,225,"/**
 * Deletes the checkpoint. Calls the overloaded method with 'false'.
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/TrashPolicyDefault.java,deleteCheckpointsImmediately,org.apache.hadoop.fs.TrashPolicyDefault:deleteCheckpointsImmediately(),227,230,"/**
 * Deletes checkpoints immediately, overriding any scheduled deletion.
 */
",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Test.java,processPath,org.apache.hadoop.fs.shell.Test:processPath(org.apache.hadoop.fs.shell.PathData),77,108,"/**
 * Processes a PathData item based on the 'flag' value.
 * Sets exitCode to 1 if processing fails.
 */
",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,access,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:access(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.permission.FsAction)",253,257,"/**
 * Checks access to a path with the given mode.
 * Delegates to the superclass implementation.
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,get,"org.apache.hadoop.fs.FileSystem:get(java.net.URI,org.apache.hadoop.conf.Configuration)",536,558,"/**
 * Retrieves a FileSystem instance for the given URI and config.
 * @param uri The URI of the filesystem.
 * @param conf Hadoop configuration.
 * @return FileSystem instance.
 */
","* Get a FileSystem for this URI's scheme and authority.
   * <ol>
   * <li>
   *   If the configuration has the property
   *   {@code ""fs.$SCHEME.impl.disable.cache""} set to true,
   *   a new instance will be created, initialized with the supplied URI and
   *   configuration, then returned without being cached.
   * </li>
   * <li>
   *   If the there is a cached FS instance matching the same URI, it will
   *   be returned.
   * </li>
   * <li>
   *   Otherwise: a new FS instance will be created, initialized with the
   *   configuration and URI, cached and returned to the caller.
   * </li>
   * </ol>
   * @param uri uri of the filesystem.
   * @param conf configrution.
   * @return filesystem instance.
   * @throws IOException if the FileSystem cannot be instantiated.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,newInstanceLocal,org.apache.hadoop.fs.FileSystem:newInstanceLocal(org.apache.hadoop.conf.Configuration),631,634,"/**
 * Creates a new LocalFileSystem instance using the given configuration.
 * @param conf Configuration object for file system creation.
 * @return A new LocalFileSystem object.
 */
","* Get a unique local FileSystem object.
   * @param conf the configuration to configure the FileSystem with
   * @return a new LocalFileSystem object.
   * @throws IOException FS creation or initialization failure.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/FsGetter.java,getNewInstance,"org.apache.hadoop.fs.viewfs.FsGetter:getNewInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",42,45,"/**
 * Creates a new FileSystem instance for the given URI and config.
 * @param uri The URI for the file system.
 * @param conf The configuration to use.
 * @return A new FileSystem instance.
 */
","* Gets new file system instance of given uri.
   * @param uri uri.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return file system.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getNewInstance,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:getNewInstance(java.net.URI,org.apache.hadoop.conf.Configuration)",234,250,"/**
 * Gets a new FileSystem instance for the given URI and configuration.
 * @param uri The URI of the file system.
 * @param conf The configuration object.
 * @return A new FileSystem instance.
 */
",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatusForFallbackLink,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatusForFallbackLink(),1244,1265,"/**
 * Lists file statuses for fallback link, adjusting paths.
 * Returns empty array if no fallback link or path exists.
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.fs.AbstractFileSystem),385,388,"/**
 * Gets a FileContext using the provided AbstractFileSystem.
 * @param defaultFS The AbstractFileSystem to use.
 */
","* Create a FileContext for specified file system using the default config.
   * 
   * @param defaultFS default fs.
   * @return a FileContext with the specified AbstractFileSystem
   *                 as the default FS.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,"org.apache.hadoop.fs.FileContext:getFileContext(java.net.URI,org.apache.hadoop.conf.Configuration)",456,473,"/**
 * Gets a FileContext for the default filesystem URI.
 * @param defaultFsUri URI of the default filesystem.
 * @param aConf Hadoop configuration.
 * @return FileContext object.
 */
","* Create a FileContext for specified default URI using the specified config.
   * 
   * @param defaultFsUri defaultFsUri.
   * @param aConf configrution.
   * @return new FileContext for specified uri
   * @throws UnsupportedFileSystemException If the file system with specified is
   *           not supported
   * @throws RuntimeException If the file system specified is supported but
   *         could not be instantiated, or if login fails.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,run,org.apache.hadoop.security.KDiag:run(java.lang.String[]),197,244,"/**
 * Parses command-line arguments, loads resources, and executes.
 * Returns 0 on success, non-zero on failure.
 */
",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,init,org.apache.hadoop.security.token.DtUtilShell:init(java.lang.String[]),116,177,"/**
 * Initializes the command based on arguments.
 * @param args Command-line arguments. Returns 0 on success, 1 on failure.
 */","* Parse the command line arguments and initialize subcommand.
   * Also will attempt to perform Kerberos login if both -principal and -keytab
   * flags are passed in args array.
   * @param args args.
   * @return 0 if the argument(s) were recognized, 1 otherwise
   * @throws Exception Exception.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/SecurityUtil.java,login,"org.apache.hadoop.security.SecurityUtil:login(org.apache.hadoop.conf.Configuration,java.lang.String,java.lang.String)",287,292,"/**
 * Logs in using provided configuration, keytab file, and username.
 * Delegates to overloaded method with localhost name.
 */
","* Login as a principal specified in config. Substitute $host in
   * user's Kerberos principal name with a dynamically looked-up fully-qualified
   * domain name of the current host.
   * 
   * @param conf
   *          conf to use
   * @param keytabFileKey
   *          the key to look for keytab file in conf
   * @param userNameKey
   *          the key to look for user's Kerberos principal name in conf
   * @throws IOException if login fails",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,call,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class,int)",544,608,"/**
 * Executes HTTP request, handles errors, and parses JSON response.
 * @param conn HttpURLConnection, expectedResponse, klass, authRetryCount
 * @return Parsed object of specified class or null.
 */",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,<init>,"org.apache.hadoop.ipc.RPC$Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager,java.lang.String)",1189,1198,"/**
 * Constructs a Server instance with the given configuration parameters.
 */
",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,org.apache.hadoop.conf.Configuration)",3264,3271,"/**
 * Constructs a Server with specified address, port, class, and handlers.
 * Delegates to a more complex constructor.
 */
",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,<init>,"org.apache.hadoop.ipc.Server:<init>(java.lang.String,int,java.lang.Class,int,int,int,org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.security.token.SecretManager)",3273,3280,"/**
 * Constructs a Server object.
 * @param bindAddress bind address, port, RPC request class, etc.
 * @throws IOException if an I/O error occurs
 */
",,,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processSaslMessage,org.apache.hadoop.ipc.Server$Connection:processSaslMessage(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto),2327,2385,"/**
 * Processes a Sasl message based on its state.
 * @param saslMessage The incoming Sasl message.
 * @return Sasl response or null if authentication is complete.
 * @throws SaslException, IOException, AccessControlException, InterruptedException
 */","* Process a saslMessge.
     * @param saslMessage received SASL message
     * @return the sasl response to send back to client
     * @throws SaslException if authentication or generating response fails, 
     *                       or SASL protocol mixup
     * @throws IOException if a SaslServer cannot be created
     * @throws AccessControlException if the requested authentication type 
     *         is not supported or trying to re-attempt negotiation.
     * @throws InterruptedException",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/RPC.java,waitForProxy,"org.apache.hadoop.ipc.RPC:waitForProxy(java.lang.Class,long,java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",305,312,"/**
 * Waits for a protocol proxy.
 * @param protocol Protocol class to wait for.
 * @return Proxy object.
 * @throws IOException if an I/O error occurs.
 */
","* Get a proxy connection to a remote server.
   *
   * @param <T> Generics Type T.
   * @param protocol protocol class
   * @param clientVersion client version
   * @param addr remote address
   * @param conf configuration to use
   * @return the proxy
   * @throws IOException if the far end through a RemoteException",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(java.lang.String),923,925,"/**
* Adds a new resource with the given name.
* @param name The name of the resource to add.
*/
","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param name resource to be added, the classpath is examined for a file 
   *             with that name.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(java.net.URL),941,943,"/**
 * Adds a resource to the system, using the provided URL.
 */","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param url url of the resource to be added, the local filesystem is 
   *            examined directly to find the resource, without referring to 
   *            the classpath.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(org.apache.hadoop.fs.Path),959,961,"/**
* Adds a resource to the resource list.
* @param file The Path object representing the resource.
*/
","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * @param file file-path of resource to be added, the local filesystem is
   *             examined directly to find the resource, without referring to 
   *             the classpath.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,addResource,org.apache.hadoop.conf.Configuration:addResource(java.io.InputStream),980,982,"/**
* Adds a resource from an input stream.
* @param in InputStream containing the resource data.
*/
","* Add a configuration resource. 
   * 
   * The properties of this resource will override properties of previously 
   * added resources, unless they were marked <a href=""#Final"">final</a>. 
   * 
   * WARNING: The contents of the InputStream will be cached, by this method. 
   * So use this sparingly because it does increase the memory consumption.
   * 
   * @param in InputStream to deserialize the object from. In will be read from
   * when a get or set is called next.  After it is read the stream will be
   * closed.",,,True,28
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/protocolPB/HAServiceProtocolClientSideTranslatorPB.java,<init>,"org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB:<init>(java.net.InetSocketAddress,org.apache.hadoop.conf.Configuration)",66,72,"/**
 * Constructs a HAServiceProtocolClientSideTranslatorPB.
 * @param addr Socket address for the RPC connection.
 * @param conf Hadoop configuration.
 */
",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doGracefulFailover,org.apache.hadoop.ha.ZKFailoverController:doGracefulFailover(),660,739,"/**
* Performs a graceful failover process to become the active node.
* Attempts to cede active status from other nodes.
*/
","* Coordinate a graceful failover. This proceeds in several phases:
   * 1) Pre-flight checks: ensure that the local node is healthy, and
   * thus a candidate for failover.
   * 2a) Determine the current active node. If it is the local node, no
   * need to failover - return success.
   * 2b) Get the other nodes
   * 3a) Ask the other nodes to yield from election for a number of seconds
   * 3b) Ask the active node to yield from the election for a number of seconds.
   * 4) Allow the normal election path to run in other threads. Wait until
   * we either become unhealthy or we see an election attempt recorded by
   * the normal code path.
   * 5) Allow the old active to rejoin the election, so a future
   * failback is possible.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,createProxy,org.apache.hadoop.ha.HealthMonitor:createProxy(),191,193,"/**
 * Creates a proxy for the HAServiceProtocol.
 * @return A proxy instance, or null if creation fails.
 */
","* Connect to the service to be monitored. Stubbed out for easier testing.
   *
   * @throws IOException raised on errors performing I/O.
   * @return HAServiceProtocol.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getHealthMonitorProxy,"org.apache.hadoop.ha.HAServiceTarget:getHealthMonitorProxy(org.apache.hadoop.conf.Configuration,int)",126,129,"/**
 * Gets a health monitor proxy.
 * @param conf Configuration object.
 * @param timeoutMs Timeout in milliseconds.
 * @throws IOException if an I/O error occurs.
 */
","* Returns a proxy to connect to the target HA service for health monitoring.
   * If {@link #getHealthMonitorAddress()} is implemented to return a non-null
   * address, then this proxy will connect to that address.  Otherwise, the
   * returned proxy defaults to using {@link #getAddress()}, which means this
   * method's behavior is identical to {@link #getProxy(Configuration, int)}.
   *
   * @param conf configuration.
   * @param timeoutMs timeout in milliseconds
   * @return a proxy to connect to the target HA service for health monitoring
   * @throws IOException if there is an error",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAServiceTarget.java,getProxy,"org.apache.hadoop.ha.HAServiceTarget:getProxy(org.apache.hadoop.conf.Configuration,int)",100,103,"/**
 * Gets a proxy using the provided configuration and timeout.
 * @param conf Configuration object. @param timeoutMs Timeout in milliseconds.
 * @return Proxy object.
 */
","* @return a proxy to connect to the target HA Service.
   * @param timeoutMs timeout in milliseconds.
   * @param conf Configuration.
   * @throws IOException raised on errors performing I/O.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/HarFileSystem.java,initialize,"org.apache.hadoop.fs.HarFileSystem:initialize(java.net.URI,org.apache.hadoop.conf.Configuration)",128,175,"/**
 * Initializes the HarFileSystem with URI and configuration.
 * @param name URI of the Har filesystem
 * @param conf Hadoop configuration
 * @throws IOException if initialization fails
 */
","* Initialize a Har filesystem per har archive. The 
   * archive home directory is the top level directory
   * in the filesystem that contains the HAR archive.
   * Be careful with this method, you do not want to go 
   * on creating new Filesystem instances per call to 
   * path.getFileSystem().
   * the uri of Har is 
   * har://underlyingfsscheme-host:port/archivepath.
   * or 
   * har:///archivepath. This assumes the underlying filesystem
   * to be used in case not specified.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Trash.java,moveToAppropriateTrash,"org.apache.hadoop.fs.Trash:moveToAppropriateTrash(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",77,122,"/**
 * Moves a file to the trash directory. Uses server defaults if available.
 * @param fs Filesystem object
 * @param p Path to move
 * @param conf Configuration object
 * @return True if moved, false otherwise.
 */
","* In case of the symlinks or mount points, one has to move the appropriate
   * trashbin in the actual volume of the path p being deleted.
   *
   * Hence we get the file system of the fully-qualified resolved-path and
   * then move the path p to the trashbin in that volume,
   * @param fs - the filesystem of path p
   * @param p - the path being deleted - to be moved to trash
   * @param conf - configuration
   * @return false if the item is already in the trash or trash is disabled
   * @throws IOException on error",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlConnection.java,connect,org.apache.hadoop.fs.FsUrlConnection:connect(),55,75,"/**
 * Connects to the specified URL, establishing an input stream.
 * Throws IOException if already connected or URI is invalid.
 */
",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(java.lang.String,org.apache.hadoop.conf.Configuration)",88,90,"/**
 * Constructs a PathData object from a path string and configuration.
 * @param pathString Path string to parse.
 * @param conf Hadoop configuration.
 * @throws IOException if an I/O error occurs.
 */
","* Creates an object to wrap the given parameters as fields.  The string
   * used to create the path will be recorded since the Path object does not
   * return exactly the same string used to initialize it
   * @param pathString a string for a path
   * @param conf the configuration file
   * @throws IOException if anything goes wrong...",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getFSofPath,"org.apache.hadoop.fs.FileSystem:getFSofPath(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",427,435,"/**
 * Gets the FileSystem of a given path.
 * @param absOrFqPath Path to get the FileSystem for.
 * @param conf Hadoop configuration.
 * @return FileSystem object.
 */
",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getNamed,"org.apache.hadoop.fs.FileSystem:getNamed(java.lang.String,org.apache.hadoop.conf.Configuration)",477,481,"/**
 * Gets a FileSystem object by name.
 * @param name FileSystem name.
 * @param conf Hadoop configuration.
 * @return FileSystem object.
 */
","* @deprecated call {@link #get(URI, Configuration)} instead.
   *
   * @param name name.
   * @param conf configuration.
   * @return file system.
   * @throws IOException If an I/O error occurred.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,getLocal,org.apache.hadoop.fs.FileSystem:getLocal(org.apache.hadoop.conf.Configuration),508,511,"/**
 * Retrieves the local file system.
 * @param conf Hadoop configuration object.
 * @return LocalFileSystem instance.
 */
","* Get the local FileSystem.
   * @param conf the configuration to configure the FileSystem with
   * if it is newly instantiated.
   * @return a LocalFileSystem
   * @throws IOException if somehow the local FS cannot be instantiated.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ChRootedFileSystem.java,<init>,"org.apache.hadoop.fs.viewfs.ChRootedFileSystem:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",124,127,"/**
 * Constructs a ChRootedFileSystem with a given URI and config.
 * @param uri The URI of the filesystem.
 * @param conf Hadoop configuration.
 * @throws IOException if an I/O error occurs.
 */
","* Constructor.
   * @param uri base file system
   * @param conf configuration
   * @throws IOException",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/FsGetter.java,get,"org.apache.hadoop.fs.viewfs.FsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)",55,57,"/**
 * Retrieves a FileSystem instance for the given URI and configuration.
 * @param uri The URI for the file system.
 * @param conf The configuration to use.
 * @return A FileSystem instance.
 */
","* Gets file system instance of given uri.
   *
   * @param uri uri.
   * @param conf configuration.
   * @throws IOException raised on errors performing I/O.
   * @return FileSystem.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,get,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme$ChildFsGetter:get(java.net.URI,org.apache.hadoop.conf.Configuration)",258,275,"/**
 * Retrieves a FileSystem for the given URI and configuration.
 * Returns default FS if scheme doesn't match, otherwise creates.
 */","* When ViewFileSystemOverloadScheme scheme and target uri scheme are
     * matching, it will not take advantage of FileSystem cache as it will
     * create instance directly. For caching needs please set
     * ""fs.viewfs.enable.inner.cache"" to true.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/Path.java,getFileSystem,org.apache.hadoop.fs.Path:getFileSystem(org.apache.hadoop.conf.Configuration),365,367,"/**
 * Retrieves a FileSystem instance using the provided configuration.
 * @param conf Configuration object for file system setup.
 * @return A FileSystem object.
 */
","* Return the FileSystem that owns this Path.
   *
   * @param conf the configuration to use when resolving the FileSystem
   * @return the FileSystem that owns this Path
   * @throws java.io.IOException thrown if there's an issue resolving the
   * FileSystem",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,getFileSystem,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:getFileSystem(),458,476,"/**
 * Gets the FileSystem, using supplied or connecting to basePath.
 * @return FileSystem object, or null if an error occurs.
 * @throws MetricsException if URI is invalid or connection fails.
 */
","* Return the supplied file system for testing or otherwise get a new file
   * system.
   *
   * @return the file system to use
   * @throws MetricsException thrown if the file system could not be retrieved",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystem.java,get,"org.apache.hadoop.fs.viewfs.ViewFileSystem$InnerCache:get(java.net.URI,org.apache.hadoop.conf.Configuration)",129,153,"/**
 * Retrieves a FileSystem for the given URI and configuration.
 * Creates a new one if not found.
 */",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFs.java,listStatus,org.apache.hadoop.fs.viewfs.ViewFs$InternalDirOfViewFs:listStatus(org.apache.hadoop.fs.Path),1159,1226,"/**
 * Lists file statuses for a given path, handling links and fallbacks.
 * @param f the path to list statuses for
 * @return array of FileStatus objects
 */","* {@inheritDoc}
     *
     * Note: listStatus on root(""/"") considers listing from fallbackLink if
     * available. If the same directory name is present in configured mount
     * path as well as in fallback link, then only the configured mount path
     * will be listed in the returned result.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(java.net.URI),440,443,"/**
 * Gets a FileContext for the given URI, using default configuration.
 * @param defaultFsUri URI of the filesystem.
 * @return FileContext object.
 */
","* Create a FileContext for specified URI using the default config.
   * 
   * @param defaultFsUri defaultFsUri.
   * @return a FileContext with the specified URI as the default FS.
   * 
   * @throws UnsupportedFileSystemException If the file system for
   *           <code>defaultFsUri</code> is not supported",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(org.apache.hadoop.conf.Configuration),485,496,"/**
 * Gets a FileContext based on the configuration.
 * @param aConf Hadoop configuration object.
 * @throws UnsupportedFileSystemException if no scheme is found.
 */
","* Create a FileContext using the passed config. Generally it is better to use
   * {@link #getFileContext(URI, Configuration)} instead of this one.
   * 
   * 
   * @param aConf configration.
   * @return new FileContext
   * @throws UnsupportedFileSystemException If file system in the config
   *           is not supported",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getLocalFSFileContext,org.apache.hadoop.fs.FileContext:getLocalFSFileContext(org.apache.hadoop.conf.Configuration),506,509,"/**
 * Gets a local FileSystem's FileContext from the given configuration.
 * @param aConf Hadoop configuration object.
 * @return Local FileContext.
 */
","* @param aConf - from which the FileContext is configured
   * @return a FileContext for the local file system using the specified config.
   * 
   * @throws UnsupportedFileSystemException If default file system in the config
   *           is not supported
   *",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,init,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:init(org.apache.commons.configuration2.SubsetConfiguration),233,265,"/**
 * Initializes the metrics configuration using provided settings.
 * @param metrics2Properties Configuration settings for metrics.
 */
",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,call,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:call(java.net.HttpURLConnection,java.lang.Object,int,java.lang.Class)",539,542,"/**
 * Delegates to overloaded call method with default authRetry.
 * @param conn HttpURLConnection, expectedResponse, klass - see other call.
 * @return Object of specified class.
 */
",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine2$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",479,493,"/**
 * Constructs a Server instance with specified configuration parameters.
 */","* Construct an RPC server.
     *
     * @param protocolClass the class of protocol
     * @param protocolImpl the protocolImpl whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param numReaders number of read threads
     * @param queueSizePerHandler the size of the queue contained
     *                            in each Handler
     * @param verbose whether each call should be logged
     * @param secretManager the server-side secret manager for each token type
     * @param portRangeConfig A config parameter that can be used to restrict
     * the range of ports used when port is 0 (an ephemeral port)
     * @param alignmentContext provides server state info on client responses
     * @throws IOException raised on errors performing I/O.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",495,535,"/**
 * Constructs a Server instance with specified configurations.
 * @param protocolClass Protocol class, or null to derive from impl.
 */","* Construct an RPC server.
     * @param protocolClass - the protocol being registered
     *     can be null for compatibility with old usage (see below for details)
     * @param protocolImpl the protocol impl that will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param alignmentContext provides server state info on client responses
     * @param numReaders input numReaders.
     * @param portRangeConfig input portRangeConfig.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param secretManager input secretManager.
     * @throws IOException raised on errors performing I/O.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,saslProcess,org.apache.hadoop.ipc.Server$Connection:saslProcess(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcSaslProto),2242,2314,"/**
 * Processes a SASL message, authenticates the user, and sends reply.
 */","* Process saslMessage and send saslResponse back
     * @param saslMessage received SASL message
     * @throws RpcServerException setup failed due to SASL negotiation
     *         failure, premature or invalid connection context, or other state 
     *         errors. This exception needs to be sent to the client. This 
     *         exception will wrap {@link RetriableException}, 
     *         {@link InvalidToken}, {@link StandbyException} or 
     *         {@link SaslException}.
     * @throws IOException if sending reply fails
     * @throws InterruptedException",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,readSSLConfiguration,"org.apache.hadoop.security.ssl.SSLFactory:readSSLConfiguration(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.ssl.SSLFactory$Mode)",162,184,"/**
 * Reads SSL configuration from resource or falls back to input config.
 * @param conf Base Configuration object.
 * @param mode Mode (CLIENT or SERVER).
 * @return SSL Configuration object.
 */
",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/authorize/ServiceAuthorizationManager.java,refresh,"org.apache.hadoop.security.authorize.ServiceAuthorizationManager:refresh(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",140,150,"/**
 * Refreshes policy using a configuration file and provider.
 * @param conf Configuration object.
 * @param provider Policy provider.
 */
",,,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/HCFSMountTableConfigLoader.java,load,"org.apache.hadoop.fs.viewfs.HCFSMountTableConfigLoader:load(java.lang.String,org.apache.hadoop.conf.Configuration)",57,113,"/**
 * Loads mount table configuration from a path.
 * @param mountTableConfigPath Path to the mount table configuration.
 * @param conf Configuration object to load into.
 */","* Loads the mount-table configuration from hadoop compatible file system and
   * add the configuration items to given configuration. Mount-table
   * configuration format should be suffixed with version number.
   * Format: {@literal mount-table.<versionNumber>.xml}
   * Example: mount-table.1.xml
   * When user wants to update mount-table, the expectation is to upload new
   * mount-table configuration file with monotonically increasing integer as
   * version number. This API loads the highest version number file. We can
   * also configure single file path directly.
   *
   * @param mountTableConfigPath : A directory path where mount-table files
   *          stored or a mount-table file path. We recommend to configure
   *          directory with the mount-table version files.
   * @param conf : to add the mount table as resource.",,,True,29
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,tryConnect,org.apache.hadoop.ha.HealthMonitor:tryConnect(),170,183,"/**
 * Attempts to create a proxy connection.
 * Sets proxy to null and updates state if connection fails.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,isOtherTargetNodeActive,"org.apache.hadoop.ha.HAAdmin:isOtherTargetNodeActive(java.lang.String,boolean)",187,215,"/**
 * Checks if other target nodes are active, returns true if any are.
 * @param targetNodeToActivate target node ID
 * @param forceActive force active flag
 * @return true if any target node is active, false otherwise.
 */
","* Checks whether other target node is active or not
   * @param targetNodeToActivate
   * @return true if other target node is active or some other exception 
   * occurred and forceActive was set otherwise false
   * @throws IOException",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,transitionToStandby,org.apache.hadoop.ha.HAAdmin:transitionToStandby(org.apache.commons.cli.CommandLine),217,234,"/**
 * Transitions a service to standby mode.
 * @param cmd Command line arguments; expects target name.
 * @return 0 on success, -1 on failure.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,checkHealth,org.apache.hadoop.ha.HAAdmin:checkHealth(org.apache.commons.cli.CommandLine),292,309,"/**
 * Checks the health of a service.
 * @param cmd Command line arguments; expects a target service name.
 * @return 0 on success, -1 on failure.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,getServiceState,org.apache.hadoop.ha.HAAdmin:getServiceState(org.apache.commons.cli.CommandLine),311,324,"/**
 * Retrieves service state from a proxy.
 * @param cmd Command line arguments; expects a target service name.
 * @return 0 on success, -1 on failure.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,getAllServiceState,org.apache.hadoop.ha.HAAdmin:getAllServiceState(),443,464,"/**
 * Retrieves and prints service states for each target.
 * Returns 0 on success, -1 if target IDs are unavailable.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,becomeActive,org.apache.hadoop.ha.ZKFailoverController:becomeActive(),408,443,"/**
 * Transitions the service to an active state, throwing ServiceFailedException on failure.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,becomeStandby,org.apache.hadoop.ha.ZKFailoverController:becomeStandby(),514,529,"/**
 * Transitions the local target to standby state, handling potential errors.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doCedeActive,org.apache.hadoop.ha.ZKFailoverController:doCedeActive(int),590,623,"/**
 * Cedes the active role, transitioning the local node to standby.
 * @param millisToCede Milliseconds to delay rejoining the election.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,preFailoverChecks,"org.apache.hadoop.ha.FailoverController:preFailoverChecks(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean)",109,156,"/**
 * Performs pre-failover checks on the target service.
 * @param from Source service, @param target Target service.
 * @param forceActive Forces failover even if not ready.
 * @throws FailoverFailedException if checks fail.
 */
","* Perform pre-failover checks on the given service we plan to
   * failover to, eg to prevent failing over to a service (eg due
   * to it being inaccessible, already active, not healthy, etc).
   *
   * An option to ignore toSvc if it claims it is not ready to
   * become active is provided in case performing a failover will
   * allow it to become active, eg because it triggers a log roll
   * so the standby can learn about new blocks and leave safemode.
   *
   * @param from currently active service
   * @param target service to make active
   * @param forceActive ignore toSvc if it reports that it is not ready
   * @throws FailoverFailedException if we should avoid failover",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,tryGracefulFence,org.apache.hadoop.ha.FailoverController:tryGracefulFence(org.apache.hadoop.ha.HAServiceTarget),168,186,"/**
 * Attempts to transition the service to standby gracefully.
 * @param svc The HAServiceTarget to transition.
 * @return True if successful, false otherwise.
 */
","* Try to get the HA state of the node at the given address. This
   * function is guaranteed to be ""quick"" -- ie it has a short timeout
   * and no retries. Its only purpose is to avoid fencing a node that
   * has already restarted.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,moveToTrash,org.apache.hadoop.fs.shell.Delete$Rm:moveToTrash(org.apache.hadoop.fs.shell.PathData),151,167,"/**
* Moves an item to the trash.
* @param item PathData object representing the item to move.
* @throws IOException if move fails.
*/
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsUrlConnection.java,getInputStream,org.apache.hadoop.fs.FsUrlConnection:getInputStream(),77,83,"/**
 * Returns the input stream. Connects if it's null.
 * @return InputStream object
 * @throws IOException if an I/O error occurs
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,recursePath,org.apache.hadoop.fs.shell.find.Find:recursePath(org.apache.hadoop.fs.shell.PathData),345,371,"/**
 * Recursively processes a path item, handling stops, depth limits,
 * and symlinks.  Modifies 'item' if following a symlink.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/find/Find.java,isPathRecursable,org.apache.hadoop.fs.shell.find.Find:isPathRecursable(org.apache.hadoop.fs.shell.PathData),373,392,"/**
 * Checks if a path is recursable, following symlinks based on options.
 * @param item PathData object to check.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Head.java,expandArgument,org.apache.hadoop.fs.shell.Head:expandArgument(java.lang.String),56,61,"/**
 * Expands an argument string into a list of PathData objects.
 * @param arg The argument string to expand.
 * @return A list containing a single PathData object.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Tail.java,expandArgument,org.apache.hadoop.fs.shell.Tail:expandArgument(java.lang.String),81,86,"/**
 * Expands a string argument into a list of PathData objects.
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystemLinkResolver.java,resolve,"org.apache.hadoop.fs.FileSystemLinkResolver:resolve(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path)",71,111,"/**
 * Resolves a path, following symlinks if enabled.
 * @param filesys The file system.
 * @param path The path to resolve.
 * @return Resolved object of type T.
 * @throws IOException If resolution fails.
 */
","* Attempt calling overridden {@link #doCall(Path)} method with
   * specified {@link FileSystem} and {@link Path}. If the call fails with an
   * UnresolvedLinkException, it will try to resolve the path and retry the call
   * by calling {@link #next(FileSystem, Path)}.
   * @param filesys FileSystem with which to try call
   * @param path Path with which to try call
   * @return Generic type determined by implementation
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,<init>,"org.apache.hadoop.fs.shell.PathData:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",100,102,"/**
 * Constructs a PathData object using a local URI and configuration.
 * @param localPath The local URI path.
 * @param conf The Hadoop configuration.
 */
","* Creates an object to wrap the given parameters as fields.  The string
   * used to create the path will be recorded since the Path object does not
   * return exactly the same string used to initialize it
   * @param localPath a local URI
   * @param conf the configuration file
   * @throws IOException if anything goes wrong...",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",2571,2576,"/**
 * Copies files from local paths to a destination path.
 * @param delSrc Delete source files after copying.
 * @param overwrite Overwrite existing files at destination.
 */
","* The src files are on the local disk.  Add it to the filesystem at
   * the given dst name.
   * delSrc indicates if the source should be removed
   * @param delSrc whether to delete the src
   * @param overwrite whether to overwrite an existing file
   * @param srcs array of paths which are source
   * @param dst path
   * @throws IOException IO failure",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2588,2593,"/**
 * Copies a local file to the destination path.
 * @param delSrc Delete source file after copy.
 * @param overwrite Overwrite destination if it exists.
 */
","* The src file is on the local disk.  Add it to the filesystem at
   * the given dst name.
   * delSrc indicates if the source should be removed
   * @param delSrc whether to delete the src
   * @param overwrite whether to overwrite an existing file
   * @param src path
   * @param dst path
   * @throws IOException IO failure",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",2648,2658,"/**
 * Copies a file from a source to a destination Path.
 * @param delSrc Delete source file after copy.
 * @param src Source Path.
 * @param dst Destination Path.
 * @param useRawLocalFileSystem Use raw local file system.
 */
","* The src file is under this filesystem, and the dst is on the local disk.
   * Copy it from the remote filesystem to the local dst name.
   * delSrc indicates if the src will be removed
   * or not. useRawLocalFileSystem indicates whether to use RawLocalFileSystem
   * as the local file system or not. RawLocalFileSystem is non checksumming,
   * So, It will not create any crc files at local.
   *
   * @param delSrc
   *          whether to delete the src
   * @param src
   *          path
   * @param dst
   *          path
   * @param useRawLocalFileSystem
   *          whether to use RawLocalFileSystem as local file system or not.
   *
   * @throws IOException for any IO error",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,confChanged,org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:confChanged(org.apache.hadoop.conf.Configuration),309,360,"/**
 * Updates the context with new local directories from the configuration.
 * @param conf Hadoop configuration object
 * @return Updated Context object
 */","This method gets called everytime before any read/write to make sure
     * that any change to localDirs is reflected immediately.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/conf/Configuration.java,getLocalPath,"org.apache.hadoop.conf.Configuration:getLocalPath(java.lang.String,java.lang.String)",2828,2848,"/**
 * Gets a local path, trying each directory in dirsProp.
 * @param dirsProp Comma-separated list of local directories.
 * @param path The path to create.
 * @return The local Path object.
 * @throws IOException If no valid local directories are found.
 */
","* Get a local file under a directory named by <i>dirsProp</i> with
   * the given <i>path</i>.  If <i>dirsProp</i> contains multiple directories,
   * then one is chosen based on <i>path</i>'s hash code.  If the selected
   * directory does not exist, an attempt is made to create it.
   *
   * @param dirsProp directory in which to locate the file.
   * @param path file-path.
   * @return local file under the directory with the given path.
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem$NflyNode:<init>(java.lang.String,java.lang.String,java.net.URI,org.apache.hadoop.conf.Configuration)",94,97,"/**
 * Constructs a NflyNode with a ChRootedFileSystem.
 * @param hostName Node hostname.
 * @param rackName Node rack name.
 * @param uri URI for the filesystem.
 * @param conf Configuration object.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getRawFileSystem,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getRawFileSystem(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",328,340,"/**
 * Resolves the raw FileSystem for a given path.
 * @param path The path to resolve.
 * @param conf Hadoop configuration.
 * @return The raw FileSystem.
 */
","* This is an admin only API to give access to its child raw file system, if
   * the path is link. If the given path is an internal directory(path is from
   * mount paths tree), it will initialize the file system of given path uri
   * directly. If path cannot be resolved to any internal directory or link, it
   * will throw NotInMountpointException. Please note, this API will not return
   * chrooted file system. Instead, this API will get actual raw file system
   * instances.
   *
   * @param path - fs uri path
   * @param conf - configuration
   * @throws IOException raised on errors performing I/O.
   * @return file system.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/ViewFileSystemOverloadScheme.java,getMountPathInfo,"org.apache.hadoop.fs.viewfs.ViewFileSystemOverloadScheme:getMountPathInfo(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",351,372,"/**
 * Resolves mount path info for a given path and configuration.
 * @param path The path to resolve.
 * @param conf Hadoop configuration.
 * @return MountPathInfo object.
 */
","* Gets the mount path info, which contains the target file system and
   * remaining path to pass to the target file system.
   *
   * @param path the path.
   * @param conf configuration.
   * @return mount path info.
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/PathData.java,expandAsGlob,"org.apache.hadoop.fs.shell.PathData:expandAsGlob(java.lang.String,org.apache.hadoop.conf.Configuration)",344,396,"/**
 * Expands a glob pattern into an array of PathData objects.
 * @param pattern Glob pattern to expand.
 * @param conf Hadoop configuration.
 * @return Array of PathData objects matching the pattern.
 */
","* Expand the given path as a glob pattern.  Non-existent paths do not
   * throw an exception because creation commands like touch and mkdir need
   * to create them.  The ""stat"" field will be null if the path does not
   * exist.
   * @param pattern the pattern to expand as a glob
   * @param conf the hadoop configuration
   * @return list of {@link PathData} objects.  if the pattern is not a glob,
   * and does not exist, the list will contain a single PathData with a null
   * stat 
   * @throws IOException anything else goes wrong...",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",1894,1932,"/**
 * Creates a Reader with specified configuration and options.
 * @param conf Configuration object
 * @param opts Option array for file/stream settings
 * @throws IOException if file or stream option is missing
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,initBloomFilter,"org.apache.hadoop.io.BloomMapFile$Reader:initBloomFilter(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",237,254,"/**
 * Initializes Bloom filter from file or falls back to MapFile.
 * @param dirName Path to directory containing Bloom filter file.
 * @param conf Hadoop configuration.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFileDumper.java,dumpInfo,"org.apache.hadoop.io.file.tfile.TFileDumper:dumpInfo(java.lang.String,java.io.PrintStream,org.apache.hadoop.conf.Configuration)",96,295,"/**
 * Dumps TFile information to a PrintStream.
 * @param file TFile path
 * @param out PrintStream to write output to
 * @param conf Hadoop configuration
 */
","* Dump information about TFile.
   * 
   * @param file
   *          Path string of the TFile
   * @param out
   *          PrintStream to output the information.
   * @param conf
   *          The configuration object.
   * @throws IOException",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,readTokenStorageFile,"org.apache.hadoop.security.Credentials:readTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",225,241,"/**
 * Reads credentials from a token storage file.
 * @param filename Path to the token storage file.
 * @param conf Hadoop configuration.
 * @return Credentials object.
 * @throws IOException if an I/O error occurs.
 */
","* Convenience method for reading a token storage file and loading its Tokens.
   * @param filename filename.
   * @param conf configuration.
   * @throws IOException  raised on errors performing I/O.
   * @return Credentials.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageFile,"org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.Credentials$SerializedFormat)",341,347,"/**
 * Writes token storage to a file.
 * @param filename File to write to.
 * @param conf Hadoop configuration.
 * @param format Serialization format.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/KeyStoreProvider.java,initFileSystem,org.apache.hadoop.security.alias.KeyStoreProvider:initFileSystem(java.net.URI),81,85,"/**
 * Initializes the file system using the provided URI.
 * @param uri The URI representing the file system.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/JavaKeyStoreProvider.java,<init>,"org.apache.hadoop.crypto.key.JavaKeyStoreProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",129,138,"/**
 * Initializes the JavaKeyStoreProvider with URI and configuration.
 * @param uri URI of the keystore.
 * @param conf Configuration object.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,getLibJars,org.apache.hadoop.util.GenericOptionsParser:getLibJars(org.apache.hadoop.conf.Configuration),372,389,"/**
 * Retrieves an array of URLs for libjars specified in the configuration.
 * @param conf Hadoop configuration object
 * @return URL array of libjars, or null if none are specified.
 */
","* If libjars are set in the conf, parse the libjars.
   * @param conf input Configuration.
   * @return libjar urls
   * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,initFs,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:initFs(),271,304,"/**
 * Initializes the file system, creating the base directory.
 * Returns true on success, throws exception if creation fails.
 */
","* Initialize the connection to HDFS and create the base directory. Also
   * launch the flush thread.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getLocalFSFileContext,org.apache.hadoop.fs.FileContext:getLocalFSFileContext(),426,429,"/**
 * Gets the file context for the local file system.
 * @return FileContext object for the local file system.
 */
","* @return a FileContext for the local file system using the default config.
   * @throws UnsupportedFileSystemException If the file system for
   *           {@link FsConstants#LOCAL_FS_URI} is not supported.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,<init>,org.apache.hadoop.fs.shell.Display$AvroFileInputStream:<init>(org.apache.hadoop.fs.FileStatus),278,289,"/**
 * Initializes AvroFileInputStream from a FileStatus.
 * @param status FileStatus object representing the Avro file.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileContext.java,getFileContext,org.apache.hadoop.fs.FileContext:getFileContext(),416,419,"/**
 * Gets the default file context using the default configuration.
 * @throws UnsupportedFileSystemException if FS is not supported.
 */
","* Create a FileContext using the default config read from the
   * $HADOOP_CONFIG/core.xml, Unspecified key-values for config are defaulted
   * from core-defaults.xml in the release jar.
   * 
   * @throws UnsupportedFileSystemException If the file system from the default
   *           configuration is not supported
   * @return file context.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeyVersion,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeyVersion(java.lang.String),616,624,"/**
 * Retrieves a KeyVersion by versionName.
 * @param versionName Version name of the KeyVersion.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getCurrentKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getCurrentKey(java.lang.String),626,634,"/**
 * Retrieves the current version of a key by name.
 * @param name Key name.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeys,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeys(),636,644,"/**
 * Retrieves a list of keys from the KMS REST API.
 * @return A list of strings representing the keys.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeysMetadata,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeysMetadata(java.lang.String[]),675,694,"/**
 * Retrieves metadata for specified keys.
 * @param keyNames array of key names to fetch metadata for
 * @return Array of Metadata objects.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKeyInternal,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKeyInternal(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",696,722,"/**
 * Creates a key version with the given name, material, and options.
 * @param name Key name
 * @param material Key material
 * @param options Key version options
 * @return KeyVersion object
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,invalidateCache,org.apache.hadoop.crypto.key.kms.KMSClientProvider:invalidateCache(java.lang.String),741,750,"/**
 * Invalidates cache with the given name.
 * @param name Cache name to invalidate.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,decryptEncryptedKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:decryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),801,835,"/**
 * Decrypts an encrypted key version using KMS.
 * @param encryptedKeyVersion Encrypted key version to decrypt.
 * @return Decrypted KeyVersion object.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,reencryptEncryptedKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:reencryptEncryptedKey(org.apache.hadoop.crypto.key.KeyProviderCryptoExtension$EncryptedKeyVersion),837,864,"/**
 * Re-encrypts an encrypted key version using KMS.
 * @param ekv The EncryptedKeyVersion to re-encrypt.
 * @return Re-encrypted EncryptedKeyVersion.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,reencryptEncryptedKeys,org.apache.hadoop.crypto.key.kms.KMSClientProvider:reencryptEncryptedKeys(java.util.List),866,906,"/**
 * Re-encrypts a list of encrypted key versions.
 * @param ekvs List of encrypted key versions to re-encrypt.
 * @throws IOException, GeneralSecurityException if re-encryption fails.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getKeyVersions,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getKeyVersions(java.lang.String),908,923,"/**
 * Retrieves key versions by name.
 * @param name Key name to fetch versions for.
 * @return List of KeyVersion objects or an empty list.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,getMetadata,org.apache.hadoop.crypto.key.kms.KMSClientProvider:getMetadata(java.lang.String),925,933,"/**
 * Retrieves metadata for a resource by name.
 * @param name Resource name; must not be empty.
 * @return Metadata object or null if not found.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,deleteKey,org.apache.hadoop.crypto.key.kms.KMSClientProvider:deleteKey(java.lang.String),935,941,"/**
 * Deletes a key by name.
 * @param name The name of the key to delete.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine2.java,getServer,"org.apache.hadoop.ipc.ProtobufRpcEngine2:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",380,390,"/**
 * Creates and returns a new RPC server instance.
 * @param conf Hadoop configuration
 * @return RPC.Server object
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,<init>,"org.apache.hadoop.ipc.ProtobufRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",446,455,"/**
 * Constructs a Server instance with specified configurations.
 * @param alignmentContext Alignment context for server.
 */","* Construct an RPC server.
     * 
     * @param protocolClass the class of protocol
     * @param protocolImpl the protocolImpl whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param portRangeConfig A config parameter that can be used to restrict
     * the range of ports used when port is 0 (an ephemeral port)
     * @param alignmentContext provides server state info on client responses
     * @param secretManager input secretManager.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param numReaders input numReaders.
     * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,getServer,"org.apache.hadoop.ipc.WritableRpcEngine:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",371,382,"/**
 * Creates and returns a new RPC server instance.
 * @param protocolClass Protocol class.
 * @param protocolImpl Protocol implementation.
 * @return RPC server instance.
 */
",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager,java.lang.String)",466,476,"/**
 * Constructs a Server with specified parameters.
 * Delegates to a constructor with null values for additional params.
 */","* Construct an RPC server.
     * @param protocolClass - the protocol being registered
     *     can be null for compatibility with old usage (see below for details)
     * @param protocolImpl the protocol impl that will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param secretManager input secretManager.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param portRangeConfig input portRangeConfig.
     * @param numReaders input numReaders.
     *
     * @deprecated use Server#Server(Class, Object,
     *      Configuration, String, int, int, int, int, boolean, SecretManager)
     * @throws IOException raised on errors performing I/O.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,saslReadAndProcess,org.apache.hadoop.ipc.Server$Connection:saslReadAndProcess(org.apache.hadoop.ipc.RpcWritable$Buffer),2178,2196,"/**
 * Processes a SASL message from the buffer.
 * @param buffer RpcWritable buffer containing the SASL message.
 * @throws RpcServerException, IOException, InterruptedException
 */",,,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/ssl/SSLFactory.java,<init>,"org.apache.hadoop.security.ssl.SSLFactory:<init>(org.apache.hadoop.security.ssl.SSLFactory$Mode,org.apache.hadoop.conf.Configuration)",136,160,"/**
 * Creates an SSLFactory with the given mode and configuration.
 * @param mode SSL mode
 * @param conf Configuration object
 */
","* Creates an SSLFactory.
   *
   * @param mode SSLFactory mode, client or server.
   * @param conf Hadoop configuration from where the SSLFactory configuration
   * will be read.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,refreshServiceAcl,"org.apache.hadoop.ipc.Server:refreshServiceAcl(org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.authorize.PolicyProvider)",767,769,"/**
* Refreshes the service ACL using the provided configuration and provider.
*/
","* Refresh the service authorization ACL for the service handled by this server.
   *
   * @param conf input Configuration.
   * @param provider input PolicyProvider.",,,True,30
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HealthMonitor.java,loopUntilConnected,org.apache.hadoop.ha.HealthMonitor:loopUntilConnected(),161,168,"/**
 * Retries connection until a proxy is successfully established.
 * Retries every connectRetryInterval milliseconds.
 */
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,transitionToActive,org.apache.hadoop.ha.HAAdmin:transitionToActive(org.apache.commons.cli.CommandLine),155,178,"/**
 * Transitions a service target to active, checking for conflicts.
 * @param cmd CommandLine object containing arguments.
 * @return 0 on success, -1 on failure.
 */
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,doFence,org.apache.hadoop.ha.ZKFailoverController:doFence(org.apache.hadoop.ha.HAServiceTarget),543,566,"/**
 * Attempts to gracefully fence a target. Throws exception on failure.
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/FailoverController.java,failover,"org.apache.hadoop.ha.FailoverController:failover(org.apache.hadoop.ha.HAServiceTarget,org.apache.hadoop.ha.HAServiceTarget,boolean,boolean)",198,260,"/**
 * Performs a failover from 'fromSvc' to 'toSvc', optionally fencing.
 * @param fromSvc Source service to failover from.
 * @param toSvc Target service to failover to.
 */
","* Failover from service 1 to service 2. If the failover fails
   * then try to failback.
   *
   * @param fromSvc currently active service
   * @param toSvc service to make active
   * @param forceFence to fence fromSvc even if not strictly necessary
   * @param forceActive try to make toSvc active even if it is not ready
   * @throws FailoverFailedException if the failover fails",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Delete.java,processPath,org.apache.hadoop.fs.shell.Delete$Rm:processPath(org.apache.hadoop.fs.shell.PathData),110,127,"/**
 * Processes a path item, moving to trash or deleting.
 * Throws PathIsDirectoryException if directory and deleteDirs is false.
 */
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,expandArgument,org.apache.hadoop.fs.shell.CopyCommands$Put:expandArgument(java.lang.String),295,304,"/**
 * Expands an argument string into a list of PathData objects.
 * Handles URI syntax exceptions by treating arg as a path.
 */
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,processOptions,org.apache.hadoop.fs.shell.CopyCommands$Merge:processOptions(java.util.LinkedList),71,89,"/**
 * Parses command-line arguments and initializes processing variables.
 */",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CopyCommands.java,expandArgument,org.apache.hadoop.fs.shell.CopyCommands$AppendToFile:expandArgument(java.lang.String),357,375,"/**
 * Expands an argument, handling stdin or parsing as a URI/Path.
 * @param arg Argument to expand.
 * @return List of PathData objects.
 */
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,getLocalDestination,org.apache.hadoop.fs.shell.CommandWithDestination:getLocalDestination(java.util.LinkedList),182,195,"/**
 * Resolves the local destination path from arguments.
 * @param args List of arguments, last one is the path.
 * @throws IOException if path is invalid.
 */
","*  The last arg is expected to be a local path, if only one argument is
   *  given then the destination will be the current directory 
   *  @param args is the list of arguments
   * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,moveFromLocalFile,"org.apache.hadoop.fs.FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",2530,2533,"/**
* Moves files from source paths to the destination path.
* @param srcs Source file paths to move.
* @param dst Destination path.
*/
","* The src files is on the local disk.  Add it to filesystem at
   * the given dst name, removing the source afterwards.
   * @param srcs source paths
   * @param dst path
   * @throws IOException IO failure",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",360,365,"/**
* Copies files from local file system to a destination path.
* @param delSrc Delete source files after copy.
* @param overwrite Overwrite existing files at destination.
* @param srcs Source paths.
* @param dst Destination path.
*/
","* The src files are on the local disk.  Add it to FS at
   * the given dst name.
   * delSrc indicates if the source should be removed",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2556,2559,"/**
 * Copies a local file to a destination. Deletes source if delSrc is true.
 * @param delSrc Delete source file after copy.
 * @param src Source file path.
 * @param dst Destination file path.
 */
","* The src file is on the local disk.  Add it to the filesystem at
   * the given dst name.
   * delSrc indicates if the source should be removed
   * @param delSrc whether to delete the src
   * @param src path
   * @param dst path
   * @throws IOException IO failure.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",372,377,"/**
 * Copies a local file to the file system.
 * @param delSrc Delete source file after copy.
 * @param overwrite Overwrite destination if exists.
 * @param src Source path.
 * @param dst Destination path.
 */
","* The src file is on the local disk.  Add it to FS at
   * the given dst name.
   * delSrc indicates if the source should be removed",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2624,2627,"/**
 * Copies a file from source to destination, optionally deleting source.
 * @param delSrc Delete source file after copy?
 * @param src Source file path.
 * @param dst Destination file path.
 */
","* Copy it a file from a remote filesystem to the local one.
   * delSrc indicates if the src will be removed or not.
   * @param delSrc whether to delete the src
   * @param src path src file in the remote filesystem
   * @param dst path local destination
   * @throws IOException IO failure",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)",394,492,"/**
 * Finds a local path for writing data, considering size and configuration.
 * @param pathStr path string, @param size size of data, @param conf Hadoop configuration
 * @param checkWrite whether to check write permissions
 * @return Path object or null if not found
 * @throws IOException if no suitable path is found
 */","Get a path from the local FS. If size is known, we go
     *  round-robin over the set of disks (via the configured dirs) and return
     *  the first complete path which has enough space.
     *  
     *  If size is not known, use roulette selection -- pick directories
     *  with probability proportional to their available space.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathToRead,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",518,539,"/**
 * Finds a local path for reading, throwing an error if not found.
 * @param pathStr path string to search for
 * @param conf Hadoop configuration
 * @return Path object if found, otherwise throws DiskErrorException
 */
","Get a path from the local FS for reading. We search through all the
     *  configured dirs for the file's existence and return the complete
     *  path to the file when we find one",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getAllLocalPathsToRead,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",603,610,"/**
 * Gets all local paths to read from a given path string.
 * @param pathStr Path string to iterate over.
 * @param conf Hadoop configuration.
 * @return Iterable of Path objects.
 */
","* Get all of the paths that currently exist in the working directories.
     * @param pathStr the path underneath the roots
     * @param conf the configuration to look up the roots in
     * @return all of the paths that exist under any of the roots
     * @throws IOException",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet,org.apache.hadoop.fs.viewfs.FsGetter)",228,275,"/**
 * Constructs a NflyFSystem with given URIs, config, replication, flags, and fsGetter.
 */
","* Creates a new Nfly instance.
   *
   * @param uris the list of uris in the mount point
   * @param conf configuration object
   * @param minReplication minimum copies to commit a write op
   * @param nflyFlags modes such readMostRecent
   * @param fsGetter to get the file system instance with the given uri
   * @throws IOException",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,runAll,org.apache.hadoop.fs.shell.Command:runAll(),128,142,"/**
 * Runs all specified sources, returning 0 on success, -1 on error.
 */","* For each source path, execute the command
   * 
   * @return 0 if it runs successfully; -1 if it fails",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,expandArgument,org.apache.hadoop.fs.shell.Command:expandArgument(java.lang.String),264,271,"/**
 * Expands a glob argument into a list of PathData objects.
 * @param arg The glob argument to expand.
 * @return List of PathData objects.
 */
","* Expand the given argument into a list of {@link PathData} objects.
   * The default behavior is to expand globs.  Commands may override to
   * perform other expansions on an argument.
   * @param arg string pattern to expand
   * @return list of {@link PathData} objects
   * @throws IOException if anything goes wrong...",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/CommandWithDestination.java,getRemoteDestination,org.apache.hadoop.fs.shell.CommandWithDestination:getRemoteDestination(java.util.LinkedList),203,221,"/**
 * Determines the remote destination path from arguments.
 * @param args Command-line arguments, last is the path.
 * @throws IOException if path is invalid or not found.
 */
","*  The last arg is expected to be a remote path, if only one argument is
   *  given then the destination will be the remote user's directory 
   *  @param args is the list of arguments
   *  @throws PathIOException if path doesn't exist or matches too many times",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",1942,1946,"/**
 * Constructs a Reader with a qualified path.
 * @param fs FileSystem object
 * @param file Path object
 * @param conf Configuration object
 * @throws IOException if an I/O error occurs
 */
","* Construct a reader by opening a file from the given file system.
     * @param fs The file system used to open the file.
     * @param file The file being read.
     * @param conf Configuration
     * @throws IOException raised on errors performing I/O.
     * @deprecated Use Reader(Configuration, Option...) instead.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Reader:<init>(org.apache.hadoop.fs.FSDataInputStream,int,long,long,org.apache.hadoop.conf.Configuration)",1958,1962,"/**
 * Constructs a Reader with configured stream, start, and length.
 * @param in Input stream, buffersize, start, length, and Configuration.
 */","* Construct a reader by the given input stream.
     * @param in An input stream.
     * @param buffersize unused
     * @param start The starting position.
     * @param length The length being read.
     * @param conf Configuration
     * @throws IOException raised on errors performing I/O.
     * @deprecated Use Reader(Configuration, Reader.Option...) instead.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,createDataFileReader,"org.apache.hadoop.io.MapFile$Reader:createDataFileReader(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",568,575,"/**
 * Creates a SequenceFile reader with specified options.
 * @param dataFile Path to the SequenceFile
 * @param conf Hadoop configuration
 * @param options Reader options
 * @return SequenceFile.Reader instance
 */
","* Override this method to specialize the type of
     * {@link SequenceFile.Reader} returned.
     *
     * @param dataFile data file.
     * @param conf configuration.
     * @param options options.
     * @throws IOException raised on errors performing I/O.
     * @return SequenceFile.Reader.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,nextRawKey,org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor:nextRawKey(),3832,3857,"/**
 * Reads the next raw key from the input stream.
 * @return True if a key was read, false otherwise.
 * @throws IOException if an I/O error occurs.
 */
","* Fills up the rawKey object with the key returned by the Reader.
       * @return true if there is a key returned; false, otherwise
       * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",1071,1195,"/**
 * Creates a Writer with options for file system, compression, etc.
 * @param conf Hadoop configuration object
 * @param opts Writer options
 * @throws IOException if an I/O error occurs
 */
","* Construct a uncompressed writer from a set of options.
     * @param conf the configuration to use
     * @param opts the options used when creating the writer
     * @throws IOException if it fails",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/file/tfile/TFile.java,main,org.apache.hadoop.io.file.tfile.TFile:main(java.lang.String[]),2348,2366,"/**
 * Dumps information for each TFile specified in the arguments.
 * @param args Command-line arguments representing TFile paths.
 */
","* Dumping the TFile information.
   * 
   * @param args
   *          A list of TFile paths.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/Credentials.java,writeTokenStorageFile,"org.apache.hadoop.security.Credentials:writeTokenStorageFile(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration)",335,339,"/**
 * Writes token storage file to the specified path, using default format.
 * @param filename File path to write the token storage.
 * @param conf Configuration object.
 */
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,doFormattedWrite,"org.apache.hadoop.security.token.DtFileOperations:doFormattedWrite(java.io.File,java.lang.String,org.apache.hadoop.security.Credentials,org.apache.hadoop.conf.Configuration)",104,114,"/**
 * Writes credentials to a file in specified format.
 * @param f File to write to.
 * @param format Format of the credentials (e.g., FORMAT_PB).
 * @param creds Credentials to write.
 * @param conf Configuration settings.
 */
","Write out a Credentials object as a local file.
   *  @param f a local File object.
   *  @param format a string equal to FORMAT_PB or FORMAT_JAVA.
   *  @param creds the Credentials object to be written out.
   *  @param conf a Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,rollLogDirIfNeeded,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:rollLogDirIfNeeded(),504,542,"/**
 * Rolls the log directory if needed, based on flush time.
 */","* Check the current directory against the time stamp.  If they're not
   * the same, create a new directory and a new log file in that directory.
   *
   * @throws MetricsException thrown if an error occurs while creating the
   * new directory or new log file",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,getJarsInDirectory,"org.apache.hadoop.fs.FileUtil:getJarsInDirectory(java.lang.String,boolean)",1777,1796,"/**
 * Finds all JAR files in a directory.
 * @param path directory path, ends with * for globbing.
 * @param useLocal use local filesystem.
 * @return List of Path objects for each JAR file.
 */
","* Returns all jars that are in the directory. It is useful in expanding a
   * wildcard path to return all jars from the directory to use in a classpath.
   *
   * @param path the path to the directory. The path may include the wildcard.
   * @param useLocal use local.
   * @return the list of jars as URLs, or an empty list if there are no jars, or
   * the directory does not exist",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Display.java,getInputStream,org.apache.hadoop.fs.shell.Display$Text:getInputStream(org.apache.hadoop.fs.shell.PathData),123,173,"/**
 * Gets an input stream for a PathData item, handling compression types.
 * @param item PathData object
 * @return InputStream for the item
 * @throws IOException if an I/O error occurs
 */
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKey(java.lang.String,org.apache.hadoop.crypto.key.KeyProvider$Options)",724,728,"/**
 * Creates a key with the given name and options.
 * @param name Key name.
 * @param options Key options.
 * @return KeyVersion object.
 */
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createKey,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:createKey(java.lang.String,byte[],org.apache.hadoop.crypto.key.KeyProvider$Options)",730,739,"/**
 * Creates a new key version with the given name, material, and options.
 * @param name Key name
 * @param material Key material
 * @param options Key creation options
 * @return KeyVersion object
 * @throws IOException if an I/O error occurs
 */
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,rollNewVersionInternal,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersionInternal(java.lang.String,byte[])",752,768,"/**
 * Creates a new version of the key with the given material.
 * @param name Key name
 * @param material Material for the new version
 * @return KeyVersion object
 * @throws IOException, NoSuchAlgorithmException
 */
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,invalidateCache,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:invalidateCache(java.lang.String),319,324,"/**
 * Invalidates cache for the given key across all providers.
 * @param keyName The key to invalidate.
 * @throws IOException if an I/O error occurs during invalidation.
 */
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/ProtobufRpcEngine.java,getServer,"org.apache.hadoop.ipc.ProtobufRpcEngine:getServer(java.lang.Class,java.lang.Object,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.conf.Configuration,org.apache.hadoop.security.token.SecretManager,java.lang.String,org.apache.hadoop.ipc.AlignmentContext)",368,378,"/**
 * Creates and returns a new RPC server instance.
 * @param protocol Protocol class
 * @param conf Configuration object
 * @return RPC server instance
 */
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Class,java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)",413,418,"/**
 * Constructs a Server with default settings.
 * @param protocolClass Protocol class.
 * @param protocolImpl Protocol implementation.
 */
","Construct an RPC server.
     * @param protocolClass class
     * @param protocolImpl the instance whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int,int,int,int,boolean,org.apache.hadoop.security.token.SecretManager)",436,445,"/**
 * Constructs a Server with specified parameters.
 * Deprecated constructor, use the other constructor instead.
 */","* Construct an RPC server.
     * @param protocolImpl the instance whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * @param numHandlers the number of method handler threads to run
     * @param verbose whether each call should be logged
     * @param numReaders input numberReaders.
     * @param queueSizePerHandler input queueSizePerHandler.
     * @param secretManager input secretManager.
     * 
     * @deprecated use Server#Server(Class, Object, 
     *      Configuration, String, int, int, int, int, boolean, SecretManager)
     * @throws IOException raised on errors performing I/O.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processRpcOutOfBandRequest,"org.apache.hadoop.ipc.Server$Connection:processRpcOutOfBandRequest(org.apache.hadoop.ipc.protobuf.RpcHeaderProtos$RpcRequestHeaderProto,org.apache.hadoop.ipc.RpcWritable$Buffer)",2984,3012,"/**
 * Processes RPC out-of-band requests based on the request header.
 * @param header RPC request header
 * @param buffer RPC data buffer
 * @throws RpcServerException on error
 */
","* Establish RPC connection setup by negotiating SASL if required, then
     * reading and authorizing the connection header
     * @param header - RPC header
     * @param buffer - stream to request payload
     * @throws RpcServerException - setup failed due to SASL
     *         negotiation failure, premature or invalid connection context,
     *         or other state errors. This exception needs to be sent to the 
     *         client.
     * @throws IOException - failed to send a response back to the client
     * @throws InterruptedException",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,connect,org.apache.hadoop.log.LogLevel$CLI:connect(java.net.URL),260,284,"/**
 * Establishes a connection to the given URL, handling HTTPS if needed.
 * @param url The URL to connect to.
 * @return URLConnection object.
 */
","* Connect to the URL. Supports HTTP/HTTPS and supports SPNEGO
     * authentication. It falls back to simple authentication if it fails to
     * initiate SPNEGO.
     *
     * @param url the URL address of the daemon servlet
     * @return a connected connection
     * @throws Exception if it can not establish a connection.",,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,<init>,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:<init>(java.net.URI,org.apache.hadoop.conf.Configuration)",378,428,"/**
* Creates a KMS client provider with given URI and configuration.
* @param uri KMS endpoint URI
* @param conf Configuration object
* @throws IOException if an I/O error occurs
*/
",,,,True,31
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,runCmd,org.apache.hadoop.ha.HAAdmin:runCmd(java.lang.String[]),391,441,"/**
 * Executes a command based on the provided arguments.
 * @param argv command-line arguments
 * @return int result code; -1 indicates failure.
 * @throws Exception if an error occurs during execution
 */",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/ZKFailoverController.java,fenceOldActive,org.apache.hadoop.ha.ZKFailoverController:fenceOldActive(byte[]),532,541,"/**
 * Fences the old active target, throwing any exceptions.
 * @param data Byte array containing target data.
 */
",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FileSystem:copyFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2518,2521,"/**
* Copies a local file to a destination Path.
* @param src Source Path
* @param dst Destination Path
*/
","* The src file is on the local disk.  Add it to filesystem at
   * the given dst name and the source is kept intact afterwards
   * @param src path
   * @param dst path
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,moveFromLocalFile,"org.apache.hadoop.fs.FileSystem:moveFromLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2542,2545,"/**
* Moves a file from the source path to the destination path.
* @param src Source file path.
* @param dst Destination file path.
*/
","* The src file is on the local disk.  Add it to the filesystem at
   * the given dst name, removing the source afterwards.
   * @param src local path
   * @param dst path
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyFromLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyFromLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",349,353,"/**
 * Copies a file from the local file system to the destination.
 * @param delSrc Whether to delete source file after copy.
 * @param src Source file path.
 * @param dst Destination file path.
 */
","* The src file is on the local disk.  Add it to FS at
   * the given dst name.
   * delSrc indicates if the source should be removed",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2601,2603,"/**
* Copies a file from a source path to a destination path.
* @param src Source file path.
* @param dst Destination file path.
*/
","* Copy it a file from the remote filesystem to the local one.
   * @param src path src file in the remote filesystem
   * @param dst path local destination
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,moveToLocalFile,"org.apache.hadoop.fs.FileSystem:moveToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2612,2614,"/**
* Copies a file to a local destination.
* @param src Source file path.
* @param dst Destination file path.
*/
","* Copy a file to the local filesystem, then delete it from the
   * remote filesystem (if successfully copied).
   * @param src path src file in the remote filesystem
   * @param dst path local destination
   * @throws IOException IO failure",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.FilterFileSystem:copyToLocalFile(boolean,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",384,388,"/**
 * Copies a file from source to local destination.
 * @param delSrc Delete source file after copy.
 * @param src Source file path.
 * @param dst Destination file path.
 */
","* The src file is under FS, and the dst is on the local disk.
   * Copy it from FS control to the local dst name.
   * delSrc indicates if the src will be removed or not.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration,boolean)",162,167,"/**
 * Gets a local path for writing data.
 * @param pathStr Path string. @param size Data size.
 * @param conf Configuration. @param checkWrite Check write access.
 * @return Local path object.
 */
","Get a path from the local FS. Pass size as 
   *  SIZE_UNKNOWN if not known apriori. We
   *  round-robin over the set of disks (via the configured dirs) and return
   *  the first complete path which has enough space 
   *  @param pathStr the requested path (this will be created on the first 
   *  available disk)
   *  @param size the size of the file that is going to be written
   *  @param conf the Configuration object
   *  @param checkWrite ensure that the path is writable
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,createTmpFileForWrite,"org.apache.hadoop.fs.LocalDirAllocator$AllocatorPerContext:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",500,512,"/**
 * Creates a temporary file for writing, ensuring sufficient space.
 * @param pathStr file path string
 * @param size file size
 * @param conf Hadoop configuration
 * @return Temporary File object
 */
","Creates a file on the local FS. Pass size as 
     * {@link LocalDirAllocator.SIZE_UNKNOWN} if not known apriori. We
     *  round-robin over the set of disks (via the configured dirs) and return
     *  a file on the first path which has enough space. The file is guaranteed
     *  to go away when the JVM exits.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathToRead,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",177,181,"/**
 * Gets the local path to read, using the provided path string and config.
 */
","Get a path from the local FS for reading. We search through all the
   *  configured dirs for the file's existence and return the complete
   *  path to the file when we find one 
   *  @param pathStr the requested file (this will be searched)
   *  @param conf the Configuration object
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getAllLocalPathsToRead,"org.apache.hadoop.fs.LocalDirAllocator:getAllLocalPathsToRead(java.lang.String,org.apache.hadoop.conf.Configuration)",190,198,"/**
 * Gets all local paths to read, using the provided path string and config.
 * @param pathStr Path string to search.
 * @param conf Configuration object.
 * @return Iterable of Path objects.
 */
","* Get all of the paths that currently exist in the working directories.
   * @param pathStr the path underneath the roots
   * @param conf the configuration to look up the roots in
   * @return all of the paths that exist under any of the roots
   * @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,<init>,"org.apache.hadoop.fs.viewfs.NflyFSystem:<init>(java.net.URI[],org.apache.hadoop.conf.Configuration,int,java.util.EnumSet)",213,216,"/**
 * Constructs NflyFSystem with URI, config, minReplication, and flags.
 * Delegates to the primary constructor.
 */
","* Creates a new Nfly instance.
   *
   * @param uris the list of uris in the mount point
   * @param conf configuration object
   * @param minReplication minimum copies to commit a write op
   * @param nflyFlags modes such readMostRecent
   * @throws IOException",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/viewfs/NflyFSystem.java,createFileSystem,"org.apache.hadoop.fs.viewfs.NflyFSystem:createFileSystem(java.net.URI[],org.apache.hadoop.conf.Configuration,java.lang.String,org.apache.hadoop.fs.viewfs.FsGetter)",944,971,"/**
 * Creates a new NflyFileSystem with given URIs, config, settings, and getter.
 * @param uris URIs for the filesystem.
 * @param conf Hadoop configuration.
 * @param settings Key-value settings string.
 * @param fsGetter Filesystem getter.
 * @return New NflyFileSystem instance.
 */
","* Initializes an nfly mountpoint in viewfs.
   *
   * @param uris destinations to replicate writes to
   * @param conf file system configuration
   * @param settings comma-separated list of k=v pairs.
   * @return an Nfly filesystem
   * @throws IOException",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,expandArguments,org.apache.hadoop.fs.shell.Command:expandArguments(java.util.LinkedList),243,254,"/**
 * Expands argument strings into a LinkedList of PathData objects.
 * @param args List of argument strings to expand.
 * @return LinkedList of PathData objects.
 */
","*  Expands a list of arguments into {@link PathData} objects.  The default
   *  behavior is to call {@link #expandArgument(String)} on each element
   *  which by default globs the argument.  The loop catches IOExceptions,
   *  increments the error count, and displays the exception.
   * @param args strings to expand into {@link PathData} objects
   * @return list of all {@link PathData} objects the arguments
   * @throws IOException if anything goes wrong...",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,open,"org.apache.hadoop.io.MapFile$Reader:open(org.apache.hadoop.fs.Path,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",532,556,"/**
 * Opens the data and index files for the SequenceFile.Reader.
 * @param dir Directory containing the SequenceFile.
 * @param comparator Comparator for keys.
 * @param conf Hadoop configuration.
 * @param options Reader options.
 */
",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,adjustPriorityQueue,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:adjustPriorityQueue(org.apache.hadoop.io.SequenceFile$Sorter$SegmentDescriptor),3598,3609,"/**
 * Adjusts the priority queue based on the segment descriptor.
 * Updates progress and either adjusts top or cleans up.
 */",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$BlockCompressWriter:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",1609,1620,"/**
 * Initializes a BlockCompressWriter with configuration and options.
 * @param conf Hadoop configuration object
 * @param options Compression options
 * @throws IOException if an I/O error occurs
 */
",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,<init>,"org.apache.hadoop.io.SequenceFile$RecordCompressWriter:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",1539,1542,"/**
 * Constructs a RecordCompressWriter with a configuration and options.
 */",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,getTokenFile,"org.apache.hadoop.security.token.DtFileOperations:getTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,java.lang.String,java.lang.String,org.apache.hadoop.conf.Configuration)",175,220,"/**
 * Fetches delegation tokens from DtFetchers, handles aliasing, and writes to file.
 * @param tokenFile File to store tokens
 * @param fileFormat File format
 * @param alias Token alias
 */
","Fetch a token from a service and save to file in the local filesystem.
   *  @param tokenFile a local File object to hold the output.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias overwrite service field of fetched token with this text.
   *  @param service use a DtFetcher implementation matching this service text.
   *  @param url pass this URL to fetcher after stripping any http/s prefix.
   *  @param renewer pass this renewer to the fetcher.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,aliasTokenFile,"org.apache.hadoop.security.token.DtFileOperations:aliasTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",230,243,"/**
 * Aliases a token in the token storage file.
 * @param tokenFile File containing tokens.
 * @param fileFormat File format.
 * @param alias New alias for the token.
 * @param service Service of the token.
 * @param conf Hadoop configuration.
 */","Alias a token from a file and save back to file in the local filesystem.
   *  @param tokenFile a local File object to hold the input and output.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias overwrite service field of fetched token with this text.
   *  @param service only apply alias to tokens matching this service text.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,appendTokenFiles,"org.apache.hadoop.security.token.DtFileOperations:appendTokenFiles(java.util.ArrayList,java.lang.String,org.apache.hadoop.conf.Configuration)",251,264,"/**
 * Appends tokens from multiple files into a single formatted file.
 * @param tokenFiles List of token files to append.
 * @param fileFormat Format for the output file.
 * @param conf Hadoop configuration.
 */
","Append tokens from list of files in local filesystem, saving to last file.
   *  @param tokenFiles list of local File objects.  Last file holds the output.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,removeTokenFromFile,"org.apache.hadoop.security.token.DtFileOperations:removeTokenFromFile(boolean,java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",275,291,"/**
 * Removes tokens matching the alias from the token file.
 * @param cancel if true, cancels matching tokens.
 */","Remove a token from a file in the local filesystem, matching alias.
   *  @param cancel cancel token as well as remove from file.
   *  @param tokenFile a local File object.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias remove only tokens matching alias; null matches all.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.
   *  @throws InterruptedException if the thread is interrupted.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,renewTokenFile,"org.apache.hadoop.security.token.DtFileOperations:renewTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,org.apache.hadoop.conf.Configuration)",301,313,"/**
 * Renews tokens matching the alias in the token file.
 * @param tokenFile Token storage file.
 * @param fileFormat File format (e.g., JSON).
 * @param alias Token alias to match.
 * @param conf Hadoop configuration.
 */","Renew a token from a file in the local filesystem, matching alias.
   *  @param tokenFile a local File object.
   *  @param fileFormat a string equal to FORMAT_PB or FORMAT_JAVA, for output
   *  @param alias renew only tokens matching alias; null matches all.
   *  @param conf Configuration object passed along.
   *  @throws IOException raised on errors performing I/O.
   *  @throws InterruptedException if the thread is interrupted.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtFileOperations.java,importTokenFile,"org.apache.hadoop.security.token.DtFileOperations:importTokenFile(java.io.File,java.lang.String,org.apache.hadoop.io.Text,java.lang.String,org.apache.hadoop.conf.Configuration)",323,339,"/**
 * Imports a token file, decodes a token, and writes credentials.
 * @param tokenFile File containing the token storage.
 * @param fileFormat Format of the token file.
 */","Import a token from a base64 encoding into the local filesystem.
   * @param tokenFile A local File object.
   * @param fileFormat A string equal to FORMAT_PB or FORMAT_JAVA, for output.
   * @param alias overwrite Service field of fetched token with this text.
   * @param base64 urlString Encoding of the token to import.
   * @param conf Configuration object passed along.
   * @throws IOException Error to import the token into the file.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/metrics2/sink/RollingFileSystemSink.java,putMetrics,org.apache.hadoop.metrics2.sink.RollingFileSystemSink:putMetrics(org.apache.hadoop.metrics2.MetricsRecord),822,861,"/**
 * Writes a metrics record to the log file.
 * @param record The metrics record to write.
 */
",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,getJarsInDirectory,org.apache.hadoop.fs.FileUtil:getJarsInDirectory(java.lang.String),1764,1766,"/**
 * Returns a list of JAR file paths in the given directory.
 * @param path The directory to search.
 */
","* Returns all jars that are in the directory. It is useful in expanding a
   * wildcard path to return all jars from the directory to use in a classpath.
   * It operates only on local paths.
   *
   * @param path the path to the directory. The path may include the wildcard.
   * @return the list of jars as URLs, or an empty list if there are no jars, or
   * the directory does not exist locally",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,expandWildcard,"org.apache.hadoop.util.GenericOptionsParser:expandWildcard(java.util.List,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.FileSystem)",495,512,"/**
 * Expands wildcard directory, adding jar files to the list.
 * @param finalPaths List to add qualified jar paths to.
 * @param path Path of the directory to expand.
 * @param fs FileSystem object.
 */
",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersion(java.lang.String),771,775,"/**
 * Rolls a new version for the given name.
 * @param name Version name.
 * @return New KeyVersion object.
 */
",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.kms.KMSClientProvider:rollNewVersion(java.lang.String,byte[])",777,786,"/**
 * Rolls a new version with the given name and material.
 * @param name Version name.
 * @param material Version material.
 * @return KeyVersion object.
 */
",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,deleteKey,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:deleteKey(java.lang.String),497,507,"/**
 * Deletes a key by name.
 * @param name The name of the key to delete.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,rollNewVersion,"org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String,byte[])",509,520,"/**
 * Rolls a new version of the key with the given name and material.
 * @param name Key name.
 * @param material Material for the new version.
 * @return New KeyVersion object.
 */
",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/LoadBalancingKMSClientProvider.java,rollNewVersion,org.apache.hadoop.crypto.key.kms.LoadBalancingKMSClientProvider:rollNewVersion(java.lang.String),522,541,"/**
 * Rolls a new version of the key with the given name.
 * @param name Key name.
 * @return New KeyVersion object.
 * @throws NoSuchAlgorithmException, IOException
 */
",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/WritableRpcEngine.java,<init>,"org.apache.hadoop.ipc.WritableRpcEngine$Server:<init>(java.lang.Object,org.apache.hadoop.conf.Configuration,java.lang.String,int)",398,402,"/**
 * Constructor for Server (deprecated).
 * Delegates to the primary constructor.
 */
","* Construct an RPC server.
     * @param instance the instance whose methods will be called
     * @param conf the configuration to use
     * @param bindAddress the address to bind on to listen for connection
     * @param port the port to listen for connections on
     * 
     * @deprecated Use #Server(Class, Object, Configuration, String, int)
     * @throws IOException raised on errors performing I/O.",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,processOneRpc,org.apache.hadoop.ipc.Server$Connection:processOneRpc(java.nio.ByteBuffer),2785,2823,"/**
 * Processes a single RPC request from the provided ByteBuffer.
 * @param bb ByteBuffer containing the RPC request.
 * @throws IOException, InterruptedException on connection failure.
 */
","* Process one RPC Request from buffer read from socket stream 
     *  - decode rpc in a rpc-Call
     *  - handle out-of-band RPC requests such as the initial connectionContext
     *  - A successfully decoded RpcCall will be deposited in RPC-Q and
     *    its response will be sent later when the request is processed.
     * 
     * Prior to this call the connectionHeader (""hrpc..."") has been handled and
     * if SASL then SASL has been established and the buf we are passed
     * has been unwrapped from SASL.
     * 
     * @param bb - contains the RPC request header and the rpc request
     * @throws IOException - internal error that should not be returned to
     *         client, typically failure to respond to client
     * @throws InterruptedException",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,process,org.apache.hadoop.log.LogLevel$CLI:process(java.lang.String),292,311,"/**
 * Processes data from a URL, printing lines starting with MARKER.
 * @param urlString URL to connect to and process data from.
 */
","* Configures the client to send HTTP/HTTPS request to the URL.
     * Supports SPENGO for authentication.
     * @param urlString URL and query string to the daemon's web UI
     * @throws Exception if unable to connect",,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createProviders,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory:createProviders(org.apache.hadoop.conf.Configuration,java.net.URL,int,java.lang.String)",311,326,"/**
 * Creates KMSClientProvider array from host list.
 * @param conf Configuration object.
 * @param origUrl Original URL object.
 * @param port Port number.
 * @param hostsPart Comma-separated host string.
 * @return KMSClientProvider array.
 */",,,,True,32
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ha/HAAdmin.java,run,org.apache.hadoop.ha.HAAdmin:run(java.lang.String[]),347,361,"/**
 * Executes the command with arguments.
 * @param argv Command-line arguments. Returns 0 on success, -1 on failure.
 */
",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileSystem.java,completeLocalOutput,"org.apache.hadoop.fs.FileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",2686,2689,"/**
* Moves a temporary local file to the final output file.
* @param fsOutputFile Path to the final output file.
* @param tmpLocalFile Path to the temporary local file.
*/
","* Called when we're all done writing to the target.
   * A local FS will do nothing, because we've written to exactly the
   * right place.
   * A remote FS will copy the contents of tmpLocalFile to the correct target at
   * fsOutputFile.
   * @param fsOutputFile path of output file
   * @param tmpLocalFile path to local tmp file
   * @throws IOException IO failure",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/ChecksumFileSystem.java,copyToLocalFile,"org.apache.hadoop.fs.ChecksumFileSystem:copyToLocalFile(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,boolean)",1019,1043,"/**
 * Copies a file or directory to a local file, optionally copying CRC.
 * @param src Source Path
 * @param dst Destination Path
 * @param copyCrc Whether to copy CRC file
 */
","* The src file is under FS, and the dst is on the local disk.
   * Copy it from FS control to the local dst name.
   * If src and dst are directories, the copyCrc parameter
   * determines whether to copy CRC files.
   * @param src src path.
   * @param dst dst path.
   * @param copyCrc copy csc flag.
   * @throws IOException if an I/O error occurs.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",145,148,"/**
 * Gets a local path for writing, using default retry behavior.
 * @param pathStr Path string to use.
 * @param size File size.
 * @param conf Configuration object.
 * @return Local path object.
 */
","Get a path from the local FS. Pass size as 
   *  SIZE_UNKNOWN if not known apriori. We
   *  round-robin over the set of disks (via the configured dirs) and return
   *  the first complete path which has enough space 
   *  @param pathStr the requested path (this will be created on the first 
   *  available disk)
   *  @param size the size of the file that is going to be written
   *  @param conf the Configuration object
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,createTmpFileForWrite,"org.apache.hadoop.fs.LocalDirAllocator:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",211,215,"/**
 * Creates a temporary file for writing.
 * @param pathStr File path.
 * @param size File size.
 * @param conf Configuration object.
 * @return File object.
 */
","Creates a temporary file in the local FS. Pass size as -1 if not known 
   *  apriori. We round-robin over the set of disks (via the configured dirs) 
   *  and select the first complete path which has enough space. A file is
   *  created on this directory. The file is guaranteed to go away when the
   *  JVM exits.
   *  @param pathStr prefix for the temporary file
   *  @param size the size of the file that is going to be written
   *  @param conf the Configuration object
   *  @return a unique temporary file
   *  @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,processRawArguments,org.apache.hadoop.fs.shell.Command:processRawArguments(java.util.LinkedList),229,232,"/**
 * Processes raw arguments after expanding them.
 * @param args LinkedList of raw argument strings.
 * @throws IOException if an I/O error occurs during processing.
 */
","* Allows commands that don't use paths to handle the raw arguments.
   * Default behavior is to expand the arguments via
   * {@link #expandArguments(LinkedList)} and pass the resulting list to
   * {@link #processArguments(LinkedList)} 
   * @param args the list of argument strings
   * @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",490,499,"/**
 * Opens a Reader for a directory of SequenceFiles.
 * @param dir Directory containing SequenceFiles.
 * @param conf Hadoop Configuration object.
 * @param opts Reader options.
 * @throws IOException if an I/O error occurs.
 */
",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,next,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:next(),3565,3591,"/**
 * Advances to the next segment. Returns true if successful, false otherwise.
 */",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Writer$Option[])",274,294,"/**
 * Creates a Writer instance with specified options and compression.
 * @param conf Hadoop Configuration.
 * @param opts Writer options.
 * @return Writer instance.
 */
","* Create a new Writer with the given options.
   * @param conf the configuration to use
   * @param opts the options to create the file with
   * @return a new Writer
   * @throws IOException raised on errors performing I/O.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Get:execute(),241,245,"/**
 * Executes the token retrieval process using provided configuration.
 */",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Edit:execute(),271,277,"/**
 * Executes the token file aliasing process for all token files.
 */
",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Append:execute(),289,292,"/**
 * Appends token files using provided format and configuration.
 */",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Remove:execute(),320,326,"/**
 * Executes the token removal process for each token file.
 */",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Renew:execute(),350,355,"/**
 * Renews token files using provided format, alias, and configuration.
 */",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,execute,org.apache.hadoop.security.token.DtUtilShell$Import:execute(),381,385,"/**
* Imports a token file using provided configuration.
*/",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,createJarWithClassPath,"org.apache.hadoop.fs.FileUtil:createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,java.util.Map)",1668,1753,"/**
 * Creates a classpath JAR with resolved entries and wildcards.
 * @param inputClassPath Classpath entries, separated by File.pathSeparator.
 * @return Array of classpath JAR path and unexpanded wildcard classpath.
 * @throws IOException if an I/O error occurs.
 */
","* Create a jar file at the given path, containing a manifest with a classpath
   * that references all specified entries.
   *
   * Some platforms may have an upper limit on command line length.  For example,
   * the maximum command line length on Windows is 8191 characters, but the
   * length of the classpath may exceed this.  To work around this limitation,
   * use this method to create a small intermediate jar with a manifest that
   * contains the full classpath.  It returns the absolute path to the new jar,
   * which the caller may set as the classpath for a new process.
   *
   * Environment variable evaluation is not supported within a jar manifest, so
   * this method expands environment variables before inserting classpath entries
   * to the manifest.  The method parses environment variables according to
   * platform-specific syntax (%VAR% on Windows, or $VAR otherwise).  On Windows,
   * environment variables are case-insensitive.  For example, %VAR% and %var%
   * evaluate to the same value.
   *
   * Specifying the classpath in a jar manifest does not support wildcards, so
   * this method expands wildcards internally.  Any classpath entry that ends
   * with * is translated to all files at that path with extension .jar or .JAR.
   *
   * @param inputClassPath String input classpath to bundle into the jar manifest
   * @param pwd Path to working directory to save jar
   * @param targetDir path to where the jar execution will have its working dir
   * @param callerEnv Map {@literal <}String, String{@literal >} caller's
   * environment variables to use for expansion
   * @return String[] with absolute path to new jar in position 0 and
   *   unexpanded wild card entry path in position 1
   * @throws IOException if there is an I/O error while writing the jar file",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,constructUrlsFromClasspath,org.apache.hadoop.util.ApplicationClassLoader:constructUrlsFromClasspath(java.lang.String),107,126,"/**
 * Creates URL[] from classpath string, handling directories with jars.
 */",,,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,validateFiles,"org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String,boolean)",425,488,"/**
 * Validates file paths, expands wildcards if enabled, and returns a comma-separated string.
 * @param files Comma-separated string of file paths.
 * @param expandWildcard Whether to expand wildcard characters.
 * @return Comma-separated string of validated file paths.
 */
","* takes input as a comma separated list of files
   * and verifies if they exist. It defaults for file:///
   * if the files specified do not have a scheme.
   * it returns the paths uri converted defaulting to file:///.
   * So an input of  /home/user/file1,/home/user/file2 would return
   * file:///home/user/file1,file:///home/user/file2.
   *
   * @param files the input files argument
   * @param expandWildcard whether a wildcard entry is allowed and expanded. If
   * true, any directory followed by a wildcard is a valid entry and is replaced
   * with the list of jars in that directory. It is used to support the wildcard
   * notation in a classpath.
   * @return a comma-separated list of validated and qualified paths, or null
   * if the input files argument is null",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,readAndProcess,org.apache.hadoop.ipc.Server$Connection:readAndProcess(),2478,2565,"/**
* Reads and processes RPC data from the channel.
* Handles connection headers, version checks, and data processing.
* @return int: Number of bytes read, or -1 on error.
*/
","* This method reads in a non-blocking fashion from the channel: 
     * this method is called repeatedly when data is present in the channel; 
     * when it has enough data to process one rpc it processes that rpc.
     * 
     * On the first pass, it processes the connectionHeader, 
     * connectionContext (an outOfBand RPC) and at most one RPC request that 
     * follows that. On future passes it will process at most one RPC request.
     *  
     * Quirky things: dataLengthBuffer (4 bytes) is used to read ""hrpc"" OR 
     * rpc request length.
     *    
     * @return -1 in case of error, else num bytes read so far
     * @throws IOException - internal error that should not be returned to
     *         client, typically failure to respond to client
     * @throws InterruptedException - if the thread is interrupted.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,unwrapPacketAndProcessRpcs,org.apache.hadoop.ipc.Server$Connection:unwrapPacketAndProcessRpcs(byte[]),2733,2767,"/**
 * Unwraps a packet and processes contained RPCs.
 * Reads and processes RPCs from the input byte array.
 */
","* Process a wrapped RPC Request - unwrap the SASL packet and process
     * each embedded RPC request 
     * @param inBuf - SASL wrapped request of one or more RPCs
     * @throws IOException - SASL packet cannot be unwrapped
     * @throws InterruptedException",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,doGetLevel,org.apache.hadoop.log.LogLevel$CLI:doGetLevel(),236,238,"/**
 * Retrieves the log level from the specified URL.
 */","* Send HTTP/HTTPS request to get log level.
     *
     * @throws HadoopIllegalArgumentException if arguments are invalid.
     * @throws Exception if unable to connect",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,doSetLevel,org.apache.hadoop.log.LogLevel$CLI:doSetLevel(),246,249,"/**
 * Sets the log level for the specified class.
 * Uses protocol, hostname, class name, and level.
 */
","* Send HTTP/HTTPS request to set log level.
     *
     * @throws HadoopIllegalArgumentException if arguments are invalid.
     * @throws Exception if unable to connect",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/kms/KMSClientProvider.java,createProvider,"org.apache.hadoop.crypto.key.kms.KMSClientProvider$Factory:createProvider(java.net.URI,org.apache.hadoop.conf.Configuration)",279,309,"/**
 * Creates a KeyProvider based on the given URI and configuration.
 * @param providerUri URI of the key provider.
 * @param conf Hadoop configuration.
 * @return KeyProvider or null if the scheme doesn't match.
 */
","* This provider expects URIs in the following form :
     * {@literal kms://<PROTO>@<AUTHORITY>/<PATH>}
     *
     * where :
     * - PROTO = http or https
     * - AUTHORITY = {@literal <HOSTS>[:<PORT>]}
     * - HOSTS = {@literal <HOSTNAME>[;<HOSTS>]}
     * - HOSTNAME = string
     * - PORT = integer
     *
     * This will always create a {@link LoadBalancingKMSClientProvider}
     * if the uri is correct.",,,True,33
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FilterFileSystem.java,completeLocalOutput,"org.apache.hadoop.fs.FilterFileSystem:completeLocalOutput(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",408,412,"/**
* Completes local output using the file system's completeLocalOutput method.
* @param fsOutputFile Output file path.
* @param tmpLocalFile Temporary file path.
*/
","* Called when we're all done writing to the target.  A local FS will
   * do nothing, because we've written to exactly the right place.  A remote
   * FS will copy the contents of tmpLocalFile to the correct target at
   * fsOutputFile.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,createTmpFileForWrite,"org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:createTmpFileForWrite(java.lang.String,long,org.apache.hadoop.conf.Configuration)",830,838,"/**
 * Creates a temporary file for writing with specified size and path.
 * @param pathStr file path string
 * @param size file size in bytes
 * @param conf Hadoop configuration
 * @return File object representing the temporary file
 */
","* Demand create the directory allocator, then create a temporary file.
     * This does not mark the file for deletion when a process exits.
     * {@link LocalDirAllocator#createTmpFileForWrite(String, long, Configuration)}.
     *
     * @param pathStr prefix for the temporary file.
     * @param size    the size of the file that is going to be written.
     * @param conf    the Configuration object.
     * @return a unique temporary file.
     * @throws IOException IO problems",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/LocalDirAllocator.java,getLocalPathForWrite,"org.apache.hadoop.fs.LocalDirAllocator:getLocalPathForWrite(java.lang.String,org.apache.hadoop.conf.Configuration)",129,132,"/**
 * Gets the local path for writing, using SIZE_UNKNOWN.
 * @param pathStr Path string to resolve.
 * @param conf Hadoop configuration.
 * @return Local path object.
 */
","Get a path from the local FS. This method should be used if the size of 
   *  the file is not known apriori. We go round-robin over the set of disks
   *  (via the configured dirs) and return the first complete path where
   *  we could create the parent directory of the passed path. 
   *  @param pathStr the requested path (this will be created on the first 
   *  available disk)
   *  @param conf the Configuration object
   *  @return the complete path to the file on a local disk
   *  @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/shell/Command.java,run,org.apache.hadoop.fs.shell.Command:run(java.lang.String[]),184,201,"/**
 * Runs the command with arguments, handles options, and errors.
 * @param argv Command-line arguments passed to the program.
 * @return Exit code based on success or errors.
 */
","* Invokes the command handler.  The default behavior is to process options,
   * expand arguments, and then process each argument.
   * <pre>
   * run
   * |{@literal ->} {@link #processOptions(LinkedList)}
   * \{@literal ->} {@link #processRawArguments(LinkedList)}
   *      |{@literal ->} {@link #expandArguments(LinkedList)}
   *      |   \{@literal ->} {@link #expandArgument(String)}*
   *      \{@literal ->} {@link #processArguments(LinkedList)}
   *          |{@literal ->} {@link #processArgument(PathData)}*
   *          |   |{@literal ->} {@link #processPathArgument(PathData)}
   *          |   \{@literal ->} {@link #processPaths(PathData, PathData...)}
   *          |        \{@literal ->} {@link #processPath(PathData)}*
   *          \{@literal ->} {@link #processNonexistentPath(PathData)}
   * </pre>
   * Most commands will chose to implement just
   * {@link #processOptions(LinkedList)} and {@link #processPath(PathData)}
   * 
   * @param argv the list of command line arguments
   * @return the exit code for the command
   * @throws IllegalArgumentException if called with invalid arguments",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,"org.apache.hadoop.io.ArrayFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",101,104,"/**
 * Constructs a Reader for the given file in the file system.
 * @param fs FileSystem object
 * @param file Path to the file
 * @param conf Configuration object
 */
","* Construct an array reader for the named file.
     * @param fs FileSystem.
     * @param file file.
     * @param conf configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",124,127,"/**
 * Constructs a Reader with a given FileSystem, directory, and comparator.
 * @param fs FileSystem, dirName directory name, comparator comparator
 */
","* Construct a set reader for the named set using the named comparator.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param comparator input comparator.
     * @param conf input Configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",510,514,"/**
 * Constructs a Reader with a directory path.
 * @param fs FileSystem object
 * @param dirName Directory name
 * @param conf Configuration object
 * @throws IOException if an I/O error occurs
 */
","* Construct a map reader for the named map.
     * @deprecated
     *
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param conf configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",526,530,"/**
 * Constructs a Reader with a directory path, config, and comparator.
 * @param fs FileSystem, dirName directory path, comparator comparator, conf config
 * @throws IOException if an I/O error occurs
 */
","* Construct a map reader for the named map using the named comparator.
     * @deprecated
     *
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param comparator WritableComparator.
     * @param conf Configuration.
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.Path,org.apache.hadoop.conf.Configuration,org.apache.hadoop.io.SequenceFile$Reader$Option[])",213,217,"/**
 * Creates a Reader for sequence files in a directory.
 * @param dir Directory containing the sequence files.
 * @param conf Hadoop configuration.
 * @param options Reader options.
 */
",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,cloneFileAttributes,"org.apache.hadoop.io.SequenceFile$Sorter:cloneFileAttributes(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.util.Progressable)",3406,3422,"/**
 * Creates a Writer with file attributes cloned from an input file.
 * @param inputFile Input file path.
 * @param outputFile Output file path.
 * @param prog Progressable object for tracking progress.
 * @return Writer object.
 */
","* Clones the attributes (like compression of the input file and creates a 
     * corresponding Writer
     * @param inputFile the path of the input file whose attributes should be 
     * cloned
     * @param outputFile the path of the output file 
     * @param prog the Progressable to report status during the file write
     * @return Writer
     * @throws IOException raised on errors performing I/O.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,fix,"org.apache.hadoop.io.MapFile:fix(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,boolean,org.apache.hadoop.conf.Configuration)",934,1014,"/**
 * Fixes a data file by creating an index file for efficient lookups.
 * @param fs Filesystem to operate on.
 * @param dir Directory containing data and index files.
 * @param keyClass Key class of the data file.
 * @param valueClass Value class of the data file.
 * @param dryrun Dry run mode.
 * @param conf Hadoop configuration.
 * @return Number of records processed.
 */
","* This method attempts to fix a corrupt MapFile by re-creating its index.
   * @param fs filesystem
   * @param dir directory containing the MapFile data and index
   * @param keyClass key class (has to be a subclass of Writable)
   * @param valueClass value class (has to be a subclass of Writable)
   * @param dryrun do not perform any changes, just report what needs to be done
   * @param conf configuration.
   * @return number of valid entries in this MapFile, or -1 if no fixing was needed
   * @throws Exception Exception.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,flush,"org.apache.hadoop.io.SequenceFile$Sorter$SortPass:flush(int,int,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,boolean)",3217,3251,"/**
 * Flushes data to an output stream, potentially creating an index.
 * @param count number of records to flush
 * @throws IOException if an I/O error occurs
 */
",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$Writer$Option[])",312,357,"/**
 * Creates a Writer with specified configuration, directory, and options.
 * @param conf Hadoop configuration
 * @param dirName Directory for Writer
 * @param opts SequenceFile Writer options
 * @throws IOException if an I/O error occurs
 */
",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class)",308,315,"/**
 * Creates a Writer instance. Delegates to the internal implementation.
 * @param fs FileSystem, configuration, path, key/value classes
 * @return Writer instance
 */
","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",330,339,"/**
 * Creates a Writer instance with specified parameters.
 * @param fs Filesystem, configuration, path, classes, compression.
 * @return Writer instance.
 */
","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",355,366,"/**
 * Creates a Writer with specified parameters.
 * @param fs FileSystem, configuration, path, classes, compression, progress
 * @return Writer object
 */
","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param progress The Progressable object to track progress.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",382,392,"/**
 * Creates a writer with specified file system, config, and compression.
 * @param fs FileSystem, config, name, keyClass, valClass, compression, codec
 * @return Writer object
 */
","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",410,423,"/**
 * Creates a writer with specified configurations.
 * @param fs FileSystem, Configuration, Path, classes, codec, progress, metadata
 * @return Writer object
 */
","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param progress The Progressable object to track progress.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable,org.apache.hadoop.io.SequenceFile$Metadata)",444,461,"/**
 * Creates a Writer instance with specified configurations.
 * @param fs FileSystem, configuration, path, classes, sizes, etc.
 * @return Writer instance.
 */
","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem.
   * @param conf The configuration.
   * @param name The name of the file.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param bufferSize buffer size for the underlaying outputstream.
   * @param replication replication factor for the file.
   * @param blockSize block size for the file.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param progress The Progressable object to track progress.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",539,551,"/**
 * Creates a writer with specified file system, config, and compression.
 * @param fs FileSystem, config, name, keyClass, valClass, compression, codec, progress
 * @return Writer object
 */
","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem. 
   * @param conf The configuration.
   * @param name The name of the file. 
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param progress The Progressable object to track progress.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)",567,577,"/**
 * Creates a Writer instance with specified configuration and metadata.
 * @param conf Configuration object
 * @param out FSDataOutputStream
 * @param keyClass Class of the key
 * @param valClass Class of the value
 * @param compressionType Compression type
 * @param codec Compression codec
 * @param metadata Metadata object
 * @return Writer instance
 */
","* Construct the preferred type of 'raw' SequenceFile Writer.
   * @param conf The configuration.
   * @param out The stream on top which the writer is to be constructed.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FSDataOutputStream,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec)",592,600,"/**
 * Creates a Writer instance with specified configuration and codec.
 * @param conf Configuration object
 * @param out FSDataOutputStream
 * @return Writer instance
 */
","* Construct the preferred type of 'raw' SequenceFile Writer.
   * @param conf The configuration.
   * @param out The stream on top which the writer is to be constructed.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.
   * @deprecated Use {@link #createWriter(Configuration, Writer.Option...)}
   *     instead.",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FileUtil.java,createJarWithClassPath,"org.apache.hadoop.fs.FileUtil:createJarWithClassPath(java.lang.String,org.apache.hadoop.fs.Path,java.util.Map)",1632,1635,"/**
 * Delegates to the overloaded method with explicit classpath.
 * @param inputClassPath Classpath to use for jar creation.
 * @param pwd Path to working directory.
 * @param callerEnv Environment variables.
 * @return Array of strings representing the jar creation command.
 */
",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ApplicationClassLoader.java,<init>,"org.apache.hadoop.util.ApplicationClassLoader:<init>(java.lang.String,java.lang.ClassLoader,java.util.List)",102,105,"/**
 * Constructs an ApplicationClassLoader with URLs from classpath.
 * @param classpath Classpath string to parse.
 * @param parent Parent classloader.
 * @param systemClasses System class list.
 * @throws MalformedURLException if classpath is invalid.
 */
",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,validateFiles,org.apache.hadoop.util.GenericOptionsParser:validateFiles(java.lang.String),405,407,"/**
 * Validates file names. Overloads validateFiles with default settings.
 */
","* Takes input as a comma separated list of files
   * and verifies if they exist. It defaults for file:///
   * if the files specified do not have a scheme.
   * it returns the paths uri converted defaulting to file:///.
   * So an input of  /home/user/file1,/home/user/file2 would return
   * file:///home/user/file1,file:///home/user/file2.
   *
   * This method does not recognize wildcards.
   *
   * @param files the input files argument
   * @return a comma-separated list of validated and qualified paths, or null
   * if the input files argument is null",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRead,org.apache.hadoop.ipc.Server$Listener:doRead(java.nio.channels.SelectionKey),1641,1671,"/**
 * Reads and processes data from a connection.
 * Handles errors and closes connection if needed.
 */
",,,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,sendLogLevelRequest,org.apache.hadoop.log.LogLevel$CLI:sendLogLevelRequest(),126,139,"/**
 * Sends a log level request based on the 'operation' value.
 * Throws IllegalArgumentException if operation is invalid.
 */
","* Send HTTP/HTTPS request to the daemon.
     * @throws HadoopIllegalArgumentException if arguments are invalid.
     * @throws Exception if unable to connect",,,True,34
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/store/DataBlocks.java,create,"org.apache.hadoop.fs.store.DataBlocks$DiskBlockFactory:create(long,int,org.apache.hadoop.fs.store.BlockUploadStatistics)",807,817,"/**
 * Creates a DataBlock on disk.
 * @param index block index, limit block size, statistics upload stats.
 * @return DataBlock object.
 */
","* Create a temp file and a {@link DiskBlock} instance to manage it.
     *
     * @param index      block index.
     * @param limit      limit of the block.
     * @param statistics statistics to update.
     * @return the new block.
     * @throws IOException IO problems",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,getTempFilePath,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getTempFilePath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",649,658,"/**
 * Creates a temporary file path using the provided allocator.
 * @param conf Hadoop configuration
 * @param localDirAllocator Local directory allocator
 * @return Path to the created temporary file
 */
","* Create temporary file based on the file path retrieved from local dir allocator
   * instance. The file is created with .bin suffix. The created file has been granted
   * posix file permissions available in TEMP_FILE_ATTRS.
   *
   * @param conf the configuration.
   * @param localDirAllocator the local dir allocator instance.
   * @return path of the file created.
   * @throws IOException if IO error occurs while local dir allocator tries to retrieve path
   * from local FS or file creation fails or permission set fails.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,run,org.apache.hadoop.fs.FsShell:run(java.lang.String[]),300,351,"/**
 * Runs the specified command with provided arguments.
 * @param argv Command-line arguments, first is the command.
 * @return Exit code of the command execution.
 */",* run,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",112,114,"/**
 * Constructs a Reader with a FileSystem, directory name, and configuration.
 */","* Construct a set reader for the named set.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param conf input Configuration.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.conf.Configuration)",219,223,"/**
 * Constructs a Reader with a Path, using the given configuration.
 * @param fs FileSystem object
 * @param dirName Directory name
 * @param conf Configuration object
 * @throws IOException if an I/O error occurs
 */
",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration,boolean)",225,229,"/**
 * Constructs a Reader with a Path, Configuration, and comparator.
 * @param fs Filesystem, dirName directory path, comparator comparator
 */",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Reader:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.conf.Configuration)",231,235,"/**
 * Constructs a Reader with a Path, Configuration, and comparator.
 * @param fs FileSystem, dirName directory path, comparator comparator
 */",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue:merge(),3623,3723,"/**
 * Merges sorted segments into a single sorted output.
 * Returns a RawKeyValueIterator for the merged data.
 */","This is the single level merge that is called multiple times 
       * depending on the factor size and the number of segments
       * @return RawKeyValueIterator
       * @throws IOException",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,run,org.apache.hadoop.io.SequenceFile$Sorter$SortPass:run(boolean),3100,3179,"/**
 * Runs the data processing pipeline.
 * @param deleteInput Whether to delete input files after processing.
 * @return The number of segments processed.
 */",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,org.apache.hadoop.io.SequenceFile$CompressionType)",82,89,"/**
 * Constructs a Writer with given configuration, FileSystem, dir, comparator, compression.
 */
","* Create a set naming the element comparator and compression type.
     *
     * @param conf input Configuration.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param comparator input comparator.
     * @param compress input compress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,org.apache.hadoop.io.SequenceFile$Writer$Option[])",158,164,"/**
 * Constructs a Writer with a configuration, directory, and options.
 */
",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,"org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)",50,55,"/**
 * Constructs a Writer with given configuration, file system, file, and value class.
 */
","* Create the named file for values of the named class.
     *
     * @param conf configuration.
     * @param fs file system.
     * @param file file.
     * @param valClass valClass.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/ArrayFile.java,<init>,"org.apache.hadoop.io.ArrayFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",68,77,"/**
 * Constructs a Writer with given configuration, file system, file,
 * value class, compression, and progress.
 */
","* Create the named file for values of the named class.
     *
     * @param conf configuration.
     * @param fs file system.
     * @param file file.
     * @param valClass valClass.
     * @param compress compress.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)",112,117,"/**
 * Constructs a Writer with a directory path.
 * @param conf Configuration object.
 * @param fs FileSystem object.
 * @param dirName Directory name.
 * @param keyClass Key class.
 * @param valClass Value class.
 */
","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs filesystem.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",132,139,"/**
 * Constructs a Writer with specified configuration and parameters.
 * @param conf Hadoop configuration; @param progress Progressable object
 */","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs fs.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @param compress compress.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",155,162,"/**
 * Constructs a Writer with specified configuration and file system.
 * @param conf Configuration object
 * @param fs FileSystem object
 * @param dirName Output directory name
 * @param keyClass Key class
 * @param valClass Value class
 * @param compress Compression type
 * @param codec Compression codec
 * @param progress Progressable object
 * @throws IOException if an I/O error occurs
 */
","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @param compress compress.
     * @param codec codec.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",175,181,"/**
 * Constructs a Writer with a directory path.
 * @param conf Configuration object
 * @param fs FileSystem object
 * @param dirName Directory name
 * @param keyClass Key class
 * @param valClass Value class
 * @param compress Compression type
 */
","* Create the named map for keys of the named class.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     * @param conf configuration.
     * @param fs fs.
     * @param dirName dirName.
     * @param keyClass keyClass.
     * @param valClass valClass.
     * @param compress compress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)",192,198,"/**
 * Constructs a Writer with Configuration, FileSystem, dirName, comparator, and value class.
 */","Create the named map using the named key comparator. 
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     * @param conf configuration.
     * @param fs fs.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",210,216,"/**
 * Constructs a Writer with specified configuration and compression.
 * @param conf Configuration object
 * @param fs FileSystem object
 * @param dirName Directory name
 * @param comparator Comparator for writable keys
 * @param valClass Value class
 * @param compress Compression type
 */
","Create the named map using the named key comparator.
     * @param conf configuration.
     * @param fs filesystem.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @param compress compress.
     * @throws IOException raised on errors performing I/O.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",231,239,"/**
 * Creates a Writer with specified parameters.
 * @param conf Configuration object.
 * @param fs FileSystem object.
 * @param dirName Output directory name.
 */","* Create the named map using the named key comparator.
     * @deprecated Use Writer(Configuration, Path, Option...)} instead.
     *
     * @param conf configuration.
     * @param fs filesystem.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @param compress CompressionType.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,<init>,"org.apache.hadoop.io.MapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",255,263,"/**
 * Constructs a Writer with specified configuration and parameters.
 * @param conf Configuration object.
 */","* Create the named map using the named key comparator.
     * @deprecated Use Writer(Configuration, Path, Option...) instead.
     *
     * @param conf configuration.
     * @param fs FileSystem.
     * @param dirName dirName.
     * @param comparator comparator.
     * @param valClass valClass.
     * @param compress CompressionType.
     * @param codec codec.
     * @param progress progress.
     * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,open,"org.apache.hadoop.io.MapFile$Merger:open(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",1059,1090,"/**
 * Opens input map files and prepares for writing to the output file.
 * @param inMapFiles array of input map file paths
 * @param outMapFile path to the output map file
 */
",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileContext,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata,java.util.EnumSet,org.apache.hadoop.fs.Options$CreateOpts[])",513,522,"/**
 * Creates a writer for the specified file, handling flags and options.
 * @param fc FileContext, conf Configuration, name Path, etc.
 * @return Writer object.
 */
","* Construct the preferred type of SequenceFile Writer.
   * @param fc The context for the specified file.
   * @param conf The configuration.
   * @param name The name of the file.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param metadata The metadata of the file.
   * @param createFlag gives the semantics of create: overwrite, append etc.
   * @param opts file creation options; see {@link CreateOpts}.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/Classpath.java,main,org.apache.hadoop.util.Classpath:main(java.lang.String[]),64,113,"/**
 * Parses command-line arguments and handles classpath or jar creation.
 */","* Main entry point.
   *
   * @param args command-line arguments",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,createClassLoader,"org.apache.hadoop.util.RunJar:createClassLoader(java.io.File,java.io.File)",344,383,"/**
 * Creates a ClassLoader, either client or standard, based on config.
 * @param file The file to load.
 * @param workDir Working directory for class loading.
 * @return A ClassLoader instance.
 */
","* Creates a classloader based on the environment that was specified by the
   * user. If HADOOP_USE_CLIENT_CLASSLOADER is specified, it creates an
   * application classloader that provides the isolation of the user class space
   * from the hadoop classes and their dependencies. It forms a class space for
   * the user jar as well as the HADOOP_CLASSPATH. Otherwise, it creates a
   * classloader that simply adds the user jar to the classpath.",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,processGeneralOptions,org.apache.hadoop.util.GenericOptionsParser:processGeneralOptions(org.apache.commons.cli.CommandLine),291,364,"/**
 * Processes general command-line options and configures Hadoop configuration.
 */","* Modify configuration according user-specified generic options.
   *
   * @param line User-specified generic options",,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,doRunLoop,org.apache.hadoop.ipc.Server$Listener$Reader:doRunLoop(),1486,1527,"/**
 * Continuously monitors and processes incoming connections and reads.
 */",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,run,org.apache.hadoop.log.LogLevel$CLI:run(java.lang.String[]),109,119,"/**
 * Executes the main program logic.
 * Parses arguments, sends log level request.
 * @return 0 on success, -1 on failure.
 */
",,,,True,35
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,getCacheFilePath,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:getCacheFilePath(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",496,500,"/**
 * Gets the cache file path.
 * @param conf Configuration object.
 * @param localDirAllocator Local directory allocator.
 * @return Cache file path.
 */
","* Return temporary file created based on the file path retrieved from local dir allocator.
   *
   * @param conf The configuration object.
   * @param localDirAllocator Local dir allocator instance.
   * @return Path of the temporary file created.
   * @throws IOException if IO error occurs while local dir allocator tries to retrieve path
   * from local FS or file creation fails or permission set fails.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,isCacheSpaceAvailable,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:isCacheSpaceAvailable(long,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",621,633,"/**
 * Checks if there's enough space in the cache directory.
 * @param fileSize Size of the file to be cached.
 * @param conf Hadoop configuration.
 * @param localDirAllocator Local directory allocator.
 * @return True if cache space is available, false otherwise.
 */
","* Determine if the cache space is available on the local FS.
   *
   * @param fileSize The size of the file.
   * @param conf The configuration.
   * @param localDirAllocator Local dir allocator instance.
   * @return True if the given file size is less than the available free space on local FS,
   * False otherwise.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(java.util.List,org.apache.hadoop.fs.Path)",3313,3319,"/**
 * Merges a list of segments into a single iterator.
 * @param segments list of segments to merge
 * @param tmpDir temporary directory for merging
 * @return merged RawKeyValueIterator
 */
","* Merges the list of segments of type <code>SegmentDescriptor</code>
     * @param segments the list of SegmentDescriptors
     * @param tmpDir the directory to write temporary files into
     * @return RawKeyValueIterator
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],boolean,int,org.apache.hadoop.fs.Path)",3349,3364,"/**
 * Merges segments from input paths into a MergeQueue.
 * @param inNames Input paths to merge.
 * @param deleteInputs Whether to delete input paths after merge.
 * @param factor Merge factor.
 * @param tmpDir Temporary directory.
 * @return MergeQueue iterator.
 */
","* Merges the contents of files passed in Path[]
     * @param inNames the array of path names
     * @param deleteInputs true if the input files should be deleted when 
     * unnecessary
     * @param factor the factor that will be used as the maximum merge fan-in
     * @param tmpDir the directory to write temporary files into
     * @return RawKeyValueIteratorMergeQueue
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",3375,3394,"/**
 * Merges input paths into a single queue.
 * @param inNames Input paths to merge.
 * @param tempDir Temporary directory for merge outputs.
 * @param deleteInputs Whether to delete input files after merge.
 * @return MergeQueue object for merged data.
 */
","* Merges the contents of files passed in Path[]
     * @param inNames the array of path names
     * @param tempDir the directory for creating temp files during merge
     * @param deleteInputs true if the input files should be deleted when 
     * unnecessary
     * @return RawKeyValueIteratorMergeQueue
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",3479,3489,"/**
 * Merges segments from index into a merge queue.
 * @param inName Input path, indexIn is the index path.
 * @param indexIn Index path containing segments to merge.
 * @param tmpDir Temporary directory for merge operations.
 * @return MergeQueue iterator for merged segments.
 */
","Used by mergePass to merge the output of the sort
     * @param inName the name of the input file containing sorted segments
     * @param indexIn the offsets of the sorted segments
     * @param tmpDir the relative directory to store intermediate results in
     * @return RawKeyValueIterator
     * @throws IOException",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sortPass,org.apache.hadoop.io.SequenceFile$Sorter:sortPass(boolean),3064,3076,"/**
 * Executes a single sort pass, deleting input if specified.
 * @param deleteInput Flag to delete input file after sorting.
 * @return Number of records sorted.
 */
",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",65,70,"/**
 * Constructs a Writer with a key comparator.
 * @param conf Configuration object.
 * @param fs FileSystem object.
 * @param dirName Output directory name.
 * @param compress Compression type.
 */
","* Create a set naming the element class and compression type.
     *
     * @param conf input Configuration.
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param keyClass input keyClass.
     * @param compress input compress.
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",90,97,"/**
 * Constructs a Writer with specified configuration and parameters.
 * @param conf Hadoop configuration; @param progress Progressable object
 */",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",99,106,"/**
 * Constructs a Writer with specified configuration, file system,
 * directory, key/value classes, compression, and progress.
 */
",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",108,115,"/**
 * Constructs a Writer with specified configuration, FS, dir, key/val classes, compression.
 */",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.util.Progressable)",117,125,"/**
 * Constructs a Writer with specified configuration and parameters.
 * @param conf Configuration object.
 */",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.util.Progressable)",127,134,"/**
 * Constructs a Writer with specified configuration and parameters.
 * @param conf Configuration object
 * @param fs FileSystem object
 * @param dirName Output directory name
 */",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class,org.apache.hadoop.io.SequenceFile$CompressionType)",136,142,"/**
 * Constructs a Writer with specified configuration and parameters.
 * @param conf Configuration object
 * @param fs FileSystem object
 * @param dirName Output directory name
 * @param comparator Comparator for writable values
 * @param compress Compression type
 * @throws IOException if an I/O error occurs
 */
",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,org.apache.hadoop.io.WritableComparator,java.lang.Class)",144,149,"/**
 * Creates a Writer with a specified directory.
 * @param conf Configuration object.
 * @param fs FileSystem object.
 * @param dirName Directory name.
 * @param comparator Comparator for writable values.
 * @param valClass Class of the writable values.
 */
",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/BloomMapFile.java,<init>,"org.apache.hadoop.io.BloomMapFile$Writer:<init>(org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class,java.lang.Class)",151,156,"/**
 * Constructs a Writer with a directory path.
 * @param conf Configuration object.
 * @param fs FileSystem instance.
 * @param dirName Directory name.
 * @param keyClass Key class.
 * @param valClass Value class.
 */
",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,main,org.apache.hadoop.io.MapFile:main(java.lang.String[]),1160,1191,"/**
 * Copies entries from a MapFile.
 * @param args Command-line arguments: inFile outFile
 */
",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SetFile.java,<init>,"org.apache.hadoop.io.SetFile$Writer:<init>(org.apache.hadoop.fs.FileSystem,java.lang.String,java.lang.Class)",50,53,"/**
 * Constructs a Writer with a FileSystem, directory name, and key class.
 */","* Create the named set for keys of the named class.
     * @deprecated pass a Configuration too
     * @param fs input FileSystem.
     * @param dirName input dirName.
     * @param keyClass input keyClass.
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/MapFile.java,merge,"org.apache.hadoop.io.MapFile$Merger:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)",1039,1053,"/**
 * Merges map files into a single output file. Deletes input files if specified.
 * @param inMapFiles Input map files to merge.
 * @param deleteInputs Whether to delete input files after merging.
 * @param outMapFile Path to the output merged map file.
 */
","* Merge multiple MapFiles to one Mapfile.
     *
     * @param inMapFiles input inMapFiles.
     * @param deleteInputs deleteInputs.
     * @param outMapFile input outMapFile.
     * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,createWriter,"org.apache.hadoop.io.SequenceFile:createWriter(org.apache.hadoop.fs.FileSystem,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.Path,java.lang.Class,java.lang.Class,int,short,long,boolean,org.apache.hadoop.io.SequenceFile$CompressionType,org.apache.hadoop.io.compress.CompressionCodec,org.apache.hadoop.io.SequenceFile$Metadata)",480,496,"/**
 * Creates a Writer instance with specified configurations and options.
 * @param fs FileSystem, Configuration, Path, classes, sizes, flags, etc.
 * @return Writer instance
 */
","* Construct the preferred type of SequenceFile Writer.
   * @param fs The configured filesystem.
   * @param conf The configuration.
   * @param name The name of the file.
   * @param keyClass The 'key' type.
   * @param valClass The 'value' type.
   * @param bufferSize buffer size for the underlaying outputstream.
   * @param replication replication factor for the file.
   * @param blockSize block size for the file.
   * @param createParent create parent directory if non-existent
   * @param compressionType The compression type.
   * @param codec The compression codec.
   * @param metadata The metadata of the file.
   * @return Returns the handle to the constructed SequenceFile Writer.
   * @throws IOException raised on errors performing I/O.",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,run,org.apache.hadoop.util.RunJar:run(java.lang.String[]),248,334,"/**
 * Runs a JAR file, extracts its contents, and executes its main class.
 * @param args Command-line arguments: jar file, main class (optional), args.
 */
",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,parseGeneralOptions,"org.apache.hadoop.util.GenericOptionsParser:parseGeneralOptions(org.apache.commons.cli.Options,java.lang.String[])",572,588,"/**
 * Parses general command-line options.
 * @param opts Options object to populate.
 * @param args Command-line arguments.
 * @return True if parsing was successful, false otherwise.
 */
","* Parse the user-specified options, get the generic options, and modify
   * configuration accordingly.
   *
   * @param opts Options to use for parsing args.
   * @param args User-specified arguments
   * @return true if the parse was successful",,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/ipc/Server.java,run,org.apache.hadoop.ipc.Server$Listener$Reader:run(),1472,1484,"/**
 * Executes the main loop of the task, closing the selector on exit.
 */
",,,,True,36
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/impl/prefetch/SingleFilePerBlockCache.java,put,"org.apache.hadoop.fs.impl.prefetch.SingleFilePerBlockCache:put(int,java.nio.ByteBuffer,org.apache.hadoop.conf.Configuration,org.apache.hadoop.fs.LocalDirAllocator)",370,413,"/**
 * Adds a block to the cache.
 * @param blockNumber Block number to add
 * @param buffer ByteBuffer containing the block data
 * @throws IOException if an I/O error occurs
 */
","* Puts the given block in this cache.
   *
   * @param blockNumber the block number, used as a key for blocks map.
   * @param buffer buffer contents of the given block to be added to this cache.
   * @param conf the configuration.
   * @param localDirAllocator the local dir allocator instance.
   * @throws IOException if either local dir allocator fails to allocate file or if IO error
   * occurs while writing the buffer content to the file.
   * @throws IllegalArgumentException if buffer is null, or if buffer.limit() is zero or negative.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],boolean,org.apache.hadoop.fs.Path)",3331,3337,"/**
 * Merges multiple paths into a single iterator.
 * @param inNames Input paths to merge.
 * @param deleteInputs Whether to delete input paths after merge.
 * @param tmpDir Temporary directory for merging.
 * @return A RawKeyValueIterator containing merged data.
 */
","* Merges the contents of files passed in Path[] using a max factor value
     * that is already set
     * @param inNames the array of path names
     * @param deleteInputs true if the input files should be deleted when 
     * unnecessary
     * @param tmpDir the directory to write temporary files into
     * @return RawKeyValueIteratorMergeQueue
     * @throws IOException raised on errors performing I/O.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,mergePass,org.apache.hadoop.io.SequenceFile$Sorter:mergePass(org.apache.hadoop.fs.Path),3458,3470,"/**
 * Merges data from input file and writes to a temporary file.
 */",sort calls this to generate the final merged output,,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/RunJar.java,main,org.apache.hadoop.util.RunJar:main(java.lang.String[]),244,246,"/**
 * Starts the application by invoking the RunJar class.
 * @param args Command-line arguments passed to the application.
 */
","Run a Hadoop job jar.  If the main class is not in the jar's manifest,
   * then it must be provided on the command line.
   *
   * @param args args.
   * @throws Throwable error.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,"org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])",178,182,"/**
 * Parses generic options using provided configuration, options, and arguments.
 */","* Create a <code>GenericOptionsParser</code> to parse given options as well 
   * as generic Hadoop options. 
   * 
   * The resulting <code>CommandLine</code> object can be obtained by 
   * {@link #getCommandLine()}.
   * 
   * @param conf the configuration to modify  
   * @param options options built by the caller 
   * @param args User-specified arguments
   * @throws IOException raised on errors performing I/O.",,,True,37
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sortAndIterate,"org.apache.hadoop.io.SequenceFile$Sorter:sortAndIterate(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",3032,3052,"/**
 * Sorts input files and returns an iterator for sorted data.
 * @param inFiles Input file paths to sort.
 * @param tempDir Temporary directory for intermediate files.
 * @param deleteInput Whether to delete input files after sorting.
 * @return KeyValueIterator for sorted data or null if no segments.
 */
","* Perform a file sort from a set of input files and return an iterator.
     * @param inFiles the files to be sorted
     * @param tempDir the directory where temp files are created during sort
     * @param deleteInput should the input files be deleted as they are read?
     * @return iterator the RawKeyValueIterator
     * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,merge,"org.apache.hadoop.io.SequenceFile$Sorter:merge(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path)",3445,3455,"/**
 * Merges input files into a single output file.
 * @param inFiles array of input file paths
 * @param outFile path to the output file
 * @throws IOException if output file already exists
 */
","Merge the provided files.
     * @param inFiles the array of input path names
     * @param outFile the final output file
     * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sort,"org.apache.hadoop.io.SequenceFile$Sorter:sort(org.apache.hadoop.fs.Path[],org.apache.hadoop.fs.Path,boolean)",3009,3022,"/**
 * Sorts input files and merges them to an output file.
 * @param inFiles Input file paths to sort.
 * @param outFile Output file path for sorted data.
 * @param deleteInput Delete input files after sorting.
 */
","* Perform a file sort from a set of input files into an output file.
     * @param inFiles the files to be sorted
     * @param outFile the sorted output file
     * @param deleteInput should the input files be deleted as they are read?
     * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,<init>,"org.apache.hadoop.service.launcher.ServiceLauncher$MinimalGenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,org.apache.commons.cli.Options,java.lang.String[])",1081,1084,"/**
 * Constructs a MinimalGenericOptionsParser with config, options, and args.
 */",,,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,"org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.commons.cli.Options,java.lang.String[])",135,138,"/**
 * Constructs a GenericOptionsParser with a default Configuration.
 * @param opts Options object.
 * @param args Command-line arguments.
 * @throws IOException If an I/O error occurs.
 */
","* Create an options parser with the given options to parse the args.
   * @param opts the options
   * @param args the command line arguments
   * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,org.apache.hadoop.util.GenericOptionsParser:<init>(java.lang.String[]),145,148,"/**
 * Constructs a GenericOptionsParser with a default configuration.
 * @param args Command-line arguments to parse.
 * @throws IOException If an I/O error occurs during parsing.
 */
","* Create an options parser to parse the args.
   * @param args the command line arguments
   * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/GenericOptionsParser.java,<init>,"org.apache.hadoop.util.GenericOptionsParser:<init>(org.apache.hadoop.conf.Configuration,java.lang.String[])",161,164,"/**
 * Constructs a GenericOptionsParser with default options.
 * @param conf Configuration object.
 * @param args Command-line arguments.
 * @throws IOException if an I/O error occurs.
 */
","* Create a <code>GenericOptionsParser</code> to parse only the generic
   * Hadoop arguments.
   * 
   * The array of string arguments other than the generic arguments can be 
   * obtained by {@link #getRemainingArgs()}.
   * 
   * @param conf the <code>Configuration</code> to modify.
   * @param args command-line arguments.
   * @throws IOException raised on errors performing I/O.",,,True,38
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/io/SequenceFile.java,sort,"org.apache.hadoop.io.SequenceFile$Sorter:sort(org.apache.hadoop.fs.Path,org.apache.hadoop.fs.Path)",3060,3062,"/**
 * Sorts the input file and writes the sorted data to the output file.
 */","* The backwards compatible interface to sort.
     * @param inFile the input file to sort.
     * @param outFile the sorted output file.
     * @throws IOException raised on errors performing I/O.",,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,createGenericOptionsParser,"org.apache.hadoop.service.launcher.ServiceLauncher:createGenericOptionsParser(org.apache.hadoop.conf.Configuration,java.lang.String[])",979,982,"/**
 * Creates a GenericOptionsParser using the provided config and args.
 * @param conf Configuration object
 * @param argArray Command line arguments
 * @return GenericOptionsParser instance
 */
","* Override point: create a generic options parser or subclass thereof.
   * @param conf Hadoop configuration
   * @param argArray array of arguments
   * @return a generic options parser to parse the arguments
   * @throws IOException on any failure",,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ConfTest.java,main,org.apache.hadoop.util.ConfTest:main(java.lang.String[]),227,300,"/**
 * Parses command line arguments and validates configuration files.
 */",,,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ToolRunner.java,run,"org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.conf.Configuration,org.apache.hadoop.util.Tool,java.lang.String[])",62,83,"/**
 * Runs a Hadoop tool with provided configuration and arguments.
 * @param conf Hadoop configuration.
 * @param tool Tool to execute.
 * @param args Command-line arguments.
 * @return Exit code of the tool.
 */
","* Runs the given <code>Tool</code> by {@link Tool#run(String[])}, after 
   * parsing with the given generic arguments. Uses the given 
   * <code>Configuration</code>, or builds one if null.
   * 
   * Sets the <code>Tool</code>'s configuration with the possibly modified 
   * version of the <code>conf</code>.  
   * 
   * @param conf <code>Configuration</code> for the <code>Tool</code>.
   * @param tool <code>Tool</code> to run.
   * @param args command-line arguments to the tool.
   * @return exit code of the {@link Tool#run(String[])} method.
   * @throws Exception Exception.",,,True,39
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,parseCommandArgs,"org.apache.hadoop.service.launcher.ServiceLauncher:parseCommandArgs(org.apache.hadoop.conf.Configuration,java.util.List)",917,970,"/**
 * Parses command-line arguments, updates configuration, and returns remaining args.
 */","* Parse the command arguments, extracting the service class as the last
   * element of the list (after extracting all the rest).
   *
   * The field {@link #commandOptions} field must already have been set.
   * @param conf configuration to use
   * @param args command line argument list
   * @return the remaining arguments
   * @throws ServiceLaunchException if processing of arguments failed",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,exec,"org.apache.hadoop.security.KDiag:exec(org.apache.hadoop.conf.Configuration,java.lang.String[])",1052,1056,"/**
 * Executes the KDiag tool.
 * @param conf Hadoop configuration.
 * @param argv Command-line arguments.
 * @return Exit code from ToolRunner.
 */
","* Inner entry point, with no logging or system exits.
   *
   * @param conf configuration
   * @param argv argument list
   * @return an exception
   * @throws Exception Exception.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/token/DtUtilShell.java,main,org.apache.hadoop.security.token.DtUtilShell:main(java.lang.String[]),393,395,"/**
 * Main entry point; runs the DtUtilShell with provided arguments.
 */",,,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/alias/CredentialShell.java,main,org.apache.hadoop.security.alias.CredentialShell:main(java.lang.String[]),534,537,"/**
 * Main entry point; runs the CredentialShell tool with provided args.
 */","* Main program.
   *
   * @param args
   *          Command line arguments
   * @throws Exception exception.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/crypto/key/KeyShell.java,main,org.apache.hadoop.crypto.key.KeyShell:main(java.lang.String[]),552,555,"/**
 * Runs the KeyShell tool with provided arguments and exits.
 * @param args Command-line arguments for the tool.
 */
","* main() entry point for the KeyShell.  While strictly speaking the
   * return is void, it will System.exit() with a return code: 0 is for
   * success and 1 for failure.
   *
   * @param args Command line arguments.
   * @throws Exception raised on errors performing I/O.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/ToolRunner.java,run,"org.apache.hadoop.util.ToolRunner:run(org.apache.hadoop.util.Tool,java.lang.String[])",95,98,"/**
 * Delegates execution to the core run method.
 * @param tool The tool instance.
 * @param args Command-line arguments.
 * @return Result code.
 */
","* Runs the <code>Tool</code> with its <code>Configuration</code>.
   * 
   * Equivalent to <code>run(tool.getConf(), tool, args)</code>.
   * 
   * @param tool <code>Tool</code> to run.
   * @param args command-line arguments to the tool.
   * @return exit code of the {@link Tool#run(String[])} method.
   * @throws Exception exception.",,,True,40
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,extractCommandOptions,"org.apache.hadoop.service.launcher.ServiceLauncher:extractCommandOptions(org.apache.hadoop.conf.Configuration,java.util.List)",896,905,"/**
 * Extracts command options from arguments.
 * @param conf Configuration object. @param args Command line arguments.
 * @return List of extracted command options.
 */
","* Extract the command options and apply them to the configuration,
   * building an array of processed arguments to hand down to the service.
   *
   * @param conf configuration to update.
   * @param args main arguments. {@code args[0]}is assumed to be
   * the service classname and is skipped.
   * @return the remaining arguments
   * @throws ExitUtil.ExitException if JVM exiting is disabled.",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/security/KDiag.java,main,org.apache.hadoop.security.KDiag:main(java.lang.String[]),1062,1072,"/**
 * Main method: Executes command and handles termination/errors.
 */
","* Main entry point.
   * @param argv args list",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/fs/FsShell.java,main,org.apache.hadoop.fs.FsShell:main(java.lang.String[]),383,395,"/**
 * Main method to run the FsShell with provided arguments.
 * Runs the shell, exits with the shell's return code.
 */
","* main() has some simple utility methods
   * @param argv the command and its arguments
   * @throws Exception upon error",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/log/LogLevel.java,main,org.apache.hadoop.log.LogLevel:main(java.lang.String[]),73,76,"/**
 * Main method: parses CLI args, runs the CLI, and exits.
 */","* A command line implementation
   * @param args input args.
   * @throws Exception exception.",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/util/FindClass.java,main,org.apache.hadoop.util.FindClass:main(java.lang.String[]),378,386,"/**
 * Runs the FindClass tool and exits with its result or error code.
 */
","* Main entry point. 
   * Runs the class via the {@link ToolRunner}, then
   * exits with an appropriate exit code. 
   * @param args argument list",,,True,41
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,launchServiceAndExit,org.apache.hadoop.service.launcher.ServiceLauncher:launchServiceAndExit(java.util.List),281,315,"/**
 * Launches the service with given arguments and handles exit codes.
 * @param args Command-line arguments passed to the service.
 */
","* Launch the service and exit.
   *
   * <ol>
   * <li>Parse the command line.</li> 
   * <li>Build the service configuration from it.</li>
   * <li>Start the service.</li>
   * <li>If it is a {@link LaunchableService}: execute it</li>
   * <li>Otherwise: wait for it to finish.</li>
   * <li>Exit passing the status code to the {@link #exit(int, String)}
   * method.</li>
   * </ol>
   * @param args arguments to the service. {@code arg[0]} is 
   * assumed to be the service classname.",,,True,42
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,serviceMain,org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.util.List),1064,1073,"/**
 * Processes command-line arguments and launches a service.
 * @param argsList List of command-line arguments.
 */
",,,,True,43
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,main,org.apache.hadoop.service.launcher.ServiceLauncher:main(java.lang.String[]),1043,1045,"/**
 * Starts the application by invoking the serviceMain method.
 * @param args Command-line arguments passed to the application.
 */
","* This is the JVM entry point for the service launcher.
   *
   * Converts the arguments to a list, then invokes {@link #serviceMain(List)}
   * @param args command line arguments.",,,True,44
/home/yfx/codecomment/codecomment/project/java/org/apache/hadoop/service/launcher/ServiceLauncher.java,serviceMain,org.apache.hadoop.service.launcher.ServiceLauncher:serviceMain(java.lang.String[]),1052,1054,"/**
 * Delegates to the overloaded serviceMain with a List of args.
 */","* Varargs version of the entry point for testing and other in-JVM use.
   * Hands off to {@link #serviceMain(List)}
   * @param args command line arguments.",,,True,44
